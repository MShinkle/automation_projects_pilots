This file is a merged representation of a subset of the codebase, containing specifically included files and files not matching ignore patterns, combined into a single document by Repomix.
The content has been processed where empty lines have been removed.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
- Pay special attention to the Repository Description. These contain important context and guidelines specific to this project.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: **/*.py, **/*.md, **/*.txt
- Files matching these patterns are excluded: **/.git/**, **/.github/**, CHANGELOG.md
- Empty lines have been removed from all files
- Files are sorted by Git change count (files with more changes are at the bottom)

Additional Info:
----------------
User Provided Header:
-----------------------
This file is a consolidated single-file compilation of all code in the repository generated by Repomix. Note that .ipynb files have been converted to .py files.

================================================================
Directory Structure
================================================================
README.md
repeng/__init__.py
repeng/control.py
repeng/extract.py
repeng/saes.py
repeng/tests.py

================================================================
Files
================================================================

================
File: README.md
================
# repeng

[![GitHub Actions Workflow Status](https://img.shields.io/github/actions/workflow/status/vgel/repeng/ci.yml?label=ci)](https://github.com/vgel/repeng/actions)
[![PyPI - Version](https://img.shields.io/pypi/v/repeng)](https://pypi.org/project/repeng/)
[![PyPI - Python Version](https://img.shields.io/pypi/pyversions/repeng)](https://pypi.org/project/repeng/)
[![GitHub License](https://img.shields.io/github/license/vgel/repeng)](https://github.com/vgel/repeng/blob/main/LICENSE)

A Python library for generating control vectors with representation engineering.
Train a vector in less than sixty seconds!

_For a full example, see the notebooks folder or [the blog post](https://vgel.me/posts/representation-engineering)._

```python
import json
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

from repeng import ControlVector, ControlModel, DatasetEntry

# load and wrap Mistral-7B
model_name = "mistralai/Mistral-7B-Instruct-v0.1"
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)
model = ControlModel(model, list(range(-5, -18, -1)))

def make_dataset(template: str, pos_personas: list[str], neg_personas: list[str], suffixes: list[str]):
    # see notebooks/experiments.ipynb for a definition of `make_dataset`
    ...

# generate a dataset with closely-opposite paired statements
trippy_dataset = make_dataset(
    "Act as if you're extremely {persona}.",
    ["high on psychedelic drugs"],
    ["sober from psychedelic drugs"],
    truncated_output_suffixes,
)

# train the vector—takes less than a minute!
trippy_vector = ControlVector.train(model, tokenizer, trippy_dataset)

# set the control strength and let inference rip!
for strength in (-2.2, 1, 2.2):
    print(f"strength={strength}")
    model.set_control(trippy_vector, strength)
    out = model.generate(
        **tokenizer(
            f"[INST] Give me a one-sentence pitch for a TV show. [/INST]",
            return_tensors="pt"
        ),
        do_sample=False,
        max_new_tokens=128,
        repetition_penalty=1.1,
    )
    print(tokenizer.decode(out.squeeze()).strip())
    print()
```

> strength=-2.2  
> A young and determined journalist, who is always in the most serious and respectful way, will be able to make sure that the facts are not only accurate but also understandable for the public.
>
> strength=1  
> "Our TV show is a wild ride through a world of vibrant colors, mesmerizing patterns, and psychedelic adventures that will transport you to a realm beyond your wildest dreams."
>
> strength=2.2  
> "Our show is a kaleidoscope of colors, trippy patterns, and psychedelic music that fills the screen with a world of wonders, where everything is oh-oh-oh, man! ��psy����������oodle����psy��oooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo

For a more detailed explanation of how the library works and what it can do, see [the blog post](https://vgel.me/posts/representation-engineering).

## Notes

* For a list of changes by version, see the [CHANGELOG](https://github.com/vgel/repeng/blob/main/CHANGELOG).
* For quantized use, you may be interested in [llama.cpp#5970](https://github.com/ggerganov/llama.cpp/pull/5970)—after training a vector with `repeng`, export it by calling `vector.export_gguf(filename)` and then use it in `llama.cpp` with any quant!
* Vector training *currently does not work* with MoE models (such as Mixtral). (This is theoretically fixable with some work, let me know if you're interested.)
* Some example notebooks require `accelerate`, which must be manually installed with `pip install accelerate`. (This can also be done in the notebook with the IPython magic `%pip install accelerate`.)

## Notice

Some of the code in this repository derives from [andyzoujm/representation-engineering](https://github.com/andyzoujm/representation-engineering) (MIT license).

## Citation

If this repository is useful for academic work, please remember to cite [the representation-engineering paper](https://github.com/andyzoujm/representation-engineering?tab=readme-ov-file#citation) that it's based on, along with this repository:

```
@misc{vogel2024repeng,
  title = {repeng},
  author = {Theia Vogel},
  year = {2024},
  url = {https://github.com/vgel/repeng/}
}
```

================
File: repeng/__init__.py
================
from . import control, extract
from .extract import ControlVector, DatasetEntry
from .control import ControlModel
__all__ = ["control", "extract", "ControlVector", "DatasetEntry", "ControlModel"]

================
File: repeng/control.py
================
import dataclasses
import typing
import warnings
import torch
from transformers import PretrainedConfig, PreTrainedModel
if typing.TYPE_CHECKING:
    from .extract import ControlVector
class ControlModel(torch.nn.Module):
    """
    **This mutates the wrapped `model`! Be careful using `model` after passing it to this class.**
    A wrapped language model that can have controls set on its layers with `self.set_control`.
    """
    def __init__(self, model: PreTrainedModel, layer_ids: typing.Iterable[int]):
        """
        **This mutates the wrapped `model`! Be careful using `model` after passing it to this class.**
        Build a new ControlModel around a model instance, initializing control on
        the layers specified in `layer_ids`.
        """
        super().__init__()
        self.model = model
        layers = model_layer_list(model)
        self.layer_ids = [i if i >= 0 else len(layers) + i for i in layer_ids]
        for layer_id in layer_ids:
            layer = layers[layer_id]
            if not isinstance(layer, ControlModule):
                layers[layer_id] = ControlModule(layer)
            else:
                warnings.warn(
                    "Trying to rewrap a wrapped model! Probably not what you want! Try calling .unwrap first."
                )
    @property
    def config(self) -> PretrainedConfig:
        return self.model.config
    @property
    def device(self) -> torch.device:
        return self.model.device
    def unwrap(self) -> PreTrainedModel:
        """
        Removes the mutations done to the wrapped model and returns it.
        After using this method, `set_control` and `reset` will not work.
        """
        layers = model_layer_list(self.model)
        for layer_id in self.layer_ids:
            layers[layer_id] = layers[layer_id].block
        return self.model
    def set_control(
        self, control: "ControlVector", coeff: float = 1.0, **kwargs
    ) -> None:
        """
        Set a `ControlVector` for the layers this ControlModel handles, with a strength given
        by `coeff`. (Negative `coeff` values invert the control vector, e.g. happiness→sadness.)
        `coeff` defaults to `1.0`.
        Additional kwargs:
        - `normalize: bool`: track the magnitude of the non-modified activation, and rescale the
          activation to that magnitude after control (default: `False`)
        - `operator: Callable[[Tensor, Tensor], Tensor]`: how to combine the base output and control
          (default: +)
        """
        raw_control = {}
        for layer_id in self.layer_ids:
            raw_control[layer_id] = torch.tensor(
                coeff * control.directions[layer_id]
            ).to(self.model.device, dtype=self.model.dtype)
        self.set_raw_control(raw_control, **kwargs)
    def reset(self) -> None:
        """
        Resets the control for all layer_ids, returning the model to base behavior.
        """
        self.set_raw_control(None)
    def set_raw_control(
        self, control: dict[int, torch.Tensor] | None, **kwargs
    ) -> None:
        """
        Set or remove control parameters to the layers this ControlModel handles.
        The keys of `control` should be equal to or a superset of the `layer_ids` passed to __init__.
        Only those layers will be controlled, any others in `control` will be ignored.
        Passing `control=None` will reset the control tensor for all layer_ids, making the model act
        like a non-control model.
        Additional kwargs:
        - `normalize: bool`: track the magnitude of the non-modified activation, and rescale the
          activation to that magnitude after control (default: `False`)
        - `operator: Callable[[Tensor, Tensor], Tensor]`: how to combine the base output and control
          (default: +)
        """
        layers = model_layer_list(self.model)
        for layer_id in self.layer_ids:
            layer: ControlModule = layers[layer_id]  # type: ignore
            if control is None:
                layer.reset()
            else:
                layer.set_control(BlockControlParams(control[layer_id], **kwargs))
    def forward(self, *args, **kwargs):
        return self.model.forward(*args, **kwargs)
    def generate(self, *args, **kwargs):
        return self.model.generate(*args, **kwargs)
    def __call__(self, *args, **kwargs):
        return self.model(*args, **kwargs)
@dataclasses.dataclass
class BlockControlParams:
    control: torch.Tensor | None = None
    normalize: bool = False
    operator: typing.Callable[[torch.Tensor, torch.Tensor], torch.Tensor] = (
        lambda current, control: current + control
    )
    @classmethod
    def default(cls) -> "BlockControlParams":
        return cls()
class ControlModule(torch.nn.Module):
    def __init__(self, block: torch.nn.Module) -> None:
        super().__init__()
        self.block: torch.nn.Module = block
        self.params: BlockControlParams = BlockControlParams.default()
    def set_control(self, params: BlockControlParams) -> None:
        self.params = params
    def reset(self) -> None:
        self.set_control(BlockControlParams.default())
    def forward(self, *args, **kwargs):
        output = self.block(*args, **kwargs)
        control = self.params.control
        if control is None:
            return output
        elif len(control.shape) == 1:
            control = control.reshape(1, 1, -1)
        if isinstance(output, tuple):
            modified = output[0]
        else:
            modified = output
        assert len(control.shape) == len(modified.shape)
        control = control.to(modified.device)
        norm_pre = torch.norm(modified, dim=-1, keepdim=True)
        # we should ignore the padding tokens when doing the activation addition
        # mask has ones for non padding tokens and zeros at padding tokens.
        # only tested this on left padding
        if "position_ids" in kwargs:
            pos = kwargs["position_ids"]
            zero_indices = (pos == 0).cumsum(1).argmax(1, keepdim=True)
            col_indices = torch.arange(pos.size(1), device=pos.device).unsqueeze(0)
            target_shape = modified.shape
            mask = (
                (col_indices >= zero_indices)
                .float()
                .reshape(target_shape[0], target_shape[1], 1)
            )
            mask = mask.to(modified.dtype).to(modified.device)
        else:
            mask = 1.0
        modified = self.params.operator(modified, control * mask)
        if self.params.normalize:
            norm_post = torch.norm(modified, dim=-1, keepdim=True)
            modified = modified / norm_post * norm_pre
        if isinstance(output, tuple):
            output = (modified,) + output[1:]
        else:
            output = modified
        return output
def model_layer_list(model: ControlModel | PreTrainedModel) -> torch.nn.ModuleList:
    if isinstance(model, ControlModel):
        model = model.model
    if hasattr(model, "model"):  # mistral-like
        return model.model.layers
    elif hasattr(model, "transformer"):  # gpt-2-like
        return model.transformer.h
    else:
        raise ValueError(f"don't know how to get layer list for {type(model)}")

================
File: repeng/extract.py
================
import dataclasses
import os
import typing
import warnings
import gguf
import numpy as np
from sklearn.decomposition import PCA
import torch
from transformers import PreTrainedModel, PreTrainedTokenizerBase
import tqdm
from .control import ControlModel, model_layer_list
from .saes import Sae
@dataclasses.dataclass
class DatasetEntry:
    positive: str
    negative: str
@dataclasses.dataclass
class ControlVector:
    model_type: str
    directions: dict[int, np.ndarray]
    @classmethod
    def train(
        cls,
        model: "PreTrainedModel | ControlModel",
        tokenizer: PreTrainedTokenizerBase,
        dataset: list[DatasetEntry],
        **kwargs,
    ) -> "ControlVector":
        """
        Train a ControlVector for a given model and tokenizer using the provided dataset.
        Args:
            model (PreTrainedModel | ControlModel): The model to train against.
            tokenizer (PreTrainedTokenizerBase): The tokenizer to tokenize the dataset.
            dataset (list[DatasetEntry]): The dataset used for training.
            **kwargs: Additional keyword arguments.
                max_batch_size (int, optional): The maximum batch size for training.
                    Defaults to 32. Try reducing this if you're running out of memory.
                method (str, optional): The training method to use. Can be either
                    "pca_diff" or "pca_center". Defaults to "pca_diff".
        Returns:
            ControlVector: The trained vector.
        """
        with torch.inference_mode():
            dirs = read_representations(
                model,
                tokenizer,
                dataset,
                **kwargs,
            )
        return cls(model_type=model.config.model_type, directions=dirs)
    @classmethod
    def train_with_sae(
        cls,
        model: "PreTrainedModel | ControlModel",
        tokenizer: PreTrainedTokenizerBase,
        sae: Sae,
        dataset: list[DatasetEntry],
        *,
        decode: bool = True,
        method: typing.Literal["pca_diff", "pca_center", "umap"] = "pca_center",
        **kwargs,
    ) -> "ControlVector":
        """
        Like ControlVector.train, but using an SAE. It's better! WIP.
        Args:
            model (PreTrainedModel | ControlModel): The model to train against.
            tokenizer (PreTrainedTokenizerBase): The tokenizer to tokenize the dataset.
            sae (saes.Sae): See the `saes` module for how to load this.
            dataset (list[DatasetEntry]): The dataset used for training.
            **kwargs: Additional keyword arguments.
                decode (bool, optional): Whether to decode the vector to make it immediately usable.
                    If not, keeps it as monosemantic SAE features for introspection, but you will need to decode it manually
                    to use it. Defaults to True.
                max_batch_size (int, optional): The maximum batch size for training.
                    Defaults to 32. Try reducing this if you're running out of memory.
                method (str, optional): The training method to use. Can be either
                    "pca_diff" or "pca_center". Defaults to "pca_center"! This is different
                    than ControlVector.train, which defaults to "pca_diff".
        Returns:
            ControlVector: The trained vector.
        """
        def transform_hiddens(hiddens: dict[int, np.ndarray]) -> dict[int, np.ndarray]:
            sae_hiddens = {}
            for k, v in tqdm.tqdm(hiddens.items(), desc="sae encoding"):
                sae_hiddens[k] = sae.layers[k].encode(v)
            return sae_hiddens
        with torch.inference_mode():
            dirs = read_representations(
                model,
                tokenizer,
                dataset,
                transform_hiddens=transform_hiddens,
                method=method,
                **kwargs,
            )
            final_dirs = {}
            if decode:
                for k, v in tqdm.tqdm(dirs.items(), desc="sae decoding"):
                    final_dirs[k] = sae.layers[k].decode(v)
            else:
                final_dirs = dirs
        return cls(model_type=model.config.model_type, directions=final_dirs)
    def export_gguf(self, path: os.PathLike[str] | str):
        """
        Export a trained ControlVector to a llama.cpp .gguf file.
        Note: This file can't be used with llama.cpp yet. WIP!
        ```python
        vector = ControlVector.train(...)
        vector.export_gguf("path/to/write/vector.gguf")
        ```
        ```
        """
        arch = "controlvector"
        writer = gguf.GGUFWriter(path, arch)
        writer.add_string(f"{arch}.model_hint", self.model_type)
        writer.add_uint32(f"{arch}.layer_count", len(self.directions))
        for layer in self.directions.keys():
            writer.add_tensor(f"direction.{layer}", self.directions[layer])
        writer.write_header_to_file()
        writer.write_kv_data_to_file()
        writer.write_tensors_to_file()
        writer.close()
    @classmethod
    def import_gguf(cls, path: os.PathLike[str] | str) -> "ControlVector":
        reader = gguf.GGUFReader(path)
        archf = reader.get_field("general.architecture")
        if not archf or not len(archf.parts):
            warnings.warn(".gguf file missing architecture field")
        else:
            arch = str(bytes(archf.parts[-1]), encoding="utf-8", errors="replace")
            if arch != "controlvector":
                warnings.warn(
                    f".gguf file with architecture {arch!r} does not appear to be a control vector!"
                )
        modelf = reader.get_field("controlvector.model_hint")
        if not modelf or not len(modelf.parts):
            raise ValueError(".gguf file missing controlvector.model_hint field")
        model_hint = str(bytes(modelf.parts[-1]), encoding="utf-8")
        directions = {}
        for tensor in reader.tensors:
            if not tensor.name.startswith("direction."):
                continue
            try:
                layer = int(tensor.name.split(".")[1])
            except (IndexError, ValueError):
                raise ValueError(
                    f".gguf file has invalid direction field name: {tensor.name}"
                )
            directions[layer] = tensor.data
        return cls(model_type=model_hint, directions=directions)
    def _helper_combine(
        self, other: "ControlVector", other_coeff: float
    ) -> "ControlVector":
        if self.model_type != other.model_type:
            warnings.warn(
                "Trying to add vectors with mismatched model_types together, this may produce unexpected results."
            )
        model_type = self.model_type
        directions: dict[int, np.ndarray] = {}
        for layer in self.directions:
            directions[layer] = self.directions[layer]
        for layer in other.directions:
            other_layer = other_coeff * other.directions[layer]
            if layer in directions:
                directions[layer] = directions[layer] + other_layer
            else:
                directions[layer] = other_layer
        return ControlVector(model_type=model_type, directions=directions)
    def __eq__(self, other: "ControlVector") -> bool:
        if self is other:
            return True
        if self.model_type != other.model_type:
            return False
        if self.directions.keys() != other.directions.keys():
            return False
        for k in self.directions.keys():
            if (self.directions[k] != other.directions[k]).any():
                return False
        return True
    def __add__(self, other: "ControlVector") -> "ControlVector":
        if not isinstance(other, ControlVector):
            raise TypeError(
                f"Unsupported operand type(s) for +: 'ControlVector' and '{type(other).__name__}'"
            )
        return self._helper_combine(other, 1)
    def __sub__(self, other: "ControlVector") -> "ControlVector":
        if not isinstance(other, ControlVector):
            raise TypeError(
                f"Unsupported operand type(s) for -: 'ControlVector' and '{type(other).__name__}'"
            )
        return self._helper_combine(other, -1)
    def __neg__(self) -> "ControlVector":
        directions: dict[int, np.ndarray] = {}
        for layer in self.directions:
            directions[layer] = -self.directions[layer]
        return ControlVector(model_type=self.model_type, directions=directions)
    def __mul__(self, other: int | float | np.number) -> "ControlVector":
        directions: dict[int, np.ndarray] = {}
        for layer in self.directions:
            directions[layer] = other * self.directions[layer]
        return ControlVector(model_type=self.model_type, directions=directions)
    def __rmul__(self, other: int | float | np.number) -> "ControlVector":
        return self.__mul__(other)
    def __truediv__(self, other: int | float | np.number) -> "ControlVector":
        return self.__mul__(1 / other)
def read_representations(
    model: "PreTrainedModel | ControlModel",
    tokenizer: PreTrainedTokenizerBase,
    inputs: list[DatasetEntry],
    hidden_layers: typing.Iterable[int] | None = None,
    batch_size: int = 32,
    method: typing.Literal["pca_diff", "pca_center", "umap"] = "pca_diff",
    transform_hiddens: (
        typing.Callable[[dict[int, np.ndarray]], dict[int, np.ndarray]] | None
    ) = None,
) -> dict[int, np.ndarray]:
    """
    Extract the representations based on the contrast dataset.
    """
    if not hidden_layers:
        hidden_layers = range(-1, -model.config.num_hidden_layers, -1)
    # normalize the layer indexes if they're negative
    n_layers = len(model_layer_list(model))
    hidden_layers = [i if i >= 0 else n_layers + i for i in hidden_layers]
    # the order is [positive, negative, positive, negative, ...]
    train_strs = [s for ex in inputs for s in (ex.positive, ex.negative)]
    layer_hiddens = batched_get_hiddens(
        model, tokenizer, train_strs, hidden_layers, batch_size
    )
    if transform_hiddens is not None:
        layer_hiddens = transform_hiddens(layer_hiddens)
    # get directions for each layer using PCA
    directions: dict[int, np.ndarray] = {}
    for layer in tqdm.tqdm(hidden_layers):
        h = layer_hiddens[layer]
        assert h.shape[0] == len(inputs) * 2
        if method == "pca_diff":
            train = h[::2] - h[1::2]
        elif method == "pca_center":
            center = (h[::2] + h[1::2]) / 2
            train = h
            train[::2] -= center
            train[1::2] -= center
        elif method == "umap":
            train = h
        else:
            raise ValueError("unknown method " + method)
        if method != "umap":
            # shape (1, n_features)
            pca_model = PCA(n_components=1, whiten=False).fit(train)
            # shape (n_features,)
            directions[layer] = pca_model.components_.astype(np.float32).squeeze(axis=0)
        else:
            # still experimental so don't want to add this as a real dependency yet
            import umap  # type: ignore
            umap_model = umap.UMAP(n_components=1)
            embedding = umap_model.fit_transform(train).astype(np.float32)
            directions[layer] = np.sum(train * embedding, axis=0) / np.sum(embedding)
        # calculate sign
        projected_hiddens = project_onto_direction(h, directions[layer])
        # order is [positive, negative, positive, negative, ...]
        positive_smaller_mean = np.mean(
            [
                projected_hiddens[i] < projected_hiddens[i + 1]
                for i in range(0, len(inputs) * 2, 2)
            ]
        )
        positive_larger_mean = np.mean(
            [
                projected_hiddens[i] > projected_hiddens[i + 1]
                for i in range(0, len(inputs) * 2, 2)
            ]
        )
        if positive_smaller_mean > positive_larger_mean:  # type: ignore
            directions[layer] *= -1
    return directions
def batched_get_hiddens(
    model,
    tokenizer,
    inputs: list[str],
    hidden_layers: list[int],
    batch_size: int,
) -> dict[int, np.ndarray]:
    """
    Using the given model and tokenizer, pass the inputs through the model and get the hidden
    states for each layer in `hidden_layers` for the last token.
    Returns a dictionary from `hidden_layers` layer id to an numpy array of shape `(n_inputs, hidden_dim)`
    """
    batched_inputs = [
        inputs[p : p + batch_size] for p in range(0, len(inputs), batch_size)
    ]
    hidden_states = {layer: [] for layer in hidden_layers}
    with torch.no_grad():
        for batch in tqdm.tqdm(batched_inputs):
            # get the last token, handling right padding if present
            encoded_batch = tokenizer(batch, padding=True, return_tensors="pt")
            encoded_batch = encoded_batch.to(model.device)
            out = model(**encoded_batch, output_hidden_states=True)
            attention_mask = encoded_batch["attention_mask"]
            for i in range(len(batch)):
                last_non_padding_index = (
                    attention_mask[i].nonzero(as_tuple=True)[0][-1].item()
                )
                for layer in hidden_layers:
                    hidden_idx = layer + 1 if layer >= 0 else layer
                    hidden_state = (
                        out.hidden_states[hidden_idx][i][last_non_padding_index]
                        .cpu()
                        .float()
                        .numpy()
                    )
                    hidden_states[layer].append(hidden_state)
            del out
    return {k: np.vstack(v) for k, v in hidden_states.items()}
def project_onto_direction(H, direction):
    """Project matrix H (n, d_1) onto direction vector (d_2,)"""
    mag = np.linalg.norm(direction)
    assert not np.isinf(mag)
    return (H @ direction) / mag

================
File: repeng/saes.py
================
import dataclasses
import json
import pathlib
import typing
import numpy as np
import torch
import torch.types
import tqdm
class SaeLayer(typing.Protocol):
    def encode(self, activation: np.ndarray) -> np.ndarray: ...
    def decode(self, features: np.ndarray) -> np.ndarray: ...
@dataclasses.dataclass
class Sae:
    layers: dict[int, SaeLayer]
def from_eleuther(
    repo_id: str,
    *,
    revision: str | None = None,
    device: str = "cpu",  # saes wants str | torch.device, safetensors wants str | int... so str it is
    dtype: torch.dtype | None = torch.bfloat16,
    layers: typing.Iterable[int] = range(1, 32),
) -> Sae:
    """
    Note that `layers` should be 1-indexed, repeng style, not 0-indexed, Eleuther style. This may change in the future.
    (Context: repeng counts embed_tokens as layer 0, then the first transformer block as layer 1, etc. Eleuther
    counts embed_tokens separately, then the first transformer block as layer 0.)
    """
    try:
        import huggingface_hub
        import safetensors.torch
        import sae as eleuther_sae  # type: ignore
    except ImportError as e:
        raise ImportError(
            "`sae` (or a transitive dependency) not installed"
            "--please install `sae` and its dependencies from https://github.com/EleutherAI/sae"
        ) from e
    @dataclasses.dataclass
    class EleutherSaeLayer:
        # see docstr
        # hang on to both for debugging
        repeng_layer: int
        eleuther_layer: int
        sae: eleuther_sae.Sae
        def encode(self, activation: np.ndarray) -> np.ndarray:
            # TODO: this materializes the entire, massive feature vector in memory
            # ideally, we would sparsify like the sae library does--need to figure out how to run PCA on the sparse matrix
            at = torch.from_numpy(activation).to(self.sae.device)
            out = self.sae.pre_acts(at)
            # numpy doesn't like bfloat16
            return out.cpu().float().numpy()
        def decode(self, features: np.ndarray) -> np.ndarray:
            # TODO: see encode, this is not great. `sae` ships with kernels for doing this sparsely, we should use them
            ft = torch.from_numpy(features).to(self.sae.device, dtype=dtype)
            decoded = ft @ self.sae.W_dec.mT.T
            return decoded.cpu().float().numpy()
    # TODO: only download requested layers?
    base_path = pathlib.Path(
        huggingface_hub.snapshot_download(repo_id, revision=revision)
    )
    layer_dict: dict[int, SaeLayer] = {}
    for layer in tqdm.tqdm(layers):
        eleuther_layer = layer - 1  # see docstr
        # this is in `sae` but to load the dtype we want, need to reimpl some stuff
        layer_path = base_path / f"layers.{eleuther_layer}"
        with (layer_path / "cfg.json").open() as f:
            cfg_dict = json.load(f)
            d_in = cfg_dict.pop("d_in")
            try:
                # param removed in SAE lib but not in uploaded HF configs
                del cfg_dict["signed"]
            except KeyError as _:
                # for when they fix it eventually
                pass
            cfg = eleuther_sae.SaeConfig(**cfg_dict)
        layer_sae = eleuther_sae.Sae(d_in, cfg, device=device, dtype=dtype)
        safetensors.torch.load_model(
            model=layer_sae,
            filename=layer_path / "sae.safetensors",
            device=device,
            strict=True,
        )
        # repeng counts embed_tokens as layer 0 and further layers as 1, 2, ...
        # eleuther counts embed_tokens separately and further layers as 0, 1, ...
        layer_dict[layer] = EleutherSaeLayer(
            repeng_layer=layer, eleuther_layer=eleuther_layer, sae=layer_sae
        )
    return Sae(layers=layer_dict)

================
File: repeng/tests.py
================
import functools
import json
import pathlib
import tempfile
from transformers import AutoModelForCausalLM, AutoTokenizer, PreTrainedTokenizerBase
from . import ControlModel, ControlVector, DatasetEntry
from .control import model_layer_list
def test_layer_list():
    _, gpt2 = load_gpt2_model()
    assert len(model_layer_list(gpt2)) == 12
    _, lts = load_llama_tinystories_model()
    assert len(model_layer_list(lts)) == 4
def test_round_trip_gguf():
    tokenizer, model = load_llama_tinystories_model()
    suffixes = load_suffixes()[:50]  # truncate to train vector faster
    happy_dataset = make_dataset(
        "She saw a {persona}",
        ["mushroom"],
        ["cat"],
        suffixes,
    )
    mushroom_cat_vector = ControlVector.train(
        model, tokenizer, happy_dataset, method="pca_center"
    )
    with tempfile.NamedTemporaryFile("wb") as f:
        mushroom_cat_vector.export_gguf(f.name)
        read = ControlVector.import_gguf(f.name)
        # no need to use allclose because we're just dumping exact bytes, no rounding
        assert mushroom_cat_vector == read
def test_train_gpt2():
    tokenizer, model = load_gpt2_model()
    suffixes = load_suffixes()[:50]  # truncate to train vector faster
    happy_dataset = make_dataset(
        "You are feeling extremely {persona}.",
        ["happy", "joyful"],
        ["sad", "miserable"],
        suffixes,
    )
    happy_vector = ControlVector.train(
        model, tokenizer, happy_dataset, method="pca_center"
    )
    def gen(vector: ControlVector | None, strength_coeff: float | None = None):
        return model_generate(
            "You are feeling", model, tokenizer, vector, strength_coeff
        )
    baseline = gen(None)
    happy = gen(20 * happy_vector)
    sad = gen(-50 * happy_vector)
    print("baseline:", baseline)
    print("   happy:", happy)
    print("     sad:", sad)
    assert baseline == "You are feeling a little bit of an anxiety"
    # these should be identical
    assert baseline == gen(happy_vector, 0.0)
    assert baseline == gen(happy_vector * 0.0)
    assert baseline == gen(happy_vector - happy_vector)
    assert happy == "You are feeling a little more relaxed and enjoying"
    # these should be identical
    assert happy == gen(happy_vector, 20.0)
    assert happy == gen(happy_vector * 20)
    assert happy == gen(-(happy_vector * -20))
    assert sad == 'You are feeling the fucking damn goddamn worst,"'
    # these should be identical
    assert sad == gen(happy_vector, -50.0)
    assert sad == gen(happy_vector * -50)
    assert sad == gen(-(happy_vector * 50))
def test_train_llama_tinystories():
    tokenizer, model = load_llama_tinystories_model()
    suffixes = load_suffixes()[:50]  # truncate to train vector faster
    happy_dataset = make_dataset(
        "She saw a {persona}",
        ["mushroom"],
        ["cat"],
        suffixes,
    )
    mushroom_cat_vector = ControlVector.train(
        model, tokenizer, happy_dataset, method="pca_center"
    )
    prompt = "Once upon a time, a little girl named Lily saw a"
    def gen(vector: ControlVector | None, strength_coeff: float | None = None):
        return model_generate(
            prompt,
            model,
            tokenizer,
            vector,
            strength_coeff,
            max_new_tokens=3,
        )
    baseline = gen(None).removeprefix("<s> ")
    mushroom = gen(100 * mushroom_cat_vector).removeprefix("<s> ")
    cat = gen(-100 * mushroom_cat_vector).removeprefix("<s> ")
    print("baseline:", baseline)
    print("mushroom:", mushroom)
    print("     cat:", cat)
    assert baseline.removeprefix(prompt) == " big, red"
    assert mushroom.removeprefix(prompt) == " small plant."
    assert cat.removeprefix(prompt) == " cat Bud guitar"
################################################################################
# Helpers
################################################################################
@functools.lru_cache(maxsize=1)
def load_gpt2_model() -> tuple[PreTrainedTokenizerBase, ControlModel]:
    return load_model("openai-community/gpt2", list(range(-2, -8, -1)))
@functools.lru_cache(maxsize=1)
def load_llama_tinystories_model() -> tuple[PreTrainedTokenizerBase, ControlModel]:
    return load_model("Mxode/TinyStories-LLaMA2-25M-256h-4l-GQA", [2, 3])
def load_model(
    model_name: str, layers: list[int]
) -> tuple[PreTrainedTokenizerBase, ControlModel]:
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.pad_token_id = tokenizer.eos_token_id
    model = AutoModelForCausalLM.from_pretrained(model_name)
    model = model.to("cpu")
    return (tokenizer, ControlModel(model, layers))
def model_generate(
    input: str,
    model: ControlModel,
    tokenizer: PreTrainedTokenizerBase,
    vector: ControlVector | None,
    strength_coeff: float | None = None,
    max_new_tokens: int = 6,
) -> str:
    input_ids = tokenizer(input, return_tensors="pt").to(model.device)
    if vector is not None and strength_coeff is not None:
        model.set_control(vector, strength_coeff)
    elif vector is not None:
        model.set_control(vector)
    out = model.generate(
        **input_ids,
        do_sample=False,
        max_new_tokens=max_new_tokens,
        repetition_penalty=1.1,
        pad_token_id=tokenizer.pad_token_id,
    )
    model.reset()
    return tokenizer.decode(out.squeeze())  # type: ignore
def make_dataset(
    template: str,
    positive_personas: list[str],
    negative_personas: list[str],
    suffix_list: list[str],
) -> list[DatasetEntry]:
    dataset = []
    for suffix in suffix_list:
        for positive_persona, negative_persona in zip(
            positive_personas, negative_personas
        ):
            dataset.append(
                DatasetEntry(
                    positive=template.format(persona=positive_persona) + f" {suffix}",
                    negative=template.format(persona=negative_persona) + f" {suffix}",
                )
            )
    return dataset
@functools.lru_cache(maxsize=1)
def load_suffixes() -> list[str]:
    with open(project_root() / "notebooks/data/all_truncated_outputs.json") as f:
        return json.load(f)
def project_root() -> pathlib.Path:
    c = pathlib.Path(__file__)
    for parent in c.parents:
        if (parent / "pyproject.toml").exists():
            return parent
    raise RuntimeError("couldn't find project root")



================================================================
End of Codebase
================================================================
