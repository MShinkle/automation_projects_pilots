This file is a merged representation of a subset of the codebase, containing specifically included files and files not matching ignore patterns, combined into a single document by Repomix.
The content has been processed where empty lines have been removed.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
- Pay special attention to the Repository Description. These contain important context and guidelines specific to this project.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: **/*.py, **/*.md, **/*.txt
- Files matching these patterns are excluded: **/.git/**, **/.github/**, CHANGELOG.md
- Empty lines have been removed from all files
- Files are sorted by Git change count (files with more changes are at the bottom)

Additional Info:
----------------
User Provided Header:
-----------------------
This file is a consolidated single-file compilation of all code in the repository generated by Repomix. Note that .ipynb files have been converted to .py files.

================================================================
Directory Structure
================================================================
build/lib/neuron_explainer/api_client.py
build/lib/neuron_explainer/explanations/attention_head_scoring.py
build/lib/neuron_explainer/explanations/calibrated_simulator.py
build/lib/neuron_explainer/explanations/explainer.py
build/lib/neuron_explainer/explanations/explanations.py
build/lib/neuron_explainer/explanations/few_shot_examples.py
build/lib/neuron_explainer/explanations/prompt_builder.py
build/lib/neuron_explainer/explanations/scoring.py
build/lib/neuron_explainer/explanations/simulator.py
build/lib/neuron_explainer/explanations/test_explainer.py
build/lib/neuron_explainer/explanations/test_simulator.py
build/lib/neuron_explainer/fast_dataclasses/__init__.py
build/lib/neuron_explainer/fast_dataclasses/fast_dataclasses.py
build/lib/neuron_explainer/fast_dataclasses/test_fast_dataclasses.py
build/lib/neuron_explainer/file_utils.py
build/lib/neuron_explainer/models/__init__.py
build/lib/neuron_explainer/models/autoencoder_context.py
build/lib/neuron_explainer/models/autoencoder.py
build/lib/neuron_explainer/models/hooks.py
build/lib/neuron_explainer/models/inference_engine_type_registry.py
build/lib/neuron_explainer/models/model_component_registry.py
build/lib/neuron_explainer/models/model_context.py
build/lib/neuron_explainer/models/model_registry.py
build/lib/neuron_explainer/models/transformer.py
build/lib/neuron_explainer/pydantic/__init__.py
build/lib/neuron_explainer/pydantic/camel_case_base_model.py
build/lib/neuron_explainer/pydantic/hashable_base_model.py
build/lib/neuron_explainer/pydantic/immutable.py
datasets.md
neuron_explainer.egg-info/dependency_links.txt
neuron_explainer.egg-info/requires.txt
neuron_explainer.egg-info/SOURCES.txt
neuron_explainer.egg-info/top_level.txt
neuron_explainer/activation_server/derived_scalar_computation.py
neuron_explainer/activation_server/dst_helpers.py
neuron_explainer/activation_server/explainer_routes.py
neuron_explainer/activation_server/explanation_datasets.py
neuron_explainer/activation_server/inference_routes.py
neuron_explainer/activation_server/interactive_model.py
neuron_explainer/activation_server/load_neurons.py
neuron_explainer/activation_server/main.py
neuron_explainer/activation_server/neuron_datasets.py
neuron_explainer/activation_server/read_routes.py
neuron_explainer/activation_server/README.md
neuron_explainer/activation_server/requests_and_responses.py
neuron_explainer/activation_server/tdb_conversions.py
neuron_explainer/activations/activation_records.py
neuron_explainer/activations/activations.py
neuron_explainer/activations/attention_utils.py
neuron_explainer/activations/derived_scalars/__init__.py
neuron_explainer/activations/derived_scalars/activations_and_metadata.py
neuron_explainer/activations/derived_scalars/attention.py
neuron_explainer/activations/derived_scalars/autoencoder.py
neuron_explainer/activations/derived_scalars/config.py
neuron_explainer/activations/derived_scalars/derived_scalar_store.py
neuron_explainer/activations/derived_scalars/derived_scalar_types.py
neuron_explainer/activations/derived_scalars/direct_effects.py
neuron_explainer/activations/derived_scalars/edge_activation.py
neuron_explainer/activations/derived_scalars/edge_attribution.py
neuron_explainer/activations/derived_scalars/indexing.py
neuron_explainer/activations/derived_scalars/locations.py
neuron_explainer/activations/derived_scalars/logprobs.py
neuron_explainer/activations/derived_scalars/make_scalar_derivers.py
neuron_explainer/activations/derived_scalars/mlp.py
neuron_explainer/activations/derived_scalars/multi_group.py
neuron_explainer/activations/derived_scalars/multi_pass_scalar_deriver.py
neuron_explainer/activations/derived_scalars/node_write.py
neuron_explainer/activations/derived_scalars/postprocessing.py
neuron_explainer/activations/derived_scalars/raw_activations.py
neuron_explainer/activations/derived_scalars/README.md
neuron_explainer/activations/derived_scalars/reconstituted.py
neuron_explainer/activations/derived_scalars/reconstituter_class.py
neuron_explainer/activations/derived_scalars/residual.py
neuron_explainer/activations/derived_scalars/scalar_deriver.py
neuron_explainer/activations/derived_scalars/tests/test_attention.py
neuron_explainer/activations/derived_scalars/tests/test_derived_scalar_store.py
neuron_explainer/activations/derived_scalars/tests/test_derived_scalar_types.py
neuron_explainer/activations/derived_scalars/tests/utils.py
neuron_explainer/activations/derived_scalars/tokens.py
neuron_explainer/activations/derived_scalars/utils.py
neuron_explainer/activations/derived_scalars/write_tensors.py
neuron_explainer/activations/hook_graph.py
neuron_explainer/activations/test_attention_utils.py
neuron_explainer/api_client.py
neuron_explainer/explanations/attention_head_scoring.py
neuron_explainer/explanations/calibrated_simulator.py
neuron_explainer/explanations/explainer.py
neuron_explainer/explanations/explanations.py
neuron_explainer/explanations/few_shot_examples.py
neuron_explainer/explanations/prompt_builder.py
neuron_explainer/explanations/scoring.py
neuron_explainer/explanations/simulator.py
neuron_explainer/explanations/test_explainer.py
neuron_explainer/explanations/test_simulator.py
neuron_explainer/fast_dataclasses/__init__.py
neuron_explainer/fast_dataclasses/fast_dataclasses.py
neuron_explainer/fast_dataclasses/test_fast_dataclasses.py
neuron_explainer/file_utils.py
neuron_explainer/models/__init__.py
neuron_explainer/models/autoencoder_context.py
neuron_explainer/models/autoencoder.py
neuron_explainer/models/hooks.py
neuron_explainer/models/inference_engine_type_registry.py
neuron_explainer/models/model_component_registry.py
neuron_explainer/models/model_context.py
neuron_explainer/models/model_registry.py
neuron_explainer/models/README.md
neuron_explainer/models/transformer.py
neuron_explainer/pydantic/__init__.py
neuron_explainer/pydantic/camel_case_base_model.py
neuron_explainer/pydantic/hashable_base_model.py
neuron_explainer/pydantic/immutable.py
neuron_explainer/scripts/create_hf_test_data.py
neuron_explainer/scripts/download_from_hf.py
neuron_explainer/tests/conftest.py
neuron_explainer/tests/test_activation_reconstituter.py
neuron_explainer/tests/test_against_data.py
neuron_explainer/tests/test_all_dsts.py
neuron_explainer/tests/test_emb_dsts.py
neuron_explainer/tests/test_hooks.py
neuron_explainer/tests/test_interactive_model.py
neuron_explainer/tests/test_model_context_get_weight.py
neuron_explainer/tests/test_offline_autoencoder_dsts.py
neuron_explainer/tests/test_online_autoencoder_dsts.py
neuron_explainer/tests/test_postprocessing.py
neuron_explainer/tests/test_reconstituted_gradients.py
neuron_explainer/tests/test_serialization_of_model_config_from_model_context.py
neuron_explainer/tests/test_trace_through_v.py
neuron_explainer/tests/test_transformer.py
neuron_viewer/public/robots.txt
neuron_viewer/README.md
README.md
setup.py
terminology.md

================================================================
Files
================================================================

================
File: build/lib/neuron_explainer/api_client.py
================
import asyncio
import contextlib
import os
import random
import traceback
from functools import wraps
from typing import Any
import httpx
import orjson
API_KEY = os.getenv("OPENAI_API_KEY")
assert API_KEY, "Please set the OPENAI_API_KEY environment variable"
API_HTTP_HEADERS = {
    "Content-Type": "application/json",
    "Authorization": "Bearer " + API_KEY,
}
BASE_API_URL = "https://api.openai.com/v1"
def async_exponential_backoff(retry_on=lambda err: True):
    """
    Returns a decorator which retries the wrapped function as long as the specified retry_on
    function returns True for the exception, applying exponential backoff with jitter after
    failures, up to a retry limit.
    """
    init_delay_s = 1
    max_delay_s = 10
    backoff_multiplier = 2.0
    jitter = 0.2
    def decorate(f):
        @wraps(f)
        async def f_retry(self, *args, **kwargs):
            max_tries = 200
            delay_s = init_delay_s
            for i in range(max_tries):
                try:
                    return await f(self, *args, **kwargs)
                except Exception as err:
                    if i == max_tries - 1:
                        print(f"Exceeded max tries ({max_tries}) on HTTP request")
                        raise
                    if not retry_on(err):
                        print("Unretryable error on HTTP request")
                        raise
                    jittered_delay = random.uniform(delay_s * (1 - jitter), delay_s * (1 + jitter))
                    await asyncio.sleep(jittered_delay)
                    delay_s = min(delay_s * backoff_multiplier, max_delay_s)
        return f_retry
    return decorate
def is_api_error(err: Exception) -> bool:
    if isinstance(err, httpx.HTTPStatusError):
        response = err.response
        error_data = response.json().get("error", {})
        error_message = error_data.get("message")
        if response.status_code in [400, 404, 415]:
            if error_data.get("type") == "idempotency_error":
                print(f"Retrying after idempotency error: {error_message} ({response.url})")
                return True
            else:
                # Invalid request
                return False
        else:
            print(f"Retrying after API error: {error_message} ({response.url})")
            return True
    elif isinstance(err, httpx.ConnectError):
        print(f"Retrying after connection error... ({err.request.url})")
        return True
    elif isinstance(err, httpx.TimeoutException):
        print(f"Retrying after a timeout error... ({err.request.url})")
        return True
    elif isinstance(err, httpx.ReadError):
        print(f"Retrying after a read error... ({err.request.url})")
        return True
    print(f"Retrying after an unexpected error: {repr(err)}")
    traceback.print_tb(err.__traceback__)
    return True
class ApiClient:
    """
    Performs inference using the OpenAI API. Supports response caching and concurrency limits."
    Cache is useful for rerunning code in jupyter notebooks without repeating identical requests
    """
    def __init__(
        self,
        model_name: str,
        max_concurrent: int | None = None,
        # If set, no more than this number of HTTP requests will be made concurrently.
        cache: bool = False,
    ):
        self.model_name = model_name
        if max_concurrent is not None:
            self.concurrency_check: asyncio.Semaphore | None = asyncio.Semaphore(max_concurrent)
        else:
            self.concurrency_check = None
        if cache:
            self.cache = {}
        else:
            self.cache = None
    @async_exponential_backoff(retry_on=is_api_error)
    async def async_generate(self, timeout: int | None = None, **kwargs: Any) -> dict:
        if self.cache is not None:
            key = orjson.dumps(kwargs)
            if key in self.cache:
                return self.cache[key]
        async with contextlib.AsyncExitStack() as stack:
            if self.concurrency_check is not None:
                await stack.enter_async_context(self.concurrency_check)
            http_client = await stack.enter_async_context(httpx.AsyncClient(timeout=timeout))
            # If the request has a "messages" key, it should be sent to the /chat/completions
            # endpoint. Otherwise, it should be sent to the /completions endpoint.
            url = BASE_API_URL + ("/chat/completions" if "messages" in kwargs else "/completions")
            kwargs["model"] = self.model_name
            response = await http_client.post(url, headers=API_HTTP_HEADERS, json=kwargs)
        # Response json has useful information but the exception doesn't include it, so print it out then reraise
        try:
            response.raise_for_status()
        except Exception as e:
            print(response.json())
            raise e
        if self.cache is not None:
            self.cache[key] = response.json()
        return response.json()
if __name__ == "__main__":
    async def main() -> None:
        client = ApiClient(model_name="gpt-3.5-turbo", max_concurrent=1)
        print(
            await client.async_generate(prompt="Why did the chicken cross the road?", max_tokens=9)
        )
    asyncio.run(main())

================
File: build/lib/neuron_explainer/explanations/attention_head_scoring.py
================
"""Uses API calls to score attention head explanations."""
from __future__ import annotations
import random
from typing import Any
import numpy as np
from sklearn.metrics import roc_auc_score
from neuron_explainer.activations.activations import (
    ActivationRecord,
    ActivationRecordSliceParams,
    load_neuron,
)
from neuron_explainer.activations.attention_utils import (
    convert_flattened_index_to_unflattened_index,
)
from neuron_explainer.api_client import ApiClient
from neuron_explainer.explanations.explainer import (
    ATTENTION_EXPLANATION_PREFIX,
    ContextSize,
    format_attention_head_token_pair_string,
)
from neuron_explainer.explanations.explanations import (
    AttentionSimulation,
    ScoredAttentionSimulation,
)
from neuron_explainer.explanations.few_shot_examples import ATTENTION_HEAD_FEW_SHOT_EXAMPLES
from neuron_explainer.explanations.prompt_builder import (
    ChatMessage,
    PromptBuilder,
    PromptFormat,
    Role,
)
class AttentionHeadOneAtATimeScorer:
    def __init__(
        self,
        model_name: str,
        prompt_format: PromptFormat = PromptFormat.CHAT_MESSAGES,
        # This parameter lets us adjust the length of the prompt when we're generating explanations
        # using older models with shorter context windows. In the future we can use it to experiment
        # with longer context windows.
        context_size: ContextSize = ContextSize.FOUR_K,
        repeat_strongly_attending_pairs: bool = False,
        max_concurrent: int | None = 10,
        cache: bool = False,
    ):
        assert (
            prompt_format == PromptFormat.CHAT_MESSAGES
        ), f"Unhandled prompt format {prompt_format}"
        self.prompt_format = prompt_format
        self.context_size = context_size
        self.client = ApiClient(model_name=model_name, max_concurrent=max_concurrent, cache=cache)
        self.repeat_strongly_attending_pairs = repeat_strongly_attending_pairs
    async def score_explanation(
        self,
        explanation: str,
        activation_records: list[ActivationRecord],
        max_activation: float,
        # The number of high and low activating token pairs to sample for simulation
        num_activations_for_scoring: int = 5,
        # The activation threshold below which a token pair is eligible for sampling
        # as a low activating pair.
        low_activation_threshold: float = 0.1,
    ) -> ScoredAttentionSimulation:
        """Score explanations based on how well they predict attention between
        top attending token pairs and random low attending token pairs."""
        # Use the activation records to generate a set of pairs for scoring.
        # 10 pairs: the five top activating pairs, and five randomly chosen pairs
        # where the activations are below 0.1 * the max value.
        candidates = []
        for i, record in enumerate(activation_records):
            sorted_activation_flat_indices = np.argsort(record.activations)[::-1]
            sorted_vals = [record.activations[idx] for idx in sorted_activation_flat_indices]
            coordinates = [
                convert_flattened_index_to_unflattened_index(flat_index)
                for flat_index in sorted_activation_flat_indices
            ]
            candidates.extend([(i, val, coords) for val, coords in zip(sorted_vals, coordinates)])
        top_activation_coordinates = [
            (candidate[0], candidate[2])
            for candidate in sorted(candidates, key=lambda x: x[1], reverse=True)
        ][:num_activations_for_scoring]
        filtered_low_activation_coordinates = [
            (candidate[0], candidate[2])
            for candidate in candidates
            if candidate[1] < low_activation_threshold * max_activation
        ]
        selected_low_activation_coordinates = random.sample(
            filtered_low_activation_coordinates,
            min(len(filtered_low_activation_coordinates), num_activations_for_scoring),
        )
        attention_simulations = []
        true_labels = [1 for _ in range(len(top_activation_coordinates))] + [
            0 for _ in range(len(selected_low_activation_coordinates))
        ]
        # No need to shuffle because the model only sees one at a time anyway.
        for coords, label in zip(
            top_activation_coordinates + selected_low_activation_coordinates, true_labels
        ):
            activation_record = activation_records[coords[0]]
            # for each pair, generate a prompt where the model is asked to predict if the token pair has a strong
            # or weak activation.
            prompt = self.make_token_pair_prompt(explanation, activation_record.tokens, coords[1])
            assert isinstance(prompt, list)
            assert isinstance(prompt[0], dict)  # Really a ChatMessage
            generate_kwargs: dict[str, Any] = {
                # Using a timeout prevents the scorer from hanging if the API server is overloaded.
                "timeout": 60,
                "n": 1,
                "max_tokens": 1,  # we only want to sample one token.
                "logprobs": True,
                "top_logprobs": 15,
                "messages": prompt,
            }
            response = await self.client.async_generate(**generate_kwargs)
            assert len(response["choices"]) == 1
            # from the response, extract the logit values for "0" (for weak) and "1" (for strong) to obtain
            # a float.
            choice = response["choices"][0]
            # for whatever reason `choice["logprobs"]["top_logprobs"]` is a list of dicts
            logprobs_dicts = choice["logprobs"]["content"][0]["top_logprobs"]
            extracted_probs = {d["token"]: d["logprob"] for d in logprobs_dicts}
            zero_prob = np.exp(extracted_probs["0"]) if "0" in extracted_probs else 0.0
            one_prob = np.exp(extracted_probs["1"]) if "1" in extracted_probs else 0.0
            total_prob = zero_prob + one_prob
            # The score is 0 * normalized probability of "0" + 1 * normalized probability of "1", which
            # reduces to just the normalized probability of "1".
            normalized_one_prob = one_prob / total_prob
            # print(f"zero_prob: {zero_prob/total_prob}, one_prob: {normalized_one_prob}")
            attention_simulations.append(
                AttentionSimulation(
                    tokens=activation_record.tokens,
                    token_pair_coords=coords[1],
                    token_pair_label=label,
                    simulation_prediction=normalized_one_prob,
                )
            )
        assert (
            len(attention_simulations)
            == len(true_labels)
            == len(top_activation_coordinates) + len(selected_low_activation_coordinates)
        )
        # ROC AUC awards a perfect score to explanations that order all of the scores
        # for pairs labeled "1" above the scores for pairs labeled "0" (even if the scores
        # for pairs labeled "0" are well above 0).
        score = roc_auc_score(
            y_true=true_labels, y_score=[sim.simulation_prediction for sim in attention_simulations]
        )
        return ScoredAttentionSimulation(
            attention_simulations=attention_simulations,
            roc_auc_score=score,
        )
    def make_token_pair_prompt(
        self, explanation: str, tokens: list[str], coords: tuple[int, int]
    ) -> str | list[ChatMessage]:
        """
        Create a prompt to send to the API to simulate the model predicting whether a token pair
        has a strong attention write norm according to the given explanation.
        """
        prompt_builder = PromptBuilder()
        prompt_builder.add_message(
            Role.SYSTEM,
            "We're studying attention heads in a neural network. Each head looks at every pair of tokens "
            "in a short token sequence and activates for pairs of tokens that fit what it is looking for. "
            "Attention heads always attend from a token to a token earlier in the sequence (or from a "
            "token to itself). We will display a token sequence and indicate a particular token pair within "
            'that sequence. The "to" token of the pair will be marked with double asterisks (e.g., **token**) '
            'and the "from" token will be marked with double square brackets (e.g., [[token]]). If the token pair '
            "consists of a token paired with itself, it will be marked with both (e.g., [[**token**]]) and "
            "no other token in the sequence will be marked. We present an explanation of what the "
            "attention head is looking for. Output 1 if the head activates for the token pair, and 0 otherwise.",
        )
        num_few_shot = 0
        for few_shot_example in ATTENTION_HEAD_FEW_SHOT_EXAMPLES:
            if not few_shot_example.simulation_examples:
                continue
            for simulation_example in few_shot_example.simulation_examples:
                self._add_per_token_pair_attention_simulation_prompt(
                    prompt_builder=prompt_builder,
                    tokens=few_shot_example.token_pair_examples[
                        simulation_example.token_pair_example_index
                    ].tokens,
                    explanation=few_shot_example.explanation,
                    simulation_coords=simulation_example.token_pair_coordinates,
                    index=num_few_shot,
                    label=simulation_example.label,
                )
                num_few_shot += 1
        self._add_per_token_pair_attention_simulation_prompt(
            prompt_builder=prompt_builder,
            tokens=tokens,
            explanation=explanation,
            simulation_coords=coords,
            index=num_few_shot,
            label=None,
        )
        return prompt_builder.build(self.prompt_format)
    def _add_per_token_pair_attention_simulation_prompt(
        self,
        prompt_builder: PromptBuilder,
        tokens: list[str],
        explanation: str,
        simulation_coords: tuple[int, int],
        index: int,
        label: int | None,  # None means this is the end of the full prompt.
    ) -> None:
        user_message = f"""
Example {index + 1}
Explanation: {ATTENTION_EXPLANATION_PREFIX} {explanation.strip()}
Sequence:\n{format_attention_head_token_pair_string(tokens, simulation_coords)}"""
        if self.repeat_strongly_attending_pairs:
            user_message += (
                f"\nThe same token pair, presented as (to_token, from_token): "
                f"({tokens[simulation_coords[1]]}, {tokens[simulation_coords[0]]})"
            )
        user_message += (
            f"\nPrediction of whether attention head {index + 1} activates on the token pair: "
        )
        prompt_builder.add_message(Role.USER, user_message)
        if label is not None:
            prompt_builder.add_message(Role.ASSISTANT, f"{label}")
if __name__ == "__main__":
    # Example usage
    async def main() -> None:
        scorer = AttentionHeadOneAtATimeScorer("gpt-4o")
        explanation = "attends from tokens to the first token in the sequence"
        attention_head = load_neuron(
            "https://openaipublic.blob.core.windows.net/neuron-explainer/gpt2_small/attn_write_norm/collated_activations_by_token_pair",
            "0",
            "5",
        )
        train_records = attention_head.train_activation_records(
            activation_record_slice_params=ActivationRecordSliceParams(n_examples_per_split=5)
        )
        scored_simulation = await scorer.score_explanation(
            explanation, train_records, max([max(record.activations) for record in train_records])
        )
        print(scored_simulation.roc_auc_score)
    import asyncio
    asyncio.run(main())

================
File: build/lib/neuron_explainer/explanations/calibrated_simulator.py
================
"""
Code for calibrating simulations of neuron behavior. Calibration refers to a process of mapping from
a space of predicted activation values (e.g. [0, 10]) to the real activation distribution for a
neuron.
See http://go/neuron_explanation_methodology for description of calibration step. Necessary for
simulating neurons in the context of ablate-to-simulation, but can be skipped when using correlation
scoring. (Calibration may still improve quality for scoring, at least for non-linear calibration
methods.)
"""
from __future__ import annotations
import asyncio
from abc import abstractmethod
from typing import Sequence
import numpy as np
from sklearn import linear_model
from neuron_explainer.activations.activations import ActivationRecord
from neuron_explainer.explanations.explanations import ActivationScale
from neuron_explainer.explanations.simulator import NeuronSimulator, SequenceSimulation
class CalibratedNeuronSimulator(NeuronSimulator):
    """
    Wrap a NeuronSimulator and calibrate it to map from the predicted activation space to the
    actual neuron activation space.
    """
    def __init__(self, uncalibrated_simulator: NeuronSimulator):
        self.uncalibrated_simulator = uncalibrated_simulator
    @classmethod
    async def create(
        cls,
        uncalibrated_simulator: NeuronSimulator,
        calibration_activation_records: Sequence[ActivationRecord],
    ) -> CalibratedNeuronSimulator:
        """
        Create and calibrate a calibrated simulator (so initialization and calibration can be done
        in one call).
        """
        calibrated_simulator = cls(uncalibrated_simulator)
        await calibrated_simulator.calibrate(calibration_activation_records)
        return calibrated_simulator
    async def calibrate(self, calibration_activation_records: Sequence[ActivationRecord]) -> None:
        """
        Determine parameters to map from the predicted activation space to the real neuron
        activation space, based on a calibration set.
        Use when simulated sequences haven't already been produced on the calibration set.
        """
        simulations = await asyncio.gather(
            *[
                self.uncalibrated_simulator.simulate(activations.tokens)
                for activations in calibration_activation_records
            ]
        )
        self.calibrate_from_simulations(calibration_activation_records, simulations)
    def calibrate_from_simulations(
        self,
        calibration_activation_records: Sequence[ActivationRecord],
        simulations: Sequence[SequenceSimulation],
    ) -> None:
        """
        Determine parameters to map from the predicted activation space to the real neuron
        activation space, based on a calibration set.
        Use when simulated sequences have already been produced on the calibration set.
        """
        flattened_activations = []
        flattened_simulated_activations: list[float] = []
        for activations, simulation in zip(calibration_activation_records, simulations):
            flattened_activations.extend(activations.activations)
            flattened_simulated_activations.extend(simulation.expected_activations)
        self._calibrate_from_flattened_activations(
            np.array(flattened_activations), np.array(flattened_simulated_activations)
        )
    @abstractmethod
    def _calibrate_from_flattened_activations(
        self,
        true_activations: np.ndarray,
        uncalibrated_activations: np.ndarray,
    ) -> None:
        """
        Determine parameters to map from the predicted activation space to the real neuron
        activation space, based on a calibration set.
        Take numpy arrays of all true activations and all uncalibrated activations on the
        calibration set over all sequences.
        """
    @abstractmethod
    def apply_calibration(self, values: Sequence[float]) -> list[float]:
        """Apply the learned calibration to a sequence of values."""
    async def simulate(self, tokens: Sequence[str]) -> SequenceSimulation:
        uncalibrated_seq_simulation = await self.uncalibrated_simulator.simulate(tokens)
        calibrated_activations = self.apply_calibration(
            uncalibrated_seq_simulation.expected_activations
        )
        calibrated_distribution_values = [
            self.apply_calibration(dv) for dv in uncalibrated_seq_simulation.distribution_values
        ]
        return SequenceSimulation(
            activation_scale=ActivationScale.NEURON_ACTIVATIONS,
            tokens=uncalibrated_seq_simulation.tokens,
            expected_activations=calibrated_activations,
            distribution_values=calibrated_distribution_values,
            distribution_probabilities=uncalibrated_seq_simulation.distribution_probabilities,
            uncalibrated_simulation=uncalibrated_seq_simulation,
        )
class UncalibratedNeuronSimulator(CalibratedNeuronSimulator):
    """Pass through the activations without trying to calibrate."""
    def __init__(self, uncalibrated_simulator: NeuronSimulator):
        super().__init__(uncalibrated_simulator)
    async def calibrate(self, calibration_activation_records: Sequence[ActivationRecord]) -> None:
        pass
    def _calibrate_from_flattened_activations(
        self,
        true_activations: np.ndarray,
        uncalibrated_activations: np.ndarray,
    ) -> None:
        pass
    def apply_calibration(self, values: Sequence[float]) -> list[float]:
        return values if isinstance(values, list) else list(values)
class LinearCalibratedNeuronSimulator(CalibratedNeuronSimulator):
    """Find a linear mapping from uncalibrated activations to true activations.
    Should not change ev_correlation_score because it is invariant to linear transformations.
    """
    def __init__(self, uncalibrated_simulator: NeuronSimulator):
        super().__init__(uncalibrated_simulator)
        self._regression: linear_model.LinearRegression | None = None
    def _calibrate_from_flattened_activations(
        self,
        true_activations: np.ndarray,
        uncalibrated_activations: np.ndarray,
    ) -> None:
        self._regression = linear_model.LinearRegression()
        self._regression.fit(uncalibrated_activations.reshape(-1, 1), true_activations)
    def apply_calibration(self, values: Sequence[float]) -> list[float]:
        if self._regression is None:
            raise ValueError("Must call calibrate() before apply_calibration")
        if len(values) == 0:
            return []
        return self._regression.predict(np.reshape(np.array(values), (-1, 1))).tolist()
class PercentileMatchingCalibratedNeuronSimulator(CalibratedNeuronSimulator):
    """
    Map the nth percentile of the uncalibrated activations to the nth percentile of the true
    activations for all n.
    This will match the distribution of true activations on the calibration set, but will be
    overconfident outside of the calibration set.
    """
    def __init__(self, uncalibrated_simulator: NeuronSimulator):
        super().__init__(uncalibrated_simulator)
        self._uncalibrated_activations: np.ndarray | None = None
        self._true_activations: np.ndarray | None = None
    def _calibrate_from_flattened_activations(
        self,
        true_activations: np.ndarray,
        uncalibrated_activations: np.ndarray,
    ) -> None:
        self._uncalibrated_activations = np.sort(uncalibrated_activations)
        self._true_activations = np.sort(true_activations)
    def apply_calibration(self, values: Sequence[float]) -> list[float]:
        if self._true_activations is None or self._uncalibrated_activations is None:
            raise ValueError("Must call calibrate() before apply_calibration")
        if len(values) == 0:
            return []
        return np.interp(
            np.array(values), self._uncalibrated_activations, self._true_activations
        ).tolist()

================
File: build/lib/neuron_explainer/explanations/explainer.py
================
"""Uses API calls to generate explanations of neuron behavior."""
from __future__ import annotations
import logging
import re
from abc import ABC, abstractmethod
from enum import Enum
from typing import Any, Sequence
import numpy as np
from neuron_explainer.activations.activation_records import (
    calculate_max_activation,
    format_activation_records,
    non_zero_activation_proportion,
)
from neuron_explainer.activations.activations import ActivationRecord
from neuron_explainer.activations.attention_utils import (
    convert_flattened_index_to_unflattened_index,
)
from neuron_explainer.api_client import ApiClient
from neuron_explainer.explanations.few_shot_examples import (
    ATTENTION_HEAD_FEW_SHOT_EXAMPLES,
    AttentionTokenPairExample,
    FewShotExampleSet,
)
from neuron_explainer.explanations.prompt_builder import (
    ChatMessage,
    PromptBuilder,
    PromptFormat,
    Role,
)
logger = logging.getLogger(__name__)
EXPLANATION_PREFIX = "this neuron activates for"
ATTENTION_EXPLANATION_PREFIX = "this attention head"
ATTENTION_SEQUENCE_SEPARATOR = "<|sequence_separator|>"
def _split_numbered_list(text: str) -> list[str]:
    """Split a numbered list into a list of strings."""
    lines = re.split(r"\n\d+\.", text)
    # Strip the leading whitespace from each line.
    return [line.lstrip() for line in lines]
class ContextSize(int, Enum):
    TWO_K = 2049
    FOUR_K = 4097
    @classmethod
    def from_int(cls, i: int) -> ContextSize:
        for context_size in cls:
            if context_size.value == i:
                return context_size
        raise ValueError(f"{i} is not a valid ContextSize")
class NeuronExplainer(ABC):
    """
    Abstract base class for Explainer classes that generate explanations from subclass-specific
    input data.
    """
    def __init__(
        self,
        model_name: str,
        prompt_format: PromptFormat = PromptFormat.CHAT_MESSAGES,
        # This parameter lets us adjust the length of the prompt when we're generating explanations
        # using older models with shorter context windows. In the future we can use it to experiment
        # with longer context windows.
        context_size: ContextSize = ContextSize.FOUR_K,
        max_concurrent: int | None = 10,
        cache: bool = False,
    ):
        self.prompt_format = prompt_format
        self.context_size = context_size
        self.client = ApiClient(model_name=model_name, max_concurrent=max_concurrent, cache=cache)
    async def generate_explanations(
        self,
        *,
        num_samples: int = 1,
        max_tokens: int = 60,
        temperature: float = 1.0,
        top_p: float = 1.0,
        **prompt_kwargs: Any,
    ) -> list[Any]:
        """Generate explanations based on subclass-specific input data."""
        prompt = self.make_explanation_prompt(max_tokens_for_completion=max_tokens, **prompt_kwargs)
        generate_kwargs: dict[str, Any] = {
            # Using a timeout prevents the explainer from hanging if the API server is overloaded.
            "timeout": 60,
            "n": num_samples,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "top_p": top_p,
        }
        if self.prompt_format == PromptFormat.CHAT_MESSAGES:
            assert isinstance(prompt, list)
            assert isinstance(prompt[0], dict)  # Really a ChatMessage
            generate_kwargs["messages"] = prompt
        else:
            assert isinstance(prompt, str)
            generate_kwargs["prompt"] = prompt
        response = await self.client.async_generate(**generate_kwargs)
        logger.debug("response in generate_explanations is %s", response)
        if self.prompt_format == PromptFormat.CHAT_MESSAGES:
            explanations = [x["message"]["content"] for x in response["choices"]]
        elif self.prompt_format in [PromptFormat.NONE, PromptFormat.INSTRUCTION_FOLLOWING]:
            explanations = [x["text"] for x in response["choices"]]
        else:
            raise ValueError(f"Unhandled prompt format {self.prompt_format}")
        return self.postprocess_explanations(explanations, prompt_kwargs)
    @abstractmethod
    def make_explanation_prompt(self, **kwargs: Any) -> str | list[ChatMessage]:
        """
        Create a prompt to send to the API to generate one or more explanations.
        A prompt can be a simple string, or a list of ChatMessages, depending on the PromptFormat
        used by this instance.
        """
        ...
    def postprocess_explanations(
        self, completions: list[str], prompt_kwargs: dict[str, Any]
    ) -> list[Any]:
        """Postprocess the completions returned by the API into a list of explanations."""
        return completions  # no-op by default
    def _prompt_is_too_long(
        self, prompt_builder: PromptBuilder, max_tokens_for_completion: int
    ) -> bool:
        # We'll get a context size error if the prompt itself plus the maximum number of tokens for
        # the completion is longer than the context size.
        prompt_length = prompt_builder.prompt_length_in_tokens(self.prompt_format)
        if prompt_length + max_tokens_for_completion > self.context_size.value:
            print(
                f"Prompt is too long: {prompt_length} + {max_tokens_for_completion} > "
                f"{self.context_size.value}"
            )
            return True
        return False
class TokenActivationPairExplainer(NeuronExplainer):
    """
    Generate explanations of neuron behavior using a prompt with lists of token/activation pairs.
    """
    def __init__(
        self,
        model_name: str,
        prompt_format: PromptFormat = PromptFormat.CHAT_MESSAGES,
        # This parameter lets us adjust the length of the prompt when we're generating explanations
        # using older models with shorter context windows. In the future we can use it to experiment
        # with 8k+ context windows.
        context_size: ContextSize = ContextSize.FOUR_K,
        few_shot_example_set: FewShotExampleSet = FewShotExampleSet.ORIGINAL,
        repeat_non_zero_activations: bool = False,
        max_concurrent: int | None = 10,
        cache: bool = False,
    ):
        super().__init__(
            model_name=model_name,
            prompt_format=prompt_format,
            max_concurrent=max_concurrent,
            cache=cache,
        )
        self.context_size = context_size
        self.few_shot_example_set = few_shot_example_set
        self.repeat_non_zero_activations = repeat_non_zero_activations
    def make_explanation_prompt(self, **kwargs: Any) -> str | list[ChatMessage]:
        original_kwargs = kwargs.copy()
        all_activation_records: Sequence[ActivationRecord] = kwargs.pop("all_activations")
        max_activation: float = kwargs.pop("max_activation")
        kwargs.setdefault("numbered_list_of_n_explanations", None)
        numbered_list_of_n_explanations: int | None = kwargs.pop("numbered_list_of_n_explanations")
        if numbered_list_of_n_explanations is not None:
            assert numbered_list_of_n_explanations > 0, numbered_list_of_n_explanations
        # This parameter lets us dynamically shrink the prompt if our initial attempt to create it
        # results in something that's too long. It's only implemented for the 4k context size.
        kwargs.setdefault("omit_n_activation_records", 0)
        omit_n_activation_records: int = kwargs.pop("omit_n_activation_records")
        max_tokens_for_completion: int = kwargs.pop("max_tokens_for_completion")
        assert not kwargs, f"Unexpected kwargs: {kwargs}"
        prompt_builder = PromptBuilder()
        prompt_builder.add_message(
            Role.SYSTEM,
            "We're studying neurons in a neural network. Each neuron looks for some particular "
            "thing in a short document. Look at the parts of the document the neuron activates for "
            "and summarize in a single sentence what the neuron is looking for. Don't list "
            "examples of words.\n\nThe activation format is token<tab>activation. Activation "
            "values range from 0 to 10. A neuron finding what it's looking for is represented by a "
            "non-zero activation value. The higher the activation value, the stronger the match.",
        )
        few_shot_examples = self.few_shot_example_set.get_examples()
        num_omitted_activation_records = 0
        for i, few_shot_example in enumerate(few_shot_examples):
            few_shot_activation_records = few_shot_example.activation_records
            if self.context_size == ContextSize.TWO_K:
                # If we're using a 2k context window, we only have room for one activation record
                # per few-shot example. (Two few-shot examples with one activation record each seems
                # to work better than one few-shot example with two activation records, in local
                # testing.)
                few_shot_activation_records = few_shot_activation_records[:1]
            elif (
                self.context_size == ContextSize.FOUR_K
                and num_omitted_activation_records < omit_n_activation_records
            ):
                # Drop the last activation record for this few-shot example to save tokens, assuming
                # there are at least two activation records.
                if len(few_shot_activation_records) > 1:
                    print(f"Warning: omitting activation record from few-shot example {i}")
                    few_shot_activation_records = few_shot_activation_records[:-1]
                    num_omitted_activation_records += 1
            self._add_per_neuron_explanation_prompt(
                prompt_builder,
                few_shot_activation_records,
                i,
                calculate_max_activation(few_shot_example.activation_records),
                numbered_list_of_n_explanations=numbered_list_of_n_explanations,
                explanation=few_shot_example.explanation,
            )
        self._add_per_neuron_explanation_prompt(
            prompt_builder,
            # If we're using a 2k context window, we only have room for two of the activation
            # records.
            (
                all_activation_records[:2]
                if self.context_size == ContextSize.TWO_K
                else all_activation_records
            ),
            len(few_shot_examples),
            max_activation,
            numbered_list_of_n_explanations=numbered_list_of_n_explanations,
            explanation=None,
        )
        # If the prompt is too long *and* we omitted the specified number of activation records, try
        # again, omitting one more. (If we didn't make the specified number of omissions, we're out
        # of opportunities to omit records, so we just return the prompt as-is.)
        if (
            self._prompt_is_too_long(prompt_builder, max_tokens_for_completion)
            and num_omitted_activation_records == omit_n_activation_records
        ):
            original_kwargs["omit_n_activation_records"] = omit_n_activation_records + 1
            return self.make_explanation_prompt(**original_kwargs)
        return prompt_builder.build(self.prompt_format)
    def _add_per_neuron_explanation_prompt(
        self,
        prompt_builder: PromptBuilder,
        activation_records: Sequence[ActivationRecord],
        index: int,
        max_activation: float,
        # When set, this indicates that the prompt should solicit a numbered list of the given
        # number of explanations, rather than a single explanation.
        numbered_list_of_n_explanations: int | None,
        explanation: str | None,  # None means this is the end of the full prompt.
    ) -> None:
        max_activation = calculate_max_activation(activation_records)
        user_message = f"""
Neuron {index + 1}
Activations:{format_activation_records(activation_records, max_activation, omit_zeros=False)}"""
        # We repeat the non-zero activations only if it was requested and if the proportion of
        # non-zero activations isn't too high.
        if (
            self.repeat_non_zero_activations
            and non_zero_activation_proportion(activation_records, max_activation) < 0.2
        ):
            user_message += (
                f"\nSame activations, but with all zeros filtered out:"
                f"{format_activation_records(activation_records, max_activation, omit_zeros=True)}"
            )
        if numbered_list_of_n_explanations is None:
            user_message += f"\nExplanation of neuron {index + 1} behavior:"
            assistant_message = ""
            # For the IF format, we want <|endofprompt|> to come before the explanation prefix.
            if self.prompt_format == PromptFormat.INSTRUCTION_FOLLOWING:
                assistant_message += f" {EXPLANATION_PREFIX}"
            else:
                user_message += f" {EXPLANATION_PREFIX}"
            prompt_builder.add_message(Role.USER, user_message)
            if explanation is not None:
                assistant_message += f" {explanation}."
            if assistant_message:
                prompt_builder.add_message(Role.ASSISTANT, assistant_message)
        else:
            if explanation is None:
                # For the final neuron, we solicit a numbered list of explanations.
                prompt_builder.add_message(
                    Role.USER,
                    f"""\nHere are {numbered_list_of_n_explanations} possible explanations for neuron {index + 1} behavior, each beginning with "{EXPLANATION_PREFIX}":\n1. {EXPLANATION_PREFIX}""",
                )
            else:
                # For the few-shot examples, we only present one explanation, but we present it as a
                # numbered list.
                prompt_builder.add_message(
                    Role.USER,
                    f"""\nHere is 1 possible explanation for neuron {index + 1} behavior, beginning with "{EXPLANATION_PREFIX}":\n1. {EXPLANATION_PREFIX}""",
                )
                prompt_builder.add_message(Role.ASSISTANT, f" {explanation}.")
    def postprocess_explanations(
        self, completions: list[str], prompt_kwargs: dict[str, Any]
    ) -> list[Any]:
        """Postprocess the explanations returned by the API"""
        numbered_list_of_n_explanations = prompt_kwargs.get("numbered_list_of_n_explanations")
        if numbered_list_of_n_explanations is None:
            return completions
        else:
            all_explanations = []
            for completion in completions:
                for explanation in _split_numbered_list(completion):
                    if explanation.startswith(EXPLANATION_PREFIX):
                        explanation = explanation[len(EXPLANATION_PREFIX) :]
                    all_explanations.append(explanation.strip())
            return all_explanations
def format_attention_head_token_pairs(
    token_pair_examples: list[AttentionTokenPairExample], omit_zeros: bool = False
) -> str:
    if omit_zeros:
        return ", ".join(
            [
                ", ".join(
                    [
                        f"({example.tokens[coords[1]]}, {example.tokens[coords[0]]})"
                        for coords in example.token_pair_coordinates
                    ]
                )
                for example in token_pair_examples
            ]
        )
    else:
        return f"\n{ATTENTION_SEQUENCE_SEPARATOR}\n".join(
            [
                f"\n{ATTENTION_SEQUENCE_SEPARATOR}\n".join(
                    [
                        f"{format_attention_head_token_pair_string(example.tokens, coords)}"
                        for coords in example.token_pair_coordinates
                    ]
                )
                for example in token_pair_examples
            ]
        )
def format_attention_head_token_pair_string(
    token_list: list[str], pair_coordinates: tuple[int, int]
) -> str:
    def format_activated_token(i: int, token: str) -> str:
        if i == pair_coordinates[0] and i == pair_coordinates[1]:
            return f"[[**{token}**]]"  # from and to
        if i == pair_coordinates[0]:
            return f"[[{token}]]"  # from
        if i == pair_coordinates[1]:
            return f"**{token}**"  # to
        return token
    return "".join([format_activated_token(i, token) for i, token in enumerate(token_list)])
def get_top_attention_coordinates(
    activation_records: list[ActivationRecord], top_k: int = 5
) -> list[tuple[int, float, tuple[int, int]]]:
    candidates = []
    for i, record in enumerate(activation_records):
        top_activation_flat_indices = np.argsort(record.activations)[::-1][:top_k]
        top_vals: list[float] = [record.activations[idx] for idx in top_activation_flat_indices]
        top_coordinates = [
            convert_flattened_index_to_unflattened_index(flat_index)
            for flat_index in top_activation_flat_indices
        ]
        candidates.extend(
            [(i, top_val, coords) for top_val, coords in zip(top_vals, top_coordinates)]
        )
    return sorted(candidates, key=lambda x: x[1], reverse=True)[:top_k]
class AttentionHeadExplainer(NeuronExplainer):
    """
    Generate explanations of attention head behavior using a prompt with lists of
    strongly attending to/from token pairs.
    Takes in NeuronRecord's corresponding to a single attention head. Extracts strongly
    activating to/from token pairs.
    """
    def __init__(
        self,
        model_name: str,
        prompt_format: PromptFormat = PromptFormat.CHAT_MESSAGES,
        # This parameter lets us adjust the length of the prompt when we're generating explanations
        # using older models with shorter context windows. In the future we can use it to experiment
        # with 8k+ context windows.
        context_size: ContextSize = ContextSize.FOUR_K,
        repeat_strongly_attending_pairs: bool = False,
        max_concurrent: int | None = 10,
        cache: bool = False,
    ):
        super().__init__(
            model_name=model_name,
            prompt_format=prompt_format,
            max_concurrent=max_concurrent,
            cache=cache,
        )
        assert (
            context_size != ContextSize.TWO_K
        ), "2k context size not supported for attention explanation"
        self.context_size = context_size
        self.repeat_strongly_attending_pairs = repeat_strongly_attending_pairs
    def make_explanation_prompt(self, **kwargs: Any) -> str | list[ChatMessage]:
        original_kwargs = kwargs.copy()
        all_activation_records: list[ActivationRecord] = kwargs.pop("all_activations")
        # This parameter lets us dynamically shrink the prompt if our initial attempt to create it
        # results in something that's too long.
        kwargs.setdefault("omit_n_token_pair_examples", 0)
        omit_n_token_pair_examples: int = kwargs.pop("omit_n_token_pair_examples")
        max_tokens_for_completion: int = kwargs.pop("max_tokens_for_completion")
        kwargs.setdefault("num_top_pairs_to_display", 0)
        num_top_pairs_to_display: int = kwargs.pop("num_top_pairs_to_display")
        assert not kwargs, f"Unexpected kwargs: {kwargs}"
        prompt_builder = PromptBuilder()
        prompt_builder.add_message(
            Role.SYSTEM,
            "We're studying attention heads in a neural network. Each head looks at every pair of tokens "
            "in a short token sequence and activates for pairs of tokens that fit what it is looking for. "
            "Attention heads always attend from a token to a token earlier in the sequence (or from a "
            'token to itself). We will display multiple instances of sequences with the "to" token '
            'surrounded by double asterisks (e.g., **token**) and the "from" token surrounded by double '
            "square brackets (e.g., [[token]]). If a token attends from itself to itself, it will be "
            "surrounded by both (e.g., [[**token**]]). Look at the pairs of tokens the head activates for "
            "and summarize in a single sentence what pattern the head is looking for. We do not display "
            "every activating pair of tokens in a sequence; you must generalize from limited examples. "
            "Remember, the head always attends to tokens earlier in the sentence (marked with ** **) from "
            "tokens later in the sentence (marked with [[ ]]), except when the head attends from a token to "
            'itself (marked with [[** **]]). The explanation takes the form: "This attention head attends '
            "to {pattern of tokens marked with ** **, which appear earlier} from {pattern of tokens marked with "
            '[[ ]], which appear later}." The explanation does not include any of the markers (** **, [[ ]]), '
            f"as these are just for your reference. Sequences are separated by `{ATTENTION_SEQUENCE_SEPARATOR}`.",
        )
        num_omitted_token_pair_examples = 0
        for i, few_shot_example in enumerate(ATTENTION_HEAD_FEW_SHOT_EXAMPLES):
            few_shot_token_pair_examples = few_shot_example.token_pair_examples
            if num_omitted_token_pair_examples < omit_n_token_pair_examples:
                # Drop the last activation record for this few-shot example to save tokens, assuming
                # there are at least two activation records.
                if len(few_shot_token_pair_examples) > 1:
                    print(f"Warning: omitting activation record from few-shot example {i}")
                    few_shot_token_pair_examples = few_shot_token_pair_examples[:-1]
                    num_omitted_token_pair_examples += 1
            few_shot_explanation: str = few_shot_example.explanation
            self._add_per_head_explanation_prompt(
                prompt_builder,
                few_shot_token_pair_examples,
                i,
                explanation=few_shot_explanation,
            )
        # each element is (record_index, attention value, (from_token_index, to_token_index))
        coords = get_top_attention_coordinates(
            all_activation_records, top_k=num_top_pairs_to_display
        )
        prompt_examples = {}
        for record_index, _, (from_token_index, to_token_index) in coords:
            if record_index not in prompt_examples:
                prompt_examples[record_index] = AttentionTokenPairExample(
                    tokens=all_activation_records[record_index].tokens,
                    token_pair_coordinates=[(from_token_index, to_token_index)],
                )
            else:
                prompt_examples[record_index].token_pair_coordinates.append(
                    (from_token_index, to_token_index)
                )
        current_head_token_pair_examples = list(prompt_examples.values())
        self._add_per_head_explanation_prompt(
            prompt_builder,
            current_head_token_pair_examples,
            len(ATTENTION_HEAD_FEW_SHOT_EXAMPLES),
            explanation=None,
        )
        # If the prompt is too long *and* we omitted the specified number of activation records, try
        # again, omitting one more. (If we didn't make the specified number of omissions, we're out
        # of opportunities to omit records, so we just return the prompt as-is.)
        if (
            self._prompt_is_too_long(prompt_builder, max_tokens_for_completion)
            and num_omitted_token_pair_examples == omit_n_token_pair_examples
        ):
            original_kwargs["omit_n_token_pair_examples"] = omit_n_token_pair_examples + 1
            return self.make_explanation_prompt(**original_kwargs)
        return prompt_builder.build(self.prompt_format)
    def _add_per_head_explanation_prompt(
        self,
        prompt_builder: PromptBuilder,
        token_pair_examples: list[
            AttentionTokenPairExample
        ],  # each dict has keys "tokens" and "token_pair_coordinates"
        index: int,
        explanation: str | None,  # None means this is the end of the full prompt.
    ) -> None:
        user_message = f"""
Attention head {index + 1}
Activations:\n{format_attention_head_token_pairs(token_pair_examples, omit_zeros=False)}"""
        if self.repeat_strongly_attending_pairs:
            user_message += (
                f"\nThe same list of strongly activating token pairs, presented as (to_token, from_token):"
                f"{format_attention_head_token_pairs(token_pair_examples, omit_zeros=True)}"
            )
        user_message += f"\nExplanation of attention head {index + 1} behavior:"
        assistant_message = ""
        # For the IF format, we want <|endofprompt|> to come before the explanation prefix.
        if self.prompt_format == PromptFormat.INSTRUCTION_FOLLOWING:
            assistant_message += f" {ATTENTION_EXPLANATION_PREFIX}"
        else:
            user_message += f" {ATTENTION_EXPLANATION_PREFIX}"
        prompt_builder.add_message(Role.USER, user_message)
        if explanation is not None:
            assistant_message += f" {explanation}."
        if assistant_message:
            prompt_builder.add_message(Role.ASSISTANT, assistant_message)

================
File: build/lib/neuron_explainer/explanations/explanations.py
================
# Dataclasses and enums for storing neuron explanations, their scores, and related data. Also,
# related helper functions.
from __future__ import annotations
import json
import math
import os.path as osp
from dataclasses import dataclass
from enum import Enum
from typing import Any
from neuron_explainer.activations.activations import NeuronId
from neuron_explainer.fast_dataclasses import FastDataclass, loads, register_dataclass
from neuron_explainer.file_utils import CustomFileHandler, file_exists, read_single_async
class ActivationScale(str, Enum):
    """Which "units" are stored in the expected_activations/distribution_values fields of a
    SequenceSimulation.
    This enum identifies whether the values represent real activations of the neuron or something
    else. Different scales are not necessarily related by a linear transformation.
    """
    NEURON_ACTIVATIONS = "neuron_activations"
    """Values represent real activations of the neuron."""
    SIMULATED_NORMALIZED_ACTIVATIONS = "simulated_normalized_activations"
    """
    Values represent simulated activations of the neuron, normalized to the range [0, 10]. This
    scale is arbitrary and should not be interpreted as a neuron activation.
    """
    HUMAN_PREDICTED_ACTIVATIONS = "human_predicted_activations"
    """
    Values represent human predictions of the neuron's activation, normalized to the range [0, 2]:
        0=not active
        1=weakly/possibly active
        2=strongly/definitely active
    Not used at present.
    """
@register_dataclass
@dataclass
class SequenceSimulation(FastDataclass):
    """The result of a simulation of neuron activations on one text sequence."""
    tokens: list[str]
    """The sequence of tokens that was simulated."""
    expected_activations: list[float]
    """Expected value of the possibly-normalized activation for each token in the sequence."""
    activation_scale: ActivationScale
    """What scale is used for values in the expected_activations field."""
    distribution_values: list[list[float]]
    """
    For each token in the sequence, a list of values from the discrete distribution of activations
    produced from simulation. Tokens will be included here if and only if they are in the top K=15
    tokens predicted by the simulator, and excluded otherwise.
    May be transformed to another unit by calibration. When we simulate a neuron, we produce a
    discrete distribution with values in the arbitrary discretized space of the neuron, e.g. 10%
    chance of 0, 70% chance of 1, 20% chance of 2. Which we store as distribution_values =
    [0, 1, 2], distribution_probabilities = [0.1, 0.7, 0.2]. When we tranform the distribution to
    the real activation units, we can correspondingly tranform the values of this distribution
    to get a distribution in the units of the neuron. e.g. if the mapping from the discretized space
    to the real activation unit of the neuron is f(x) = x/2, then the distribution becomes 10%
    chance of 0, 70% chance of 0.5, 20% chance of 1. Which we store as distribution_values =
    [0, 0.5, 1], distribution_probabilities = [0.1, 0.7, 0.2].
    """
    distribution_probabilities: list[list[float]]
    """
    For each token in the sequence, the probability of the corresponding value in
    distribution_values.
    """
    uncalibrated_simulation: "SequenceSimulation" | None = None
    """The result of the simulation before calibration."""
SequenceSimulation.field_renamed("unit", "activation_scale")
SequenceSimulation.field_deleted("response")
@register_dataclass
@dataclass
class ScoredSequenceSimulation(FastDataclass):
    """
    SequenceSimulation result with a score (for that sequence only) and ground truth activations.
    """
    sequence_simulation: SequenceSimulation
    """The result of a simulation of neuron activations."""
    true_activations: list[float]
    """Ground truth activations on the sequence (not normalized)"""
    ev_correlation_score: float
    """
    Correlation coefficient between the expected values of the normalized activations from the
    simulation and the unnormalized true activations of the neuron on the text sequence.
    """
    rsquared_score: float | None = None
    """R^2 of the simulated activations."""
    absolute_dev_explained_score: float | None = None
    """
    Score based on absolute difference between real and simulated activations.
    absolute_dev_explained_score = 1 - mean(abs(real-predicted))/ mean(abs(real))
    """
    def __eq__(self, other: Any) -> bool:
        if not isinstance(other, ScoredSequenceSimulation):
            return False
        if len(self.__dict__.keys()) != len(other.__dict__.keys()):
            return False
        # Since NaN != NaN in Python, we need to make an exception for this case when checking for equality
        # of two ScoredSequenceSimulation objects.
        for field_name in self.__dict__.keys():
            if field_name not in other.__dict__:
                return False
            self_val, other_val = self.__dict__[field_name], other.__dict__[field_name]
            if self_val != other_val:
                if not (
                    isinstance(self_val, float)
                    and math.isnan(self_val)
                    and isinstance(other_val, float)
                    and math.isnan(other_val)
                ):
                    return False
        return True
ScoredSequenceSimulation.field_renamed("simulation", "sequence_simulation")
@register_dataclass
@dataclass
class ScoredSimulation(FastDataclass):
    """Result of scoring a neuron simulation on multiple sequences."""
    scored_sequence_simulations: list[ScoredSequenceSimulation]
    """ScoredSequenceSimulation for each sequence"""
    ev_correlation_score: float | None = None
    """
    Correlation coefficient between the expected values of the normalized activations from the
    simulation and the unnormalized true activations on a dataset created from all score_results.
    (Note that this is not equivalent to averaging across sequences.)
    """
    rsquared_score: float | None = None
    """R^2 of the simulated activations."""
    absolute_dev_explained_score: float | None = None
    """
    Score based on absolute difference between real and simulated activations.
    absolute_dev_explained_score = 1 - mean(abs(real-predicted))/ mean(abs(real)).
    """
    def get_preferred_score(self) -> float | None:
        """
        This method may return None in cases where the score is undefined, for example if the
        normalized activations were all zero, yielding a correlation coefficient of NaN.
        """
        return self.ev_correlation_score
@register_dataclass
@dataclass
class ScoredExplanation(FastDataclass):
    """Simulator parameters and the results of scoring it on multiple sequences"""
    explanation: str
    scored_simulation: ScoredSimulation
    """Result of scoring the neuron simulator on multiple sequences."""
    def get_preferred_score(self) -> float | None:
        """
        This method may return None in cases where the score is undefined, for example if the
        normalized activations were all zero, yielding a correlation coefficient of NaN.
        """
        return self.scored_simulation.get_preferred_score()
ScoredExplanation.was_previously_named("ScoredExplanationOrBaseline")
ScoredExplanation.field_renamed("explanation_or_baseline", "explanation")
@register_dataclass
@dataclass
class NeuronSimulationResults(FastDataclass):
    """Simulation results and scores for a neuron."""
    neuron_id: NeuronId
    scored_explanations: list[ScoredExplanation]
NeuronSimulationResults.field_renamed("scored_explanation_or_baseline_list", "scored_explanations")
@register_dataclass
@dataclass
class AttentionSimulation(FastDataclass):
    tokens: list[str]
    token_pair_coords: tuple[int, int]
    """The coordinates of the token pair that we're simulating attention for."""
    token_pair_label: int
    """Either 0 or 1 for negative or positive label, respectively."""
    simulation_prediction: float
    """The predicted label for the token pair from the attention simulator."""
@register_dataclass
@dataclass
class ScoredAttentionSimulation(FastDataclass):
    """Result of scoring an attention head simulation on multiple sequences."""
    attention_simulations: list[AttentionSimulation]
    """ScoredSequenceSimulation for each sequence"""
    roc_auc_score: float | None = None
    """
    Area under the ROC curve for the attention predictions. Each AttentionSimulation is
    essentially a single binary classification.
    """
    def get_preferred_score(self) -> float | None:
        return self.roc_auc_score
@register_dataclass
@dataclass
class ScoredAttentionExplanation(FastDataclass):
    """Simulator parameters and the results of scoring it on multiple sequences"""
    explanation: str
    scored_attention_simulation: ScoredAttentionSimulation
    """Result of scoring the neuron simulator on multiple sequences."""
    def get_preferred_score(self) -> float | None:
        """
        This method may return None in cases where the score is undefined, for example if the
        normalized activations were all zero, yielding a correlation coefficient of NaN.
        """
        return self.scored_attention_simulation.get_preferred_score()
@register_dataclass
@dataclass
class AttentionSimulationResults(FastDataclass):
    """Simulation results and scores for an attention head."""
    # Typing this as NeuronId is not ideal but I'm not sure if we want to rename the type to something more general.
    attention_head_id: NeuronId
    scored_explanations: list[ScoredAttentionExplanation]
AttentionSimulationResults.field_renamed("attention_id", "attention_head_id")
def load_neuron_explanations(
    explanations_path: str, layer_index: str | int, neuron_index: str | int
) -> NeuronSimulationResults | None:
    """Load scored explanations for the specified neuron."""
    file = osp.join(explanations_path, str(layer_index), f"{neuron_index}.jsonl")
    if not file_exists(file):
        return None
    with CustomFileHandler(file) as f:
        for line in f:
            return loads(line)
    return None
async def load_neuron_explanations_async(
    explanations_path: str, layer_index: str | int, neuron_index: str | int
) -> NeuronSimulationResults | None:
    """Load scored explanations for the specified neuron, asynchronously."""
    return await read_explanation_file(
        osp.join(explanations_path, str(layer_index), f"{neuron_index}.jsonl")
    )
async def read_file(filename: str) -> str | None:
    """Read the contents of the given file as a string, asynchronously. File can be a
    local file or a remote file."""
    try:
        raw_contents = await read_single_async(filename)
    except FileNotFoundError:
        return None
    lines = []
    for line in raw_contents.decode("utf-8").split("\n"):
        if len(line) > 0:
            lines.append(line)
    assert len(lines) == 1, filename
    return lines[0]
async def read_explanation_file(explanation_filename: str) -> NeuronSimulationResults | None:
    """Load scored explanations from the given filename, asynchronously."""
    line = await read_file(explanation_filename)
    return loads(line) if line is not None else None
async def read_json_file(filename: str) -> dict | None:
    """Read the contents of the given file as a JSON object, asynchronously."""
    line = await read_file(filename)
    return json.loads(line) if line is not None else None

================
File: build/lib/neuron_explainer/explanations/few_shot_examples.py
================
# Few-shot examples for generating and simulating neuron explanations.
from __future__ import annotations
from dataclasses import dataclass
from enum import Enum
from neuron_explainer.activations.activations import ActivationRecord
from neuron_explainer.fast_dataclasses import FastDataclass
@dataclass
class Example(FastDataclass):
    activation_records: list[ActivationRecord]
    explanation: str
    first_revealed_activation_indices: list[int]
    """
    For each activation record, the index of the first token for which the activation value in the
    prompt should be an actual number rather than "unknown".
    Examples all start with the activations rendered as "unknown", then transition to revealing
    specific normalized activation values. The goal is to lead the model to predict that activation
    sequences will eventually transition to predicting specific activation values instead of just
    "unknown". This lets us cheat and get predictions of activation values for every token in a
    single round of inference by having the activations in the sequence we're predicting always be
    "unknown" in the prompt: the model will always think that maybe the next token will be a real
    activation.
    """
    token_index_to_score: int | None = None
    """
    If the prompt is used as an example for one-token-at-a-time scoring, this is the index of the
    token to score.
    """
class FewShotExampleSet(Enum):
    """Determines which few-shot examples to use when sampling explanations."""
    ORIGINAL = "original"
    COLANGV2 = "colangv2"
    TEST = "test"
    @classmethod
    def from_string(cls, string: str) -> FewShotExampleSet:
        for example_set in FewShotExampleSet:
            if example_set.value == string:
                return example_set
        raise ValueError(f"Unrecognized example set: {string}")
    def get_examples(self) -> list[Example]:
        """Returns regular examples for use in a few-shot prompt."""
        if self is FewShotExampleSet.ORIGINAL:
            return ORIGINAL_EXAMPLES
        elif self is FewShotExampleSet.COLANGV2:
            return COLANGV2_EXAMPLES
        elif self is FewShotExampleSet.TEST:
            return TEST_EXAMPLES
        else:
            raise ValueError(f"Unhandled example set: {self}")
    def get_single_token_prediction_example(self) -> Example:
        """
        Returns an example suitable for use in a subprompt for predicting a single token's
        normalized activation, for use with the "one token at a time" scoring approach.
        """
        if self is FewShotExampleSet.COLANGV2:
            return COLANGV2_SINGLE_TOKEN_EXAMPLE
        elif self is FewShotExampleSet.TEST:
            return TEST_SINGLE_TOKEN_EXAMPLE
        else:
            raise ValueError(f"Unhandled example set: {self}")
TEST_EXAMPLES = [
    Example(
        activation_records=[
            ActivationRecord(
                tokens=["a", "b", "c"],
                activations=[1.0, 0.0, 0.0],
            ),
            ActivationRecord(
                tokens=["d", "e", "f"],
                activations=[0.0, 1.0, 0.0],
            ),
        ],
        explanation="vowels",
        first_revealed_activation_indices=[0, 1],
    ),
]
TEST_SINGLE_TOKEN_EXAMPLE = Example(
    activation_records=[
        ActivationRecord(
            activations=[0.0, 0.0, 1.0],
            tokens=["g", "h", "i"],
        ),
    ],
    first_revealed_activation_indices=[],
    token_index_to_score=2,
    explanation="test explanation",
)
ORIGINAL_EXAMPLES = [
    Example(
        activation_records=[
            ActivationRecord(
                tokens=[
                    "t",
                    "urt",
                    "ur",
                    "ro",
                    " is",
                    " fab",
                    "ulously",
                    " funny",
                    " and",
                    " over",
                    " the",
                    " top",
                    " as",
                    " a",
                    " '",
                    "very",
                    " sneaky",
                    "'",
                    " but",
                    "ler",
                    " who",
                    " excel",
                    "s",
                    " in",
                    " the",
                    " art",
                    " of",
                    " impossible",
                    " disappearing",
                    "/",
                    "re",
                    "app",
                    "earing",
                    " acts",
                ],
                activations=[
                    -0.71,
                    -1.85,
                    -2.39,
                    -2.58,
                    -1.34,
                    -1.92,
                    -1.69,
                    -0.84,
                    -1.25,
                    -1.75,
                    -1.42,
                    -1.47,
                    -1.51,
                    -0.8,
                    -1.89,
                    -1.56,
                    -1.63,
                    0.44,
                    -1.87,
                    -2.55,
                    -2.09,
                    -1.76,
                    -1.33,
                    -0.88,
                    -1.63,
                    -2.39,
                    -2.63,
                    -0.99,
                    2.83,
                    -1.11,
                    -1.19,
                    -1.33,
                    4.24,
                    -1.51,
                ],
            ),
            ActivationRecord(
                tokens=[
                    "esc",
                    "aping",
                    " the",
                    " studio",
                    " ,",
                    " pic",
                    "col",
                    "i",
                    " is",
                    " warm",
                    "ly",
                    " affecting",
                    " and",
                    " so",
                    " is",
                    " this",
                    " ad",
                    "roit",
                    "ly",
                    " minimalist",
                    " movie",
                    " .",
                ],
                activations=[
                    -0.69,
                    4.12,
                    1.83,
                    -2.28,
                    -0.28,
                    -0.79,
                    -2.2,
                    -2.03,
                    -1.77,
                    -1.71,
                    -2.44,
                    1.6,
                    -1,
                    -0.38,
                    -1.93,
                    -2.09,
                    -1.63,
                    -1.94,
                    -1.82,
                    -1.64,
                    -1.32,
                    -1.92,
                ],
            ),
        ],
        first_revealed_activation_indices=[10, 3],
        explanation="present tense verbs ending in 'ing'",
    ),
    Example(
        activation_records=[
            ActivationRecord(
                tokens=[
                    "as",
                    " sac",
                    "char",
                    "ine",
                    " movies",
                    " go",
                    " ,",
                    " this",
                    " is",
                    " likely",
                    " to",
                    " cause",
                    " massive",
                    " cardiac",
                    " arrest",
                    " if",
                    " taken",
                    " in",
                    " large",
                    " doses",
                    " .",
                ],
                activations=[
                    -0.14,
                    -1.37,
                    -0.68,
                    -2.27,
                    -1.46,
                    -1.11,
                    -0.9,
                    -2.48,
                    -2.07,
                    -3.49,
                    -2.16,
                    -1.79,
                    -0.23,
                    -0.04,
                    4.46,
                    -1.02,
                    -2.26,
                    -2.95,
                    -1.49,
                    -1.46,
                    -0.6,
                ],
            ),
            ActivationRecord(
                tokens=[
                    "shot",
                    " perhaps",
                    " '",
                    "art",
                    "istically",
                    "'",
                    " with",
                    " handheld",
                    " cameras",
                    " and",
                    " apparently",
                    " no",
                    " movie",
                    " lights",
                    " by",
                    " jo",
                    "aquin",
                    " b",
                    "aca",
                    "-",
                    "as",
                    "ay",
                    " ,",
                    " the",
                    " low",
                    "-",
                    "budget",
                    " production",
                    " swings",
                    " annoy",
                    "ingly",
                    " between",
                    " vert",
                    "igo",
                    " and",
                    " opacity",
                    " .",
                ],
                activations=[
                    -0.09,
                    -3.53,
                    -0.72,
                    -2.36,
                    -1.05,
                    -1.12,
                    -2.49,
                    -2.14,
                    -1.98,
                    -1.59,
                    -2.62,
                    -2,
                    -2.73,
                    -2.87,
                    -3.23,
                    -1.11,
                    -2.23,
                    -0.97,
                    -2.28,
                    -2.37,
                    -1.5,
                    -2.81,
                    -1.73,
                    -3.14,
                    -2.61,
                    -1.7,
                    -3.08,
                    -4,
                    -0.71,
                    -2.48,
                    -1.39,
                    -1.96,
                    -1.09,
                    4.37,
                    -0.74,
                    -0.5,
                    -0.62,
                ],
            ),
        ],
        first_revealed_activation_indices=[5, 20],
        explanation="words related to physical medical conditions",
    ),
    Example(
        activation_records=[
            # The sense of togetherness in our town is strong.
            ActivationRecord(
                tokens=[
                    "the",
                    " sense",
                    " of",
                    " together",
                    "ness",
                    " in",
                    " our",
                    " town",
                    " is",
                    " strong",
                    " .",
                ],
                activations=[
                    0,
                    0,
                    0,
                    1,
                    2,
                    0,
                    0.23,
                    0.5,
                    0,
                    0,
                    0,
                ],
            ),
            ActivationRecord(
                tokens=[
                    "a",
                    " buoy",
                    "ant",
                    " romantic",
                    " comedy",
                    " about",
                    " friendship",
                    " ,",
                    " love",
                    " ,",
                    " and",
                    " the",
                    " truth",
                    " that",
                    " we",
                    "'re",
                    " all",
                    " in",
                    " this",
                    " together",
                    " .",
                ],
                activations=[
                    -0.15,
                    -2.33,
                    -1.4,
                    -2.17,
                    -2.53,
                    -0.85,
                    0.23,
                    -1.89,
                    0.09,
                    -0.47,
                    -0.5,
                    -0.58,
                    -0.87,
                    0.22,
                    0.58,
                    1.34,
                    0.98,
                    2.21,
                    2.84,
                    1.7,
                    -0.89,
                ],
            ),
        ],
        first_revealed_activation_indices=[0, 10],
        explanation="phrases related to community",
    ),
]
COLANGV2_EXAMPLES = [
    Example(
        activation_records=[
            ActivationRecord(
                tokens=[
                    "The",
                    " editors",
                    " of",
                    " Bi",
                    "opol",
                    "ym",
                    "ers",
                    " are",
                    " delighted",
                    " to",
                    " present",
                    " the",
                    " ",
                    "201",
                    "8",
                    " Murray",
                    " Goodman",
                    " Memorial",
                    " Prize",
                    " to",
                    " Professor",
                    " David",
                    " N",
                    ".",
                    " Ber",
                    "atan",
                    " in",
                    " recognition",
                    " of",
                    " his",
                    " seminal",
                    " contributions",
                    " to",
                    " bi",
                    "oph",
                    "ysics",
                    " and",
                    " their",
                    " impact",
                    " on",
                    " our",
                    " understanding",
                    " of",
                    " charge",
                    " transport",
                    " in",
                    " biom",
                    "olecules",
                    ".\n\n",
                    "In",
                    "aug",
                    "ur",
                    "ated",
                    " in",
                    " ",
                    "200",
                    "7",
                    " in",
                    " honor",
                    " of",
                    " the",
                    " Bi",
                    "opol",
                    "ym",
                    "ers",
                    " Found",
                    "ing",
                    " Editor",
                    ",",
                    " the",
                    " prize",
                    " is",
                    " awarded",
                    " for",
                    " outstanding",
                    " accomplishments",
                ],
                activations=[
                    0,
                    0.01,
                    0.01,
                    0,
                    0,
                    0,
                    -0.01,
                    0,
                    -0.01,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0.04,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    3.39,
                    0.12,
                    0,
                    -0.01,
                    0,
                    0,
                    0,
                    0,
                    -0,
                    0,
                    -0,
                    0,
                    0,
                    -0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    -0,
                    0,
                    0,
                    -0.01,
                    0,
                    0.41,
                    0,
                    0,
                    0,
                    -0.01,
                    0,
                    0,
                    0,
                    0,
                    0,
                ],
            ),
            # We sometimes exceed the max context size when this is included :(
            # We can uncomment this if we start using an 8k context size.
            # ActivationRecord(
            #     tokens=[
            #         " We",
            #         " are",
            #         " proud",
            #         " of",
            #         " our",
            #         " national",
            #         " achievements",
            #         " in",
            #         " mastering",
            #         " all",
            #         " aspects",
            #         " of",
            #         " the",
            #         " fuel",
            #         " cycle",
            #         ".",
            #         " The",
            #         " current",
            #         " international",
            #         " interest",
            #         " in",
            #         " closing",
            #         " the",
            #         " fuel",
            #         " cycle",
            #         " is",
            #         " a",
            #         " vind",
            #         "ication",
            #         " of",
            #         " Dr",
            #         ".",
            #         " B",
            #         "hab",
            #         "ha",
            #         "s",
            #         " pioneering",
            #         " vision",
            #         " and",
            #         " genius",
            #     ],
            #     activations=[
            #         -0,
            #         -0,
            #         0,
            #         -0,
            #         -0,
            #         0,
            #         0,
            #         0,
            #         -0,
            #         0,
            #         0,
            #         -0,
            #         0,
            #         -0.01,
            #         0,
            #         0,
            #         -0,
            #         -0,
            #         0,
            #         0,
            #         0,
            #         -0,
            #         -0,
            #         -0.01,
            #         0,
            #         0,
            #         -0,
            #         0,
            #         0,
            #         0,
            #         0,
            #         0,
            #         -0,
            #         0,
            #         0,
            #         0,
            #         2.15,
            #         0,
            #         0,
            #         0.03,
            #     ],
            # ),
        ],
        first_revealed_activation_indices=[7],  # , 19],
        explanation="language related to something being groundbreaking",
    ),
    Example(
        activation_records=[
            ActivationRecord(
                tokens=[
                    '{"',
                    "widget",
                    "Class",
                    '":"',
                    "Variant",
                    "Matrix",
                    "Widget",
                    '","',
                    "back",
                    "order",
                    "Message",
                    '":"',
                    "Back",
                    "ordered",
                    '","',
                    "back",
                    "order",
                    "Message",
                    "Single",
                    "Variant",
                    '":"',
                    "This",
                    " item",
                    " is",
                    " back",
                    "ordered",
                    '.","',
                    "ordered",
                    "Selection",
                    '":',
                    "true",
                    ',"',
                    "product",
                    "Variant",
                    "Id",
                    '":',
                    "0",
                    ',"',
                    "variant",
                    "Id",
                    "Field",
                    '":"',
                    "product",
                    "196",
                    "39",
                    "_V",
                    "ariant",
                    "Id",
                    '","',
                    "back",
                    "order",
                    "To",
                    "Message",
                    "Single",
                    "Variant",
                    '":"',
                    "This",
                    " item",
                    " is",
                    " back",
                    "ordered",
                    " and",
                    " is",
                    " expected",
                    " by",
                    " {",
                    "0",
                    "}.",
                    '","',
                    "low",
                    "Price",
                    '":',
                    "999",
                    "9",
                    ".",
                    "0",
                    ',"',
                    "attribute",
                    "Indexes",
                    '":[',
                    '],"',
                    "productId",
                    '":',
                    "196",
                    "39",
                    ',"',
                    "price",
                    "V",
                    "ariance",
                    '":',
                    "true",
                    ',"',
                ],
                activations=[
                    -0.03,
                    0,
                    0,
                    0,
                    4.2,
                    0,
                    0,
                    0,
                    0,
                    -0,
                    0,
                    -0,
                    0,
                    0,
                    0,
                    0,
                    -0.01,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    -0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    -0.03,
                    0,
                    0,
                    0,
                    0,
                    -0.02,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    -0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    -0,
                    -0,
                    0,
                    0,
                    0,
                    0.01,
                    -0.01,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    -0.02,
                    0,
                    0,
                    0,
                    0,
                    0,
                    1.24,
                    0,
                    0,
                    0,
                ],
            ),
            ActivationRecord(
                tokens=[
                    "A",
                    " regular",
                    " look",
                    " at",
                    " the",
                    " ups",
                    " and",
                    " downs",
                    " of",
                    " variant",
                    " covers",
                    " in",
                    " the",
                    " comics",
                    " industry",
                    "\n\n",
                    "Here",
                    " are",
                    " the",
                    " Lego",
                    " variant",
                    " sketch",
                    " covers",
                    " by",
                    " Leon",
                    "el",
                    " Cast",
                    "ell",
                    "ani",
                    " for",
                    " a",
                    " variety",
                    " of",
                    " Marvel",
                    " titles",
                    ",",
                ],
                activations=[
                    0,
                    -0.01,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    6.52,
                    0,
                    0,
                    0,
                    -0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    1.62,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    -0,
                    0,
                    0,
                    0,
                    -0,
                    0,
                ],
            ),
        ],
        first_revealed_activation_indices=[2, 8],
        explanation="the word variant and other words with the same vari root",
    ),
]
COLANGV2_SINGLE_TOKEN_EXAMPLE = Example(
    activation_records=[
        ActivationRecord(
            tokens=[
                "B",
                "10",
                " ",
                "111",
                " MON",
                "DAY",
                ",",
                " F",
                "EB",
                "RU",
                "ARY",
                " ",
                "11",
                ",",
                " ",
                "201",
                "9",
                " DON",
                "ATE",
                "fake higher scoring token",  # See below.
            ],
            activations=[
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0.37,
                # This fake activation makes the previous token's activation normalize to 8, which
                # might help address overconfidence in "10" activations for the one-token-at-a-time
                # scoring prompt. This value and the associated token don't actually appear anywhere
                # in the prompt.
                0.45,
            ],
        ),
    ],
    first_revealed_activation_indices=[],
    token_index_to_score=18,
    explanation="instances of the token 'ate' as part of another word",
)
@dataclass
class AttentionSimulationExample(FastDataclass):
    token_pair_example_index: int
    token_pair_coordinates: tuple[int, int]
    label: int
@dataclass
class AttentionTokenPairExample(FastDataclass):
    tokens: list[str]
    token_pair_coordinates: list[tuple[int, int]]
@dataclass
class AttentionHeadFewShotExample(FastDataclass):
    token_pair_examples: list[AttentionTokenPairExample]
    explanation: str
    simulation_examples: list[AttentionSimulationExample] | None = None
ATTENTION_HEAD_FEW_SHOT_EXAMPLES: list[AttentionHeadFewShotExample] = [
    # gpt2-xl, layer 1, head 1
    AttentionHeadFewShotExample(
        token_pair_examples=[
            AttentionTokenPairExample(
                tokens=[
                    " dreams",
                    " of",
                    " a",
                    " future",
                    " like",
                    " her",
                    " biggest",
                    " idol",
                    ",",
                    " who",
                    " was",
                    " also",
                    " born",
                    " visually",
                    " impaired",
                    ".",
                    "\n",
                    "\n",
                    '"',
                    "My",
                    " ultimate",
                    " dream",
                    " would",
                    " be",
                    " to",
                    " sing",
                    " at",
                    " Carol",
                    "s",
                    " [",
                    "by",
                    " Candle",
                    "light",
                    "]",
                    " and",
                    " to",
                    " become",
                    " a",
                    " famous",
                    " musician",
                    " like",
                    " Andrea",
                    " Bo",
                    "cell",
                    "i",
                    " ...",
                    " and",
                    " to",
                    " show",
                    " people",
                    " that",
                    " if",
                    " you",
                    " have",
                    " a",
                    " disability",
                    " it",
                    " doesn",
                    "'t",
                    " matter",
                    ',"',
                    " she",
                    " said",
                    ".",
                ],
                # 45 = "attended from", 33 = "attended to"
                token_pair_coordinates=[(45, 33)],
            ),
            AttentionTokenPairExample(
                tokens=[
                    "omes",
                    " Ever",
                    " Sequ",
                    "enced",
                    "]",
                    "\n",
                    "\n",
                    "One",
                    " mystery",
                    " of",
                    " cat",
                    " development",
                    " is",
                    " how",
                    " cats",
                    " have",
                    " come",
                    " to",
                    " have",
                    " such",
                    " varied",
                    " coats",
                    ",",
                    " from",
                    " solid",
                    " colours",
                    " to",
                    ' "',
                    "mac",
                    "ke",
                    "rel",
                    '"',
                    " tab",
                    "by",
                    " patterns",
                    " of",
                    " thin",
                    " vertical",
                    " stripes",
                    ".",
                    " The",
                    " researchers",
                    " were",
                    " particularly",
                    " interested",
                    " in",
                    " what",
                    " turns",
                    " the",
                    " mac",
                    "ke",
                    "rel",
                    " pattern",
                    " into",
                    " a",
                    ' "',
                    "bl",
                    "ot",
                    "ched",
                    '"',
                    " tab",
                    "by",
                    " pattern",
                    ",",
                ],
                token_pair_coordinates=[(5, 4)],
            ),
            AttentionTokenPairExample(
                tokens=[
                    ",",
                    " 6",
                    ",",
                    " 8",
                    ",",
                    " 4",
                    "]",
                    "':",
                    "rb",
                    ".",
                    "sort",
                    ".",
                    "slice",
                    "(",
                    "1",
                    ",",
                    "2",
                    ");",
                    " #",
                    " More",
                    " advanced",
                    ",",
                    " this",
                    " is",
                    " Ruby",
                    "'s",
                    " map",
                    " and",
                    " each",
                    "_",
                    "with",
                    "_",
                    "index",
                    " #",
                    " This",
                    " shows",
                    " the",
                    " :",
                    "rb",
                    " post",
                    "fix",
                    "-",
                    "operator",
                    " sugar",
                    " instead",
                    " of",
                    " EV",
                    "AL",
                    ' "[',
                    "1",
                    ",",
                    "2",
                    ",",
                    "3",
                    ",",
                    "4",
                    "]",
                    '":',
                    "rb",
                    " .",
                    "map",
                    "(",
                    "->",
                    " $",
                ],
                token_pair_coordinates=[(7, 6)],
            ),
            AttentionTokenPairExample(
                tokens=[
                    " him",
                    " a",
                    " W",
                    "N",
                    " [",
                    "white",
                    " nationalist",
                    "]",
                    " until",
                    " there",
                    " is",
                    " an",
                    " indication",
                    " as",
                    " such",
                    "...",
                    " The",
                    " fact",
                    " that",
                    " he",
                    " targeted",
                    " a",
                    " church",
                    " gives",
                    " me",
                    " an",
                    " ink",
                    "ling",
                    " that",
                    " it",
                    " was",
                    " religion",
                    "-",
                    "related",
                    ',"',
                    " wrote",
                    " White",
                    "Virgin",
                    "ian",
                    ".",
                    "\n",
                    "\n",
                    '"',
                    "Yep",
                    ",",
                    " bad",
                    " news",
                    " for",
                    " gun",
                    " rights",
                    " advocates",
                    " as",
                    " well",
                    ',"',
                    " wrote",
                    " math",
                    "the",
                    "ory",
                    "l",
                    "over",
                    "2008",
                    ".",
                    ' "',
                    "Another",
                ],
                token_pair_coordinates=[(15, 7)],
            ),
            AttentionTokenPairExample(
                tokens=[
                    "23",
                    "]",
                    "\n",
                    "\n",
                    "While",
                    " preparing",
                    " to",
                    " take",
                    " the",
                    " fight",
                    " to",
                    " Prim",
                    "ord",
                    "us",
                    ",",
                    " B",
                    "alth",
                    "azar",
                    " learned",
                    " about",
                    " T",
                    "aim",
                    "i",
                    "'s",
                    " machine",
                    " and",
                    " how",
                    " it",
                    " could",
                    " supposedly",
                    " kill",
                    " two",
                    " Elder",
                    " Dragons",
                    " with",
                    " a",
                    " single",
                    " blow",
                    ",",
                    " which",
                    " p",
                    "iqu",
                    "ed",
                    " his",
                    " interest",
                    ".",
                    " This",
                    " piece",
                    " of",
                    " news",
                    ",",
                    " as",
                    " well",
                    " as",
                    " Mar",
                    "j",
                    "ory",
                    "'s",
                    " sudden",
                    " departure",
                    " from",
                    " his",
                    " side",
                    " which",
                ],
                token_pair_coordinates=[(3, 1)],
            ),
        ],
        explanation="attends to the latest closing square bracket from arbitrary subsequent tokens",
    ),
    # gpt2-xl, layer 2, head 8
    AttentionHeadFewShotExample(
        simulation_examples=[
            AttentionSimulationExample(
                token_pair_example_index=0,
                token_pair_coordinates=(63, 15),
                label=0,
            ),
            AttentionSimulationExample(
                token_pair_example_index=0,
                token_pair_coordinates=(50, 15),
                label=1,
            ),
        ],
        token_pair_examples=[
            AttentionTokenPairExample(
                tokens=[
                    " he",
                    " said",
                    ".",
                    ' "',
                    "Coming",
                    " off",
                    " winning",
                    " the",
                    " year",
                    " before",
                    ",",
                    " I",
                    " love",
                    " playing",
                    " links",
                    " golf",
                    ",",
                    " and",
                    " I",
                    " love",
                    " playing",
                    " the",
                    " week",
                    " before",
                    " a",
                    " major",
                    ".",
                    " It",
                    " was",
                    " tough",
                    " to",
                    " miss",
                    " it",
                    ".",
                    " I",
                    "'m",
                    " just",
                    " glad",
                    " to",
                    " be",
                    " back",
                    '."',
                    "\n",
                    "\n",
                    "F",
                    "owler",
                    " out",
                    "played",
                    " his",
                    " partners",
                    " Rory",
                    " Mc",
                    "Il",
                    "roy",
                    " (",
                    "74",
                    ")",
                    " and",
                    " Hen",
                    "rik",
                    " St",
                    "enson",
                    " (",
                    "72",
                ],
                token_pair_coordinates=[(50, 15)],
            ),
            AttentionTokenPairExample(
                tokens=[
                    " Club",
                    ":",
                    "\n",
                    "\n",
                    "1",
                    ".",
                    " World",
                    " renowned",
                    " golf",
                    " course",
                    "\n",
                    "\n",
                    "2",
                    ".",
                    " Vern",
                    " Mor",
                    "com",
                    " designed",
                    " golf",
                    " course",
                    "\n",
                    "\n",
                    "3",
                    ".",
                    " Great",
                    " family",
                    " holiday",
                    " destination",
                    "\n",
                    "\n",
                    "4",
                    ".",
                    " Play",
                    " amid",
                    " our",
                    " resident",
                    " Eastern",
                    " Grey",
                    " k",
                    "ang",
                    "aroo",
                    " population",
                    "\n",
                    "\n",
                    "5",
                    ".",
                    " Terr",
                    "ific",
                    " friendly",
                    " staff",
                    "\n",
                    "\n",
                    "6",
                    ".",
                    " Natural",
                    " pictures",
                    "que",
                    " bush",
                    " setting",
                    "\n",
                    "\n",
                    "7",
                    ".",
                    " L",
                ],
                token_pair_coordinates=[(9, 8)],
            ),
            AttentionTokenPairExample(
                tokens=[
                    "615",
                    " rpm",
                    " on",
                    " average",
                    ").",
                    " As",
                    " a",
                    " result",
                    ",",
                    " each",
                    " player",
                    " was",
                    " hitting",
                    " longer",
                    " drives",
                    " on",
                    " their",
                    " best",
                    " shots",
                    ",",
                    " while",
                    " achieving",
                    " a",
                    " stra",
                    "ighter",
                    " ball",
                    " flight",
                    " that",
                    " was",
                    " less",
                    " affected",
                    " by",
                    " wind",
                    ".",
                    "\n",
                    "\n",
                    "Every",
                    " Golf",
                    "WR",
                    "X",
                    " Member",
                    " gained",
                    " yard",
                    "age",
                    " with",
                    " a",
                    " new",
                    " Taylor",
                    "Made",
                    " driver",
                    ";",
                    " the",
                    " largest",
                    " distance",
                    " gain",
                    " was",
                    " an",
                    " impressive",
                    " +",
                    "10",
                    ".",
                    "1",
                    " yards",
                    ",",
                ],
                token_pair_coordinates=[(47, 37)],
            ),
            AttentionTokenPairExample(
                tokens=[
                    " of",
                    " being",
                    "?",
                    " Well",
                    ",",
                    " having",
                    " perfected",
                    " the",
                    " art",
                    " of",
                    " swimming",
                    ",",
                    " Phelps",
                    " has",
                    " moved",
                    " on",
                    " to",
                    " another",
                    " cherished",
                    " summer",
                    " past",
                    "ime",
                    " ",
                    " golf",
                    ".",
                    " Here",
                    " he",
                    " is",
                    " participating",
                    " in",
                    " the",
                    " Dun",
                    "hill",
                    " Links",
                    " Championship",
                    " at",
                    " Kings",
                    "b",
                    "arn",
                    "s",
                    " in",
                    " Scotland",
                    " today",
                    ".",
                    " The",
                    " greens",
                    " over",
                    " there",
                    " are",
                    " really",
                    " big",
                    ",",
                    " so",
                    " the",
                    " opportunity",
                    " for",
                    " 50",
                    "-",
                    "yard",
                    " put",
                    "ts",
                    " exist",
                    ".",
                    " Of",
                ],
                token_pair_coordinates=[(45, 23)],
            ),
            AttentionTokenPairExample(
                tokens=[
                    "OTUS",
                    " is",
                    " getting",
                    " to",
                    " see",
                    " aliens",
                    ".",
                    "\n",
                    "\n",
                    "RELATED",
                    ":",
                    " Barack",
                    " Obama",
                    " joins",
                    " second",
                    " D",
                    ".",
                    "C",
                    ".-",
                    "area",
                    " golf",
                    " club",
                    "\n",
                    "\n",
                    '"',
                    "He",
                    " goes",
                    ",",
                    " '",
                    "they",
                    "'re",
                    " freaking",
                    " crazy",
                    " looking",
                    ".'",
                    " And",
                    " then",
                    " he",
                    " walks",
                    " up",
                    ",",
                    " makes",
                    " his",
                    " put",
                    "t",
                    ",",
                    " turns",
                    " back",
                    ",",
                    " walks",
                    " off",
                    " the",
                    " green",
                    ",",
                    " leaves",
                    " it",
                    " at",
                    " that",
                    " and",
                    " gives",
                    " me",
                    " a",
                    " wink",
                    ',"',
                ],
                token_pair_coordinates=[(52, 20)],
            ),
        ],
        explanation='attends to the token "golf" from golf-related tokens',
    ),
    # gpt2-xl, layer 1, head 10
    AttentionHeadFewShotExample(
        simulation_examples=[
            AttentionSimulationExample(
                token_pair_example_index=0,
                token_pair_coordinates=(37, 36),
                label=0,
            ),
            AttentionSimulationExample(
                token_pair_example_index=0,
                token_pair_coordinates=(14, 12),
                label=1,
            ),
        ],
        token_pair_examples=[
            AttentionTokenPairExample(
                tokens=[
                    " security",
                    " by",
                    " requiring",
                    " the",
                    " user",
                    " to",
                    " enter",
                    " a",
                    " numeric",
                    " code",
                    " sent",
                    " to",
                    " his",
                    " or",
                    " her",
                    " cellphone",
                    " in",
                    " addition",
                    " to",
                    " a",
                    " password",
                    ".",
                    " A",
                    " lot",
                    " of",
                    " websites",
                    " have",
                    " offered",
                    " this",
                    " feature",
                    " for",
                    " years",
                    ",",
                    " but",
                    " Int",
                    "uit",
                    " just",
                    " made",
                    " it",
                    " widely",
                    " available",
                    " earlier",
                    " this",
                    " year",
                    ".",
                    "\n",
                    "\n",
                    '"',
                    "When",
                    " you",
                    " give",
                    " your",
                    " most",
                    " sensitive",
                    " data",
                    " and",
                    " that",
                    " of",
                    " your",
                    " family",
                    " to",
                    " a",
                    " company",
                    ",",
                ],
                token_pair_coordinates=[(14, 12)],
            ),
            AttentionTokenPairExample(
                tokens=[
                    " 3",
                    " months",
                    ",",
                    " they",
                    " separated",
                    " the",
                    " men",
                    " and",
                    " women",
                    " here",
                    ".",
                    " I",
                    " don",
                    "'t",
                    " know",
                    " where",
                    " they",
                    " took",
                    " the",
                    " men",
                    " and",
                    " the",
                    " children",
                    ",",
                    " but",
                    " they",
                    " took",
                    " us",
                    " women",
                    " to",
                    " Syria",
                    ".",
                    " They",
                    " kept",
                    " us",
                    " in",
                    " an",
                    " underground",
                    " prison",
                    ".",
                    " My",
                    " only",
                    " wish",
                    " is",
                    " that",
                    " my",
                    " children",
                    " and",
                    " husband",
                    " escape",
                    " ISIS",
                    ".",
                    " They",
                    " brought",
                    " us",
                    " here",
                    " from",
                    " Raqqa",
                    ",",
                    " my",
                    " sisters",
                    " from",
                    " the",
                    " PKK",
                ],
                token_pair_coordinates=[(8, 6)],
            ),
            AttentionTokenPairExample(
                tokens=[
                    " an",
                    " emphasis",
                    " on",
                    " the",
                    " pursuit",
                    " of",
                    " power",
                    " despite",
                    " interpersonal",
                    " costs",
                    '."',
                    "\n",
                    "\n",
                    "The",
                    " study",
                    ",",
                    " which",
                    " involved",
                    " over",
                    " 600",
                    " young",
                    " men",
                    " and",
                    " women",
                    ",",
                    " makes",
                    " a",
                    " strong",
                    " case",
                    " for",
                    " assessing",
                    " such",
                    " traits",
                    " as",
                    ' "',
                    "r",
                    "uth",
                    "less",
                    " ambition",
                    ',"',
                    ' "',
                    "dis",
                    "comfort",
                    " with",
                    " leadership",
                    '"',
                    " and",
                    ' "',
                    "hub",
                    "rist",
                    "ic",
                    " pride",
                    '"',
                    " to",
                    " understand",
                    " psychopath",
                    "ologies",
                    ".",
                    "\n",
                    "\n",
                    "The",
                    " researchers",
                    " looked",
                    " at",
                ],
                token_pair_coordinates=[(23, 21)],
            ),
            AttentionTokenPairExample(
                tokens=[
                    " 4",
                    " hours",
                    ".",
                    " These",
                    " results",
                    ",",
                    " differ",
                    " between",
                    " men",
                    " and",
                    " women",
                    ",",
                    " however",
                    ".",
                    " We",
                    " can",
                    " see",
                    " that",
                    " although",
                    " both",
                    " groups",
                    " have",
                    " a",
                    " large",
                    " cluster",
                    " of",
                    " people",
                    " at",
                    " exactly",
                    " 40",
                    " hours",
                    " per",
                    " week",
                    ",",
                    " there",
                    " are",
                    " more",
                    " men",
                    " reporting",
                    " hours",
                    " above",
                    " 40",
                    ",",
                    " whereas",
                    " there",
                    " are",
                    " more",
                    " women",
                    " reporting",
                    " hours",
                    " below",
                    " 40",
                    ".",
                    " Result",
                    " 3",
                    ":",
                    " Male",
                    " Hours",
                    " Work",
                    "ed",
                    " [",
                    "Info",
                    "]",
                    " Owner",
                ],
                token_pair_coordinates=[(10, 8)],
            ),
            AttentionTokenPairExample(
                tokens=[
                    " they",
                    " were",
                    " perceived",
                    " as",
                    " more",
                    " emotional",
                    ",",
                    " which",
                    " made",
                    " participants",
                    " more",
                    " confident",
                    " in",
                    " their",
                    " own",
                    " opinion",
                    '."',
                    "\n",
                    "\n",
                    "Ms",
                    " Sal",
                    "erno",
                    " said",
                    " both",
                    " men",
                    " and",
                    " women",
                    " reacted",
                    " in",
                    " the",
                    " same",
                    " way",
                    " to",
                    " women",
                    " expressing",
                    " themselves",
                    " angrily",
                    ".",
                    "\n",
                    "\n",
                    '"',
                    "Particip",
                    "ants",
                    " confidence",
                    " in",
                    " their",
                    " own",
                    " verdict",
                    " dropped",
                    " significantly",
                    " after",
                    " male",
                    " hold",
                    "outs",
                    " expressed",
                    " anger",
                    ',"',
                    " the",
                    " paper",
                    "'s",
                    " findings",
                    " stated",
                    ".",
                    "\n",
                ],
                token_pair_coordinates=[(26, 24)],
            ),
        ],
        explanation="attends to male-related tokens from paired female-related tokens",
    ),
    # gpt2-xl, layer 1, head 3
    AttentionHeadFewShotExample(
        token_pair_examples=[
            AttentionTokenPairExample(
                tokens=[
                    "\n",
                    "********************************",
                    "************",
                    "***",
                    "\n",
                    "\n",
                    "V",
                    "iet",
                    "namese",
                    " Ministry",
                    " of",
                    " Foreign",
                    " Affairs",
                    " spokesperson",
                    " Le",
                    " Hai",
                    " Bin",
                    "h",
                    " is",
                    " seen",
                    " in",
                    " this",
                    " file",
                    " photo",
                    ".",
                    " .",
                    " Tu",
                    "oi",
                    " Tre",
                    "\n",
                    "\n",
                    "The",
                    " Ministry",
                    " of",
                    " Foreign",
                    " Affairs",
                    " has",
                    " ordered",
                    " a",
                    " thorough",
                    " investigation",
                    " into",
                    " a",
                    " case",
                    " in",
                    " which",
                    " a",
                    " Vietnamese",
                    " fisherman",
                    " was",
                    " shot",
                    " dead",
                    " on",
                    " his",
                    " boat",
                    " in",
                    " Vietnam",
                    "'s",
                    " Tru",
                    "ong",
                    " Sa",
                    " (",
                    "Spr",
                    "at",
                ],
                token_pair_coordinates=[(1, 1)],
            ),
            AttentionTokenPairExample(
                tokens=[
                    " J",
                    "okin",
                    "en",
                    " tells",
                    " a",
                    " much",
                    " different",
                    " story",
                    ".",
                    " He",
                    " almost",
                    " sounded",
                    " like",
                    " a",
                    " pitch",
                    "man",
                    ".",
                    "\n",
                    "\n",
                    '"',
                    "All",
                    " the",
                    " staff",
                    ",",
                    " team",
                    " service",
                    " guys",
                    ",",
                    " all",
                    " the",
                    " trainers",
                    ",",
                    " they",
                    "'re",
                    " unbelievable",
                    " guys",
                    ',"',
                    " said",
                    " J",
                    "okin",
                    "en",
                    ".",
                    ' "',
                    "It",
                    "'s",
                    " not",
                    " just",
                    " the",
                    " players",
                    ",",
                    " it",
                    "'s",
                    " the",
                    " staff",
                    " around",
                    " the",
                    " team",
                    ".",
                    " I",
                    " feel",
                    " really",
                    " bad",
                    " for",
                    " them",
                ],
                token_pair_coordinates=[(1, 1)],
            ),
            AttentionTokenPairExample(
                tokens=[
                    " a",
                    " Pv",
                    "E",
                    " game",
                    ",",
                    " we",
                    " probably",
                    " would",
                    " use",
                    " it",
                    " but",
                    " based",
                    " on",
                    " the",
                    " tests",
                    " we",
                    "'ve",
                    " run",
                    " on",
                    " it",
                    ",",
                    " that",
                    " wouldn",
                    "'t",
                    " be",
                    " our",
                    " first",
                    " choice",
                    " for",
                    " a",
                    " live",
                    " R",
                    "v",
                    "R",
                    " game",
                    ".",
                    " Now",
                    ",",
                    " could",
                    " we",
                    " use",
                    " it",
                    " for",
                    " prototyp",
                    "ing",
                    "?",
                    " Yep",
                    ",",
                    " we",
                    " are",
                    " already",
                    " doing",
                    " that",
                    ".",
                    " Second",
                    ",",
                    " as",
                    " to",
                    " other",
                    " engines",
                    " there",
                    " are",
                    " both",
                    " financial",
                ],
                token_pair_coordinates=[(1, 1)],
            ),
            AttentionTokenPairExample(
                tokens=[
                    "-",
                    "tun",
                    "er",
                    " is",
                    " also",
                    " custom",
                    "isable",
                    " for",
                    " hassle",
                    "-",
                    "free",
                    " experimentation",
                    ".",
                    " The",
                    " St",
                    "rix",
                    " X",
                    "399",
                    "-",
                    "E",
                    " Gaming",
                    " takes",
                    " up",
                    " to",
                    " three",
                    " double",
                    "-",
                    "wide",
                    " cards",
                    " in",
                    " SLI",
                    " or",
                    " Cross",
                    "Fire",
                    "X",
                    ".",
                    " Primary",
                    " graphics",
                    " slots",
                    " are",
                    " protected",
                    " by",
                    " Safe",
                    "Slot",
                    " from",
                    " damages",
                    " that",
                    " heavy",
                    " GPU",
                    " cool",
                    "ers",
                    " can",
                    " potentially",
                    " cause",
                    ".",
                    "\n",
                    "\n",
                    "Personal",
                    "ised",
                    " RGB",
                    " lighting",
                    " is",
                    " made",
                    " possible",
                ],
                token_pair_coordinates=[(1, 1)],
            ),
            AttentionTokenPairExample(
                tokens=[
                    " to",
                    " abolish",
                    " such",
                    " a",
                    " complex",
                    "?",
                    " Are",
                    " there",
                    " ways",
                    " ve",
                    "gans",
                    " can",
                    " eat",
                    " more",
                    " sustain",
                    "ably",
                    "?",
                    " What",
                    " are",
                    " some",
                    " of",
                    " the",
                    " health",
                    " challenges",
                    " for",
                    " new",
                    " ve",
                    "gans",
                    ",",
                    " and",
                    " how",
                    " can",
                    " we",
                    " raise",
                    " awareness",
                    " of",
                    " these",
                    " issues",
                    " so",
                    " that",
                    ",",
                    " for",
                    " instance",
                    ",",
                    " medical",
                    " professionals",
                    " are",
                    " more",
                    " supportive",
                    " of",
                    " vegan",
                    "ism",
                    "?",
                    "\n",
                    "\n",
                    "Moreover",
                    ",",
                    " it",
                    " is",
                    " essential",
                    " that",
                    " ve",
                    "gans",
                    " differentiate",
                ],
                token_pair_coordinates=[(1, 1)],
            ),
        ],
        explanation="attends from the second token in the sequence to the second token in the sequence",
    ),
]

================
File: build/lib/neuron_explainer/explanations/prompt_builder.py
================
from __future__ import annotations
from enum import Enum
from typing import TypedDict
import tiktoken
class Role(str, Enum):
    """See https://platform.openai.com/docs/guides/chat"""
    SYSTEM = "system"
    USER = "user"
    ASSISTANT = "assistant"
ChatMessage = TypedDict(
    "ChatMessage",
    {
        "role": Role,
        "content": str,
    },
)
class PromptFormat(str, Enum):
    """
    Different ways of formatting the components of a prompt into the format accepted by the relevant
    API server endpoint.
    """
    NONE = "none"
    """Suitable for use with models that don't use special tokens for instructions."""
    INSTRUCTION_FOLLOWING = "instruction_following"
    """Suitable for IF models that use <|endofprompt|>."""
    CHAT_MESSAGES = "chat_messages"
    """
    Suitable for ChatGPT models that use a structured turn-taking role+content format. Generates a
    list of ChatMessage dicts that can be sent to the /chat/completions endpoint.
    """
    @classmethod
    def from_string(cls, s: str) -> PromptFormat:
        for prompt_format in cls:
            if prompt_format.value == s:
                return prompt_format
        raise ValueError(f"{s} is not a valid PromptFormat")
class PromptBuilder:
    """Class for accumulating components of a prompt and then formatting them into an output."""
    def __init__(self, allow_extra_system_messages: bool = False) -> None:
        """The `allow_extra_system_messages` instance variable allows the caller to specify that the prompt
        should be allowed to contain system messages after the very first one."""
        self._messages: list[ChatMessage] = []
        self._allow_extra_system_messages = allow_extra_system_messages
    def add_message(self, role: Role, message: str) -> None:
        self._messages.append(ChatMessage(role=role, content=message))
    def prompt_length_in_tokens(self, prompt_format: PromptFormat) -> int:
        # TODO(sbills): Make the model/encoding configurable. This implementation assumes GPT-4.
        encoding = tiktoken.get_encoding("cl100k_base")
        if prompt_format == PromptFormat.CHAT_MESSAGES:
            # Approximately-correct implementation adapted from this documentation:
            # https://platform.openai.com/docs/guides/chat/introduction
            num_tokens = 0
            for message in self._messages:
                num_tokens += (
                    4  # every message follows <|im_start|>{role/name}\n{content}<|im_end|>\n
                )
                num_tokens += len(encoding.encode(message["content"], allowed_special="all"))
            num_tokens += 2  # every reply is primed with <|im_start|>assistant
            return num_tokens
        else:
            prompt_str = self.build(prompt_format)
            assert isinstance(prompt_str, str)
            return len(encoding.encode(prompt_str, allowed_special="all"))
    def build(self, prompt_format: PromptFormat) -> str | list[ChatMessage]:
        """
        Validates the messages added so far (reasonable alternation of assistant vs. user, etc.)
        and returns either a regular string (maybe with <|endofprompt|> tokens) or a list of
        ChatMessages suitable for use with the /chat/completions endpoint.
        """
        # Create a deep copy of the messages so we can modify it and so that the caller can't
        # modify the internal state of this object.
        messages = [message.copy() for message in self._messages]
        expected_next_role = Role.SYSTEM
        for message in messages:
            role = message["role"]
            assert role == expected_next_role or (
                self._allow_extra_system_messages and role == Role.SYSTEM
            ), f"Expected message from {expected_next_role} but got message from {role}"
            if role == Role.SYSTEM:
                expected_next_role = Role.USER
            elif role == Role.USER:
                expected_next_role = Role.ASSISTANT
            elif role == Role.ASSISTANT:
                expected_next_role = Role.USER
        if prompt_format == PromptFormat.INSTRUCTION_FOLLOWING:
            last_user_message = None
            for message in messages:
                if message["role"] == Role.USER:
                    last_user_message = message
            assert last_user_message is not None
            last_user_message["content"] += "<|endofprompt|>"
        if prompt_format == PromptFormat.CHAT_MESSAGES:
            return messages
        elif prompt_format in [PromptFormat.NONE, PromptFormat.INSTRUCTION_FOLLOWING]:
            return "".join(message["content"] for message in messages)
        else:
            raise ValueError(f"Unknown prompt format: {prompt_format}")

================
File: build/lib/neuron_explainer/explanations/scoring.py
================
from __future__ import annotations
import asyncio
import logging
from typing import Any, Callable, Coroutine, Sequence
import numpy as np
from neuron_explainer.activations.activations import ActivationRecord
from neuron_explainer.api_client import ApiClient
from neuron_explainer.explanations.calibrated_simulator import (
    CalibratedNeuronSimulator,
    LinearCalibratedNeuronSimulator,
    UncalibratedNeuronSimulator,
)
from neuron_explainer.explanations.explanations import (
    ScoredSequenceSimulation,
    ScoredSimulation,
    SequenceSimulation,
)
from neuron_explainer.explanations.simulator import (
    ExplanationNeuronSimulator,
    LogprobFreeExplanationTokenSimulator,
    NeuronSimulator,
)
def flatten_list(list_of_lists: Sequence[Sequence[Any]]) -> list[Any]:
    return [item for sublist in list_of_lists for item in sublist]
def correlation_score(
    real_activations: Sequence[float] | np.ndarray,
    predicted_activations: Sequence[float] | np.ndarray,
) -> float:
    score = np.corrcoef(real_activations, predicted_activations)[0, 1]
    if np.isnan(score):
        return 0.0
    return score
def score_from_simulation(
    real_activations: ActivationRecord,
    simulation: SequenceSimulation,
    score_function: Callable[[Sequence[float] | np.ndarray, Sequence[float] | np.ndarray], float],
) -> float:
    return score_function(real_activations.activations, simulation.expected_activations)
def rsquared_score_from_sequences(
    real_activations: Sequence[float] | np.ndarray,
    predicted_activations: Sequence[float] | np.ndarray,
) -> float:
    return float(
        1
        - np.mean(np.square(np.array(real_activations) - np.array(predicted_activations)))
        / np.mean(np.square(np.array(real_activations)))
    )
def absolute_dev_explained_score_from_sequences(
    real_activations: Sequence[float] | np.ndarray,
    predicted_activations: Sequence[float] | np.ndarray,
) -> float:
    return float(
        1
        - np.mean(np.abs(np.array(real_activations) - np.array(predicted_activations)))
        / np.mean(np.abs(np.array(real_activations)))
    )
async def make_uncalibrated_explanation_simulator(
    explanation: str,
    client: ApiClient,
    **kwargs: Any,
) -> CalibratedNeuronSimulator:
    """Make a simulator that doesn't apply any calibration."""
    simulator = LogprobFreeExplanationTokenSimulator(client, explanation, **kwargs)
    calibrated_simulator = UncalibratedNeuronSimulator(simulator)
    return calibrated_simulator
async def make_explanation_simulator(
    explanation: str,
    calibration_activation_records: Sequence[ActivationRecord],
    client: ApiClient,
    calibrated_simulator_class: type[CalibratedNeuronSimulator] = LinearCalibratedNeuronSimulator,
    **kwargs: Any,
) -> CalibratedNeuronSimulator:
    """
    Make a simulator that uses an explanation to predict activations and calibrates it on the given
    activation records.
    """
    simulator = ExplanationNeuronSimulator(client, explanation, **kwargs)
    calibrated_simulator = calibrated_simulator_class(simulator)
    await calibrated_simulator.calibrate(calibration_activation_records)
    return calibrated_simulator
async def _simulate_and_score_sequence(
    simulator: NeuronSimulator, activations: ActivationRecord
) -> ScoredSequenceSimulation:
    """Score an explanation of a neuron by how well it predicts activations on a sentence."""
    sequence_simulation = await simulator.simulate(activations.tokens)
    logging.debug(sequence_simulation)
    rsquared_score = score_from_simulation(
        activations, sequence_simulation, rsquared_score_from_sequences
    )
    absolute_dev_explained_score = score_from_simulation(
        activations, sequence_simulation, absolute_dev_explained_score_from_sequences
    )
    scored_sequence_simulation = ScoredSequenceSimulation(
        sequence_simulation=sequence_simulation,
        true_activations=activations.activations,
        ev_correlation_score=score_from_simulation(
            activations, sequence_simulation, correlation_score
        ),
        rsquared_score=rsquared_score,
        absolute_dev_explained_score=absolute_dev_explained_score,
    )
    return scored_sequence_simulation
def aggregate_scored_sequence_simulations(
    scored_sequence_simulations: list[ScoredSequenceSimulation],
) -> ScoredSimulation:
    """
    Aggregate a list of scored sequence simulations. The logic for doing this is non-trivial for EV
    scores, since we want to calculate the correlation over all activations from all sequences at
    once rather than simply averaging per-sequence correlations.
    """
    all_true_activations: list[float] = []
    all_expected_values: list[float] = []
    for scored_sequence_simulation in scored_sequence_simulations:
        all_true_activations.extend(scored_sequence_simulation.true_activations or [])
        all_expected_values.extend(
            scored_sequence_simulation.sequence_simulation.expected_activations
        )
    ev_correlation_score = (
        correlation_score(all_true_activations, all_expected_values)
        if len(all_true_activations) > 0
        else None
    )
    rsquared_score = rsquared_score_from_sequences(all_true_activations, all_expected_values)
    absolute_dev_explained_score = absolute_dev_explained_score_from_sequences(
        all_true_activations, all_expected_values
    )
    return ScoredSimulation(
        scored_sequence_simulations=scored_sequence_simulations,
        ev_correlation_score=ev_correlation_score,
        rsquared_score=rsquared_score,
        absolute_dev_explained_score=absolute_dev_explained_score,
    )
async def simulate_and_score(
    simulator: NeuronSimulator,
    activation_records: Sequence[ActivationRecord],
) -> ScoredSimulation:
    """
    Score an explanation of a neuron by how well it predicts activations on the given text
    sequences.
    """
    scored_sequence_simulations = await asyncio.gather(
        *[
            _simulate_and_score_sequence(
                simulator,
                activation_record,
            )
            for activation_record in activation_records
        ]
    )
    return aggregate_scored_sequence_simulations(scored_sequence_simulations)
async def make_simulator_and_score(
    make_simulator: Coroutine[None, None, NeuronSimulator],
    activation_records: Sequence[ActivationRecord],
) -> ScoredSimulation:
    """Chain together creating the simulator and using it to score activation records."""
    simulator = await make_simulator
    return await simulate_and_score(simulator, activation_records)

================
File: build/lib/neuron_explainer/explanations/simulator.py
================
"""Uses API calls to simulate neuron activations based on an explanation."""
from __future__ import annotations
import asyncio
import json
import logging
from abc import ABC, abstractmethod
from collections import OrderedDict
from enum import Enum
from typing import Any, Sequence
import numpy as np
from neuron_explainer.activations.activation_records import (
    calculate_max_activation,
    format_activation_records,
    format_sequences_for_simulation,
    normalize_activations,
)
from neuron_explainer.activations.activations import ActivationRecord
from neuron_explainer.api_client import ApiClient
from neuron_explainer.explanations.explainer import EXPLANATION_PREFIX
from neuron_explainer.explanations.explanations import ActivationScale, SequenceSimulation
from neuron_explainer.explanations.few_shot_examples import FewShotExampleSet
from neuron_explainer.explanations.prompt_builder import (
    ChatMessage,
    PromptBuilder,
    PromptFormat,
    Role,
)
logger = logging.getLogger(__name__)
# Our prompts use normalized activation values, which map any range of positive activations to the
# integers from 0 to 10.
MAX_NORMALIZED_ACTIVATION = 10
VALID_ACTIVATION_TOKENS_ORDERED = [str(i) for i in range(MAX_NORMALIZED_ACTIVATION + 1)]
VALID_ACTIVATION_TOKENS = set(VALID_ACTIVATION_TOKENS_ORDERED)
class SimulationType(str, Enum):
    """How to simulate neuron activations. Values correspond to subclasses of NeuronSimulator."""
    ALL_AT_ONCE = "all_at_once"
    """
    Use a single prompt with <unknown> tokens; calculate EVs using logprobs.
    Implemented by ExplanationNeuronSimulator.
    """
    ONE_AT_A_TIME = "one_at_a_time"
    """
    Use a separate prompt for each token being simulated; calculate EVs using logprobs.
    Implemented by ExplanationTokenByTokenSimulator.
    """
    @classmethod
    def from_string(cls, s: str) -> SimulationType:
        for simulation_type in SimulationType:
            if simulation_type.value == s:
                return simulation_type
        raise ValueError(f"Invalid simulation type: {s}")
def compute_expected_value(
    norm_probabilities_by_distribution_value: OrderedDict[int, float]
) -> float:
    """
    Given a map from distribution values (integers on the range [0, 10]) to normalized
    probabilities, return an expected value for the distribution.
    """
    return np.dot(
        np.array(list(norm_probabilities_by_distribution_value.keys())),
        np.array(list(norm_probabilities_by_distribution_value.values())),
    )
def parse_top_logprobs(top_logprobs: dict[str, float]) -> OrderedDict[int, float]:
    """
    Given a map from tokens to logprobs, return a map from distribution values (integers on the
    range [0, 10]) to unnormalized probabilities (in the sense that they may not sum to 1).
    """
    probabilities_by_distribution_value = OrderedDict()
    for token, logprob in top_logprobs.items():
        if token in VALID_ACTIVATION_TOKENS:
            token_as_int = int(token)
            probabilities_by_distribution_value[token_as_int] = np.exp(logprob)
    return probabilities_by_distribution_value
def compute_predicted_activation_stats_for_token(
    top_logprobs: dict[str, float],
) -> tuple[OrderedDict[int, float], float]:
    probabilities_by_distribution_value = parse_top_logprobs(top_logprobs)
    total_p_of_distribution_values = sum(probabilities_by_distribution_value.values())
    norm_probabilities_by_distribution_value = OrderedDict(
        {
            distribution_value: p / total_p_of_distribution_values
            for distribution_value, p in probabilities_by_distribution_value.items()
        }
    )
    expected_value = compute_expected_value(norm_probabilities_by_distribution_value)
    return (
        norm_probabilities_by_distribution_value,
        expected_value,
    )
# Adapted from tether/tether/core/encoder.py.
def convert_to_byte_array(s: str) -> bytearray:
    byte_array = bytearray()
    assert s.startswith("bytes:"), s
    s = s[6:]
    while len(s) > 0:
        if s[0] == "\\":
            # Hex encoding.
            assert s[1] == "x"
            assert len(s) >= 4
            byte_array.append(int(s[2:4], 16))
            s = s[4:]
        else:
            # Regular ascii encoding.
            byte_array.append(ord(s[0]))
            s = s[1:]
    return byte_array
def handle_byte_encoding(
    response_tokens: Sequence[str], merged_response_index: int
) -> tuple[str, int]:
    """
    Handle the case where the current token is a sequence of bytes. This may involve merging
    multiple response tokens into a single token.
    """
    response_token = response_tokens[merged_response_index]
    if response_token.startswith("bytes:"):
        byte_array = bytearray()
        while True:
            byte_array = convert_to_byte_array(response_token) + byte_array
            try:
                # If we can decode the byte array as utf-8, then we're done.
                response_token = byte_array.decode("utf-8")
                break
            except UnicodeDecodeError:
                # If not, then we need to merge the previous response token into the byte
                # array.
                merged_response_index -= 1
                response_token = response_tokens[merged_response_index]
    return response_token, merged_response_index
def was_token_split(current_token: str, response_tokens: Sequence[str], start_index: int) -> bool:
    """
    Return whether current_token (a token from the subject model) was split into multiple tokens by
    the simulator model (as represented by the tokens in response_tokens). start_index is the index
    in response_tokens at which to begin looking backward to form a complete token. It is usually
    the first token *before* the delimiter that separates the token from the normalized activation,
    barring some unusual cases.
    This mainly happens if the subject model uses a different tokenizer than the simulator model.
    But it can also happen in cases where Unicode characters are split. This function handles both
    cases.
    """
    merged_response_tokens = ""
    merged_response_index = start_index
    while len(merged_response_tokens) < len(current_token):
        response_token = response_tokens[merged_response_index]
        response_token, merged_response_index = handle_byte_encoding(
            response_tokens, merged_response_index
        )
        merged_response_tokens = response_token + merged_response_tokens
        merged_response_index -= 1
    # It's possible that merged_response_tokens is longer than current_token at this point,
    # since the between-lines delimiter may have been merged into the original token. But it
    # should always be the case that merged_response_tokens ends with current_token.
    assert merged_response_tokens.endswith(current_token)
    num_merged_tokens = start_index - merged_response_index
    token_was_split = num_merged_tokens > 1
    if token_was_split:
        logger.debug(
            "Warning: token from the subject model was split into 2+ tokens by the simulator model."
        )
    return token_was_split
def parse_simulation_response(
    response: dict[str, Any],
    prompt_format: PromptFormat,
    tokens: Sequence[str],
) -> SequenceSimulation:
    """
    Parse an API response to a simulation prompt.
    Args:
        response: response from the API
        prompt_format: how the prompt was formatted
        tokens: list of tokens as strings in the sequence where the neuron is being simulated
    """
    choice = response["choices"][0]
    if prompt_format == PromptFormat.CHAT_MESSAGES:
        text = choice["message"]["content"]
    elif prompt_format in [
        PromptFormat.NONE,
        PromptFormat.INSTRUCTION_FOLLOWING,
    ]:
        text = choice["text"]
    else:
        raise ValueError(f"Unhandled prompt format {prompt_format}")
    response_tokens = choice["logprobs"]["tokens"]
    top_logprobs = choice["logprobs"]["top_logprobs"]
    token_text_offset = choice["logprobs"]["text_offset"]
    # This only works because the sequence "<start>" tokenizes into multiple tokens if it appears in
    # a text sequence in the prompt.
    scoring_start = text.rfind("<start>")
    expected_values = []
    original_sequence_tokens: list[str] = []
    distribution_values: list[list[float]] = []
    distribution_probabilities: list[list[float]] = []
    for i in range(2, len(response_tokens)):
        if len(original_sequence_tokens) == len(tokens):
            # Make sure we haven't hit some sort of off-by-one error.
            # TODO(sbills): Generalize this to handle different tokenizers.
            reached_end = response_tokens[i + 1] == "<" and response_tokens[i + 2] == "end"
            assert reached_end, f"{response_tokens[i-3:i+3]}"
            break
        if token_text_offset[i] >= scoring_start:
            # We're looking for the first token after a tab. This token should be the text
            # "unknown" if hide_activations=True or a normalized activation (0-10) otherwise.
            # If it isn't, that means that the tab is not appearing as a delimiter, but rather
            # as a token, in which case we should move on to the next response token.
            if response_tokens[i - 1] == "\t":
                if response_tokens[i] != "unknown":
                    logger.debug("Ignoring tab token that is not followed by an 'unknown' token.")
                    continue
                # j represents the index of the token in a "token<tab>activation" line, barring
                # one of the unusual cases handled below.
                j = i - 2
                current_token = tokens[len(original_sequence_tokens)]
                if current_token == response_tokens[j] or was_token_split(
                    current_token, response_tokens, j
                ):
                    # We're in the normal case where the tokenization didn't throw off the
                    # formatting or in the token-was-split case, which we handle the usual way.
                    current_top_logprobs = top_logprobs[i]
                    (
                        norm_probabilities_by_distribution_value,
                        expected_value,
                    ) = compute_predicted_activation_stats_for_token(
                        current_top_logprobs,
                    )
                    current_distribution_values = list(
                        norm_probabilities_by_distribution_value.keys()
                    )
                    current_distribution_probabilities = list(
                        norm_probabilities_by_distribution_value.values()
                    )
                else:
                    # We're in a case where the tokenization resulted in a newline being folded into
                    # the token. We can't do our usual prediction of activation stats for the token,
                    # since the model did not observe the original token. Instead, we use dummy
                    # values. See the TODO elsewhere in this file about coming up with a better
                    # prompt format that avoids this situation.
                    newline_folded_into_token = "\n" in response_tokens[j]
                    assert (
                        newline_folded_into_token
                    ), f"`{current_token=}` {response_tokens[j-3:j+3]=}"
                    logger.debug(
                        "Warning: newline before a token<tab>activation line was folded into the token"
                    )
                    current_distribution_values = []
                    current_distribution_probabilities = []
                    expected_value = 0.0
                original_sequence_tokens.append(current_token)
                # These values are ints, but for backward compatibility we store them as floats.
                distribution_values.append([float(v) for v in current_distribution_values])
                distribution_probabilities.append(current_distribution_probabilities)
                expected_values.append(expected_value)
    return SequenceSimulation(
        activation_scale=ActivationScale.SIMULATED_NORMALIZED_ACTIVATIONS,
        expected_activations=expected_values,
        distribution_values=distribution_values,
        distribution_probabilities=distribution_probabilities,
        tokens=original_sequence_tokens,
    )
class NeuronSimulator(ABC):
    """Abstract base class for simulating neuron behavior."""
    @abstractmethod
    async def simulate(self, tokens: Sequence[str]) -> SequenceSimulation:
        """Simulate the behavior of a neuron based on an explanation."""
        ...
class ExplanationNeuronSimulator(NeuronSimulator):
    """
    Simulate neuron behavior based on an explanation.
    This class uses a few-shot prompt with examples of other explanations and activations. This
    prompt allows us to score all of the tokens at once using a nifty trick involving logprobs.
    """
    def __init__(
        self,
        client: ApiClient,
        explanation: str,
        few_shot_example_set: FewShotExampleSet = FewShotExampleSet.ORIGINAL,
        prompt_format: PromptFormat = PromptFormat.CHAT_MESSAGES,
    ):
        self.client = client
        self.explanation = explanation
        self.few_shot_example_set = few_shot_example_set
        self.prompt_format = prompt_format
    async def simulate(
        self,
        tokens: Sequence[str],
    ) -> SequenceSimulation:
        prompt = self.make_simulation_prompt(tokens)
        generate_kwargs: dict[str, Any] = {
            "max_tokens": 0,
            "echo": True,
            "logprobs": 15,
            "timeout": 10,
        }
        # We can't use the CHAT_MESSAGES prompt for scoring, since it only works with the production API endpoint
        # and production no longer returns logprobs. A simulator method which doesn't require logprobs is a WIP.
        assert self.prompt_format != PromptFormat.CHAT_MESSAGES
        assert isinstance(prompt, str)
        generate_kwargs["prompt"] = prompt
        response = await self.client.async_generate(**generate_kwargs)
        logger.debug("response in score_explanation_by_activations is %s", response)
        result = parse_simulation_response(response, self.prompt_format, tokens)
        logger.debug("result in score_explanation_by_activations is %s", result)
        return result
    # TODO(sbills): The current token<tab>activation format can result in improper tokenization.
    # In particular, if the token is itself a tab, we may get a single "\t\t" token rather than two
    # "\t" tokens. Consider using a separator that does not appear in any multi-character tokens.
    def make_simulation_prompt(self, tokens: Sequence[str]) -> str | list[ChatMessage]:
        """Create a few-shot prompt for predicting neuron activations for the given tokens."""
        # TODO(sbills): The prompts in this file are subtly different from the ones in explainer.py.
        # Consider reconciling them.
        prompt_builder = PromptBuilder()
        prompt_builder.add_message(
            Role.SYSTEM,
            """We're studying neurons in a neural network.
Each neuron looks for some particular thing in a short document.
Look at summary of what the neuron does, and try to predict how it will fire on each token.
The activation format is token<tab>activation, activations go from 0 to 10, "unknown" indicates an unknown activation. Most activations will be 0.
""",
        )
        few_shot_examples = self.few_shot_example_set.get_examples()
        for i, example in enumerate(few_shot_examples):
            prompt_builder.add_message(
                Role.USER,
                f"\n\nNeuron {i + 1}\nExplanation of neuron {i + 1} behavior: {EXPLANATION_PREFIX} "
                f"{example.explanation}",
            )
            formatted_activation_records = format_activation_records(
                example.activation_records,
                calculate_max_activation(example.activation_records),
                start_indices=example.first_revealed_activation_indices,
            )
            prompt_builder.add_message(
                Role.ASSISTANT, f"\nActivations: {formatted_activation_records}\n"
            )
        prompt_builder.add_message(
            Role.USER,
            f"\n\nNeuron {len(few_shot_examples) + 1}\nExplanation of neuron "
            f"{len(few_shot_examples) + 1} behavior: {EXPLANATION_PREFIX} "
            f"{self.explanation.strip()}",
        )
        prompt_builder.add_message(
            Role.ASSISTANT, f"\nActivations: {format_sequences_for_simulation([tokens])}"
        )
        return prompt_builder.build(self.prompt_format)
class ExplanationDummySimulator(NeuronSimulator):
    """
    A dummy class, returns all zero activations.
    """
    def __init__(
        self,
        client: ApiClient,
        explanation: str,
        **kwargs: Any,
    ) -> None:
        pass
    async def simulate(
        self,
        tokens: Sequence[str],
    ) -> SequenceSimulation:
        return SequenceSimulation(
            tokens=list(tokens),
            expected_activations=[0.0] * len(tokens),
            activation_scale=ActivationScale.SIMULATED_NORMALIZED_ACTIVATIONS,
            distribution_values=[[] for _ in tokens],
            distribution_probabilities=[[] for _ in tokens],
        )
class ExplanationTokenByTokenSimulator(NeuronSimulator):
    """
    Simulate neuron behavior based on an explanation.
    Unlike ExplanationNeuronSimulator, this class uses one few-shot prompt per token to calculate
    expected activations. This is slower. This class gets a one-token completion and calculates an
    expected value from that token's logprobs.
    """
    def __init__(
        self,
        client: ApiClient,
        explanation: str,
        few_shot_example_set: FewShotExampleSet = FewShotExampleSet.COLANGV2,
        prompt_format: PromptFormat = PromptFormat.INSTRUCTION_FOLLOWING,
    ):
        assert (
            few_shot_example_set != FewShotExampleSet.ORIGINAL
        ), "This simulator doesn't support the ORIGINAL few-shot example set."
        self.client = client
        self.explanation = explanation
        self.few_shot_example_set = few_shot_example_set
        self.prompt_format = prompt_format
    async def simulate(
        self,
        tokens: Sequence[str],
    ) -> SequenceSimulation:
        responses_by_token = await asyncio.gather(
            *[
                self._get_activation_stats_for_single_token(tokens, self.explanation, token_index)
                for token_index in range(len(tokens))
            ]
        )
        expected_values, distribution_values, distribution_probabilities = [], [], []
        for response in responses_by_token:
            activation_logprobs = response["choices"][0]["logprobs"]["top_logprobs"][0]
            (
                norm_probabilities_by_distribution_value,
                expected_value,
            ) = compute_predicted_activation_stats_for_token(
                activation_logprobs,
            )
            distribution_values.append(
                [float(v) for v in norm_probabilities_by_distribution_value.keys()]
            )
            distribution_probabilities.append(
                list(norm_probabilities_by_distribution_value.values())
            )
            expected_values.append(expected_value)
        result = SequenceSimulation(
            activation_scale=ActivationScale.SIMULATED_NORMALIZED_ACTIVATIONS,
            expected_activations=expected_values,
            distribution_values=distribution_values,
            distribution_probabilities=distribution_probabilities,
            tokens=list(tokens),  # SequenceSimulation expects List type
        )
        logger.debug("result in score_explanation_by_activations is %s", result)
        return result
    async def _get_activation_stats_for_single_token(
        self,
        tokens: Sequence[str],
        explanation: str,
        token_index_to_score: int,
    ) -> dict:
        prompt = self.make_single_token_simulation_prompt(
            tokens,
            explanation,
            token_index_to_score=token_index_to_score,
        )
        return await self.client.async_generate(
            prompt=prompt, max_tokens=1, echo=False, logprobs=15
        )
    def _add_single_token_simulation_subprompt(
        self,
        prompt_builder: PromptBuilder,
        activation_record: ActivationRecord,
        neuron_index: int,
        explanation: str,
        token_index_to_score: int,
        end_of_prompt: bool,
    ) -> None:
        trimmed_activation_record = ActivationRecord(
            tokens=activation_record.tokens[: token_index_to_score + 1],
            activations=activation_record.activations[: token_index_to_score + 1],
        )
        prompt_builder.add_message(
            Role.USER,
            f"""
Neuron {neuron_index}
Explanation of neuron {neuron_index} behavior: {EXPLANATION_PREFIX} {explanation.strip()}
Text:
{"".join(trimmed_activation_record.tokens)}
Last token in the text:
{trimmed_activation_record.tokens[-1]}
Last token activation, considering the token in the context in which it appeared in the text:
""",
        )
        if not end_of_prompt:
            normalized_activations = normalize_activations(
                trimmed_activation_record.activations, calculate_max_activation([activation_record])
            )
            prompt_builder.add_message(
                Role.ASSISTANT, str(normalized_activations[-1]) + ("" if end_of_prompt else "\n\n")
            )
    def make_single_token_simulation_prompt(
        self,
        tokens: Sequence[str],
        explanation: str,
        token_index_to_score: int,
    ) -> str | list[ChatMessage]:
        """Make a few-shot prompt for predicting the neuron's activation on a single token."""
        assert explanation != ""
        prompt_builder = PromptBuilder(allow_extra_system_messages=True)
        prompt_builder.add_message(
            Role.SYSTEM,
            """We're studying neurons in a neural network. Each neuron looks for some particular thing in a short document. Look at  an explanation of what the neuron does, and try to predict its activations on a particular token.
The activation format is token<tab>activation, and activations range from 0 to 10. Most activations will be 0.
""",
        )
        few_shot_examples = self.few_shot_example_set.get_examples()
        for i, example in enumerate(few_shot_examples):
            prompt_builder.add_message(
                Role.USER,
                f"Neuron {i + 1}\nExplanation of neuron {i + 1} behavior: {EXPLANATION_PREFIX} "
                f"{example.explanation}\n",
            )
            formatted_activation_records = format_activation_records(
                example.activation_records,
                calculate_max_activation(example.activation_records),
                start_indices=None,
            )
            prompt_builder.add_message(
                Role.ASSISTANT,
                f"Activations: {formatted_activation_records}\n\n",
            )
        prompt_builder.add_message(
            Role.SYSTEM,
            "Now, we're going predict the activation of a new neuron on a single token, "
            "following the same rules as the examples above. Activations still range from 0 to 10.",
        )
        single_token_example = self.few_shot_example_set.get_single_token_prediction_example()
        assert single_token_example.token_index_to_score is not None
        self._add_single_token_simulation_subprompt(
            prompt_builder,
            single_token_example.activation_records[0],
            len(few_shot_examples) + 1,
            explanation,
            token_index_to_score=single_token_example.token_index_to_score,
            end_of_prompt=False,
        )
        activation_record = ActivationRecord(
            tokens=list(tokens[: token_index_to_score + 1]),  # ActivationRecord expects List type.
            activations=[0.0] * len(tokens),
        )
        self._add_single_token_simulation_subprompt(
            prompt_builder,
            activation_record,
            len(few_shot_examples) + 2,
            explanation,
            token_index_to_score,
            end_of_prompt=True,
        )
        return prompt_builder.build(self.prompt_format)
def _parse_no_logprobs_completion_json(
    completion: str,
    tokens: Sequence[str],
) -> list[float]:
    """
    Parse a completion into a list of simulated activations. If the model did not faithfully
    reproduce the token sequence, return a list of 0s. If the model's activation for a token
    is not a number between 0 and 10 (inclusive), substitute 0.
    Args:
        completion: completion from the API
        tokens: list of tokens as strings in the sequence where the neuron is being simulated
    """
    zero_prediction: list[float] = [0.0] * len(tokens)
    try:
        completion_json: dict = json.loads(completion)
        if "activations" not in completion_json:
            logger.error(
                "The key 'activations' is not in the logprob free simulator response. Not a severe error, throw rate depends on how well the model can reproduce a particular JSON format."
            )
            return zero_prediction
        activations = completion_json["activations"]
        if len(activations) != len(tokens):
            return zero_prediction
        predicted_activations: list[float] = []
        # check that there is a token and activation value
        # no need to double check the token matches exactly
        for activation in activations:
            if "token" not in activation:
                predicted_activations.append(0)
                continue
            if "activation" not in activation:
                predicted_activations.append(0)
                continue
            # Ensure activation value is between 0-10 inclusive
            try:
                predicted_activation_float = float(activation["activation"])
                if (
                    predicted_activation_float < 0
                    or predicted_activation_float > MAX_NORMALIZED_ACTIVATION
                ):
                    predicted_activations.append(0.0)
                else:
                    predicted_activations.append(predicted_activation_float)
            except ValueError:
                predicted_activations.append(0)
            except TypeError:
                predicted_activations.append(0)
        logger.debug("predicted activations: %s", predicted_activations)
        return predicted_activations
    except json.JSONDecodeError:
        logger.error(
            "Error: the logprob free simulator response is not valid JSON. Not a severe error, throw rate depends on the model's ability to produce JSON."
        )
        return zero_prediction
def _format_record_for_logprob_free_simulation_json(
    explanation: str,
    activation_record: ActivationRecord,
    include_activations: bool = False,
) -> str:
    if include_activations:
        assert len(activation_record.tokens) == len(
            activation_record.activations
        ), f"{len(activation_record.tokens)=}, {len(activation_record.activations)=}"
    return json.dumps(
        {
            "to_find": explanation,
            "document": "".join(activation_record.tokens),
            "activations": [
                {
                    "token": token,
                    "activation": activation_record.activations[i] if include_activations else None,
                }
                for i, token in enumerate(activation_record.tokens)
            ],
        }
    )
class LogprobFreeExplanationTokenSimulator(NeuronSimulator):
    """
    Simulate neuron behavior based on an explanation.
    Unlike ExplanationNeuronSimulator and ExplanationTokenByTokenSimulator, this class does not rely on
    logprobs to calculate expected activations. Instead, it uses a few-shot prompt that displays all of the
    tokens at once, and requests that the model repeat the tokens with the activations appended. Sampling
    is with temperature = 0. Thus, the activations are deterministic. Also, each activation for a token
    is a function of all the activations that came previously and all of the tokens in the sequence, not
    just the current and previous tokens. In the case where the model does not faithfully reproduce the
    token sequence, the simulator will return a response where every predicted activation is 0.
    The tokens and activations in the prompt are formatted as a JSON object, which empirically improves
    the likelihood that the model will faithfully reproduce the token sequence.
    """
    def __init__(
        self,
        client: ApiClient,
        explanation: str,
        few_shot_example_set: FewShotExampleSet = FewShotExampleSet.COLANGV2,
        prompt_format: PromptFormat = PromptFormat.CHAT_MESSAGES,
    ):
        assert (
            few_shot_example_set != FewShotExampleSet.ORIGINAL
        ), "This simulator doesn't support the ORIGINAL few-shot example set."
        assert (
            prompt_format == PromptFormat.CHAT_MESSAGES
        ), "This simulator only supports the CHAT_MESSAGES prompt format."
        self.client = client
        self.explanation = explanation
        self.few_shot_example_set = few_shot_example_set
        self.prompt_format = prompt_format
    async def simulate(
        self,
        tokens: Sequence[str],
    ) -> SequenceSimulation:
        cleaned_tokens = []
        # Sanitize the token list to increase the chance that the model will faithfully reproduce it.
        for token in tokens:
            cleaned_tokens.append(
                token.replace("<|endoftext|>", "<|not_endoftext|>")
                .encode("ascii", errors="backslashreplace")
                .decode("ascii")
            )
        prompt = self.make_simulation_prompt(
            tokens,
            self.explanation,
        )
        response = await self.client.async_generate(messages=prompt, max_tokens=2000, temperature=0)
        assert len(response["choices"]) == 1
        choice = response["choices"][0]
        completion = choice["message"]["content"]
        predicted_activations = _parse_no_logprobs_completion_json(completion, cleaned_tokens)
        result = SequenceSimulation(
            activation_scale=ActivationScale.SIMULATED_NORMALIZED_ACTIVATIONS,
            expected_activations=predicted_activations,
            # Since the predicted activation is just a sampled token, we don't have a distribution.
            distribution_values=[],
            distribution_probabilities=[],
            tokens=list(tokens),  # SequenceSimulation expects List type
        )
        return result
    def make_simulation_prompt(
        self,
        tokens: Sequence[str],
        explanation: str,
    ) -> str | list[ChatMessage]:
        """Make a few-shot prompt for predicting the neuron's activations on a sequence.
        This prompt only gives the model one sequence per neuron in the few shot examples."""
        assert explanation != ""
        prompt_builder = PromptBuilder(allow_extra_system_messages=True)
        prompt_builder.add_message(
            Role.SYSTEM,
            """We're studying neurons in a neural network. Each neuron looks for certain things in a short document. Your task is to read the explanation of what the neuron does, and predict the neuron's activations for each token in the document.
For each document, you will see the full text of the document, then the tokens in the document with the activation left blank. You will print, in valid json, the exact same tokens verbatim, but with the activation values filled in according to the explanation. Pay special attention to the explanation's description of the context and order of tokens or words.
Fill out the activation values from 0 to 10. Please think carefully.";
""",
        )
        few_shot_examples = self.few_shot_example_set.get_examples()
        for example in few_shot_examples:
            prompt_builder.add_message(
                Role.USER,
                _format_record_for_logprob_free_simulation_json(
                    explanation=example.explanation,
                    activation_record=example.activation_records[0],
                    include_activations=False,
                ),
            )
            # Example of this few shot user message.
            """
            {
                "to_find": "hello",
                "document": "The",
                "activations": [
                    {
                        "token": "The",
                        "activation": null
                    },
                    ...
                ]
            }
            """
            prompt_builder.add_message(
                Role.ASSISTANT,
                _format_record_for_logprob_free_simulation_json(
                    explanation=example.explanation,
                    activation_record=example.activation_records[0],
                    include_activations=True,
                ),
            )
            # Example of this few shot assistant message:
            """
            {
                "to_find": "hello",
                "document": "The",
                "activations": [
                    {
                        "token": "The",
                        "activation": 10
                    },
                    ...
                ]
            }
            """
        prompt_builder.add_message(
            Role.USER,
            _format_record_for_logprob_free_simulation_json(
                explanation=explanation,
                activation_record=ActivationRecord(tokens=list(tokens), activations=[]),
                include_activations=False,
            ),
        )
        # Example of the final user message:
        """
        {
            "to_find": "hello",
            "document": "The",
            "activations": [
                {
                    "token": "The",
                    "activation": null
                },
                ...
            ]
        }
        """
        return prompt_builder.build(self.prompt_format)
if __name__ == "__main__":
    from neuron_explainer.activations.activations import load_neuron
    neuron = load_neuron(
        "https://openaipublic.blob.core.windows.net/neuron-explainer/data/collated-activations/",
        "21",
        "2932",
    )
    client = ApiClient(model_name="gpt-4o", max_concurrent=5)
    simulator = LogprobFreeExplanationTokenSimulator(
        client=client, explanation="Canada or things related to Canada"
    )
    result = asyncio.run(simulator.simulate(neuron.most_positive_activation_records[0].tokens))
    for token, real, activation in zip(
        result.tokens,
        neuron.most_positive_activation_records[0].activations,
        result.expected_activations,
    ):
        print(str(token), real, activation)

================
File: build/lib/neuron_explainer/explanations/test_explainer.py
================
import asyncio
from typing import Any
from neuron_explainer.explanations.explainer import TokenActivationPairExplainer
from neuron_explainer.explanations.few_shot_examples import TEST_EXAMPLES, FewShotExampleSet
from neuron_explainer.explanations.prompt_builder import ChatMessage, PromptFormat, Role
def setup_module(unused_module: Any) -> None:
    # Make sure we have an event loop, since the attempt to create the Semaphore in
    # ApiClient will fail without it.
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
def test_if_formatting() -> None:
    expected_prompt = """We're studying neurons in a neural network. Each neuron looks for some particular thing in a short document. Look at the parts of the document the neuron activates for and summarize in a single sentence what the neuron is looking for. Don't list examples of words.
The activation format is token<tab>activation. Activation values range from 0 to 10. A neuron finding what it's looking for is represented by a non-zero activation value. The higher the activation value, the stronger the match.
Neuron 1
Activations:
<start>
a	10
b	0
c	0
<end>
<start>
d	0
e	10
f	0
<end>
Explanation of neuron 1 behavior: this neuron activates for vowels.
Neuron 2
Activations:
<start>
a	10
b	0
c	0
<end>
<start>
d	0
e	10
f	0
<end>
Explanation of neuron 2 behavior:<|endofprompt|> this neuron activates for"""
    explainer = TokenActivationPairExplainer(
        model_name="gpt-4o",
        prompt_format=PromptFormat.INSTRUCTION_FOLLOWING,
        few_shot_example_set=FewShotExampleSet.TEST,
    )
    prompt = explainer.make_explanation_prompt(
        all_activations=TEST_EXAMPLES[0].activation_records,
        max_activation=1.0,
        max_tokens_for_completion=20,
    )
    assert prompt == expected_prompt
def test_chat_format() -> None:
    expected_prompt = [
        ChatMessage(
            role=Role.SYSTEM,
            content="""We're studying neurons in a neural network. Each neuron looks for some particular thing in a short document. Look at the parts of the document the neuron activates for and summarize in a single sentence what the neuron is looking for. Don't list examples of words.
The activation format is token<tab>activation. Activation values range from 0 to 10. A neuron finding what it's looking for is represented by a non-zero activation value. The higher the activation value, the stronger the match.""",
        ),
        ChatMessage(
            role=Role.USER,
            content="""
Neuron 1
Activations:
<start>
a	10
b	0
c	0
<end>
<start>
d	0
e	10
f	0
<end>
Explanation of neuron 1 behavior: this neuron activates for""",
        ),
        ChatMessage(
            role=Role.ASSISTANT,
            content=" vowels.",
        ),
        ChatMessage(
            role=Role.USER,
            content="""
Neuron 2
Activations:
<start>
a	10
b	0
c	0
<end>
<start>
d	0
e	10
f	0
<end>
Explanation of neuron 2 behavior: this neuron activates for""",
        ),
    ]
    explainer = TokenActivationPairExplainer(
        model_name="gpt-4o",
        prompt_format=PromptFormat.CHAT_MESSAGES,
        few_shot_example_set=FewShotExampleSet.TEST,
    )
    prompt = explainer.make_explanation_prompt(
        all_activations=TEST_EXAMPLES[0].activation_records,
        max_activation=1.0,
        max_tokens_for_completion=20,
    )
    assert isinstance(prompt, list)
    assert isinstance(prompt[0], dict)  # Really a ChatMessage
    for actual_message, expected_message in zip(prompt, expected_prompt):
        assert actual_message["role"] == expected_message["role"]
        assert actual_message["content"] == expected_message["content"]
    assert prompt == expected_prompt

================
File: build/lib/neuron_explainer/explanations/test_simulator.py
================
# ruff: noqa: W291
from neuron_explainer.explanations.few_shot_examples import FewShotExampleSet
from neuron_explainer.explanations.prompt_builder import ChatMessage, PromptFormat, Role
from neuron_explainer.explanations.simulator import (
    ExplanationNeuronSimulator,
    ExplanationTokenByTokenSimulator,
)
def test_make_explanation_simulation_prompt_if_format() -> None:
    expected_prompt = """We're studying neurons in a neural network.
Each neuron looks for some particular thing in a short document.
Look at summary of what the neuron does, and try to predict how it will fire on each token.
The activation format is token<tab>activation, activations go from 0 to 10, "unknown" indicates an unknown activation. Most activations will be 0.
Neuron 1
Explanation of neuron 1 behavior: this neuron activates for vowels
Activations: 
<start>
a	10
b	0
c	0
<end>
<start>
d	unknown
e	10
f	0
<end>
Neuron 2
Explanation of neuron 2 behavior: this neuron activates for EXPLANATION<|endofprompt|>
Activations: 
<start>
0	unknown
1	unknown
2	unknown
<end>
"""
    prompt = ExplanationNeuronSimulator(
        client=None,  # type: ignore
        explanation="EXPLANATION",
        few_shot_example_set=FewShotExampleSet.TEST,
        prompt_format=PromptFormat.INSTRUCTION_FOLLOWING,
    ).make_simulation_prompt(
        tokens=[str(x) for x in range(3)],
    )
    assert prompt == expected_prompt
def test_make_explanation_simulation_prompt_chat_format() -> None:
    expected_prompt = [
        ChatMessage(
            role=Role.SYSTEM,
            content="""We're studying neurons in a neural network.
Each neuron looks for some particular thing in a short document.
Look at summary of what the neuron does, and try to predict how it will fire on each token.
The activation format is token<tab>activation, activations go from 0 to 10, "unknown" indicates an unknown activation. Most activations will be 0.
""",
        ),
        ChatMessage(
            role=Role.USER,
            content="""
Neuron 1
Explanation of neuron 1 behavior: this neuron activates for vowels""",
        ),
        ChatMessage(
            role=Role.ASSISTANT,
            content="""
Activations: 
<start>
a	10
b	0
c	0
<end>
<start>
d	unknown
e	10
f	0
<end>
""",
        ),
        ChatMessage(
            role=Role.USER,
            content="""
Neuron 2
Explanation of neuron 2 behavior: this neuron activates for EXPLANATION""",
        ),
        ChatMessage(
            role=Role.ASSISTANT,
            content="""
Activations: 
<start>
0	unknown
1	unknown
2	unknown
<end>
""",
        ),
    ]
    prompt = ExplanationNeuronSimulator(
        client=None,  # type: ignore
        explanation="EXPLANATION",
        few_shot_example_set=FewShotExampleSet.TEST,
        prompt_format=PromptFormat.CHAT_MESSAGES,
    ).make_simulation_prompt(
        tokens=[str(x) for x in range(3)],
    )
    assert isinstance(prompt, list)
    assert isinstance(prompt[0], dict)  # Really a ChatMessage
    for actual_message, expected_message in zip(prompt, expected_prompt):
        assert actual_message["role"] == expected_message["role"]
        assert actual_message["content"] == expected_message["content"]
    assert prompt == expected_prompt
def test_make_token_by_token_simulation_prompt_if_format() -> None:
    expected_prompt = """We're studying neurons in a neural network. Each neuron looks for some particular thing in a short document. Look at  an explanation of what the neuron does, and try to predict its activations on a particular token.
The activation format is token<tab>activation, and activations range from 0 to 10. Most activations will be 0.
Neuron 1
Explanation of neuron 1 behavior: this neuron activates for vowels
Activations: 
<start>
a	10
b	0
c	0
<end>
<start>
d	0
e	10
f	0
<end>
Now, we're going predict the activation of a new neuron on a single token, following the same rules as the examples above. Activations still range from 0 to 10.
Neuron 2
Explanation of neuron 2 behavior: this neuron activates for numbers and nothing else
Text:
ghi
Last token in the text:
i
Last token activation, considering the token in the context in which it appeared in the text:
10
Neuron 3
Explanation of neuron 3 behavior: this neuron activates for numbers and nothing else
Text:
01
Last token in the text:
1
Last token activation, considering the token in the context in which it appeared in the text:
<|endofprompt|>"""
    prompt = ExplanationTokenByTokenSimulator(
        client=None,  # type: ignore
        explanation="EXPLANATION",
        few_shot_example_set=FewShotExampleSet.TEST,
        prompt_format=PromptFormat.INSTRUCTION_FOLLOWING,
    ).make_single_token_simulation_prompt(
        tokens=[str(x) for x in range(3)],
        explanation="numbers and nothing else",
        token_index_to_score=1,
    )
    assert prompt == expected_prompt
def test_make_token_by_token_simulation_prompt_chat_format() -> None:
    expected_prompt = [
        ChatMessage(
            role=Role.SYSTEM,
            content="""We're studying neurons in a neural network. Each neuron looks for some particular thing in a short document. Look at  an explanation of what the neuron does, and try to predict its activations on a particular token.
The activation format is token<tab>activation, and activations range from 0 to 10. Most activations will be 0.
""",
        ),
        ChatMessage(
            role=Role.USER,
            content="""Neuron 1
Explanation of neuron 1 behavior: this neuron activates for vowels
""",
        ),
        ChatMessage(
            role=Role.ASSISTANT,
            content="""Activations: 
<start>
a	10
b	0
c	0
<end>
<start>
d	0
e	10
f	0
<end>
""",
        ),
        ChatMessage(
            role=Role.SYSTEM,
            content="Now, we're going predict the activation of a new neuron on a single token, following the same rules as the examples above. Activations still range from 0 to 10.",
        ),
        ChatMessage(
            role=Role.USER,
            content="""
Neuron 2
Explanation of neuron 2 behavior: this neuron activates for numbers and nothing else
Text:
ghi
Last token in the text:
i
Last token activation, considering the token in the context in which it appeared in the text:
""",
        ),
        ChatMessage(
            role=Role.ASSISTANT,
            content="""10
""",
        ),
        ChatMessage(
            role=Role.USER,
            content="""
Neuron 3
Explanation of neuron 3 behavior: this neuron activates for numbers and nothing else
Text:
01
Last token in the text:
1
Last token activation, considering the token in the context in which it appeared in the text:
""",
        ),
    ]
    prompt = ExplanationTokenByTokenSimulator(
        client=None,  # type: ignore
        explanation="EXPLANATION",
        few_shot_example_set=FewShotExampleSet.TEST,
        prompt_format=PromptFormat.CHAT_MESSAGES,
    ).make_single_token_simulation_prompt(
        tokens=[str(x) for x in range(3)],
        explanation="numbers and nothing else",
        token_index_to_score=1,
    )
    assert isinstance(prompt, list)
    assert isinstance(prompt[0], dict)  # Really a ChatMessage
    for actual_message, expected_message in zip(prompt, expected_prompt):
        assert actual_message["role"] == expected_message["role"]
        assert actual_message["content"] == expected_message["content"]
    assert prompt == expected_prompt

================
File: build/lib/neuron_explainer/fast_dataclasses/__init__.py
================
from .fast_dataclasses import FastDataclass, dumps, loads, register_dataclass
__all__ = ["FastDataclass", "dumps", "loads", "register_dataclass"]

================
File: build/lib/neuron_explainer/fast_dataclasses/fast_dataclasses.py
================
# Utilities for dataclasses that are very fast to serialize and deserialize, with limited data
# validation. Fields must not be tuples, since they get serialized and then deserialized as lists.
#
# The unit tests for this library show how to use it.
import json
from dataclasses import dataclass, field, fields, is_dataclass
from functools import partial
from typing import Any
import orjson
dataclasses_by_name = {}
# TODO(williamrs): remove deserialization using fieldnames once all data is in the new format
dataclasses_by_fieldnames = {}
@dataclass
class FastDataclass:
    dataclass_name: str = field(init=False)
    def __post_init__(self) -> None:
        self.dataclass_name = self.__class__.__name__
    @classmethod
    def field_renamed(cls, old_name: str, new_name: str) -> None:
        if not hasattr(cls, "field_renames"):
            cls.field_renames = {}  # type: ignore
        assert old_name not in cls.field_renames, f"{old_name} already renamed"  # type: ignore
        existing_field_names = [f.name for f in fields(cls)]
        assert new_name in existing_field_names, f"{new_name} not in {existing_field_names}"
        assert old_name not in existing_field_names, f"{old_name} still in {existing_field_names}"
        cls.field_renames[old_name] = new_name  # type: ignore
    # Type checking is suppressed in these two functions because mypy doesn't like that we're
    # setting attributes at runtime. We need to do that because if we defined the attributes at the
    # class level, they'd be shared across subclasses.
    @classmethod
    def field_deleted(cls, field_name: str) -> None:
        if not hasattr(cls, "deleted_fields"):
            cls.deleted_fields = []  # type: ignore
        assert field_name not in cls.deleted_fields, f"{field_name} already deleted"  # type: ignore
        existing_field_names = [f.name for f in fields(cls)]
        assert field_name not in existing_field_names, f"{field_name} still in use"
        cls.deleted_fields.append(field_name)  # type: ignore
    @classmethod
    def was_previously_named(cls, old_name: str) -> None:
        assert old_name not in dataclasses_by_name, f"{old_name} still in use as a dataclass name"
        dataclasses_by_name[old_name] = cls
        name_set = frozenset(f.name for f in fields(cls) if f.name != "dataclass_name")
        dataclasses_by_fieldnames[name_set] = cls
def register_dataclass(cls):  # type: ignore
    assert is_dataclass(cls), "Only dataclasses can be registered."
    dataclasses_by_name[cls.__name__] = cls
    name_set = frozenset(f.name for f in fields(cls) if f.name != "dataclass_name")
    dataclasses_by_fieldnames[name_set] = cls
    return cls
def dumps(obj: Any) -> bytes:
    return orjson.dumps(obj, option=orjson.OPT_SERIALIZE_NUMPY)
def _object_hook(d: Any, backwards_compatible: bool = True) -> Any:
    # If d is a list, recurse.
    if isinstance(d, list):
        return [_object_hook(x, backwards_compatible=backwards_compatible) for x in d]
    # If d is not a dict, return it as is.
    if not isinstance(d, dict):
        return d
    cls = None
    if "dataclass_name" in d:
        if d["dataclass_name"] in dataclasses_by_name:
            cls = dataclasses_by_name[d["dataclass_name"]]
        else:
            assert backwards_compatible, (
                f"Dataclass {d['dataclass_name']} not found, set backwards_compatible=True if you "
                f"are okay with that."
            )
    # Load objects created without dataclass_name set.
    else:
        # Try our best to find a dataclass if backwards_compatible is True.
        if backwards_compatible:
            d_fields = frozenset(d.keys())
            if d_fields in dataclasses_by_fieldnames:
                cls = dataclasses_by_fieldnames[d_fields]
            elif len(d_fields) > 0:
                # Check if the fields are a subset of a dataclass (if the dataclass had extra fields
                # added since the data was created). Note that this will fail if fields were removed
                # from the dataclass.
                for key, possible_cls in dataclasses_by_fieldnames.items():
                    if d_fields.issubset(key):
                        cls = possible_cls
                        break
                else:
                    print(f"Could not find dataclass for {d_fields} {cls}")
    if cls is not None:
        # Apply renames and deletions.
        if hasattr(cls, "field_renames"):
            d = {cls.field_renames.get(k, k): v for k, v in d.items()}
        if hasattr(cls, "deleted_fields"):
            d = {k: v for k, v in d.items() if k not in cls.deleted_fields}
    new_d = {
        k: _object_hook(v, backwards_compatible=backwards_compatible)
        for k, v in d.items()
        if k != "dataclass_name"
    }
    if cls is not None:
        return cls(**new_d)
    else:
        return new_d
def loads(s: str | bytes, backwards_compatible: bool = True) -> Any:
    return json.loads(
        s,
        object_hook=partial(_object_hook, backwards_compatible=backwards_compatible),
    )

================
File: build/lib/neuron_explainer/fast_dataclasses/test_fast_dataclasses.py
================
import json
from dataclasses import dataclass
import pytest
from .fast_dataclasses import FastDataclass, dumps, loads, register_dataclass
# Inheritance is a bit tricky with our setup. dataclass_name must be set for instances of these
# classes to serialize and deserialize correctly, but if it's given a default value, then subclasses
# can't have any fields that don't have default values, because of how constructors are generated
# for dataclasses (fields with no default value can't follow those with default values). To work
# around this, we set dataclass_name in __post_init__ on the base class, which is called after the
# constructor. The implementation does the right thing for both the base class and the subclass.
@register_dataclass
@dataclass
class DataclassC(FastDataclass):
    ints: list[int]
@register_dataclass
@dataclass
class DataclassC_ext(DataclassC):
    s: str
@register_dataclass
@dataclass
class DataclassB(FastDataclass):
    str_to_c: dict[str, DataclassC]
    cs: list[DataclassC]
@register_dataclass
@dataclass
class DataclassA(FastDataclass):
    floats: list[float]
    strings: list[str]
    bs: list[DataclassB]
@register_dataclass
@dataclass
class DataclassD(FastDataclass):
    s1: str
    s2: str = "default"
def test_dataclasses() -> None:
    a = DataclassA(
        floats=[1.0, 2.0],
        strings=["a", "b"],
        bs=[
            DataclassB(
                str_to_c={"a": DataclassC(ints=[1, 2]), "b": DataclassC(ints=[3, 4])},
                cs=[DataclassC(ints=[5, 6]), DataclassC_ext(ints=[7, 8], s="s")],
            ),
            DataclassB(
                str_to_c={"c": DataclassC_ext(ints=[9, 10], s="t"), "d": DataclassC(ints=[11, 12])},
                cs=[DataclassC(ints=[13, 14]), DataclassC(ints=[15, 16])],
            ),
        ],
    )
    assert loads(dumps(a)) == a
def test_c_and_c_ext() -> None:
    c_ext = DataclassC_ext(ints=[3, 4], s="s")
    assert loads(dumps(c_ext)) == c_ext
    c = DataclassC(ints=[1, 2])
    assert loads(dumps(c)) == c
def test_bad_serialized_data() -> None:
    assert type(loads(dumps(DataclassC(ints=[3, 4])))) == DataclassC
    assert type(loads('{"ints": [3, 4]}', backwards_compatible=False)) == dict
    assert type(loads('{"ints": [3, 4], "dataclass_name": "DataclassC"}')) == DataclassC
    with pytest.raises(TypeError):
        loads('{"ints": [3, 4], "bogus_extra_field": "foo", "dataclass_name": "DataclassC"}')
    with pytest.raises(TypeError):
        loads('{"ints_field_is_missing": [3, 4], "dataclass_name": "DataclassC"}')
    assert type(loads('{"s1": "test"}', backwards_compatible=False)) == dict
    assert type(loads('{"s1": "test"}', backwards_compatible=True)) == DataclassD
def test_renames_and_deletes() -> None:
    @register_dataclass
    @dataclass
    class FooV1(FastDataclass):
        a: int
        b: str
        c: float
    # Version with c renamed.
    @register_dataclass
    @dataclass
    class FooV2(FastDataclass):
        a: int
        b: str
        c_renamed: float
    # Version with c renamed and b deleted.
    @register_dataclass
    @dataclass
    class FooV3(FastDataclass):
        a: int
        c_renamed: float
    # Basic sanity checks.
    foo_v1 = FooV1(a=1, b="hello", c=3.14)
    foo_v1_bytes = dumps(foo_v1)
    assert loads(foo_v1_bytes) == foo_v1
    # Deserializing a FooV1 as a FooV2 should fail.
    foo_v1_dict = json.loads(foo_v1_bytes)
    # Change the dataclass_name so this will be interpreted as a FooV2.
    foo_v1_dict["dataclass_name"] = "FooV2"
    foo_v1_as_v2_bytes = dumps(foo_v1_dict)
    with pytest.raises(TypeError):
        loads(foo_v1_as_v2_bytes)
    # Register the field's rename, then try again. This time it should succeed.
    FooV2.field_renamed("c", "c_renamed")
    foo_v1_as_v2 = loads(foo_v1_as_v2_bytes)
    assert type(foo_v1_as_v2) == FooV2
    assert foo_v1_as_v2.a == foo_v1.a
    assert foo_v1_as_v2.b == foo_v1.b
    assert foo_v1_as_v2.c_renamed == foo_v1.c
    # Deserializing a FooV1 as a FooV3 should fail, even with the rename.
    FooV3.field_renamed("c", "c_renamed")
    # Change the dataclass_name so this will be interpreted as a FooV3.
    foo_v1_dict["dataclass_name"] = "FooV3"
    foo_v1_as_v3_bytes = dumps(foo_v1_dict)
    with pytest.raises(TypeError):
        loads(foo_v1_as_v3_bytes)
    # Register the field's deletion, then try again. This time it should succeed.
    FooV3.field_deleted("b")
    foo_v1_as_v3 = loads(foo_v1_as_v3_bytes)
    assert type(foo_v1_as_v3) == FooV3
    assert foo_v1_as_v3.a == foo_v1.a
    assert foo_v1_as_v3.c_renamed == foo_v1.c
def test_class_rename() -> None:
    @register_dataclass
    @dataclass
    class BarRenamed(FastDataclass):
        d: str
        e: int
    BarRenamed.was_previously_named("Bar")
    # We should be able to deserialize an old Bar as a BarRenamed.
    bar = BarRenamed(d="hello", e=1)
    bar_bytes = dumps(bar)
    bar_dict = json.loads(bar_bytes)
    # Change the dataclass_name so this looks like an old Bar.
    bar_dict["dataclass_name"] = "Bar"
    bar_as_bar_bytes = dumps(bar_dict)
    bar_as_bar_renamed = loads(bar_as_bar_bytes)
    assert type(bar_as_bar_renamed) == BarRenamed
    assert bar_as_bar_renamed.d == bar.d
    assert bar_as_bar_renamed.e == bar.e
def test_invalid_renames_and_deletions() -> None:
    @register_dataclass
    @dataclass
    class OldBaz(FastDataclass):
        a: int
        b: str
        c: float
    @register_dataclass
    @dataclass
    class Baz(FastDataclass):
        a: int
        b: str
        c: float
    # Renaming a field should fail if the new name doesn't exist.
    with pytest.raises(AssertionError):
        Baz.field_renamed("d", "d_renamed")
    # Renaming a field should fail if the old name is still in use.
    with pytest.raises(AssertionError):
        Baz.field_renamed("a", "b")
    # Deleting a field should fail if the name is still in use.
    with pytest.raises(AssertionError):
        Baz.field_deleted("a")
    # Renaming a dataclass should fail if the old name is still in use.
    with pytest.raises(AssertionError):
        Baz.was_previously_named("OldBaz")
    # Renaming the same field twice should fail.
    Baz.field_renamed("old_c", "c")
    with pytest.raises(AssertionError):
        Baz.field_renamed("old_c", "b")
    # Deleting a field twice should fail.
    Baz.field_deleted("z")
    with pytest.raises(AssertionError):
        Baz.field_deleted("z")

================
File: build/lib/neuron_explainer/file_utils.py
================
import io
import os
import urllib.request
from io import IOBase
import aiohttp
def file_exists(filepath: str) -> bool:
    if filepath.startswith("https://"):
        try:
            urllib.request.urlopen(filepath)
            return True
        except urllib.error.HTTPError:
            return False
    else:
        # It's a local file.
        return os.path.exists(filepath)
class CustomFileHandler:
    def __init__(self, filepath: str, mode: str) -> None:
        self.filepath = filepath
        self.mode = mode
        self.file = None
    def __enter__(self) -> IOBase:
        assert not self.filepath.startswith("az://"), "Azure blob storage is not supported"
        if self.filepath.startswith("https://"):
            assert self.mode in ["r", "rb"], "Only read mode is supported for remote files"
            remote_data = urllib.request.urlopen(self.filepath)
            if "b" in self.mode:
                # Read the content into a BytesIO object for binary mode
                self.file = io.BytesIO(remote_data.read())
            else:
                # Decode the content and use StringIO for text mode (less common for torch.load)
                self.file = io.StringIO(remote_data.read().decode())
        else:
            # Create the subdirectories if they don't exist
            directory = os.path.dirname(self.filepath)
            os.makedirs(directory, exist_ok=True)
            self.file = open(self.filepath, self.mode)
            if "b" in self.mode:
                # Ensure the file is seekable; if not, read into a BytesIO object
                try:
                    self.file.seek(0)
                except io.UnsupportedOperation:
                    self.file.close()
                    with open(self.filepath, self.mode) as f:
                        self.file = io.BytesIO(f.read())
        return self.file
    def __exit__(self, exc_type, exc_val, exc_tb) -> bool:
        # Close the file if it's open
        if self.file is not None:
            self.file.close()
        # Propagate exceptions
        return False
async def read_single_async(filepath: str) -> bytes:
    if filepath.startswith("https://"):
        async with aiohttp.ClientSession() as session:
            async with session.get(filepath) as response:
                return await response.read()
    else:
        with open(filepath, "rb") as f:
            return f.read()
def copy_to_local_cache(src: str, dst: str) -> None:
    if not os.path.exists(os.path.dirname(dst)):
        os.makedirs(os.path.dirname(dst), exist_ok=True)
    if src.startswith("https://"):
        with urllib.request.urlopen(src) as response, open(dst, "wb") as out_file:
            data = response.read()  # Consider chunked reading for large files
            out_file.write(data)
    else:
        with open(src, "rb") as in_file, open(dst, "wb") as out_file:
            data = in_file.read()
            out_file.write(data)

================
File: build/lib/neuron_explainer/models/__init__.py
================
from .autoencoder import Autoencoder
from .hooks import Hooks, TransformerHooks
from .transformer import Transformer, TransformerConfig

================
File: build/lib/neuron_explainer/models/autoencoder_context.py
================
import os
from dataclasses import dataclass, field
from typing import Union
import torch
from neuron_explainer.activations.derived_scalars.derived_scalar_types import DerivedScalarType
from neuron_explainer.file_utils import copy_to_local_cache, file_exists
from neuron_explainer.models import Autoencoder
from neuron_explainer.models.model_component_registry import Dimension, LayerIndex, NodeType
@dataclass(frozen=True)
class AutoencoderSpec:
    """Parameters used in the construction of an AutoencoderConfig object. Seperate so we don't need to validate when constructed"""
    dst: DerivedScalarType
    autoencoder_path_by_layer_index: dict[LayerIndex, str]
@dataclass(frozen=True)
class AutoencoderConfig:
    """
    This class specifies a set of autoencoders to load from disk, for one or more layer indices.
    The activation location type indicates the type of activation that the autoencoder was trained
    on, and that will be fed into the autoencoder.
    """
    dst: DerivedScalarType
    autoencoder_path_by_layer_index: dict[LayerIndex, str]
    def __post_init__(self) -> None:
        assert len(self.autoencoder_path_by_layer_index) > 0
        if len(self.autoencoder_path_by_layer_index) > 1:
            assert (
                None not in self.autoencoder_path_by_layer_index.keys()
            ), "layer_indices must be [None], or a list of int layer indices"
    @classmethod
    def from_spec(cls, params: AutoencoderSpec) -> "AutoencoderConfig":
        return cls(
            dst=params.dst,
            autoencoder_path_by_layer_index=params.autoencoder_path_by_layer_index,
        )
@dataclass(frozen=True)
class AutoencoderContext:
    autoencoder_config: AutoencoderConfig
    device: torch.device
    _cached_autoencoders_by_path: dict[str, Autoencoder] = field(default_factory=dict)
    omit_dead_latents: bool = False
    """
    Omit dead latents to save memory. Only happens if self.warmup() is called. Because we omit the
    same number of latents from all autoencoders, we can only omit the smallest number of dead
    latents among all autoencoders.
    """
    @property
    def num_autoencoder_directions(self) -> int:
        """Note that this property might change after warmup() is called, if omit_dead_latents is True."""
        if len(self._cached_autoencoders_by_path) == 0:
            raise ValueError(
                "num_autoencoder_directions is not populated yet. Call warmup() first."
            )
        else:
            # all autoencoders have the same number of directions, so we can just check one
            first_autoencoder = next(iter(self._cached_autoencoders_by_path.values()))
            return first_autoencoder.latent_bias.shape[0]
    @property
    def _min_n_dead_latents(self) -> int:
        return min(
            count_dead_latents(autoencoder)
            for autoencoder in self._cached_autoencoders_by_path.values()
        )
    @property
    def dst(self) -> DerivedScalarType:
        return self.autoencoder_config.dst
    @property
    def layer_indices(self) -> set[LayerIndex]:
        return set(self.autoencoder_config.autoencoder_path_by_layer_index.keys())
    def get_autoencoder(self, layer_index: LayerIndex) -> Autoencoder:
        autoencoder_azure_path = self.autoencoder_config.autoencoder_path_by_layer_index.get(
            layer_index
        )
        if autoencoder_azure_path is None:
            raise ValueError(f"No autoencoder path for layer_index {layer_index}")
        else:
            if autoencoder_azure_path in self._cached_autoencoders_by_path:
                autoencoder = self._cached_autoencoders_by_path[autoencoder_azure_path]
            else:
                # Check if the autoencoder is cached on disk
                disk_cache_path = os.path.join(
                    "/tmp", autoencoder_azure_path.replace("https://", "")
                )
                if file_exists(disk_cache_path):
                    print(f"Loading autoencoder from disk cache: {disk_cache_path}")
                else:
                    print(f"Reading autoencoder from blob storage: {autoencoder_azure_path}")
                    copy_to_local_cache(autoencoder_azure_path, disk_cache_path)
                state_dict = torch.load(disk_cache_path, map_location=self.device)
                # released autoencoders are saved as a dict for better compatibility
                assert isinstance(state_dict, dict)
                autoencoder = Autoencoder.from_state_dict(state_dict, strict=False).to(self.device)
                self._cached_autoencoders_by_path[autoencoder_azure_path] = autoencoder
            # freeze the autoencoder
            for p in autoencoder.parameters():
                p.requires_grad = False
            return autoencoder
    def warmup(self) -> None:
        """Load all autoencoders into memory."""
        for layer_index in self.layer_indices:
            self.get_autoencoder(layer_index)
        # num_autoencoder_directions is always populated after warmup
        n_latents = self.num_autoencoder_directions
        if self.omit_dead_latents:
            # drop the dead latents to save memory, but keep the same number of directions for all autoencoders
            if self._min_n_dead_latents > 0:
                print(f"Omitting {self._min_n_dead_latents} dead latents from all autoencoders")
                n_latents_to_keep = n_latents - self._min_n_dead_latents
                for key, autoencoder in self._cached_autoencoders_by_path.items():
                    self._cached_autoencoders_by_path[key] = omit_least_active_latents(
                        autoencoder, n_latents_to_keep=n_latents_to_keep
                    )
    def get_parameterized_dimension_sizes(self) -> dict[Dimension, int]:
        """A dictionary specifying the size of the parameterized dimensions; for convenient use with ScalarDerivers"""
        return {
            Dimension.AUTOENCODER_LATENTS: self.num_autoencoder_directions,
        }
    @property
    def autoencoder_node_type(self) -> NodeType | None:
        return _autoencoder_node_type_by_input_dst.get(self.dst)
_autoencoder_node_type_by_input_dst = {
    # add more mappings as needed
    DerivedScalarType.MLP_POST_ACT: NodeType.MLP_AUTOENCODER_LATENT,
    DerivedScalarType.RESID_DELTA_MLP_FROM_MLP_POST_ACT: NodeType.MLP_AUTOENCODER_LATENT,
    DerivedScalarType.RESID_DELTA_MLP: NodeType.MLP_AUTOENCODER_LATENT,
    DerivedScalarType.RESID_DELTA_ATTN: NodeType.ATTENTION_AUTOENCODER_LATENT,
    DerivedScalarType.ATTN_WRITE: NodeType.ATTENTION_AUTOENCODER_LATENT,
}
@dataclass(frozen=True)
class MultiAutoencoderContext:
    autoencoder_context_by_node_type: dict[NodeType, AutoencoderContext]
    @classmethod
    def from_context_or_multi_context(
        cls,
        input: Union[AutoencoderContext, "MultiAutoencoderContext", None],
    ) -> Union["MultiAutoencoderContext", None]:
        if isinstance(input, AutoencoderContext):
            return cls.from_autoencoder_context_list([input])
        elif input is None:
            return None
        else:
            return input
    @classmethod
    def from_autoencoder_context_list(
        cls, autoencoder_context_list: list[AutoencoderContext]
    ) -> "MultiAutoencoderContext":
        # check if there are duplicate node types
        node_types = [
            _autoencoder_node_type_by_input_dst[autoencoder_context.dst]
            for autoencoder_context in autoencoder_context_list
        ]
        if len(node_types) != len(set(node_types)):
            raise ValueError(f"Cannot load two autoencoders with the same node type ({node_types})")
        return cls(
            autoencoder_context_by_node_type={
                _autoencoder_node_type_by_input_dst[autoencoder_context.dst]: autoencoder_context
                for autoencoder_context in autoencoder_context_list
            }
        )
    def get_autoencoder_context(
        self, node_type: NodeType | None = None
    ) -> AutoencoderContext | None:
        if node_type is None or node_type == NodeType.AUTOENCODER_LATENT:  # handle default case
            return self.get_single_autoencoder_context()
        else:
            return self.autoencoder_context_by_node_type.get(node_type, None)
    @property
    def has_single_autoencoder_context(self) -> bool:
        return len(self.autoencoder_context_by_node_type) == 1
    def get_single_autoencoder_context(self) -> AutoencoderContext:
        assert self.has_single_autoencoder_context
        return next(iter(self.autoencoder_context_by_node_type.values()))
    def get_autoencoder(
        self, layer_index: LayerIndex, node_type: NodeType | None = None
    ) -> Autoencoder:
        autoencoder_context = self.get_autoencoder_context(node_type)
        assert autoencoder_context is not None
        return autoencoder_context.get_autoencoder(layer_index)
    def warmup(self) -> None:
        """Load all autoencoders into memory."""
        for node_type, autoencoder_context in self.autoencoder_context_by_node_type.items():
            print(f"Warming up autoencoder {node_type}")
            autoencoder_context.warmup()
def get_decoder_weight(autoencoder: Autoencoder) -> torch.Tensor:
    return autoencoder.decoder.weight.T  # shape (n_latents, d_ff)
def get_autoencoder_output_weight_by_layer_index(
    autoencoder_context: AutoencoderContext,
) -> dict[LayerIndex, torch.Tensor]:
    return {
        layer_index: get_decoder_weight(
            autoencoder_context.get_autoencoder(layer_index)
        )  # shape (n_latents, d_ff)
        for layer_index in autoencoder_context.layer_indices
    }
ACTIVATION_FREQUENCY_THRESHOLD_FOR_DEAD_LATENTS = 1e-8
def count_dead_latents(autoencoder: Autoencoder) -> int:
    if hasattr(autoencoder, "latents_activation_frequency"):
        if torch.all(autoencoder.latents_activation_frequency == 0):
            raise ValueError("latents_activation_frequency is all zeros, all latents are dead.")
        dead_latents_mask = (
            autoencoder.latents_activation_frequency
            < ACTIVATION_FREQUENCY_THRESHOLD_FOR_DEAD_LATENTS
        )
        num_dead_latents = int(dead_latents_mask.sum().item())
        return num_dead_latents
    else:
        return 0
def omit_least_active_latents(
    autoencoder: Autoencoder,
    n_latents_to_keep: int,
    # if preserve_indices=True, ignore the stored activation frequencies, and keep the first indices.
    # this is to preserve latent indices compared to the original autoencoder.
    preserve_indices: bool = True,
) -> Autoencoder:
    n_latents_original = int(autoencoder.latent_bias.shape[0])
    if n_latents_to_keep >= n_latents_original:
        return autoencoder
    device: torch.device = autoencoder.encoder.weight.device
    # create the dead latent mask (True for live latents, False for dead latents)
    mask = torch.ones(n_latents_original, dtype=torch.bool, device=device)
    if preserve_indices or not hasattr(autoencoder, "latents_activation_frequency"):
        # drop the last latents
        mask[n_latents_to_keep:] = 0
    else:
        # drop the least active latents
        order = torch.argsort(autoencoder.latents_activation_frequency, descending=True)
        mask[order[n_latents_to_keep:]] = 0
    # apply the mask to a new autoencoder
    n_latents = int(mask.sum().item())
    d_model = autoencoder.pre_bias.shape[0]
    new_autoencoder = Autoencoder(n_latents, d_model).to(device)
    new_autoencoder.encoder.weight.data = autoencoder.encoder.weight[mask, :].clone()
    new_autoencoder.decoder.weight.data = autoencoder.decoder.weight[:, mask].clone()
    new_autoencoder.latent_bias.data = autoencoder.latent_bias[mask].clone()
    new_autoencoder.stats_last_nonzero.data = autoencoder.stats_last_nonzero[mask].clone()
    if hasattr(autoencoder, "latents_mean_square"):
        new_autoencoder.register_buffer(
            "latents_mean_square", torch.zeros(n_latents, dtype=torch.float)
        )
        new_autoencoder.latents_mean_square.data = autoencoder.latents_mean_square[mask].clone()
    if hasattr(autoencoder, "latents_activation_frequency"):
        new_autoencoder.register_buffer(
            "latents_activation_frequency", torch.ones(n_latents, dtype=torch.float)
        )
        new_autoencoder.latents_activation_frequency.data = (
            autoencoder.latents_activation_frequency[mask].clone()
        )
    return new_autoencoder

================
File: build/lib/neuron_explainer/models/autoencoder.py
================
from typing import Callable
import torch
import torch.nn as nn
import torch.nn.functional as F
class Autoencoder(nn.Module):
    """Sparse autoencoder
    Implements:
        latents = activation(encoder(x - pre_bias) + latent_bias)
        recons = decoder(latents) + pre_bias
    """
    def __init__(
        self, n_latents: int, n_inputs: int, activation: Callable = nn.ReLU(), tied: bool = False
    ) -> None:
        """
        :param n_latents: dimension of the autoencoder latent
        :param n_inputs: dimensionality of the original data (e.g residual stream, number of MLP hidden units)
        :param activation: activation function
        :param tied: whether to tie the encoder and decoder weights
        """
        super().__init__()
        self.pre_bias = nn.Parameter(torch.zeros(n_inputs))
        self.encoder: nn.Module = nn.Linear(n_inputs, n_latents, bias=False)
        self.latent_bias = nn.Parameter(torch.zeros(n_latents))
        self.activation = activation
        if tied:
            self.decoder: nn.Linear | TiedTranspose = TiedTranspose(self.encoder)
        else:
            self.decoder = nn.Linear(n_latents, n_inputs, bias=False)
        self.stats_last_nonzero: torch.Tensor
        self.latents_activation_frequency: torch.Tensor
        self.latents_mean_square: torch.Tensor
        self.register_buffer("stats_last_nonzero", torch.zeros(n_latents, dtype=torch.long))
        self.register_buffer(
            "latents_activation_frequency", torch.ones(n_latents, dtype=torch.float)
        )
        self.register_buffer("latents_mean_square", torch.zeros(n_latents, dtype=torch.float))
    def encode_pre_act(self, x: torch.Tensor, latent_slice: slice = slice(None)) -> torch.Tensor:
        """
        :param x: input data (shape: [batch, n_inputs])
        :param latent_slice: slice of latents to compute
            Example: latent_slice = slice(0, 10) to compute only the first 10 latents.
        :return: autoencoder latents before activation (shape: [batch, n_latents])
        """
        x = x - self.pre_bias
        latents_pre_act = F.linear(
            x, self.encoder.weight[latent_slice], self.latent_bias[latent_slice]
        )
        return latents_pre_act
    def encode(self, x: torch.Tensor) -> torch.Tensor:
        """
        :param x: input data (shape: [batch, n_inputs])
        :return: autoencoder latents (shape: [batch, n_latents])
        """
        return self.activation(self.encode_pre_act(x))
    def decode(self, latents: torch.Tensor) -> torch.Tensor:
        """
        :param latents: autoencoder latents (shape: [batch, n_latents])
        :return: reconstructed data (shape: [batch, n_inputs])
        """
        return self.decoder(latents) + self.pre_bias
    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        :param x: input data (shape: [batch, n_inputs])
        :return:  autoencoder latents pre activation (shape: [batch, n_latents])
                  autoencoder latents (shape: [batch, n_latents])
                  reconstructed data (shape: [batch, n_inputs])
        """
        latents_pre_act = self.encode_pre_act(x)
        latents = self.activation(latents_pre_act)
        recons = self.decode(latents)
        # set all indices of self.stats_last_nonzero where (latents != 0) to 0
        self.stats_last_nonzero *= (latents == 0).all(dim=0).long()
        self.stats_last_nonzero += 1
        return latents_pre_act, latents, recons
    @classmethod
    def from_state_dict(
        cls, state_dict: dict[str, torch.Tensor], strict: bool = True
    ) -> "Autoencoder":
        n_latents, d_model = state_dict["encoder.weight"].shape
        autoencoder = cls(n_latents, d_model)
        # Retrieve activation
        activation_class_name = state_dict.pop("activation", "ReLU")
        activation_class = ACTIVATIONS_CLASSES.get(activation_class_name, nn.ReLU)
        activation_state_dict = state_dict.pop("activation_state_dict", {})
        if hasattr(activation_class, "from_state_dict"):
            autoencoder.activation = activation_class.from_state_dict(
                activation_state_dict, strict=strict
            )
        else:
            autoencoder.activation = activation_class()
            if hasattr(autoencoder.activation, "load_state_dict"):
                autoencoder.activation.load_state_dict(activation_state_dict, strict=strict)
        # Load remaining state dict
        autoencoder.load_state_dict(state_dict, strict=strict)
        return autoencoder
    def state_dict(self, destination=None, prefix="", keep_vars=False):
        sd = super().state_dict(destination, prefix, keep_vars)
        sd[prefix + "activation"] = self.activation.__class__.__name__
        if hasattr(self.activation, "state_dict"):
            sd[prefix + "activation_state_dict"] = self.activation.state_dict()
        return sd
class TiedTranspose(nn.Module):
    def __init__(self, linear: nn.Linear):
        super().__init__()
        self.linear = linear
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        assert self.linear.bias is None
        return F.linear(x, self.linear.weight.t(), None)
    @property
    def weight(self) -> torch.Tensor:
        return self.linear.weight.t()
    @property
    def bias(self) -> torch.Tensor:
        return self.linear.bias
class TopK(nn.Module):
    def __init__(self, k: int, postact_fn: Callable = nn.ReLU()) -> None:
        super().__init__()
        self.k = k
        self.postact_fn = postact_fn
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        topk = torch.topk(x, k=self.k, dim=-1)
        values = self.postact_fn(topk.values)
        # make all other values 0
        result = torch.zeros_like(x)
        result.scatter_(-1, topk.indices, values)
        return result
    def state_dict(self, destination=None, prefix="", keep_vars=False):
        state_dict = super().state_dict(destination, prefix, keep_vars)
        state_dict.update({prefix + "k": self.k, prefix + "postact_fn": self.postact_fn.__class__.__name__})
        return state_dict
    @classmethod
    def from_state_dict(cls, state_dict: dict[str, torch.Tensor], strict: bool = True) -> "TopK":
        k = state_dict["k"]
        postact_fn = ACTIVATIONS_CLASSES[state_dict["postact_fn"]]()
        return cls(k=k, postact_fn=postact_fn)
ACTIVATIONS_CLASSES = {
    "ReLU": nn.ReLU,
    "Identity": nn.Identity,
    "TopK": TopK,
}

================
File: build/lib/neuron_explainer/models/hooks.py
================
from collections import OrderedDict
from copy import deepcopy
import torch
class Hooks:
    """A callable that is a sequence of callables"""
    def __init__(self):
        self._hooks = []
        self.bound_kwargs = {}
    def __call__(self, x, *args, **kwargs):
        for hook in self._hooks:
            x = hook(x, *args, **kwargs, **self.bound_kwargs)
        return x
    def append(self, hook):
        self._hooks.append(hook)
        return self
    def bind(self, **kwargs):
        for key, value in kwargs.items():
            if key in self.bound_kwargs:
                raise ValueError(f"Key {key} already bound")
            self.bound_kwargs[key] = value
        return self
    def unbind(self, keys: list):
        for key in keys:
            del self.bound_kwargs[key]
        return self
    def __repr__(self, indent=0, name=None):
        import inspect
        indent_str = " " * indent
        full_name = f"{name}" if name is not None else self.name
        if self.bound_kwargs:
            full_name += f" {self.bound_kwargs}"
        if self.is_empty():
            return f"{indent_str}{full_name}"
        def hook_repr(hook):
            if "indent" in inspect.signature(hook.__class__.__repr__).parameters:
                return hook.__repr__(indent=indent + 4)
            else:
                return indent_str + " " * 4 + repr(hook)
        hooks_repr = "\n".join(f"{hook_repr(hook)}" for hook in self._hooks)
        return f"{indent_str}{full_name}\n{hooks_repr}"
    @property
    def name(self):
        return self.__class__.__name__
    def is_empty(self):
        return len(self._hooks) == 0
# takes a gradient hook and makes into a forward pass hook
def grad_hook_wrapper(grad_hook):
    def fwd_hook(act, *args, **kwargs):
        class _IdentityWithGradHook(torch.autograd.Function):
            @staticmethod
            def forward(ctx, tensor):
                return tensor
            @staticmethod
            def backward(ctx, grad_output):
                grad_output = grad_hook(grad_output, *args, **kwargs)
                return grad_output
        return _IdentityWithGradHook.apply(act)
    return fwd_hook
class HookCollection:
    def __init__(self):
        self.all_hooks = OrderedDict()
    def bind(self, **kwargs):
        for hook in self.all_hooks.values():
            hook.bind(**kwargs)
        return self
    def unbind(self, keys):
        for hook in self.all_hooks.values():
            hook.unbind(keys)
        return self
    def append_all(self, hook):
        for hooks in self.all_hooks.values():
            try:
                hooks.append_all(hook)
            except AttributeError:
                hooks.append(hook)
        return self
    def append_to_path(self, hook_location_name, hook):
        """
        Adds a hook to a location in a nested hook collection with a dot-separated name.
        e.g. `self.append_to_path("resid.torso.post_mlp.fwd", hook)` adds `hook` to
        `self.all_hooks["resid"].all_hooks["torso"].all_hooks["post_mlp"].all_hooks["fwd"]`
        """
        hook_location_parts = hook_location_name.split(".", 1)  # split at first dot, if present
        top_level_location = hook_location_parts[0]
        assert top_level_location in self.all_hooks
        if len(hook_location_parts) == 1:  # no dot in path
            self.all_hooks[top_level_location].append(hook)
        else:  # at least one dot in path -> split outputs two parts
            sub_location = hook_location_parts[1]
            self.all_hooks[top_level_location].append_to_path(sub_location, hook)
        return self
    def __deepcopy__(self, memo):
        # can't use deepcopy because of __getattr__
        new = self.__class__()
        new.all_hooks = deepcopy(self.all_hooks)
        return new
    def add_subhooks(self, name, subhooks):
        self.all_hooks[name] = subhooks
        return self
    def __getattr__(self, name):
        if name in self.all_hooks:
            return self.all_hooks[name]
        else:
            raise AttributeError(f"HookCollection has no attribute {name}")
    def __repr__(self, indent=0, name=None):
        indent_str = " " * indent
        full_name = f"{name}" if name is not None else self.__class__.__name__
        prefix = f"{name}." if name is not None else ""
        hooks_repr = "\n".join(
            hook.__repr__(indent + 4, f"{prefix}{hook_name}")
            for hook_name, hook in self.all_hooks.items()
        )
        return f"{indent_str}{full_name}\n{hooks_repr}"
    def is_empty(self):
        return all(hook.is_empty() for hook in self.all_hooks.values())
class FwdBwdHooks(HookCollection):
    def __init__(self):
        super().__init__()
        # By default, all grad hooks are applied after all forward hooks.  This way,
        # the gradients are given for the final "hooked" output of the forward pass.
        # If you want gradients for an intermediate output, you should simply
        # append_fwd(from_grad_hook(hook)) at the appropriate time.
        self.add_subhooks("fwd", Hooks())
        self.add_subhooks("bwd", WrapperHooks(wrapper=grad_hook_wrapper))
        self.add_subhooks("fwd2", Hooks())
    def append_fwd(self, fwd_hook):
        self.fwd.append(fwd_hook)
        return self
    def append_bwd(self, bwd_hook):
        self.bwd.append(bwd_hook)
        return self
    def append_fwd2(self, fwd2_hook):
        self.fwd2.append(fwd2_hook)
        return self
    def __call__(self, x, *args, **kwargs):
        # hooks into fwd, then bwd, then fwd2
        x = self.fwd(x, *args, **kwargs)
        x = self.bwd(x, *args, **kwargs)
        x = self.fwd2(x, *args, **kwargs)
        return x
class MLPHooks(HookCollection):
    def __init__(self):
        super().__init__()
        self.add_subhooks("pre_act", FwdBwdHooks())
        self.add_subhooks("post_act", FwdBwdHooks())
class NormalizationHooks(HookCollection):
    def __init__(self):
        super().__init__()
        self.add_subhooks("post_mean_subtraction", FwdBwdHooks())
        self.add_subhooks("scale", FwdBwdHooks())
        self.add_subhooks("post_scale", FwdBwdHooks())
class AttentionHooks(HookCollection):
    def __init__(self):
        super().__init__()
        self.add_subhooks("q", FwdBwdHooks())
        self.add_subhooks("k", FwdBwdHooks())
        self.add_subhooks("v", FwdBwdHooks())
        self.add_subhooks("qk_logits", FwdBwdHooks())
        self.add_subhooks("qk_softmax_denominator", FwdBwdHooks())
        self.add_subhooks("qk_probs", FwdBwdHooks())
        self.add_subhooks("v_out", FwdBwdHooks())  # pre-final projection
class ResidualStreamTorsoHooks(HookCollection):
    def __init__(self):
        super().__init__()
        self.add_subhooks("post_ln_attn", FwdBwdHooks())
        self.add_subhooks("ln_attn", NormalizationHooks())
        self.add_subhooks("delta_attn", FwdBwdHooks())
        self.add_subhooks("post_attn", FwdBwdHooks())
        self.add_subhooks("ln_mlp", NormalizationHooks())
        self.add_subhooks("post_ln_mlp", FwdBwdHooks())
        self.add_subhooks("delta_mlp", FwdBwdHooks())
        self.add_subhooks("post_mlp", FwdBwdHooks())
class ResidualStreamHooks(HookCollection):
    def __init__(self):
        super().__init__()
        self.add_subhooks("post_emb", FwdBwdHooks())
        self.add_subhooks("torso", ResidualStreamTorsoHooks())
        self.add_subhooks("ln_f", NormalizationHooks())
        self.add_subhooks("post_ln_f", FwdBwdHooks())
class TransformerHooks(HookCollection):
    def __init__(self):
        super().__init__()
        self.add_subhooks("mlp", MLPHooks())
        self.add_subhooks("attn", AttentionHooks())
        self.add_subhooks("resid", ResidualStreamHooks())
        self.add_subhooks("logits", FwdBwdHooks())
class WrapperHooks(Hooks):
    def __init__(self, wrapper):
        self.wrapper = wrapper
        super().__init__()
    def append(self, fn):
        self._hooks.append(self.wrapper(fn))
class ConditionalHooks(Hooks):
    def __init__(self, condition):
        self.condition = condition
        super().__init__()
    def __call__(self, x, *args, **kwargs):
        cond = self.condition(x, *args, **kwargs)
        if cond:
            x = super().__call__(x, *args, **kwargs)
        return x
class AtLayers(ConditionalHooks):
    def __init__(self, at_layers: int | list[int]):
        if isinstance(at_layers, int):
            at_layers = [at_layers]
        self.at_layers = at_layers
        def at_layers_condition(x, *, layer, **kwargs):
            return layer in at_layers
        super().__init__(condition=at_layers_condition)
    @property
    def name(self):
        return f"{self.__class__.__name__}({self.at_layers})"
class AutoencoderHooks(HookCollection):
    """
    Hooks into the hidden dimension of an autoencoder.
    """
    def __init__(self, encode, decode, add_error=False):
        super().__init__()
        # hooks
        self.add_subhooks("latents", FwdBwdHooks())
        self.add_subhooks("reconstruction", FwdBwdHooks())
        self.add_subhooks("error", FwdBwdHooks())
        # autoencoder functions
        self.encode = encode
        self.decode = decode
        # if add_error is True, add the error to the reconstruction.
        self.add_error = add_error
    def __call__(self, x, *args, **kwargs):
        latents = self.encode(x)
        if self.add_error:
            # Here, the latents are cloned twice:
            # - the first clone is passed through `self.latents` and `self.reconstruction`
            # - the second clone is passed through `self.error`
            latents_to_hook = latents.clone()
            latents_to_error = latents.clone()
        else:
            latents_to_hook = latents
        latents_to_hook = self.latents(latents_to_hook, *args, **kwargs)  # call hooks
        reconstruction = self.decode(latents_to_hook)
        reconstruction = self.reconstruction(reconstruction, *args, **kwargs)  # call hooks
        if self.add_error:
            error = x - self.decode(latents_to_error)
            error = self.error(error, *args, **kwargs)  # call hooks
            return reconstruction + error
        else:
            error = x - reconstruction
            error = self.error(error, *args, **kwargs)  # call hooks
            return reconstruction
    def __deepcopy__(self, memo):
        # can't use deepcopy because of __getattr__
        new = self.__class__(self.encode, self.decode, self.add_error)
        new.all_hooks = deepcopy(self.all_hooks)
        return new

================
File: build/lib/neuron_explainer/models/inference_engine_type_registry.py
================
# This file supports translation between universal representation and inference engine-specific
# representations. The current codebase only supports one inference engine, but that may change in
# the future.
from enum import Enum
from neuron_explainer.models.model_component_registry import ActivationLocationType, LayerIndex
# TODO: Consider using a stronger type here.
StandardModelHookLocationType = str
HookLocationType = StandardModelHookLocationType
class InferenceEngineType(str, Enum):
    STANDARD = "standard"
_standard_model_hook_location_type_by_activation_location_type: dict[
    ActivationLocationType, StandardModelHookLocationType
] = {
    ActivationLocationType.RESID_POST_EMBEDDING: "resid/post_emb",
    ActivationLocationType.RESID_POST_MLP: "resid/post_mlp",
    ActivationLocationType.MLP_PRE_ACT: "mlp/pre_act",
    ActivationLocationType.MLP_POST_ACT: "mlp/post_act",
    ActivationLocationType.ATTN_QUERY: "attn/q",
    ActivationLocationType.ATTN_KEY: "attn/k",
    ActivationLocationType.ATTN_VALUE: "attn/v",
    ActivationLocationType.ATTN_QK_LOGITS: "attn/qk_logits",
    ActivationLocationType.ATTN_QK_PROBS: "attn/qk_probs",
    ActivationLocationType.ATTN_WEIGHTED_SUM_OF_VALUES: "attn/v_out",
    ActivationLocationType.RESID_DELTA_ATTN: "resid/delta_attn",
    ActivationLocationType.RESID_DELTA_MLP: "resid/delta_mlp",
    ActivationLocationType.RESID_POST_ATTN: "resid/post_attn",
    ActivationLocationType.RESID_POST_MLP: "resid/post_mlp",
    ActivationLocationType.LOGITS: "logits",
    ActivationLocationType.RESID_FINAL_LAYER_NORM_SCALE: "resid/ln_f/scale",
    ActivationLocationType.ATTN_INPUT_LAYER_NORM_SCALE: "resid/ln_attn/scale",
    ActivationLocationType.MLP_INPUT_LAYER_NORM_SCALE: "resid/ln_mlp/scale",
}
_hook_location_type_by_activation_location_type_by_inference_engine_type: dict[
    InferenceEngineType, dict[ActivationLocationType, HookLocationType]
] = {
    InferenceEngineType.STANDARD: _standard_model_hook_location_type_by_activation_location_type,
}
_activation_location_type_by_hook_location_type_by_inference_engine_type: dict[
    InferenceEngineType, dict[HookLocationType, ActivationLocationType]
] = {
    inference_engine_type: {v: k for k, v in hook_location_type_by_activation_location_type.items()}
    for inference_engine_type, hook_location_type_by_activation_location_type in _hook_location_type_by_activation_location_type_by_inference_engine_type.items()
}
standard_model_activation_location_types: set[ActivationLocationType] = set(
    _hook_location_type_by_activation_location_type_by_inference_engine_type[
        InferenceEngineType.STANDARD
    ].keys()
)
def get_hook_location_type_for_activation_location_type(
    activation_location_type: ActivationLocationType, inference_engine_type: InferenceEngineType
) -> HookLocationType:
    assert (
        inference_engine_type
        in _hook_location_type_by_activation_location_type_by_inference_engine_type
    ), f"Unknown inference_engine_type {inference_engine_type}"
    return _hook_location_type_by_activation_location_type_by_inference_engine_type[
        inference_engine_type
    ][activation_location_type]
def get_activation_location_type_for_hook_location_type(
    hook_location_type: HookLocationType, inference_engine_type: InferenceEngineType
) -> ActivationLocationType:
    assert (
        inference_engine_type
        in _activation_location_type_by_hook_location_type_by_inference_engine_type
    ), f"Unknown inference_engine_type {inference_engine_type}"
    return _activation_location_type_by_hook_location_type_by_inference_engine_type[
        inference_engine_type
    ][hook_location_type]
def parse_standard_location_str(location: str) -> tuple[str, LayerIndex]:
    """
    Our transformer implementation uses location strings like "7/mlp/post_act" or "resid/post_emb"
    to capture both the location type and the layer index (if applicable). This function parses
    those strings, returning the location type and layer index. The location type is specific to our
    transformer implementation: call get_activation_location_type_for_hook_location_type to convert
    it into an ActivationLocationType.
    """
    if location[0].isdigit():
        # Example: 7/mlp/post_act
        return location[location.find("/") + 1 :], int(location.split("/")[0])
    else:
        # Example: resid/post_emb
        return location, None

================
File: build/lib/neuron_explainer/models/model_component_registry.py
================
"""
This file contains Enums of three kinds of objects that are common across
LM architectures: weight locations, activation locations, and model dimensions.
The weight locations and activation locations are associated
to tuples of dimensions, to define their standardized shapes. This file is intended to
serve as a source of truth for those standards.
Abbreviations appearing in this file:
attn = attention
emb = embedding
ln = layer norm
ln_f = final layer norm
mlp = multi-layer perceptron
act = activation
resid = residual
q = query
k = key
v = value
qk = query-key inner product (with 1/sqrt(query_and_key_channels) scaling already applied)
"""
from dataclasses import dataclass
from enum import Enum, EnumMeta, auto, unique
from typing import Optional
class WeightLocationType(str, Enum):
    """These are the names of tensors that are expected to be found in model weights."""
    MLP_TO_HIDDEN = "mlp.to_hidden"
    MLP_TO_RESIDUAL = "mlp.to_residual"
    EMBEDDING = "embedding"
    UNEMBEDDING = "unembedding"
    POSITION_EMBEDDING = "position_embedding"
    ATTN_TO_QUERY = "attn.to_query"
    ATTN_TO_KEY = "attn.to_key"
    ATTN_TO_VALUE = "attn.to_value"
    ATTN_TO_RESIDUAL = "attn.to_residual"
    LAYER_NORM_GAIN_FINAL = "layer_norm_gain.final"
    LAYER_NORM_BIAS_FINAL = "layer_norm_bias.final"
    LAYER_NORM_GAIN_PRE_ATTN = "layer_norm_gain.pre_attn"
    LAYER_NORM_GAIN_PRE_MLP = "layer_norm_gain.pre_mlp"
    @property
    def is_mlp_specific(self) -> bool:
        return self in {
            WeightLocationType.MLP_TO_HIDDEN,
            WeightLocationType.MLP_TO_RESIDUAL,
            WeightLocationType.LAYER_NORM_GAIN_PRE_MLP,
        }
    @property
    def has_no_layers(self) -> bool:
        # if True, there is one of this type of weight tensor per model, and the layer index is set
        # as None wherever used (these occur at the very beginning or end of the model)
        # if False, there is one of this type of weight tensor per layer, and the tensor additionally
        # requires a layer index to specify
        return self in {
            WeightLocationType.EMBEDDING,
            WeightLocationType.UNEMBEDDING,
            WeightLocationType.POSITION_EMBEDDING,
            WeightLocationType.LAYER_NORM_GAIN_FINAL,
            WeightLocationType.LAYER_NORM_BIAS_FINAL,
        }
    @property
    def is_absolute_position_embedding_specific(self) -> bool:
        return self in {WeightLocationType.POSITION_EMBEDDING}
    @property
    def shape_spec(self) -> tuple["Dimension", ...]:
        return weight_shape_by_location_type[self]
class EnumMetaContains(EnumMeta):
    def __contains__(cls, item: object) -> bool:
        # enables the syntax "if item in enum:"
        return isinstance(item, cls) or item in cls._value2member_map_
LayerIndex = int | None  # None refers to an activation with no layer index (e.g. embeddings)
class LocationWithinLayer(int, Enum):
    """
    Coarsely specifies the location of a tensor within a layer. Each of the following is mapped to an
    int, and the ordering is from the beginning of the layer to the end.
    This is to be inferred for a scalar deriver based on information available to it.
    The level of granularity is enough to determine whether one node is upstream of a different node,
    but no more. For example, it doesn't distinguish between attention pre- and post-softmax, but it does
    distinguish between an attention head and the residual stream locations immediately before and after
    the attention layer.
    THESE VALUES ARE NOT INTENDED TO BE SERIALIZED.
    The strategy for inferring the LocationWithinLayer is to check
    (1) the node type of the ScalarDeriver
    (2) the activation location type of the ScalarDeriver (if it corresponds to a raw activation)
    (3) the dst of the ScalarDeriver
    (4) any other information available to the ScalarDeriver.
    Based on any one
    piece of information alone, it may be ambiguous what the location within layer is. For example,
    DerivedScalarType.RESID_POST_ATTN and DerivedScalarType.RESID_POST_MLP both correspond to the same
    node type, but post-attn appears earlier in the layer than post-mlp. So the location within layer
    is left ambiguous after checking the node, but clarified after checking the activation location type.
    """
    END_OF_PREV_LAYER = (
        auto()
    )  # auto() assigns increasing integer values to the enum values, starting from 1
    ATTN = auto()
    RESID_POST_ATTN = auto()
    MLP = auto()
    RESID_POST_MLP = auto()
class NodeType(str, Enum):
    """
    A "node" is defined as a model component associated with a scalar activation per
    token or per token pair. The canonical example is an MLP neuron. An activation
    for which the NodeType is defined has the node as the last dimension of the
    activation tensor.
    """
    ATTENTION_HEAD = "attention_head"
    QK_CHANNEL = "qk_channel"
    V_CHANNEL = "v_channel"
    MLP_NEURON = "mlp_neuron"
    AUTOENCODER_LATENT = "autoencoder_latent"
    MLP_AUTOENCODER_LATENT = "mlp_autoencoder_latent"
    ATTENTION_AUTOENCODER_LATENT = "attention_autoencoder_latent"
    # TODO: remove this hack, and make NodeType depend on the token dimensions
    AUTOENCODER_LATENT_BY_TOKEN_PAIR = "autoencoder_latent_by_token_pair"
    LAYER = "layer"
    RESIDUAL_STREAM_CHANNEL = "residual_stream_channel"
    VOCAB_TOKEN = "vocab_token"
    @property
    def location_within_layer(self) -> Optional["LocationWithinLayer"]:
        # this uses the information available to infer the location within a layer of a specific node_type.
        # It returns None in cases where the location within layer is ambiguous based on the information
        # provided; e.g. for residual stream node types, it might be post-attn or post-mlp.
        # (with activation location type
        # for additional clarification). It returns None in cases where the location within layer is ambiguous based on the information
        # provided; one example is for autoencoder latents, which might be based on any dst. In this case, further information from the
        # DSTConfig is used.
        # It throws an error if the node_type is *never* associated with a location within layer (e.g. vocab tokens)
        match self:
            case NodeType.MLP_NEURON:
                return LocationWithinLayer.MLP
            case NodeType.ATTENTION_HEAD | NodeType.QK_CHANNEL | NodeType.V_CHANNEL:
                return LocationWithinLayer.ATTN
            case (
                NodeType.RESIDUAL_STREAM_CHANNEL
                | NodeType.LAYER
                | NodeType.AUTOENCODER_LATENT
                | NodeType.MLP_AUTOENCODER_LATENT
                | NodeType.ATTENTION_AUTOENCODER_LATENT
                | NodeType.AUTOENCODER_LATENT_BY_TOKEN_PAIR
            ):
                # these node types are ambiguous based on the information provided
                return None
            case NodeType.VOCAB_TOKEN:
                # users should not be asking about the location within layer of vocab tokens; this indicates something's wrong
                raise ValueError("Vocab tokens don't have a location within layer")
            case _:
                raise NotImplementedError(f"Unknown node type {self=}")
    @property
    def is_autoencoder_latent(self) -> bool:
        return self in {
            NodeType.AUTOENCODER_LATENT,
            NodeType.MLP_AUTOENCODER_LATENT,
            NodeType.ATTENTION_AUTOENCODER_LATENT,
            NodeType.AUTOENCODER_LATENT_BY_TOKEN_PAIR,
        }
class ActivationLocationType(str, Enum, metaclass=EnumMetaContains):
    """These are the names of activations expected to be instantiated during a forward pass. All activations are
    pre-layer norm unless otherwise specified (RESID_POST_XYZ_LAYER_NORM)."""
    RESID_POST_EMBEDDING = "resid.post_emb"
    RESID_DELTA_ATTN = "resid.delta_attn"
    RESID_POST_ATTN = "resid.post_attn"
    RESID_DELTA_MLP = "resid.delta_mlp"
    RESID_POST_MLP = "resid.post_mlp"
    RESID_POST_MLP_LAYER_NORM = "resid.post_mlp_ln"
    RESID_POST_ATTN_LAYER_NORM = "resid.post_attn_ln"
    RESID_POST_FINAL_LAYER_NORM = "resid.post_ln_f"
    MLP_INPUT_LAYER_NORM_SCALE = "mlp_ln.scale"
    ATTN_INPUT_LAYER_NORM_SCALE = "attn_ln.scale"
    RESID_FINAL_LAYER_NORM_SCALE = "resid.ln_f.scale"
    ATTN_QUERY = "attn.q"
    ATTN_KEY = "attn.k"
    ATTN_VALUE = "attn.v"
    ATTN_QK_LOGITS = "attn.qk_logits"
    ATTN_QK_PROBS = "attn.qk_probs"
    ATTN_WEIGHTED_SUM_OF_VALUES = "attn.v_out"
    MLP_PRE_ACT = "mlp.pre_act"
    MLP_POST_ACT = "mlp.post_act"
    LOGITS = "logits"
    ONLINE_AUTOENCODER_LATENT = "online_autoencoder_latent"
    ONLINE_MLP_AUTOENCODER_LATENT = "online_mlp_autoencoder_latent"
    ONLINE_ATTENTION_AUTOENCODER_LATENT = "online_attention_autoencoder_latent"
    ONLINE_MLP_AUTOENCODER_ERROR = "online_mlp_autoencoder_error"
    ONLINE_RESIDUAL_MLP_AUTOENCODER_ERROR = "online_residual_mlp_autoencoder_error"
    ONLINE_RESIDUAL_ATTENTION_AUTOENCODER_ERROR = "online_residual_attention_autoencoder_error"
    @property
    def has_no_layers(self) -> bool:
        # if True, there is one of this type of activation tensor per model, and the layer index is set
        # as None wherever used (these occur at the very beginning or end of the model)
        # if False, there is one of this type of activation tensor per layer, and the tensor additionally
        # requires a layer index to specify
        return self in {
            ActivationLocationType.RESID_POST_EMBEDDING,
            ActivationLocationType.RESID_FINAL_LAYER_NORM_SCALE,
            ActivationLocationType.RESID_POST_FINAL_LAYER_NORM,
            ActivationLocationType.LOGITS,
        }
    @property
    def shape_spec_per_token_sequence(self) -> tuple["Dimension", ...]:
        return _activation_shape_per_token_sequence_by_location_type[self]
    @property
    def ndim_per_token_sequence(self) -> int:
        return len(self.shape_spec_per_token_sequence)
    @property
    def exists_by_default(self) -> bool:
        # this returns True if the activation is expected to exist by default in the model, and False if
        # it needs to be added using hooks
        return self not in {
            ActivationLocationType.ONLINE_AUTOENCODER_LATENT,
        }
    @property
    def node_type(self) -> NodeType:
        """The last index of an activation tensor can correspond to a type of object
        in the network called a 'node'. This can be an MLP neuron, an attention head, an autoencoder
        latent, etc. If we don't yet have a name for the last dimension of an activation tensor,
        this throws an error."""
        last_dimension = self.shape_spec_per_token_sequence[-1]
        if last_dimension in node_type_by_dimension:
            return node_type_by_dimension[last_dimension]
        else:
            raise NotImplementedError(f"Unknown node type for {last_dimension=}")
    @property
    def location_within_layer(self) -> LocationWithinLayer | None:
        # this uses the information available to infer the location within a layer of a specific node_type (with activation location type
        # for additional clarification). It returns None in cases where the location within layer is ambiguous based on the information
        # provided; one example is for autoencoder latents, which might be based on any dst. In this case, further information from the
        # DSTConfig is needed.
        # It throws an error if the node_type is not associated with a location within layer.
        if self.node_type.location_within_layer is None:
            if self.node_type == NodeType.RESIDUAL_STREAM_CHANNEL:
                if self == ActivationLocationType.RESID_POST_EMBEDDING:
                    return None
                elif self == ActivationLocationType.RESID_DELTA_ATTN:
                    return LocationWithinLayer.ATTN
                elif self == ActivationLocationType.RESID_POST_ATTN:
                    return LocationWithinLayer.RESID_POST_ATTN
                elif self == ActivationLocationType.RESID_DELTA_MLP:
                    return LocationWithinLayer.MLP
                elif self == ActivationLocationType.RESID_POST_MLP:
                    return LocationWithinLayer.RESID_POST_MLP
                else:
                    return None
            else:
                return None
        else:
            return self.node_type.location_within_layer
class Dimension(str, Enum):
    """Dimensions correspond to the names of dimensions of activation tensors, and can depend on the input,
    the model, or e.g. parameters of added subgraphs such as autoencoders.
    The dimensions below are taken to be 'per layer' wherever applicable.
    Dimensions associated with attention heads (e.g. value channels) are taken to be 'per attention head'.
    """
    SEQUENCE_TOKENS = "sequence_tokens"
    ATTENDED_TO_SEQUENCE_TOKENS = "attended_to_sequence_tokens"
    """These are the names of dimensions of activation tensors that are intrinsic to a model,
    and are not a consequence of a particular input sequence. The shape of activations will in general
    depend on these and on Dimension, above. The shape of weights will in general depend only on
    these."""
    # "context" refers to the number of tokens in the sequence being processed by the model
    # "max_context_length" refers to the maximum number of tokens that can be processed by the
    # model (relevant for models with absolute position embeddingsg)
    MAX_CONTEXT_LENGTH = "max_context_length"
    # "residual_stream_channels" means the same as "d_model"
    RESIDUAL_STREAM_CHANNELS = "residual_stream_channels"
    VOCAB_SIZE = "vocab_size"
    ATTN_HEADS = "attn_heads"
    QUERY_AND_KEY_CHANNELS = "query_and_key_channels"
    VALUE_CHANNELS = "value_channels"
    MLP_ACTS = "mlp_acts"
    LAYERS = "layers"
    SINGLETON = "singleton"  # always 1
    """These are the names of dimensions that are not intrinsic to a model's activations, but that in some
    way parameterize its activations (currently just including autoencoder latents, but in future possibly
    including other methods for finding useful directions within activations)."""
    AUTOENCODER_LATENTS = "autoencoder_latents"
    # TODO: remove this hack, and make NodeType depend on the token dimensions
    AUTOENCODER_LATENTS_BY_TOKEN_PAIR = "autoencoder_latents_by_token_pair"
    @property
    def is_sequence_token_dimension(self) -> bool:
        return self in {Dimension.SEQUENCE_TOKENS, Dimension.ATTENDED_TO_SEQUENCE_TOKENS}
    @property
    def is_parameterized_dimension(self) -> bool:
        return self in {Dimension.AUTOENCODER_LATENTS}
    @property
    def is_model_intrinsic(self) -> bool:
        """this is True for dimensions that depend only on the model. This is False for dimensions that depend on either (1) the input being processed, or (2) some parameterization
        of the model activations (e.g. autoencoder latents).
        """
        return not (self.is_parameterized_dimension or self.is_sequence_token_dimension)
node_type_by_dimension: dict[Dimension, NodeType] = {
    Dimension.MLP_ACTS: NodeType.MLP_NEURON,
    Dimension.ATTN_HEADS: NodeType.ATTENTION_HEAD,
    Dimension.QUERY_AND_KEY_CHANNELS: NodeType.QK_CHANNEL,
    Dimension.VALUE_CHANNELS: NodeType.V_CHANNEL,
    Dimension.SINGLETON: NodeType.LAYER,
    Dimension.RESIDUAL_STREAM_CHANNELS: NodeType.RESIDUAL_STREAM_CHANNEL,
    Dimension.VOCAB_SIZE: NodeType.VOCAB_TOKEN,
    Dimension.AUTOENCODER_LATENTS: NodeType.AUTOENCODER_LATENT,
    Dimension.AUTOENCODER_LATENTS_BY_TOKEN_PAIR: NodeType.AUTOENCODER_LATENT_BY_TOKEN_PAIR,
}
_activation_shape_per_token_sequence_by_location_type: dict[
    ActivationLocationType, tuple[Dimension, ...]
] = {
    # this is a standard convention for the shape of activation tensors per token sequence. All activations
    # by convention have either Dimension.SEQUENCE_TOKENS, Dimension.ATTENDED_TO_SEQUENCE_TOKENS,
    # or both, as their first dimension. The ordering of dimensions is generally:
    # 1. tokens
    # 2. dimensions with a privileged basis (e.g. attention heads, MLP neurons), descending in order of
    # hierarchy
    # 3. dimensions without a privileged basis (e.g. residual stream or attention head hidden dimension)
    ActivationLocationType.RESID_POST_EMBEDDING: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.RESIDUAL_STREAM_CHANNELS,
    ),
    ActivationLocationType.RESID_DELTA_ATTN: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.RESIDUAL_STREAM_CHANNELS,
    ),
    ActivationLocationType.RESID_POST_ATTN: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.RESIDUAL_STREAM_CHANNELS,
    ),
    ActivationLocationType.RESID_DELTA_MLP: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.RESIDUAL_STREAM_CHANNELS,
    ),
    ActivationLocationType.RESID_POST_MLP: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.RESIDUAL_STREAM_CHANNELS,
    ),
    ActivationLocationType.RESID_POST_MLP_LAYER_NORM: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.RESIDUAL_STREAM_CHANNELS,
    ),
    ActivationLocationType.RESID_POST_ATTN_LAYER_NORM: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.RESIDUAL_STREAM_CHANNELS,
    ),
    ActivationLocationType.RESID_POST_FINAL_LAYER_NORM: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.RESIDUAL_STREAM_CHANNELS,
    ),
    ActivationLocationType.MLP_INPUT_LAYER_NORM_SCALE: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.SINGLETON,
    ),
    ActivationLocationType.ATTN_INPUT_LAYER_NORM_SCALE: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.SINGLETON,
    ),
    ActivationLocationType.RESID_FINAL_LAYER_NORM_SCALE: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.SINGLETON,
    ),
    ActivationLocationType.ATTN_QUERY: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.ATTN_HEADS,
        Dimension.QUERY_AND_KEY_CHANNELS,
    ),
    ActivationLocationType.ATTN_KEY: (
        Dimension.ATTENDED_TO_SEQUENCE_TOKENS,
        Dimension.ATTN_HEADS,
        Dimension.QUERY_AND_KEY_CHANNELS,
    ),
    ActivationLocationType.ATTN_VALUE: (
        Dimension.ATTENDED_TO_SEQUENCE_TOKENS,
        Dimension.ATTN_HEADS,
        Dimension.VALUE_CHANNELS,
    ),
    ActivationLocationType.ATTN_QK_LOGITS: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.ATTENDED_TO_SEQUENCE_TOKENS,
        Dimension.ATTN_HEADS,
    ),
    ActivationLocationType.ATTN_QK_PROBS: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.ATTENDED_TO_SEQUENCE_TOKENS,
        Dimension.ATTN_HEADS,
    ),
    ActivationLocationType.ATTN_WEIGHTED_SUM_OF_VALUES: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.ATTN_HEADS,
        Dimension.VALUE_CHANNELS,
    ),
    ActivationLocationType.MLP_PRE_ACT: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.MLP_ACTS,
    ),
    ActivationLocationType.MLP_POST_ACT: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.MLP_ACTS,
    ),
    ActivationLocationType.ONLINE_AUTOENCODER_LATENT: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.AUTOENCODER_LATENTS,
    ),
    ActivationLocationType.ONLINE_MLP_AUTOENCODER_LATENT: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.AUTOENCODER_LATENTS,
    ),
    ActivationLocationType.ONLINE_ATTENTION_AUTOENCODER_LATENT: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.AUTOENCODER_LATENTS,
    ),
    ActivationLocationType.ONLINE_MLP_AUTOENCODER_ERROR: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.MLP_ACTS,
    ),
    ActivationLocationType.ONLINE_RESIDUAL_MLP_AUTOENCODER_ERROR: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.RESIDUAL_STREAM_CHANNELS,
    ),
    ActivationLocationType.ONLINE_RESIDUAL_ATTENTION_AUTOENCODER_ERROR: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.RESIDUAL_STREAM_CHANNELS,
    ),
    ActivationLocationType.LOGITS: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.VOCAB_SIZE,
    ),
}
weight_shape_by_location_type: dict[WeightLocationType, tuple[Dimension, ...]] = {
    # this is a standard convention for the shape of weight tensors. All weights by convention have
    # the privileged basis at the top of the hierarchy first, if applicable (e.g. attention heads), and the
    # remaining dimensions are ordered: input, then output. Some tensors (e.g. biases, layernorm parameters) have only
    # a single dimension.
    WeightLocationType.POSITION_EMBEDDING: (
        Dimension.MAX_CONTEXT_LENGTH,
        Dimension.RESIDUAL_STREAM_CHANNELS,
    ),
    WeightLocationType.EMBEDDING: (
        Dimension.VOCAB_SIZE,
        Dimension.RESIDUAL_STREAM_CHANNELS,
    ),
    WeightLocationType.UNEMBEDDING: (
        Dimension.RESIDUAL_STREAM_CHANNELS,
        Dimension.VOCAB_SIZE,
    ),
    WeightLocationType.ATTN_TO_QUERY: (
        Dimension.ATTN_HEADS,
        Dimension.RESIDUAL_STREAM_CHANNELS,
        Dimension.QUERY_AND_KEY_CHANNELS,
    ),
    WeightLocationType.ATTN_TO_KEY: (
        Dimension.ATTN_HEADS,
        Dimension.RESIDUAL_STREAM_CHANNELS,
        Dimension.QUERY_AND_KEY_CHANNELS,
    ),
    WeightLocationType.ATTN_TO_VALUE: (
        Dimension.ATTN_HEADS,
        Dimension.RESIDUAL_STREAM_CHANNELS,
        Dimension.VALUE_CHANNELS,
    ),
    WeightLocationType.ATTN_TO_RESIDUAL: (
        Dimension.ATTN_HEADS,
        Dimension.VALUE_CHANNELS,
        Dimension.RESIDUAL_STREAM_CHANNELS,
    ),
    WeightLocationType.MLP_TO_HIDDEN: (
        Dimension.RESIDUAL_STREAM_CHANNELS,
        Dimension.MLP_ACTS,
    ),
    WeightLocationType.MLP_TO_RESIDUAL: (
        Dimension.MLP_ACTS,
        Dimension.RESIDUAL_STREAM_CHANNELS,
    ),
    WeightLocationType.LAYER_NORM_GAIN_PRE_MLP: (Dimension.RESIDUAL_STREAM_CHANNELS,),
    WeightLocationType.LAYER_NORM_GAIN_PRE_ATTN: (Dimension.RESIDUAL_STREAM_CHANNELS,),
    WeightLocationType.LAYER_NORM_GAIN_FINAL: (Dimension.RESIDUAL_STREAM_CHANNELS,),
    WeightLocationType.LAYER_NORM_BIAS_FINAL: (Dimension.RESIDUAL_STREAM_CHANNELS,),
}
def get_dimension_index_of_weight_location_type(
    weight_location_type: WeightLocationType, dimension: Dimension
) -> int:
    """Returns the index of a dimension within a weight tensor, and raises an error if the
    dimension is found 0 or >1 time (0 can happen, but >1 indicates a bug somewhere)."""
    assert weight_shape_by_location_type[weight_location_type].count(dimension) == 1
    return weight_shape_by_location_type[weight_location_type].index(dimension)
@unique
class PassType(str, Enum):
    FORWARD = "forward"
    BACKWARD = "backward"
    def __repr__(self) -> str:
        return f"'{self.value}'"
@dataclass(frozen=True)
class ActivationLocationTypeAndPassType:
    activation_location_type: ActivationLocationType
    pass_type: PassType
    @property
    def exists_by_default(self) -> bool:
        return self.activation_location_type.exists_by_default

================
File: build/lib/neuron_explainer/models/model_context.py
================
import os
from abc import ABC, abstractmethod
from typing import Any
import tiktoken
import torch
import torch.nn as nn
from neuron_explainer.file_utils import CustomFileHandler
from neuron_explainer.models import Transformer, TransformerConfig
from neuron_explainer.models.inference_engine_type_registry import InferenceEngineType
from neuron_explainer.models.model_component_registry import (
    Dimension,
    LayerIndex,
    WeightLocationType,
    get_dimension_index_of_weight_location_type,
    weight_shape_by_location_type,
)
from neuron_explainer.models.model_registry import get_standard_model_spec
ALLOWED_SPECIAL_TOKENS = {"<|endoftext|>"}
class InvalidTokenException(Exception):
    pass
class ModelContext(ABC):
    def __init__(self, model_name: str, device: torch.device) -> None:
        self.model_name = model_name
        self.device = device
    # takes a WeightLocationType and optional layer
    # returns the tensor
    @abstractmethod
    def _get_weight_helper(
        self,
        location_type: WeightLocationType,
        layer: LayerIndex = None,
        device: torch.device | None = None,
    ) -> torch.Tensor:
        ...
    def get_weight(
        self,
        location_type: WeightLocationType,
        layer: LayerIndex = None,
        normalize_dim: Dimension | None = None,
        device: torch.device | None = None,
    ) -> torch.Tensor:
        """Returns the specified weights, with shape checking and optional normalization.
        Tensors returned by this method are not cloned, so please be sure not to perform in-place
        edits on them!
        """
        assert (
            location_type in weight_shape_by_location_type
        ), f"location_type_str {location_type} not found"
        weight = self._get_weight_helper(
            location_type=location_type, layer=layer, device=device or self.device
        )
        if normalize_dim is not None:
            weight = nn.functional.normalize(
                weight,
                dim=get_dimension_index_of_weight_location_type(location_type, normalize_dim),
            )
        weight_shape_spec = weight_shape_by_location_type[location_type]
        expected_shape = self.get_shape_from_spec(weight_shape_spec)
        assert (
            weight.shape == expected_shape
        ), f"Expected shape {expected_shape} but got {weight.shape}"
        # We don't want to return tensors that have gradients enabled, so we detach. Ideally we'd
        # also clone since we don't want callers to inadvertently edit the weights, but doing so
        # uses a lot of memory, so instead we just ask politely in the docstring.
        return weight.detach()
    # get Encoding -> call this in the base class
    @abstractmethod
    def get_encoding(self) -> tiktoken.Encoding:
        ...
    def encode(self, string: str) -> list[int]:
        return self.get_encoding().encode(string, allowed_special=ALLOWED_SPECIAL_TOKENS)
    def decode_token(self, token: int) -> str:
        return self.get_encoding().decode([token])
    def decode(self, token_list: list[int]) -> str:
        return self.get_encoding().decode(token_list)
    def encode_token_str(self, token_str: str) -> int:
        token_int_list = self.encode(token_str)
        if len(token_int_list) != 1:
            raise InvalidTokenException(
                f"'{token_str}' decoded to {token_int_list}; wanted exactly 1 token"
            )
        return token_int_list[0]
    @abstractmethod
    def get_dim_size(self, model_dimension_spec: Dimension) -> int:
        ...
    def get_shape_from_spec(self, shape_spec: tuple[Dimension, ...]) -> tuple[int, ...]:
        expected_shape: tuple[int, ...] = tuple(
            self.get_dim_size(dimension_spec) if dimension_spec != Dimension.SINGLETON else 1
            for dimension_spec in shape_spec
        )
        return expected_shape
    @abstractmethod
    def get_or_create_model(self) -> Transformer:
        """Returns an instantiated model which can be used to run forward passes.
        The first call to this method results in a new model being created. Subsequent calls return
        the same cached model instance.
        """
        ...
    def decode_token_list(self, token_list: list[int]) -> list[str]:
        return [self.decode_token(token=token) for token in token_list]
    def encode_token_str_list(self, token_str_list: list[str]) -> list[int]:
        return [self.encode_token_str(token_str=token_str) for token_str in token_str_list]
    @classmethod
    def from_model_type(
        cls,
        model_type: str,
        inference_engine_type: InferenceEngineType = InferenceEngineType.STANDARD,
        **kwargs: Any,
    ) -> "ModelContext":
        device = kwargs.pop("device", get_default_device())
        if inference_engine_type == InferenceEngineType.STANDARD:
            return StandardModelContext(model_name=model_type, device=device, **kwargs)
        else:
            raise ValueError(f"Unsupported inference_engine_type {inference_engine_type}")
    @property
    def n_neurons(self) -> int:
        return self.get_dim_size(Dimension.MLP_ACTS)
    @property
    def n_attention_heads(self) -> int:
        return self.get_dim_size(Dimension.ATTN_HEADS)
    @property
    def n_layers(self) -> int:
        return self.get_dim_size(Dimension.LAYERS)
    @property
    def n_residual_stream_channels(self) -> int:
        return self.get_dim_size(Dimension.RESIDUAL_STREAM_CHANNELS)
    @property
    def n_vocab(self) -> int:
        return self.get_dim_size(Dimension.VOCAB_SIZE)
    @property
    def n_context(self) -> int:
        return self.get_dim_size(Dimension.MAX_CONTEXT_LENGTH)
    @abstractmethod
    def get_model_config_as_dict(self) -> dict[str, Any]:
        ...
# Note: If you're seeing mysterious crashes while running on a MacBook, try switching from "mps" to
# "cpu".
def get_default_device() -> torch.device:
    # TODO: Figure out why test_interactive_model.py crashes on the "mps" backend, then remove
    # this workaround.
    is_pytest = "PYTEST_CURRENT_TEST" in os.environ
    if torch.cuda.is_available():
        return torch.device("cuda", 0)
    elif torch.backends.mps.is_available() and not is_pytest:
        return torch.device("mps", 0)
    else:
        return torch.device("cpu")
class StandardModelContext(ModelContext):
    def __init__(self, model_name: str, device: torch.device | None = None) -> None:
        if device is None:
            device = get_default_device()
        super().__init__(model_name=model_name, device=device)
        self._model_spec = get_standard_model_spec(self.model_name)
        self.load_path = self._model_spec.model_path
        self._config = TransformerConfig.load(f"{self.load_path}/config.json")
        # Once a transformer has been created via get_or_create_model, we cache it. Subsequent calls
        # to get_or_create_model return the cached instance.
        self._cached_transformer: Transformer | None = None
    @classmethod
    def from_model_type(
        cls,
        model_type: str,
        inference_engine_type: InferenceEngineType = InferenceEngineType.STANDARD,
        **kwargs: Any,
    ) -> ModelContext:  # specifically a StandardModelContext, but to satisfy mypy
        assert (
            inference_engine_type == InferenceEngineType.STANDARD
        ), "don't set a different inference_engine_type kwarg here"
        model_context = super().from_model_type(
            model_type=model_type, inference_engine_type=InferenceEngineType.STANDARD, **kwargs
        )
        assert isinstance(model_context, StandardModelContext)
        return model_context
    def get_dim_size(self, model_dimension_spec: Dimension) -> int:
        # TODO(sbills): This should really be a match statement.
        dimension_by_dimension_spec: dict[Dimension, int] = {
            Dimension.MAX_CONTEXT_LENGTH: self._config.ctx_window,
            Dimension.RESIDUAL_STREAM_CHANNELS: self._config.d_model,
            Dimension.VOCAB_SIZE: self.get_encoding().n_vocab,
            Dimension.ATTN_HEADS: self._config.n_heads,
            Dimension.QUERY_AND_KEY_CHANNELS: self._config.d_head_qk,
            Dimension.VALUE_CHANNELS: self._config.d_head_v,
            Dimension.MLP_ACTS: self._config.d_ff,
            Dimension.MLP_ACTS: self._config.d_ff,
            Dimension.LAYERS: self._config.n_layers,
        }
        return dimension_by_dimension_spec[model_dimension_spec]
    def _get_weight_helper(
        self,
        location_type: WeightLocationType,
        layer: LayerIndex = None,
        device: torch.device | None = None,
    ) -> torch.Tensor:
        info_by_type: dict[WeightLocationType, dict] = {
            WeightLocationType.MLP_TO_HIDDEN: dict(
                part=f"xf_layers.{layer}.mlp.in_layer.weight",
                reshape="hr->rh",
            ),
            WeightLocationType.MLP_TO_RESIDUAL: dict(
                part=f"xf_layers.{layer}.mlp.out_layer.weight",
                reshape="rh->hr",
            ),
            WeightLocationType.EMBEDDING: dict(
                part="tok_embed.weight",
            ),
            WeightLocationType.UNEMBEDDING: dict(
                part="unembed.weight",
                reshape="vr->rv",
            ),
            WeightLocationType.POSITION_EMBEDDING: dict(
                part="pos_embed.weight",
            ),
            WeightLocationType.ATTN_TO_QUERY: dict(
                part=f"xf_layers.{layer}.attn.q_proj.weight",
                split=(0, self._config.n_heads),
                reshape="hqr->hrq",
            ),
            WeightLocationType.ATTN_TO_KEY: dict(
                part=f"xf_layers.{layer}.attn.k_proj.weight",
                split=(0, self._config.n_heads),
                reshape="hkr->hrk",
            ),
            WeightLocationType.ATTN_TO_VALUE: dict(
                part=f"xf_layers.{layer}.attn.v_proj.weight",
                split=(0, self._config.n_heads),
                reshape="hvr->hrv",
            ),
            WeightLocationType.ATTN_TO_RESIDUAL: dict(
                part=f"xf_layers.{layer}.attn.out_proj.weight",
                split=(1, self._config.n_heads),
                reshape="rhv->hvr",
            ),
            WeightLocationType.LAYER_NORM_GAIN_FINAL: dict(
                part="final_ln.weight",
                broadcast=True,
            ),
            WeightLocationType.LAYER_NORM_BIAS_FINAL: dict(
                part="final_ln.bias",
            ),
            WeightLocationType.LAYER_NORM_GAIN_PRE_ATTN: dict(
                part=f"xf_layers.{layer}.ln_1.weight",
            ),
            WeightLocationType.LAYER_NORM_GAIN_PRE_MLP: dict(
                part=f"xf_layers.{layer}.ln_2.weight",
            ),
        }
        info = info_by_type.get(location_type)
        if info is None:
            raise NotImplementedError(f"Unsupported weight location type: {location_type}")
        part = info["part"]
        split = info.get("split")
        reshape = info.get("reshape")
        if self._cached_transformer is None:
            with CustomFileHandler(f"{self.load_path}/model_pieces/{part}.pt", "rb") as f:
                weight = torch.load(f, map_location=device or self.device)
        else:
            weight = self._cached_transformer.state_dict()[part].to(device or self.device)
        if split is not None:
            (dim_split, n_split) = split
            w_shape = list(weight.shape)
            w_shape_new = (
                w_shape[:dim_split]
                + [n_split, w_shape[dim_split] // n_split]
                + w_shape[dim_split + 1 :]
            )
            weight = weight.reshape(*w_shape_new)
        if reshape is not None:
            weight = torch.einsum(reshape, weight)
        # Some tensors are sometimes stored with a subset of dimensions and then broadcasted in the model
        # E.g. the final layer norm gain is stored as a scalar
        # Broadcast flag indicates that we should broadcast them to the expected shape
        broadcast = info.get("broadcast")
        if broadcast is True:
            expected_shape = self.get_shape_from_spec(weight_shape_by_location_type[location_type])
            weight = weight.expand(expected_shape)
        return weight
    def get_or_create_model(
        self,
        device: torch.device | None = None,
        simplify: bool = False,
    ) -> Transformer:
        if self._cached_transformer is None:
            self._cached_transformer = Transformer.load(
                self.load_path, simplify=simplify, device=device or self.device
            )
        return self._cached_transformer
    def get_encoding(self) -> tiktoken.Encoding:
        return tiktoken.get_encoding(self._config.enc)
    def get_model_config_as_dict(self) -> dict[str, Any]:
        return self._config.to_dict()
class StubModelContext(ModelContext):
    # TODO: maybe make a unified interface for the Config objects of ModelContext objects, and
    # have this be a StubConfig instead of a StubContext
    """This is a fake model context object for use in testing. It just works as a holder for
    a mapping from model dimension to size (int)."""
    def __init__(
        self,
        size_by_model_dimension_spec: dict[Dimension, int],
    ):
        super().__init__(model_name="stub", device=torch.device("cpu"))
        self._size_by_model_dimension_spec = size_by_model_dimension_spec
    def _get_weight_helper(
        self,
        location_type: WeightLocationType,
        layer: LayerIndex = None,
        device: torch.device | None = None,
    ) -> torch.Tensor:
        raise NotImplementedError
    def get_encoding(self) -> tiktoken.Encoding:
        raise NotImplementedError
    def get_or_create_model(self) -> Transformer:
        raise NotImplementedError
    def get_model_config_as_dict(self) -> dict[str, Any]:
        raise NotImplementedError
    def get_dim_size(self, model_dimension_spec: Dimension) -> int:
        if model_dimension_spec in self._size_by_model_dimension_spec:
            return self._size_by_model_dimension_spec[model_dimension_spec]
        else:
            raise NotImplementedError
# TODO: make this robust to whether the transformer is 'simplified' in our terminology
# once the .simplify() operation is extended to cover final layer norm gain
def get_unembedding_with_ln_gain(model_context: ModelContext) -> torch.Tensor:
    """
    returns an unembedding matrix multiplied by the layer norm gain (a d_model-dimensional vector)
    for the final layer
    """
    Unemb_without_ln_gain = model_context.get_weight(
        location_type=WeightLocationType.UNEMBEDDING,
        device=model_context.device,
    )
    ln_gain_final = model_context.get_weight(
        location_type=WeightLocationType.LAYER_NORM_GAIN_FINAL,
        device=model_context.device,
    )
    return torch.einsum("ov,o->ov", Unemb_without_ln_gain, ln_gain_final)
def get_embedding(model_context: ModelContext) -> torch.Tensor:
    """
    returns an embedding matrix. Note that there is no layer norm in between the embedding tensor and
    the residual stream
    """
    return model_context.get_weight(
        location_type=WeightLocationType.EMBEDDING,
        device=model_context.device,
    )

================
File: build/lib/neuron_explainer/models/model_registry.py
================
from dataclasses import dataclass
import torch
from neuron_explainer.activations.derived_scalars import DerivedScalarType
from neuron_explainer.models import Transformer
from neuron_explainer.models.autoencoder_context import (
    AutoencoderConfig,
    AutoencoderContext,
    AutoencoderSpec,
)
@dataclass(frozen=True)
class StandardModelSpec:
    model_path: str  # checkpoint path
_MODEL_SPECS: dict[str, StandardModelSpec] = {
    # GPT-2 series
    "gpt2-small": StandardModelSpec(
        model_path="https://openaipublic.blob.core.windows.net/neuron-explainer/subject-models/gpt2/small"
    ),
    "gpt2-medium": StandardModelSpec(
        model_path="https://openaipublic.blob.core.windows.net/neuron-explainer/subject-models/gpt2/medium"
    ),
    "gpt2-large": StandardModelSpec(
        model_path="https://openaipublic.blob.core.windows.net/neuron-explainer/subject-models/gpt2/large"
    ),
    "gpt2-xl": StandardModelSpec(
        model_path="https://openaipublic.blob.core.windows.net/neuron-explainer/subject-models/gpt2/xl"
    ),
}
_AUTOENCODER_SPECS: dict[str, dict[str, AutoencoderSpec]] = {
    "gpt2-small": {
        # released December 2023
        "ae-mlp-post-act-v1": AutoencoderSpec(
            dst=DerivedScalarType.MLP_POST_ACT,
            autoencoder_path_by_layer_index={
                layer_index: f"https://openaipublic.blob.core.windows.net/sparse-autoencoder/gpt2-small/mlp_post_act/autoencoders/{layer_index}.pt"
                for layer_index in range(12)
            },
        ),
        "ae-resid-delta-mlp-v1": AutoencoderSpec(
            dst=DerivedScalarType.RESID_DELTA_MLP,
            autoencoder_path_by_layer_index={
                layer_index: f"https://openaipublic.blob.core.windows.net/sparse-autoencoder/gpt2-small/resid_delta_mlp/autoencoders/{layer_index}.pt"
                for layer_index in range(12)
            },
        ),
        # released March 2024
        "ae-mlp-post-act-v4": AutoencoderSpec(
            dst=DerivedScalarType.MLP_POST_ACT,
            autoencoder_path_by_layer_index={
                layer_index: f"https://openaipublic.blob.core.windows.net/sparse-autoencoder/gpt2-small/mlp_post_act_v4/autoencoders/{layer_index}.pt"
                for layer_index in range(12)
            },
        ),
        "ae-resid-delta-mlp-v4": AutoencoderSpec(
            dst=DerivedScalarType.RESID_DELTA_MLP,
            autoencoder_path_by_layer_index={
                layer_index: f"https://openaipublic.blob.core.windows.net/sparse-autoencoder/gpt2-small/resid_delta_mlp_v4/autoencoders/{layer_index}.pt"
                for layer_index in range(12)
            },
        ),
        "ae-resid-delta-attn-v4": AutoencoderSpec(
            dst=DerivedScalarType.RESID_DELTA_ATTN,
            autoencoder_path_by_layer_index={
                layer_index: f"https://openaipublic.blob.core.windows.net/sparse-autoencoder/gpt2-small/resid_delta_attn_v4/autoencoders/{layer_index}.pt"
                for layer_index in range(12)
            },
        ),
    },
}
def list_autoencoder_names(model_name: str = "gpt2-small") -> list[str]:
    return list(_AUTOENCODER_SPECS[model_name].keys())
def get_standard_model_spec(model_name: str) -> StandardModelSpec:
    return _MODEL_SPECS[model_name]
def load_standard_transformer(model_name: str, device: torch.device | None = None) -> Transformer:
    print(f"Loading standard model {model_name}...")
    model_spec = get_standard_model_spec(model_name)
    return load_standard_transformer_from_model_spec(model_spec, device=device)
def load_standard_transformer_from_model_spec(
    model_spec: StandardModelSpec, device: torch.device | None = None
) -> Transformer:
    return Transformer.load(
        model_spec.model_path,
        dtype=torch.float32,
        device=device,
    )
def make_autoencoder_context(
    model_name: str,
    autoencoder_name: str,
    device: torch.device,
    omit_dead_latents: bool = False,
) -> AutoencoderContext:
    try:
        autoencoder_spec = _AUTOENCODER_SPECS[model_name][autoencoder_name]
    except KeyError:
        raise ValueError(
            f"No autoencoder spec found for model {model_name} and autoencoder {autoencoder_name}. "
            f"Available autoencoders for model {model_name} are: {list(_AUTOENCODER_SPECS[model_name].keys())}"
        )
    autoencoder_config = AutoencoderConfig.from_spec(autoencoder_spec)
    autoencoder_context = AutoencoderContext(
        autoencoder_config=autoencoder_config,
        device=device,
        omit_dead_latents=omit_dead_latents,
    )
    return autoencoder_context

================
File: build/lib/neuron_explainer/models/transformer.py
================
import json
import os.path as osp
import pickle
from concurrent.futures import ThreadPoolExecutor
from copy import deepcopy
from dataclasses import asdict, dataclass
from functools import cache
from typing import Any, Self, Union
import numpy as np
import tiktoken
import torch
import torch.nn as nn
from torch import Tensor
from torch.distributions.categorical import Categorical
from torch.utils.checkpoint import checkpoint
from neuron_explainer.file_utils import CustomFileHandler, copy_to_local_cache, file_exists
from neuron_explainer.models.hooks import (
    AttentionHooks,
    MLPHooks,
    NormalizationHooks,
    TransformerHooks,
)
# for static analysis
Device = Union[torch.device, str]
# NOTE: some code from this file related to attention, MLP, and layernorm operations is copy-pasted in
# neuron_explainer/activations/derived_scalars/reconstituted.py; if those operations change here, they should correspondingly
# be changed in that file.
class SerializableDataclass:
    def to_dict(self) -> dict:
        return asdict(self)
    @classmethod
    def from_dict(cls, d) -> Self:
        return cls(**d)
    def save(self, path: str) -> None:
        if path.endswith((".pkl", ".pickle")):
            with CustomFileHandler(path, "wb") as f:
                pickle.dump(self.to_dict(), f)
        elif path.endswith(".json"):
            with CustomFileHandler(path, "w") as f:
                json.dump(self.to_dict(), f)
        else:
            raise ValueError(f"Unknown file extension for {path}")
    @classmethod
    def load(cls, path: str) -> Self:
        if path.endswith((".pkl", ".pickle")):
            with CustomFileHandler(path, "rb") as f:
                return cls.from_dict(pickle.load(f))
        elif path.endswith(".json"):
            with CustomFileHandler(path, "r") as f:
                return cls.from_dict(json.load(f))
        else:
            raise ValueError(f"Unknown file extension for {path}")
@dataclass
class TransformerConfig(SerializableDataclass):
    enc: str = "gpt2"
    ctx_window: int = 1024
    d_model: int = 256
    n_layers: int = 2
    # attn
    m_attn: float = 1
    n_heads: int = 8
    # mlp
    m_mlp: float = 4
    @property
    def d_ff(self) -> int:
        return int(self.d_model * self.m_mlp)
    @property
    def d_attn_qk(self) -> int:
        return int(self.d_model * self.m_attn)
    @property
    def d_attn_v(self) -> int:
        return int(self.d_model * self.m_attn)
    @property
    def d_head_qk(self) -> int:
        return safe_div(self.d_attn_qk, self.n_heads)
    @property
    def d_head_v(self) -> int:
        return safe_div(self.d_attn_v, self.n_heads)
def default_device() -> torch.device:
    return torch.device("cuda" if torch.cuda.is_available() else "cpu")
def safe_div(numerator: int, denominator: int) -> int:
    assert numerator % denominator == 0
    return numerator // denominator
# ====================
# Attention utilities
# ====================
@cache
def causal_attn_mask(size: int, device: Device = "cpu") -> Tensor:
    return torch.tril(torch.ones(size, size)).bool().to(device)
def split_heads(Z: Tensor, n_heads: int) -> Tensor:
    batch, seq, d_attn = Z.shape
    return Z.reshape(batch, seq, n_heads, d_attn // n_heads)
def merge_heads(Z: Tensor) -> Tensor:
    batch, seq, n_heads, d_head = Z.shape
    return Z.reshape(batch, seq, n_heads * d_head)
# ===================================
# MLP utilities
# ===================================
def gelu(x: Tensor) -> Tensor:
    return 0.5 * x * (1 + torch.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * torch.pow(x, 3))))
    # return x * torch.sigmoid(1.702 * x)
# ========================================
# Sampling, padding and related utilities
# ========================================
def prep_input_and_right_pad_for_forward_pass(
    X: list[list[int]], device: Device = "cpu"
) -> tuple[Tensor, Tensor]:
    # Helper function. The two tensors returned by this function are suitable to be passed to
    # Transformer.forward.
    return prep_input_and_pad(X, "right", device)
def prep_input_and_pad(
    X: list[list[int]], pad_side: str, device: Device = "cpu"
) -> tuple[Tensor, Tensor]:
    # X is a list of tokenized prompts; prompts may have unequal lengths. This function will
    # left-pad X by putting "-1" in all the slots where a prompt is shorter than the longest prompt.
    # Then convert X into a tensor of int tokens. Then build the pad tensor by looking for the
    # "-1"s. Then fill the "-1"s in X with "0"s so the embedding layer doesn't get upset.
    max_len = max([len(prompt) for prompt in X])
    def pad(x):
        padding = [-1] * (max_len - len(x))
        if pad_side == "left":
            return padding + x
        elif pad_side == "right":
            return x + padding
        else:
            raise ValueError(f"pad_side must be 'left' or 'right', not {pad_side}")
    X_tensor = torch.LongTensor([pad(prompt) for prompt in X]).to(device)
    pad = X_tensor == -1
    X_tensor = torch.where(X_tensor == -1, 0, X_tensor)
    return X_tensor, pad
def prep_pos_from_pad_and_prev_lens(pad: Tensor, prev_lens: Tensor) -> Tensor:
    # pad has shape b x s, prev_lens has shape b x 1.
    # For position embedding, we need a tensor of shape (b x s) whose
    # entries are the positions of X in the sequence. When sampling with
    # prompts of unequal length, X is left padded with pad tokens. The
    # position tensor needs to take that into account.
    pos = torch.logical_not(pad).long().cumsum(dim=-1) - 1
    pos = torch.where(pos == -1, 0, pos)
    return pos + prev_lens
def nucleus_sample(logits: Tensor, top_p: float) -> Tensor:
    # top_p in [0,1] is the total probability mass of top outputs.
    # based on https://nn.labml.ai/sampling/nucleus.html
    # input shape: [..., n_vocab] -> output shape: [...]
    sorted_logits, idxs = torch.sort(logits, dim=-1, descending=True)
    sorted_probs = torch.softmax(sorted_logits, dim=-1)
    cum_probs = torch.cumsum(sorted_probs, dim=-1)
    # logic to ensure there is always at least one token with nonzero
    # probability when selecting nucleus.
    p0 = cum_probs[..., 0]
    top_p = torch.where(p0 > top_p, p0, top_p)[..., None]
    # sampling
    do_not_sample = cum_probs > top_p
    sorted_logits = sorted_logits.masked_fill(do_not_sample, float("-inf"))
    dist = Categorical(logits=sorted_logits)
    samples = dist.sample()
    tokens = idxs.gather(-1, samples.unsqueeze(-1)).squeeze(-1)
    return tokens
# ===============
# Layer Norm
# ===============
class Norm(nn.Module):
    """LayerNorm reimplementation with hooks."""
    def __init__(
        self,
        size: int,
        eps: float = 1e-5,
        device: Device | None = None,
        dtype: torch.dtype | None = None,
    ):
        super().__init__()
        kwargs = {"device": device, "dtype": dtype}
        self.size = size
        self.weight = nn.Parameter(torch.empty(size, **kwargs))  # type: ignore[arg-type]
        self.bias = nn.Parameter(torch.empty(size, **kwargs))  # type: ignore[arg-type]
        self.eps = eps
    def forward(self, x: Tensor, hooks: NormalizationHooks = None) -> Tensor:
        if hooks is None:
            hooks = NormalizationHooks()
        # always do norm in fp32
        orig_dtype = x.dtype
        x = x.float()
        x = x - x.mean(axis=-1, keepdim=True)  # [batch, pos, length]
        x = hooks.post_mean_subtraction(x)
        scale = torch.sqrt((x**2).mean(dim=-1, keepdim=True) + self.eps)
        scale = hooks.scale(scale)
        x = x / scale
        x = hooks.post_scale(x)
        ret = x * self.weight + self.bias
        return ret.to(orig_dtype)
def apply_layernorm_foldin(ln: Norm, linears: list[nn.Linear]) -> None:
    # folds in a layernorm weight/bias into the next linear layer.
    # ln(x) = W_ln * (x - x.mean())/(x.std()) + b_ln
    # linear(ln(x)) = W_linear * (W_ln * (x - x.mean())/(x.std()) + b_ln) + b_linear
    W_ln = ln.weight.float()
    b_ln = ln.bias.float()
    for linear in linears:
        W_linear = linear.weight.float()
        b_linear = linear.bias.float()
        W_composed = W_linear * W_ln[None, :]
        b_composed = None
        b_composed = b_linear + W_linear @ b_ln
        # should only copy after new weights are calculated
        linear.weight.data.copy_(W_composed)
        linear.bias.data.copy_(b_composed)
    ln.weight.data[:] = 1
    ln.bias.data[:] = 0
# ===========================================
# Attention layers and associated components
# ===========================================
@dataclass
class KeyValueCache:
    """KV cache to save on compute"""
    K_cache: Tensor | None = None  # b x s_old x d
    V_cache: Tensor | None = None  # b x s_old x d
    pad_cache: Tensor | None = None  # b x s_old
    def update(self, K: Tensor, V: Tensor, pad: Tensor):
        # K, V have shape: b x (s_new - s_old) x d
        # pad has shape: b x (s_new - s_old)
        new = self.K_cache is None
        self.K_cache = K if new else torch.cat([self.K_cache, K], dim=1)
        self.V_cache = V if new else torch.cat([self.V_cache, V], dim=1)
        self.pad_cache = pad if new else torch.cat([self.pad_cache, pad], dim=1)
        return self.K_cache, self.V_cache, self.pad_cache
class MultiHeadedDotProductSelfAttention(nn.Module):
    """A configurable multi-headed dot product attention layer."""
    def __init__(
        self,
        cfg: TransformerConfig,
        layer_idx: int,
        device: Device | None = None,
        dtype: torch.dtype | None = None,
    ):
        super().__init__()
        self.n_heads = cfg.n_heads
        # make layers
        kwargs = {"device": device, "dtype": dtype}
        self.q_proj = nn.Linear(cfg.d_model, cfg.d_attn_qk, **kwargs)
        self.k_proj = nn.Linear(cfg.d_model, cfg.d_attn_qk, **kwargs)
        self.v_proj = nn.Linear(cfg.d_model, cfg.d_attn_v, **kwargs)
        self.out_proj = nn.Linear(cfg.d_attn_v, cfg.d_model, **kwargs)
        self.qk_scale = 1 / np.sqrt(np.sqrt(cfg.d_head_qk))
        self.cfg = cfg
    def forward(
        self,
        X: Tensor,
        kv_cache: KeyValueCache | None = None,
        pad: Tensor | None = None,
        hooks: AttentionHooks = AttentionHooks(),
    ) -> tuple[Tensor, KeyValueCache]:
        Q = self.q_proj(X)
        K = self.k_proj(X)
        V = self.v_proj(X)
        # update KV cache
        if kv_cache is None:
            kv_cache = KeyValueCache()
        K, V, pad = kv_cache.update(K, V, pad)
        # split apart heads, rescale QK
        Q = split_heads(Q, self.n_heads) * self.qk_scale
        Q = hooks.q(Q)  # bshd
        K = split_heads(K, self.n_heads) * self.qk_scale
        K = hooks.k(K)  # bshd
        V = split_heads(V, self.n_heads)
        V = hooks.v(V)  # bshd
        # useful for calculations below
        n_queries, n_keys = Q.shape[1], K.shape[1]
        # softmax multi-headed dot product attention
        pre_softmax = torch.einsum("bqhd,bkhd -> bhqk", Q, K)
        # apply causal attention mask
        M = causal_attn_mask(n_keys, device=X.device)
        M = M[None, None, -n_queries:]  # make M broadcastable to batch, head
        pre_softmax = pre_softmax.masked_fill(torch.logical_not(M), float("-inf"))
        # apply pad mask
        if pad is not None and torch.any(pad):
            # we only mask out pad tokens for non-pad query tokens
            # (because masking all pad tokens => empty rows => NaNs later)
            pad_mask = torch.bitwise_xor(pad[:, None, :], pad[:, :, None])
            # make pad broadcastable on head dim, and slice for current queries only
            pad_mask = pad_mask[:, None, -n_queries:]
            # apply pad mask
            pre_softmax = pre_softmax.masked_fill(pad_mask, float("-inf"))
        pre_softmax = torch.einsum("bhqk->bqkh", pre_softmax)
        pre_softmax = hooks.qk_logits(pre_softmax)
        pre_softmax = pre_softmax.float()  # for numerical stability
        if hooks.qk_softmax_denominator.is_empty():
            attn = torch.softmax(pre_softmax, dim=-2)
        else:
            # factor out softmax in order to hook
            pre_softmax_max = torch.max(pre_softmax, -2, keepdim=True)[0].detach()
            numerator = torch.exp(pre_softmax - pre_softmax_max)
            denominator = numerator.sum(dim=-2, keepdim=True)
            denominator = hooks.qk_softmax_denominator(denominator)
            attn = numerator / denominator
        attn = attn.to(Q.dtype)
        attn = hooks.qk_probs(attn)
        out = torch.einsum("bqkh,bkhd->bqhd", attn, V)
        out = hooks.v_out(out)
        out = merge_heads(out)  # concatenate results from all heads
        # final output projection
        return self.out_proj(out), kv_cache
# =====================================
# MLP layers and associated components
# =====================================
class MLP(nn.Module):
    """An MLP for a transformer is a simple two-layer network."""
    def __init__(
        self, cfg: TransformerConfig, device: Device | None = None, dtype: torch.dtype | None = None
    ):
        super().__init__()
        kwargs = {"device": device, "dtype": dtype}
        self.in_layer = nn.Linear(cfg.d_model, cfg.d_ff, **kwargs)
        self.out_layer = nn.Linear(cfg.d_ff, cfg.d_model, **kwargs)
        self.act = gelu
    def forward(self, X: Tensor, hooks: MLPHooks = MLPHooks()) -> Tensor:
        pre = self.in_layer(X)
        pre = hooks.pre_act(pre)
        a = self.act(pre)
        a = hooks.post_act(a, out_layer=self.out_layer)
        out = self.out_layer(a)
        return out
# =============
# Transformers
# =============
class TransformerLayer(nn.Module):
    def __init__(
        self,
        cfg: TransformerConfig,
        layer_idx: int,
        device: Device | None = None,
        dtype: torch.dtype | None = None,
    ):
        super().__init__()
        kwargs = {"device": device, "dtype": dtype}
        self.cfg = cfg
        self.attn = MultiHeadedDotProductSelfAttention(cfg, layer_idx, **kwargs)
        self.mlp = MLP(cfg, **kwargs)
        self.ln_1 = Norm(cfg.d_model, **kwargs)
        self.ln_2 = Norm(cfg.d_model, **kwargs)
        self.layer_idx = layer_idx
    def simplify(self) -> None:
        ln_1_linears: list[Any] = [
            self.attn.q_proj,
            self.attn.k_proj,
            self.attn.v_proj,
        ]
        apply_layernorm_foldin(self.ln_1, ln_1_linears)
        ln_2_linears: list[Any] = [self.mlp.in_layer]
        apply_layernorm_foldin(self.ln_2, ln_2_linears)
    def attn_block(
        self, X: Tensor, kv_cache: KeyValueCache | None, pad: Tensor | None, hooks: TransformerHooks
    ) -> Tensor:
        ln_X = self.ln_1(X, hooks.resid.torso.ln_attn)
        ln_X = hooks.resid.torso.post_ln_attn(ln_X)
        attn_delta, kv_cache = self.attn(ln_X, kv_cache, pad, hooks.attn)
        attn_delta = hooks.resid.torso.delta_attn(attn_delta)
        return attn_delta, kv_cache
    def mlp_block(self, X: Tensor, hooks: TransformerHooks) -> Tensor:
        ln_X = self.ln_2(X, hooks.resid.torso.ln_mlp)
        ln_X = hooks.resid.torso.post_ln_mlp(ln_X)
        mlp_delta = self.mlp(ln_X, hooks.mlp)
        mlp_delta = hooks.resid.torso.delta_mlp(mlp_delta)
        return mlp_delta
    def forward(
        self,
        X: Tensor,
        kv_cache: KeyValueCache | None = None,
        pad: Tensor | None = None,
        hooks: TransformerHooks = TransformerHooks(),
    ) -> tuple[Tensor, KeyValueCache]:
        attn_delta, kv_cache = self.attn_block(X, kv_cache, pad, hooks)
        X = X + attn_delta
        X = hooks.resid.torso.post_attn(X)
        mlp_delta = self.mlp_block(X, hooks)
        X = X + mlp_delta
        X = hooks.resid.torso.post_mlp(X)
        return X, kv_cache
class HiddenState:
    """A hidden state for a transformer. Tracks prompt lengths and KV caches."""
    def __init__(self, n_layers: int):
        self.prev_lens = 0
        self.kv_caches = [None for _ in range(n_layers)]
    def set_prev_lens(self, prev_lens) -> None:
        self.prev_lens = prev_lens
    def __getitem__(self, idx: int):
        return self.kv_caches[idx]
    def __setitem__(self, idx: int, value: KeyValueCache | None):
        self.kv_caches[idx] = value
class Transformer(nn.Module):
    def __init__(
        self,
        cfg: TransformerConfig,
        # recomputing is optional, and it trades off compute for memory.
        recompute: bool = False,
        device: Device | None = None,
        dtype: torch.dtype | None = None,
    ):
        super().__init__()
        self.cfg = cfg
        self.enc = tiktoken.get_encoding(self.cfg.enc)
        self.n_vocab = self.enc.n_vocab
        self.recompute = recompute
        self.dtype = dtype
        # build network
        kwargs = {"device": device, "dtype": dtype}
        self.tok_embed = nn.Embedding(self.n_vocab, cfg.d_model, **kwargs)
        self.pos_embed = nn.Embedding(cfg.ctx_window, cfg.d_model, **kwargs)
        self.xf_layers = nn.ModuleList(
            [TransformerLayer(cfg, idx, **kwargs) for idx in range(cfg.n_layers)]
        )
        self.final_ln = Norm(cfg.d_model, **kwargs)
        self.unembed = nn.Linear(cfg.d_model, self.n_vocab, bias=False, **kwargs)
    def simplify(self):
        for xf_layer in self.xf_layers:
            xf_layer.simplify()
        # NOTE: we can't fold layer norm into unembedding layer
        # because it has no bias
        # apply_layernorm_foldin(self.final_ln, [self.unembed])
    @property
    def device(self) -> Device:
        return next(self.parameters()).device
    def set_recompute(self, recompute: bool) -> None:
        self.recompute = recompute
    def forward(
        self,
        tokens: Tensor,
        H: HiddenState | None = None,
        pad: Tensor | None = None,
        hooks: TransformerHooks = TransformerHooks(),
    ) -> tuple[Tensor, HiddenState]:
        """
        Forward pass through the transformer!
        During evaluation or first forward pass in sampling:
            X is expected to be a [batch_size x sequence_length]-shaped LongTensor of encoded prompts.
            H is expected to be None.
            pad is a [batch_size x sequence_length]-shaped boolean Tensor. "1"s mean "ignore this
            token". This parameter must be set if not all encoded prompts in X have the same length.
            Note that activations observed by hooks will include padded values.
        During sampling after first forward pass:
            X is expected to be the new part of the sequences (eg most recently sampled tokens).
            H is expected to have KV-caches of all Keys and Values for prior tokens.
            pad is expected to be None (new tokens are not pad tokens).
        Returns a tuple containing the resulting logits tensor and a new hidden state consisting of a KV cache.
        """
        X, H, pad, hooks = self.run_embed(tokens, H, pad, hooks)
        X, H, pad, hooks = self.run_torso(X, H, pad, hooks)
        return self.run_unembed(X, H, hooks)
    def run_embed(
        self,
        tokens: Tensor,
        H: HiddenState | None = None,
        pad: Tensor | None = None,
        hooks: TransformerHooks = TransformerHooks(),
    ) -> tuple[Tensor, HiddenState, Tensor | None, TransformerHooks]:
        assert tokens.dtype == torch.long, "tokens must be sequences of tokens."
        if H is None:
            H = HiddenState(self.cfg.n_layers)
        if pad is None:
            pad = torch.zeros_like(tokens, dtype=torch.bool)
        # embedding
        X = self.tok_embed(tokens)
        # position encoding logic to support sampling with prompts of unequal length.
        pos = prep_pos_from_pad_and_prev_lens(pad, H.prev_lens)
        seq_lens = (pos[:, -1] + 1).unsqueeze(-1)
        assert all(
            seq_lens <= self.cfg.ctx_window
        ), f"sequences must fit in the context window {self.cfg.ctx_window}."
        H.set_prev_lens(seq_lens)
        X = X + self.pos_embed(pos)
        X = hooks.resid.post_emb(X)
        return X, H, pad, hooks
    def run_torso(
        self,
        X: Tensor,
        H: HiddenState | None,
        pad: Tensor | None,
        hooks: TransformerHooks,
    ) -> tuple[Tensor, HiddenState, Tensor | None, TransformerHooks]:
        # transformer torso
        for i, xf_layer in enumerate(self.xf_layers):
            hooks_layer_i = deepcopy(hooks).bind(layer=i)
            if self.recompute:
                X, H[i] = checkpoint(xf_layer, X, H[i], pad, hooks_layer_i)
            else:
                X, H[i] = xf_layer(X, H[i], pad, hooks_layer_i)
        return X, H, pad, hooks
    def run_ln_f(
        self,
        X: Tensor,
        H: HiddenState | None,
        hooks: TransformerHooks,
    ) -> tuple[Tensor, HiddenState, TransformerHooks]:
        X = self.final_ln(X, hooks.resid.ln_f)
        X = hooks.resid.post_ln_f(X)
        return X, H, hooks
    def run_unembed(
        self,
        X: Tensor,
        H: HiddenState | None,
        hooks: TransformerHooks,
    ) -> tuple[Tensor, HiddenState]:
        # unembedding
        X, H, hooks = self.run_ln_f(X, H, hooks)
        X = self.unembed(X)
        X = hooks.logits(X)
        return X, H
    def sample(
        self,
        prompts: str | list[str] | list[int] | list[list[int]],
        num_tokens: int = 5,
        temperature: float = 1.0,
        top_p: float | None = None,
        hooks: TransformerHooks = TransformerHooks(),
    ) -> dict[str, Any]:
        """
        Sampling with the transformer!
        If top_p is set, then nucleus sampling is used.
        Otherwise, the sampling will be Categorical.
        If temperature=0, sampling is deterministic (and top_p is ignored).
        (Warning: when using torch.use_deterministic_algorithms(True),
        nucleus sampling will throw an error. It depends on torch.cumsum,
        which unfortunately has no deterministic implementation in torch.)
        Output is a dict {'tokens': list[list[int]], 'strings': list[str]}
        """
        prompts = [prompts] if isinstance(prompts, str) else prompts
        if isinstance(prompts[0], str):
            X: list[list[int]] = [self.enc.encode(prompt) for prompt in prompts]
        elif isinstance(prompts[0], int):
            X = [prompts]
        else:
            X = prompts
        X, pad = prep_input_and_pad(X, "left", self.device)
        H = None
        beta = 1 / max(temperature, 1e-10)
        out = {
            "tokens": [[] for _ in prompts],
            "strings": ["" for _ in prompts],
        }
        # sampling loop
        for _ in range(num_tokens):
            with torch.no_grad():
                # get logits
                Y, H = self.forward(X, H, pad, hooks=hooks)
                logits = Y[:, -1] * beta
                # sampling only works if logits are floats
                logits = logits.float()
                # perform sampling
                if temperature == 0:
                    tokens = torch.argmax(logits, dim=-1)
                elif top_p is not None:
                    tokens = nucleus_sample(logits, top_p)
                else:
                    tokens = Categorical(logits=logits).sample()
                X, pad = tokens.unsqueeze(-1), None
            for batch_idx, token in enumerate(tokens):
                out["tokens"][batch_idx].append(token.item())
                out["strings"][batch_idx] += self.enc.decode([token.item()])
        return out
    @classmethod
    def load(
        cls,
        name_or_path: str,
        device: Device | None = None,
        dtype: torch.dtype | None = None,
        simplify: bool = False,
        simplify_kwargs: dict[str, Any] | None = None,
    ) -> "Transformer":
        if name_or_path.startswith("https://"):
            path = name_or_path
        else:
            path = f"https://openaipublic.blob.core.windows.net/neuron-explainer/subject-models/{name_or_path.replace('-', '/')}"
        xf = cls.from_checkpoint(
            path,
            device=device,
            dtype=dtype,
        )
        if simplify:
            if simplify_kwargs is None:
                simplify_kwargs = {}
            xf.simplify(**simplify_kwargs)
        return xf
    def save_checkpoint(
        self,
        path: str,
    ) -> None:
        self.cfg.save(osp.join(path, "config.json"))
        pieces_path = osp.join(path, "model_pieces")
        for k, v in self.state_dict().items():
            with CustomFileHandler(osp.join(pieces_path, f"{k}.pt"), "wb") as f:
                torch.save(v, f)
    def load_state_from_checkpoint(
        self, path: str, device: Device | None = None, dtype: torch.dtype | None = None
    ):
        pieces_path = osp.join(path, "model_pieces")
        piece_names = set(self.state_dict().keys())
        piece_files = [f"{k}.pt" for k in piece_names]
        if dtype is not None:
            assert isinstance(dtype, torch.dtype), "Must provide valid dtype."
        device = device or self.device
        with ThreadPoolExecutor(max_workers=50) as executor:
            k_to_future = {
                fname[: -len(".pt")]: executor.submit(
                    _load_piece, osp.join(pieces_path, fname), device, dtype
                )
                for fname in piece_files
            }
            d = {k: future.result() for k, future in k_to_future.items()}
        self.load_state_dict(d)
    @classmethod
    def from_checkpoint(
        cls, path: str, device: Device | None = None, dtype: torch.dtype | None = None
    ) -> "Transformer":
        if device is None:
            device = default_device()
        cfg = TransformerConfig.load(osp.join(path, "config.json"))
        xf = cls(cfg, device=device, dtype=dtype)
        xf.load_state_from_checkpoint(path, device=device, dtype=dtype)
        return xf
def _load_piece(
    file_path: str, device: Device, dtype: torch.dtype | None
) -> tuple[str, torch.Tensor]:
    disk_cache_path = osp.join(
        "/tmp/neuron-explainer-model-pieces-cache", file_path.replace("https://", "")
    )
    if not file_exists(disk_cache_path):
        copy_to_local_cache(file_path, disk_cache_path)
    with CustomFileHandler(disk_cache_path, "rb") as f:
        t = torch.load(f, map_location=device)
        if dtype is not None:
            t = t.to(dtype)
    return t

================
File: build/lib/neuron_explainer/pydantic/__init__.py
================
from .camel_case_base_model import CamelCaseBaseModel
from .hashable_base_model import HashableBaseModel
from .immutable import immutable
__all__ = ["CamelCaseBaseModel", "HashableBaseModel", "immutable"]

================
File: build/lib/neuron_explainer/pydantic/camel_case_base_model.py
================
from pydantic import BaseModel
def to_camel(string: str) -> str:
    return "".join(word.capitalize() if i > 0 else word for i, word in enumerate(string.split("_")))
class CamelCaseBaseModel(BaseModel):
    """
    Base model that will automatically generate camelCase aliases for fields. Python code can use
    either snake_case or camelCase names. When Typescript code is generated, it will only use the
    camelCase names.
    """
    class Config:
        alias_generator = to_camel
        allow_population_by_field_name = True

================
File: build/lib/neuron_explainer/pydantic/hashable_base_model.py
================
from .camel_case_base_model import CamelCaseBaseModel
class HashableBaseModel(CamelCaseBaseModel):
    def __hash__(self) -> int:
        values = tuple(getattr(self, name) for name in self.__annotations__.keys())
        # Convert lists to tuples.
        values = tuple(value if not isinstance(value, list) else tuple(value) for value in values)
        return hash(values)

================
File: build/lib/neuron_explainer/pydantic/immutable.py
================
from typing import TypeVar
from pydantic import BaseConfig, BaseModel
T = TypeVar("T", bound=BaseModel)
def immutable(cls: type[T]) -> type[T]:
    """
    Makes a Pydantic model immutable.
    Annotate a Pydantic class with `@immutable` to prevent the values of its fields from being
    changed after an instance is constructed. (This only guarantees shallow immutability of course:
    fields may have their internal state change.)
    """
    class Config(BaseConfig):
        frozen: bool = True
    cls.Config = Config
    return cls

================
File: datasets.md
================
# Collated activation datasets

This document lists the collated activation datasets that are compatible with the Transformer Debugger. These datasets contain some top-activating examples for each MLP neuron, attention head, and autoencoder latent, as well as the corresponding activations for each token (or token pair) in the example. They provide a way to visualize what each neuron, attention head, or autoencoder latent is selective for (obviously in an incomplete way). These activation datasets are used by the [neuron viewer](neuron_viewer/README.md) to display the top-activating examples for each component, and are also typically used for [automated interpretability](https://openai.com/research/language-models-can-explain-neurons-in-language-models).

The activations datasets are located on Azure Blob Storage, for example accessible via the [`blobfile`](https://github.com/blobfile/blobfile) library. 

# GPT-2 small

Collated activation datasets are available for both the MLP neurons and the attention heads. MLP neuron activations are recorded for each token, while attention head activations are recorded for each token pair. 

The datasets are located at the following paths:
> - MLP neurons: `https://openaipublic.blob.core.windows.net/neuron-explainer/gpt2_small_data/collated-activations/{layer_index}/{neuron_index}.json`
> - Attention heads: `https://openaipublic.blob.core.windows.net/neuron-explainer/gpt2_small/attn_write_norm/collated-activations-by-token-pair/{layer_index}/{head_index}.json`

with the following parameters:
- `layer_index` is in range(12)
- `neuron_index` is in range(3084)
- `head_index` is in range(12)


## GPT-2 small - MLP autoencoders

MLP autoencoders were trained either on the MLP neurons (after the activation function), or on the MLP-layer output that is written to the residual stream. See [Autoencoders for GPT-2 small](neuron_explainer/models/README.md#sparse-autoencoder) for more details. 

The datasets are located at the following paths:

> - MLP latents: `https://openaipublic.blob.core.windows.net/neuron-explainer/gpt2-small/autoencoder_latent/{autoencoder_input}{version}/collated-activations/{layer_index}/{latent_index}.pt`

with the following parameters:
- `autoencoder_input` is in ["mlp_post_act", "resid_delta_mlp"]
- `version` is in ["", "_v4"]. (The `_v4` versions use slightly different hyperparameters, and should be preferred.)
- `layer_index` is in range(12)
- `latent_index` is in range(32768)

## GPT-2 small - Attention autoencoders

Attention autoencoders were trained on the attention-layer output that is written to the residual stream. See [Autoencoders for GPT-2 small](neuron_explainer/models/README.md#sparse-autoencoder) for more details. The `collated-activations` dataset contains autoencoder latent activations for each token, while the `collated-activations-by-token-pair` dataset contains autoencoder latent *attribution* to each token pair. To compute the attribution given an autoencoder latent `L` and a token pair `(T1, T2)`, we multiply the attention pattern `A(T1, T2)` with the gradient of `L` with respect to the attention pattern: `attribution_L(T1, T2) = A(T1, T2) * L/A(T1, T2)`. 

The datasets are located at the following paths:

> - Attention latents (by token): `https://openaipublic.blob.core.windows.net/neuron-explainer/gpt2-small/autoencoder_latent/resid_delta_attn_v4/collated-activations/{layer_index}/{latent_index}.pt`
> - Attention latents (by token pair): `https://openaipublic.blob.core.windows.net/neuron-explainer/gpt2-small/autoencoder_latent/resid_delta_attn_v4/collated-activations-by-token-pair/{layer_index}/{latent_index}.pt`

with the following parameters:
- `layer_index` is in range(12)
- `latent_index` is in range(10240)



# GPT-2 xl

For GPT-2 xl, only the MLP neurons activations are available. The datasets are located at the following paths:
> - MLP neurons: `https://openaipublic.blob.core.windows.net/neuron-explainer/data/collated-activations/{layer_index}/{neuron_index}.json`

with the following parameters:
- `layer_index` is in range(48)
- `neuron_index` is in range(6400)

================
File: neuron_explainer.egg-info/dependency_links.txt
================


================
File: neuron_explainer.egg-info/requires.txt
================
aiohttp
click
fastapi==0.97
fire
httpx>=0.22
mypy==1.7.1
numpy
orjson
pre-commit
pydantic<2.0.0
pytest
pytest-asyncio
scikit-learn
starlette
tiktoken
torch>=1.13
uvicorn

================
File: neuron_explainer.egg-info/SOURCES.txt
================
LICENSE
README.md
pyproject.toml
setup.py
neuron_explainer/__init__.py
neuron_explainer/api_client.py
neuron_explainer/file_utils.py
neuron_explainer.egg-info/PKG-INFO
neuron_explainer.egg-info/SOURCES.txt
neuron_explainer.egg-info/dependency_links.txt
neuron_explainer.egg-info/requires.txt
neuron_explainer.egg-info/top_level.txt
neuron_explainer/explanations/__init__.py
neuron_explainer/explanations/attention_head_scoring.py
neuron_explainer/explanations/calibrated_simulator.py
neuron_explainer/explanations/explainer.py
neuron_explainer/explanations/explanations.py
neuron_explainer/explanations/few_shot_examples.py
neuron_explainer/explanations/prompt_builder.py
neuron_explainer/explanations/scoring.py
neuron_explainer/explanations/simulator.py
neuron_explainer/explanations/test_explainer.py
neuron_explainer/explanations/test_simulator.py
neuron_explainer/fast_dataclasses/__init__.py
neuron_explainer/fast_dataclasses/fast_dataclasses.py
neuron_explainer/fast_dataclasses/test_fast_dataclasses.py
neuron_explainer/models/__init__.py
neuron_explainer/models/autoencoder.py
neuron_explainer/models/autoencoder_context.py
neuron_explainer/models/hooks.py
neuron_explainer/models/inference_engine_type_registry.py
neuron_explainer/models/model_component_registry.py
neuron_explainer/models/model_context.py
neuron_explainer/models/model_registry.py
neuron_explainer/models/transformer.py
neuron_explainer/pydantic/__init__.py
neuron_explainer/pydantic/camel_case_base_model.py
neuron_explainer/pydantic/hashable_base_model.py
neuron_explainer/pydantic/immutable.py

================
File: neuron_explainer.egg-info/top_level.txt
================
neuron_explainer

================
File: neuron_explainer/activation_server/derived_scalar_computation.py
================
"""
This file contains code to add hooks relevant to a list of scalar derivers, and to run forward
passes with those hooks to populate a DerivedScalarStore with the value of the scalars in question.
"""
import gc
import time
from dataclasses import dataclass
from typing import Any, Callable, TypeVar
import torch
from fastapi import HTTPException
from neuron_explainer.activation_server.requests_and_responses import (
    GroupId,
    InferenceAndTokenData,
    InferenceData,
    LossFnConfig,
    LossFnName,
)
from neuron_explainer.activations.derived_scalars import DerivedScalarType
from neuron_explainer.activations.derived_scalars.derived_scalar_store import (
    DerivedScalarStore,
    RawActivationStore,
)
from neuron_explainer.activations.derived_scalars.direct_effects import (
    AttentionDirectEffectReconstituter,
)
from neuron_explainer.activations.derived_scalars.indexing import (
    DETACH_LAYER_NORM_SCALE,
    AblationSpec,
    ActivationIndex,
    AttentionTraceType,
    TraceConfig,
    make_python_slice_from_all_or_one_indices,
)
from neuron_explainer.activations.derived_scalars.logprobs import LogitReconstituter
from neuron_explainer.activations.derived_scalars.multi_group import (
    MultiGroupDerivedScalarStore,
    MultiGroupScalarDerivers,
)
from neuron_explainer.activations.derived_scalars.reconstituter_class import ActivationReconstituter
from neuron_explainer.activations.derived_scalars.scalar_deriver import (
    ActivationLocationTypeAndPassType,
    DstConfig,
    ScalarDeriver,
)
from neuron_explainer.activations.hook_graph import AutoencoderHookGraph, TransformerHookGraph
from neuron_explainer.models import Transformer
from neuron_explainer.models.autoencoder_context import AutoencoderContext, MultiAutoencoderContext
from neuron_explainer.models.model_component_registry import (
    ActivationLocationType,
    Dimension,
    LayerIndex,
    NodeType,
    PassType,
)
from neuron_explainer.models.model_context import (
    InvalidTokenException,
    ModelContext,
    StandardModelContext,
)
from neuron_explainer.models.transformer import prep_input_and_right_pad_for_forward_pass
T = TypeVar("T")
# a nested dict of lists of tuples, where each tuple contains a DerivedScalarType and a
# DerivedScalarTypeConfig (the necessary information to specify a ScalarDeriver). The nested dict is
# keyed first by spec_name and then by group_id, where spec_name is the name associated with a
# ProcessingRequestSpec, and group_id refers to a GroupId enum value (each GroupId referring to a
# set of DSTs).
DstAndConfigsByProcessingStep = dict[str, dict[GroupId, list[tuple[DerivedScalarType, DstConfig]]]]
# a nested dict of lists of ScalarDerivers, parallel to and constructed from
# DstAndConfigsByProcessingStep
ScalarDeriversByProcessingStep = dict[str, dict[GroupId, list[ScalarDeriver]]]
# a nested dict of DerivedScalarStores, parallel to and constructed using
# ScalarDeriversByProcessingStep; also uses RawActivationStore as input (though note that
# RawActivationStore is a single object used for the entire nested dict)
DerivedScalarStoreByProcessingStep = dict[str, dict[GroupId, DerivedScalarStore]]
@dataclass(frozen=True)
class DerivedScalarComputationParams:
    input_token_ints: list[int]
    multi_group_scalar_derivers_by_processing_step: dict[str, MultiGroupScalarDerivers]
    loss_fn_for_backward_pass: Callable[[torch.Tensor], torch.Tensor] | None
    device_for_raw_activations: torch.device
    ablation_specs: list[AblationSpec] | None
    trace_config: TraceConfig | None
    @property
    def prompt_length(self) -> int:
        return len(self.input_token_ints)
    @property
    def activation_location_type_and_pass_types(self) -> list[ActivationLocationTypeAndPassType]:
        return list(
            {
                alt_and_pt
                for mgsd in self.multi_group_scalar_derivers_by_processing_step.values()
                for alt_and_pt in mgsd.activation_location_type_and_pass_types
            }
        )
def construct_logit_diff_loss_fn(
    model_context: ModelContext,
    target_tokens: list[str],
    distractor_tokens: list[str],
    subtract_mean: bool,
) -> Callable[[torch.Tensor], torch.Tensor]:
    try:
        target_tokens_as_ints = model_context.encode_token_str_list(target_tokens)
        distractor_tokens_as_ints = model_context.encode_token_str_list(distractor_tokens)
    except InvalidTokenException as e:
        raise HTTPException(status_code=400, detail=str(e))
    def loss_fn_for_backward_pass(output_logits: torch.Tensor) -> torch.Tensor:
        assert output_logits.ndim == 3
        nbatch, ntoken, nlogit = output_logits.shape
        assert nbatch == 1
        assert len(target_tokens_as_ints) > 0
        target_mean = output_logits[:, -1, target_tokens_as_ints].mean(-1)
        if len(distractor_tokens_as_ints) == 0:
            loss = target_mean.mean()  # average logits for target tokens
            if subtract_mean:
                loss -= output_logits[:, -1, :].mean()
            return loss
        else:
            assert (
                not subtract_mean
            ), "subtract_mean not a meaningful option when distractor_tokens is specified"
            distractor_mean = output_logits[:, -1, distractor_tokens_as_ints].mean(-1)
            return (
                target_mean - distractor_mean
            ).mean()  # difference between average logits for target and distractor tokens
    return loss_fn_for_backward_pass
def construct_probs_loss_fn(
    model_context: ModelContext, target_tokens: list[str]
) -> Callable[[torch.Tensor], torch.Tensor]:
    try:
        target_tokens_as_ints = model_context.encode_token_str_list(target_tokens)
    except InvalidTokenException as e:
        raise HTTPException(status_code=400, detail=str(e))
    def loss_fn_for_backward_pass(output_logits: torch.Tensor) -> torch.Tensor:
        assert output_logits.ndim == 3
        output_probs = torch.softmax(output_logits, dim=-1)
        nbatch, ntoken, nlogit = output_probs.shape
        assert nbatch == 1
        assert len(target_tokens_as_ints) > 0
        target_sum = output_probs[:, -1, target_tokens_as_ints].sum(-1)
        return target_sum.mean()  # average summed probs for target tokens
    return loss_fn_for_backward_pass
def construct_zero_loss_fn() -> Callable[[torch.Tensor], torch.Tensor]:
    """This loss function is used for running a backward pass that will be interrupted
    by ablating some desired parameters. Parameters downstream of the ablated parameters
    will have a gradient of 0, and parameters upstream of the ablated parameters will
    in general have a non-zero gradient."""
    def loss_fn_for_backward_pass(output_logits: torch.Tensor) -> torch.Tensor:
        return 0.0 * output_logits.sum()
    return loss_fn_for_backward_pass
def maybe_construct_loss_fn_for_backward_pass(
    model_context: ModelContext, config: LossFnConfig | None
) -> Callable[[torch.Tensor], torch.Tensor] | None:
    if config is None:
        return None
    else:
        if config.name == LossFnName.LOGIT_DIFF:
            assert config.target_tokens is not None
            target_tokens = config.target_tokens
            distractor_tokens = config.distractor_tokens or []
            return construct_logit_diff_loss_fn(
                model_context=model_context,
                target_tokens=target_tokens,
                distractor_tokens=distractor_tokens,
                subtract_mean=False,
            )
        elif config.name == LossFnName.LOGIT_MINUS_MEAN:
            assert config.target_tokens is not None
            assert config.distractor_tokens is None
            return construct_logit_diff_loss_fn(
                model_context=model_context,
                target_tokens=config.target_tokens,
                distractor_tokens=[],
                subtract_mean=True,
            )
        elif config.name == LossFnName.PROBS:
            assert config.target_tokens is not None
            assert config.distractor_tokens is None
            target_tokens = config.target_tokens
            return construct_probs_loss_fn(model_context=model_context, target_tokens=target_tokens)
        elif config.name == LossFnName.ZERO:
            return construct_zero_loss_fn()
        else:
            raise NotImplementedError(f"Unknown loss fn name: {config.name}")
ablatable_activation_location_type_by_node_type = {
    NodeType.MLP_NEURON: ActivationLocationType.MLP_POST_ACT,
    NodeType.ATTENTION_HEAD: ActivationLocationType.ATTN_QK_PROBS,
    NodeType.RESIDUAL_STREAM_CHANNEL: ActivationLocationType.RESID_POST_MLP,
    NodeType.AUTOENCODER_LATENT: ActivationLocationType.ONLINE_AUTOENCODER_LATENT,
}
def compute_derived_scalar_groups_for_input_token_ints(
    model_context: StandardModelContext,
    multi_autoencoder_context: MultiAutoencoderContext | None,
    batched_ds_computation_params: list[DerivedScalarComputationParams],
) -> tuple[
    list[dict[str, MultiGroupDerivedScalarStore]], list[InferenceData], list[RawActivationStore]
]:
    """This function runs a batched forward pass on the given batch of input token sequences, with
    hooks added to the transformer to extract the activations needed to compute the scalars in
    multi_group_scalar_derivers for each batch element. It then returns a batch of dicts of
    DerivedScalarStores by group_id containing the relevant derived scalars for each token in
    the input, as well as a batch of InferenceData objects containing tokenized inputs and other metadata,
    and a batch of RawActivationStores, each of which was used to compute the respective
    dict of DerivedScalarStores. These RawActivationStores can be used to compute additional derived scalars
    post-hoc.
    """
    (
        batched_raw_activation_store,
        batched_loss,
        batched_activation_value_for_backward_pass,
        batched_memory_used_before,
        inference_time,
    ) = run_inference_and_populate_raw_store(
        model_context=model_context,
        multi_autoencoder_context=multi_autoencoder_context,
        batched_ds_computation_params=batched_ds_computation_params,
    )
    assert (
        len(batched_raw_activation_store)
        == len(batched_ds_computation_params)
        == len(batched_loss)
        == len(batched_activation_value_for_backward_pass)
        == len(batched_memory_used_before)
    )
    batched_multi_group_ds_store_by_processing_step: list[
        dict[str, MultiGroupDerivedScalarStore]
    ] = []
    (
        batched_multi_group_ds_store_by_processing_step,
        batched_memory_used_after,
    ) = construct_ds_stores_from_raw(
        batched_raw_activation_store,
        batched_ds_computation_params,
    )
    batched_inference_data: list[InferenceData] = []
    for (
        loss,
        activation_value_for_backward_pass,
        memory_used_before,
        memory_used_after,
    ) in zip(
        batched_loss,
        batched_activation_value_for_backward_pass,
        batched_memory_used_before,
        batched_memory_used_after,
    ):
        inference_data = InferenceData(
            inference_time=inference_time,
            loss=loss,
            activation_value_for_backward_pass=activation_value_for_backward_pass,
            memory_used_before=memory_used_before,
            memory_used_after=memory_used_after,
        )
        batched_inference_data.append(inference_data)
    return (
        batched_multi_group_ds_store_by_processing_step,
        batched_inference_data,
        batched_raw_activation_store,
    )
def get_activation_index_and_reconstitute_activation_fn(
    transformer: Transformer,
    multi_autoencoder_context: MultiAutoencoderContext | None,
    trace_config: TraceConfig,
) -> tuple[ActivationIndex, Callable[[torch.Tensor, torch.Tensor | None], torch.Tensor]]:
    """
    This function returns the ActivationIndex corresponding to the preceding residual stream
    index implied by the trace_config. It also returns a function taking one tensor, used to recompute
    the activation specified by the trace_config from the residual stream.
    """
    assert trace_config.attention_trace_type != AttentionTraceType.V
    if trace_config.node_type.is_autoencoder_latent:
        assert multi_autoencoder_context is not None
        autoencoder_context = multi_autoencoder_context.get_autoencoder_context(
            trace_config.node_type
        )
        assert autoencoder_context is not None
    else:
        autoencoder_context = None
    act_reconstituter = ActivationReconstituter.from_trace_config(
        transformer=transformer,
        autoencoder_context=autoencoder_context,
        trace_config=trace_config,
    )
    activation_index_for_reconstituter = act_reconstituter.get_residual_activation_index_for_node_index(
        # convert trace_config to node index
        trace_config.node_index
    )
    def reconstitute_activation_fn(
        upstream_resid: torch.Tensor, _unused_downstream_resid: torch.Tensor | None
    ) -> torch.Tensor:
        assert _unused_downstream_resid is None
        reconstitute_activation = (
            act_reconstituter.make_reconstitute_activation_fn_for_trace_config(
                trace_config=trace_config
            )
        )
        return reconstitute_activation(upstream_resid)
    return activation_index_for_reconstituter, reconstitute_activation_fn
def get_activation_indices_and_reconstitute_direct_effect_fn(
    model_context: ModelContext,
    multi_autoencoder_context: MultiAutoencoderContext | None,
    trace_config: TraceConfig,
    loss_fn_for_backward_pass: Callable[[torch.Tensor], torch.Tensor] | None,
) -> tuple[
    ActivationIndex,
    ActivationIndex,
    Callable[[torch.Tensor, torch.Tensor | None], torch.Tensor],
]:
    """
    For use with AttentionTraceType.V. This function returns the ActivationIndex corresponding to the
    residual stream before the upstream (attention V) node as well as the ActivationIndex corresponding to the
    residual stream before the downstream node, or before the loss. It also returns a function taking two arguments,
    both residual stream activations, which computes the direct effect of the upstream activation on the downstream
    activation.
    If trace_config.attention_trace_config.downstream_trace_config is a normal TraceConfig, then it specifies a downstream
    node's activation, which will be used to compute a gradient. If it is None, it is assumed that the output logits (in
    their entirety) are being reconstructed instead, and a loss function computed, to compute the gradient.
    """
    assert trace_config.attention_trace_type == AttentionTraceType.V
    if trace_config.downstream_trace_config is None:
        assert loss_fn_for_backward_pass is not None
        assert isinstance(model_context, StandardModelContext)  # for typechecking
        logit_reconstituter = LogitReconstituter(
            model_context=model_context,
            detach_layer_norm_scale=DETACH_LAYER_NORM_SCALE,
        )
        downstream_activation_index_for_reconstituter = (
            logit_reconstituter.get_residual_activation_index()
        )
        def reconstitute_gradient_fn(downstream_resid: torch.Tensor) -> torch.Tensor:
            reconstitute_gradient = logit_reconstituter.make_reconstitute_gradient_of_loss_fn(
                loss_fn=loss_fn_for_backward_pass
            )
            return reconstitute_gradient(downstream_resid)
    else:
        if trace_config.node_type.is_autoencoder_latent:
            assert multi_autoencoder_context is not None
            autoencoder_context = multi_autoencoder_context.get_autoencoder_context(
                trace_config.node_type
            )
            assert autoencoder_context is not None
        else:
            autoencoder_context = None
        act_reconstituter = ActivationReconstituter.from_trace_config(
            transformer=model_context.get_or_create_model(),
            autoencoder_context=autoencoder_context,
            trace_config=trace_config.downstream_trace_config,
        )
        downstream_trace_config = trace_config.downstream_trace_config
        downstream_activation_index_for_reconstituter = act_reconstituter.get_residual_activation_index_for_node_index(
            # convert trace_config to node index
            downstream_trace_config.node_index
        )
        def reconstitute_gradient_fn(
            downstream_resid: torch.Tensor,
        ) -> torch.Tensor:
            reconstitute_gradient_with_args = (
                act_reconstituter.make_reconstitute_gradient_fn_for_trace_config(
                    trace_config=downstream_trace_config
                )
            )
            return reconstitute_gradient_with_args(
                downstream_resid,
                downstream_trace_config.layer_index,
                downstream_trace_config.pass_type,
            )
    assert trace_config.layer_index is not None
    direct_effect_reconstituter = AttentionDirectEffectReconstituter(
        model_context=model_context,
        layer_indices=[trace_config.layer_index],
        detach_layer_norm_scale=DETACH_LAYER_NORM_SCALE,
    )
    upstream_activation_index_for_reconstituter = direct_effect_reconstituter.get_residual_activation_index_for_node_index(
        # convert trace_config to node index
        trace_config.node_index
    )
    upstream_scalar_hook = direct_effect_reconstituter.make_scalar_hook_for_node_index(
        trace_config.node_index
    )
    def reconstitute_direct_effect_fn(
        upstream_resid: torch.Tensor,
        downstream_resid: torch.Tensor | None,
    ) -> torch.Tensor:
        assert downstream_resid is not None
        gradient = reconstitute_gradient_fn(downstream_resid).detach()
        activations = direct_effect_reconstituter.reconstitute_activations(
            resid=upstream_resid,
            grad=gradient,
            layer_index=trace_config.layer_index,
            pass_type=trace_config.pass_type,
        )
        return upstream_scalar_hook(activations)
    return (
        upstream_activation_index_for_reconstituter,
        downstream_activation_index_for_reconstituter,
        reconstitute_direct_effect_fn,
    )
def replace_activation_index_using_reconstituter(
    model_context: ModelContext,
    multi_autoencoder_context: MultiAutoencoderContext | None,
    batched_ds_computation_params: list[DerivedScalarComputationParams],
) -> tuple[
    list[ActivationIndex | None],
    list[ActivationIndex | None],
    list[Callable[[torch.Tensor, torch.Tensor | None], torch.Tensor]],
]:
    """
    Where trace_config occurs in batched_ds_computation_params, convert it to the
    upstream_activation_index (and optionally also downstream_activation_index)
    corresponding to the preceding residual stream required by a Reconstituter. Also
    return a function, generated from the Reconstituter, to obtain the activation
    corresponding to the original trace_config from the residual stream.
    """
    batched_reconstitute_activation_fn: list[
        Callable[[torch.Tensor, torch.Tensor | None], torch.Tensor]
    ] = []
    batched_upstream_activation_index_to_grab: list[ActivationIndex | None] = []
    batched_downstream_activation_index_to_grab: list[ActivationIndex | None] = []
    for ds_computation_params_index in range(len(batched_ds_computation_params)):
        ds_computation_params = batched_ds_computation_params[ds_computation_params_index]
        trace_config = ds_computation_params.trace_config
        if trace_config is not None:
            if trace_config.attention_trace_type == AttentionTraceType.V:
                (
                    upstream_activation_index_for_reconstituter,
                    downstream_activation_index_for_reconstituter,
                    reconstitute_activation_fn,
                ) = get_activation_indices_and_reconstitute_direct_effect_fn(
                    model_context=model_context,
                    multi_autoencoder_context=multi_autoencoder_context,
                    trace_config=trace_config,
                    loss_fn_for_backward_pass=ds_computation_params.loss_fn_for_backward_pass,
                )
            else:
                (
                    upstream_activation_index_for_reconstituter,
                    reconstitute_activation_fn,
                ) = get_activation_index_and_reconstitute_activation_fn(
                    transformer=model_context.get_or_create_model(),
                    multi_autoencoder_context=multi_autoencoder_context,
                    trace_config=trace_config,
                )
                downstream_activation_index_for_reconstituter = None
        else:
            upstream_activation_index_for_reconstituter = None
            downstream_activation_index_for_reconstituter = None
            def dummy_reconstitute_activation_fn(
                resid: torch.Tensor, grad: torch.Tensor | None
            ) -> torch.Tensor:
                raise NotImplementedError("This function should not be called")
            reconstitute_activation_fn = dummy_reconstitute_activation_fn
        if upstream_activation_index_for_reconstituter is not None:
            assert (
                upstream_activation_index_for_reconstituter.activation_location_type.node_type
                == NodeType.RESIDUAL_STREAM_CHANNEL
            )
        if downstream_activation_index_for_reconstituter is not None:
            assert (
                downstream_activation_index_for_reconstituter.activation_location_type.node_type
                == NodeType.RESIDUAL_STREAM_CHANNEL
            )
        batched_upstream_activation_index_to_grab.append(
            upstream_activation_index_for_reconstituter
        )
        batched_downstream_activation_index_to_grab.append(
            downstream_activation_index_for_reconstituter
        )
        batched_reconstitute_activation_fn.append(reconstitute_activation_fn)
    return (
        batched_upstream_activation_index_to_grab,
        batched_downstream_activation_index_to_grab,
        batched_reconstitute_activation_fn,
    )
def run_inference_and_populate_raw_store(
    model_context: StandardModelContext,
    multi_autoencoder_context: MultiAutoencoderContext | None,
    batched_ds_computation_params: list[DerivedScalarComputationParams],
) -> tuple[
    list[RawActivationStore],
    list[float | None],
    list[float | None],
    list[float | None],
    float,
]:
    """
    This populates a dict of ActivationsAndMetadata objects for each batch element, and returns
    inference-related stats.
     - batched_requested_activations_by_location_type_and_pass_type: stored activations
     - batched_loss_floats: loss values for each batch element, if a loss function was specified
     - batched_activation_value_for_backward_pass_floats: value of activation for which a backward pass was
    computed for each batch element, if an activation index for backward pass was specified
     - batched_memory_used_before: amount of memory allocated to the GPU before the forward pass for each batch element
     - inference_time: time used for the forward pass, in seconds
    """
    for params in batched_ds_computation_params:
        trace_config = params.trace_config
        if trace_config is not None:
            if trace_config.node_type.is_autoencoder_latent:
                assert multi_autoencoder_context is not None
                assert (
                    multi_autoencoder_context.get_autoencoder_context(trace_config.node_type)
                    is not None
                ), f"Autoencoder context not found for {trace_config.node_type}"
    transformer = model_context.get_or_create_model()
    batched_input_token_ints = [params.input_token_ints for params in batched_ds_computation_params]
    tokens_tensor, pad_tensor = prep_input_and_right_pad_for_forward_pass(
        batched_input_token_ints, transformer.device
    )
    (
        batched_upstream_activation_index_for_backward_pass,
        batched_downstream_activation_index_for_backward_pass,
        batched_reconstitute_activation_fn,
    ) = replace_activation_index_using_reconstituter(
        model_context=model_context,
        multi_autoencoder_context=multi_autoencoder_context,
        batched_ds_computation_params=batched_ds_computation_params,
    )
    batched_activation_index_for_backward_pass_by_name = [
        {
            "upstream": upstream_activation_index_for_backward_pass,
            "downstream": downstream_activation_index_for_backward_pass,
        }
        for (
            upstream_activation_index_for_backward_pass,
            downstream_activation_index_for_backward_pass,
        ) in zip(
            batched_upstream_activation_index_for_backward_pass,
            batched_downstream_activation_index_for_backward_pass,
        )
    ]
    (
        transformer_graph,  # Stores the hooks
        batched_requested_activations_by_location_type_and_pass_type,  # Stores the activations from the hooks after the forward pass
        batched_requested_attached_activations_for_backward_pass_by_name,
    ) = get_transformer_graph_hooks_and_activation_caches(
        multi_autoencoder_context=multi_autoencoder_context,
        batched_ds_computation_params=batched_ds_computation_params,
        batched_activation_index_for_backward_pass_by_name=batched_activation_index_for_backward_pass_by_name,
    )
    assert len(batched_requested_activations_by_location_type_and_pass_type) == len(
        batched_ds_computation_params
    )
    for (
        requested_activations_by_location_type_and_pass_type,
        ds_computation_params,
    ) in zip(
        batched_requested_activations_by_location_type_and_pass_type,
        batched_ds_computation_params,
    ):
        pass_types = [
            activation_location_type_and_pass_type.pass_type
            for activation_location_type_and_pass_type in requested_activations_by_location_type_and_pass_type.keys()
        ]
        if any(pass_type == PassType.BACKWARD for pass_type in pass_types):
            assert (
                ds_computation_params.loss_fn_for_backward_pass is not None
                or ds_computation_params.trace_config is not None
            ), "loss_fn_for_backward_pass or trace_config must be defined if gradients are required"
    batched_device_for_raw_activations = [
        params.device_for_raw_activations for params in batched_ds_computation_params
    ]
    t0 = time.time()
    cuda_available = torch.cuda.is_available()
    if cuda_available and any(
        device.type == "cuda" for device in batched_device_for_raw_activations
    ):
        torch.cuda.empty_cache()
    batched_memory_used_before: list[float | None] = [
        torch.cuda.memory_allocated(device) if device.type == "cuda" and cuda_available else None
        for device in batched_device_for_raw_activations
    ]
    logits, _ = transformer.forward(
        tokens_tensor, pad=pad_tensor, hooks=transformer_graph.as_transformer_hooks()
    )
    batched_loss: list[torch.Tensor | None] = []
    batched_activation_value_for_backward_pass: list[torch.Tensor | None] = []
    for batch_index, (
        ds_computation_params,
        requested_attached_activation_for_backward_pass_by_name,
        reconstitute_activation_fn,
        activation_index_for_backward_pass_by_name,
    ) in enumerate(
        zip(
            batched_ds_computation_params,
            batched_requested_attached_activations_for_backward_pass_by_name,
            batched_reconstitute_activation_fn,
            batched_activation_index_for_backward_pass_by_name,
        )
    ):
        loss_fn_for_backward_pass = ds_computation_params.loss_fn_for_backward_pass
        if loss_fn_for_backward_pass is not None:
            loss = loss_fn_for_backward_pass(logits[batch_index].unsqueeze(0))
        else:
            loss = None
        if activation_index_for_backward_pass_by_name["upstream"] is not None:
            assert requested_attached_activation_for_backward_pass_by_name["upstream"] is not None
            activation_value_for_backward_pass = reconstitute_activation_fn(
                requested_attached_activation_for_backward_pass_by_name["upstream"],
                requested_attached_activation_for_backward_pass_by_name["downstream"],
            )
        else:
            activation_value_for_backward_pass = None
        batched_loss.append(loss)
        batched_activation_value_for_backward_pass.append(activation_value_for_backward_pass)
    populated_losses: list[torch.Tensor] = []
    for loss, value in zip(batched_loss, batched_activation_value_for_backward_pass):
        # backward pass is computed from value if it is not None, otherwise from loss
        if value is not None:
            populated_losses.append(value)
        elif loss is not None:
            populated_losses.append(loss)
    if len(populated_losses):
        assert all(isinstance(loss, torch.Tensor) for loss in populated_losses)
        loss_sum = sum(populated_losses)
        assert isinstance(loss_sum, torch.Tensor)
        loss_sum.backward()
    inference_time = time.time() - t0
    batched_loss_floats: list[float | None] = [
        loss.item() if loss is not None and not torch.isnan(loss) else None for loss in batched_loss
    ]
    batched_activation_value_for_backward_pass_floats: list[float | None] = [
        activation.item() if activation is not None else None
        for activation in batched_activation_value_for_backward_pass
    ]
    batched_raw_activation_store: list[RawActivationStore] = []
    for (requested_activations_by_location_type_and_pass_type,) in zip(
        batched_requested_activations_by_location_type_and_pass_type,
    ):
        raw_activation_store = RawActivationStore.from_nested_dict_of_activations(
            requested_activations_by_location_type_and_pass_type
        )
        batched_raw_activation_store.append(raw_activation_store)
    assert (
        len(batched_raw_activation_store)
        == len(batched_loss_floats)
        == len(batched_activation_value_for_backward_pass_floats)
        == len(batched_memory_used_before)
        == len(batched_ds_computation_params)
    )  # returns one batch element per input param setting
    return (
        batched_raw_activation_store,
        batched_loss_floats,
        batched_activation_value_for_backward_pass_floats,
        batched_memory_used_before,
        inference_time,
    )
def construct_ds_stores_from_raw(
    batched_raw_activation_store: list[RawActivationStore],
    batched_ds_computation_params: list[DerivedScalarComputationParams],
) -> tuple[list[dict[str, MultiGroupDerivedScalarStore]], list[float | None]]:
    batched_multi_group_ds_store_by_processing_step: list[
        dict[str, MultiGroupDerivedScalarStore]
    ] = []
    batched_memory_used_after: list[float | None] = []
    for (
        raw_activation_store,
        ds_computation_params,
    ) in zip(
        batched_raw_activation_store,
        batched_ds_computation_params,
    ):
        multi_group_scalar_derivers_by_processing_step = (
            ds_computation_params.multi_group_scalar_derivers_by_processing_step
        )
        multi_group_ds_store_by_processing_step: dict[str, MultiGroupDerivedScalarStore] = {}
        for (
            spec_name,
            multi_group_scalar_derivers,
        ) in multi_group_scalar_derivers_by_processing_step.items():
            multi_group_ds_store_by_processing_step[
                spec_name
            ] = MultiGroupDerivedScalarStore.derive_from_raw(
                raw_activation_store, multi_group_scalar_derivers
            )
        batched_multi_group_ds_store_by_processing_step.append(
            multi_group_ds_store_by_processing_step
        )
        device_for_raw_activations = ds_computation_params.device_for_raw_activations
        memory_used_after = None
        if torch.cuda.is_available() and device_for_raw_activations.type == "cuda":
            gc.collect()
            memory_used_after = torch.cuda.memory_allocated(device_for_raw_activations)
        batched_memory_used_after.append(memory_used_after)
    return (
        batched_multi_group_ds_store_by_processing_step,
        batched_memory_used_after,
    )
def get_transformer_graph_hooks_and_activation_caches(
    multi_autoencoder_context: MultiAutoencoderContext | None,
    batched_ds_computation_params: list[DerivedScalarComputationParams],
    batched_activation_index_for_backward_pass_by_name: list[dict[str, ActivationIndex | None]],
) -> tuple[
    TransformerHookGraph,
    list[dict[ActivationLocationTypeAndPassType, dict[LayerIndex, torch.Tensor]]],
    list[dict[str, torch.Tensor | None]],
]:
    """This is a helper function that returns:
    1. a TransformerHookGraph object containing hooks for the given
    scalar derivers (this can be passed to Transformer using the as_transformer_hooks method to add
    hooks to the transformer forward and backward pass)
    2. dictionaries mapping each activation location type to the activations requested
    (before it is filled with activations during forward passes), one dictionary per batch element
    3. dictionaries each containing just one value, the scalar tensor on which the backward pass can be run (
    unlike the activations in 2, this tensor is still attached to the pytorch model), one dictionary per batch element
    """
    batched_activation_location_type_and_pass_types = [
        params.activation_location_type_and_pass_types for params in batched_ds_computation_params
    ]
    batched_device = [params.device_for_raw_activations for params in batched_ds_computation_params]
    batched_ablation_specs = [params.ablation_specs for params in batched_ds_computation_params]
    batched_prompt_lengths = [params.prompt_length for params in batched_ds_computation_params]
    # This is a callable that can be used similarly to a Hooks object.
    transformer_graph = TransformerHookGraph()
    """This step constructs the activation location types needed (for the case where they don't already exist). Any hooks to be added
    to that location type can then be appended to transformer_graph in the normal way.
    """
    # steps:
    # 1. add autoencoder graph to transformer_graph (injected autoencoders specified in multi_autoencoder_context are
    #    hooked in a second set of forward hooks, called after ablating and saving hooks, and before activation grabbing hooks)
    #
    # for each batched element:
    #     2. add ablating hooks
    #     3. add activation grabbing hooks (storing without detaching, for backward pass). These come last, after ablating and saving hooks.
    #     4. add saving hooks (storing with detaching)
    #
    # Because the autoencoder is hooked in a second set of hooks, followed by the grabbing hooks, they are always called last:
    # - fwd hooks: ablate activations, save activations, ...
    # - bwd hooks: ablate gradients, save gradients, ...
    # - fwd2 hooks: autoencoder (convert to latent, ablate latents, grab latents, save latents, convert back to activations), grab activations, ...
    # add autoencoder graph
    if multi_autoencoder_context is not None:
        has_multiple_autoencoders = (
            len(multi_autoencoder_context.autoencoder_context_by_node_type) > 1
        )
        for (
            node_type,
            autoencoder_context,
        ) in multi_autoencoder_context.autoencoder_context_by_node_type.items():
            subgraph = AutoencoderHookGraph(
                autoencoder_context, is_one_of_multiple_autoencoders=has_multiple_autoencoders
            )
            subgraph_name = f"{node_type.value}"
            transformer_graph.inject_subgraph(subgraph, subgraph_name)
    (
        batched_requested_activations_by_location_type_and_pass_type,
        batched_requested_attached_activation_for_backward_pass_by_name,
    ) = ([], [])
    assert (
        len(batched_activation_location_type_and_pass_types)
        == len(batched_ablation_specs)
        == len(batched_activation_index_for_backward_pass_by_name)
        == len(batched_prompt_lengths)
    )
    for i in range(len(batched_activation_location_type_and_pass_types)):
        # add ablating hooks
        ablation_spec = batched_ablation_specs[i]
        if ablation_spec is not None:
            add_ablating_hooks(transformer_graph, ablation_spec, batch_index=i)
        # add activation grabbing hooks; the grabbed activations are stored in dicts, keyed by the
        # string name assigned to them
        requested_attached_activation_for_backward_pass_by_name: dict[str, torch.Tensor | None] = {}
        for (
            name,
            activation_index_for_backward_pass,
        ) in batched_activation_index_for_backward_pass_by_name[i].items():
            if activation_index_for_backward_pass is not None:
                requested_attached_activation_for_backward_pass_by_name = add_grabbing_hook_for_backward_pass(
                    requested_attached_activation_for_backward_pass_by_name,
                    name,
                    transformer_graph=transformer_graph,
                    activation_index_for_backward_pass=activation_index_for_backward_pass,
                    batch_index=i,
                    append_to_fwd2=True,  # append to fwd2 when using Reconstituter to obtain gradients, so that
                    # the preceding residual stream backward pass hooks can be called after running .backward()
                    # from the grabbed activation
                )
            else:
                requested_attached_activation_for_backward_pass_by_name[name] = None
        # add saving hooks
        requested_activations_by_location_type_and_pass_type = add_saving_hooks(
            transformer_graph=transformer_graph,
            activation_location_type_and_pass_types=batched_activation_location_type_and_pass_types[
                i
            ],
            device=batched_device[i],
            unpadded_prompt_length=batched_prompt_lengths[i],
            batch_index=i,
        )
        batched_requested_activations_by_location_type_and_pass_type.append(
            requested_activations_by_location_type_and_pass_type
        )
        batched_requested_attached_activation_for_backward_pass_by_name.append(
            requested_attached_activation_for_backward_pass_by_name
        )
    return (
        transformer_graph,
        batched_requested_activations_by_location_type_and_pass_type,
        batched_requested_attached_activation_for_backward_pass_by_name,
    )
def create_activation_grabbing_hook_fn(
    attached_activation_dict: dict[str, torch.Tensor | None],
    name: str,
    activation_index: ActivationIndex,
    batch_index: int,
) -> tuple[Callable, ActivationLocationTypeAndPassType, LayerIndex, dict[str, torch.Tensor | None]]:
    assert (
        name not in attached_activation_dict
    ), f"Name {name} already exists in attached_activation_dict"
    def activation_grabbing_hook_fn(act: torch.Tensor, **kwargs: Any) -> torch.Tensor:
        indices: tuple[slice | int, ...] = (
            batch_index,
        ) + make_python_slice_from_all_or_one_indices(
            activation_index.tensor_indices
        )  # use the batch_index to index into the batch dimension
        attached_activation_dict[name] = act[indices]  # .clone()
        return act
    return (
        activation_grabbing_hook_fn,
        ActivationLocationTypeAndPassType(
            activation_location_type=activation_index.activation_location_type,
            pass_type=activation_index.pass_type,
        ),
        activation_index.layer_index,
        attached_activation_dict,
    )
def add_grabbing_hook_for_backward_pass(
    attached_activation_dict: dict[str, torch.Tensor | None],
    name: str,
    transformer_graph: TransformerHookGraph,
    activation_index_for_backward_pass: ActivationIndex,
    batch_index: int,
    append_to_fwd2: bool = False,
) -> dict[str, torch.Tensor | None]:
    """This is a helper function that returns a TransformerHooks object containing hooks at naturally existing hook locations
    (e.g. MLP post-activations, rather than autoencoder latent activations) for the given
    scalar derivers, and a dictionary mapping each activation location type with a naturally existing hook to the activations requested
    (before it is filled with activations during forward passes)."""
    (
        hook_fn,
        activation_location_type_and_pass_type,
        layer_index,
        attached_activation_dict,
    ) = create_activation_grabbing_hook_fn(
        attached_activation_dict, name, activation_index_for_backward_pass, batch_index=batch_index
    )
    transformer_graph.append(
        activation_location_type_and_pass_type,
        hook_fn,
        layer_indices=layer_index,
        append_to_fwd2=append_to_fwd2,
    )
    return attached_activation_dict
def create_ablating_hook_fn(
    ablation_spec: AblationSpec,
    batch_index: int,
) -> tuple[Callable, ActivationLocationTypeAndPassType, LayerIndex]:
    activation_location_type_and_pass_type = ActivationLocationTypeAndPassType(
        activation_location_type=ablation_spec.index.activation_location_type,
        pass_type=ablation_spec.index.pass_type,
    )
    layer_index = ablation_spec.index.layer_index
    def ablating_hook_fn(act: torch.Tensor, **kwargs: Any) -> torch.Tensor:
        # Initial dimension is batch
        act = act.clone()
        python_slice_operator: tuple[slice | int, ...] = (
            slice(batch_index, batch_index + 1),
        ) + make_python_slice_from_all_or_one_indices(ablation_spec.index.tensor_indices)
        assert len(python_slice_operator) == len(act.shape), (
            len(python_slice_operator),
            python_slice_operator,
            len(act.shape),
            act.shape,
        )
        act[python_slice_operator] = ablation_spec.value
        return act
    return ablating_hook_fn, activation_location_type_and_pass_type, layer_index
def add_ablating_hooks(
    transformer_graph: TransformerHookGraph,
    ablation_specs: list[AblationSpec],
    batch_index: int,
) -> None:
    """This is a helper function that returns a TransformerHooks object containing hooks at naturally existing hook locations
    (e.g. MLP post-activations, rather than autoencoder latent activations) for the given
    scalar derivers, and a dictionary mapping each activation location type with a naturally existing hook to the activations requested
    (before it is filled with activations during forward passes)."""
    for ablation_spec in ablation_specs:
        (
            hook_fn,
            activation_location_type_and_pass_type,
            layer_index,
        ) = create_ablating_hook_fn(ablation_spec, batch_index=batch_index)
        transformer_graph.append(
            activation_location_type_and_pass_type,
            hook_fn,
            layer_indices=layer_index,
        )
def create_saving_hook_fn(
    device: torch.device,
    activation_location_type_and_pass_type: ActivationLocationTypeAndPassType,
    unpadded_prompt_length: int,
    batch_index: int,
) -> tuple[Callable, dict[LayerIndex, torch.Tensor]]:
    requested_activations_by_layer_index = {}
    def saving_hook_fn(act: torch.Tensor, **kwargs: Any) -> torch.Tensor:
        layer_index = kwargs.get("layer", None)
        # First dimension is batch, second dimension is sequence length. We truncate the sequence
        # length to the unpadded prompt length. If the third dimension is also sequence length, we
        # truncate that too.
        shape_spec = (
            activation_location_type_and_pass_type.activation_location_type.shape_spec_per_token_sequence
        )
        def get_slice_for_dim(dim: Dimension) -> slice:
            if dim.is_sequence_token_dimension:
                return slice(None, unpadded_prompt_length)
            else:
                return slice(None)
        truncated_act = act[(batch_index,) + tuple([get_slice_for_dim(dim) for dim in shape_spec])]
        requested_activations_by_layer_index[layer_index] = truncated_act.detach().to(device)
        return act
    return saving_hook_fn, requested_activations_by_layer_index
def add_saving_hooks(
    transformer_graph: TransformerHookGraph,
    activation_location_type_and_pass_types: list[ActivationLocationTypeAndPassType],
    device: torch.device,
    unpadded_prompt_length: int,
    batch_index: int,
) -> dict[ActivationLocationTypeAndPassType, dict[LayerIndex, torch.Tensor]]:
    """This is a helper function that returns a TransformerHooks object containing hooks at naturally existing hook locations
    (e.g. MLP post-activations, rather than autoencoder latent activations) for the given
    scalar derivers, and a dictionary mapping each activation location type with a naturally existing hook to the activations requested
    (before it is filled with activations during forward passes)."""
    requested_activations_by_location_type_and_pass_type = {}
    for activation_location_type_and_pass_type in activation_location_type_and_pass_types:
        (
            hook_fn,
            requested_activations_by_location_type_and_pass_type[
                activation_location_type_and_pass_type
            ],
        ) = create_saving_hook_fn(
            device,
            activation_location_type_and_pass_type=activation_location_type_and_pass_type,
            unpadded_prompt_length=unpadded_prompt_length,
            batch_index=batch_index,
        )
        transformer_graph.append(
            activation_location_type_and_pass_type,
            hook_fn,
        )
    return requested_activations_by_location_type_and_pass_type
def apply_default_dst_configs_to_dst_and_config_list(
    model_context: StandardModelContext,
    multi_autoencoder_context: MultiAutoencoderContext | None,
    dst_and_config_list: list[tuple[DerivedScalarType, DstConfig | None]],
) -> list[tuple[DerivedScalarType, DstConfig]]:
    def get_default_dst_config(
        dst: DerivedScalarType,
    ) -> DstConfig:
        return DstConfig(
            model_context=model_context,
            multi_autoencoder_context=multi_autoencoder_context,
            derive_gradients=not dst.requires_grad_for_forward_pass,
        )
    return [
        (dst, config if config is not None else get_default_dst_config(dst))
        for dst, config in dst_and_config_list
    ]
def get_ds_computation_params_for_prompt(
    model_context: StandardModelContext,
    autoencoder_context: MultiAutoencoderContext | AutoencoderContext | None,
    dst_and_config_list: list[
        tuple[DerivedScalarType, DstConfig | None]
    ],  # None -> default config for dst
    prompt: str,
    loss_fn_for_backward_pass: Callable[[torch.Tensor], torch.Tensor] | None,
    trace_config: TraceConfig | None,
    ablation_specs: list[AblationSpec],
) -> DerivedScalarComputationParams:
    assert (loss_fn_for_backward_pass is None) or (trace_config is None)
    multi_autoencoder_context = MultiAutoencoderContext.from_context_or_multi_context(
        autoencoder_context
    )
    dst_and_config_list_with_default_config = apply_default_dst_configs_to_dst_and_config_list(
        model_context, multi_autoencoder_context, dst_and_config_list
    )
    multi_group_scalar_derivers_by_processing_step = {
        "dummy": MultiGroupScalarDerivers.from_dst_and_config_list(
            dst_and_config_list_with_default_config
        )  # "dummy" is a placeholder processing step name
    }
    input_token_ints = model_context.encode(prompt)
    return DerivedScalarComputationParams(
        input_token_ints=input_token_ints,
        multi_group_scalar_derivers_by_processing_step=multi_group_scalar_derivers_by_processing_step,
        loss_fn_for_backward_pass=loss_fn_for_backward_pass,
        device_for_raw_activations=model_context.device,
        ablation_specs=ablation_specs,
        trace_config=trace_config,
    )
def get_derived_scalars_for_prompt(
    model_context: StandardModelContext,
    dst_and_config_list: list[
        tuple[DerivedScalarType, DstConfig | None]
    ],  # None -> default config for dst
    prompt: str,
    loss_fn_for_backward_pass: Callable[[torch.Tensor], torch.Tensor] | None = None,
    trace_config: TraceConfig | None = None,
    autoencoder_context: MultiAutoencoderContext | AutoencoderContext | None = None,
    ablation_specs: list[AblationSpec] = [],
) -> tuple[DerivedScalarStore, InferenceAndTokenData, RawActivationStore]:
    """
    Lightweight function to populate a DerivedScalarStore given information specifying the prompt, loss function, and derived scalars to compute.
    """
    multi_autoencoder_context = MultiAutoencoderContext.from_context_or_multi_context(
        autoencoder_context
    )
    input_token_ints = model_context.encode(prompt)
    input_token_strings = [model_context.decode_token(token_int) for token_int in input_token_ints]
    dst_and_config_list_with_default_config = apply_default_dst_configs_to_dst_and_config_list(
        model_context, multi_autoencoder_context, dst_and_config_list
    )
    multi_group_scalar_derivers_by_processing_step = {
        "dummy": MultiGroupScalarDerivers.from_dst_and_config_list(
            dst_and_config_list_with_default_config
        )
    }  # "dummy" is a placeholder processing step name
    ds_computation_params = DerivedScalarComputationParams(
        input_token_ints=input_token_ints,
        multi_group_scalar_derivers_by_processing_step=multi_group_scalar_derivers_by_processing_step,
        loss_fn_for_backward_pass=loss_fn_for_backward_pass,
        device_for_raw_activations=model_context.device,
        trace_config=trace_config,
        ablation_specs=ablation_specs,
    )
    batched_multi_group_ds_store_by_processing_step: list[dict[str, MultiGroupDerivedScalarStore]]
    (
        batched_multi_group_ds_store_by_processing_step,
        batched_inference_data,
        batched_raw_activation_store,
    ) = compute_derived_scalar_groups_for_input_token_ints(
        model_context=model_context,
        multi_autoencoder_context=multi_autoencoder_context,
        batched_ds_computation_params=[ds_computation_params],
    )
    ds_store = batched_multi_group_ds_store_by_processing_step[0]["dummy"].to_single_ds_store()
    inference_data = batched_inference_data[0]
    inference_and_token_data = InferenceAndTokenData(
        **inference_data.dict(),
        tokens_as_ints=input_token_ints,
        tokens_as_strings=input_token_strings,
    )
    raw_activation_store = batched_raw_activation_store[0]
    return ds_store, inference_and_token_data, raw_activation_store
def get_batched_derived_scalars_for_prompt(
    model_context: StandardModelContext,
    batched_dst_and_config_list: list[
        list[tuple[DerivedScalarType, DstConfig | None]]
    ],  # None -> default config for dst
    batched_prompt: list[str],
    loss_fn_for_backward_pass: Callable[[torch.Tensor], torch.Tensor] | None = None,
    trace_config: TraceConfig | None = None,
    autoencoder_context: MultiAutoencoderContext | AutoencoderContext | None = None,
    ablation_specs: list[AblationSpec] = [],
) -> tuple[list[DerivedScalarStore], list[InferenceAndTokenData], list[RawActivationStore]]:
    """
    Lightweight function to populate a DerivedScalarStore given information specifying the prompt, loss function, and derived scalars to compute.
    """
    multi_autoencoder_context = MultiAutoencoderContext.from_context_or_multi_context(
        autoencoder_context
    )
    assert len(batched_dst_and_config_list) == len(batched_prompt)
    batched_ds_computation_params = [
        get_ds_computation_params_for_prompt(
            model_context=model_context,
            autoencoder_context=autoencoder_context,
            dst_and_config_list=dst_and_config_list,
            prompt=prompt,
            loss_fn_for_backward_pass=loss_fn_for_backward_pass,
            trace_config=trace_config,
            ablation_specs=ablation_specs,
        )
        for prompt, dst_and_config_list in zip(batched_prompt, batched_dst_and_config_list)
    ]
    batched_multi_group_ds_store_by_processing_step: list[dict[str, MultiGroupDerivedScalarStore]]
    (
        batched_multi_group_ds_store_by_processing_step,
        batched_inference_data,
        batched_raw_activation_store,
    ) = compute_derived_scalar_groups_for_input_token_ints(
        model_context=model_context,
        multi_autoencoder_context=multi_autoencoder_context,
        batched_ds_computation_params=batched_ds_computation_params,
    )
    batched_ds_store = [
        multi_group_ds_store["dummy"].to_single_ds_store()
        for multi_group_ds_store in batched_multi_group_ds_store_by_processing_step
    ]
    batched_inference_and_token_data = []
    for ds_computation_params, inference_data in zip(
        batched_ds_computation_params, batched_inference_data
    ):
        input_token_ints = ds_computation_params.input_token_ints
        input_token_strings = [
            model_context.decode_token(token_int) for token_int in input_token_ints
        ]
        batched_inference_and_token_data.append(
            InferenceAndTokenData(
                **inference_data.dict(),
                tokens_as_ints=input_token_ints,
                tokens_as_strings=input_token_strings,
            )
        )
    return batched_ds_store, batched_inference_and_token_data, batched_raw_activation_store

================
File: neuron_explainer/activation_server/dst_helpers.py
================
# Small helper functions for working with derived scalars in the context of activation server
# request handling.
import math
from typing import Any, Callable, TypeVar
import torch
from neuron_explainer.activation_server.requests_and_responses import *
from neuron_explainer.activations.derived_scalars.derived_scalar_store import DerivedScalarStore
from neuron_explainer.activations.derived_scalars.derived_scalar_types import DerivedScalarType
from neuron_explainer.activations.derived_scalars.indexing import (
    DerivedScalarIndex,
    MirroredNodeIndex,
)
from neuron_explainer.models.model_component_registry import Dimension
T = TypeVar("T")
def _float_tensor_to_list(x: torch.Tensor) -> list[float]:
    return [x if math.isfinite(x) else -999 for x in x.tolist()]
def _torch_to_tensor_nd(x: torch.Tensor) -> TensorND:
    ndim = x.ndim
    if ndim == 0:
        return Tensor0D(value=x.item())
    elif ndim == 1:
        return Tensor1D(value=_float_tensor_to_list(x))
    elif ndim == 2:
        return Tensor2D(value=[_float_tensor_to_list(row) for row in x])
    elif ndim == 3:
        return Tensor3D(value=[[_float_tensor_to_list(row) for row in matrix] for matrix in x])
    else:
        raise NotImplementedError(f"Unknown ndim: {ndim}")
def _get_dims_to_keep(
    dst: DerivedScalarType, keep_dimension_fn: Callable[[Dimension], bool]
) -> list[Dimension]:
    return [dim for dim in dst.shape_spec_per_token_sequence if keep_dimension_fn(dim)]
def _sum_dst(
    ds_store: DerivedScalarStore,
    dst: DerivedScalarType,
    keep_dimension_fn: Callable[[Dimension], bool],
    abs_mode: bool,
) -> torch.Tensor:
    dims_to_keep = _get_dims_to_keep(dst, keep_dimension_fn)
    store_for_dst = ds_store.filter_dsts([dst])
    activations_and_metadata = next(
        iter(store_for_dst.activations_and_metadata_by_dst_and_pass_type.values())
    )
    ndim_before_sum = len(activations_and_metadata.shape)
    if abs_mode:
        sum_for_dst = store_for_dst.sum_abs(dims_to_keep=dims_to_keep)
    else:
        sum_for_dst = store_for_dst.sum(dims_to_keep=dims_to_keep)
    assert len(sum_for_dst.shape) == len(
        dims_to_keep
    ), f"{sum_for_dst.shape=}, {ndim_before_sum=}, {dims_to_keep=}"
    return sum_for_dst
def get_intermediate_sum_by_dst(
    ds_store: DerivedScalarStore,
    keep_dimension_fn: Callable[[Dimension], bool],
    abs_mode: bool = False,
) -> dict[DerivedScalarType, TensorND]:
    dict_of_torch_tensors = {
        dst: _sum_dst(ds_store, dst, keep_dimension_fn, abs_mode=abs_mode) for dst in ds_store.dsts
    }
    return {dst: _torch_to_tensor_nd(x) for dst, x in dict_of_torch_tensors.items()}
def get_ds_index_from_node_index(
    node_index: MirroredNodeIndex,
    dsts: list[DerivedScalarType],
) -> DerivedScalarIndex:
    """
    Converts from a MirroredNodeIndex (more general, e.g. defined by a NodeType such as MLP neurons)
    to a DerivedScalarIndex (more specific, e.g. defined by a DerivedScalarType such as MLP write
    norm) conditional on the given derived scalar types, which are assumed to be unique for each
    NodeType.
    """
    dsts_matching_node_type = [dst for dst in dsts if dst.node_type == node_index.node_type]
    assert len(dsts_matching_node_type) == 1, (
        f"Expected exactly one derived scalar type to have node type {node_index.node_type}, "
        f"but found {dsts_matching_node_type} in {dsts}"
    )
    return DerivedScalarIndex.from_node_index(
        node_index=node_index,
        dst=dsts_matching_node_type[0],
    )
def assert_tensor(tensor: Any) -> torch.Tensor:
    # for mypy
    assert isinstance(tensor, torch.Tensor)
    return tensor

================
File: neuron_explainer/activation_server/explainer_routes.py
================
"""Routes / endpoints related to generating and scoring explanations."""
from __future__ import annotations
import os
import os.path as osp
from enum import Enum, unique
from typing import Any, TypeVar
from fastapi import FastAPI, HTTPException
from neuron_explainer.activation_server.explanation_datasets import (
    AZURE_EXPLANATION_DATASET_REGISTRY,
    get_local_cached_explanation_directory,
)
from neuron_explainer.activation_server.load_neurons import load_neuron_from_datasets
from neuron_explainer.activation_server.read_routes import NodeIdAndDatasets
from neuron_explainer.activations.activations import (
    ActivationRecordSliceParams,
    NeuronId,
    NeuronRecord,
)
from neuron_explainer.activations.derived_scalars import DerivedScalarType
from neuron_explainer.api_client import ApiClient
from neuron_explainer.explanations.attention_head_scoring import AttentionHeadOneAtATimeScorer
from neuron_explainer.explanations.explainer import (
    AttentionHeadExplainer,
    NeuronExplainer,
    TokenActivationPairExplainer,
)
from neuron_explainer.explanations.explanations import (
    AttentionSimulationResults,
    NeuronSimulationResults,
    ScoredAttentionExplanation,
    ScoredExplanation,
)
from neuron_explainer.explanations.prompt_builder import PromptFormat
from neuron_explainer.explanations.scoring import (
    make_simulator_and_score,
    make_uncalibrated_explanation_simulator,
)
from neuron_explainer.fast_dataclasses.fast_dataclasses import dumps, loads
from neuron_explainer.file_utils import file_exists, read_single_async
from neuron_explainer.models.model_component_registry import NodeType
from neuron_explainer.pydantic import CamelCaseBaseModel, immutable
T = TypeVar("T", bound="BaseMethodId")
@unique
class BaseMethodId(str, Enum):
    @classmethod
    def from_string(cls: type[T], s: str) -> T:
        for method_id in cls:
            if method_id.value == s:
                return method_id
        raise ValueError(f"{s} is not a valid {cls.__name__}")
@unique
class NeuronExplainAndScoreMethodId(BaseMethodId):
    BASELINE = "baseline"
_NEURON_EXPLAINER_REGISTRY: dict[NeuronExplainAndScoreMethodId, NeuronExplainer] = {
    NeuronExplainAndScoreMethodId.BASELINE: TokenActivationPairExplainer(
        model_name="gpt-4o",
        cache=True,
        prompt_format=PromptFormat.CHAT_MESSAGES,
    ),
}
@unique
class AttentionExplainAndScoreMethodId(BaseMethodId):
    BASELINE = "baseline"
# Maybe in the future will split this into one for the explainer and one for the scorer
_ATTENTION_EXPLAINER_REGISTRY: dict[
    AttentionExplainAndScoreMethodId, tuple[AttentionHeadExplainer, AttentionHeadOneAtATimeScorer]
] = {
    AttentionExplainAndScoreMethodId.BASELINE: (
        AttentionHeadExplainer(
            model_name="gpt-4o",
            prompt_format=PromptFormat.CHAT_MESSAGES,
            repeat_strongly_attending_pairs=True,
        ),
        AttentionHeadOneAtATimeScorer(
            model_name="gpt-4o",
            prompt_format=PromptFormat.CHAT_MESSAGES,
        ),
    )
}
@immutable
class ExplanationResult(CamelCaseBaseModel):
    explanations: list[str]
    # TODO(sbills): Get consistent about "dataset" vs "dataset_path".
    dataset: str
@immutable
class ScoreRequest(NodeIdAndDatasets):
    explanation: str
    max_sequences: int | None = None
@immutable
class ScoreResult(CamelCaseBaseModel):
    score: float
    dataset_path: str
@unique
class ActivationCategory(str, Enum):
    NEURON = "neuron"
    ATTENTION_HEAD = "attention_head"
def define_explainer_routes(
    app: FastAPI,
    neuron_method_id: NeuronExplainAndScoreMethodId,
    attention_head_method_id: AttentionExplainAndScoreMethodId,
) -> None:
    simulation_client = ApiClient(
        model_name="gpt-4o",
        max_concurrent=5,
        cache=True,
    )
    neuron_explainer = _NEURON_EXPLAINER_REGISTRY[neuron_method_id]
    attention_head_explainer, attention_head_scorer = _ATTENTION_EXPLAINER_REGISTRY[
        attention_head_method_id
    ]
    def _map_dst_to_activation_category(
        dst: DerivedScalarType,
    ) -> ActivationCategory:
        if dst in [
            DerivedScalarType.MLP_POST_ACT,
            DerivedScalarType.MLP_AUTOENCODER_LATENT,
            DerivedScalarType.ONLINE_MLP_AUTOENCODER_LATENT,
            DerivedScalarType.AUTOENCODER_LATENT,
            DerivedScalarType.ONLINE_AUTOENCODER_LATENT,
        ]:
            return ActivationCategory.NEURON
        elif dst in [
            DerivedScalarType.ATTN_WRITE_NORM,
            DerivedScalarType.ATTENTION_AUTOENCODER_LATENT,
            DerivedScalarType.ONLINE_ATTENTION_AUTOENCODER_LATENT,
            DerivedScalarType.FLATTENED_ATTN_WRITE_TO_LATENT_SUMMED_OVER_HEADS,
        ]:
            return ActivationCategory.ATTENTION_HEAD
        else:
            raise HTTPException(status_code=422, detail=f"Unsupported derived scalar type {dst}")
    def _get_azure_explanation_path(request: NodeIdAndDatasets, dataset_path: str) -> str | None:
        if dataset_path in AZURE_EXPLANATION_DATASET_REGISTRY:
            expl_dir = AZURE_EXPLANATION_DATASET_REGISTRY[dataset_path]
            return osp.join(expl_dir, str(request.layer_index), f"{request.activation_index}.jsonl")
        return None
    def _get_local_cached_explanation_path(request: NodeIdAndDatasets, dataset_path: str) -> str:
        if request.dst.node_type == NodeType.ATTENTION_HEAD:
            method_id_str = str(attention_head_method_id)
        else:
            method_id_str = str(neuron_method_id)
        cache_dir = get_local_cached_explanation_directory(dataset_path)
        return osp.join(
            cache_dir,
            f"cache_{request.dst}_{method_id_str}",
            str(request.layer_index),
            f"{request.activation_index}.jsonl",
        )
    def _verify_cached_simulation_results(
        request: NodeIdAndDatasets,
        simulation_results: Any,
    ) -> None:
        """Verifies the type and id of the cached simulation results."""
        if not isinstance(simulation_results, NeuronSimulationResults) and not isinstance(
            simulation_results, AttentionSimulationResults
        ):
            raise HTTPException(
                status_code=422, detail=f"Unexpected type {type(simulation_results)} in cache"
            )
        elem_id = (
            simulation_results.neuron_id
            if isinstance(simulation_results, NeuronSimulationResults)
            else simulation_results.attention_head_id
        )
        if (
            elem_id.layer_index != request.layer_index
            or elem_id.neuron_index != request.activation_index
        ):
            raise HTTPException(
                status_code=422,
                detail=f"Cache id mismatch: requested ({request.layer_index}, {request.activation_index}, cache contained ({elem_id.layer_index}, {elem_id.layer_index})",
            )
    def _merge_simulation_results(
        azure_simulation_results: NeuronSimulationResults | AttentionSimulationResults | None,
        local_simulation_results: NeuronSimulationResults | AttentionSimulationResults | None,
    ) -> NeuronSimulationResults | AttentionSimulationResults | None:
        """Merge scored explanations from the local cache and azure into a single NeuronSimulationResults
        or AttentionSimulationResults object."""
        if azure_simulation_results is None and local_simulation_results is None:
            return None
        if isinstance(azure_simulation_results, NeuronSimulationResults) or isinstance(
            local_simulation_results, NeuronSimulationResults
        ):
            assert (
                isinstance(azure_simulation_results, NeuronSimulationResults)
                or azure_simulation_results is None
            )
            assert (
                isinstance(local_simulation_results, NeuronSimulationResults)
                or local_simulation_results is None
            )
            unique_scored_explanations = {}
            if azure_simulation_results is not None:
                for scored_explanation in azure_simulation_results.scored_explanations:
                    unique_scored_explanations[scored_explanation.explanation] = scored_explanation
            if local_simulation_results is not None:
                for scored_explanation in local_simulation_results.scored_explanations:
                    unique_scored_explanations[scored_explanation.explanation] = scored_explanation
            return NeuronSimulationResults(
                neuron_id=(
                    azure_simulation_results.neuron_id
                    if azure_simulation_results is not None
                    else local_simulation_results.neuron_id  # type: ignore # mypy doesn't understand that both can't be None
                ),
                scored_explanations=list(unique_scored_explanations.values()),
            )
        else:
            assert (
                isinstance(azure_simulation_results, AttentionSimulationResults)
                or azure_simulation_results is None
            )
            assert (
                isinstance(local_simulation_results, AttentionSimulationResults)
                or local_simulation_results is None
            )
            unique_scored_attn_explanations = {}
            if azure_simulation_results is not None:
                for scored_attn_explanation in azure_simulation_results.scored_explanations:
                    unique_scored_attn_explanations[
                        scored_attn_explanation.explanation
                    ] = scored_attn_explanation
            if local_simulation_results is not None:
                for scored_attn_explanation in local_simulation_results.scored_explanations:
                    unique_scored_attn_explanations[
                        scored_attn_explanation.explanation
                    ] = scored_attn_explanation
            return AttentionSimulationResults(
                attention_head_id=(
                    azure_simulation_results.attention_head_id
                    if azure_simulation_results is not None
                    else local_simulation_results.attention_head_id  # type: ignore # mypy doesn't understand that both can't be None
                ),
                scored_explanations=list(unique_scored_attn_explanations.values()),
            )
    async def _check_disk_for_simulation_results(
        request: NodeIdAndDatasets, dataset_path: str
    ) -> NeuronSimulationResults | AttentionSimulationResults | None:
        """
        If the request is for scored explanations in one of the public sets on azure, return them if any exist.
        Include any scored explanations in the local cache. If there are no explanations in azure or the local
        cache, return None.
        """
        azure_path = _get_azure_explanation_path(request, dataset_path)
        cache_path = _get_local_cached_explanation_path(request, dataset_path)
        azure_simulation_results, local_simulation_results = None, None
        if azure_path is not None and file_exists(azure_path):
            azure_simulation_results = loads(
                await read_single_async(azure_path), backwards_compatible=False
            )
            _verify_cached_simulation_results(request, azure_simulation_results)
        if file_exists(cache_path):
            local_simulation_results = loads(
                await read_single_async(cache_path), backwards_compatible=False
            )
            _verify_cached_simulation_results(request, local_simulation_results)
        # Merge the results from azure and the local cache, deduplicating any scored explanations.
        # Thus we have a single object that contains all the scored explanations for the node.
        combined_simulation_results = _merge_simulation_results(
            azure_simulation_results, local_simulation_results
        )
        return combined_simulation_results
    async def _explain_neuron(neuron_record: NeuronRecord) -> list[str]:
        if neuron_record.max_activation < 0:
            raise HTTPException(status_code=422, detail="Neuron is not activated on the dataset")
        train_activation_records = neuron_record.train_activation_records(
            activation_record_slice_params=ActivationRecordSliceParams(n_examples_per_split=5)
        )
        return await neuron_explainer.generate_explanations(
            all_activations=train_activation_records,
            max_activation=neuron_record.max_activation,
        )
    # NeuronRecord contains attention head activation info (it's an outdated name)
    async def _explain_attention_head(attention_record: NeuronRecord) -> list[str]:
        train_activation_records = attention_record.train_activation_records(
            activation_record_slice_params=ActivationRecordSliceParams(n_examples_per_split=5)
        )
        return await attention_head_explainer.generate_explanations(
            all_activations=train_activation_records,
            max_tokens=50,
            num_top_pairs_to_display=5,
        )
    @app.post("/explain", response_model=ExplanationResult, tags=["explainer"])
    async def explain(request: NodeIdAndDatasets) -> ExplanationResult:
        dataset_path, neuron_record = await load_neuron_from_datasets(request)
        cached_simulation_results = await _check_disk_for_simulation_results(request, dataset_path)
        if cached_simulation_results is not None:
            explanations = [s.explanation for s in cached_simulation_results.scored_explanations]
        else:
            activation_category = _map_dst_to_activation_category(request.dst)
            if activation_category == ActivationCategory.ATTENTION_HEAD:
                explanations = await _explain_attention_head(neuron_record)
            elif activation_category == ActivationCategory.NEURON:
                explanations = await _explain_neuron(neuron_record)
            else:
                raise HTTPException(
                    status_code=422,
                    detail=f"Unsupported activation category for explanation: {activation_category}",
                )
        return ExplanationResult(explanations=explanations, dataset=dataset_path)
    async def _score_neuron(
        cached_simulation_results: NeuronSimulationResults | None,
        neuron: NeuronRecord,
        request: ScoreRequest,
        max_sequences: int | None = None,
    ) -> tuple[float | None, NeuronSimulationResults]:
        """Score an explanation for a neuron. Add it to the cached set of simulation
        results, or create the simulation results object if the cache was empty."""
        if neuron.max_activation < 0:
            raise HTTPException(status_code=422, detail="Neuron is not activated on the dataset")
        valid_activation_records = neuron.valid_activation_records(
            activation_record_slice_params=ActivationRecordSliceParams(n_examples_per_split=5)
        )
        if max_sequences is not None:
            valid_activation_records = valid_activation_records[:max_sequences]
        scored_simulation = await make_simulator_and_score(
            make_uncalibrated_explanation_simulator(
                request.explanation,
                simulation_client,
                prompt_format=PromptFormat.CHAT_MESSAGES,
            ),
            valid_activation_records,
        )
        scored_explanation = ScoredExplanation(
            explanation=request.explanation,
            scored_simulation=scored_simulation,
        )
        if cached_simulation_results is not None:
            cached_simulation_results.scored_explanations.append(scored_explanation)
        else:
            cached_simulation_results = NeuronSimulationResults(
                neuron_id=NeuronId(
                    neuron_index=request.activation_index,
                    layer_index=request.layer_index,
                ),
                scored_explanations=[scored_explanation],
            )
        return scored_explanation.get_preferred_score(), cached_simulation_results
    async def _score_attention_head(
        cached_simulation_results: AttentionSimulationResults | None,
        attention_record: NeuronRecord,
        request: ScoreRequest,
        max_sequences: int | None = None,
    ) -> tuple[float | None, AttentionSimulationResults]:
        """Score an explanation for an attention head. Add it to the cached set of simulation
        results, or create the simulation results object if the cache was empty."""
        if attention_record.max_activation < 0:
            raise HTTPException(
                status_code=422, detail="Attention head is not activated on the dataset"
            )
        valid_activation_records = attention_record.valid_activation_records(
            activation_record_slice_params=ActivationRecordSliceParams(n_examples_per_split=5)
        )
        if max_sequences is not None:
            valid_activation_records = valid_activation_records[:max_sequences]
        scored_attention_simulation = await attention_head_scorer.score_explanation(
            activation_records=valid_activation_records,
            explanation=request.explanation,
            max_activation=attention_record.max_activation,
        )
        scored_explanation = ScoredAttentionExplanation(
            explanation=request.explanation,
            scored_attention_simulation=scored_attention_simulation,
        )
        if cached_simulation_results is not None:
            cached_simulation_results.scored_explanations.append(scored_explanation)
        else:
            cached_simulation_results = AttentionSimulationResults(
                attention_head_id=NeuronId(
                    neuron_index=request.activation_index,
                    layer_index=request.layer_index,
                ),
                scored_explanations=[scored_explanation],
            )
        return scored_explanation.get_preferred_score(), cached_simulation_results
    def _cache_simulation_results_locally(
        request: ScoreRequest,
        dataset_path: str,
        cached_simulation_results: NeuronSimulationResults | AttentionSimulationResults,
    ) -> None:
        # Overwrite the cache with the updated set of simulation results.
        # Always cache locally because we can't write to the public azure bucket.
        cache_path = _get_local_cached_explanation_path(request, dataset_path)
        # Create the directories if they don't exist
        os.makedirs(os.path.dirname(cache_path), exist_ok=True)
        with open(cache_path, "wb") as f:
            f.write(dumps(cached_simulation_results))
    @app.post("/score", response_model=ScoreResult, tags=["explainer"])
    async def score(request: ScoreRequest) -> ScoreResult:
        dataset_path, neuron_record = await load_neuron_from_datasets(request)
        cached_simulation_results = await _check_disk_for_simulation_results(request, dataset_path)
        # Cache hit: return the score for the matching explanation.
        if cached_simulation_results is not None and any(
            s.explanation == request.explanation
            for s in cached_simulation_results.scored_explanations
        ):
            score = [
                s.get_preferred_score()
                for s in cached_simulation_results.scored_explanations
                if s.explanation == request.explanation
            ][0]
            if score is None:
                raise HTTPException(status_code=500, detail="Score is unexpectedly undefined")
            return ScoreResult(score=score, dataset_path=dataset_path)
        # Cache miss: compute the score for the requested explanation.
        activation_category = _map_dst_to_activation_category(request.dst)
        if activation_category == ActivationCategory.ATTENTION_HEAD:
            assert cached_simulation_results is None or isinstance(
                cached_simulation_results, AttentionSimulationResults
            )
            # Score and update the cache storage object.
            score, cached_simulation_results = await _score_attention_head(
                cached_simulation_results,
                neuron_record,
                request,
                max_sequences=request.max_sequences,
            )
        elif activation_category == ActivationCategory.NEURON:
            assert cached_simulation_results is None or isinstance(
                cached_simulation_results, NeuronSimulationResults
            )
            # Score and update the cache storage object.
            score, cached_simulation_results = await _score_neuron(
                cached_simulation_results,
                neuron_record,
                request,
                max_sequences=request.max_sequences,
            )
        else:
            raise HTTPException(
                status_code=422,
                detail=f"Unsupported activation category for scoring: {activation_category}",
            )
        if score is None:
            raise HTTPException(status_code=500, detail="Score is unexpectedly undefined")
        _cache_simulation_results_locally(request, dataset_path, cached_simulation_results)
        return ScoreResult(score=score, dataset_path=dataset_path)

================
File: neuron_explainer/activation_server/explanation_datasets.py
================
import os
from neuron_explainer.activation_server.load_neurons import convert_dataset_path_to_short_name
# Maps from neuron dataset path to explanation dataset path.
AZURE_EXPLANATION_DATASET_REGISTRY = {
    "https://openaipublic.blob.core.windows.net/neuron-explainer/data/collated-activations/": "https://openaipublic.blob.core.windows.net/neuron-explainer/data/explanations/",
    "https://openaipublic.blob.core.windows.net/neuron-explainer/gpt2_small_data/collated-activations/": "https://openaipublic.blob.core.windows.net/neuron-explainer/gpt2_small_data/explanations/",
}
def get_local_cached_explanation_directory(dataset_path: str) -> str:
    root_project_directory = os.path.dirname(
        os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    )
    dataset_short_name = convert_dataset_path_to_short_name(dataset_path)
    return f"{root_project_directory}/cached_explanations/{dataset_short_name}"
async def get_all_explanation_datasets(neuron_dataset: str) -> list[str]:
    """
    Get all explanation datasets for a given neuron dataset. Search the public azure bucket and also
    the local filesystem cache. Returns a list of paths to the explanation datasets.
    Path can be an azure path (beginning with `https://`) or a local path.
    """
    datasets = []
    if neuron_dataset in AZURE_EXPLANATION_DATASET_REGISTRY:
        datasets.append(AZURE_EXPLANATION_DATASET_REGISTRY[neuron_dataset])
    local_cache_dir = get_local_cached_explanation_directory(neuron_dataset)
    # Iterate through folders to get a list of dirs.
    # There will be different local cache directories if the user generates scored explanations for
    # the same neuron dataset using different neuron/attention explainer registry entries (i.e. so
    # that AttentionExplainAndScoreMethodId or NeuronExplainAndScoreMethodId differ).
    if os.path.exists(local_cache_dir) and os.path.isdir(local_cache_dir):
        for entry in os.listdir(local_cache_dir):
            candidate_path = os.path.join(local_cache_dir, entry)
            if os.path.isdir(candidate_path):
                datasets.append(candidate_path)
    return datasets

================
File: neuron_explainer/activation_server/inference_routes.py
================
"""Routes / endpoints related to performing inference on the subject model."""
from fastapi import FastAPI, HTTPException
from neuron_explainer.activation_server.interactive_model import InteractiveModel
from neuron_explainer.activation_server.requests_and_responses import (
    BatchedRequest,
    BatchedResponse,
    BatchedTdbRequest,
    DerivedAttentionScalarsRequest,
    DerivedAttentionScalarsResponse,
    DerivedScalarsRequest,
    DerivedScalarsResponse,
    ModelInfoResponse,
    MultipleTopKDerivedScalarsRequest,
    MultipleTopKDerivedScalarsResponse,
)
def define_inference_routes(
    app: FastAPI,
    model: InteractiveModel | None,
    mlp_autoencoder_name: str | None,
    attn_autoencoder_name: str | None,
) -> None:
    def assert_model() -> None:
        if model is None:
            raise HTTPException(
                status_code=500,
                detail="Inference model not running. Restart the activation server with run_model=True to use inference endpoints.",
            )
    @app.post("/derived_scalars", response_model=DerivedScalarsResponse, tags=["inference"])
    async def derived_scalars(request: DerivedScalarsRequest) -> DerivedScalarsResponse:
        assert_model()
        assert model is not None  # redundant; needed for mypy
        return await model.get_derived_scalars(request)
    @app.post(
        "/derived_attention_scalars",
        response_model=DerivedAttentionScalarsResponse,
        tags=["inference"],
    )
    async def derived_attention_scalars(
        request: DerivedAttentionScalarsRequest,
    ) -> DerivedAttentionScalarsResponse:
        assert_model()
        assert model is not None  # redundant; needed for mypy
        return await model.get_derived_attention_scalars(request)
    @app.post(
        "/multiple_top_k_derived_scalars",
        response_model=MultipleTopKDerivedScalarsResponse,
        tags=["inference"],
    )
    async def multiple_top_k_derived_scalars(
        request: MultipleTopKDerivedScalarsRequest,
    ) -> MultipleTopKDerivedScalarsResponse:
        assert_model()
        assert model is not None  # redundant; needed for mypy
        return await model.get_multiple_top_k_derived_scalars(request)
    @app.post("/batched", response_model=BatchedResponse, tags=["inference"])
    async def batched(request: BatchedRequest) -> BatchedResponse:
        assert_model()
        assert model is not None  # redundant; needed for mypy
        return await model.handle_batched_request(request)
    @app.post("/batched_tdb", response_model=BatchedResponse, tags=["inference"])
    async def batched_tdb(request: BatchedTdbRequest) -> BatchedResponse:
        assert_model()
        assert model is not None  # redundant; needed for mypy
        return await model.handle_batched_tdb_request(request)
    @app.post("/model_info", response_model=ModelInfoResponse, tags=["inference"])
    def model_info() -> ModelInfoResponse:
        assert_model()
        assert model is not None  # redundant; needed for mypy
        return model.get_model_info(
            mlp_autoencoder_name=mlp_autoencoder_name, attn_autoencoder_name=attn_autoencoder_name
        )

================
File: neuron_explainer/activation_server/interactive_model.py
================
"""
Class for performing inference on a model in real time, i.e. in the context of interactive user
requests.
Requests consist of two parts: an InferenceRequestSpec and a XRequestSpec.
- InferenceRequestSpec contains the information necessary for computing a forward and backward
  pass on a model (prompt, loss function, in future ablation info).
- XRequestSpec contains the information necessary for computing a derived scalar of type X
  (activation, derived scalar type, layer index, etc.) This can include information necessary for
  inserting hooks into the model, e.g. in the case of the online autoencoder latent.
Functions for handling requests first compute DerivedScalarStore, then call a helper function that
takes XRequestSpec + DerivedScalarStore as input. CombinedRequestSpec contains
InferenceRequestSpec + a list of [XRequestSpec, YRequestSpec, ...]. It first computes
DerivedScalarStore, then calls relevant helper functions to generate a response containing sub
responses for the various sub request specs.
"""
import asyncio
from collections import defaultdict
from dataclasses import asdict, dataclass
from typing import Callable, TypeVar
import torch
from fastapi import HTTPException
from neuron_explainer.activation_server.derived_scalar_computation import (
    DerivedScalarComputationParams,
    DstAndConfigsByProcessingStep,
    InferenceAndTokenData,
    InferenceData,
    compute_derived_scalar_groups_for_input_token_ints,
    maybe_construct_loss_fn_for_backward_pass,
)
from neuron_explainer.activation_server.dst_helpers import (
    assert_tensor,
    get_intermediate_sum_by_dst,
)
from neuron_explainer.activation_server.load_neurons import load_neuron_from_datasets
from neuron_explainer.activation_server.read_routes import (
    TokenAndRawAttentionScalars,
    normalize_attention_token_scalars,
    zip_tokens_and_attention_activations,
)
from neuron_explainer.activation_server.requests_and_responses import *
from neuron_explainer.activation_server.tdb_conversions import (
    convert_tdb_request_spec_to_inference_sub_request,
)
from neuron_explainer.activations.derived_scalars.derived_scalar_store import DerivedScalarStore
from neuron_explainer.activations.derived_scalars.derived_scalar_types import DerivedScalarType
from neuron_explainer.activations.derived_scalars.indexing import (
    DerivedScalarIndex,
    MirroredNodeIndex,
    NodeIndex,
    TraceConfig,
)
from neuron_explainer.activations.derived_scalars.multi_group import (
    MultiGroupDerivedScalarStore,
    MultiGroupScalarDerivers,
)
from neuron_explainer.activations.derived_scalars.postprocessing import (
    DerivedScalarPostprocessor,
    TokenPairAttributionConverter,
    TokenReadConverter,
    TokenWriteConverter,
)
from neuron_explainer.activations.derived_scalars.scalar_deriver import DstConfig
from neuron_explainer.activations.derived_scalars.tokens import (
    TopTokens,
    get_most_upvoted_and_downvoted_tokens_for_nodes,
)
from neuron_explainer.models.autoencoder_context import AutoencoderContext, MultiAutoencoderContext
from neuron_explainer.models.inference_engine_type_registry import InferenceEngineType
from neuron_explainer.models.model_component_registry import (
    ActivationLocationType,
    Dimension,
    NodeType,
    PassType,
)
from neuron_explainer.models.model_context import ModelContext, StandardModelContext
from neuron_explainer.models.transformer import Transformer
from neuron_explainer.pydantic import CamelCaseBaseModel, immutable
T = TypeVar("T")
TOKEN_READ_DSTS: list[DerivedScalarType] = [
    DerivedScalarType.VOCAB_TOKEN_WRITE_TO_INPUT_DIRECTION,
]
PROMPT_LENGTH_LIMIT = 500
@immutable
class TopKData(CamelCaseBaseModel):
    """The results of an individual top-k operation, which is represented as a TopKParams."""
    activations: list[float]
    node_indices: list[MirroredNodeIndex]
    vocab_token_strings_for_indices: list[str | None] | None
    # This is the total of all activations, including non-top-k activations.
    intermediate_sum_activations_by_dst: dict[DerivedScalarType, TensorND]
@dataclass(frozen=True)
class RequestResponseCorrespondence:
    request_class: type
    request_spec_class: type
    request_spec_name: str
    response_class: type
    response_data_class: type
    response_data_name: str
REQUEST_RESPONSE_CORRESPONDENCE_REGISTRY: list[RequestResponseCorrespondence] = [
    RequestResponseCorrespondence(
        request_class=DerivedScalarsRequest,
        request_spec_class=DerivedScalarsRequestSpec,
        request_spec_name="derived_scalars_request_spec",
        response_class=DerivedScalarsResponse,
        response_data_class=DerivedScalarsResponseData,
        response_data_name="derived_scalars_response_data",
    ),
    RequestResponseCorrespondence(
        request_class=DerivedAttentionScalarsRequest,
        request_spec_class=DerivedAttentionScalarsRequestSpec,
        request_spec_name="derived_attention_scalars_request_spec",
        response_class=DerivedAttentionScalarsResponse,
        response_data_class=DerivedAttentionScalarsResponseData,
        response_data_name="derived_attention_scalars_response_data",
    ),
    RequestResponseCorrespondence(
        request_class=MultipleTopKDerivedScalarsRequest,
        request_spec_class=MultipleTopKDerivedScalarsRequestSpec,
        request_spec_name="multiple_top_k_derived_scalars_request_spec",
        response_class=MultipleTopKDerivedScalarsResponse,
        response_data_class=MultipleTopKDerivedScalarsResponseData,
        response_data_name="multiple_top_k_derived_scalars_response_data",
    ),
    RequestResponseCorrespondence(
        request_class=ScoredTokensRequest,
        request_spec_class=ScoredTokensRequestSpec,
        request_spec_name="scored_tokens_request_spec",
        response_class=ScoredTokensResponse,
        response_data_class=ScoredTokensResponseData,
        response_data_name="scored_tokens_response_data",
    ),
    RequestResponseCorrespondence(
        request_class=TokenPairAttributionRequest,
        request_spec_class=TokenPairAttributionRequestSpec,
        request_spec_name="token_pair_attribution_request_spec",
        response_class=TokenPairAttributionResponse,
        response_data_class=TokenPairAttributionResponseData,
        response_data_name="token_pair_attribution_response_data",
    ),
]
def get_corresponding_object(
    object: str | type, object_category: str, desired_category: str
) -> str | type:
    correspondence_of_interest = [
        correspondence
        for correspondence in REQUEST_RESPONSE_CORRESPONDENCE_REGISTRY
        if getattr(correspondence, object_category) == object
    ]
    assert (
        len(correspondence_of_interest) == 1
    ), f"Found {len(correspondence_of_interest)} correspondences for {object_category} {object}"
    return getattr(correspondence_of_interest[0], desired_category)
def _make_vocab_token_string_for_node_index(
    model_context: ModelContext, node_index: NodeIndex
) -> str | None:
    if node_index.node_type == NodeType.VOCAB_TOKEN:
        last_index = node_index.tensor_indices[-1]
        if last_index is None or last_index >= model_context.n_vocab:
            return None
        return model_context.decode_token(last_index)
    return None
def _make_vocab_token_strings_for_indices(
    model_context: ModelContext, activation_indices: list[NodeIndex]
) -> list[str | None] | None:
    vocab_token_strings_for_indices = [
        _make_vocab_token_string_for_node_index(model_context, node_index)
        for node_index in activation_indices
    ]
    if all(vocab_token_string is None for vocab_token_string in vocab_token_strings_for_indices):
        return None
    else:
        return vocab_token_strings_for_indices
def _unique_list_in_original_order(original_list: list[T]) -> list[T]:
    """
    Returns a list containing the unique elements of the original list, in the same order as the
    original list. `list(set(original_list))` does not preserve the original order.
    """
    unique_list = []
    seen = set()
    for item in original_list:
        if item not in seen:
            unique_list.append(item)
            seen.add(item)
    return unique_list
class InteractiveModel:
    def __init__(
        self,
        transformer: Transformer,
        standard_model_context: StandardModelContext,
        autoencoder_context: AutoencoderContext | MultiAutoencoderContext | None = None,
    ) -> None:
        self.transformer = transformer
        self._standard_model_context = standard_model_context
        self._multi_autoencoder_context = MultiAutoencoderContext.from_context_or_multi_context(
            autoencoder_context
        )
        # We only allow one batched request to be handled at a time. Concurrent batched requests
        # tend to result in cuda OOMs.
        self._batched_request_lock = asyncio.Lock()
    @property
    def has_mlp_autoencoder(self) -> bool:
        return self._multi_autoencoder_context is not None and any(
            node_type == NodeType.MLP_AUTOENCODER_LATENT
            for node_type in self._multi_autoencoder_context.autoencoder_context_by_node_type.keys()
        )
    @property
    def has_attention_autoencoder(self) -> bool:
        return self._multi_autoencoder_context is not None and any(
            node_type == NodeType.ATTENTION_AUTOENCODER_LATENT
            for node_type in self._multi_autoencoder_context.autoencoder_context_by_node_type.keys()
        )
    def get_model_info(
        self, mlp_autoencoder_name: str | None, attn_autoencoder_name: str | None
    ) -> ModelInfoResponse:
        return ModelInfoResponse(
            model_name=self._standard_model_context.model_name,
            has_mlp_autoencoder=self.has_mlp_autoencoder,
            has_attention_autoencoder=self.has_attention_autoencoder,
            n_layers=self._standard_model_context.n_layers,
            mlp_autoencoder_name=mlp_autoencoder_name,
            attention_autoencoder_name=attn_autoencoder_name,
        )
    def encode(self, string: str) -> list[int]:
        return self._standard_model_context.encode(string)
    @classmethod
    def from_model_name(cls, model_name: str) -> "InteractiveModel":
        standard_model_context = StandardModelContext(model_name=model_name)
        return cls.from_standard_model_context(standard_model_context)
    @classmethod
    def from_standard_model_context(
        cls, standard_model_context: StandardModelContext
    ) -> "InteractiveModel":
        return cls(
            transformer=standard_model_context.get_or_create_model(),
            standard_model_context=standard_model_context,
        )
    @classmethod
    def from_standard_model_context_and_autoencoder_context(
        cls,
        standard_model_context: StandardModelContext,
        autoencoder_context: AutoencoderContext | MultiAutoencoderContext,
    ) -> "InteractiveModel":
        return cls(
            transformer=standard_model_context.get_or_create_model(),
            standard_model_context=standard_model_context,
            autoencoder_context=autoencoder_context,
        )
    async def _handle_inference_request(
        self, inference_request: InferenceRequest
    ) -> InferenceResponse:
        request_type = type(inference_request)
        processing_spec_name = get_corresponding_object(
            request_type, "request_class", "request_spec_name"
        )
        assert isinstance(processing_spec_name, str)
        response_class = get_corresponding_object(request_type, "request_class", "response_class")
        assert isinstance(response_class, type)
        response_data_name = get_corresponding_object(
            request_type, "request_class", "response_data_name"
        )
        assert isinstance(response_data_name, str)
        processing_request_spec = getattr(inference_request, processing_spec_name)
        # We handle the singular case by wrapping the request in a batched request, to avoid the need to
        # special-case non-batched requests.
        batched_request = BatchedRequest(
            inference_sub_requests=[
                InferenceSubRequest(
                    inference_request_spec=inference_request.inference_request_spec,
                    processing_request_spec_by_name={processing_spec_name: processing_request_spec},
                )
            ]
        )
        batched_response = await self.handle_batched_request(batched_request)
        assert len(batched_response.inference_sub_responses) == 1
        sub_response = batched_response.inference_sub_responses[0]
        return response_class(
            inference_and_token_data=sub_response.inference_response.inference_and_token_data,
            **{
                response_data_name: sub_response.processing_response_data_by_name[
                    processing_spec_name
                ]
            },
        )
    async def get_derived_scalars(
        self, inference_request: DerivedScalarsRequest
    ) -> DerivedScalarsResponse:
        response = await self._handle_inference_request(inference_request)
        assert isinstance(response, DerivedScalarsResponse)
        return response
    async def _get_derived_scalars_from_ds_store(
        self,
        request_spec: DerivedScalarsRequestSpec,
        ds_store: DerivedScalarStore,
    ) -> DerivedScalarsResponseData:
        ds_index = DerivedScalarIndex(
            dst=request_spec.dst,
            pass_type=request_spec.pass_type,
            layer_index=request_spec.layer_index,
            tensor_indices=(None, request_spec.activation_index),
        )
        activations = ds_store[ds_index]
        index_of_sequence = NodeIndex.from_ds_index(ds_index)
        index_base_dict = asdict(index_of_sequence)
        index_base_dict.pop("tensor_indices")
        activations_to_return = assert_tensor(activations)
        assert activations_to_return.ndim == 1, activations_to_return.shape
        indices_to_return = []
        for token_index in range(activations_to_return.shape[0]):
            indices_to_return.append(
                MirroredNodeIndex(
                    **index_base_dict,
                    tensor_indices=(token_index,) + index_of_sequence.tensor_indices[1:],
                )
            )
        if request_spec.normalize_activations_using_neuron_record is None:
            normalized_activations = None
        else:
            _, neuron_record = await load_neuron_from_datasets(
                request_spec.normalize_activations_using_neuron_record
            )
            normalized_activations = (
                torch.clamp(activations_to_return, min=0) / neuron_record.max_activation
            ).tolist()
        return DerivedScalarsResponseData(
            activations=activations_to_return.tolist(),
            normalized_activations=normalized_activations,
            node_indices=indices_to_return,
            top_tokens=self._get_top_tokens(
                request_spec, ds_store, indices_to_return, activations_to_return
            ),
        )
    def _get_top_tokens(
        self,
        request_spec: DerivedScalarsRequestSpec,
        ds_store: DerivedScalarStore,
        indices_to_return: list[MirroredNodeIndex],
        activations_to_return: torch.Tensor,
    ) -> TopTokens | None:
        top_and_bottom_t_tokens_upvoted = request_spec.num_top_tokens
        if top_and_bottom_t_tokens_upvoted is None:
            # This data wasn't requested.
            return None
        else:
            assert top_and_bottom_t_tokens_upvoted > 0
            if request_spec.dst in TOKEN_READ_DSTS:
                # This DST is used for calculating token reads: basically, which vocab tokens most
                # "upvote" the node of interest. In this case, we don't use TokenWriteConverter.
                # Instead, we perform the usual top-t logic on the raw activations from the
                # DerivedScalarStore.
                token_write = ds_store[
                    DerivedScalarIndex(
                        dst=request_spec.dst,
                        pass_type=request_spec.pass_type,
                        layer_index=None,
                        tensor_indices=(0, None),  # (First sequence token, all activations)
                    )
                ]
                # In some cases the token write may be all 0s, for example if we're handling an
                # autoencoder latent whose activation on this token is zero. Return None in this
                # case.
                if torch.all(token_write == 0):
                    return None
                # We never need to do the flipping logic for token reads, since we use an ablation
                # to force the gradient to be positive.
                flip_upvoted_and_downvoted = False
            else:
                # The request is using a regular DST. We apply the TokenWriteConverter to get the
                # token write for one of the requested nodes.
                token_write_converter = TokenWriteConverter(
                    model_context=self._standard_model_context,
                    multi_autoencoder_context=self._multi_autoencoder_context,
                )
                token_write = token_write_converter.postprocess(
                    # We can use any of the indices to return, since they all match in all aspects
                    # except the sequence token index.
                    indices_to_return[0],
                    ds_store,
                )
                if activations_to_return[0] == 0:
                    # If the activation is 0, we don't get any information about the top and bottom
                    # tokens. In GELU models this should only happen for autoencoder latents, which
                    # tend to have sparse activations.
                    return None
                else:
                    # If the activation is negative, the top and bottom tokens need to be flipped.
                    # This means that we swap upvoted/downvoted and positive/negative for the
                    # associated scalars.
                    flip_upvoted_and_downvoted = activations_to_return[0].item() < 0
        # Unsqueeze to get the shape expected by the helper functions that do the top-t logic.
        token_write = token_write.unsqueeze(0)
        assert (
            token_write.ndim == 2
        ), f"Expected token_write.ndim == 2, but got {token_write.shape=}"
        assert torch.isfinite(
            token_write
        ).all(), "token_write tensor should only contain finite values"
        return get_most_upvoted_and_downvoted_tokens_for_nodes(
            self._standard_model_context,
            token_write,
            top_and_bottom_t_tokens_upvoted,
            flip_upvoted_and_downvoted,
        )[0]
    async def get_derived_attention_scalars(
        self, inference_request: DerivedAttentionScalarsRequest
    ) -> DerivedAttentionScalarsResponse:
        response = await self._handle_inference_request(inference_request)
        assert isinstance(response, DerivedAttentionScalarsResponse)
        return response
    async def _get_derived_attention_scalars_from_ds_store(
        self,
        request_spec: DerivedAttentionScalarsRequestSpec,
        ds_store: DerivedScalarStore,
        tokens_as_ints: list[int],
    ) -> DerivedAttentionScalarsResponseData:
        if request_spec.dst == DerivedScalarType.UNFLATTENED_ATTN_WRITE_NORM:
            # Dimensions for DerivedScalarType.UNFLATTENED_ATTN_WRITE_NORM: (
            #     Dimension.SEQUENCE_TOKENS,
            #     Dimension.ATTENDED_TO_SEQUENCE_TOKENS,
            #     Dimension.ATTN_HEADS,
            # )
            head_index = request_spec.activation_index
            tensor_indices = (None, None, head_index)  # type: tuple[int | None, ...]
        elif request_spec.dst == DerivedScalarType.ATTN_WRITE_TO_LATENT_SUMMED_OVER_HEADS:
            # Dimensions for DerivedScalarType.ATTN_WRITE_TO_LATENT_SUMMED_OVER_HEADS: (
            #     Dimension.SEQUENCE_TOKENS,
            #     Dimension.ATTENDED_TO_SEQUENCE_TOKENS,
            #     Dimension.SINGLETON,
            # )
            tensor_indices = (None, None, 0)
        else:
            raise NotImplementedError(f"Unsupported DST: {request_spec.dst}")
        ds_index = DerivedScalarIndex(
            dst=request_spec.dst,
            pass_type=PassType.FORWARD,
            layer_index=request_spec.layer_index,
            tensor_indices=tensor_indices,
        )
        activations = assert_tensor(ds_store[ds_index])
        assert activations.ndim == 2
        token_and_raw_attention_scalars_list: list[TokenAndRawAttentionScalars] = []
        tokens_as_strings = [
            self._standard_model_context.decode_token(token) for token in tokens_as_ints
        ]
        assert len(tokens_as_strings) == activations.shape[0] == activations.shape[1]
        for i in range(len(tokens_as_strings)):
            # We already indexed by the attention head (last dimension), so now we index by sequence
            # token and attended-to token. For the attended-to token, we want the current token and
            # all preceding tokens. (Subsequent tokens are masked.) This flattened representation is
            # a bit odd, but it's what the normalization function expects.
            scalars = activations[i, : i + 1]
            assert scalars.ndim == 1, scalars.ndim
            token_and_raw_attention_scalars_list.append(
                TokenAndRawAttentionScalars(
                    token=tokens_as_strings[i],
                    scalars=scalars.tolist(),
                )
            )
        list_of_sequence_lists = [[token_and_raw_attention_scalars_list]]
        if request_spec.normalize_activations_using_neuron_record is not None:
            _, neuron_record = await load_neuron_from_datasets(
                request_spec.normalize_activations_using_neuron_record
            )
            # We add the most positive activation records to the list of sequence lists used for
            # normalization. We don't care about the results for those sequences, but including them
            # means that we'll get the appropriate max values for normalization.
            list_of_sequence_lists.append(
                zip_tokens_and_attention_activations(neuron_record.most_positive_activation_records)
            )
        # This function handles nested lists, so we need to nest and unnest when invoking it.
        # If we added the most positive activation records to the list of sequence lists, they will
        # effectively be dropped when we index into the result (they're at index 1).
        token_and_attention_scalars_list = normalize_attention_token_scalars(
            list_of_sequence_lists
        )[0][0]
        return DerivedAttentionScalarsResponseData(
            token_and_attention_scalars_list=token_and_attention_scalars_list
        )
    async def get_scored_tokens(self, request: ScoredTokensRequest) -> ScoredTokensResponse:
        response = await self._handle_inference_request(request)
        assert isinstance(response, ScoredTokensResponse)
        return response
    def _get_token_scoring_postprocessor(
        self, token_scoring_type: TokenScoringType
    ) -> DerivedScalarPostprocessor:
        match token_scoring_type:
            case TokenScoringType.UPVOTED_OUTPUT_TOKENS:
                return TokenWriteConverter(
                    model_context=self._standard_model_context,
                    multi_autoencoder_context=self._multi_autoencoder_context,
                )
            case (
                TokenScoringType.INPUT_TOKENS_THAT_UPVOTE_MLP
                | TokenScoringType.INPUT_TOKENS_THAT_UPVOTE_ATTN_Q
                | TokenScoringType.INPUT_TOKENS_THAT_UPVOTE_ATTN_K
            ):
                return TokenReadConverter(
                    model_context=self._standard_model_context,
                    multi_autoencoder_context=self._multi_autoencoder_context,
                )
            case _:
                raise NotImplementedError(f"Unsupported token_scoring_type: {token_scoring_type}")
    def _should_score_node(self, node_type: NodeType, token_scoring_type: TokenScoringType) -> bool:
        match token_scoring_type:
            case TokenScoringType.UPVOTED_OUTPUT_TOKENS:
                return True
            case TokenScoringType.INPUT_TOKENS_THAT_UPVOTE_MLP:
                return node_type in [
                    NodeType.MLP_NEURON,
                    NodeType.AUTOENCODER_LATENT,
                    NodeType.MLP_AUTOENCODER_LATENT,
                ]
            case (
                TokenScoringType.INPUT_TOKENS_THAT_UPVOTE_ATTN_Q
                | TokenScoringType.INPUT_TOKENS_THAT_UPVOTE_ATTN_K
            ):
                # TODO(dan): These token scoring types are currently disabled. Work through the
                # errors, then re-enable them.
                # return node_type in [NodeType.ATTENTION_HEAD, NodeType.AUTOENCODER_LATENT_BY_TOKEN_PAIR]
                return False
            case _:
                raise NotImplementedError(f"Unsupported token_scoring_type: {token_scoring_type}")
    def _transform_node_indices_for_attn_q_or_k(
        self,
        all_node_indices: list[NodeIndex],
        activation_location_type: ActivationLocationType,
    ) -> list[NodeIndex]:
        return [
            (
                node_index.to_subnode_index(activation_location_type)
                # mypy has trouble figuring out the type of this list comprehension.
                if node_index.node_type == NodeType.ATTENTION_HEAD
                else node_index
            )
            for node_index in all_node_indices
        ]
    def _transform_node_indices(
        self, all_node_indices: list[NodeIndex], token_scoring_type: TokenScoringType
    ) -> list[NodeIndex]:
        match token_scoring_type:
            case (
                TokenScoringType.UPVOTED_OUTPUT_TOKENS
                | TokenScoringType.INPUT_TOKENS_THAT_UPVOTE_MLP
            ):
                return all_node_indices
            case TokenScoringType.INPUT_TOKENS_THAT_UPVOTE_ATTN_Q:
                return self._transform_node_indices_for_attn_q_or_k(
                    all_node_indices, ActivationLocationType.ATTN_QUERY
                )
            case TokenScoringType.INPUT_TOKENS_THAT_UPVOTE_ATTN_K:
                return self._transform_node_indices_for_attn_q_or_k(
                    all_node_indices, ActivationLocationType.ATTN_KEY
                )
            case _:
                raise NotImplementedError(f"Unhandled token_scoring_type: {token_scoring_type}")
    def _get_group_id_for_token_scoring(self, token_scoring_type: TokenScoringType) -> GroupId:
        match token_scoring_type:
            case TokenScoringType.UPVOTED_OUTPUT_TOKENS:
                return GroupId.TOKEN_WRITE
            case (
                TokenScoringType.INPUT_TOKENS_THAT_UPVOTE_MLP
                | TokenScoringType.INPUT_TOKENS_THAT_UPVOTE_ATTN_Q
                | TokenScoringType.INPUT_TOKENS_THAT_UPVOTE_ATTN_K
            ):
                return GroupId.TOKEN_READ
            case _:
                raise NotImplementedError(f"Unhandled token_scoring_type: {token_scoring_type}")
    def _get_scored_tokens(
        self,
        request_spec: ScoredTokensRequestSpec,
        multi_group_ds_store: MultiGroupDerivedScalarStore,
        all_node_indices: list[NodeIndex],
    ) -> ScoredTokensResponseData:
        # Do postprocessing for all of the nodes to get the top and bottom tokens for that node.
        # Use the derived scalars associated with the TOKEN_WRITE or TOKEN_READ group.
        token_scoring_type = request_spec.token_scoring_type
        ds_store = multi_group_ds_store.get_ds_store(
            self._get_group_id_for_token_scoring(token_scoring_type)
        )
        postprocessor = self._get_token_scoring_postprocessor(token_scoring_type)
        should_score_node = [
            self._should_score_node(node_index.node_type, token_scoring_type)
            for node_index in all_node_indices
        ]
        all_node_indices_to_score = [
            node_index
            for node_index, should_score in zip(all_node_indices, should_score_node)
            if should_score
        ]
        if len(all_node_indices_to_score) > 0:
            scored_token_scalars_list = postprocessor.postprocess_multiple_nodes(
                all_node_indices_to_score, ds_store
            )
            token_scalars_list = [
                scored_token_scalars_list.pop(0) if should_score else None
                for should_score in should_score_node
            ]
        else:
            token_scalars_list = [None for _ in range(len(all_node_indices))]
        non_none_token_scalars = [ts for ts in token_scalars_list if ts is not None]
        if len(non_none_token_scalars) == 0:
            # This can happen in situations where the token scoring type doesn't apply to any of the
            # nodes.
            top_tokens_list = []
        else:
            token_scalars_tensor = torch.stack(non_none_token_scalars)
            assert (
                token_scalars_tensor.ndim == 2
            ), f"Expected token_writes.ndim == 2, but got {token_scalars_tensor.shape=}"
            assert torch.isfinite(
                token_scalars_tensor
            ).all(), "token_scalars_tensor should only contain finite values"
            top_tokens_list = get_most_upvoted_and_downvoted_tokens_for_nodes(
                self._standard_model_context, token_scalars_tensor, request_spec.num_tokens
            )
        # Now create a version of top_tokens_list that has the same length as all_node_indices, with
        # None at the indices for the nodes we didn't score.
        final_top_tokens_list: list[TopTokens | None] = []
        top_tokens_index = 0
        for token_scalars in token_scalars_list:
            if token_scalars is None:
                # If the node wasn't scored, add None.
                final_top_tokens_list.append(None)
            else:
                # If the node was scored, add the corresponding TopTokens from top_tokens_list.
                final_top_tokens_list.append(top_tokens_list[top_tokens_index])
                top_tokens_index += 1
        assert top_tokens_index == len(top_tokens_list)
        assert len(final_top_tokens_list) == len(all_node_indices)
        return ScoredTokensResponseData(
            node_indices=[
                MirroredNodeIndex.from_node_index(node_index) for node_index in all_node_indices
            ],
            top_tokens_list=final_top_tokens_list,
        )
    def _get_token_pair_attribution(
        self,
        request_spec: TokenPairAttributionRequestSpec,
        multi_group_ds_store: MultiGroupDerivedScalarStore,
        all_node_indices: list[NodeIndex],
    ) -> TokenPairAttributionResponseData:
        """Returns attended-to tokens with most positive attributions for attention write autoencoder latents."""
        ds_store = multi_group_ds_store.get_ds_store(group_id=GroupId.TOKEN_PAIR_ATTRIBUTION)
        postprocessor = TokenPairAttributionConverter(
            model_context=self._standard_model_context,
            multi_autoencoder_context=self._multi_autoencoder_context,
            num_tokens_attended_to=request_spec.num_tokens_attended_to,
        )
        # sort the top token-attended-to by the value of the attribution
        node_indices = []  # type: list[MirroredNodeIndex]
        top_tokens_attended_to_list = []  # type: list[TopTokensAttendedTo | None]
        for node_index in all_node_indices:
            node_indices.append(MirroredNodeIndex.from_node_index(node_index))
            try:
                postprocessed = postprocessor.postprocess(node_index, ds_store)
                top_tokens_attended_to = postprocessed.topk(k=request_spec.num_tokens_attended_to)
                top_tokens_attended_to_list.append(
                    TopTokensAttendedTo(
                        token_indices=top_tokens_attended_to.indices.cpu().numpy().tolist(),
                        attributions=top_tokens_attended_to.values.cpu().numpy().tolist(),
                    )
                )
            except ValueError:
                top_tokens_attended_to_list.append(None)
                continue
        return TokenPairAttributionResponseData(
            node_indices=node_indices, top_tokens_attended_to_list=top_tokens_attended_to_list
        )
    async def get_multiple_top_k_derived_scalars(
        self, request: MultipleTopKDerivedScalarsRequest
    ) -> MultipleTopKDerivedScalarsResponse:
        """This request is assumed to have multiple group_ids, where values within each group_id
        are comparable (for example a group called "write_norm" might have MLP write norm and attention write norm;
        or "act_times_grad" might have MLP post-act act*grad and attention post-softmax act*grad). Across group IDs,
        the values are assumed to be attributable to the same set of node types (e.g. MLP neurons; or attention heads).
        Example:
        ---------------------------------------------------------
        | Group ID       | DerivedScalarType   | NodeType       |
        ---------------------------------------------------------
        | write_norm     | mlp_write_norm      | mlp_neuron     |
        | write_norm     | attn_write_norm     | attention_head |
        | act_times_grad | mlp_act_times_grad  | mlp_neuron     |
        | act_times_grad | attn_act_times_grad | attention_head |
        ---------------------------------------------------------
        The response contains a list of NodeIndex objects, which identify a NodeType and e.g. token_index, neuron_index
        tuple. These indices correspond to derived scalar values that are extremal for some derived scalar type. It also contains
        a dict of the corresponding derived scalar values, keyed by group_id, where the i'th element of each list corresponds to
        the i'th NodeIndex in the list of ActivationIndices.
        """
        response = await self._handle_inference_request(request)
        assert isinstance(response, MultipleTopKDerivedScalarsResponse)
        return response
    def _compute_multi_group_ds_store(
        self,
        # In the future we may want to pass in a single list of compound types instead of three parallel lists.
        batched_inference_request_spec: list[InferenceRequestSpec],
        # Sometimes T will be GroupId; sometimes it will be tuple[str, GroupId].
        batched_dst_and_configs_by_processing_step: list[DstAndConfigsByProcessingStep],
        # Return three parallel lists:
        #   1) a batched list of input token ints
        #   2) a batched list of derived scalar stores
        #   3) a batched list of inference data objects
        # Each list should be the same length.
    ) -> tuple[
        list[list[int]],
        list[dict[str, MultiGroupDerivedScalarStore]],
        list[InferenceData],
    ]:
        """
        Helper method (first step) that computes a DerivedScalarStore for each group ID. The
        quantities within each group ID are intended to be comparable (e.g. the write vector norms
        of attention heads, and the write vector norms of MLP neurons).
        """
        assert len(batched_inference_request_spec) == len(
            batched_dst_and_configs_by_processing_step
        )
        batched_ds_computation_params = []
        for inference_request_spec, dst_and_configs_by_processing_step in zip(
            batched_inference_request_spec,
            batched_dst_and_configs_by_processing_step,
        ):
            prompt = inference_request_spec.prompt
            unpadded_tokens_as_ints = self.encode(prompt)
            tokens_as_ints = unpadded_tokens_as_ints
            multi_group_scalar_derivers_by_processing_step = {
                spec_name: (
                    MultiGroupScalarDerivers.from_dst_and_config_list_by_group_id(
                        dst_and_config_list_by_group_id=dst_and_configs_by_processing_step[
                            spec_name
                        ],
                    )
                )
                for spec_name in dst_and_configs_by_processing_step.keys()
            }
            # if at least one of the scalar derivers is intended to operate on GPU,
            # then we'll use GPU for the raw activations. Otherwise the CPU.
            devices_for_raw_activations = []
            for (
                multi_group_scalar_derivers
            ) in multi_group_scalar_derivers_by_processing_step.values():
                devices_for_raw_activations += (
                    multi_group_scalar_derivers.devices_for_raw_activations
                )
            if any(device.type == "cuda" for device in devices_for_raw_activations):
                device_for_raw_activations = torch.device("cuda", 0)
            elif any(device.type == "mps" for device in devices_for_raw_activations):
                device_for_raw_activations = torch.device("mps")
            else:
                device_for_raw_activations = torch.device("cpu")
            trace_config = (  # mirrored -> non-mirrored TraceConfig
                inference_request_spec.trace_config.to_trace_config()
                if inference_request_spec.trace_config is not None
                else None
            )
            ds_computation_params = DerivedScalarComputationParams(
                input_token_ints=tokens_as_ints,
                multi_group_scalar_derivers_by_processing_step=multi_group_scalar_derivers_by_processing_step,
                loss_fn_for_backward_pass=maybe_construct_loss_fn_for_backward_pass(
                    model_context=self._standard_model_context,
                    config=inference_request_spec.loss_fn_config,
                ),
                trace_config=trace_config,
                ablation_specs=inference_request_spec.ablation_specs,
                device_for_raw_activations=device_for_raw_activations,
            )
            batched_ds_computation_params.append(ds_computation_params)
        batched_multi_group_ds_store_by_processing_step: list[
            dict[str, MultiGroupDerivedScalarStore]
        ]
        (
            batched_multi_group_ds_store_by_processing_step,
            batched_inference_data,
            _,
        ) = compute_derived_scalar_groups_for_input_token_ints(
            model_context=self._standard_model_context,
            batched_ds_computation_params=batched_ds_computation_params,
            multi_autoencoder_context=self._multi_autoencoder_context,
        )
        return (
            [params.input_token_ints for params in batched_ds_computation_params],
            batched_multi_group_ds_store_by_processing_step,
            batched_inference_data,
        )
    def _get_multiple_top_k_derived_scalars_from_multi_group_ds_store(
        self,
        request_spec: MultipleTopKDerivedScalarsRequestSpec,
        multi_group_ds_store: MultiGroupDerivedScalarStore,
        all_node_indices: list[NodeIndex],
    ) -> MultipleTopKDerivedScalarsResponseData:
        """
        Helper method (the second and final step) that computes top k activations for each group
        name starting from a pre-computed DerivedScalarStore, per group ID. The quantities within
        each group ID are intended to be comparable (e.g. the write vector norms of attention
        heads, and the write vector norms of MLP neurons).
        This computes the top k model component, token combinations, and returns them in a
        MultipleTopKDerivedScalarsResponseData object.
        """
        assert len(all_node_indices) > 0, "Expected at least one node index"
        activations_by_group_id = (
            multi_group_ds_store.get_derived_scalars_by_group_id_for_node_indices(all_node_indices)
        )
        intermediate_sum_by_dst_by_group_id: dict[GroupId, dict[DerivedScalarType, TensorND]] = {}
        for group_id in activations_by_group_id.keys():
            intermediate_sum_by_dst_by_group_id[group_id] = compute_intermediate_sum_by_dst(
                ds_store=multi_group_ds_store.get_ds_store(group_id),
                dimensions_to_keep_for_intermediate_sum=request_spec.dimensions_to_keep_for_intermediate_sum,
            )
        return MultipleTopKDerivedScalarsResponseData(
            activations_by_group_id={
                group_id: activations.tolist()
                for group_id, activations in activations_by_group_id.items()
            },
            node_indices=[
                MirroredNodeIndex.from_node_index(node_index) for node_index in all_node_indices
            ],
            vocab_token_strings_for_indices=_make_vocab_token_strings_for_indices(
                self._standard_model_context,
                all_node_indices,
            ),
            intermediate_sum_activations_by_dst_by_group_id=intermediate_sum_by_dst_by_group_id,
        )
    def _get_dst_and_configs_by_processing_step_for_singular_request(
        self, request: InferenceSubRequest
    ) -> DstAndConfigsByProcessingStep:
        """
        Wrapper that calls the helper to infer the correct configs for each sub_request_spec, and then
        performs a sanity check to confirm that backward pass activations are not being requested at any
        layers deeper than the layer from which the backward pass is being computed.
        Returns:
            dst_and_config_list_dict: a nested dict of lists of tuples, where each tuple
                contains a DerivedScalarType and a DerivedScalarTypeConfig. The nested dict is keyed first by
                spec_name and then by group_id, where spec_name is the name of the processing_request_spec, and group_id
                refers to a GroupId enum value (each GroupId referring to a set of DSTs).
        """
        inference_request_spec = request.inference_request_spec
        processing_request_spec_by_name = request.processing_request_spec_by_name
        dst_and_configs_by_processing_step: DstAndConfigsByProcessingStep = {}
        for spec_name, processing_request_spec in processing_request_spec_by_name.items():
            dst_and_configs_by_processing_step[
                spec_name
            ] = self._get_dst_and_config_list_by_group_id_from_request_spec(
                inference_request_spec=inference_request_spec,
                processing_request_spec=processing_request_spec,
                preceding_dst_and_config_lists=dst_and_configs_by_processing_step,
            )
        return dst_and_configs_by_processing_step
    async def handle_batched_tdb_request(
        self, batched_tdb_request: BatchedTdbRequest
    ) -> BatchedResponse:
        inference_sub_requests = [
            convert_tdb_request_spec_to_inference_sub_request(tdb_request_spec)
            for tdb_request_spec in batched_tdb_request.sub_requests
        ]
        # TODO(sbills): Return a TDB-specific response rather than just returning a regular
        # BatchedResponse.
        return await self.handle_batched_request(
            BatchedRequest(inference_sub_requests=inference_sub_requests)
        )
    async def handle_batched_request(self, batched_request: BatchedRequest) -> BatchedResponse:
        """For high level overview, see STEP 0, 1, 2 below"""
        async with self._batched_request_lock:
            # STEP 0: infer the derived scalar types and configs needed for each group ID, by
            # examining the processing_request_spec for each group ID, and the information in the
            # inference_request_spec shared by all group IDs.
            batched_dst_and_config_list_by_processing_step = []
            for inference_request in batched_request.inference_sub_requests:
                batched_dst_and_config_list_by_processing_step.append(
                    self._get_dst_and_configs_by_processing_step_for_singular_request(
                        inference_request
                    )
                )
            # Confirm that the prompts for all of the batched (multiple) top-k sub-requests have the
            # the same length (in tokens). If they don't, we can't aggregate node indices across
            # them. They should also be less than PROMPT_LENGTH_LIMIT tokens long.
            prompt_lengths = []
            batched_tokens_as_ints = []
            for inference_request in batched_request.inference_sub_requests:
                tokens_as_ints = self._standard_model_context.encode(
                    inference_request.inference_request_spec.prompt
                )
                if any(
                    isinstance(spec, MultipleTopKDerivedScalarsRequestSpec)
                    for spec in inference_request.processing_request_spec_by_name.values()
                ):
                    batched_tokens_as_ints.append(tokens_as_ints)
                    prompt_lengths.append(len(tokens_as_ints))
            if any(prompt_length > PROMPT_LENGTH_LIMIT for prompt_length in prompt_lengths):
                raise HTTPException(
                    status_code=400,
                    detail=(
                        f"Prompts must be less than {PROMPT_LENGTH_LIMIT} tokens long for batched top-k requests. "
                        f"Got these prompt lengths: {prompt_lengths}"
                    ),
                )
            if len(set(prompt_lengths)) > 1:
                # Build an error message that gives the tokenized prompts (as strings) with their
                # lengths.
                tokens_as_strings_list = [
                    self._standard_model_context.decode_token_list(tokens_as_ints)
                    for tokens_as_ints in batched_tokens_as_ints
                ]
                prompt_lengths_str = ", ".join(
                    f"{tokens_as_strings} ({len(tokens_as_strings)} tokens)\n"
                    for tokens_as_strings in tokens_as_strings_list
                )
                raise HTTPException(
                    status_code=400,
                    detail=(
                        f"All prompts must have the same length for batched top-k requests. "
                        f"Got these prompts:\n{prompt_lengths_str}"
                    ),
                )
            # STEP 1: run the forward pass, saving the activations and metadata required for each of
            # the derived scalar types and configs needed for each group ID; then compute the
            # derived scalars for each group ID from the common set of activations and metadata
            # objects.
            (
                batched_tokens_as_ints,
                batched_multi_group_ds_store_by_processing_step,
                batched_inference_data,
            ) = self._compute_multi_group_ds_store(
                batched_inference_request_spec=[
                    inference_request.inference_request_spec
                    for inference_request in batched_request.inference_sub_requests
                ],
                batched_dst_and_configs_by_processing_step=batched_dst_and_config_list_by_processing_step,
            )
            # Identify the node indices of the top (and possibly bottom) k activations for each
            # group sub-request, collated by spec name. We need to know all of the indices before we
            # can put together the results, since we ensure that all sub-requests cover all nodes
            # that were in the top k for any sub-request.
            # NOTE: all_node_indices is separated by spec name (i.e. processing step) but cumulative
            # across the batch index
            all_node_indices_by_spec_name: dict[str, list[NodeIndex]] = defaultdict(list)
            for (
                request,
                multi_group_ds_store_by_processing_step,
            ) in zip(
                batched_request.inference_sub_requests,
                batched_multi_group_ds_store_by_processing_step,
            ):
                for (
                    processing_spec_name,
                    processing_request_spec,
                ) in request.processing_request_spec_by_name.items():
                    if isinstance(processing_request_spec, MultipleTopKDerivedScalarsRequestSpec):
                        (
                            transform_activations_fn_for_top_k,
                            transform_indices_fn_for_top_k,
                        ) = maybe_construct_transform_fns_for_top_k(
                            processing_request_spec.token_index,
                        )
                        multi_group_ds_store = multi_group_ds_store_by_processing_step[
                            processing_spec_name
                        ]
                        # if top_and_bottom_k is not provided, returns all elements
                        node_indices, _ = multi_group_ds_store.topk(
                            top_and_bottom_k=processing_request_spec.top_and_bottom_k,
                            transform_activations_fn=transform_activations_fn_for_top_k,
                            transform_indices_fn=transform_indices_fn_for_top_k,
                        )
                        all_node_indices_by_spec_name[processing_spec_name] += node_indices
                    elif isinstance(
                        processing_request_spec,
                        (ScoredTokensRequestSpec, TokenPairAttributionRequestSpec),
                    ):
                        # These request specs always depend on another spec in the top-level
                        # request and use the same node indices.
                        all_node_indices_by_spec_name[
                            processing_spec_name
                        ] = all_node_indices_by_spec_name[
                            processing_request_spec.depends_on_spec_name
                        ]
            # Dedupe the node indices, maintaining the original order.
            all_node_indices_by_spec_name = {
                spec_name: _unique_list_in_original_order(all_node_indices)
                for spec_name, all_node_indices in all_node_indices_by_spec_name.items()
            }
            # STEP 2: compute the response data for each sub request spec; this can be simply
            # extracting a particular derived scalar at a particular activation index, or can be
            # computing the top k derived scalars, or can be computing the top k of multiple
            # scalars, and combining them.
            batched_inference_responses = []
            assert (
                len(batched_request.inference_sub_requests)
                == len(batched_multi_group_ds_store_by_processing_step)
                == len(batched_dst_and_config_list_by_processing_step)
                == len(batched_tokens_as_ints)
                == len(batched_inference_data)
            )
            for (
                inference_sub_request,
                multi_group_ds_store_by_processing_step,
                tokens_as_ints,
                inference_data,
            ) in zip(
                batched_request.inference_sub_requests,
                batched_multi_group_ds_store_by_processing_step,
                batched_tokens_as_ints,
                batched_inference_data,
            ):
                processing_response_data_by_name: dict[str, ProcessingResponseData] = {}
                for (
                    processing_spec_name,
                    processing_request_spec,
                ) in inference_sub_request.processing_request_spec_by_name.items():
                    relevant_multi_group_ds_store = multi_group_ds_store_by_processing_step[
                        processing_spec_name
                    ]
                    # The key may not present in the case where a non-top-k request uses a spec name
                    # that is not present in any of the (multi) top-k requests, or in the case where
                    # there are no (multi) top-k requests at all.
                    all_node_indices: list[NodeIndex] = all_node_indices_by_spec_name.get(
                        processing_spec_name, []
                    )
                    processing_response_data = await self._handle_processing_request(
                        processing_request_spec=processing_request_spec,
                        multi_group_ds_store=relevant_multi_group_ds_store,
                        all_node_indices=all_node_indices,
                        tokens_as_ints=tokens_as_ints,
                    )
                    expected_response_data_class = get_corresponding_object(
                        type(processing_request_spec), "request_spec_class", "response_data_class"
                    )
                    assert isinstance(expected_response_data_class, type)
                    assert isinstance(
                        processing_response_data,
                        expected_response_data_class,
                    )
                    processing_response_data_by_name[
                        processing_spec_name
                    ] = processing_response_data
                # put the sub responses together in one object to be returned
                combined_response = InferenceResponseAndResponseDict(
                    inference_response=InferenceResponse(
                        inference_and_token_data=self._get_inference_response_data(
                            tokens_as_ints=tokens_as_ints,
                            inference_data=inference_data,
                        ),
                    ),
                    processing_response_data_by_name=processing_response_data_by_name,
                )
                # check that the type of each sub response data is as expected given the type of the
                # corresponding sub request spec
                for name in combined_response.processing_response_data_by_name.keys():
                    processing_request_spec = inference_sub_request.processing_request_spec_by_name[
                        name
                    ]
                    expected_response_data_class = get_corresponding_object(
                        type(processing_request_spec), "request_spec_class", "response_data_class"
                    )
                    assert isinstance(expected_response_data_class, type)
                    assert isinstance(
                        combined_response.processing_response_data_by_name[name],
                        expected_response_data_class,
                    ), "Response data class mismatch; perhaps cast to wrong type by Pydantic?"
                batched_inference_responses.append(combined_response)
            return BatchedResponse(inference_sub_responses=batched_inference_responses)
    async def _handle_processing_request(
        self,
        processing_request_spec: ProcessingRequestSpec,
        multi_group_ds_store: MultiGroupDerivedScalarStore,
        all_node_indices: list[NodeIndex],
        tokens_as_ints: list[int],
    ) -> ProcessingResponseData:
        """
        Figure out the type of the processing_request_spec and handle it with the appropriate helper method
        to return ResponseData of the appropriate type (e.g. DerivedScalarsRequestSpec ->
        DerivedScalarsResponseData).
        """
        if isinstance(processing_request_spec, DerivedAttentionScalarsRequestSpec):
            ds_store = multi_group_ds_store.to_single_ds_store()
            return await self._get_derived_attention_scalars_from_ds_store(
                request_spec=processing_request_spec,
                ds_store=ds_store,
                tokens_as_ints=tokens_as_ints,
            )
        elif isinstance(processing_request_spec, DerivedScalarsRequestSpec):
            ds_store = multi_group_ds_store.to_single_ds_store()
            return await self._get_derived_scalars_from_ds_store(
                request_spec=processing_request_spec,
                ds_store=ds_store,
            )
        elif isinstance(processing_request_spec, MultipleTopKDerivedScalarsRequestSpec):
            return self._get_multiple_top_k_derived_scalars_from_multi_group_ds_store(
                request_spec=processing_request_spec,
                multi_group_ds_store=multi_group_ds_store,
                all_node_indices=all_node_indices,
            )
        elif isinstance(processing_request_spec, ScoredTokensRequestSpec):
            return self._get_scored_tokens(
                request_spec=processing_request_spec,
                multi_group_ds_store=multi_group_ds_store,
                all_node_indices=all_node_indices,
            )
        elif isinstance(processing_request_spec, TokenPairAttributionRequestSpec):
            return self._get_token_pair_attribution(
                request_spec=processing_request_spec,
                multi_group_ds_store=multi_group_ds_store,
                all_node_indices=all_node_indices,
            )
        else:
            raise ValueError(f"Unhandled request_spec type: {type(processing_request_spec)}")
    def _get_inference_response_data(
        self, tokens_as_ints: list[int], inference_data: InferenceData
    ) -> InferenceAndTokenData:
        return InferenceAndTokenData(
            tokens_as_ints=tokens_as_ints,
            tokens_as_strings=self._standard_model_context.decode_token_list(tokens_as_ints),
            **inference_data.dict(),
        )
    def _get_dst_config(
        self,
        inference_request_spec: InferenceRequestSpec,
        dsts: list[DerivedScalarType],
        pass_type: PassType,
    ) -> DstConfig:
        """
        Infers the DerivedScalarTypeConfig required to compute the given DerivedScalarType(s) and pass_type.
        This auto-populates:
        derive_gradients based on the requested pass_type
        model_context and autoencoder context based on the InteractiveModel properties themselves
        layer_index_for_grad based on whether the backward pass originates from the loss at the model's outputs,
        or based on a particular activation at an intermediate location in the network
        """
        if inference_request_spec.activation_index_for_within_layer_grad is not None:
            trace_config = TraceConfig.from_activation_index(
                activation_index=(
                    inference_request_spec.activation_index_for_within_layer_grad.to_activation_index()
                )  # convert mirrored to un-mirrored trace config
            )
        elif inference_request_spec.trace_config is not None:
            # starts as MirroredTraceConfig, but DSTConfig takes TraceConfig
            trace_config = inference_request_spec.trace_config.to_trace_config()
        else:
            trace_config = None
        if DerivedScalarType.ATTN_WRITE_TO_LATENT_SUMMED_OVER_HEADS in dsts:
            assert inference_request_spec.activation_index_for_within_layer_grad is not None
            layer_index = inference_request_spec.activation_index_for_within_layer_grad.layer_index
            assert layer_index is not None
            layer_indices = [layer_index]  # type: list[int] | None
        else:
            layer_indices = None
        if any(
            dsts == [read_dst] for read_dst in TOKEN_READ_DSTS
        ):  # requesting just one token read dst
            # This is a special DST used for calculating top "input tokens" / token reads. When it's
            # used, there should always be an ablation that specifies the activation index of
            # interest, which needs to be plumbed through the DstConfig using a dedicated field.
            ablation_specs = inference_request_spec.ablation_specs
            assert ablation_specs is not None
            assert len(ablation_specs) == 1
            activation_index_for_fake_grad = ablation_specs[0].index.to_activation_index()
        else:
            activation_index_for_fake_grad = None
        return DstConfig(
            derive_gradients=(pass_type == PassType.BACKWARD),
            inference_engine_type=InferenceEngineType.STANDARD,
            model_context=self._standard_model_context,
            layer_indices=layer_indices,
            multi_autoencoder_context=self._multi_autoencoder_context,
            trace_config=trace_config,
            activation_index_for_fake_grad=activation_index_for_fake_grad,
        )
    def _singleton_dst_and_config_list_by_group_id(
        self,
        dst: DerivedScalarType,
        inference_request_spec: InferenceRequestSpec,
        pass_type: PassType,
    ) -> dict[GroupId, list[tuple[DerivedScalarType, DstConfig]]]:
        return {
            GroupId.SINGLETON: [
                (
                    dst,
                    self._get_dst_config(
                        inference_request_spec=inference_request_spec,
                        dsts=[dst],
                        pass_type=pass_type,
                    ),
                )
            ]
        }
    def _get_dst_and_config_list_by_group_id_from_request_spec(
        self,
        inference_request_spec: InferenceRequestSpec,
        processing_request_spec: ProcessingRequestSpec,
        # Some request specs piggyback on the same DSTs and configs as others in the request.
        # Those request specs must be ordered after their dependencies.
        preceding_dst_and_config_lists: DstAndConfigsByProcessingStep,
    ) -> dict[GroupId, list[tuple[DerivedScalarType, DstConfig]]]:
        """
        Wraps the helper that infers DerivedScalarTypeConfig, parsing the information on requested
        DerivedScalarType objects from the request_spec using logic that depends
        on the type of the request_spec (i.e. the kinds of information on model internals requested).
        This auto-populates:
        derive_gradients based on the requested pass_type
        model_context and autoencoder context based on the InteractiveModel properties themselves
        layer_index_for_grad based on whether the backward pass originates from the loss at the model's outputs,
        or based on a particular activation at an intermediate location in the network
        """
        pass_type = getattr(processing_request_spec, "pass_type", PassType.FORWARD)
        if isinstance(
            processing_request_spec, (DerivedScalarsRequestSpec, DerivedAttentionScalarsRequestSpec)
        ):
            dst_and_config_list_by_group_id = self._singleton_dst_and_config_list_by_group_id(
                dst=processing_request_spec.dst,
                inference_request_spec=inference_request_spec,
                pass_type=pass_type,
            )
        elif isinstance(processing_request_spec, MultipleTopKDerivedScalarsRequestSpec):
            dst_config_by_group_id = {
                group_id: self._get_dst_config(
                    inference_request_spec=inference_request_spec,
                    dsts=dst_list,
                    pass_type=pass_type,
                )
                for group_id, dst_list in processing_request_spec.dst_list_by_group_id.items()
            }
            dst_and_config_list_by_group_id = {
                group_id: [
                    (
                        dst,
                        dst_config_by_group_id[group_id],
                    )
                    for dst in dst_list
                ]
                for group_id, dst_list in processing_request_spec.dst_list_by_group_id.items()
            }
        elif isinstance(processing_request_spec, ScoredTokensRequestSpec):
            token_scoring_type = processing_request_spec.token_scoring_type
            postprocessor = self._get_token_scoring_postprocessor(token_scoring_type)
            group_id_for_request = self._get_group_id_for_token_scoring(token_scoring_type)
            dst_and_config_list_by_group_id = self._get_dst_and_config_list_from_preceding_requests(
                processing_request_spec=processing_request_spec,
                preceding_dst_and_config_lists=preceding_dst_and_config_lists,
                postprocessor=self._get_token_scoring_postprocessor(token_scoring_type),
                group_id_for_request=self._get_group_id_for_token_scoring(token_scoring_type),
            )
        elif isinstance(processing_request_spec, TokenPairAttributionRequestSpec):
            postprocessor = TokenPairAttributionConverter(
                model_context=self._standard_model_context,
                multi_autoencoder_context=self._multi_autoencoder_context,
                num_tokens_attended_to=processing_request_spec.num_tokens_attended_to,
            )
            group_id_for_request = GroupId.TOKEN_PAIR_ATTRIBUTION
            dst_and_config_list_by_group_id = self._get_dst_and_config_list_from_preceding_requests(
                processing_request_spec=processing_request_spec,
                preceding_dst_and_config_lists=preceding_dst_and_config_lists,
                postprocessor=postprocessor,
                group_id_for_request=group_id_for_request,
            )
        else:
            raise NotImplementedError(f"Unknown request spec type: {type(processing_request_spec)}")
        return dst_and_config_list_by_group_id
    def _get_dst_and_config_list_from_preceding_requests(
        self,
        processing_request_spec: ScoredTokensRequestSpec | TokenPairAttributionRequestSpec,
        preceding_dst_and_config_lists: DstAndConfigsByProcessingStep,
        postprocessor: DerivedScalarPostprocessor,
        group_id_for_request: GroupId,
    ) -> dict[GroupId, list[tuple[DerivedScalarType, DstConfig]]]:
        # Find the first dst_and_config_list that has the spec_name we're depending on.
        # NOTE(dan): should this be union rather than first dst_and_config_list?
        assert hasattr(processing_request_spec, "depends_on_spec_name")
        dst_and_config_list = None
        for spec_name in preceding_dst_and_config_lists.keys():
            if spec_name == processing_request_spec.depends_on_spec_name:
                dst_and_config_list = next(iter(preceding_dst_and_config_lists[spec_name].values()))
                break
        assert dst_and_config_list is not None
        dst_and_config_list_by_group_id = {
            group_id_for_request: postprocessor.get_input_dst_and_config_list(
                requested_dst_and_config_list=dst_and_config_list
            )
        }
        return dst_and_config_list_by_group_id
def maybe_construct_transform_fns_for_top_k(
    token_index: int | None,
) -> tuple[Callable[[torch.Tensor], torch.Tensor] | None, Callable[[NodeIndex], NodeIndex] | None]:
    """
    In some cases, we may want to compute the top k activations on a transformed version of the DerivedScalarStore.
    This function constructs a transformation to apply to the activations, and a corresponding transformation to
    apply to the NodeIndex objects returned by the .topk operation, if any, to index the un-transformed DerivedScalarStore.
    `None` means no transformation is to be performed.
    Currently, we use this for computing top k over a single token index only, and converting the NodeIndex objects
    to index the original, un-transformed DerivedScalarStore.
    """
    if token_index is None:
        return None, None
    else:
        def access_token_index(x: torch.Tensor) -> torch.Tensor:
            return x[token_index].unsqueeze(0)
        def convert_to_original_token_index(node_index: NodeIndex) -> NodeIndex:
            assert (
                node_index.tensor_indices[0] == 0
            )  # assumed that token dimension was converted to singleton
            # replace the 0 token index with the original token_index
            return node_index.with_updates(
                tensor_indices=(token_index, *node_index.tensor_indices[1:])
            )
        return access_token_index, convert_to_original_token_index
def compute_intermediate_sum_by_dst(
    ds_store: DerivedScalarStore,
    dimensions_to_keep_for_intermediate_sum: list[Dimension],
) -> dict[DerivedScalarType, TensorND]:
    # Calculate intermediate sums.
    def keep_dimension_fn(dim: Dimension) -> bool:
        return dim in dimensions_to_keep_for_intermediate_sum
    intermediate_sum_by_dst = get_intermediate_sum_by_dst(
        ds_store=ds_store,
        keep_dimension_fn=keep_dimension_fn,
    )
    return intermediate_sum_by_dst

================
File: neuron_explainer/activation_server/load_neurons.py
================
from fastapi import HTTPException
from neuron_explainer.activation_server.neuron_datasets import (
    NEURON_DATASET_METADATA_REGISTRY,
    get_neuron_dataset_metadata_by_short_name_and_dst,
)
from neuron_explainer.activations.activations import NeuronRecord, load_neuron_async
from neuron_explainer.activations.derived_scalars import DerivedScalarType
from neuron_explainer.pydantic import CamelCaseBaseModel, immutable
@immutable
class NodeIdAndDatasets(CamelCaseBaseModel):
    dst: DerivedScalarType
    layer_index: int
    activation_index: int
    datasets: list[str]
    """A list of dataset paths or short names."""
def resolve_neuron_dataset(dataset: str, dst: DerivedScalarType) -> str:
    if dataset.startswith("https://"):
        return dataset
    else:
        # It's the short name for a dataset, like "gpt2-small". We have to look up the metadata.
        dataset_metadata = get_neuron_dataset_metadata_by_short_name_and_dst(dataset, dst)
        return dataset_metadata.neuron_dataset_path
def convert_dataset_path_to_short_name(dataset_path: str) -> str:
    assert dataset_path.startswith("https://")
    short_name = None
    for metadata in NEURON_DATASET_METADATA_REGISTRY.values():
        if metadata.neuron_dataset_path == dataset_path:
            short_name = metadata.short_name
            break
    assert (
        short_name is not None
    ), f"Could not find short name for {dataset_path}. If you're trying to use a custom dataset, ensure that you have added it to neuron_datasets.py:NEURON_DATASET_METADATA_REGISTRY."
    return short_name
async def load_neuron_from_datasets(
    node_id_and_datasets: NodeIdAndDatasets,
) -> tuple[str, NeuronRecord]:
    """
    Load a neuron record of the specified dst (e.g. DerivedScalarType.MLP_POST_ACT) from a list of
    datasets, returning the data from the first dataset that has the neuron.
    Used to allow first trying a dataset that only covers a subset of neurons for a model,
    with a fallback to another dataset that covers all neurons.
    """
    dst = node_id_and_datasets.dst
    datasets = node_id_and_datasets.datasets
    dataset_paths = [resolve_neuron_dataset(dataset, dst) for dataset in datasets]
    layer_index = node_id_and_datasets.layer_index
    activation_index = node_id_and_datasets.activation_index
    for dataset_path in dataset_paths:
        try:
            return dataset_path, await load_neuron_async(
                dataset_path, layer_index, activation_index
            )
        except FileNotFoundError:
            pass
    raise HTTPException(
        status_code=404,
        detail=f"Could not find {dst} {layer_index}:{activation_index} in {dataset_paths}",
    )

================
File: neuron_explainer/activation_server/main.py
================
"""Starts the activation server. Methods on the server are defined in separate files."""
import datetime
import os
import re
import signal
import fire
import torch
import uvicorn
from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from fastapi.routing import APIRoute
from starlette.exceptions import HTTPException as StarletteHTTPException
from neuron_explainer.activation_server.explainer_routes import (
    AttentionExplainAndScoreMethodId,
    NeuronExplainAndScoreMethodId,
    define_explainer_routes,
)
from neuron_explainer.activation_server.inference_routes import define_inference_routes
from neuron_explainer.activation_server.interactive_model import InteractiveModel
from neuron_explainer.activation_server.read_routes import define_read_routes
from neuron_explainer.activation_server.requests_and_responses import GroupId
from neuron_explainer.models.autoencoder_context import AutoencoderContext  # noqa: F401
from neuron_explainer.models.autoencoder_context import MultiAutoencoderContext
from neuron_explainer.models.model_context import StandardModelContext, get_default_device
from neuron_explainer.models.model_registry import make_autoencoder_context
def main(
    host_name: str = "localhost",
    port: int = 80,
    model_name: str = "gpt2-small",
    mlp_autoencoder_name: str | None = None,
    attn_autoencoder_name: str | None = None,
    run_model: bool = True,
    neuron_method: str = "baseline",
    attention_head_method: str = "baseline",
    cuda_memory_debugging: bool = False,
) -> None:
    neuron_method_id = NeuronExplainAndScoreMethodId.from_string(neuron_method)
    attention_head_method_id = AttentionExplainAndScoreMethodId.from_string(attention_head_method)
    def custom_generate_unique_id(route: APIRoute) -> str:
        return f"{route.tags[0]}-{route.name}"
    app = FastAPI(generate_unique_id_function=custom_generate_unique_id)
    allow_origin_regex_str = r"https?://localhost(:\d+)?$"
    allow_origin_regex = re.compile(allow_origin_regex_str)
    app.add_middleware(
        CORSMiddleware,
        allow_origin_regex=allow_origin_regex_str,
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )
    # We don't just want to disable CORS for successful responses: we also want to do it for error
    # responses, which FastAPI's middleware doesn't cover. This allows the client to see helpful
    # information like the HTTP status code, which is otherwise hidden from it. To do this, we add
    # two exception handlers. It's possible we could just get away with the first one, but GPT-4
    # thought it was good to include both and who am I to disagree?
    def add_access_control_headers(request: Request, response: JSONResponse) -> JSONResponse:
        origin = request.headers.get("origin")
        # This logic does something similar to what the standard CORSMiddleware does. You can't
        # use a regex in the actual response header, but you can run the regex on the server and
        # then choose to include the header if it matches the origin.
        if origin and allow_origin_regex.fullmatch(origin):
            response.headers["Access-Control-Allow-Origin"] = origin
            response.headers["Access-Control-Allow-Methods"] = "*"
            response.headers["Access-Control-Allow-Headers"] = "*"
        return response
    @app.exception_handler(Exception)
    async def handle_unhandled_exception(request: Request, exc: Exception) -> JSONResponse:
        print("************** Handling an unhandled exception ***********************")
        print(f"Exception type: {type(exc).__name__}")
        print(f"Exception details: {exc}")
        response = add_access_control_headers(
            request,
            JSONResponse(status_code=500, content={"message": "Unhandled server exception"}),
        )
        # Check if this exception is a cuda OOM, which is unrecoverable. If it is, we should kill
        # the server.
        if isinstance(exc, torch.cuda.OutOfMemoryError):
            print("***** Killing server due to cuda OOM *****")
            # Use SIGKILL so the return code of the top-level process is *not* 0.
            os.kill(os.getpid(), signal.SIGKILL)
        return response
    @app.exception_handler(StarletteHTTPException)
    async def handle_starlette_http_exception(request: Request, exc: HTTPException) -> JSONResponse:
        return add_access_control_headers(
            request, JSONResponse(status_code=exc.status_code, content={"message": exc.detail})
        )
    @app.get("/", tags=["hello_world"])
    def read_root() -> dict[str, str]:
        return {"Hello": "World"}
    # The FastAPI client code generation setup only generates TypeScript classes for types
    # referenced from top-level endpoints. In some cases we want to share a type across client and
    # server that isn't referenced in this way. For example, GroupId is used in requests, but only
    # as a key in a dictionary, and the generated TypeScript for dictionaries treats enum values as
    # strings, so GroupId isn't referenced in the generated TypeScript. To work around this, we
    # define a dummy endpoint that references GroupId, which causes the client code generation to
    # generate a TypeScript class for it. The same trick can be used for other types in the future.
    @app.get("/force_client_code_generation", tags=["hello_world"])
    def force_client_code_generation(group_id: GroupId) -> None:
        return None
    @app.get("/dump_memory_snapshot", tags=["memory"])
    def dump_memory_snapshot() -> str:
        if not cuda_memory_debugging:
            raise HTTPException(
                status_code=400,
                detail="The cuda_memory_debugging flag must be set to dump a memory snapshot",
            )
        formatted_time = datetime.datetime.now().strftime("%H%M%S")
        filename = f"torch_memory_snapshot_{formatted_time}.pkl"
        torch.cuda.memory._dump_snapshot(filename)
        return f"Dumped cuda memory snapshot to {filename}"
    if run_model:
        if cuda_memory_debugging:
            torch.cuda.memory._record_memory_history(max_entries=100000)
        device = get_default_device()
        standard_model_context = StandardModelContext(model_name, device=device)
        if mlp_autoencoder_name is not None or attn_autoencoder_name is not None:
            autoencoder_context_list = [
                make_autoencoder_context(
                    model_name=model_name,
                    autoencoder_name=autoencoder_name,
                    device=device,
                    omit_dead_latents=True,
                )
                for autoencoder_name in [mlp_autoencoder_name, attn_autoencoder_name]
                if autoencoder_name is not None
            ]
            multi_autoencoder_context = MultiAutoencoderContext.from_autoencoder_context_list(
                autoencoder_context_list
            )
            multi_autoencoder_context.warmup()
            model = InteractiveModel.from_standard_model_context_and_autoencoder_context(
                standard_model_context, multi_autoencoder_context
            )
        else:
            model = InteractiveModel.from_standard_model_context(standard_model_context)
    else:
        model = None
    define_read_routes(app)
    define_explainer_routes(
        app=app,
        neuron_method_id=neuron_method_id,
        attention_head_method_id=attention_head_method_id,
    )
    define_inference_routes(
        app=app,
        model=model,
        mlp_autoencoder_name=mlp_autoencoder_name,
        attn_autoencoder_name=attn_autoencoder_name,
    )
    # TODO(sbills): Make reload=True work. We need to pass something like "main:app" as a string
    # rather than passing a FastAPI object directly.
    uvicorn.run(app, host=host_name, port=port)
if __name__ == "__main__":
    fire.Fire(main)
"""
For local testing without running a subject model:
python neuron_explainer/activation_server/main.py --run_model False --port 8000
"""

================
File: neuron_explainer/activation_server/neuron_datasets.py
================
"""
Library for looking up neuron datasets and their associated metadata by name.
"""
from neuron_explainer.activations.derived_scalars import DerivedScalarType
from neuron_explainer.pydantic import CamelCaseBaseModel, immutable
@immutable
class NeuronDatasetMetadata(CamelCaseBaseModel):
    short_name: str
    """Short name for the dataset, like "gpt2-small"."""
    derived_scalar_type: str
    """The type of scalar that the neuron records in this dataset contain, e.g. DerivedScalarType.MLP_POST_ACT."""
    user_visible_name: str
    """Name for humans to read, like "GPT-2 small"."""
    neuron_dataset_path: str
    """Path to the neuron dataset generated by collate_activations."""
    # Take care when adding new fields to this class. If they aren't optional, existing metadata
    # files will cause errors when they're read. You can make them required after ensuring that all
    # metadata files have been updated.
NEURON_DATASET_METADATA_REGISTRY = {}
def register_neuron_dataset_metadata(
    short_name: str, derived_scalar_type: str, user_visible_name: str, neuron_dataset_path: str
) -> None:
    NEURON_DATASET_METADATA_REGISTRY[
        (short_name, DerivedScalarType(derived_scalar_type))
    ] = NeuronDatasetMetadata(
        short_name=short_name,
        derived_scalar_type=derived_scalar_type,
        user_visible_name=user_visible_name,
        neuron_dataset_path=neuron_dataset_path,
    )
register_neuron_dataset_metadata(
    short_name="gpt2-xl",
    derived_scalar_type="mlp_post_act",
    user_visible_name="GPT-2 XL - MLP neurons",
    neuron_dataset_path="https://openaipublic.blob.core.windows.net/neuron-explainer/data/collated-activations/",
)
register_neuron_dataset_metadata(
    short_name="gpt2-small",
    derived_scalar_type="mlp_post_act",
    user_visible_name="GPT-2 small - MLP neurons",
    neuron_dataset_path="https://openaipublic.blob.core.windows.net/neuron-explainer/gpt2_small_data/collated-activations/",
)
register_neuron_dataset_metadata(
    short_name="gpt2-small",
    derived_scalar_type="attn_write_norm",
    user_visible_name="GPT-2 small - Attention write by token pair",
    neuron_dataset_path="https://openaipublic.blob.core.windows.net/neuron-explainer/gpt2_small/attn_write_norm/collated-activations-by-token-pair",
)
register_neuron_dataset_metadata(
    short_name="gpt2-small_ae-mlp-post-act-v1",
    derived_scalar_type="mlp_autoencoder_latent",
    user_visible_name="GPT-2 small - MLP autoencoder (post-act) v1",
    neuron_dataset_path="https://openaipublic.blob.core.windows.net/neuron-explainer/gpt2_small/autoencoder_latent/mlp_post_act_v1/collated-activations",
)
register_neuron_dataset_metadata(
    short_name="gpt2-small_ae-resid-delta-mlp-v1",
    derived_scalar_type="mlp_autoencoder_latent",
    user_visible_name="GPT-2 small - MLP autoencoder (write) v1",
    neuron_dataset_path="https://openaipublic.blob.core.windows.net/neuron-explainer/gpt2_small/autoencoder_latent/resid_delta_mlp_v1/collated-activations",
)
register_neuron_dataset_metadata(
    short_name="gpt2-small_ae-mlp-post-act-v4",
    derived_scalar_type="mlp_autoencoder_latent",
    user_visible_name="GPT-2 small - MLP autoencoder (post-act) v4",
    neuron_dataset_path="https://openaipublic.blob.core.windows.net/neuron-explainer/gpt2_small/autoencoder_latent/mlp_post_act_v4/collated-activations",
)
register_neuron_dataset_metadata(
    short_name="gpt2-small_ae-resid-delta-mlp-v4",
    derived_scalar_type="mlp_autoencoder_latent",
    user_visible_name="GPT-2 small - MLP autoencoder (write) v4",
    neuron_dataset_path="https://openaipublic.blob.core.windows.net/neuron-explainer/gpt2_small/autoencoder_latent/resid_delta_mlp_v4/collated-activations",
)
register_neuron_dataset_metadata(
    short_name="gpt2-small_ae-resid-delta-attn-v4",
    derived_scalar_type="attention_autoencoder_latent",
    user_visible_name="GPT-2 small - Attention autoencoder (write) v4",
    neuron_dataset_path="https://openaipublic.blob.core.windows.net/neuron-explainer/gpt2_small/autoencoder_latent/resid_delta_attn_v4/collated-activations",
)
register_neuron_dataset_metadata(
    short_name="gpt2-small_ae-resid-delta-attn-v4",
    derived_scalar_type="flattened_attn_write_to_latent_summed_over_heads",
    user_visible_name="GPT-2 small - Attention autoencoder (write) v4 by token pair",
    neuron_dataset_path="https://openaipublic.blob.core.windows.net/neuron-explainer/gpt2_small/autoencoder_latent/resid_delta_attn_v4/collated-activations-by-token-pair",
)
def get_all_neuron_dataset_metadata() -> list[NeuronDatasetMetadata]:
    return list(NEURON_DATASET_METADATA_REGISTRY.values())
def get_neuron_dataset_metadata_by_short_name_and_dst(
    short_name: str, dst: DerivedScalarType
) -> NeuronDatasetMetadata:
    name_and_type = (short_name, dst)
    metadata = NEURON_DATASET_METADATA_REGISTRY.get(name_and_type)
    if metadata is None:
        error_message = f"Could not find collated activation dataset for {name_and_type}. Available datasets are: "
        error_message += ", ".join(
            f'("{short_name}", "{dst}")' for (short_name, dst) in NEURON_DATASET_METADATA_REGISTRY
        )
        if short_name.endswith("_undefined"):
            # This is likely due to the URL not providing the correct autoencoder name
            if dst in [
                DerivedScalarType.FLATTENED_ATTN_WRITE_TO_LATENT_SUMMED_OVER_HEADS,
                DerivedScalarType.ATTENTION_AUTOENCODER_LATENT,
            ]:
                autoencoder_type = "attention "
            elif dst == DerivedScalarType.MLP_AUTOENCODER_LATENT:
                autoencoder_type = "mlp "
            else:
                autoencoder_type = ""
            error_message += (
                f"\nMay need to specify the {autoencoder_type}autoencoder name in the URL."
            )
        raise Exception(error_message)
    return metadata

================
File: neuron_explainer/activation_server/read_routes.py
================
"""Routes / endpoints related to reading existing data from Azure blob storage."""
from __future__ import annotations
import asyncio
from typing import Callable, Sequence, TypeVar
from fastapi import FastAPI, HTTPException
from neuron_explainer.activation_server.explanation_datasets import get_all_explanation_datasets
from neuron_explainer.activation_server.load_neurons import (
    NodeIdAndDatasets,
    load_neuron_from_datasets,
    resolve_neuron_dataset,
)
from neuron_explainer.activation_server.neuron_datasets import (
    NeuronDatasetMetadata,
    get_all_neuron_dataset_metadata,
)
from neuron_explainer.activations.activations import ActivationRecord
from neuron_explainer.activations.attention_utils import get_attended_to_sequence_lengths
from neuron_explainer.activations.derived_scalars import DerivedScalarType
from neuron_explainer.activations.derived_scalars.tokens import TokenAndRawScalar, TokenAndScalar
from neuron_explainer.explanations.explanations import (
    ScoredExplanation,
    load_neuron_explanations_async,
)
from neuron_explainer.pydantic import CamelCaseBaseModel, immutable
T = TypeVar("T")
@immutable
class AttributedScoredExplanation(CamelCaseBaseModel):
    explanation: str
    score: float | None
    """
    None means that the explanation's score is undefined, maybe because the neuron is never active.
    """
    dataset_name: str
@immutable
class ExistingExplanationsRequest(CamelCaseBaseModel):
    dst: DerivedScalarType
    layer_index: int
    activation_index: int
    # Exactly one of explanation_datasets and neuron_dataset must be specified. If
    # explanation_datasets is specified, only those datasets will be used. If neuron_dataset is
    # specified, all of its explanation datasets will be used.
    explanation_datasets: list[str]
    neuron_dataset: str | None
@immutable
class TokenAndRawAttentionScalars(CamelCaseBaseModel):
    # same as TokenAndAttentionScalars, but without any of the postprocessing
    # applied
    token: str
    scalars: list[float]
@immutable
class TokenAndAttentionScalars(TokenAndRawAttentionScalars):
    # attention_scalars can be any quantity that is a scalar per attention head
    # and per token pair. The most straightforward examples are pre- and post-softmax
    # attention scores; norm of attention head write vector is another option
    # Causally masked attention results in attention scalars for each pair of
    # tokens, which read from an earlier token and write to a later token. The
    # TokenAndAttentionScalars object for the nth token in a sequence contains
    # a list of n floats, summarizing in some way the values written to the nth
    # (current) token by reading from all n-1 earlier tokens, plus the current
    # (nth) token. Because the things read and written are vectors rather than
    # scalars, there are presumably many ways to summarize them as scalars;
    # this object contains just one kind of summary.
    normalized_scalars: list[float]
    total_scalar_in: float
    normalized_total_scalar_in: float
    max_scalar_in: float
    normalized_max_scalar_in: float
    total_scalar_out: float
    normalized_total_scalar_out: float
    max_scalar_out: float
    normalized_max_scalar_out: float
@immutable
class NeuronRecordResponse(CamelCaseBaseModel):
    dataset: str
    max_activation: float
    top_activations: list[list[TokenAndScalar]]
    random_sample: list[list[TokenAndScalar]]
@immutable
class AttentionHeadRecordResponse(CamelCaseBaseModel):
    dataset: str
    max_attention_activation: float
    most_positive_token_sequences: list[list[TokenAndAttentionScalars]]
    random_token_sequences: list[list[TokenAndAttentionScalars]]
def zip_tokens_and_activations(
    activation_records: list[ActivationRecord],
) -> list[list[TokenAndRawScalar]]:
    sequences = []
    for activation_record in activation_records:
        sequence = []
        for token, activation in zip(activation_record.tokens, activation_record.activations):
            sequence.append(
                TokenAndRawScalar(
                    token=token,
                    scalar=activation,
                )
            )
        sequences.append(sequence)
    return sequences
def convert_activation_records_to_token_and_activation_lists(
    all_activation_record_lists: list[list[ActivationRecord]],
) -> list[list[list[TokenAndScalar]]]:
    zipped_tokens_and_raw_activations = [
        zip_tokens_and_activations(activation_record_list)
        for activation_record_list in all_activation_record_lists
    ]
    return normalize_token_scalars(zipped_tokens_and_raw_activations)
def zip_tokens_and_attention_activations(
    activation_records: list[ActivationRecord],
) -> list[list[TokenAndRawAttentionScalars]]:
    """
    This function takes a list of activation records and returns a list of lists of TokenAndRawAttentionScalars, one
    outer list element per sequence, and one inner list element per sequence token.
    There are multiple attention activations per sequence token (one per token in the "attended to sequence" for that sequence token).
    Because the activation record holds the activations in a flattened form, where (sequence token index, attended to sequence token index)
    are combined into a single index, we need to reconstruct the nested list structure, inferring it from the number of flattened
    activations and the number of sequence tokens.
    """
    sequences: list[list[TokenAndRawAttentionScalars]] = []
    for activation_record in activation_records:
        sequence = convert_token_sequence_to_token_and_raw_attention_scalars(
            activation_record.tokens, activation_record.activations
        )
        sequences.append(sequence)
    return sequences
def convert_token_sequence_to_token_and_raw_attention_scalars(
    token_strings: list[str], activations: list[float]
) -> list[TokenAndRawAttentionScalars]:
    num_sequence_tokens = len(token_strings)
    num_activations = len(activations)
    attended_to_sequence_lengths_indexed_by_sequence_token = get_attended_to_sequence_lengths(
        num_sequence_tokens, num_activations
    )
    attention_activations_list_indexed_by_sequence_token = []
    start = 0
    for (
        attended_to_sequence_length_for_sequence_token
    ) in attended_to_sequence_lengths_indexed_by_sequence_token:
        end = start + attended_to_sequence_length_for_sequence_token
        attention_activations_list_indexed_by_sequence_token.append(activations[start:end])
        start = end
    assert len(attention_activations_list_indexed_by_sequence_token) == len(token_strings)
    sequence = []
    for token_string, activations in zip(
        token_strings, attention_activations_list_indexed_by_sequence_token
    ):
        sequence.append(
            TokenAndRawAttentionScalars(
                token=token_string,
                scalars=activations,
            )
        )
    return sequence
def convert_activation_records_to_token_and_attention_activations_lists(
    all_activation_record_lists: list[list[ActivationRecord]],
) -> list[list[list[TokenAndAttentionScalars]]]:
    zipped_tokens_and_raw_attention_activations = [
        zip_tokens_and_attention_activations(activation_record_list)
        for activation_record_list in all_activation_record_lists
    ]
    return normalize_attention_token_scalars(zipped_tokens_and_raw_attention_activations)
def define_read_routes(app: FastAPI) -> None:
    @app.post(
        "/existing_explanations", response_model=list[AttributedScoredExplanation], tags=["read"]
    )
    async def existing_explanations(
        request: ExistingExplanationsRequest,
    ) -> list[AttributedScoredExplanation]:
        def convert_scored_explanation(
            scored_explanation: ScoredExplanation, explanation_dataset: str
        ) -> AttributedScoredExplanation:
            return AttributedScoredExplanation(
                explanation=scored_explanation.explanation,
                score=scored_explanation.get_preferred_score(),
                dataset_name=explanation_dataset.split("/")[-1],
            )
        async def load_and_convert_explanations(
            explanation_dataset: str,
        ) -> list[AttributedScoredExplanation]:
            neuron_simulation_results = await load_neuron_explanations_async(
                explanation_dataset, request.layer_index, request.activation_index
            )
            if neuron_simulation_results is None:
                return []
            else:
                return [
                    convert_scored_explanation(scored_explanation, explanation_dataset)
                    for scored_explanation in neuron_simulation_results.scored_explanations
                    if scored_explanation.explanation is not None
                ]
        if not ((len(request.explanation_datasets) > 0) ^ (request.neuron_dataset is not None)):
            raise HTTPException(
                status_code=400,
                detail="Exactly one of explanation_datasets and neuron_dataset must be specified.",
            )
        if len(request.explanation_datasets) > 0:
            explanation_datasets = request.explanation_datasets
        else:
            assert request.neuron_dataset is not None  # Redundant assert; mypy needs this.
            neuron_dataset = resolve_neuron_dataset(request.neuron_dataset, request.dst)
            explanation_datasets = await get_all_explanation_datasets(neuron_dataset)
        tasks = [load_and_convert_explanations(dataset) for dataset in explanation_datasets]
        scored_explanation_lists = await asyncio.gather(*tasks)
        # Flatten the list of lists.
        return [item for sublist in scored_explanation_lists for item in sublist]
    @app.post("/neuron_record", response_model=NeuronRecordResponse, tags=["read"])
    async def neuron_record(request: NodeIdAndDatasets) -> NeuronRecordResponse:
        dataset_path, neuron_record = await load_neuron_from_datasets(request)
        top_activations, random_sample = convert_activation_records_to_token_and_activation_lists(
            [
                neuron_record.most_positive_activation_records,
                neuron_record.random_sample,
            ]
        )
        return NeuronRecordResponse(
            dataset=dataset_path,
            max_activation=neuron_record.max_activation,
            top_activations=top_activations,
            random_sample=random_sample,
        )
    @app.post("/attention_head_record", response_model=AttentionHeadRecordResponse, tags=["read"])
    async def attention_head_record(request: NodeIdAndDatasets) -> AttentionHeadRecordResponse:
        dataset_path, neuron_record = await load_neuron_from_datasets(request)
        (
            most_positive_token_sequences,
            random_token_sequences,
        ) = convert_activation_records_to_token_and_attention_activations_lists(
            [
                neuron_record.most_positive_activation_records,
                neuron_record.random_sample,
            ]
        )
        return AttentionHeadRecordResponse(
            dataset=dataset_path,
            max_attention_activation=neuron_record.max_activation,
            most_positive_token_sequences=most_positive_token_sequences,
            random_token_sequences=random_token_sequences,
        )
    @app.post(
        "/neuron_datasets_metadata", response_model=list[NeuronDatasetMetadata], tags=["read"]
    )
    def neuron_datasets_metadata() -> list[NeuronDatasetMetadata]:
        return get_all_neuron_dataset_metadata()
def flatten(list_of_lists: list[list[T]]) -> list[T]:
    return [item for sublist in list_of_lists for item in sublist]
def normalize_token_scalars(
    list_of_sequence_lists: list[list[list[TokenAndRawScalar]]],
) -> list[list[list[TokenAndScalar]]]:
    """The input ist a list of lists of lists of TokenAndRawScalar objects; the
    outer list is indexed by provenance, e.g. top or random token sequences;
    the middle list is indexed by token sequence; the inner list is indexed by token
    within a sequence.
    flatten() collapses the outermost level of nesting. We first flatten the whole thing,
    to compute the max activation across all tokens in all sequences of all provenances.
    We floor at 0, so if all activations are negative max_activation is 0.
    Then we step through each token in each sequence in each provenance, and divide by
    that max activation, and floor at 0, to get a normalized activation with ceiling at 1
    and floor at 0. This normalized_activation goes in a TokenAndScalar object, which
    is in a nested list structure parallel to the input structure."""
    indexed_by_token: list[TokenAndRawScalar] = flatten(flatten(list_of_sequence_lists))
    indexed_by_token = [
        TokenAndRawScalar(token=d.token, scalar=max(d.scalar, 0)) for d in indexed_by_token
    ]
    max_scalar = max([d.scalar for d in indexed_by_token]) or 0
    neuron_scale = create_scale_linear(max_scalar)
    return [
        [
            [
                TokenAndScalar(
                    token=d.token,
                    scalar=d.scalar,
                    normalized_scalar=neuron_scale(d.scalar),
                )
                for d in sequence
            ]
            for sequence in list_of_sequences
        ]
        for list_of_sequences in list_of_sequence_lists
    ]
def create_scale_linear(value: float) -> Callable[[float], float]:
    return lambda x: max((x - 0) / (value - 1e-5), 0)
def normalize_attention_token_scalars(
    list_of_sequence_lists: list[list[list[TokenAndRawAttentionScalars]]],
) -> list[list[list[TokenAndAttentionScalars]]]:
    """The high level setup is analogous to normalize_token_scalars, but using
    TokenAndRawAttentionScalars objects, which have a list of floats associated
    to each token. There are several kinds of normalizations applied:
    1. Normalizing all scalars in all provenances, in all sequences, in all tokens,
    all scalars within a token's scalars, to the same scale so that they are in
    the range [0, 1].
    2. Summarizing the lists of scalars per token in some way to get a single scalar
    per token, and then normalizing those scalars to the same scale, as described in 1.and
    a. one way to summarize is to compute a summary statistic on the "scalars in", which are
    the list of floats associated to each token.
    b. the other way is to compute a summary statistic on the "scalars out", which are the
    ith entry in each list of of floats that contains an ith entry.
    i. one summary statistic used is the "total", or sum of all scalars in or out
    ii. another summary statistic used is the "max", or maximum of all scalars in or out
    These normalized and summarized scalars are then stored in a TokenAndAttentionScalars object,
    which is returned in a nested list structure parallel to the input structure.
    """
    indexed_by_token_sequence = flatten(list_of_sequence_lists)
    indexed_by_token = flatten(indexed_by_token_sequence)
    indexed_by_token = [
        TokenAndRawAttentionScalars(token=d.token, scalars=[max(a, 0) for a in d.scalars])
        for d in indexed_by_token
    ]
    max_scalar = max([a for d in indexed_by_token for a in d.scalars]) or 0
    total_scalar_scale = create_scale_linear(max_scalar)
    def compute_summary_of_scalar_in(
        attn_token_sequence_list: list[TokenAndRawAttentionScalars],
        operation: Callable[[Sequence[float | None]], float],
    ) -> list[float]:
        return [operation(d.scalars) for d in attn_token_sequence_list]
    def _get_entry_if_available(scalars: Sequence[float], index: int) -> float | None:
        # if index is out of bounds, return None; otherwise return the entry at that index
        return scalars[index] if index < len(scalars) else None
    def _sum_non_none(scalars: Sequence[float | None]) -> float:
        # sum all non-None entries in scalars
        return sum([a for a in scalars if a is not None])
    def _max_non_none(scalars: Sequence[float | None]) -> float:
        # return the max of all non-None entries in scalars
        return max([a for a in scalars if a is not None])
    def compute_summary_of_scalar_out(
        attn_token_sequence_list: list[TokenAndRawAttentionScalars],
        operation: Callable[[Sequence[float | None]], float],
    ) -> list[float]:
        for i in range(len(attn_token_sequence_list)):
            # the attended to sequence length at token i is at most i+1
            assert len(attn_token_sequence_list[i].scalars) <= i + 1, (
                i,
                len(attn_token_sequence_list[i].scalars),
            )
        scalar_out = [
            operation(
                [
                    _get_entry_if_available(attn_token_sequence_list[i].scalars, j)
                    for i in range(j, len(attn_token_sequence_list))
                ]
            )
            for j in range(len(attn_token_sequence_list))
        ]
        return scalar_out
    def compute_total_scalar_in(
        attn_token_sequence_list: list[TokenAndRawAttentionScalars],
    ) -> list[float]:
        return compute_summary_of_scalar_in(attn_token_sequence_list, _sum_non_none)
    def compute_max_scalar_in(
        attn_token_sequence_list: list[TokenAndRawAttentionScalars],
    ) -> list[float]:
        return compute_summary_of_scalar_in(attn_token_sequence_list, _max_non_none)
    def compute_total_scalar_out(
        attn_token_sequence_list: list[TokenAndRawAttentionScalars],
    ) -> list[float]:
        return compute_summary_of_scalar_out(attn_token_sequence_list, _sum_non_none)
    def compute_max_scalar_out(
        attn_token_sequence_list: list[TokenAndRawAttentionScalars],
    ) -> list[float]:
        return compute_summary_of_scalar_out(attn_token_sequence_list, _max_non_none)
    def compute_scalar_summary_and_scale(
        compute_scalar_summary_function: Callable[[list[TokenAndRawAttentionScalars]], list[float]],
        list_of_sequence_lists: list[list[list[TokenAndRawAttentionScalars]]],
    ) -> tuple[list[list[list[float]]], Callable[[float], float]]:
        scalar_indexed_by_token_sequence_list: list[list[list[float]]] = [
            [compute_scalar_summary_function(sequence) for sequence in sequence_list]
            for sequence_list in list_of_sequence_lists
        ]
        scalar_indexed_by_token: list[float] = flatten(
            flatten(scalar_indexed_by_token_sequence_list)
        )
        scalar_indexed_by_token = [max(a, 0) for a in scalar_indexed_by_token]
        scalar_scale: Callable[[float], float] = create_scale_linear(
            max(scalar_indexed_by_token) or 0
        )
        return scalar_indexed_by_token_sequence_list, scalar_scale
    (
        total_scalar_in_indexed_by_token_sequence_list,
        total_scalar_in_scale,
    ) = compute_scalar_summary_and_scale(compute_total_scalar_in, list_of_sequence_lists)
    (
        max_scalar_in_indexed_by_token_sequence_list,
        max_scalar_in_scale,
    ) = compute_scalar_summary_and_scale(compute_max_scalar_in, list_of_sequence_lists)
    (
        total_scalar_out_indexed_by_token_sequence_list,
        total_scalar_out_scale,
    ) = compute_scalar_summary_and_scale(compute_total_scalar_out, list_of_sequence_lists)
    (
        max_scalar_out_indexed_by_token_sequence_list,
        max_scalar_out_scale,
    ) = compute_scalar_summary_and_scale(compute_max_scalar_out, list_of_sequence_lists)
    return [
        [
            [
                TokenAndAttentionScalars(
                    token=d.token,
                    scalars=d.scalars,
                    normalized_scalars=[total_scalar_scale(a) for a in d.scalars],
                    total_scalar_in=total_scalar_in_indexed_by_token_sequence_list[seq_list_idx][
                        seq_idx
                    ][token_idx],
                    normalized_total_scalar_in=total_scalar_in_scale(
                        total_scalar_in_indexed_by_token_sequence_list[seq_list_idx][seq_idx][
                            token_idx
                        ]
                    ),
                    max_scalar_in=max_scalar_in_indexed_by_token_sequence_list[seq_list_idx][
                        seq_idx
                    ][token_idx],
                    normalized_max_scalar_in=max_scalar_in_scale(
                        max_scalar_in_indexed_by_token_sequence_list[seq_list_idx][seq_idx][
                            token_idx
                        ]
                    ),
                    total_scalar_out=total_scalar_out_indexed_by_token_sequence_list[seq_list_idx][
                        seq_idx
                    ][token_idx],
                    normalized_total_scalar_out=total_scalar_out_scale(
                        total_scalar_out_indexed_by_token_sequence_list[seq_list_idx][seq_idx][
                            token_idx
                        ]
                    ),
                    max_scalar_out=max_scalar_out_indexed_by_token_sequence_list[seq_list_idx][
                        seq_idx
                    ][token_idx],
                    normalized_max_scalar_out=max_scalar_out_scale(
                        max_scalar_out_indexed_by_token_sequence_list[seq_list_idx][seq_idx][
                            token_idx
                        ]
                    ),
                )
                for token_idx, d in enumerate(sequence)
            ]
            for seq_idx, sequence in enumerate(list_of_sequences)
        ]
        for seq_list_idx, list_of_sequences in enumerate(list_of_sequence_lists)
    ]

================
File: neuron_explainer/activation_server/README.md
================
# Activation Server

Backend server for getting activation, neuron and explanation data via HTTP, either from Azure blob storage or from inference on a subject or assistant model.

- Gets activations, loss, or other inference data for individual neurons by performing inference on a subject model
- Reads existing neuron/explanation data from blob storage and returns it via HTTP
- Generates and scores explanations using the OpenAI API

## Running the server

To run activation server for GPT-2 small:

```sh
python neuron_explainer/activation_server/main.py --model_name gpt2-small --port 8000
```

To be able to replace MLP neurons with MLP autoencoder latents, and/or attention heads with attention autoencoder latents, 
the activation server needs to run the corresponding autoencoder:

```sh
python neuron_explainer/activation_server/main.py  --model_name gpt2-small --port 8000 --mlp_autoencoder_name ae-resid-delta-mlp-v4
python neuron_explainer/activation_server/main.py  --model_name gpt2-small --port 8000 --attn_autoencoder_name ae-resid-delta-attn-v4
python neuron_explainer/activation_server/main.py  --model_name gpt2-small --port 8000 --attn_autoencoder_name ae-resid-delta-attn-v4 --mlp_autoencoder_name ae-resid-delta-mlp-v4
```

Running the activation server with autoencoders will add one or two toggle buttons in the Transformer Debugger UI,
to switch between MLP neurons and MLP autoencoder latents ("Use MLP autoencoder"),
or between attention heads and attention autoencoder latents ("Use Attention autoencoder").

See all the available autoencoder names with
```py
from neuron_explainer.models.model_registry import list_autoencoder_names
print(list_autoencoder_names("gpt2-small"))
```

## Generating client libraries

Typescript client libraries for interacting with the activation server are auto-generated based on the server code. If you make any changes to this directory, be sure to regenerate the client libraries:

First, start up a local activation server without running a model:

```sh
python neuron_explainer/activation_server/main.py --run_model False --port 8000
```

Then in another terminal, run:

```sh
cd neuron_viewer
npm run generate-client
```

To run these steps without waiting on activation server to spin up, you can also run

```sh
python neuron_explainer/activation_server/main.py --run_model False --port 8000 &
while ! lsof -i :8000; do sleep 1; done; cd neuron_viewer; npm run generate-client; git commit -am "generate client"
kill -9 $(lsof -t -i:8000)
cd ../../..
```

in the second terminal.

## Code organization

- [main.py](main.py): Entry point for the server.
- \*\_routes.py: Route / endpoint definitions, organized by functionality. See individual files.
- [interactive_model.py](interactive_model.py): Code for performing inference on a model, used by [inference_routes.py](inference_routes.py).

## Debugging cuda memory issues

Pytorch has helpful utilities for debugging cuda memory issues, particularly OOMs. To enable cuda memory debugging, set `--cuda_memory_debugging True` when starting the server (presumably on a devbox). See [main.py](main.py) for the implementation. This will enable memory tracking when the server starts up and dump a snapshot every time the server receives a request to its `/dump_memory_snapshot` endpoint. Once you've generated a snapshot, run the following commands to generate and open an HTML page for viewing it:

```sh
# Download the memory viz script.
curl -o _memory_viz.py https://raw.githubusercontent.com/pytorch/pytorch/main/torch/cuda/_memory_viz.py

# Generate the HTML page.
python _memory_viz.py trace_plot torch_memory_snapshot_*.pkl -o snapshot.html

# Open the HTML page.
open snapshot.html
```

Click on individual bars to see a stack trace for the allocation.

See this blog post for more suggestions: https://pytorch.org/blog/understanding-gpu-memory-1/

## CORS

The activation server is configured to allow requests from any localhost origin. If you decide to
run one or both of the servers remotely, you will need to update the CORS configuration in
[main.py](main.py) to allow requests from the appropriate origins.

## Auto-explanation and scoring

[explainer_routes.py](explainer_routes.py) includes endpoints for explaining and scoring nodes (e.g. MLP neurons, autoencoder latents, attention heads, etc.). Scoring requires a simulator that estimates activations on a given token sequence using an explanation for a node. In the original "Language models can explain neurons in language models" paper, we use a simulator (`ExplanationNeuronSimulator`) that requires the model backing it to return logprobs for the input prompt. Thus, simulation of activations on a token sequence can be performed in a single forward pass. Unfortunately, this logprob-based methodology we used in our last release is no longer feasible since logprobs aren't available for prompt tokens for the relevant production models. This repo has as its default simulator a much slower design where the prompt completion includes the simulated activations (`LogprobFreeExplanationTokenSimulator`). This design is also less reliable, because the completion doesn't necessarily fit the expected format or accurately reproduce the token sequence. For many simulation requests, this simulator will log an error and return all zero estimated activations.

================
File: neuron_explainer/activation_server/requests_and_responses.py
================
"""
Request and response definitions. This file shouldn't contain any functions, other than those
defined on the dataclasses.
Requests to InteractiveModel have two parts: an InferenceRequestSpec, specifying how to
run inference to obtain activations, and a ProcessingRequestSpec, specifying how to process those
activations to obtain derived scalars. An InferenceSubRequest contains information for a single
inference step (forward and optionally also backward pass), and one or more ProcessingRequestSpecs
to process activations from the same inference step. A BatchedRequest contains one or more
InferenceSubRequests, whose inference steps are run in parallel, and whose processing steps are
performed sequentially.
InferenceRequests are analogous to single InferenceSubRequests, and are processed stand-alone rather
than in a batch.
TdbRequests compactly specify the information in InferenceRequestSpec and ProcessingRequestSpec,
with only the degrees of freedom permitted by the TDB UI. TdbRequests are converted to
InferenceSubRequests and ProcessingRequestSpecs in tdb_conversions.py. BatchedTdbRequests are
analogous to BatchedRequests.
"""
import math
from enum import Enum
from typing import Any, Literal, Union
import torch
from pydantic import root_validator
from neuron_explainer.activation_server.load_neurons import NodeIdAndDatasets
from neuron_explainer.activation_server.read_routes import TokenAndAttentionScalars
from neuron_explainer.activations.derived_scalars.derived_scalar_types import DerivedScalarType
from neuron_explainer.activations.derived_scalars.indexing import (
    AblationSpec,
    MirroredActivationIndex,
    MirroredNodeIndex,
    MirroredTraceConfig,
    NodeAblation,
    NodeToTrace,
)
from neuron_explainer.activations.derived_scalars.tokens import TopTokens
from neuron_explainer.models.model_component_registry import Dimension, LayerIndex, PassType
from neuron_explainer.pydantic import CamelCaseBaseModel, immutable
########## Types used by multiple requests and/or responses ##########
# NOTE: other than TDB_REQUEST_SPEC, these all contain params for processing activations
# only, and not for specifying how to run inference to obtain those activations
class SpecType(Enum):
    ACTIVATIONS_REQUEST_SPEC = "activations_request_spec"
    DERIVED_SCALARS_REQUEST_SPEC = "derived_scalars_request_spec"
    DERIVED_ATTENTION_SCALARS_REQUEST_SPEC = "derived_attention_scalars_request_spec"
    MULTIPLE_TOP_K_DERIVED_SCALARS_REQUEST_SPEC = "multiple_top_k_derived_scalars_request_spec"
    SCORED_TOKENS_REQUEST_SPEC = "scored_tokens_request_spec"
    TDB_REQUEST_SPEC = "tdb_request_spec"
    TOKEN_PAIR_ATTRIBUTION_REQUEST_SPEC = "token_pair_attribution_request_spec"
class ProcessingResponseDataType(Enum):
    DERIVED_SCALARS_RESPONSE_DATA = "derived_scalars_response_data"
    DERIVED_ATTENTION_SCALARS_RESPONSE_DATA = "derived_attention_scalars_response_data"
    MULTIPLE_TOP_K_DERIVED_SCALARS_RESPONSE_DATA = "multiple_top_k_derived_scalars_response_data"
    SCORED_TOKENS_RESPONSE_DATA = "scored_tokens_response_data"
    TOKEN_PAIR_ATTRIBUTION_RESPONSE_DATA = "token_pair_attribution_response_data"
class LossFnName(str, Enum):
    LOGIT_DIFF = "logit_diff"
    LOGIT_MINUS_MEAN = "logit_minus_mean"
    PROBS = "probs"
    ZERO = "zero"
class LossFnConfig(CamelCaseBaseModel):
    name: LossFnName
    target_tokens: list[str] | None = None
    distractor_tokens: list[str] | None = None
@immutable
class InferenceRequestSpec(CamelCaseBaseModel):
    """The minimum specification for performing a forward and/or backward pass on a model, with hooks at some set of layers."""
    prompt: str
    ablation_specs: list[AblationSpec] | None = None
    # note that loss_fn_config and trace_config are mutually exclusive
    loss_fn_config: LossFnConfig | None = None
    # used for performing a backward pass from an internal point within the network
    trace_config: MirroredTraceConfig | None = None
    # used for tracing latent activations back to the activations for the DSTs which they encode
    activation_index_for_within_layer_grad: MirroredActivationIndex | None = None
@immutable
class InferenceRequest(CamelCaseBaseModel):
    inference_request_spec: InferenceRequestSpec
class InferenceData(CamelCaseBaseModel):
    inference_time: float
    memory_used_before: float | None
    memory_used_after: float | None
    log: str | None = None
    loss: float | None = None
    activation_value_for_backward_pass: float | None = None
@immutable
class InferenceAndTokenData(InferenceData):
    tokens_as_ints: list[int]
    tokens_as_strings: list[str]
@immutable
class InferenceResponse(CamelCaseBaseModel):
    inference_and_token_data: InferenceAndTokenData
class GroupId(str, Enum):
    """Identifiers for groups in multi-top-k requests."""
    ACT_TIMES_GRAD = "act_times_grad"
    ACTIVATION = "activation"
    DIRECT_WRITE_TO_GRAD = "direct_write_to_grad"
    DIRECTION_WRITE = "direction_write"
    LOGITS = "logits"
    MLP_LAYER_WRITE = "mlp_layer_write"
    # Used in situations where there's only one group.
    SINGLETON = "singleton"
    # Used for projecting write vectors of nodes to token space.
    TOKEN_WRITE = "token_write"
    # Used for projecting read vectors of nodes to token space.
    TOKEN_READ = "token_read"
    WRITE_NORM = "write_norm"
    # Used for token pair attribution requests.
    TOKEN_PAIR_ATTRIBUTION = "token_pair_attribution"
    @property
    def exclude_bottom_k(self) -> bool:
        # if False, top k should return both the top k largest and smallest/(most negative) activations;
        # otherwise, should return the top k largest only. Generally, exclude_bottom_k = True is
        # appropriate for scalars that are non-negative (the values closest to 0 are not particularly interesting).
        # exclude_bottom_k = False is appropriate for scalars that can be positive or negative (the most negative values
        # may be interesting).
        return self in {
            GroupId.WRITE_NORM,
            GroupId.ACTIVATION,
            GroupId.LOGITS,  # logits can be positive or negative, but generally we are interested the most likely
            # tokens to be sampled, which are the most positive logits
        }
########## Tensors ##########
class TensorType(Enum):
    TENSOR_0D = "tensor_0d"
    TENSOR_1D = "tensor_1d"
    TENSOR_2D = "tensor_2d"
    TENSOR_3D = "tensor_3d"
class TorchableTensor(CamelCaseBaseModel):
    tensor_type: TensorType
    value: Any
    def torch(self) -> torch.Tensor:
        return torch.tensor(self.value)
@immutable
class Tensor0D(TorchableTensor):
    tensor_type: TensorType = TensorType.TENSOR_0D
    value: float
@immutable
class Tensor1D(TorchableTensor):
    tensor_type: TensorType = TensorType.TENSOR_1D
    value: list[float]
@immutable
class Tensor2D(TorchableTensor):
    tensor_type: TensorType = TensorType.TENSOR_2D
    value: list[list[float]]
@immutable
class Tensor3D(TorchableTensor):
    tensor_type: TensorType = TensorType.TENSOR_3D
    value: list[list[list[float]]]
TensorND = Union[Tensor0D, Tensor1D, Tensor2D, Tensor3D]
########## Model info ##########
@immutable
class ModelInfoResponse(CamelCaseBaseModel):
    model_name: str | None
    has_mlp_autoencoder: bool
    mlp_autoencoder_name: str | None
    has_attention_autoencoder: bool
    attention_autoencoder_name: str | None
    n_layers: int
########## Derived scalars ##########
@immutable
class DerivedScalarsRequestSpec(CamelCaseBaseModel):
    # note: the spec_type field is not to be populated by the user at __init__, but is
    # required for pydantic to distinguish between different XRequestSpec classes
    spec_type: Literal[
        SpecType.DERIVED_SCALARS_REQUEST_SPEC
    ] = SpecType.DERIVED_SCALARS_REQUEST_SPEC
    dst: DerivedScalarType
    layer_index: LayerIndex
    activation_index: int
    normalize_activations_using_neuron_record: NodeIdAndDatasets | None = None
    """
    If non-None, the response will include normalized activations. The max scalar used for
    normalization will be the max scalar in the neuron record specified by the NodeIdAndDatasets.
    """
    pass_type: PassType = PassType.FORWARD
    num_top_tokens: int | None = None
    """
    If non-None, return the top and bottom tokens for the node, according to the scoring
    methodology associated with the derived scalar type.
    """
@immutable
class DerivedScalarsRequest(InferenceRequest):
    derived_scalars_request_spec: DerivedScalarsRequestSpec
@immutable
class DerivedScalarsResponseData(CamelCaseBaseModel):
    response_data_type: ProcessingResponseDataType = (
        ProcessingResponseDataType.DERIVED_SCALARS_RESPONSE_DATA
    )
    activations: list[float]
    normalized_activations: list[float] | None
    """
    The same activations, but normalized to [0, 1] using the max scalar in the specified neuron
    record. Only set if normalize_activations_using_neuron_record was specified in the request.
    """
    node_indices: list[MirroredNodeIndex]
    top_tokens: TopTokens | None
    """
    While this response covers multiple nodes, those nodes differ only in the sequence token index:
    they all correspond to a single component (per go/tdb-terminology). Top tokens are the same for
    all nodes associated with a single component, so we only need to return one set of top tokens
    for the entire component. This will be None if num_top_tokens is None or if the activation was
    zero, preventing the relevant write vector from being computed.
    """
@immutable
class DerivedScalarsResponse(InferenceResponse):
    derived_scalars_response_data: DerivedScalarsResponseData
########## Derived attention scalars ##########
@immutable
class DerivedAttentionScalarsRequestSpec(CamelCaseBaseModel):
    # note: the spec_type field is not to be populated by the user at __init__, but is
    # required for pydantic to distinguish between different XRequestSpec classes
    spec_type: Literal[
        SpecType.DERIVED_ATTENTION_SCALARS_REQUEST_SPEC
    ] = SpecType.DERIVED_ATTENTION_SCALARS_REQUEST_SPEC
    dst: DerivedScalarType
    layer_index: LayerIndex
    activation_index: int
    normalize_activations_using_neuron_record: NodeIdAndDatasets | None = None
    """
    If non-None, the response will include normalized activations. The max scalars used for
    normalization will be the max scalars in the neuron record specified by the NodeIdAndDatasets.
    """
@immutable
class DerivedAttentionScalarsRequest(InferenceRequest):
    derived_attention_scalars_request_spec: DerivedAttentionScalarsRequestSpec
@immutable
class DerivedAttentionScalarsResponseData(CamelCaseBaseModel):
    response_data_type: ProcessingResponseDataType = (
        ProcessingResponseDataType.DERIVED_ATTENTION_SCALARS_RESPONSE_DATA
    )
    token_and_attention_scalars_list: list[TokenAndAttentionScalars]
@immutable
class DerivedAttentionScalarsResponse(InferenceResponse):
    derived_attention_scalars_response_data: DerivedAttentionScalarsResponseData
########## (Multi) top-k ##########
# This dataclass is not used in any requests or responses. It's used internally to represent a top-k
# operation performed as part of servicing a MultipleTopKDerivedScalarsRequest.
@immutable
class TopKParams(CamelCaseBaseModel):
    dst_list: list[DerivedScalarType]
    token_index: int | None
    top_and_bottom_k: int | None = None
    pass_type: PassType = PassType.FORWARD
    exclude_bottom_k: bool = False
    dimensions_to_keep_for_intermediate_sum: list[Dimension] = [
        Dimension.SEQUENCE_TOKENS,
        Dimension.ATTENDED_TO_SEQUENCE_TOKENS,
    ]
@immutable
class MultipleTopKDerivedScalarsRequestSpec(CamelCaseBaseModel):
    # note: the spec_type field is not to be populated by the user at __init__, but is
    # required for pydantic to distinguish between different XRequestSpec classes
    spec_type: Literal[
        SpecType.MULTIPLE_TOP_K_DERIVED_SCALARS_REQUEST_SPEC
    ] = SpecType.MULTIPLE_TOP_K_DERIVED_SCALARS_REQUEST_SPEC
    dst_list_by_group_id: dict[GroupId, list[DerivedScalarType]]
    # dsts for each group ID are assumed to have defined node_type,
    # all node_types assumed to be distinct within a group_id, and all group_ids to
    # contain the same set of node_types.
    token_index: int | None
    top_and_bottom_k: int | None = None
    pass_type: PassType = PassType.FORWARD
    dimensions_to_keep_for_intermediate_sum: list[Dimension] = [
        Dimension.SEQUENCE_TOKENS,
        Dimension.ATTENDED_TO_SEQUENCE_TOKENS,
    ]
    def get_top_k_params_for_group_id(self, group_id: GroupId) -> TopKParams:
        """
        A MultipleTopKDerivedScalarsRequestSpec object contains the information necessary to
        generate multiple TopKParams objects, one for each group ID. This function returns the
        TopKParams for a specific group ID.
        """
        dst_list = self.dst_list_by_group_id[group_id]
        exclude_bottom_k = group_id.exclude_bottom_k
        # Convert the instance to a dictionary
        data = self.dict()
        # Remove the fields that are not needed in TopKDerivedScalarsRequestSpec
        data.pop("dst_list_by_group_id")
        data.pop("spec_type")
        # Add the fields specific to TopKDerivedScalarsRequestSpec
        data["dst_list"] = dst_list
        data["exclude_bottom_k"] = exclude_bottom_k
        return TopKParams(**data)
# All sub-requests within this request must have comparable prompts, since when top-k operations
# within the batch will union over node indices (within each spec name).
@immutable
class MultipleTopKDerivedScalarsRequest(InferenceRequest):
    multiple_top_k_derived_scalars_request_spec: MultipleTopKDerivedScalarsRequestSpec
@immutable
class MultipleTopKDerivedScalarsResponseData(CamelCaseBaseModel):
    response_data_type: ProcessingResponseDataType = (
        ProcessingResponseDataType.MULTIPLE_TOP_K_DERIVED_SCALARS_RESPONSE_DATA
    )
    # Activations associated with top-k nodes for this sub-request, as well as top-k nodes with the
    # same spec name in other (multi) top-k requests in this batched request.
    activations_by_group_id: dict[GroupId, list[float]]
    # Indices for top-k nodes associated with this request, as well as top-k nodes with the same
    # spec name in other (multi) top-k requests in this batched request.
    node_indices: list[MirroredNodeIndex]
    vocab_token_strings_for_indices: list[str | None] | None
    # sum_... entries indicate total of all activations in group, including non-top-k activations
    intermediate_sum_activations_by_dst_by_group_id: dict[
        GroupId, dict[DerivedScalarType, TensorND]
    ]
    @root_validator
    def check_consistency(cls, values: dict[str, Any]) -> dict[str, Any]:
        activations_by_group_id = values.get("activations_by_group_id")
        assert activations_by_group_id is not None
        node_indices = values.get("node_indices")
        assert node_indices is not None
        vocab_token_strings_for_indices = values.get("vocab_token_strings_for_indices")
        for group_id, activations in activations_by_group_id.items():
            assert len(node_indices) == len(activations), (
                f"Expected len(node_indices) == len(activations) for group_id {group_id},"
                f" but got len(node_indices)={len(node_indices)}, len(activations)={len(activations)}"
            )
            assert all(math.isfinite(activation) for activation in activations), (
                f"Expected all activations to be finite for group_id {group_id},"
                f" but got activations={activations}"
            )
        if vocab_token_strings_for_indices is not None:
            assert len(node_indices) == len(vocab_token_strings_for_indices), (
                f"Expected len(node_indices) == len(vocab_token_strings_for_indices),"
                f" but got len(node_indices)={len(node_indices)}, len(vocab_token_strings_for_indices)={len(vocab_token_strings_for_indices)}"
            )
        return values
@immutable
class MultipleTopKDerivedScalarsResponse(InferenceResponse):
    multiple_top_k_derived_scalars_response_data: MultipleTopKDerivedScalarsResponseData
########## Scored tokens ##########
class TokenScoringType(Enum):
    """Methods by which vocab tokens may be scored."""
    # Score tokens by the degree to which this node directly upvotes them. This is basically the
    # "logit lens".
    UPVOTED_OUTPUT_TOKENS = "upvoted_output_tokens"
    # Score tokens by the degree to which they directly upvote this node. Three flavors, each of
    # which applies to both "raw" components like neurons and attention heads, as well as
    # autoencoder latents:
    #  1) Upvoting MLP nodes
    #  2) Upvoting the Q part of attention nodes
    #  3) Upvoting the K part of attention nodes
    INPUT_TOKENS_THAT_UPVOTE_MLP = "input_tokens_that_upvote_mlp"
    INPUT_TOKENS_THAT_UPVOTE_ATTN_Q = "input_tokens_that_upvote_attn_q"
    INPUT_TOKENS_THAT_UPVOTE_ATTN_K = "input_tokens_that_upvote_attn_k"
@immutable
class ScoredTokensRequestSpec(CamelCaseBaseModel):
    # note: the spec_type field is not to be populated by the user at __init__, but is
    # required for pydantic to distinguish between different XRequestSpec classes
    spec_type: Literal[SpecType.SCORED_TOKENS_REQUEST_SPEC] = SpecType.SCORED_TOKENS_REQUEST_SPEC
    # How tokens should be scored.
    token_scoring_type: TokenScoringType
    # A value of e.g. 10 means 10 top and 10 bottom tokens.
    num_tokens: int
    # Which nodes do we want to get scored tokens for, and which DSTs and DST configs should we use?
    # This request spec refers to another request spec and grabs those values from it.
    depends_on_spec_name: str
@immutable
class ScoredTokensRequest(InferenceRequest):
    scored_tokens_request_spec: ScoredTokensRequestSpec
@immutable
class ScoredTokensResponseData(CamelCaseBaseModel):
    response_data_type: ProcessingResponseDataType = (
        ProcessingResponseDataType.SCORED_TOKENS_RESPONSE_DATA
    )
    # These two lists are parallel and have the same length. "None" values in top_tokens_list
    # indicate that the specified TokenScoringType does not apply to the corresponding node.
    node_indices: list[MirroredNodeIndex]
    top_tokens_list: list[TopTokens | None]
@immutable
class ScoredTokensResponse(InferenceResponse):
    scored_tokens_response_data: ScoredTokensResponseData
########## TDB-specific ##########
class ComponentTypeForMlp(Enum):
    """The type of component / fundamental unit to use for MLP layers.
    This determines which types of node appear in the node table to represent the MLP layers.
    Neurons are the fundamental unit of MLP layers, but autoencoder latents are more interpretable.
    """
    NEURON = "neuron"
    AUTOENCODER_LATENT = "autoencoder_latent"
class ComponentTypeForAttention(Enum):
    """The type of component / fundamental unit to use for Attention layers.
    This determines which types of node appear in the node table to represent the Attention layers.
    Heads are the fundamental unit of Attention layers, but autoencoder latents are more interpretable.
    """
    ATTENTION_HEAD = "attention_head"
    AUTOENCODER_LATENT = "autoencoder_latent"
@immutable
class TdbRequestSpec(CamelCaseBaseModel):
    # note: the spec_type field is not to be populated by the user at __init__, but is
    # required for pydantic to distinguish between different XRequestSpec classes
    spec_type: Literal[SpecType.TDB_REQUEST_SPEC] = SpecType.TDB_REQUEST_SPEC
    prompt: str
    target_tokens: list[str]
    distractor_tokens: list[str]
    component_type_for_mlp: ComponentTypeForMlp
    """Whether to use neurons or autoencoder latents as the basic unit for MLP layers."""
    component_type_for_attention: ComponentTypeForAttention
    """Whether to use heads or autoencoder latents as the basic unit for attention layers."""
    top_and_bottom_k_for_node_table: int
    """The number of top and bottom nodes to calculate for each column in the node table."""
    hide_early_layers_when_ablating: bool
    """Whether to exclude layers before the first ablated layer from the results."""
    node_ablations: list[NodeAblation] | None
    upstream_node_to_trace: NodeToTrace | None
    """The primary node at which a gradient is being computed"""
    downstream_node_to_trace: NodeToTrace | None
    """In the case where upstream_node_to_trace is an attention value subnode, you can also
    provide a downstream node to trace. A gradient is first computed with respect to this downstream
    node, and then the direct effect of the upstream node on this gradient direction is computed. A
    gradient is then computed with respect to that quantity, propagated back to upstream activations.
    In the case where no downstream node is provided, the loss is used as the "downstream node"."""
@immutable
class BatchedTdbRequest(CamelCaseBaseModel):
    sub_requests: list[TdbRequestSpec]
########## Attribution ##########
@immutable
class TopTokensAttendedTo(CamelCaseBaseModel):
    token_indices: list[int]  # in sequence
    attributions: list[float]
@immutable
class TokenPairAttributionRequestSpec(CamelCaseBaseModel):
    # note: the spec_type field is not to be populated by the user at __init__, but is
    # required for pydantic to distinguish between different XRequestSpec classes
    spec_type: Literal[
        SpecType.TOKEN_PAIR_ATTRIBUTION_REQUEST_SPEC
    ] = SpecType.TOKEN_PAIR_ATTRIBUTION_REQUEST_SPEC
    num_tokens_attended_to: int
    # Which nodes do we want to get scored tokens for, and which DSTs and DST configs should we use?
    # This request spec refers to another request spec and grabs those values from it.
    depends_on_spec_name: str
@immutable
class TokenPairAttributionRequest(InferenceRequest):
    token_pair_attribution_request_spec: TokenPairAttributionRequestSpec
@immutable
class TokenPairAttributionResponseData(CamelCaseBaseModel):
    response_data_type: ProcessingResponseDataType = (
        ProcessingResponseDataType.TOKEN_PAIR_ATTRIBUTION_RESPONSE_DATA
    )
    # These two lists are parallel and have the same length. "None" values in top_tokens_attended_to_list
    # indicate that token-pair attribution does not apply to the corresponding node.
    node_indices: list[MirroredNodeIndex]
    top_tokens_attended_to_list: list[TopTokensAttendedTo | None]
@immutable
class TokenPairAttributionResponse(InferenceResponse):
    token_pair_attribution_response_data: TokenPairAttributionResponseData
########## Batching ##########
# Order from most to least specific
# See https://docs.pydantic.dev/1.10/usage/types/#unions
ProcessingRequestSpec = Union[
    MultipleTopKDerivedScalarsRequestSpec,
    DerivedScalarsRequestSpec,
    DerivedAttentionScalarsRequestSpec,
    ScoredTokensRequestSpec,
    TokenPairAttributionRequestSpec,
]
# Order from most to least specific
# See https://docs.pydantic.dev/1.10/usage/types/#unions
ProcessingResponseData = Union[
    MultipleTopKDerivedScalarsResponseData,
    DerivedScalarsResponseData,
    DerivedAttentionScalarsResponseData,
    ScoredTokensResponseData,
    TokenPairAttributionResponseData,
]
@immutable
class InferenceSubRequest(CamelCaseBaseModel):
    inference_request_spec: InferenceRequestSpec
    processing_request_spec_by_name: dict[str, ProcessingRequestSpec] = {}
@immutable
class InferenceResponseAndResponseDict(CamelCaseBaseModel):
    inference_response: InferenceResponse
    processing_response_data_by_name: dict[str, ProcessingResponseData] = {}
@immutable
class BatchedRequest(CamelCaseBaseModel):
    inference_sub_requests: list[InferenceSubRequest]
@immutable
class BatchedResponse(CamelCaseBaseModel):
    inference_sub_responses: list[InferenceResponseAndResponseDict]

================
File: neuron_explainer/activation_server/tdb_conversions.py
================
# Code for converting between client-friendly TDB request/response dataclasses and internal
# representations used during request processing.
from typing import TypeVar
from neuron_explainer.activation_server.requests_and_responses import *
from neuron_explainer.activation_server.requests_and_responses import GroupId
from neuron_explainer.activations.derived_scalars.derived_scalar_types import DerivedScalarType
from neuron_explainer.activations.derived_scalars.indexing import (
    DETACH_LAYER_NORM_SCALE,
    AblationSpec,
    AttentionTraceType,
    MirroredActivationIndex,
    MirroredNodeIndex,
    MirroredTraceConfig,
    NodeAblation,
    NodeToTrace,
    PreOrPostAct,
    TraceConfig,
)
from neuron_explainer.models.model_component_registry import (
    ActivationLocationType,
    Dimension,
    NodeType,
    PassType,
)
T = TypeVar("T")
def convert_tdb_request_spec_to_inference_sub_request(
    tdb_request_spec: TdbRequestSpec,
) -> InferenceSubRequest:
    """
    The client sends a TdbRequestSpec, but internally we do all processing in terms of
    InferenceSubRequests. This function converts from the client representation to the server
    representation.
    """
    loss_fn_config: LossFnConfig | None = LossFnConfig(
        name=LossFnName.LOGIT_DIFF,
        target_tokens=tdb_request_spec.target_tokens,
        distractor_tokens=tdb_request_spec.distractor_tokens,
    )
    ablation_specs = [
        node_ablation_to_ablation_spec(ablation)
        for ablation in (tdb_request_spec.node_ablations or [])
    ] + [
        AblationSpec(
            index=MirroredActivationIndex(
                activation_location_type=ActivationLocationType.RESID_FINAL_LAYER_NORM_SCALE,
                pass_type=PassType.BACKWARD,
                tensor_indices=("All", "All"),  # ablate at all positions in the sequence
                layer_index=None,
            ),
            value=0,
        )
    ]
    current_token_index = -1
    trace_config = None
    if tdb_request_spec.upstream_node_to_trace is None:
        assert tdb_request_spec.downstream_node_to_trace is None
    else:
        (
            trace_config,
            trace_token_index,
        ) = nodes_to_trace_to_trace_config(
            tdb_request_spec.upstream_node_to_trace, tdb_request_spec.downstream_node_to_trace
        )
        if trace_token_index is not None:
            current_token_index = trace_token_index
    if trace_config is None:  # not tracing -> DO compute loss
        pass
    elif trace_config.attention_trace_type == AttentionTraceType.V:  # tracing attention through V
        if trace_config.downstream_trace_config is None:
            pass  # tracing through V with no downstream trace -> DO compute loss
        else:
            loss_fn_config = (
                None  # tracing through V, but also with downstream trace -> DON'T compute loss
            )
    else:
        loss_fn_config = (
            None  # tracing something other than attention through V -> DON'T compute loss
        )
    inference_request_spec = InferenceRequestSpec(
        prompt=tdb_request_spec.prompt,
        loss_fn_config=loss_fn_config,
        ablation_specs=ablation_specs,
        trace_config=MirroredTraceConfig.from_trace_config(trace_config) if trace_config else None,
    )
    spec_by_component_for_top_k = MultipleTopKDerivedScalarsRequestSpec(
        token_index=None,
        dst_list_by_group_id=make_grouped_dsts_per_component(
            tdb_request_spec.component_type_for_mlp,
            tdb_request_spec.component_type_for_attention,
        ),
        top_and_bottom_k=tdb_request_spec.top_and_bottom_k_for_node_table,
    )
    spec_by_component_always_mlp_for_token_display = MultipleTopKDerivedScalarsRequestSpec(
        token_index=None,
        # the response to this request is to be used for summarizing the effects of entire
        # attention and MLP layers per token; thus, using a different basis for the activations
        # within a layer is not helpful, and we use MLP activations themselves.
        dst_list_by_group_id=make_grouped_dsts_per_component(
            ComponentTypeForMlp.NEURON, ComponentTypeForAttention.ATTENTION_HEAD
        ),
        top_and_bottom_k=1,
        dimensions_to_keep_for_intermediate_sum=[
            Dimension.SEQUENCE_TOKENS,
            Dimension.ATTENDED_TO_SEQUENCE_TOKENS,
        ],
    )
    def scored_tokens_request_spec(
        token_scoring_type: TokenScoringType,
    ) -> ScoredTokensRequestSpec:
        return ScoredTokensRequestSpec(
            token_scoring_type=token_scoring_type,
            num_tokens=10,
            # Our scored tokens requests are associated with the "topKComponents" request spec. This
            # means that they use the same node indices, DSTs and DST configs.
            depends_on_spec_name="topKComponents",
        )
    def token_pair_attribution_request_spec() -> TokenPairAttributionRequestSpec:
        return TokenPairAttributionRequestSpec(
            num_tokens_attended_to=3,
            # Our scored tokens requests are associated with the "topKComponents" request spec. This
            # means that they use the same node indices, DSTs and DST configs.
            depends_on_spec_name="topKComponents",
        )
    processing_request_spec_by_name: dict[str, ProcessingRequestSpec] = {
        "topKComponents": spec_by_component_for_top_k,
        "componentSumsForTokenDisplay": spec_by_component_always_mlp_for_token_display,
        # It's important for these request specs to come after the "topKComponents" request spec,
        # since they depend on data generated for that request spec.
        "upvotedOutputTokens": scored_tokens_request_spec(TokenScoringType.UPVOTED_OUTPUT_TOKENS),
        "inputTokensThatUpvoteMlp": scored_tokens_request_spec(
            TokenScoringType.INPUT_TOKENS_THAT_UPVOTE_MLP
        ),
        "inputTokensThatUpvoteAttnQ": scored_tokens_request_spec(
            TokenScoringType.INPUT_TOKENS_THAT_UPVOTE_ATTN_Q
        ),
        "inputTokensThatUpvoteAttnK": scored_tokens_request_spec(
            TokenScoringType.INPUT_TOKENS_THAT_UPVOTE_ATTN_K
        ),
        "tokenPairAttribution": token_pair_attribution_request_spec(),
    }
    spec_by_vocab_token = MultipleTopKDerivedScalarsRequestSpec(
        dst_list_by_group_id={GroupId.LOGITS: [DerivedScalarType.LOGITS]},
        top_and_bottom_k=100,
        token_index=current_token_index,
    )
    processing_request_spec_by_name["topOutputTokenLogits"] = spec_by_vocab_token
    return InferenceSubRequest(
        inference_request_spec=inference_request_spec,
        processing_request_spec_by_name=processing_request_spec_by_name,
    )
def node_ablation_to_ablation_spec(node_ablation: NodeAblation) -> AblationSpec:
    node_index = node_ablation.node_index
    value = node_ablation.value
    match node_index.node_type:
        case NodeType.ATTENTION_HEAD:
            activation_location_type = ActivationLocationType.ATTN_QK_PROBS
            indices = [
                get_sequence_token_index(node_index),
                "All",
                get_activation_index(node_index),
            ]
        case NodeType.MLP_NEURON:
            activation_location_type = ActivationLocationType.MLP_POST_ACT
            indices = [
                get_sequence_token_index(node_index),
                get_activation_index(node_index),
            ]
        case (
            NodeType.AUTOENCODER_LATENT
            | NodeType.MLP_AUTOENCODER_LATENT
            | NodeType.ATTENTION_AUTOENCODER_LATENT
        ):
            from neuron_explainer.activations.derived_scalars.autoencoder import (
                get_autoencoder_alt_from_node_type,
            )
            activation_location_type = get_autoencoder_alt_from_node_type(node_index.node_type)
            indices = [
                get_sequence_token_index(node_index),
                get_activation_index(node_index),
            ]
        case _:
            raise ValueError(f"Unknown node type {node_index.node_type}")
    return AblationSpec(
        index=MirroredActivationIndex(
            activation_location_type=activation_location_type,
            pass_type=PassType.FORWARD,
            # mypy has trouble understanding that all of the values that can be assigned to indices
            # match AllOrOneIndices.
            tensor_indices=indices,  # type: ignore
            layer_index=node_index.layer_index,
        ),
        value=value,
    )
def get_sequence_token_index(node_index: MirroredNodeIndex) -> int:
    return assert_non_none(node_index.tensor_indices[0])
def get_activation_index(node_index: MirroredNodeIndex) -> int:
    return assert_non_none(node_index.tensor_indices[-1])
def assert_non_none(value: T | None) -> T:
    assert value is not None
    return value
def make_grouped_dsts_per_component(
    component_type_for_mlp: ComponentTypeForMlp,
    component_type_for_attention: ComponentTypeForAttention,
) -> dict[GroupId, list[DerivedScalarType]]:
    # common dsts for all components
    dsts = {
        GroupId.WRITE_NORM: [
            DerivedScalarType.RESID_POST_EMBEDDING_NORM,
        ],
        GroupId.ACT_TIMES_GRAD: [
            DerivedScalarType.TOKEN_ATTRIBUTION,
        ],
        GroupId.DIRECTION_WRITE: [
            DerivedScalarType.RESID_POST_EMBEDDING_PROJ_TO_FINAL_RESIDUAL_GRAD,
        ],
        GroupId.ACTIVATION: [
            DerivedScalarType.ALWAYS_ONE,  # the resid post embedding is considered to have an
            # "activation" of 1.0 at every position, for display purposes
        ],
    }
    match component_type_for_mlp:
        case ComponentTypeForMlp.NEURON:
            dsts[GroupId.WRITE_NORM].append(DerivedScalarType.MLP_WRITE_NORM)
            dsts[GroupId.ACT_TIMES_GRAD].append(DerivedScalarType.MLP_ACT_TIMES_GRAD)
            dsts[GroupId.DIRECTION_WRITE].append(DerivedScalarType.MLP_WRITE_TO_FINAL_RESIDUAL_GRAD)
            dsts[GroupId.ACTIVATION].append(DerivedScalarType.MLP_POST_ACT)
        case ComponentTypeForMlp.AUTOENCODER_LATENT:
            dsts[GroupId.WRITE_NORM].append(DerivedScalarType.ONLINE_MLP_AUTOENCODER_WRITE_NORM)
            dsts[GroupId.ACT_TIMES_GRAD].append(
                DerivedScalarType.ONLINE_MLP_AUTOENCODER_ACT_TIMES_GRAD
            )
            dsts[GroupId.DIRECTION_WRITE].append(
                DerivedScalarType.ONLINE_MLP_AUTOENCODER_WRITE_TO_FINAL_RESIDUAL_GRAD
            )
            dsts[GroupId.ACTIVATION].append(DerivedScalarType.ONLINE_MLP_AUTOENCODER_LATENT)
        case _:
            raise ValueError(f"Unknown component type {component_type_for_mlp} in TdbRequestSpec")
    match component_type_for_attention:
        case ComponentTypeForAttention.ATTENTION_HEAD:
            dsts[GroupId.WRITE_NORM].append(DerivedScalarType.UNFLATTENED_ATTN_WRITE_NORM)
            dsts[GroupId.ACT_TIMES_GRAD].append(DerivedScalarType.UNFLATTENED_ATTN_ACT_TIMES_GRAD)
            dsts[GroupId.DIRECTION_WRITE].append(
                DerivedScalarType.UNFLATTENED_ATTN_WRITE_TO_FINAL_RESIDUAL_GRAD
            )
            dsts[GroupId.ACTIVATION].append(DerivedScalarType.ATTN_QK_PROBS)
        case ComponentTypeForAttention.AUTOENCODER_LATENT:
            dsts[GroupId.WRITE_NORM].append(
                DerivedScalarType.ONLINE_ATTENTION_AUTOENCODER_WRITE_NORM
            )
            dsts[GroupId.ACT_TIMES_GRAD].append(
                DerivedScalarType.ONLINE_ATTENTION_AUTOENCODER_ACT_TIMES_GRAD
            )
            dsts[GroupId.DIRECTION_WRITE].append(
                DerivedScalarType.ONLINE_ATTENTION_AUTOENCODER_WRITE_TO_FINAL_RESIDUAL_GRAD
            )
            dsts[GroupId.ACTIVATION].append(DerivedScalarType.ONLINE_ATTENTION_AUTOENCODER_LATENT)
        case _:
            raise ValueError(
                f"Unknown component type {component_type_for_attention} in TdbRequestSpec"
            )
    return dsts
def nodes_to_trace_to_trace_config(
    upstream_node_to_trace: NodeToTrace,
    downstream_node_to_trace: NodeToTrace | None,
) -> tuple[TraceConfig, int | None]:
    node_index = upstream_node_to_trace.node_index
    attention_trace_type = upstream_node_to_trace.attention_trace_type
    if downstream_node_to_trace is None:
        downstream_trace_config = None
    else:
        # only trace through V admits a downstream node to trace
        assert attention_trace_type == AttentionTraceType.V
        # don't assign downstream trace_token_index to a variable, as it's not used
        downstream_trace_config, _ = nodes_to_trace_to_trace_config(
            downstream_node_to_trace,
            None,  # treat the downstream node as the "upstream" node to trace
        )  # NOTE: downstream node must not trace through V
    trace_token_index = node_index.tensor_indices[0]
    return (
        TraceConfig(
            node_index=node_index.to_node_index(),
            pre_or_post_act=PreOrPostAct.PRE,
            attention_trace_type=attention_trace_type,
            downstream_trace_config=downstream_trace_config,
            detach_layer_norm_scale=DETACH_LAYER_NORM_SCALE,
        ),
        trace_token_index,
    )
def named_attention_head_indices(node_index: MirroredNodeIndex) -> tuple[int, int, int]:
    if node_index.node_type != NodeType.ATTENTION_HEAD:
        raise ValueError("Incorrect nodeType for namedAttentionHeadIndices function")
    (
        attended_from_token_index,
        attended_to_token_index,
        attention_head_index,
    ) = node_index.tensor_indices
    return (
        assert_non_none(attended_from_token_index),
        assert_non_none(attended_to_token_index),
        assert_non_none(attention_head_index),
    )

================
File: neuron_explainer/activations/activation_records.py
================
"""Utilities for formatting activation records into prompts."""
import math
from typing import Any, Sequence
from neuron_explainer.activations.activations import ActivationRecord
UNKNOWN_ACTIVATION_STRING = "unknown"
def relu(x: float) -> float:
    return max(0.0, x)
def calculate_max_activation(activation_records: Sequence[ActivationRecord]) -> float:
    """Return the maximum activation value of the neuron across all the activation records."""
    flattened = [
        # Relu is used to assume any values less than 0 are indicating the neuron is in the resting
        # state.
        max(relu(x) for x in activation_record.activations)
        for activation_record in activation_records
    ]
    return max(flattened)
def normalize_activations(activation_record: list[float], max_activation: float) -> list[int]:
    """Convert raw neuron activations to integers on the range [0, 10]."""
    if max_activation <= 0:
        # raise ValueError(f"max_activation must be positive, got {max_activation}")
        return [
            0 for x in activation_record
        ]  # commented the above line to allow GPT2 datasets to render (Dan)
    # Relu is used to assume any values less than 0 are indicating the neuron is in the resting
    # state.
    return [min(10, math.floor(10 * relu(x) / max_activation)) for x in activation_record]
def normalize_activations_symmetric(
    activation_record: list[float], max_activation: float
) -> list[int]:
    """Convert raw neuron activations to integers on the range [-10, 10]."""
    max_abs_activation = (
        max_activation  # clients expect kwarg "max_activation", so leaving this for now
    )
    if max_abs_activation == 0.0:
        # raise ValueError(f"max_activation must be positive, got {max_activation}")
        return [0 for x in activation_record]
    # Unlike normalize_activations, this function doesn't apply relu, since we want to show negative
    # activations as well.
    return [max(min(10, math.trunc(10 * x / max_abs_activation)), -10) for x in activation_record]
def truncate_negative_activations(activation_record: ActivationRecord) -> ActivationRecord:
    """Truncate activations to 0 if they are negative."""
    return ActivationRecord(
        tokens=activation_record.tokens,
        activations=[max(0, x) for x in activation_record.activations],
    )
def truncate_negative_activations_list(
    activation_records: Sequence[ActivationRecord],
) -> list[ActivationRecord]:
    """Truncate activations to 0 if they are negative."""
    return [truncate_negative_activations(x) for x in activation_records]
def _format_activation_record(
    activation_record: ActivationRecord,
    max_activation: float,
    omit_zeros: bool,
    hide_activations: bool = False,
    start_index: int = 0,
) -> str:
    """Format neuron activations into a string, suitable for use in prompts."""
    tokens = activation_record.tokens
    normalized_activations = normalize_activations(activation_record.activations, max_activation)
    if omit_zeros:
        assert (not hide_activations) and start_index == 0, "Can't hide activations and omit zeros"
        tokens = [
            token for token, activation in zip(tokens, normalized_activations) if activation > 0
        ]
        normalized_activations = [x for x in normalized_activations if x > 0]
    entries = []
    assert len(tokens) == len(normalized_activations)
    for index, token, activation in zip(range(len(tokens)), tokens, normalized_activations):
        activation_string = str(int(activation))
        if hide_activations or index < start_index:
            activation_string = UNKNOWN_ACTIVATION_STRING
        entries.append(f"{token}\t{activation_string}")
    return "\n".join(entries)
def format_activation_records(
    activation_records: Sequence[ActivationRecord],
    max_activation: float,
    *,
    omit_zeros: bool = False,
    start_indices: list[int] | None = None,
    hide_activations: bool = False,
) -> str:
    """Format a list of activation records into a string."""
    return (
        "\n<start>\n"
        + "\n<end>\n<start>\n".join(
            [
                _format_activation_record(
                    activation_record,
                    max_activation,
                    omit_zeros=omit_zeros,
                    hide_activations=hide_activations,
                    start_index=0 if start_indices is None else start_indices[i],
                )
                for i, activation_record in enumerate(activation_records)
            ]
        )
        + "\n<end>\n"
    )
def _format_tokens_for_simulation(tokens: Sequence[str]) -> str:
    """
    Format tokens into a string with each token marked as having an "unknown" activation, suitable
    for use in prompts.
    """
    entries = []
    for token in tokens:
        entries.append(f"{token}\t{UNKNOWN_ACTIVATION_STRING}")
    return "\n".join(entries)
def format_sequences_for_simulation(
    all_tokens: Sequence[Sequence[str]],
) -> str:
    """
    Format a list of lists of tokens into a string with each token marked as having an "unknown"
    activation, suitable for use in prompts.
    """
    return (
        "\n<start>\n"
        + "\n<end>\n<start>\n".join(
            [_format_tokens_for_simulation(tokens) for tokens in all_tokens]
        )
        + "\n<end>\n"
    )
def non_zero_activation_proportion(
    activation_records: Sequence[ActivationRecord], max_activation: float
) -> float:
    """Return the proportion of activation values that aren't zero."""
    total_activations_count = sum(
        [len(activation_record.activations) for activation_record in activation_records]
    )
    normalized_activations = [
        normalize_activations(activation_record.activations, max_activation)
        for activation_record in activation_records
    ]
    non_zero_activations_count = sum(
        [len([x for x in activations if x != 0]) for activations in normalized_activations]
    )
    return non_zero_activations_count / total_activations_count
def get_attribute_or_key(activation_record: Any, attribute_name: str) -> Any:
    if isinstance(activation_record, dict):
        assert attribute_name in activation_record, f"{attribute_name} not in activation_record"
        return activation_record[attribute_name]
    else:
        assert hasattr(activation_record, attribute_name)
        return getattr(activation_record, attribute_name)

================
File: neuron_explainer/activations/activations.py
================
# Dataclasses and enums for storing neuron-indexed information about activations. Also, related
# helper functions.
import math
import os.path as osp
from dataclasses import dataclass, field
from neuron_explainer.fast_dataclasses import FastDataclass, loads, register_dataclass
from neuron_explainer.file_utils import CustomFileHandler, file_exists, read_single_async
@register_dataclass
@dataclass
class ActivationRecord(FastDataclass):
    """Collated lists of tokens and their activations for a single neuron."""
    activations: list[float]
    """Raw activation values for the neuron on each token in the text sequence."""
    tokens: list[str]
    """Tokens in the text sequence, represented as strings."""
@register_dataclass
@dataclass
class NeuronId(FastDataclass):
    """Identifier for a neuron in an artificial neural network."""
    neuron_index: int
    """The neuron's index within in its layer. Indices start from 0 in each layer."""
    layer_index: int
    """The index of layer the neuron is in. The first layer used during inference has index 0."""
    # TODO(dan): add a derived scalar type field, to allow for different types of 'nodes' (e.g. attention heads)
    # and change the name of this class to something more general, like NodeId (whatever is decided on).
NeuronId.field_renamed("idx", "neuron_index")
NeuronId.field_renamed("layer", "layer_index")
def _check_slices(
    slices_by_split: dict[str, slice],
    expected_num_values: int,
) -> None:
    """Assert that the slices are disjoint and fully cover the intended range."""
    indices = set()
    sum_of_slice_lengths = 0
    n_splits = len(slices_by_split.keys())
    for s in slices_by_split.values():
        subrange = range(expected_num_values)[s]
        sum_of_slice_lengths += len(subrange)
        indices |= set(subrange)
    assert (
        sum_of_slice_lengths == expected_num_values
    ), f"{sum_of_slice_lengths=} != {expected_num_values=}"
    stride = n_splits
    expected_indices = set.union(
        *[set(range(start_index, expected_num_values, stride)) for start_index in range(n_splits)]
    )
    assert indices == expected_indices, f"{indices=} != {expected_indices=}"
def get_slices_for_splits(
    splits: list[str],
    num_activation_records_per_split: int,
) -> dict[str, slice]:
    """
    Get equal-sized interleaved subsets for each of a list of splits, given the number of elements
    to include in each split.
    """
    stride = len(splits)
    num_activation_records_for_even_splits = num_activation_records_per_split * stride
    slices_by_split = {
        split: slice(split_index, num_activation_records_for_even_splits, stride)
        for split_index, split in enumerate(splits)
    }
    _check_slices(
        slices_by_split=slices_by_split,
        expected_num_values=num_activation_records_for_even_splits,
    )
    return slices_by_split
@dataclass
class ActivationRecordSliceParams:
    """How to select splits (train, valid, etc.) of activation records."""
    n_examples_per_split: int | None
    """The number of examples to include in each split."""
@register_dataclass
@dataclass
class NeuronRecord(FastDataclass):
    """Neuron-indexed activation data, including summary stats and notable activation records."""
    neuron_id: NeuronId
    """Identifier for the neuron."""
    random_sample: list[ActivationRecord] = field(default_factory=list)
    """
    Random activation records for this neuron. The random sample is independent from those used for
    other neurons.
    """
    random_sample_by_quantile: list[list[ActivationRecord]] | None = None
    """
    Random samples of activation records in each of the specified quantiles. None if quantile
    tracking is disabled.
    """
    quantile_boundaries: list[float] | None = None
    """Boundaries of the quantiles used to generate the random_sample_by_quantile field."""
    # Moments of activations
    mean: float | None = math.nan
    variance: float | None = math.nan
    skewness: float | None = math.nan
    kurtosis: float | None = math.nan
    most_positive_activation_records: list[ActivationRecord] = field(default_factory=list)
    """
    Activation records with the most positive figure of merit value for this neuron over all dataset
    examples.
    """
    @property
    def max_activation(self) -> float:
        """Return the maximum activation value over all top-activating activation records."""
        return max([max(ar.activations) for ar in self.most_positive_activation_records])
    def _get_top_activation_slices(
        self, activation_record_slice_params: ActivationRecordSliceParams
    ) -> dict[str, slice]:
        splits = ["train", "calibration", "valid", "test"]
        n_examples_per_split = activation_record_slice_params.n_examples_per_split
        if n_examples_per_split is None:
            n_examples_per_split = len(self.most_positive_activation_records) // len(splits)
        assert len(self.most_positive_activation_records) >= n_examples_per_split * len(splits)
        return get_slices_for_splits(splits, n_examples_per_split)
    def _get_random_activation_slices(
        self, activation_record_slice_params: ActivationRecordSliceParams
    ) -> dict[str, slice]:
        splits = ["calibration", "valid", "test"]
        n_examples_per_split = activation_record_slice_params.n_examples_per_split
        if n_examples_per_split is None:
            n_examples_per_split = len(self.random_sample) // len(splits)
        assert len(self.random_sample) >= n_examples_per_split * len(splits)
        return get_slices_for_splits(splits, n_examples_per_split)
    def train_activation_records(
        self,
        activation_record_slice_params: ActivationRecordSliceParams,
    ) -> list[ActivationRecord]:
        """
        Train split, typically used for generating explanations. Consists exclusively of
        top-activating records since context window limitations make it difficult to include
        random records.
        """
        return self.most_positive_activation_records[
            self._get_top_activation_slices(activation_record_slice_params)["train"]
        ]
    def calibration_activation_records(
        self,
        activation_record_slice_params: ActivationRecordSliceParams,
    ) -> list[ActivationRecord]:
        """
        Calibration split, typically used for calibrating neuron simulations. Consists of
        top-activating records and random records in a 1:1 ratio.
        """
        return (
            self.most_positive_activation_records[
                self._get_top_activation_slices(activation_record_slice_params)["calibration"]
            ]
            + self.random_sample[
                self._get_random_activation_slices(activation_record_slice_params)["calibration"]
            ]
        )
    def valid_activation_records(
        self,
        activation_record_slice_params: ActivationRecordSliceParams,
    ) -> list[ActivationRecord]:
        """
        Validation split, typically used for evaluating explanations, either automatically with
        simulation + correlation coefficient scoring, or manually by humans. Consists of
        top-activating records and random records in a 1:1 ratio.
        """
        return (
            self.most_positive_activation_records[
                self._get_top_activation_slices(activation_record_slice_params)["valid"]
            ]
            + self.random_sample[
                self._get_random_activation_slices(activation_record_slice_params)["valid"]
            ]
        )
    def test_activation_records(
        self,
        activation_record_slice_params: ActivationRecordSliceParams,
    ) -> list[ActivationRecord]:
        """
        Test split, typically used for explanation evaluations that can't use the validation split.
        Consists of top-activating records and random records in a 1:1 ratio.
        """
        return (
            self.most_positive_activation_records[
                self._get_top_activation_slices(activation_record_slice_params)["test"]
            ]
            + self.random_sample[
                self._get_random_activation_slices(activation_record_slice_params)["test"]
            ]
        )
def neuron_exists(dataset_path: str, layer: str | int, neuron: str | int) -> bool:
    """Return whether the specified neuron exists."""
    file = osp.join(dataset_path, str(layer), f"{neuron}.json")
    return file_exists(file)
def load_neuron(dataset_path: str, layer: str | int, neuron: str | int) -> NeuronRecord:
    """Load the NeuronRecord for the specified neuron."""
    file = osp.join(dataset_path, str(layer), f"{neuron}.json")
    with CustomFileHandler(file, "r") as f:
        neuron_record = loads(f.read(), backwards_compatible=True)
        neuron_record.neuron_id.layer_index = int(
            neuron_record.neuron_id.layer_index
        )  # in case it was a string
        if not isinstance(neuron_record, NeuronRecord):
            raise ValueError(
                "Stored data incompatible with current version of NeuronRecord dataclass."
            )
        return neuron_record
async def load_neuron_async(dataset_path: str, layer: str | int, neuron: str | int) -> NeuronRecord:
    """Async version of load_neuron."""
    file = osp.join(dataset_path, str(layer), f"{neuron}.json")
    return await read_neuron_file(file)
async def read_neuron_file(neuron_filename: str) -> NeuronRecord:
    """Like load_neuron_async, but takes a raw neuron filename."""
    raw_contents = await read_single_async(neuron_filename)
    neuron_record = loads(raw_contents.decode("utf-8"), backwards_compatible=True)
    if not isinstance(neuron_record, NeuronRecord):
        raise ValueError("Stored data incompatible with current version of NeuronRecord dataclass.")
    return neuron_record

================
File: neuron_explainer/activations/attention_utils.py
================
"""
Contains math utilities for converting from flattened representations of attention activations
(which are a scalar per token pair) to nested lists. The inner lists are attention activations
related to attention from the same token (to different tokens).
Tested in ./test_attention_utils.py.
"""
import math
import numpy as np
def _inverse_triangular_number(n: int) -> int:
    # the m'th triangular number t_m satisfies t_m = m(m+1)/2
    # this function asserts that n is a triangular number, and returns the unique m such that t_m = n
    # this is used to infer the number of sequence tokens from the number of activations
    assert n >= 0
    m: int = (
        math.floor(math.sqrt(1 + 8 * n)) - 1
    ) // 2  # from quadratic formula applied to m(m+1)/2 = n
    assert m * (m + 1) // 2 == n
    return m
def get_max_num_attended_to_sequence_tokens(num_sequence_tokens: int, num_activations: int) -> int:
    # Attended to sequences are assumed to increase in length up to a maximum length, and then stay at that
    # length for the remainder of the sequence. The maximum attended to sequence length is at most equal to the sequence length,
    # but is permitted to be less
    num_sequence_token_pairs = num_sequence_tokens * (num_sequence_tokens + 1) // 2
    if num_activations == num_sequence_token_pairs:
        # the maximum attended to sequence length is equal to the sequence length
        return num_sequence_tokens
    else:
        # the maximum attended to sequence length is less than the sequence length, and
        assert num_activations < num_sequence_token_pairs
        num_missing_activations = num_sequence_token_pairs - num_activations
        num_missing_sequence_tokens = _inverse_triangular_number(num_missing_activations)
        max_num_attended_to_sequence_tokens = num_sequence_tokens - num_missing_sequence_tokens
        assert max_num_attended_to_sequence_tokens > 0
        return max_num_attended_to_sequence_tokens
def get_attended_to_sequence_length_per_sequence_token(
    num_sequence_tokens: int, max_num_attended_to_sequence_tokens: int
) -> list[int]:
    # given a num_sequence_tokens and a max_num_attended_to_sequence_tokens, return a list of length num_sequence_tokens
    # where the ith element is the length of the attended to sequence for the ith sequence token.
    # The length of the attended to sequence starts at 1, increases up to max_num_attended_to_sequence_tokens, by 1 with each
    # token, and then stays at max_num_attended_to_sequence_tokens for the remainder of the sequence
    assert num_sequence_tokens >= max_num_attended_to_sequence_tokens
    attended_to_sequence_lengths = list(range(1, max_num_attended_to_sequence_tokens + 1))
    if num_sequence_tokens > max_num_attended_to_sequence_tokens:
        attended_to_sequence_lengths.extend(
            [
                max_num_attended_to_sequence_tokens
                for _ in range(num_sequence_tokens - max_num_attended_to_sequence_tokens)
            ]
        )
    return attended_to_sequence_lengths
def get_attended_to_sequence_lengths(num_sequence_tokens: int, num_activations: int) -> list[int]:
    max_num_attended_to_sequence_tokens = get_max_num_attended_to_sequence_tokens(
        num_sequence_tokens, num_activations
    )
    return get_attended_to_sequence_length_per_sequence_token(
        num_sequence_tokens, max_num_attended_to_sequence_tokens
    )
def _convert_flattened_index_to_unflattened_index_assuming_square_matrix(
    flat_index: int,
) -> tuple[int, int]:
    # this con
    n = math.floor((-1 + math.sqrt(1 + 8 * flat_index)) / 2)
    m = flat_index - n * (n + 1) // 2
    return n, m
def convert_flattened_index_to_unflattened_index(
    flattened_index: int,
    num_sequence_tokens: int | None = None,
    num_activations: int | None = None,
) -> tuple[int, int]:
    # given a flattened index, return the unflattened index
    # if the attention matrix is square (most common), then the flattened_index uniquely determines the index within the square matrix
    # if the attention matrix has more rows (sequence tokens) than columns (attended-to sequence tokens), then num_sequence_tokens
    # and num_activations are required to determine the index within the matrix
    # specify both num_sequence_tokens and num_activations, or neither
    assert not (num_sequence_tokens is None) ^ (num_activations is None)
    if (
        num_sequence_tokens is None
        or num_activations == num_sequence_tokens * (num_sequence_tokens + 1) // 2
    ):
        assume_square_matrix = True
    else:
        assume_square_matrix = False
    if assume_square_matrix:
        return _convert_flattened_index_to_unflattened_index_assuming_square_matrix(flattened_index)
    else:
        assert num_sequence_tokens is not None
        assert num_activations is not None
        assert flattened_index < num_activations
        sequence_lengths = get_attended_to_sequence_lengths(num_sequence_tokens, num_activations)
        sequence_lengths_cumsum = np.cumsum([0] + sequence_lengths)
        sequence_index = int(
            np.searchsorted(sequence_lengths_cumsum, flattened_index, side="right") - 1
        )
        assert sequence_lengths_cumsum[sequence_index] <= flattened_index, (
            sequence_lengths_cumsum[sequence_index],
            flattened_index,
        )
        assert sequence_lengths_cumsum[sequence_index + 1] >= flattened_index, (
            sequence_lengths_cumsum[sequence_index + 1],
            flattened_index,
        )
        index_within_sequence = flattened_index - sequence_lengths_cumsum[sequence_index]
        return sequence_index, index_within_sequence

================
File: neuron_explainer/activations/derived_scalars/__init__.py
================
from .derived_scalar_types import DerivedScalarType

================
File: neuron_explainer/activations/derived_scalars/activations_and_metadata.py
================
"""
RawActivationStore collects raw activations in ActivationsAndMetadata objects, associated with the
location type and pass type of that raw activation.
"""
import dataclasses
from dataclasses import dataclass
from typing import Any, Callable, Literal
import numpy as np
import torch
from neuron_explainer.activations.derived_scalars.derived_scalar_types import DerivedScalarType
from neuron_explainer.activations.derived_scalars.indexing import DerivedScalarIndex
from neuron_explainer.activations.derived_scalars.locations import LayerIndexer
from neuron_explainer.models.model_component_registry import (
    ActivationLocationType,
    ActivationLocationTypeAndPassType,
    LayerIndex,
    PassType,
)
LayerwiseTransformFn = Callable[[torch.Tensor, LayerIndex], torch.Tensor]
MultiArgLayerwiseTransformFn = Callable[..., torch.Tensor]
@dataclass(frozen=True)
class ActivationsAndMetadata:
    """Contains data about the internal state of the network during inference, indexed by layer."""
    dst: DerivedScalarType
    pass_type: PassType
    activations_by_layer_index: dict[LayerIndex, torch.Tensor]  # layer index is None iff
    def __post_init__(self) -> None:
        # assert that all the devices are the same or None
        device = self.device
        if len(self.activations_by_layer_index) > 0:
            for activations in self.activations_by_layer_index.values():
                if activations.device is not None:
                    assert activations.device == device
    @property
    def layer_indices(self) -> list[LayerIndex]:
        return safe_sorted(list(self.activations_by_layer_index.keys()))
    @property
    def device(self) -> torch.device | None:
        if len(self.activations_by_layer_index) > 0:
            return next(iter(self.activations_by_layer_index.values())).device
        else:
            # if there are no activations in this object (e.g. if performing a backward pass
            # from a layer 0 attention head, and examining MLP activations) then there are no
            # tensors and thus no devices
            return None
    def cpu(self) -> "ActivationsAndMetadata":
        """Move the activations tensors to cpu."""
        return dataclasses.replace(
            self,
            activations_by_layer_index=_move_tensor_to_cpu_by_layer_index(
                self.activations_by_layer_index
            ),
        )
    def clone(self) -> "ActivationsAndMetadata":
        """Clone the activations tensors."""
        return dataclasses.replace(
            self,
            activations_by_layer_index=_clone_tensor_by_layer_index(
                self.activations_by_layer_index
            ),
        )
    def _remap_layer_indices(
        self, source_layer_index_by_layer_index: dict[LayerIndex, LayerIndex | Literal["Dummy"]]
    ) -> "ActivationsAndMetadata":
        """ending_layer_indices specifies the new layer_index to which each layer_index in self.layer_indices is to be assigned,
        The resulting ActivationsAndMetadata object will have the same layer_indices as the starting object, with undefined layer_indices filled in with
        dummy (empty) tensors.
        See the docstring of apply_layer_indexer for more information.
        """
        activations_by_layer_index = _remap_tensor_by_layer_index(
            self.activations_by_layer_index, source_layer_index_by_layer_index
        )
        return dataclasses.replace(
            self,
            activations_by_layer_index=activations_by_layer_index,
        )
    def apply_layer_indexer(
        self,
        layer_indexer: LayerIndexer,
        desired_layer_indices: (
            list[LayerIndex] | None
        ) = None,  # indicates layer indices to keep; None indicates keep all
    ) -> "ActivationsAndMetadata":
        """DSTs can require an activation from an arbitrary layer (e.g. the n'th layer) and an activation from a different (e.g. constant or offset) layer
        (e.g. the final layer, or the previous layer); for example, MLP activations and the gradient at the final residual stream
        location. The LayerIndexer specifies how to map the layer indices of the activations_by_layer_index in e.g. the residual stream gradient ActivationsAndMetadata object
        to the layer indices of the activations_by_layer_index in the WriteToFinalResidual derived scalar.
        If desired_layer_indices is not None, then the resulting object will have only the layer indices in desired_layer_indices.
        If desired_layer_indices is None, then the resulting object will have the same layer indices as the starting object.
        """
        if desired_layer_indices is None:
            desired_layer_indices = self.layer_indices
        layer_index_source_list = layer_indexer(desired_layer_indices)
        assert len(layer_index_source_list) == len(desired_layer_indices)
        layer_index_source_by_ending_layer_index = dict(
            zip(desired_layer_indices, layer_index_source_list)
        )
        return self._remap_layer_indices(layer_index_source_by_ending_layer_index)
    def apply_transform_fn_to_activations(
        self,
        transform_fn: Callable[[torch.Tensor], torch.Tensor],
        output_dst: DerivedScalarType,
        output_pass_type: PassType,
    ) -> "ActivationsAndMetadata":
        """Convenience method to apply the same function to activations_by_layer_index at every
        layer index."""
        def transform_fn_with_unused_layer_index(
            activations: torch.Tensor, _: LayerIndex
        ) -> torch.Tensor:
            return transform_fn(activations)
        return self.apply_layerwise_transform_fn_to_activations(
            transform_fn_with_unused_layer_index,
            output_dst=output_dst,
            output_pass_type=output_pass_type,
        )
    def apply_transform_fn_to_multiple_activations(
        self,
        transform_fn: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],
        others: tuple["ActivationsAndMetadata", ...],
        output_dst: DerivedScalarType,
        output_pass_type: PassType,
    ) -> "ActivationsAndMetadata":
        """Same as above, but transform_fn takes a second tensor as an argument.
        Its activations_by_layer_index tensors are used as that second argument."""
        def transform_fn_with_unused_layer_index(
            activations: torch.Tensor,
            *other_activations: torch.Tensor,
            layer_index: LayerIndex,
        ) -> torch.Tensor:
            return transform_fn(activations, *other_activations)
        return self.apply_layerwise_transform_fn_to_multiple_activations(
            transform_fn_with_unused_layer_index,
            others=others,
            output_dst=output_dst,
            output_pass_type=output_pass_type,
        )
    # TODO: this function should take transform_fn_with_layer_index, a Callable[[torch.Tensor, LayerIndex], torch.Tensor]
    def apply_layerwise_transform_fn_to_activations(
        self,
        layerwise_transform_fn: LayerwiseTransformFn,
        output_dst: DerivedScalarType,
        output_pass_type: PassType,
    ) -> "ActivationsAndMetadata":
        """layerwise_transform_fn is a Callable[[torch.Tensor, LayerIndex], torch.Tensor]. This function applies the transform_fn
        to the activations_by_layer_index at each layer index, and returns a new ActivationsAndMetadata
        object with the transformed activations."""
        def _layerwise_transform_activations_by_layer_index(
            activations_by_layer_index: dict[LayerIndex, torch.Tensor],
            layerwise_transform_fn: LayerwiseTransformFn,
        ) -> dict[LayerIndex, torch.Tensor]:
            return {
                layer_index: layerwise_transform_fn(activations, layer_index)
                for layer_index, activations in activations_by_layer_index.items()
            }
        return dataclasses.replace(
            self,
            activations_by_layer_index=_layerwise_transform_activations_by_layer_index(
                self.activations_by_layer_index,
                layerwise_transform_fn=layerwise_transform_fn,
            ),
            dst=output_dst,
            pass_type=output_pass_type,
        )
    def apply_layerwise_transform_fn_to_multiple_activations(
        self,
        layerwise_transform_fn: MultiArgLayerwiseTransformFn,  # input to Callable is N tensors
        # this must take a layer_index kwarg, to distinguish it from the arbitrary number of tensor args
        others: tuple["ActivationsAndMetadata", ...],  # N - 1 entries
        output_dst: DerivedScalarType,
        output_pass_type: PassType,
    ) -> "ActivationsAndMetadata":
        """Same as above, but the transform_fn takes
        two tensors as arguments, and the activations_by_layer_index tensors from 'other' are used
        as the second argument."""
        def _layerwise_transform_multiple_activations_by_layer_index(
            activations_by_layer_index: dict[LayerIndex, torch.Tensor],
            layerwise_transform_fn: MultiArgLayerwiseTransformFn,
            other_activations_by_layer_index_tuple: tuple[dict[LayerIndex, torch.Tensor], ...],
        ) -> dict[LayerIndex, torch.Tensor]:
            for other_activations_by_layer_index in other_activations_by_layer_index_tuple:
                assert set(activations_by_layer_index.keys()) == set(
                    other_activations_by_layer_index.keys()
                ), (
                    f"{activations_by_layer_index.keys()=} , "
                    f"{other_activations_by_layer_index.keys()=}"
                )
            return {
                layer_index: layerwise_transform_fn(
                    activations,
                    *[
                        other_activations_by_layer_index[layer_index]
                        for other_activations_by_layer_index in other_activations_by_layer_index_tuple
                    ],
                    layer_index=layer_index,
                )
                for layer_index, activations in activations_by_layer_index.items()
            }
        self_activations_by_layer_index = self.activations_by_layer_index
        other_activations_by_layer_index_tuple = tuple(
            other.activations_by_layer_index for other in others
        )
        for other_activations_by_layer_index in other_activations_by_layer_index_tuple:
            assert set(other_activations_by_layer_index.keys()) == set(
                self_activations_by_layer_index.keys()
            ), (
                f"{other_activations_by_layer_index.keys()=} , "
                f"{self_activations_by_layer_index.keys()=}"
            )
        return dataclasses.replace(
            self,
            activations_by_layer_index=_layerwise_transform_multiple_activations_by_layer_index(
                self_activations_by_layer_index,
                layerwise_transform_fn=layerwise_transform_fn,
                other_activations_by_layer_index_tuple=other_activations_by_layer_index_tuple,
            ),
            dst=output_dst,
            pass_type=output_pass_type,
        )
    def filter_layers(self, layer_indices: list[int] | None) -> "ActivationsAndMetadata":
        """Returns a new ActivationsAndMetadata object with only the specified layer indices."""
        if layer_indices is None:
            return self
        else:
            return dataclasses.replace(
                self,
                activations_by_layer_index={
                    layer_index: activations
                    for layer_index, activations in self.activations_by_layer_index.items()
                    if layer_index in layer_indices or layer_index is None
                },
            )
    @property
    def shape(self) -> tuple[int, ...]:
        first_value = next(iter(self.activations_by_layer_index.values()))
        shape = first_value.shape
        return tuple(shape)
    def __eq__(self, other: Any) -> bool:
        """
        Note that this uses torch.allclose, rather than checking for precise equality.
        This permits ActivationsAndMetadata to be "equal" while having different dst
        and pass type. This is useful for situations where we want to compare two derived scalars
        that should be the same but that are computed in different ways
        """
        if not isinstance(other, ActivationsAndMetadata):
            return False
        def check_activations_by_layer_index_equality(
            self_activations_by_layer_index: dict[LayerIndex, torch.Tensor],
            other_activations_by_layer_index: dict[LayerIndex, torch.Tensor],
        ) -> bool:
            # check indices
            if set(self_activations_by_layer_index.keys()) != set(
                other_activations_by_layer_index.keys()
            ):
                return False
            # check shapes and then values
            for layer_index in self_activations_by_layer_index.keys():
                if (
                    self_activations_by_layer_index[layer_index].shape
                    != other_activations_by_layer_index[layer_index].shape
                ):
                    return False
                if not torch.allclose(
                    self_activations_by_layer_index[layer_index],
                    other_activations_by_layer_index[layer_index],
                ):
                    return False
            return True
        if not check_activations_by_layer_index_equality(
            self.activations_by_layer_index, other.activations_by_layer_index
        ):
            return False
        return True
    def __add__(self, other: "ActivationsAndMetadata") -> "ActivationsAndMetadata":
        def add_fn(*args: torch.Tensor) -> torch.Tensor:
            return torch.sum(torch.stack(args), dim=0)
        return self.apply_transform_fn_to_multiple_activations(
            add_fn, (other,), output_dst=self.dst, output_pass_type=self.pass_type
        )
    def __sub__(self, other: "ActivationsAndMetadata") -> "ActivationsAndMetadata":
        def sub_fn(*args: torch.Tensor) -> torch.Tensor:
            return torch.sub(args[0], args[1])
        return self.apply_transform_fn_to_multiple_activations(
            sub_fn, (other,), output_dst=self.dst, output_pass_type=self.pass_type
        )
    def max(self) -> tuple[torch.Tensor, DerivedScalarIndex]:
        values, indices = self.topk(1, largest=True)
        return values[0], indices[0]
    def sum(self) -> torch.Tensor:
        return torch.sum(torch.stack(list(self.activations_by_layer_index.values())))
    def sum_abs(self) -> torch.Tensor:
        return torch.sum(torch.abs(torch.stack(list(self.activations_by_layer_index.values()))))
    def topk(self, k: int, largest: bool) -> tuple[torch.Tensor, list[DerivedScalarIndex]]:
        # this first computes topk values and indices for each layer, then stacks them and computes topk values and indices
        # the topk for the overall stack. This avoids instantiating a second copy of all the data
        # in self.activations_by_layer_index
        if k > self.numel():
            k = self.numel()  # if k > numel is requested, return everything
        def get_topk_indices(activations: torch.Tensor) -> torch.Tensor:
            if k >= activations.numel():
                return torch.argsort(activations.flatten(), descending=largest)
            else:
                _, indices = torch.topk(activations.flatten(), k, largest=largest)
            return indices
        def get_topk_values(
            activations: torch.Tensor, indices: torch.Tensor, layer_index: LayerIndex
        ) -> torch.Tensor:
            # layer_index is unused, but required as a keyword argument
            return torch.gather(activations.flatten(), 0, indices)
        topk_indices = self.apply_transform_fn_to_activations(
            get_topk_indices, output_dst=self.dst, output_pass_type=self.pass_type
        )
        topk_values = self.apply_layerwise_transform_fn_to_multiple_activations(
            get_topk_values, (topk_indices,), output_dst=self.dst, output_pass_type=self.pass_type
        )
        topk_values_list = []
        for layer_index in self.layer_indices:
            topk_values_list.append(topk_values.activations_by_layer_index[layer_index])
        stacked_topk_values = torch.stack(topk_values_list)
        overall_topk_values, overall_topk_indices = torch.topk(
            stacked_topk_values.flatten(), k, largest=largest
        )
        overall_topk_layer_index_indices, overall_topk_topk_indices = np.unravel_index(
            overall_topk_indices.cpu().numpy(), stacked_topk_values.shape
        )
        overall_topk_layer_indices = [
            self.layer_indices[i] for i in overall_topk_layer_index_indices
        ]
        overall_topk_ds_indices = [
            DerivedScalarIndex(
                dst=self.dst,
                pass_type=self.pass_type,
                layer_index=layer_index,
                tensor_indices=tuple(
                    int(x)
                    for x in np.unravel_index(
                        int(
                            topk_indices.activations_by_layer_index[layer_index][
                                overall_topk_topk_indices[i]
                            ].item()
                        ),
                        self.activations_by_layer_index[layer_index].shape,
                    )
                ),  # cast from np.int64 to int
            )
            for i, layer_index in enumerate(overall_topk_layer_indices)
        ]
        return overall_topk_values, overall_topk_ds_indices
    def numel(self) -> int:
        return sum(activations.numel() for activations in self.activations_by_layer_index.values())
def _fill_in_activations_by_layer_index_at_layer_indices(
    activations_by_layer_index: dict[LayerIndex, torch.Tensor], layer_indices: set[LayerIndex]
) -> dict[LayerIndex, torch.Tensor]:
    """
    some activations might not be filled in to start, e.g. if they are from a layer after the point
    from which a backward pass was computed. In this case, missing values in activations_by_layer_index.values()
    are filled in to be zero tensors. This is well motivated because dUpstreamActivation/dDownstreamActivation is
    in fact 0.
    """
    if None in activations_by_layer_index:
        assert len(activations_by_layer_index) == 1
        default_tensor = activations_by_layer_index.pop(None)
    else:
        example_tensor = next(iter(activations_by_layer_index.values()))
        default_tensor = torch.zeros_like(example_tensor)
    for layer_index in layer_indices:
        if layer_index not in activations_by_layer_index:
            activations_by_layer_index[layer_index] = default_tensor
    return activations_by_layer_index
def _remap_tensor_by_layer_index(
    starting_tensor_by_layer_index: dict[LayerIndex, torch.Tensor],
    source_layer_index_by_layer_index: dict[LayerIndex, LayerIndex | Literal["Dummy"]],
) -> dict[LayerIndex, torch.Tensor]:
    # TODO: clarify comment to indicate "Dummy" tensors can be either "truly 0" tensors, as in the case of backward passes, or "invalid"
    # tensors, as in the case of referring to a layer that does not exist.
    """source_layer_index_by_layer_index specifies the starting layer_index (value) from which each new layer_index (key) should get its tensor,
    The resulting dict of tensors will have the same layer_indices as the starting dict, with undefined layer_indices filled in with
    zero tensors. Used when shifting layer indices of ActivationsAndMetadata objects, prior to applying some downstream transformation to them.
    "Dummy" is used to indicate that the corresponding activations are somehow 'invalid', e.g. coming from a previous layer when there is no
    previous layer.
    Callers will look for all the same activations in each layer, even though some activations in some layers will not be used.
    Therefore, we use dummy activations to pass to those callers, with the floats within those activations not affecting the output of
    the callers."""
    if len(starting_tensor_by_layer_index) == 0:
        return {
            layer_index: torch.tensor(0.0)
            for layer_index in source_layer_index_by_layer_index.keys()
        }
    example_tensor = next(iter(starting_tensor_by_layer_index.values()))
    device = example_tensor.device
    dummy_tensor = torch.tensor(0.0, device=device)  # maybe change to have shape of example_tensor
    # fill in missing starting indices with default values (empty tensors)
    non_dummy_source_layer_indices = [
        source_layer_index
        for source_layer_index in list(source_layer_index_by_layer_index.values())
        if source_layer_index != "Dummy"
    ]
    starting_tensor_by_layer_index = _fill_in_activations_by_layer_index_at_layer_indices(
        starting_tensor_by_layer_index, set(non_dummy_source_layer_indices)
    )
    def _get_value_or_dummy(
        starting_tensor_by_layer_index: dict[LayerIndex, torch.Tensor],
        source_layer_index_or_dummy: LayerIndex | Literal["Dummy"],
    ) -> torch.Tensor:
        if source_layer_index_or_dummy == "Dummy":
            return dummy_tensor
        else:
            assert (
                source_layer_index_or_dummy in starting_tensor_by_layer_index
            ), f"{source_layer_index_or_dummy=} , {starting_tensor_by_layer_index.keys()=}"
            return starting_tensor_by_layer_index[source_layer_index_or_dummy]
    # fill in ending_layer_indices with the corresponding starting_layer_index
    return {
        layer_index: _get_value_or_dummy(starting_tensor_by_layer_index, source_layer_index)
        for layer_index, source_layer_index in source_layer_index_by_layer_index.items()
    }
def _clone_tensor_by_layer_index(
    tensor_by_layer_index: dict[LayerIndex, torch.Tensor]
) -> dict[LayerIndex, torch.Tensor]:
    return {
        # In addition to cloning, we detach. It's hard to imagine a situation where we'd want the
        # cloned tensor to remain part of the computation graph.
        layer_index: tensor.detach().clone()
        for layer_index, tensor in tensor_by_layer_index.items()
    }
def _move_tensor_to_cpu_by_layer_index(
    tensor_by_layer_index: dict[LayerIndex, torch.Tensor]
) -> dict[LayerIndex, torch.Tensor]:
    return {layer_index: tensor.cpu() for layer_index, tensor in tensor_by_layer_index.items()}
def safe_sorted(x: list[LayerIndex]) -> list[LayerIndex]:
    if len(x) > 1:
        # a list of integer layer indices, e.g. [0, 1, 2, 5] (for location types with layer indices, like MLP activations)
        assert all(element is not None for element in x)
        return sorted(x)  # type: ignore
    else:
        # a single list with either [None] (for location types with no layer indices, like embeddings)
        # or a single layer index (for location types with layer indices, like MLP activations)
        return x
@dataclass(frozen=True)
class RawActivationStore:
    """
    Holds activations of multiple HookLocationTypeAndPassType's, and computes derived scalars from them given one or
    more scalar derivers and requested pass types. This class is intended to be used for computing derived scalars.
    """
    activations_by_sub_activation_location_type_and_pass_type: dict[
        ActivationLocationTypeAndPassType, ActivationsAndMetadata
    ]
    def get_activations_and_metadata(
        self,
        activation_location_type: ActivationLocationType,
        pass_type: PassType,
    ) -> ActivationsAndMetadata:
        return self.activations_by_sub_activation_location_type_and_pass_type[
            ActivationLocationTypeAndPassType(
                activation_location_type=activation_location_type, pass_type=pass_type
            )
        ]
    @classmethod
    def from_nested_dict_of_activations(
        cls,
        activations_by_sub_activation_location_type_and_pass_type: dict[
            ActivationLocationTypeAndPassType, dict[LayerIndex, torch.Tensor]
        ],
    ) -> "RawActivationStore":
        """This automatically constructs ActivationsAndMetadata objects from dicts,
        as well as the overall RawActivationStore object from the resulting ActivationsAndMetadata objects.
        """
        activations_and_metadata_by_activation_location_type_and_pass_type = {}
        for (
            activation_location_type_and_pass_type,
            activations_by_layer_index,
        ) in activations_by_sub_activation_location_type_and_pass_type.items():
            dst = DerivedScalarType.from_activation_location_type(
                activation_location_type_and_pass_type.activation_location_type
            )
            activations_and_metadata = ActivationsAndMetadata(
                activations_by_layer_index=activations_by_layer_index,
                dst=dst,
                pass_type=activation_location_type_and_pass_type.pass_type,
            )
            activations_and_metadata_by_activation_location_type_and_pass_type[
                activation_location_type_and_pass_type
            ] = activations_and_metadata
        return cls(activations_and_metadata_by_activation_location_type_and_pass_type)

================
File: neuron_explainer/activations/derived_scalars/attention.py
================
"""
This file contains code to compute derived scalars related to attention heads, such as the norm of
the vector written by an attention head to the residual stream between a pair of tokens, or the
lower triangle of attention scores from later tokens to earlier tokens, flattened into a single
vector.
"""
from typing import Callable
import torch
import torch.nn
from neuron_explainer.activations.derived_scalars.autoencoder import (
    make_autoencoder_activation_fn_derivative,
    make_autoencoder_latent_grad_wrt_residual_input_scalar_source,
    make_autoencoder_latent_scalar_deriver_factory,
    make_autoencoder_pre_act_encoder_derivative,
)
from neuron_explainer.activations.derived_scalars.derived_scalar_types import DerivedScalarType
from neuron_explainer.activations.derived_scalars.direct_effects import (
    compute_attn_write_to_residual_direction_from_attn_weighted_values,
    convert_scalar_deriver_to_write_to_final_residual_grad,
)
from neuron_explainer.activations.derived_scalars.locations import ConstantLayerIndexer
from neuron_explainer.activations.derived_scalars.raw_activations import (
    get_scalar_sources_for_activation_location_types,
    make_scalar_deriver_factory_for_act_times_grad,
    make_scalar_deriver_factory_for_activation_location_type,
)
from neuron_explainer.activations.derived_scalars.scalar_deriver import (
    DerivedScalarSource,
    DstConfig,
    RawScalarSource,
    ScalarDeriver,
    ScalarSource,
)
from neuron_explainer.activations.derived_scalars.write_tensors import (
    get_attn_write_tensor_by_layer_index,
)
from neuron_explainer.models.model_component_registry import (
    ActivationLocationType,
    Dimension,
    LayerIndex,
    NodeType,
    PassType,
    WeightLocationType,
)
from neuron_explainer.models.model_context import ModelContext
def flatten_lower_triangle(t: torch.Tensor) -> torch.Tensor:
    """This turns the lower triangular entries of a tensor (that is, entries with
    i <= j for i and j indexing the last two dimensions) into a tensor flattened
    along the last two dimensions. This is used for turning attention-related activations
    which would normally be list[list[float]] for each token sequence, into a list[float],
    which is compatible with NeuronRecord's."""
    # Get the shape of the tensor
    shape = t.shape
    # Get the last dimension N (tensor should be ... x M x N)
    M, N = shape[-2:]
    # Get the indices of the lower triangular part
    row_indices, col_indices = torch.tril_indices(M, N)
    # Reshape the tensor so that all preceding dimensions are combined into one
    t_reshaped = t.reshape(-1, M, N)
    # Extract the lower triangular part and flatten it
    flattened_lower_triangle = t_reshaped[:, row_indices, col_indices]
    # Reshape back to separate out the original preceding dimensions
    new_shape = list(shape[:-2]) + [-1]
    flattened_lower_triangle = flattened_lower_triangle.reshape(new_shape)
    return flattened_lower_triangle
def _convert_rectangular_activations_to_flattened_lower_triangle(
    rectangular_activations: torch.Tensor,
) -> torch.Tensor:
    """
    This function takes a tensor with the first two dimensions indexed by rows i and columns j.
    It keeps the entries where i <= j and flattens the first 2 dimensions into a 1-d list.
    """
    assert rectangular_activations.ndim == 3, rectangular_activations.shape
    num_sequence_tokens, num_attended_to_sequence_tokens, nheads = rectangular_activations.shape
    rectangular_activations = torch.einsum(
        "fth->hft", rectangular_activations
    )  # (nheads, num_sequence_tokens, num_attended_to_sequence_tokens
    flattened_activations = flatten_lower_triangle(rectangular_activations)
    flattened_activations = torch.einsum(
        "hs->sh", flattened_activations
    )  # (num_token_pairs, nheads)
    min_num_tokens = min(num_sequence_tokens, num_attended_to_sequence_tokens)
    num_token_pairs_before_attended_to_sequence_is_saturated = (
        min_num_tokens * (min_num_tokens + 1) // 2
    )
    if num_sequence_tokens > num_attended_to_sequence_tokens:
        num_token_pairs_after_attended_to_sequence_is_saturated = (
            num_sequence_tokens - num_attended_to_sequence_tokens
        ) * num_attended_to_sequence_tokens
        num_token_pairs = (
            num_token_pairs_before_attended_to_sequence_is_saturated
            + num_token_pairs_after_attended_to_sequence_is_saturated
        )
    else:
        num_token_pairs = num_token_pairs_before_attended_to_sequence_is_saturated
    # if num_sequence_tokens == num_attended_to_sequence_tokens, then the i>=j entries form a lower triangle (first term)
    # if num_sequence_tokens > num_attended_to_sequence_tokens, then there is an additional rectangle of entries
    # below that lower triangle with i > j (second term)
    assert flattened_activations.shape == (
        num_token_pairs,
        nheads,
    ), f"{flattened_activations.shape=} != {(num_token_pairs, nheads)=}"
    return flattened_activations
def unflatten_lower_triangle(flattened: torch.Tensor, M: int, N: int) -> torch.Tensor:
    """Inverse of flatten_lower_triangle.
    This function reconstructs a tensor from its flattened lower triangular part.
    The upper triangular part of the reconstructed tensor is filled with zeros.
    """
    # Get the indices of the lower triangular part
    row_indices, col_indices = torch.tril_indices(M, N)
    # Create an empty tensor to fill in the original shape
    new_shape = list(flattened.shape[:-1]) + [M, N]
    reconstructed = torch.zeros(new_shape, dtype=flattened.dtype, device=flattened.device)
    # Fill in the lower triangular part of the reconstructed tensor
    reconstructed[..., row_indices, col_indices] = flattened
    return reconstructed
def unflatten_lower_triangle_and_sum_columns(
    flattened: torch.Tensor, M: int, N: int
) -> torch.Tensor:
    """Equivalent to unflatten_lower_triangle(...).sum(dim=-1), less memory, more time.
    This function calculates the sum (over the last dimension) of the lower triangular part of a tensor
    from its flattened representation without instantiating the full matrix.
    """
    # Get the indices of the lower triangular part
    row_indices, col_indices = torch.tril_indices(M, N)
    num_elements = row_indices.shape[0]
    assert flattened.shape[-1] == num_elements
    # Create an empty tensor to store the sum of each row
    new_shape = list(flattened.shape[:-1]) + [M]
    reconstructed_summed = torch.zeros(new_shape, dtype=flattened.dtype, device=flattened.device)
    # Sum the elements over columns
    for i in range(num_elements):
        reconstructed_summed[..., row_indices[i]] += flattened[..., i]
    return reconstructed_summed
def _compute_v_times_Wo_norm(
    attn_value: torch.Tensor,  # thd
    W_O: torch.Tensor,  # hdo
) -> torch.Tensor:
    """
    This function computes the norm of the product of attention values and the output weight matrix
    (W_O in the transformer-circuits convention, or c_proj in many internal settings).
    The attention values are represented by the tensor attn_value and the output weight matrix by W_O.
    The function returns a tensor representing the norm of the product.
    """
    num_attended_to_sequence_tokens, nheads, d_head = attn_value.shape
    assert W_O.shape[:2] == (nheads, d_head)  # third dim is d_model
    v_times_Wo = torch.einsum("thd,hdo->tho", attn_value, W_O)
    return torch.linalg.norm(v_times_Wo, dim=-1)
def _compute_attn_write_norm_from_attn_and_value(
    attn_post_softmax: torch.Tensor,  # fth
    attn_value: torch.Tensor,  # thd
    W_O: torch.Tensor,  # hdo
    pass_type: PassType,
) -> torch.Tensor:
    """Computes the norm of the write vector from each token to each other token,
    by multiplying two saved activations and a model weight.
    For the forward pass, this is attn_post_softmax * ||W_O @ V|| (elementwise multiplication).
    For the backward pass (the gradient with respect to the forward pass quantity),
    this is grad_attn_post_softmax / ||W_O @ V|| (elementwise division)"""
    # attn_post_softmax: (nheads, num_sequence_tokens, num_attended_to_sequence_tokens)
    # attn_value: (num_attended_to_sequence_tokens, nheads, d_head)
    # W_O: (nheads, d_head, d_model)
    # output: (num_sequence_tokens, num_attended_to_sequence_tokens, nheads)
    num_sequence_tokens, num_attended_to_sequence_tokens, nheads = attn_post_softmax.shape
    d_head = attn_value.shape[-1]
    assert attn_value.shape == (num_attended_to_sequence_tokens, nheads, d_head)
    v_times_Wo_norm = _compute_v_times_Wo_norm(attn_value, W_O)
    assert v_times_Wo_norm.shape == (num_attended_to_sequence_tokens, nheads)
    if pass_type == PassType.FORWARD:
        v_times_Wo_norm_factor = v_times_Wo_norm
    else:
        assert pass_type == PassType.BACKWARD
        v_times_Wo_norm_factor = 1 / v_times_Wo_norm
    output = torch.einsum("fth,th->fth", attn_post_softmax, v_times_Wo_norm_factor)
    assert output.shape == (num_sequence_tokens, num_attended_to_sequence_tokens, nheads)
    return output
def _get_attn_write_for_one_layer_index(
    model_context: ModelContext,
    layer_index: int,
) -> torch.Tensor:
    """Returns a dictionary mapping layer index to the write weight matrix for that layer."""
    return get_attn_write_tensor_by_layer_index(
        model_context=model_context,
        layer_indices=[layer_index],
    )[layer_index]
def _make_unflattened_attn_write_norm_tensor_calculate_derived_scalar_fn(
    model_context: ModelContext,
    layer_indices: list[int],
) -> Callable[[tuple[torch.Tensor, ...], LayerIndex, PassType], torch.Tensor]:
    """Returns a function that takes a tuple of tensors containing
    post softmax attention values and value vectors, and returns a tensor
    of the norm of the write vector from each token to each other token."""
    nheads = model_context.n_attention_heads
    d_model = model_context.n_residual_stream_channels
    assert all(isinstance(layer_index, int) for layer_index in layer_indices)
    W_O_by_layer_index = get_attn_write_tensor_by_layer_index(
        model_context=model_context,
        layer_indices=layer_indices,
    )
    def _attn_write_norm_tensor_calculate_derived_scalar_fn(
        raw_activation_data_tuple: tuple[torch.Tensor, ...],
        layer_index: LayerIndex,
        pass_type: PassType,
    ) -> torch.Tensor:
        """The returned processing hook, to be passed as an argument to the ScalarDeriver."""
        if len(raw_activation_data_tuple) == 2:
            attn_post_softmax_data, attn_value_data = raw_activation_data_tuple
            assert (
                pass_type == PassType.FORWARD
            ), f"Need gradient of attn_post_softmax for backward pass, got {raw_activation_data_tuple}"
        else:
            assert len(raw_activation_data_tuple) == 3
            # first position is forward pass attn_post_softmax,
            # second is backward pass attn_post_softmax,
            # third is attn_value
            if pass_type == PassType.FORWARD:
                attn_post_softmax_data, _, attn_value_data = raw_activation_data_tuple
            else:
                assert pass_type == PassType.BACKWARD
                _, attn_post_softmax_data, attn_value_data = raw_activation_data_tuple
        assert layer_index in W_O_by_layer_index
        return _compute_attn_write_norm_from_attn_and_value(
            attn_post_softmax=attn_post_softmax_data,
            attn_value=attn_value_data,
            W_O=W_O_by_layer_index[layer_index],
            pass_type=pass_type,
        )
    return _attn_write_norm_tensor_calculate_derived_scalar_fn
def make_unflattened_attn_write_norm_scalar_deriver(
    dst_config: DstConfig,
) -> ScalarDeriver:
    """Returns a ScalarDeriver object for the attention write norm, between any
    two tokens. The actual attention write vector is the vector sum of many attention
    write vectors. Note that the norm of this vector sum will be less that the sum of the
    attention write norms computed here."""
    model_context = dst_config.get_model_context()
    layer_indices = dst_config.layer_indices or list(range(model_context.n_layers))
    if dst_config.derive_gradients:
        sub_scalar_sources: tuple[ScalarSource, ...] = (
            RawScalarSource(
                activation_location_type=ActivationLocationType.ATTN_QK_PROBS,
                pass_type=PassType.FORWARD,
            ),
            RawScalarSource(
                activation_location_type=ActivationLocationType.ATTN_QK_PROBS,
                pass_type=PassType.BACKWARD,
            ),
            RawScalarSource(
                activation_location_type=ActivationLocationType.ATTN_VALUE,
                pass_type=PassType.FORWARD,
            ),
        )
    else:
        sub_scalar_sources = (
            RawScalarSource(
                activation_location_type=ActivationLocationType.ATTN_QK_PROBS,
                pass_type=PassType.FORWARD,
            ),
            RawScalarSource(
                activation_location_type=ActivationLocationType.ATTN_VALUE,
                pass_type=PassType.FORWARD,
            ),
        )
    return ScalarDeriver(
        dst=DerivedScalarType.UNFLATTENED_ATTN_WRITE_NORM,
        dst_config=dst_config,
        sub_scalar_sources=sub_scalar_sources,
        tensor_calculate_derived_scalar_fn=_make_unflattened_attn_write_norm_tensor_calculate_derived_scalar_fn(
            model_context=model_context,
            layer_indices=layer_indices,
        ),
    )
def make_attn_write_norm_scalar_deriver(
    dst_config: DstConfig,
) -> ScalarDeriver:
    scalar_deriver = make_unflattened_attn_write_norm_scalar_deriver(dst_config)
    return scalar_deriver.apply_transform_fn_to_output(
        _convert_rectangular_activations_to_flattened_lower_triangle,
        pass_type_to_transform=PassType.FORWARD,
        output_dst=DerivedScalarType.ATTN_WRITE_NORM,
    )
def make_flattened_attn_post_softmax_scalar_deriver(
    dst_config: DstConfig,
) -> ScalarDeriver:
    scalar_deriver = make_scalar_deriver_factory_for_activation_location_type(
        ActivationLocationType.ATTN_QK_PROBS
    )(dst_config)
    return scalar_deriver.apply_transform_fn_to_output(
        _convert_rectangular_activations_to_flattened_lower_triangle,
        pass_type_to_transform=PassType.FORWARD,
        output_dst=DerivedScalarType.FLATTENED_ATTN_POST_SOFTMAX,
    )
def make_flattened_attn_post_softmax_act_times_grad_scalar_deriver(
    dst_config: DstConfig,
) -> ScalarDeriver:
    scalar_deriver = make_scalar_deriver_factory_for_act_times_grad(
        ActivationLocationType.ATTN_QK_PROBS,
        DerivedScalarType.UNFLATTENED_ATTN_ACT_TIMES_GRAD,
    )(dst_config)
    return scalar_deriver.apply_transform_fn_to_output(
        _convert_rectangular_activations_to_flattened_lower_triangle,
        pass_type_to_transform=PassType.FORWARD,
        output_dst=DerivedScalarType.ATTN_ACT_TIMES_GRAD,
    )
def make_attn_act_times_grad_per_sequence_token_scalar_deriver(
    dst_config: DstConfig,
) -> ScalarDeriver:
    assert not dst_config.derive_gradients, "Gradients not defined for act times grad"
    scalar_deriver = make_scalar_deriver_factory_for_act_times_grad(
        ActivationLocationType.ATTN_QK_PROBS,
        DerivedScalarType.UNFLATTENED_ATTN_ACT_TIMES_GRAD,
    )(dst_config)
    return scalar_deriver.apply_transform_fn_to_output(
        lambda activations: activations.sum(dim=1),
        pass_type_to_transform=PassType.FORWARD,
        output_dst=DerivedScalarType.ATTN_ACT_TIMES_GRAD_PER_SEQUENCE_TOKEN,
    )
def make_attn_write_norm_per_sequence_token_tensor_calculate_derived_scalar_fn(
    model_context: ModelContext,
    layer_indices: list[int],
) -> Callable[[tuple[torch.Tensor, ...], LayerIndex, PassType], torch.Tensor]:
    W_O_by_layer_index = get_attn_write_tensor_by_layer_index(
        model_context=model_context,
        layer_indices=layer_indices,
    )
    def attn_write_norm_per_sequence_token_tensor_calculate_derived_scalar_fn(
        raw_activation_data_tuple: tuple[torch.Tensor, ...],
        layer_index: LayerIndex,
        pass_type: PassType,
    ) -> torch.Tensor:
        assert layer_index in W_O_by_layer_index
        assert len(raw_activation_data_tuple) == 1
        attn_weighted_sum_of_values = raw_activation_data_tuple[0]
        attn_write_norm_per_sequence_token = _compute_v_times_Wo_norm(
            attn_value=attn_weighted_sum_of_values,
            W_O=W_O_by_layer_index[layer_index],
        )
        return attn_write_norm_per_sequence_token
    return attn_write_norm_per_sequence_token_tensor_calculate_derived_scalar_fn
def make_attn_write_norm_per_sequence_token_scalar_deriver(
    dst_config: DstConfig,
) -> ScalarDeriver:
    activation_location_type = ActivationLocationType.ATTN_WEIGHTED_SUM_OF_VALUES
    sub_scalar_sources = get_scalar_sources_for_activation_location_types(
        activation_location_type, dst_config.derive_gradients
    )
    model_context = dst_config.get_model_context()
    layer_indices = dst_config.layer_indices or list(range(model_context.n_layers))
    attn_write_norm_per_sequence_token_tensor_calculate_derived_scalar_fn = (
        make_attn_write_norm_per_sequence_token_tensor_calculate_derived_scalar_fn(
            model_context=model_context,
            layer_indices=layer_indices,
        )
    )
    return ScalarDeriver(
        dst=DerivedScalarType.ATTN_WRITE_NORM_PER_SEQUENCE_TOKEN,
        dst_config=dst_config,
        sub_scalar_sources=sub_scalar_sources,
        tensor_calculate_derived_scalar_fn=attn_write_norm_per_sequence_token_tensor_calculate_derived_scalar_fn,
    )
def convert_attn_weighted_value_scalar_deriver_to_write_to_residual_direction_in_same_layer(
    attn_weighted_value_scalar_deriver: ScalarDeriver,
    direction_scalar_source: ScalarSource,
    output_dst: DerivedScalarType,
) -> ScalarDeriver:
    model_context = attn_weighted_value_scalar_deriver.dst_config.get_model_context()
    dst_config = attn_weighted_value_scalar_deriver.dst_config
    direction_layer_index = direction_scalar_source.layer_index
    assert dst_config.layer_indices is not None
    assert len(dst_config.layer_indices) == 1
    assert dst_config.layer_indices[0] == direction_layer_index
    W_O = _get_attn_write_for_one_layer_index(
        model_context=model_context,
        layer_index=direction_layer_index,
    )
    def attn_write_to_residual_direction_tensor_calculate_derived_scalar_fn(
        attn_weighted_values: torch.Tensor,
        residual_grad: torch.Tensor,
        layer_index: LayerIndex,
        pass_type: PassType,
    ) -> torch.Tensor:
        assert layer_index == direction_layer_index
        assert (
            pass_type == PassType.FORWARD
        ), "write to final residual grad only defined for forward pass"
        return compute_attn_write_to_residual_direction_from_attn_weighted_values(
            attn_weighted_values=attn_weighted_values,
            residual_direction=residual_grad,
            W_O=W_O,
            pass_type=pass_type,
        )
    return attn_weighted_value_scalar_deriver.apply_layerwise_transform_fn_to_output_and_other_tensor(
        layerwise_transform_fn=attn_write_to_residual_direction_tensor_calculate_derived_scalar_fn,
        pass_type_to_transform=PassType.FORWARD,
        output_dst=output_dst,
        other_scalar_source=direction_scalar_source,
    )
def make_attn_write_to_final_residual_grad_per_sequence_token_scalar_deriver(
    dst_config: DstConfig,
) -> ScalarDeriver:
    attn_weighted_value_scalar_deriver = make_scalar_deriver_factory_for_activation_location_type(
        ActivationLocationType.ATTN_WEIGHTED_SUM_OF_VALUES,
    )(dst_config)
    return convert_scalar_deriver_to_write_to_final_residual_grad(
        scalar_deriver=attn_weighted_value_scalar_deriver,
        output_dst=DerivedScalarType.ATTN_WRITE_TO_FINAL_RESIDUAL_GRAD_PER_SEQUENCE_TOKEN,
        use_existing_backward_pass_for_final_residual_grad=True,
    )
def _check_attn_post_softmax_shape(attn_post_softmax: torch.Tensor) -> None:
    num_sequence_tokens, num_attended_to_sequence_tokens, nheads = attn_post_softmax.shape
    assert num_attended_to_sequence_tokens == num_sequence_tokens
def _compute_attn_write_from_attn_and_value(
    attn_post_softmax: torch.Tensor,  # fth
    attn_value: torch.Tensor,  # thd
    W_O: torch.Tensor,  # hdo (heads, d_head, d_model)
    pass_type: PassType,
) -> torch.Tensor:
    assert (
        pass_type == PassType.FORWARD
    ), "only forward pass implemented for now for attn write projection from value"
    _check_attn_post_softmax_shape(attn_post_softmax)
    num_attended_to_sequence_tokens, n_heads, d_head = attn_value.shape
    assert attn_post_softmax.shape[1] == num_attended_to_sequence_tokens
    assert attn_post_softmax.shape[2] == n_heads
    num_sequence_tokens, num_attended_to_sequence_tokens, n_heads = attn_post_softmax.shape
    assert W_O.shape[:2] == (n_heads, d_head)
    d_model = W_O.shape[2]
    attn_value_times_Wo = torch.einsum("thd,hdv->thv", attn_value, W_O)
    attn_weighted_v_times_Wo = torch.einsum("fth,thv->fthv", attn_post_softmax, attn_value_times_Wo)
    assert attn_weighted_v_times_Wo.shape == (
        num_sequence_tokens,
        num_attended_to_sequence_tokens,
        n_heads,
        d_model,
    )
    return attn_weighted_v_times_Wo
def make_unflattened_attn_write_to_final_residual_grad_scalar_deriver(
    dst_config: DstConfig,
) -> ScalarDeriver:
    scalar_deriver = make_attn_weighted_value_scalar_deriver(
        dst_config=dst_config,
    )
    return convert_scalar_deriver_to_write_to_final_residual_grad(
        scalar_deriver=scalar_deriver,
        output_dst=DerivedScalarType.UNFLATTENED_ATTN_WRITE_TO_FINAL_RESIDUAL_GRAD,
        use_existing_backward_pass_for_final_residual_grad=True,
    )
def make_unflattened_attn_write_to_final_activation_residual_grad_scalar_deriver(
    dst_config: DstConfig,
) -> ScalarDeriver:
    attn_weighted_value_scalar_deriver = make_attn_weighted_value_scalar_deriver(
        dst_config=dst_config,
    )
    return convert_scalar_deriver_to_write_to_final_residual_grad(
        scalar_deriver=attn_weighted_value_scalar_deriver,
        output_dst=DerivedScalarType.UNFLATTENED_ATTN_WRITE_TO_FINAL_ACTIVATION_RESIDUAL_GRAD,
        use_existing_backward_pass_for_final_residual_grad=False,
    )
def make_flattened_attn_write_to_final_residual_grad_scalar_deriver(
    dst_config: DstConfig,
) -> ScalarDeriver:
    scalar_deriver = make_unflattened_attn_write_to_final_residual_grad_scalar_deriver(dst_config)
    return scalar_deriver.apply_transform_fn_to_output(
        _convert_rectangular_activations_to_flattened_lower_triangle,
        pass_type_to_transform=PassType.FORWARD,
        output_dst=DerivedScalarType.ATTN_WRITE_TO_FINAL_RESIDUAL_GRAD,
    )
def make_attn_write_tensor_calculate_derived_scalar_fn(
    model_context: ModelContext,
    layer_indices: list[int],
) -> Callable[[tuple[torch.Tensor, ...], LayerIndex, PassType], torch.Tensor]:
    W_O_by_layer_index = get_attn_write_tensor_by_layer_index(
        model_context=model_context,
        layer_indices=layer_indices,
    )
    def attn_write_tensor_calculate_derived_scalar_fn(
        raw_activation_data_tuple: tuple[torch.Tensor, ...],
        layer_index: LayerIndex,
        pass_type: PassType,
    ) -> torch.Tensor:
        assert layer_index in W_O_by_layer_index
        assert len(raw_activation_data_tuple) == 2
        assert (
            pass_type == PassType.FORWARD
        ), "write to residual stream only implemented for forward pass"
        attn_post_softmax, attn_value = raw_activation_data_tuple
        attn_write_per_sequence_token = _compute_attn_write_from_attn_and_value(
            attn_post_softmax=attn_post_softmax,
            attn_value=attn_value,
            W_O=W_O_by_layer_index[layer_index],
            pass_type=pass_type,
        )
        return attn_write_per_sequence_token
    return attn_write_tensor_calculate_derived_scalar_fn
def make_attn_write_scalar_deriver(
    dst_config: DstConfig,
) -> ScalarDeriver:
    """Returns a ScalarDeriver object for the attention write, between any two tokens,
    and for each head."""
    model_context = dst_config.get_model_context()
    layer_indices = dst_config.layer_indices or list(range(model_context.n_layers))
    assert not dst_config.derive_gradients
    sub_scalar_sources = (
        RawScalarSource(
            activation_location_type=ActivationLocationType.ATTN_QK_PROBS,
            pass_type=PassType.FORWARD,
        ),
        RawScalarSource(
            activation_location_type=ActivationLocationType.ATTN_VALUE, pass_type=PassType.FORWARD
        ),
    )
    return ScalarDeriver(
        dst=DerivedScalarType.ATTN_WRITE,
        dst_config=dst_config,
        sub_scalar_sources=sub_scalar_sources,
        tensor_calculate_derived_scalar_fn=make_attn_write_tensor_calculate_derived_scalar_fn(
            model_context=model_context,
            layer_indices=layer_indices,
        ),
    )
def make_attn_write_sum_heads_scalar_deriver(
    dst_config: DstConfig,
) -> ScalarDeriver:
    """Returns a ScalarDeriver object for the attention write, between any two tokens,
    summed over all heads."""
    scalar_deriver = make_attn_write_scalar_deriver(dst_config)
    def _sum_heads(attn_write: torch.Tensor) -> torch.Tensor:
        n_tokens_written_to, n_tokens_attended_to, n_heads, d_model = attn_write.shape
        return attn_write.sum(dim=2)
    return scalar_deriver.apply_transform_fn_to_output(
        _sum_heads,
        pass_type_to_transform=PassType.FORWARD,
        output_dst=DerivedScalarType.ATTN_WRITE_SUM_HEADS,
    )
def make_attn_weighted_value_scalar_deriver(
    dst_config: DstConfig,
) -> ScalarDeriver:
    """Returns a scalar deriver for the attention value weighted by the post-softmax
    attention between each pair of tokens. Output shape: (n_tokens, n_attended_to_tokens, n_heads, v_channels)
    """
    def attn_weighted_value_tensor_calculate_derived_scalar_fn(
        raw_activation_data_tuple: tuple[torch.Tensor, ...],
        layer_index: LayerIndex,
        pass_type: PassType,
    ) -> torch.Tensor:
        assert len(raw_activation_data_tuple) == 2, ([t.shape for t in raw_activation_data_tuple],)
        assert pass_type == PassType.FORWARD, "weighted value only implemented for forward pass"
        attn_post_softmax, attn_value = raw_activation_data_tuple
        _check_attn_post_softmax_shape(attn_post_softmax)
        attn_weighted_value = torch.einsum("fth,thd->fthd", attn_post_softmax, attn_value)
        return attn_weighted_value
    return ScalarDeriver(
        dst=DerivedScalarType.ATTN_WEIGHTED_VALUE,
        dst_config=dst_config,
        sub_scalar_sources=(
            RawScalarSource(
                activation_location_type=ActivationLocationType.ATTN_QK_PROBS,
                pass_type=PassType.FORWARD,
            ),
            RawScalarSource(
                activation_location_type=ActivationLocationType.ATTN_VALUE,
                pass_type=PassType.FORWARD,
            ),
        ),
        tensor_calculate_derived_scalar_fn=attn_weighted_value_tensor_calculate_derived_scalar_fn,
    )
def make_attn_write_to_latent_scalar_deriver(
    dst_config: DstConfig,
) -> ScalarDeriver:
    """Returns the attention value weighted by the post-softmax attention between each pair of tokens,
    and projected to the gradient of autoencoder latent wrt input.
    Output has shape (n_tokens, n_attended_to_tokens, n_heads).
    """
    attn_weighted_value_scalar_deriver = make_attn_weighted_value_scalar_deriver(
        dst_config
    )  # derive scalar of shape (n_tokens, n_attended_to_tokens, n_heads, v_channels)
    direction_scalar_source = make_autoencoder_latent_grad_wrt_residual_input_scalar_source(
        dst_config, NodeType.ATTENTION_AUTOENCODER_LATENT
    )  # derive scalar of shape (res_channels) = (n_heads * v_channels)
    return convert_attn_weighted_value_scalar_deriver_to_write_to_residual_direction_in_same_layer(
        attn_weighted_value_scalar_deriver=attn_weighted_value_scalar_deriver,
        direction_scalar_source=direction_scalar_source,
        output_dst=DerivedScalarType.ATTN_WRITE_TO_LATENT,
    )  # derive scalar of shape (n_tokens, n_attended_to_tokens, n_heads)
def make_attn_write_to_latent_summed_over_heads_scalar_deriver(
    dst_config: DstConfig,
) -> ScalarDeriver:
    """Returns the attention value weighted by the post-softmax attention between each pair of tokens,
     summed over the heads, and projected to the gradient of autoencoder latent wrt input.
    Output has shape (n_tokens, n_attended_to_tokens).
    """
    attn_write_to_latent_scalar_deriver = make_attn_write_to_latent_scalar_deriver(
        dst_config
    )  # derive scalar of shape (n_tokens, n_attended_to_tokens, n_heads)
    return attn_write_to_latent_scalar_deriver.apply_transform_fn_to_output(
        lambda activations: activations.sum(dim=-1)[..., None],
        pass_type_to_transform=PassType.FORWARD,
        output_dst=DerivedScalarType.ATTN_WRITE_TO_LATENT_SUMMED_OVER_HEADS,
    )  # derive scalar of shape (n_tokens, n_attended_to_tokens, 1)
def make_flattened_attn_write_to_latent_summed_over_heads_scalar_deriver(
    dst_config: DstConfig,
) -> ScalarDeriver:
    """Returns the attention value weighted by the post-softmax attention between each pair of tokens,
    summed over the heads, and projected to the gradient of autoencoder latent wrt input,\
    flattening the lower triangular part of the output.
    Output has shape (n_token_pairs).
    """
    attn_write_to_latent_scalar_deriver = make_attn_write_to_latent_scalar_deriver(
        dst_config
    )  # derive scalar of shape (n_tokens, n_attended_to_tokens, n_heads)
    return attn_write_to_latent_scalar_deriver.apply_transform_fn_to_output(
        lambda activations: _convert_rectangular_activations_to_flattened_lower_triangle(
            activations.sum(dim=-1)[..., None]
        ),
        pass_type_to_transform=PassType.FORWARD,
        output_dst=DerivedScalarType.FLATTENED_ATTN_WRITE_TO_LATENT_SUMMED_OVER_HEADS,
    )  # derive scalar of shape (n_token_pairs, 1)
def make_flattened_attn_write_to_latent_summed_over_heads_batched_scalar_deriver(
    dst_config: DstConfig,
) -> ScalarDeriver:
    """Same as make_flattened_attn_write_to_latent_summed_over_heads_scalar_deriver, but over
    all latents at the same time.
    Assumes that the autoencoder has a single encoder layer and a ReLU, to compute the
    gradient (attribution) without backprop.
    Output has shape (n_token_pairs, n_latents).
    """
    autoencoder_node_type = NodeType.ATTENTION_AUTOENCODER_LATENT
    attn_write_sum_heads_scalar_deriver = make_attn_write_sum_heads_scalar_deriver(
        dst_config
    )  # derive scalar of shape (n_tokens, n_attended_to_tokens, d_model)
    autoencoder_scalar_deriver = make_autoencoder_latent_scalar_deriver_factory(
        autoencoder_node_type
    )(
        dst_config
    )  # derive scalar of shape (n_tokens, n_latents)
    assert dst_config.layer_indices is not None
    assert len(dst_config.layer_indices) == 1
    layer_index = dst_config.layer_indices[0]
    autoencoder_context = dst_config.get_autoencoder_context(autoencoder_node_type)
    assert autoencoder_context is not None
    pre_act_encoder_derivative = make_autoencoder_pre_act_encoder_derivative(
        autoencoder_context, layer_index
    )
    activation_fn_derivative = make_autoencoder_activation_fn_derivative(
        autoencoder_context, layer_index
    )
    def attn_write_to_latents_tensor_calculate_derived_scalar_fn(
        attn_write_sum_heads: torch.Tensor,
        autoencoder_latents: torch.Tensor,
        layer_index: LayerIndex,
        pass_type: PassType,
    ) -> torch.Tensor:
        assert layer_index == layer_index
        assert pass_type == PassType.FORWARD, "write to latents only defined for forward pass"
        n_tokens, n_tokens_attended_to, d_model = attn_write_sum_heads.shape
        assert autoencoder_latents.shape[0] == n_tokens
        n_tokens, n_latents = autoencoder_latents.shape
        # from (n_tokens, n_tokens_attended_to, d_model) to (n_tokens, n_tokens_attended_to, n_latents)
        projection = pre_act_encoder_derivative(attn_write_sum_heads)
        d_latent_d_pre_act = activation_fn_derivative(autoencoder_latents)
        direct_write_to_latents = torch.einsum("tul,tl->tul", projection, d_latent_d_pre_act)
        flattened_direct_write_to_latents = (
            _convert_rectangular_activations_to_flattened_lower_triangle(direct_write_to_latents)
        )
        return flattened_direct_write_to_latents
    new_scalar_deriver = (
        attn_write_sum_heads_scalar_deriver.apply_layerwise_transform_fn_to_output_and_other_tensor(
            layerwise_transform_fn=attn_write_to_latents_tensor_calculate_derived_scalar_fn,
            pass_type_to_transform=PassType.FORWARD,
            output_dst=DerivedScalarType.FLATTENED_ATTN_WRITE_TO_LATENT_SUMMED_OVER_HEADS_BATCHED,
            other_scalar_source=DerivedScalarSource(
                scalar_deriver=autoencoder_scalar_deriver,
                pass_type=PassType.FORWARD,
                layer_indexer=ConstantLayerIndexer(layer_index),
            ),
        )
    )
    return new_scalar_deriver  # derive scalar of shape (n_token_pairs, n_latents)
def make_attn_write_to_latent_per_sequence_token_scalar_deriver(
    dst_config: DstConfig,
) -> ScalarDeriver:
    """Returns the attention value weighted by the post-softmax attention between each pair of tokens,
     summed over the attended-to tokens, and projected to the gradient of autoencoder latent wrt input.
    Output has shape (n_tokens, n_heads).
    """
    attn_weighted_sum_of_value_scalar_deriver = (
        make_scalar_deriver_factory_for_activation_location_type(
            ActivationLocationType.ATTN_WEIGHTED_SUM_OF_VALUES
        )(dst_config)
    )  # derive scalar of shape (n_tokens, n_heads, v_channels)
    direction_scalar_source = make_autoencoder_latent_grad_wrt_residual_input_scalar_source(
        dst_config
    )  # derive scalar of shape (res_channels) = (n_heads * v_channels)
    return convert_attn_weighted_value_scalar_deriver_to_write_to_residual_direction_in_same_layer(
        attn_weighted_value_scalar_deriver=attn_weighted_sum_of_value_scalar_deriver,
        direction_scalar_source=direction_scalar_source,
        output_dst=DerivedScalarType.ATTN_WRITE_TO_LATENT_PER_SEQUENCE_TOKEN,
    )  # derive scalar of shape (n_tokens, n_heads)
def make_attn_write_to_latent_per_sequence_token_batched_scalar_deriver(
    dst_config: DstConfig,
) -> ScalarDeriver:
    """Same as make_attn_write_to_latent_per_sequence_token_scalar_deriver, but over
    all latents at the same time.
    Assumes that the autoencoder has a single encoder layer and a ReLU, to compute the
    gradient (attribution) without backprop.
    Output has shape (n_tokens, n_heads, n_latents).
    """
    autoencoder_node_type = NodeType.ATTENTION_AUTOENCODER_LATENT
    attn_weighted_sum_of_value_scalar_deriver = (
        make_scalar_deriver_factory_for_activation_location_type(
            ActivationLocationType.ATTN_WEIGHTED_SUM_OF_VALUES
        )(dst_config)
    )  # derive scalar of shape (n_tokens, n_heads, v_channels)
    autoencoder_scalar_deriver = make_autoencoder_latent_scalar_deriver_factory(
        autoencoder_node_type
    )(
        dst_config
    )  # derive scalar of shape (n_tokens, n_latents)
    assert dst_config.layer_indices is not None
    assert len(dst_config.layer_indices) == 1
    layer_index = dst_config.layer_indices[0]
    autoencoder_context = dst_config.get_autoencoder_context(autoencoder_node_type)
    assert autoencoder_context is not None
    pre_act_encoder_derivative = make_autoencoder_pre_act_encoder_derivative(
        autoencoder_context, layer_index
    )
    activation_fn_derivative = make_autoencoder_activation_fn_derivative(
        autoencoder_context, layer_index
    )
    model_context = dst_config.get_model_context()
    W_O = model_context.get_weight(
        location_type=WeightLocationType.ATTN_TO_RESIDUAL,
        layer=layer_index,
        device=model_context.device,
    )  # shape (n_heads, d_head, d_model)
    def attn_write_to_latents_tensor_calculate_derived_scalar_fn(
        attn_weighted_sum_of_value: torch.Tensor,
        autoencoder_latents: torch.Tensor,
        layer_index: LayerIndex,
        pass_type: PassType,
    ) -> torch.Tensor:
        assert layer_index == layer_index
        assert pass_type == PassType.FORWARD, "write to latents only defined for forward pass"
        n_tokens, n_heads, d_head = attn_weighted_sum_of_value.shape
        assert autoencoder_latents.shape[0] == n_tokens
        n_tokens, n_latents = autoencoder_latents.shape
        assert W_O.shape[:2] == (n_heads, d_head)
        n_heads, d_head, d_model = W_O.shape
        # from (n_heads, d_head, d_model) to (n_heads, d_head, n_latents)
        Wo_encoder = pre_act_encoder_derivative(W_O)
        Wo_encoder = Wo_encoder.to(attn_weighted_sum_of_value.dtype)
        projection = torch.einsum("thd,hdl->thl", attn_weighted_sum_of_value, Wo_encoder)
        d_latent_d_pre_act = activation_fn_derivative(autoencoder_latents)
        direct_write_to_latents = torch.einsum("thl,tl->thl", projection, d_latent_d_pre_act)
        return direct_write_to_latents
    new_scalar_deriver = attn_weighted_sum_of_value_scalar_deriver.apply_layerwise_transform_fn_to_output_and_other_tensor(
        layerwise_transform_fn=attn_write_to_latents_tensor_calculate_derived_scalar_fn,
        pass_type_to_transform=PassType.FORWARD,
        output_dst=DerivedScalarType.ATTN_WRITE_TO_LATENT_PER_SEQUENCE_TOKEN_BATCHED,
        other_scalar_source=DerivedScalarSource(
            scalar_deriver=autoencoder_scalar_deriver,
            pass_type=PassType.FORWARD,
            layer_indexer=ConstantLayerIndexer(layer_index),
        ),
    )
    return new_scalar_deriver  # derive scalar of shape (n_token_pairs, n_heads, n_latents)
def make_reshape_fn(dst: DerivedScalarType) -> Callable:
    """Create a reshape function to apply to the output tensors."""
    output_dim = dst.shape_spec_per_token_sequence
    error_msg = f"Unexpected output_dim: {output_dim}. Please add a reshape function."
    if len(output_dim) == 2:
        # Regular activations are already 2d and don't need to be reshaped.
        assert output_dim[0] == Dimension.SEQUENCE_TOKENS
        assert output_dim[1].is_model_intrinsic
        reshape_fn = lambda x: x
    elif len(output_dim) == 3:
        assert output_dim[0] == Dimension.SEQUENCE_TOKENS
        assert output_dim[2].is_model_intrinsic
        if output_dim[1] == Dimension.ATTENDED_TO_SEQUENCE_TOKENS:
            # E.g. attention activations that are indexed both by current-token and token-attended-to.
            # Here, we move the two indexing dimensions to the end, we extract the lower triangle indices,
            # we flatten the lower triangle indices into a single dimension, and we move that dimension to the front.
            reshape_fn = lambda x: flatten_lower_triangle(x.permute(2, 0, 1)).permute(1, 0)
        elif output_dim[1] == Dimension.ATTN_HEADS:
            # E.g. attention activations that are split by attention heads.
            # Here, we merge the two model dimensions into one.
            reshape_fn = lambda x: x.reshape(x.shape[0], -1)
        else:
            raise NotImplementedError(error_msg)
    elif len(output_dim) == 4:
        assert output_dim[0] == Dimension.SEQUENCE_TOKENS
        assert output_dim[3].is_model_intrinsic
        if (
            output_dim[1] == Dimension.ATTENDED_TO_SEQUENCE_TOKENS
            and output_dim[2] == Dimension.ATTN_HEADS
        ):
            # Here, we move the two indexing dimensions to the end, we extract the lower triangle indices,
            # we flatten the lower triangle indices into a single dimension, and we move that dimension to the front.
            # Then we merge the merged input dimension with the attention heads dimension.
            reshape_fn = (
                lambda x: flatten_lower_triangle(x.permute(2, 3, 0, 1))
                .permute(2, 0, 1)
                .reshape(-1, x.shape[3])
            )
        else:
            raise NotImplementedError(error_msg)
    else:
        raise NotImplementedError(error_msg)
    return reshape_fn

================
File: neuron_explainer/activations/derived_scalars/autoencoder.py
================
"""
This file contains code to compute derived scalars related to autoencoder latents post-hoc
(that is, from pre-existing MLP activations). Typically, the derived scalars consist of the
autoencoder latent activation multiplied by some other quantity.
"""
from typing import Callable
import torch
from neuron_explainer.activations.derived_scalars.derived_scalar_types import DerivedScalarType
from neuron_explainer.activations.derived_scalars.direct_effects import (
    convert_scalar_deriver_to_write_to_final_residual_grad,
)
from neuron_explainer.activations.derived_scalars.raw_activations import (
    convert_scalar_deriver_to_write_norm,
    convert_scalar_deriver_to_write_vector,
    no_op_tensor_calculate_derived_scalar_fn,
)
from neuron_explainer.activations.derived_scalars.reconstituted import make_apply_autoencoder
from neuron_explainer.activations.derived_scalars.reconstituter_class import (
    WriteLatentReconstituter,
)
from neuron_explainer.activations.derived_scalars.scalar_deriver import (
    DerivedScalarSource,
    DstConfig,
    RawScalarSource,
    ScalarDeriver,
)
from neuron_explainer.activations.derived_scalars.utils import detach_and_clone
from neuron_explainer.activations.derived_scalars.write_tensors import (
    get_autoencoder_write_tensor_by_layer_index,
    get_mlp_write_tensor_by_layer_index_with_autoencoder_context,
)
from neuron_explainer.models.autoencoder_context import AutoencoderContext
from neuron_explainer.models.model_component_registry import (
    ActivationLocationType,
    LayerIndex,
    NodeType,
    PassType,
)
def make_autoencoder_latent_scalar_deriver_factory(
    node_type: NodeType | None = None,
) -> Callable[[DstConfig], ScalarDeriver]:
    def make_autoencoder_latent_scalar_deriver(
        dst_config: DstConfig,
    ) -> ScalarDeriver:
        # import here to avoid circular import
        from neuron_explainer.activations.derived_scalars.make_scalar_derivers import (
            make_scalar_deriver,
        )
        layer_indices = dst_config.layer_indices
        if layer_indices is None:
            model_context = dst_config.get_model_context()
            layer_indices = list(range(model_context.n_layers))
        autoencoder_context = dst_config.get_autoencoder_context(node_type)
        assert autoencoder_context is not None
        autoencoder_dst = autoencoder_context.dst
        autoencoder_dst = maybe_convert_autoencoder_dst(autoencoder_dst)
        scalar_deriver = make_scalar_deriver(autoencoder_dst, dst_config)
        apply_autoencoder = make_apply_autoencoder(autoencoder_context)
        def new_tensor_calculate_derived_scalar_fn(
            derived_scalar_tensor: torch.Tensor,
            layer_index: LayerIndex,
            pass_type: PassType,
        ) -> torch.Tensor:
            assert pass_type == PassType.FORWARD
            return apply_autoencoder(derived_scalar_tensor, layer_index)
        output_dst = DerivedScalarType.AUTOENCODER_LATENT.update_from_autoencoder_node_type(
            node_type
        )
        new_scalar_deriver = scalar_deriver.apply_layerwise_transform_fn_to_output(
            layerwise_transform_fn=new_tensor_calculate_derived_scalar_fn,
            pass_type_to_transform=PassType.FORWARD,
            output_dst=output_dst,
        )
        return new_scalar_deriver
    return make_autoencoder_latent_scalar_deriver
def _make_autoencoder_latent_grad_wrt_input_scalar_deriver_helper(
    dst_config: DstConfig,
    output_dst: DerivedScalarType,
    node_type: NodeType | None = None,
) -> ScalarDeriver:
    """Compute the gradient from a particular autoencoder latent, with respect to the
    autoencoder input directions.
    Requires:
    >>> dst_config.layer_indices = [layer_index]
    >>> dst_config.trace_config.tensor_indices = ("All", latent_index)
    """
    # import here to avoid circular import
    from neuron_explainer.activations.derived_scalars.make_scalar_derivers import (
        make_scalar_deriver,
    )
    trace_config = dst_config.trace_config
    assert trace_config is not None
    assert trace_config.node_type in [
        NodeType.AUTOENCODER_LATENT,
        NodeType.MLP_AUTOENCODER_LATENT,
        NodeType.ATTENTION_AUTOENCODER_LATENT,
        NodeType.AUTOENCODER_LATENT_BY_TOKEN_PAIR,
    ]
    assert trace_config.tensor_indices[0] == "All"
    assert isinstance(trace_config.tensor_indices[1], int)
    latent_index_for_grad = trace_config.tensor_indices[1]
    layer_index = trace_config.layer_index
    assert layer_index is not None
    if dst_config.layer_indices is not None:
        assert [layer_index] == dst_config.layer_indices
    autoencoder_context = dst_config.get_autoencoder_context(node_type)
    assert autoencoder_context is not None
    autoencoder_dst = autoencoder_context.dst
    autoencoder_dst = maybe_convert_autoencoder_dst(autoencoder_dst)
    scalar_deriver = make_scalar_deriver(autoencoder_dst, dst_config)
    apply_autoencoder = make_apply_autoencoder(autoencoder_context, use_no_grad=False)
    def new_tensor_calculate_derived_scalar_fn(
        derived_scalar_tensor: torch.Tensor,
    ) -> torch.Tensor:
        derived_scalar_tensor = detach_and_clone(derived_scalar_tensor, requires_grad=True)
        latents = apply_autoencoder(derived_scalar_tensor, layer_index)
        latents[:, latent_index_for_grad].sum(dim=0).backward()  # sum over tokens
        assert derived_scalar_tensor.grad is not None
        return derived_scalar_tensor.grad
    new_scalar_deriver = scalar_deriver.apply_transform_fn_to_output(
        transform_fn=new_tensor_calculate_derived_scalar_fn,
        pass_type_to_transform=PassType.FORWARD,
        output_dst=output_dst,
    )
    return new_scalar_deriver
def make_autoencoder_latent_grad_wrt_residual_input_scalar_deriver(
    dst_config: DstConfig,
    node_type: NodeType | None = None,
) -> ScalarDeriver:
    autoencoder_context = dst_config.get_autoencoder_context(node_type)
    assert autoencoder_context is not None
    assert dst_config.trace_config is not None
    latent_index = dst_config.trace_config.node_index
    assert latent_index is not None
    latent_reconstituter = WriteLatentReconstituter(autoencoder_context)
    return latent_reconstituter.make_gradient_scalar_deriver_for_latent_index(
        latent_index=latent_index,
        dst_config=dst_config,
        output_dst=DerivedScalarType.AUTOENCODER_LATENT_GRAD_WRT_RESIDUAL_INPUT,
    )
def make_autoencoder_latent_grad_wrt_residual_input_scalar_source(
    dst_config: DstConfig,
    node_type: NodeType | None = None,
) -> DerivedScalarSource:
    autoencoder_context = dst_config.get_autoencoder_context(node_type)
    assert autoencoder_context is not None
    assert dst_config.trace_config is not None
    latent_index = dst_config.trace_config.node_index
    assert latent_index is not None
    latent_reconstituter = WriteLatentReconstituter(autoencoder_context)
    return latent_reconstituter.make_gradient_scalar_source_for_latent_index(
        latent_index=latent_index,
        dst_config=dst_config,
        output_dst=DerivedScalarType.AUTOENCODER_LATENT_GRAD_WRT_RESIDUAL_INPUT,
    )
def make_autoencoder_latent_grad_wrt_mlp_post_act_input_scalar_deriver(
    dst_config: DstConfig,
    node_type: NodeType | None = None,
) -> ScalarDeriver:
    """Output shape (n_tokens, n_neurons)"""
    autoencoder_context = dst_config.get_autoencoder_context(node_type)
    assert autoencoder_context is not None
    assert autoencoder_context.dst == DerivedScalarType.MLP_POST_ACT
    return _make_autoencoder_latent_grad_wrt_input_scalar_deriver_helper(
        dst_config,
        output_dst=DerivedScalarType.AUTOENCODER_LATENT_GRAD_WRT_MLP_POST_ACT_INPUT,
        node_type=node_type,
    )
def maybe_convert_autoencoder_dst(autoencoder_dst: DerivedScalarType) -> DerivedScalarType:
    if autoencoder_dst == DerivedScalarType.RESID_DELTA_MLP:
        # TODO: Consider removing this workaround and using RESID_DELTA_MLP directly.
        autoencoder_dst = DerivedScalarType.RESID_DELTA_MLP_FROM_MLP_POST_ACT
    return autoencoder_dst
def make_autoencoder_write_norm_scalar_deriver_factory(
    node_type: NodeType | None = None,
) -> Callable[[DstConfig], ScalarDeriver]:
    def make_autoencoder_write_norm_scalar_deriver(
        dst_config: DstConfig,
    ) -> ScalarDeriver:
        model_context = dst_config.get_model_context()
        dst = DerivedScalarType.AUTOENCODER_WRITE_NORM.update_from_autoencoder_node_type(node_type)
        autoencoder_context = dst_config.get_autoencoder_context(node_type)
        assert autoencoder_context is not None
        write_tensor_by_layer_index = get_autoencoder_write_tensor_by_layer_index(
            autoencoder_context, model_context
        )
        scalar_deriver = make_autoencoder_latent_scalar_deriver_factory(node_type)(dst_config)
        return convert_scalar_deriver_to_write_norm(
            scalar_deriver, write_tensor_by_layer_index, dst
        )
    return make_autoencoder_write_norm_scalar_deriver
def get_autoencoder_alt_from_node_type(node_type: NodeType | None) -> ActivationLocationType:
    """Get the corresponding activation_location_type from a NodeType."""
    return {
        NodeType.AUTOENCODER_LATENT: ActivationLocationType.ONLINE_AUTOENCODER_LATENT,
        NodeType.MLP_AUTOENCODER_LATENT: ActivationLocationType.ONLINE_MLP_AUTOENCODER_LATENT,
        NodeType.ATTENTION_AUTOENCODER_LATENT: ActivationLocationType.ONLINE_ATTENTION_AUTOENCODER_LATENT,
    }[node_type or NodeType.AUTOENCODER_LATENT]
def make_online_autoencoder_latent_scalar_deriver_factory(
    node_type: NodeType | None = None,
) -> Callable[[DstConfig], ScalarDeriver]:
    def make_online_autoencoder_latent_scalar_deriver(
        dst_config: DstConfig,
    ) -> ScalarDeriver:
        autoencoder_context = dst_config.get_autoencoder_context(node_type)
        assert autoencoder_context is not None
        dst = DerivedScalarType.ONLINE_AUTOENCODER_LATENT.update_from_autoencoder_node_type(
            node_type
        )
        activation_location_type = get_autoencoder_alt_from_node_type(node_type)
        return ScalarDeriver(
            dst=dst,
            dst_config=dst_config,
            sub_scalar_sources=(
                RawScalarSource(
                    activation_location_type=activation_location_type,
                    pass_type=PassType.FORWARD,
                ),
            ),
            tensor_calculate_derived_scalar_fn=no_op_tensor_calculate_derived_scalar_fn,
        )
    return make_online_autoencoder_latent_scalar_deriver
def make_online_autoencoder_write_norm_scalar_deriver_factory(
    node_type: NodeType | None = None,
) -> Callable[[DstConfig], ScalarDeriver]:
    def make_online_autoencoder_write_norm_scalar_deriver(
        dst_config: DstConfig,
    ) -> ScalarDeriver:
        model_context = dst_config.get_model_context()
        autoencoder_context = dst_config.get_autoencoder_context(node_type)
        assert autoencoder_context is not None
        dst = DerivedScalarType.ONLINE_AUTOENCODER_WRITE_NORM.update_from_autoencoder_node_type(
            node_type
        )
        write_tensor_by_layer_index = get_autoencoder_write_tensor_by_layer_index(
            autoencoder_context, model_context
        )
        scalar_deriver = make_online_autoencoder_latent_scalar_deriver_factory(node_type)(
            dst_config
        )
        return convert_scalar_deriver_to_write_norm(
            scalar_deriver, write_tensor_by_layer_index, dst
        )
    return make_online_autoencoder_write_norm_scalar_deriver
def make_online_autoencoder_latentwise_write_scalar_deriver_factory(
    node_type: NodeType | None = None,
) -> Callable[[DstConfig], ScalarDeriver]:
    def make_online_autoencoder_latentwise_write_scalar_deriver(
        dst_config: DstConfig,
    ) -> ScalarDeriver:
        model_context = dst_config.get_model_context()
        autoencoder_context = dst_config.get_autoencoder_context(node_type)
        assert autoencoder_context is not None
        dst = DerivedScalarType.ONLINE_AUTOENCODER_WRITE.update_from_autoencoder_node_type(
            node_type
        )
        write_tensor_by_layer_index = get_autoencoder_write_tensor_by_layer_index(
            autoencoder_context, model_context
        )
        scalar_deriver = make_online_autoencoder_latent_scalar_deriver_factory(node_type)(
            dst_config
        )
        return convert_scalar_deriver_to_write_vector(
            scalar_deriver, write_tensor_by_layer_index, dst
        )
    return make_online_autoencoder_latentwise_write_scalar_deriver
def make_online_autoencoder_write_to_final_residual_grad_scalar_deriver_factory(
    node_type: NodeType | None = None,
) -> Callable[[DstConfig], ScalarDeriver]:
    def make_online_autoencoder_write_to_final_residual_grad_scalar_deriver(
        dst_config: DstConfig,
    ) -> ScalarDeriver:
        scalar_deriver = make_online_autoencoder_latent_scalar_deriver_factory(node_type)(
            dst_config
        )
        dst = DerivedScalarType.ONLINE_AUTOENCODER_WRITE_TO_FINAL_RESIDUAL_GRAD.update_from_autoencoder_node_type(
            node_type
        )
        return convert_scalar_deriver_to_write_to_final_residual_grad(
            scalar_deriver, dst, use_existing_backward_pass_for_final_residual_grad=True
        )
    return make_online_autoencoder_write_to_final_residual_grad_scalar_deriver
def make_online_autoencoder_write_to_final_activation_residual_grad_scalar_deriver_factory(
    node_type: NodeType | None = None,
) -> Callable[[DstConfig], ScalarDeriver]:
    def make_online_autoencoder_write_to_final_activation_residual_grad_scalar_deriver(
        dst_config: DstConfig,
    ) -> ScalarDeriver:
        scalar_deriver = make_online_autoencoder_latent_scalar_deriver_factory(node_type)(
            dst_config
        )
        dst = DerivedScalarType.ONLINE_AUTOENCODER_WRITE_TO_FINAL_ACTIVATION_RESIDUAL_GRAD.update_from_autoencoder_node_type(
            node_type
        )
        return convert_scalar_deriver_to_write_to_final_residual_grad(
            scalar_deriver, dst, use_existing_backward_pass_for_final_residual_grad=False
        )
    return make_online_autoencoder_write_to_final_activation_residual_grad_scalar_deriver
def make_online_autoencoder_act_times_grad_scalar_deriver_factory(
    node_type: NodeType | None = None,
) -> Callable[[DstConfig], ScalarDeriver]:
    def make_online_autoencoder_act_times_grad_scalar_deriver(
        dst_config: DstConfig,
    ) -> ScalarDeriver:
        act_scalar_deriver = make_online_autoencoder_latent_scalar_deriver_factory(node_type)(
            dst_config
        )
        dst = DerivedScalarType.ONLINE_AUTOENCODER_ACT_TIMES_GRAD.update_from_autoencoder_node_type(
            node_type
        )
        activity_location_type = get_autoencoder_alt_from_node_type(node_type)
        return act_scalar_deriver.apply_layerwise_transform_fn_to_output_and_other_tensor(
            layerwise_transform_fn=lambda act, grad, layer_index, pass_type: act * grad,
            pass_type_to_transform=PassType.FORWARD,  # act
            other_scalar_source=RawScalarSource(
                activation_location_type=activity_location_type,
                pass_type=PassType.BACKWARD,  # grad
            ),
            output_dst=dst,
        )
    return make_online_autoencoder_act_times_grad_scalar_deriver
def make_online_autoencoder_error_scalar_deriver_factory(
    activation_location_type: ActivationLocationType,
) -> Callable[[DstConfig], ScalarDeriver]:
    required_node_type = {
        ActivationLocationType.ONLINE_MLP_AUTOENCODER_ERROR: NodeType.MLP_NEURON,
        ActivationLocationType.ONLINE_RESIDUAL_MLP_AUTOENCODER_ERROR: NodeType.RESIDUAL_STREAM_CHANNEL,
        ActivationLocationType.ONLINE_RESIDUAL_ATTENTION_AUTOENCODER_ERROR: NodeType.RESIDUAL_STREAM_CHANNEL,
    }[activation_location_type]
    required_autoencoder_node_type = {
        ActivationLocationType.ONLINE_MLP_AUTOENCODER_ERROR: NodeType.MLP_AUTOENCODER_LATENT,
        ActivationLocationType.ONLINE_RESIDUAL_MLP_AUTOENCODER_ERROR: NodeType.MLP_AUTOENCODER_LATENT,
        ActivationLocationType.ONLINE_RESIDUAL_ATTENTION_AUTOENCODER_ERROR: NodeType.ATTENTION_AUTOENCODER_LATENT,
    }[activation_location_type]
    dst = DerivedScalarType.from_activation_location_type(activation_location_type)
    def make_online_autoencoder_error_scalar_deriver(
        dst_config: DstConfig,
    ) -> ScalarDeriver:
        autoencoder_context = dst_config.get_autoencoder_context(required_autoencoder_node_type)
        assert autoencoder_context is not None
        assert autoencoder_context.dst.node_type == required_node_type, (
            autoencoder_context.dst,
            required_node_type,
            activation_location_type,
        )
        return ScalarDeriver(
            dst=dst,
            dst_config=dst_config,
            sub_scalar_sources=(
                RawScalarSource(
                    activation_location_type=activation_location_type,
                    pass_type=PassType.FORWARD,
                ),
            ),
            tensor_calculate_derived_scalar_fn=no_op_tensor_calculate_derived_scalar_fn,
        )
    return make_online_autoencoder_error_scalar_deriver
def make_online_mlp_autoencoder_error_act_times_grad_scalar_deriver(
    dst_config: DstConfig,
) -> ScalarDeriver:
    act_scalar_deriver = make_online_autoencoder_error_scalar_deriver_factory(
        ActivationLocationType.ONLINE_MLP_AUTOENCODER_ERROR
    )(dst_config)
    return act_scalar_deriver.apply_layerwise_transform_fn_to_output_and_other_tensor(
        layerwise_transform_fn=lambda act, grad, layer_index, pass_type: act * grad,
        pass_type_to_transform=PassType.FORWARD,  # act
        other_scalar_source=RawScalarSource(
            activation_location_type=ActivationLocationType.ONLINE_MLP_AUTOENCODER_ERROR,
            pass_type=PassType.BACKWARD,  # grad
        ),
        output_dst=DerivedScalarType.ONLINE_MLP_AUTOENCODER_ERROR_ACT_TIMES_GRAD,
    )
def make_online_mlp_autoencoder_error_write_norm_scalar_deriver(
    dst_config: DstConfig,
) -> ScalarDeriver:
    model_context = dst_config.get_model_context()
    autoencoder_context = dst_config.get_autoencoder_context(NodeType.MLP_AUTOENCODER_LATENT)
    assert autoencoder_context is not None
    write_tensor_by_layer_index = get_mlp_write_tensor_by_layer_index_with_autoencoder_context(
        autoencoder_context, model_context
    )
    scalar_deriver = make_online_autoencoder_error_scalar_deriver_factory(
        ActivationLocationType.ONLINE_MLP_AUTOENCODER_ERROR
    )(dst_config)
    return convert_scalar_deriver_to_write_norm(
        scalar_deriver,
        write_tensor_by_layer_index,
        DerivedScalarType.ONLINE_MLP_AUTOENCODER_ERROR_WRITE_NORM,
    )
def make_online_mlp_autoencoder_error_write_to_final_residual_grad_scalar_deriver(
    dst_config: DstConfig,
) -> ScalarDeriver:
    scalar_deriver = make_online_autoencoder_error_scalar_deriver_factory(
        ActivationLocationType.ONLINE_MLP_AUTOENCODER_ERROR
    )(dst_config)
    return convert_scalar_deriver_to_write_to_final_residual_grad(
        scalar_deriver,
        DerivedScalarType.ONLINE_MLP_AUTOENCODER_ERROR_WRITE_TO_FINAL_RESIDUAL_GRAD,
        use_existing_backward_pass_for_final_residual_grad=True,
    )
# helpers for autoencoder gradient
def make_autoencoder_pre_act_encoder_derivative(
    autoencoder_context: AutoencoderContext,
    layer_index: int,
    latent_index: int | None = None,
) -> Callable[[torch.Tensor], torch.Tensor]:
    autoencoder = autoencoder_context.get_autoencoder(layer_index)
    if isinstance(autoencoder.encoder, torch.nn.Linear):
        # if the encoder is linear, then the derivative is just the encoder weight matrix
        encoder = autoencoder.encoder.weight  # shape (n_latents, n_inputs)
        if latent_index is not None:
            encoder = encoder[latent_index : latent_index + 1].clone()
            # ^ need to clone to avoid MPS backend crash
        def pre_act_encoder_derivative(autoencoder_input: torch.Tensor) -> torch.Tensor:
            return autoencoder_input @ encoder.T
        return pre_act_encoder_derivative
    else:
        raise NotImplementedError("Only implemented for linear encoder for now")
def make_autoencoder_activation_fn_derivative(
    autoencoder_context: AutoencoderContext,
    layer_index: int,
) -> Callable[[torch.Tensor], torch.Tensor]:
    autoencoder = autoencoder_context.get_autoencoder(layer_index)
    if _is_relu(autoencoder.activation):
        # if the activation is ReLU, then the derivative is just a step function
        def relu_derivative(post_activations: torch.Tensor) -> torch.Tensor:
            return (post_activations > 0).to(post_activations.dtype)
        return relu_derivative
    else:
        raise NotImplementedError("Only implemented for ReLU activation function for now")
def _is_relu(activation: Callable) -> bool:
    """More robust than isinstance(activation, torch.nn.ReLU), which doesn't always work."""
    if isinstance(activation, torch.nn.ReLU):
        return True
    else:
        test_input = torch.randn(10) * 10 ** torch.randn(10)
        return torch.equal(activation(test_input), torch.nn.ReLU()(test_input))

================
File: neuron_explainer/activations/derived_scalars/config.py
================
from dataclasses import dataclass
import torch
from neuron_explainer.activations.derived_scalars.derived_scalar_types import DerivedScalarType
from neuron_explainer.activations.derived_scalars.indexing import (
    ActivationIndex,
    NodeIndex,
    TraceConfig,
)
from neuron_explainer.models.autoencoder_context import (
    AutoencoderConfig,
    AutoencoderContext,
    MultiAutoencoderContext,
)
from neuron_explainer.models.inference_engine_type_registry import InferenceEngineType
from neuron_explainer.models.model_component_registry import NodeType, PassType
from neuron_explainer.models.model_context import (
    ModelContext,
    StandardModelContext,
    get_default_device,
)
from neuron_explainer.models.transformer import Transformer
@dataclass(frozen=True)
class DstConfig:
    """Holds any information that's necessary to construct a ScalarDeriver for any
    DerivedScalarType. Note that which fields are set and used will depend on
    which DerivedScalarType is being used. For DerivedScalarType's that are 1:1 with
    HookLocationType's, all fields can be None."""
    # derive gradients specifies whether to compute the gradients of the loss function with
    # respect to the derived scalar. In general, this may require loading some combination of
    # activations and gradients from blobstore. If False, then we can get away with loading
    # fewer kinds of activations and gradients.
    derive_gradients: bool = False
    model_name: str | None = None
    """If model_name and model_context are both provided, then the model_name associated with model_context will be asserted
    to match model_name. Other than that, model_name is not used in that case (model_context overrides)."""
    inference_engine_type: InferenceEngineType = InferenceEngineType.STANDARD
    layer_indices: list[int] | None = None
    """Contains flags relevant to reading activations from blobstore (e.g. non-standard shape conventions of tensors)"""
    autoencoder_config: AutoencoderConfig | None = None
    """Contains paths to autoencoders that will be used to derive scalar values, as well as the activation location type
    which the autoencoder acts on.
    If autoencoder_config and autoencoder_context are both provided, then the autoencoder_config associated with autoencoder_context will be asserted
    to match autoencoder_config. Other than that, autoencoder_config is not used in that case (autoencoder_context overrides)."""
    model_context: ModelContext | None = None
    """Optionally, model_context can be used in place of model_name to pre-load model weights and use
    them to derive scalars. If provided, model_context will supersede model_name."""
    autoencoder_context: AutoencoderContext | None = None
    multi_autoencoder_context: MultiAutoencoderContext | None = None
    """Optionally, autoencoder_context can be used in place of autoencoder_config to pre-load autoencoders and use
    them to derive autoencoder latents."""
    trace_config: TraceConfig | None = None
    """Some DSTs depend on from what location and/or layer the backward pass was computed. "None" (the default)
    means to use some loss (a function of ActivationLocationType.LOGITS) to perform the backward pass, rather than
    some intermediate activation. This field can be used to override that default, for example by specifying
    a TraceConfig using an ActivationIndex with ActivationLocationType.MLP_POST_ACT if computing a backward pass from a particular MLP activation."""
    node_index_for_attention_write: NodeIndex | None = None
    """Some DSTs examine the direction being read from corresponding to some particular projection of an attention write vector.
    In this case, the node index of the attention write vector in question can be specified here. Note that if activation_index_for_grad
    is specified, this should be prior to the layer_index of the trace_config."""
    device_for_raw_activations: torch.device | None = None
    """In some cases, neither model_context nor autoencoder_context is provided, but we must still infer a device. This is for
    those cases"""
    activation_index_for_fake_grad: ActivationIndex | None = None
    """Some DSTs compute a "fake gradient" by running a full backward pass, computing the gradient of a loss function which is identically 0,
    but ablating the gradient corresponding to activation_index_for_fake_grad to be 1. This is useful for computing the "read direction" of a
    derived scalar even when its gradient and activation might be 0 for a particular prompt."""
    node_index_for_attribution: NodeIndex | None = None
    """Some DSTs compute attribution of edges into or out of a single node. This config option specifies which node."""
    detach_layer_norm_scale_for_attribution: bool = False
    """When computing act * grad attribution of an edge, one can choose whether to detach the layer norm scale immediately
    before the downstream node."""
    def get_device(self) -> torch.device:
        if self.model_context is not None:
            return self.model_context.device
        elif self.autoencoder_context is not None:
            return self.autoencoder_context.device
        elif self.device_for_raw_activations is not None:
            return self.device_for_raw_activations
        else:
            return get_default_device()
    def get_model_context(self) -> ModelContext:
        if self.model_context is not None:
            return self.model_context
        else:
            assert self.model_name is not None
            return ModelContext.from_model_type(
                self.model_name,
                device=get_default_device(),
                inference_engine_type=self.inference_engine_type,
            )
    def get_autoencoder_context(
        self, node_type: NodeType | None = None
    ) -> AutoencoderContext | None:
        autoencoder_context: AutoencoderContext | None = None
        if self.autoencoder_context is not None:
            autoencoder_context = self.autoencoder_context
        elif self.multi_autoencoder_context is not None:
            autoencoder_context = self.multi_autoencoder_context.get_autoencoder_context(node_type)
        elif self.autoencoder_config is not None:
            autoencoder_config = self.autoencoder_config
            assert autoencoder_config is not None
            autoencoder_context = AutoencoderContext(autoencoder_config, device=self.get_device())
        else:
            return None
        assert autoencoder_context is not None, str(node_type)
        if node_type is not None and node_type != NodeType.AUTOENCODER_LATENT:
            assert (
                node_type == autoencoder_context.autoencoder_node_type
            ), f"{node_type=} {autoencoder_context.autoencoder_node_type=}"
        return autoencoder_context
    def requires_grad_for_type(self, dst: DerivedScalarType) -> bool:
        if self.derive_gradients:
            required_pass_types = [PassType.FORWARD, PassType.BACKWARD]
        else:
            required_pass_types = [PassType.FORWARD]
        return any(
            dst.requires_grad_for_pass_type(required_pass_type)
            for required_pass_type in required_pass_types
        )
    def get_n_layers(self) -> int:
        """DstConfigs are always meant to be used on a single particular model. This
        function will error if that model has not been specified."""
        return self.get_model_context().n_layers
    def get_autoencoder_dst(self, node_type: NodeType | None = None) -> DerivedScalarType | None:
        """DstConfigs are not always used with autoencoders, so they don't need to specify
        the autoencoder. This function will return None if the autoencoder context has not
        been specified."""
        autoencoder_context = self.get_autoencoder_context(node_type)
        if autoencoder_context is None:
            return None
        else:
            return autoencoder_context.dst
    def get_or_create_model(self) -> Transformer:
        """
        Returns a transformer (only valid if a StandardModelContext was specified in the config).
        """
        model_context = self.get_model_context()
        assert isinstance(model_context, StandardModelContext)
        return model_context.get_or_create_model()
    def __post_init__(self) -> None:
        config_setting_to_device: dict[str, torch.device] = {}
        if self.model_context is not None:
            assert self.model_context.model_name == self.model_name or self.model_name is None
            config_setting_to_device["model"] = self.model_context.device
        if self.autoencoder_context is not None:
            assert (self.autoencoder_context.autoencoder_config == self.autoencoder_config) or (
                self.autoencoder_config is None
            )
            config_setting_to_device["autoencoder"] = self.autoencoder_context.device
        if self.multi_autoencoder_context is not None:
            for (
                node_type,
                autoencoder_context,
            ) in self.multi_autoencoder_context.autoencoder_context_by_node_type.items():
                config_setting_to_device[node_type] = autoencoder_context.device
        if self.device_for_raw_activations is not None:
            config_setting_to_device["raw activations"] = self.device_for_raw_activations
        if len(config_setting_to_device) > 1:
            assert (
                len(set(config_setting_to_device.values())) == 1
            ), f"All devices provided must match, but {config_setting_to_device=}"
        if self.node_index_for_attention_write is not None:
            assert self.node_index_for_attention_write.node_type == NodeType.ATTENTION_HEAD
            assert self.node_index_for_attention_write.layer_index is not None
            if (
                self.trace_config is not None
                and self.trace_config.node_type is not NodeType.VOCAB_TOKEN
            ):
                assert self.trace_config.layer_index is not None
        if self.trace_config is not None:
            # backward pass from a backward pass activation is not supported
            assert self.trace_config.pass_type == PassType.FORWARD

================
File: neuron_explainer/activations/derived_scalars/derived_scalar_store.py
================
"""
This file contains infra for storing multiple ActivationsAndMetadata objects, and computing derived
scalars from them.
The DerivedScalarStore supports functionality like applying a transformation to all the
ActivationsAndMetadata objects within it (for example, computing a max across the token dimension,
or computing the absolute value). It also supports functionality like computing the topk activations
across all the ActivationsAndMetadata objects within it, and returning the corresponding
DerivedScalarIndex objects.
"""
from dataclasses import asdict
from typing import Any, Callable
import numpy as np
import torch
from neuron_explainer.activations.derived_scalars.activations_and_metadata import (
    ActivationsAndMetadata,
    PassType,
    RawActivationStore,
    safe_sorted,
)
from neuron_explainer.activations.derived_scalars.derived_scalar_types import DerivedScalarType
from neuron_explainer.activations.derived_scalars.indexing import (
    AttentionTraceType,
    DerivedScalarIndex,
    MirroredActivationIndex,
    MirroredNodeIndex,
    NodeIndex,
)
from neuron_explainer.activations.derived_scalars.scalar_deriver import ScalarDeriver
from neuron_explainer.file_utils import CustomFileHandler
from neuron_explainer.models.model_component_registry import Dimension, NodeType
from neuron_explainer.pydantic import CamelCaseBaseModel, immutable
@immutable
class AblationSpec(CamelCaseBaseModel):
    """A specification for performing ablation on a model."""
    index: MirroredActivationIndex
    value: float
@immutable
class NodeAblation(CamelCaseBaseModel):
    """A specification for tracing an upstream node.
    This data structure is used by the client. The server converts it to an AblationSpec.
    """
    node_index: MirroredNodeIndex
    value: float
def get_topk_of_tensor_list(
    tensor_list: list[torch.Tensor],
) -> tuple[torch.Tensor, list[int], list[tuple[int, ...]]]:
    """Given a list of tensors, returns the top k values and their indices (within the overall list and within each tensor)
    across all tensors in the list.
    """
    flattened_tensor = torch.cat([tensor.flatten() for tensor in tensor_list]).flatten()
    topk_values, topk_flat_indices = torch.topk(flattened_tensor, k=flattened_tensor.shape[0])
    topk_flat_indices = topk_flat_indices.cpu().numpy()
    cumsum_list_lengths = np.cumsum([tensor.numel() for tensor in tensor_list])
    def convert_flat_index_to_list_and_tensor_indices(
        flat_index: int,
    ) -> tuple[int, tuple[int, ...]]:
        list_index = np.searchsorted(cumsum_list_lengths, flat_index, side="right")
        if list_index > 0:
            flat_index -= cumsum_list_lengths[list_index - 1]
        tensor_indices = tuple(
            int(index) for index in np.unravel_index(flat_index, tensor_list[list_index].shape)
        )  # cast from np.int64 to int
        return (
            list_index,
            tensor_indices,  # type: ignore
        )
    list_indices_tuple, tensor_indices_tuple = zip(
        *[
            convert_flat_index_to_list_and_tensor_indices(flat_index)
            for flat_index in topk_flat_indices
        ]
    )
    return (
        topk_values,
        list(list_indices_tuple),
        list(tensor_indices_tuple),
    )
@immutable
class UpstreamNodeToTrace(CamelCaseBaseModel):
    """A specification for tracing an upstream node.
    This data structure is used by the client. The server converts it to an activation index and
    an ablation spec.
    """
    node_index: MirroredNodeIndex
    attention_trace_type: AttentionTraceType | None
class DerivedScalarStore:
    """
    An object holding one or more types of derived scalars, providing convenience methods for indexing and performing
    common computations, such as top k activations. Note that this class is passed derived scalars at initialization,
    and is not intended to derive them in the first place, but can apply simple transformations to them.
    """
    activations_and_metadata_by_dst_and_pass_type: dict[
        tuple[DerivedScalarType, PassType], ActivationsAndMetadata
    ]
    def __init__(
        self,
        activations_and_metadata_by_dst_and_pass_type: dict[
            tuple[DerivedScalarType, PassType], ActivationsAndMetadata
        ],
        # this contains a dict of different derived scalars, keyed by their derived scalar type and pass type
    ):
        self.activations_and_metadata_by_dst_and_pass_type = (
            activations_and_metadata_by_dst_and_pass_type
        )
        self.sorted_layer_indices_by_dst_and_pass_type = {
            (dst, pass_type): safe_sorted(
                list(activations_and_metadata.activations_by_layer_index.keys())
            )
            for (
                dst,
                pass_type,
            ), activations_and_metadata in self.activations_and_metadata_by_dst_and_pass_type.items()
        }
        # assert that everything is on the same device
        for activations_and_metadata in self.activations_and_metadata_by_dst_and_pass_type.values():
            assert activations_and_metadata.device in [
                self.device,
                None,
            ], f"Device mismatch detected: {self.device=} {activations_and_metadata.device=}"
    @classmethod
    def from_list(
        cls, activations_and_metadata_list: list[ActivationsAndMetadata]
    ) -> "DerivedScalarStore":
        dst_and_pass_types = [
            (activations_and_metadata.dst, activations_and_metadata.pass_type)
            for activations_and_metadata in activations_and_metadata_list
        ]
        assert len(set(dst_and_pass_types)) == len(
            dst_and_pass_types
        ), "All ActivationsAndMetadata must have unique dst"
        activations_and_metadata_by_dst_and_pass_type = {
            (
                activations_and_metadata.dst,
                activations_and_metadata.pass_type,
            ): activations_and_metadata
            for activations_and_metadata in activations_and_metadata_list
        }  # activations_and_metadata objects have shape: dict by layer index, tensor: n_tokens, n_neurons or n_tokens, n_tokens, n_attn_heads
        return cls(activations_and_metadata_by_dst_and_pass_type)
    @property
    def device(self) -> torch.device | None:
        # any of the activations_and_metadata that have a non-None device are constrained to share a device
        # (see __init__). If any of the activations_and_metadata have a non-None device, this returns that device;
        # else it returns None
        device: torch.device | None = None
        for activations_and_metadata in self.activations_and_metadata_by_dst_and_pass_type.values():
            sub_device = activations_and_metadata.device
            if sub_device is not None:
                device = sub_device
                break
        return device
    def topk(
        self,
        k: int | None,
        pass_type: PassType = PassType.FORWARD,
        dsts: list[DerivedScalarType] | None = None,
        layer_indices: list[int] | None = None,
        largest: bool = True,
    ) -> tuple[torch.Tensor, list[DerivedScalarIndex]]:
        """The strategy here, similar to the topk method of ActivationsAndMetadata, is to compute
        topk for each DST separately, and then combine them. This avoids instantiating a big tensor
        containing the entire contents of the DerivedScalarStore at any point."""
        preprocessed = self
        if layer_indices is not None:
            preprocessed = preprocessed.filter_layers(layer_indices)
        if dsts is not None:
            preprocessed = preprocessed.filter_dsts(dsts)
        preprocessed = preprocessed.filter_pass_type(pass_type)
        if k is None:
            k = preprocessed.numel()
        assert k is not None
        topk_by_dst_and_pass_type = {
            (dst, pass_type): self.activations_and_metadata_by_dst_and_pass_type[
                (dst, pass_type)
            ].topk(
                k=k,
                largest=largest,
            )
            for dst in preprocessed.dsts
        }
        (
            overall_topk_values,
            overall_topk_list_indices,
            overall_topk_tensor_indices,
        ) = get_topk_of_tensor_list([topk[0] for topk in topk_by_dst_and_pass_type.values()])
        topk_ds_indices = [topk[1] for topk in topk_by_dst_and_pass_type.values()]
        overall_topk_ds_indices = []
        for list_index, tensor_indices in zip(
            overall_topk_list_indices, overall_topk_tensor_indices
        ):
            assert len(tensor_indices) == 1
            overall_topk_ds_indices.append(topk_ds_indices[list_index][tensor_indices[0]])
        return overall_topk_values, overall_topk_ds_indices
    def sum(
        self,
        node_type: NodeType | None = None,
        layer_indices: list[int] | None = None,
        dims_to_keep: list[Dimension] | None = None,
    ) -> torch.Tensor:
        # to run this function, need to have the shapes as expected (e.g. can't have previously run transform functions
        # that change shapes of activations)
        self._check_activation_ndims()
        filtered = self
        if node_type is not None:
            filtered = filtered.filter_node_types(node_type)
        if layer_indices is not None:
            filtered = filtered.filter_layers(layer_indices)
        if dims_to_keep is None:
            sum_function = torch.sum
        else:
            dsts = filtered.dsts
            shape_specs = [dst.shape_spec_per_token_sequence for dst in dsts]
            shape_spec = shape_specs[0]
            assert all(
                shape_spec == shape_spec for shape_spec in shape_specs[1:]
            ), f"Expected all shape specs to be the same, but got {shape_specs}"
            assert all(dim in shape_specs[0] for dim in dims_to_keep)
            int_dims_to_keep = {shape_spec.index(dim) for dim in dims_to_keep}
            assert len(int_dims_to_keep) == len(dims_to_keep)
            int_dims_to_discard = [
                dim for dim in range(len(shape_spec)) if dim not in int_dims_to_keep
            ]
            assert len(int_dims_to_discard) == len(shape_spec) - len(dims_to_keep)
            def sum_function(tensor: torch.Tensor) -> torch.Tensor:  # type: ignore
                if len(int_dims_to_discard) == 0:
                    # torch.sum(x, dim=[]) is the same as torch.sum(x, dim=None); our desired behavior is to sum nothing
                    return tensor
                else:
                    summed = torch.sum(tensor, dim=int_dims_to_discard)
                    assert summed.ndim == len(dims_to_keep)
                    return summed
        each_activation_summed = filtered.apply_transform_fn_to_activations(sum_function)
        total: torch.Tensor = 0.0  # type: ignore
        for (
            activations_and_metadata
        ) in each_activation_summed.activations_and_metadata_by_dst_and_pass_type.values():
            for (
                activation_total_for_layer
            ) in activations_and_metadata.activations_by_layer_index.values():
                if dims_to_keep is not None:
                    assert activation_total_for_layer.ndim == len(dims_to_keep), (
                        activation_total_for_layer.shape,
                        dims_to_keep,
                    )
                total += activation_total_for_layer
        if dims_to_keep is not None:
            assert total.ndim == len(dims_to_keep)
        return total
    def sum_abs(
        self,
        node_type: NodeType | None = None,
        layer_indices: list[int] | None = None,
        dims_to_keep: list[Dimension] | None = None,
    ) -> torch.Tensor:
        # convenience function for summing the absolute value of all activations
        return self.apply_transform_fn_to_activations(torch.abs).sum(
            node_type=node_type, layer_indices=layer_indices, dims_to_keep=dims_to_keep
        )
    def max(
        self,
        node_type: NodeType | None = None,
        layer_indices: list[int] | None = None,
    ) -> tuple[torch.Tensor, DerivedScalarIndex]:
        # convenience function for computing the maximum value of all activations
        if node_type is None:
            dsts = None
        else:
            dsts = [dst for dst in self.dsts if dst.node_type == node_type]
        values, indices = self.topk(
            k=1,
            largest=True,
            pass_type=PassType.FORWARD,
            dsts=dsts,
            layer_indices=layer_indices,
        )
        return values[0], indices[0]
    def max_abs(
        self,
        node_type: NodeType | None = None,
        layer_indices: list[int] | None = None,
    ) -> tuple[torch.Tensor, DerivedScalarIndex]:
        # convenience function for computing the maximum absolute value of all activations
        return self.apply_transform_fn_to_activations(torch.abs).max(
            node_type=node_type, layer_indices=layer_indices
        )
    def num_nonzero(
        self,
        node_type: NodeType | None = None,
        layer_indices: list[int] | None = None,
        dims_to_keep: list[Dimension] | None = None,
    ) -> torch.Tensor:
        # convenience function for counting the number of nonzero activations
        return self.apply_transform_fn_to_activations(lambda x: (x != 0).float()).sum(
            node_type=node_type, layer_indices=layer_indices, dims_to_keep=dims_to_keep
        )
    def numel(
        self,
    ) -> int:
        # convenience function for counting the number of nonzero activations
        running_total = 0
        for activations_and_metadata in self.activations_and_metadata_by_dst_and_pass_type.values():
            running_total += activations_and_metadata.numel()
        return running_total
    def apply_transform_fn_to_activations(
        self, transform_fn: Callable[[torch.Tensor], torch.Tensor]
    ) -> "DerivedScalarStore":
        return DerivedScalarStore(
            {
                (
                    dst,
                    pass_type,
                ): activations_and_metadata.apply_transform_fn_to_activations(
                    transform_fn,
                    output_dst=dst,
                    output_pass_type=pass_type,
                )
                for (
                    dst,
                    pass_type,
                ), activations_and_metadata in self.activations_and_metadata_by_dst_and_pass_type.items()
            }
        )
    def apply_transform_fn_to_multiple_activations(
        self,
        transform_fn: Callable[..., torch.Tensor],
        others: tuple["DerivedScalarStore", ...],
    ) -> "DerivedScalarStore":
        for other in others:
            assert set(self.activations_and_metadata_by_dst_and_pass_type.keys()) == set(
                other.activations_and_metadata_by_dst_and_pass_type.keys()
            )
        return DerivedScalarStore(
            {
                (
                    dst,
                    pass_type,
                ): activations_and_metadata.apply_transform_fn_to_multiple_activations(
                    transform_fn,
                    others=tuple(
                        other.activations_and_metadata_by_dst_and_pass_type[(dst, pass_type)]
                        for other in others
                    ),
                    output_dst=dst,
                    output_pass_type=pass_type,
                )
                for (
                    dst,
                    pass_type,
                ), activations_and_metadata in self.activations_and_metadata_by_dst_and_pass_type.items()
            }
        )
    def average(
        self,
        others: tuple["DerivedScalarStore", ...],
    ) -> "DerivedScalarStore":
        def average_fn(*args: torch.Tensor) -> torch.Tensor:
            return torch.mean(torch.stack(args), dim=0)
        return self.apply_transform_fn_to_multiple_activations(average_fn, others)
    def __add__(self, other: "DerivedScalarStore") -> "DerivedScalarStore":
        def add_fn(*args: torch.Tensor) -> torch.Tensor:
            return torch.sum(torch.stack(args), dim=0)
        return self.apply_transform_fn_to_multiple_activations(add_fn, (other,))
    def __sub__(self, other: "DerivedScalarStore") -> "DerivedScalarStore":
        def sub_fn(*args: torch.Tensor) -> torch.Tensor:
            return torch.sub(args[0], args[1])
        return self.apply_transform_fn_to_multiple_activations(sub_fn, (other,))
    def __eq__(self, other: Any) -> bool:
        """
        note that this uses torch.allclose, rather than checking for precise equality
        """
        if not isinstance(other, DerivedScalarStore):
            return False
        dst_and_pts = self.activations_and_metadata_by_dst_and_pass_type.keys()
        other_dst_and_pts = other.activations_and_metadata_by_dst_and_pass_type.keys()
        if not set(dst_and_pts) == set(other_dst_and_pts):
            return False
        for dst_and_pt in dst_and_pts:
            if (
                not self.activations_and_metadata_by_dst_and_pass_type[dst_and_pt]
                == other.activations_and_metadata_by_dst_and_pass_type[dst_and_pt]
            ):
                return False
        return True
    def filter_with_function(
        self, filter_fn: Callable[[DerivedScalarType, PassType], bool]
    ) -> "DerivedScalarStore":
        return DerivedScalarStore(
            {
                (
                    dst,
                    pass_type,
                ): activations_and_metadata
                for (
                    dst,
                    pass_type,
                ), activations_and_metadata in self.activations_and_metadata_by_dst_and_pass_type.items()
                if filter_fn(dst, pass_type)
            }
        )
    def filter_dsts(self, dsts: list[DerivedScalarType]) -> "DerivedScalarStore":
        def filter_fn(dst: DerivedScalarType, pass_type: PassType) -> bool:
            return dst in dsts
        return self.filter_with_function(filter_fn)
    def filter_pass_type(self, pass_type: PassType) -> "DerivedScalarStore":
        def filter_fn(dst: DerivedScalarType, pass_type: PassType) -> bool:
            return pass_type == pass_type
        return self.filter_with_function(filter_fn)
    def filter_dst_and_pass_types(
        self, dst_and_pass_types: list[tuple[DerivedScalarType, PassType]]
    ) -> "DerivedScalarStore":
        def filter_fn(dst: DerivedScalarType, pass_type: PassType) -> bool:
            return (dst, pass_type) in dst_and_pass_types
        return self.filter_with_function(filter_fn)
    def filter_node_types(self, node_type: NodeType) -> "DerivedScalarStore":
        def filter_fn(dst: DerivedScalarType, pass_type: PassType) -> bool:
            return dst.node_type == node_type
        return self.filter_with_function(filter_fn)
    def filter_layers(self, layer_indices: list[int] | None) -> "DerivedScalarStore":
        # layer_indices is None means keep all layers
        if layer_indices is None:
            return self
        else:
            return DerivedScalarStore(
                {
                    (
                        dst,
                        pass_type,
                    ): activations_and_metadata.filter_layers(layer_indices)
                    for (
                        dst,
                        pass_type,
                    ), activations_and_metadata in self.activations_and_metadata_by_dst_and_pass_type.items()
                }
            )
    @property
    def dsts(self) -> set[DerivedScalarType]:
        return {
            dst for dst, _pass_type in self.activations_and_metadata_by_dst_and_pass_type.keys()
        }
    @property
    def node_types(self) -> set[NodeType]:
        return {dst.node_type for dst in self.dsts}
    @property
    def pass_types(self) -> set[PassType]:
        return {
            pass_type
            for _dst, pass_type in self.activations_and_metadata_by_dst_and_pass_type.keys()
        }
    def __getitem__(self, key: DerivedScalarIndex | list[DerivedScalarIndex]) -> torch.Tensor:
        # indexed by index within layer_indices
        if isinstance(key, list):
            items = [self.__getitem__(k) for k in key]
            if all(isinstance(item, torch.Tensor) for item in items):
                return torch.stack(items)
            else:
                assert all(
                    isinstance(item, float) for item in items
                ), f"Expected all items to be torch tensors or floats, but got {items}"
                return torch.tensor(items)
        else:
            layer_indices = self.sorted_layer_indices_by_dst_and_pass_type[(key.dst, key.pass_type)]
            layer_index = key.layer_index
            assert (
                layer_index in layer_indices
            ), f"Layer index {layer_index} not in layer_indices {layer_indices}"
            indices_for_tensor: tuple[slice | int | None, ...] = tuple(
                slice(None) if index is None else index for index in key.tensor_indices
            )
            tensor_for_layer = self.activations_and_metadata_by_dst_and_pass_type[
                (key.dst, key.pass_type)
            ].activations_by_layer_index[layer_index]
            assert key.dst is not None
            assert len(indices_for_tensor) <= tensor_for_layer.ndim, (
                f"Too many indices for tensor of shape {tensor_for_layer.shape} "
                f"and indices {indices_for_tensor}; "
                f"{key.dst=}, {key.pass_type=}, {key.layer_index=}"
            )
            return tensor_for_layer[indices_for_tensor]
    def _check_activation_ndims(self) -> None:
        # ensure that the shapes for the activation tensors are consistent with the shape specs
        # for the derived scalar types
        for (
            dst,
            pass_type,
        ), activations_and_metadata in self.activations_and_metadata_by_dst_and_pass_type.items():
            shape_spec = dst.shape_spec_per_token_sequence
            for activations in activations_and_metadata.activations_by_layer_index.values():
                assert activations.ndim == len(shape_spec), (
                    f"Expected activations to have ndim {len(shape_spec)}, but got {activations.shape=} "
                    f"for {dst=}, {pass_type=}"
                )
    def to_dict(self) -> dict[tuple[str, str], Any]:
        return {
            (dst.value, pt.value): _convert_activations_to_string_keyed_dict(acts.clone())
            for (dst, pt), acts in self.activations_and_metadata_by_dst_and_pass_type.items()
        }
    @classmethod
    def from_dict(
        cls, dict_version: dict[tuple[str, str], Any], skip_missing_dsts: bool
    ) -> "DerivedScalarStore":
        activations_and_metadata_by_dst_and_pass_type: dict[
            tuple[DerivedScalarType, PassType], ActivationsAndMetadata
        ] = {}
        for (dst_value, pt_value), activations_dict in dict_version.items():
            # optionally ignoring missing DSTs (e.g. if we're loading a DerivedScalarStore from before some
            # DerivedScalarTypes were deleted)
            try:
                dst = DerivedScalarType(
                    dst_value
                )  # Enum __init__ is idempotent, so dst_value can be str or DerivedScalarType
            except ValueError:
                if skip_missing_dsts:
                    print(
                        "=" * 30
                        + f"WARNING: SKIPPING MISSING DST {dst_value} AT PASS TYPE {pt_value}"
                        + "=" * 30
                    )
                    continue
                else:
                    raise
            pt = PassType(pt_value)
            assert activations_dict["dst"] == dst.value
            assert activations_dict["pass_type"] == pt.value
            activations_and_metadata_by_dst_and_pass_type[
                (dst, pt)
            ] = _convert_string_keyed_dict_to_activations(activations_dict)
        return cls(activations_and_metadata_by_dst_and_pass_type)
    def save_to_file(self, path: str) -> None:
        with CustomFileHandler(path, "wb") as f:
            torch.save(
                self.to_dict(),
                f,
            )
    @classmethod
    def load_from_file(
        cls, path: str, map_location: torch.device | None = None, skip_missing_dsts: bool = False
    ) -> "DerivedScalarStore":
        with CustomFileHandler(path, "rb") as f:
            serialized_data = torch.load(f, map_location=map_location)
        return cls.from_dict(serialized_data, skip_missing_dsts)
    @classmethod
    def derive_from_raw(
        cls, raw_activation_store: RawActivationStore, scalar_derivers: list[ScalarDeriver]
    ) -> "DerivedScalarStore":
        assert len(scalar_derivers) == len(
            {scalar_deriver.dst for scalar_deriver in scalar_derivers}
        )
        activations_and_metadata_by_dst_and_pass_type: dict[
            tuple[DerivedScalarType, PassType], ActivationsAndMetadata
        ] = {}
        for scalar_deriver in scalar_derivers:
            for pass_type in scalar_deriver.derivable_pass_types:
                activations_and_metadata_by_dst_and_pass_type[
                    (scalar_deriver.dst, pass_type)
                ] = scalar_deriver.derive_from_raw(raw_activation_store, pass_type)
        return cls(activations_and_metadata_by_dst_and_pass_type)
    def get_dst_for_node_type(self, node_type: NodeType) -> DerivedScalarType:
        assert node_type in self.node_types
        dsts = [dst for dst in self.dsts if dst.node_type == node_type]
        assert len(dsts) == 1, f"Expected exactly one DST for node type {node_type}, but got {dsts}"
        return dsts[0]
    def get_ds_indices_for_node_indices(
        self, node_indices: list[NodeIndex]
    ) -> list[DerivedScalarIndex]:
        return [
            DerivedScalarIndex.from_node_index(
                node_index, self.get_dst_for_node_type(node_index.node_type)
            )
            for node_index in node_indices
        ]
    def get_derived_scalars_for_node_indices(self, node_indices: list[NodeIndex]) -> torch.Tensor:
        return self[self.get_ds_indices_for_node_indices(node_indices)]
def _convert_activations_to_string_keyed_dict(
    activations_and_metadata: ActivationsAndMetadata,
) -> dict[str, Any]:
    dict_version = asdict(activations_and_metadata)
    dict_version["dst"] = activations_and_metadata.dst.value
    dict_version["pass_type"] = activations_and_metadata.pass_type.value
    return dict_version
def _convert_string_keyed_dict_to_activations(
    dict_version: dict[str, Any],
) -> ActivationsAndMetadata:
    dict_version["dst"] = DerivedScalarType(dict_version["dst"])
    dict_version["pass_type"] = PassType(dict_version["pass_type"])
    return ActivationsAndMetadata(**dict_version)

================
File: neuron_explainer/activations/derived_scalars/derived_scalar_types.py
================
from enum import Enum
from neuron_explainer.models.model_component_registry import (
    ActivationLocationType,
    Dimension,
    LocationWithinLayer,
    NodeType,
    PassType,
    node_type_by_dimension,
)
### ENUM; ADD NEW TYPES HERE, AND ALSO IN REGISTRIES shape_spec_per_token_sequence_by_dst
# in this file and _DERIVED_SCALAR_TYPE_REGISTRY in make_scalar_derivers.py ###
# Note: activation server client libraries should be regenerated after editing this enum, which is
# mirrored in typescript. See neuron_explainer/activation_server/README.md to learn how to
# regenerate them.
class DerivedScalarType(str, Enum):
    """
    List of implemented derived location types. When implementing a new one, add a make_<dst>
    function to _DERIVED_SCALAR_TYPE_REGISTRY in make_scalar_derivers.py and add the name of the
    derived location type to this enum.
    If implementing a new HookLocationType, also add its DerivedScalarType (trivially computed from
    the activations) to this enum, and add a row like this to the registry:
    DerivedScalarType.NEW_HOOK_LOCATION_TYPE: make_scalar_deriver_factory_for_hook_location_type(
        "new_hook_location_type"
    )
    Activations of DerivedScalarTypes for a given token sequence either have one or two dimensions
    indexed by tokens:
    1. sequence_tokens: all activations have as their first dimension the number of tokens in the
       sequence
    2. attended_to_sequence_tokens: pre- and post-softmax attention, and other activations derived
       from those, have an additional token dimension, corresponding to "attended to" tokens
       (sequence_tokens being the "attended from" tokens).
    The token dimensions can in general be represented as a (num_sequence_tokens,
    num_sequence_tokens) matrix, but in some settings this might be represented as a non-square
    matrix (num_sequence_tokens != num_attended_to_sequence_tokens), e.g. if there are irrelevant
    padding tokens we wish to leave out.
    For DerivedScalarTypes not using attended_to_sequence_tokens, a
    num_attended_to_sequence_tokens=None argument still gets passed around in computing the expected
    shape of the activations. This argument gets ignored.
    """
    # correspond 1:1 with HookLocationTypes
    LOGITS = "logits"
    RESID_POST_EMBEDDING = "resid_post_embedding"
    MLP_PRE_ACT = "mlp_pre_act"
    MLP_POST_ACT = "mlp_post_act"
    RESID_DELTA_MLP = "resid_delta_mlp"
    RESID_POST_MLP = "resid_post_mlp"
    ATTN_QUERY = "attn_query"
    ATTN_KEY = "attn_key"
    ATTN_VALUE = "attn_value"
    ATTN_QK_LOGITS = "attn_qk_logits"  # uses attended_to_sequence_tokens, with 2 token dimensions
    ATTN_QK_PROBS = "attn_qk_probs"  # uses attended_to_sequence_tokens, with 2 token dimensions
    ATTN_WEIGHTED_SUM_OF_VALUES = "attn_weighted_sum_of_values"
    RESID_DELTA_ATTN = "resid_delta_attn"
    RESID_POST_ATTN = "resid_post_attn"
    RESID_FINAL_LAYER_NORM_SCALE = "resid_final_layer_norm_scale"
    ATTN_INPUT_LAYER_NORM_SCALE = "attn_input_layer_norm_scale"
    MLP_INPUT_LAYER_NORM_SCALE = "mlp_input_layer_norm_scale"
    # additional hooks
    ONLINE_AUTOENCODER_LATENT = "online_autoencoder_latent"
    # derived from HookLocationTypes
    ATTN_WRITE_NORM = "attn_write_norm"  # uses attended_to_sequence_tokens, 1 token dimension, a flattened representation of lower triangle of 2D attention matrix
    FLATTENED_ATTN_POST_SOFTMAX = "flattened_attn_post_softmax"  # uses attended_to_sequence_tokens, 1 token dimension, a flattened representation of lower triangle of 2D attention matrix
    ATTN_ACT_TIMES_GRAD = "attn_act_times_grad"  # uses attended_to_sequence_tokens, 1 token dimension, a flattened representation of lower triangle of 2D attention matrix
    RESID_DELTA_MLP_FROM_MLP_POST_ACT = "resid_delta_mlp_from_mlp_post_act"  # aka "mlp_write"
    MLP_WRITE_NORM = "mlp_write_norm"
    MLP_ACT_TIMES_GRAD = "mlp_act_times_grad"
    AUTOENCODER_LATENT = "autoencoder_latent"
    AUTOENCODER_WRITE_NORM = "autoencoder_write_norm"
    MLP_WRITE_TO_FINAL_RESIDUAL_GRAD = "mlp_write_to_final_residual_grad"
    ATTN_WRITE_NORM_PER_SEQUENCE_TOKEN = "attn_write_norm_per_sequence_token"
    ATTN_WRITE_TO_FINAL_RESIDUAL_GRAD_PER_SEQUENCE_TOKEN = (
        "attn_write_to_final_residual_grad_per_sequence_token"
    )
    ATTN_ACT_TIMES_GRAD_PER_SEQUENCE_TOKEN = "attn_act_times_grad_per_sequence_token"
    RESID_POST_EMBEDDING_NORM = "resid_post_embedding_norm"
    RESID_POST_MLP_NORM = "resid_post_mlp_norm"
    MLP_LAYER_WRITE_NORM = "mlp_layer_write_norm"  # could also be called RESID_DELTA_MLP_NORM
    RESID_POST_ATTN_NORM = "resid_post_attn_norm"
    ATTN_LAYER_WRITE_NORM = "attn_layer_write_norm"  # could also be called RESID_DELTA_ATTN_NORM
    RESID_POST_EMBEDDING_PROJ_TO_FINAL_RESIDUAL_GRAD = (
        "resid_post_embedding_proj_to_final_residual_grad"
    )
    RESID_POST_MLP_PROJ_TO_FINAL_RESIDUAL_GRAD = "resid_post_mlp_proj_to_final_residual_grad"
    MLP_LAYER_WRITE_TO_FINAL_RESIDUAL_GRAD = "mlp_layer_write_to_final_residual_grad"  # could also be called RESID_DELTA_MLP_PROJ_TO_FINAL_RESIDUAL_GRAD
    RESID_POST_ATTN_PROJ_TO_FINAL_RESIDUAL_GRAD = "resid_post_attn_proj_to_final_residual_grad"
    ATTN_LAYER_WRITE_TO_FINAL_RESIDUAL_GRAD = "attn_layer_write_to_final_residual_grad"  # could also be called RESID_DELTA_ATTN_PROJ_TO_FINAL_RESIDUAL_GRAD
    UNFLATTENED_ATTN_ACT_TIMES_GRAD = "unflattened_attn_act_times_grad"
    UNFLATTENED_ATTN_WRITE_NORM = "unflattened_attn_write_norm"
    UNFLATTENED_ATTN_WRITE_TO_FINAL_RESIDUAL_GRAD = "unflattened_attn_write_to_final_residual_grad"
    ATTN_WRITE_TO_FINAL_RESIDUAL_GRAD = "attn_write_to_final_residual_grad"
    ONLINE_AUTOENCODER_ACT_TIMES_GRAD = "online_autoencoder_act_times_grad"
    ONLINE_AUTOENCODER_WRITE_NORM = "online_autoencoder_write_norm"
    ONLINE_AUTOENCODER_WRITE_TO_FINAL_RESIDUAL_GRAD = (
        "online_autoencoder_write_to_final_residual_grad"
    )
    ONLINE_MLP_AUTOENCODER_ERROR = "online_mlp_autoencoder_error"
    ONLINE_RESIDUAL_MLP_AUTOENCODER_ERROR = "online_residual_mlp_autoencoder_error"
    ONLINE_RESIDUAL_ATTENTION_AUTOENCODER_ERROR = "online_residual_attention_autoencoder_error"
    ONLINE_MLP_AUTOENCODER_ERROR_ACT_TIMES_GRAD = "online_mlp_autoencoder_error_act_times_grad"
    ONLINE_MLP_AUTOENCODER_ERROR_WRITE_NORM = "online_mlp_autoencoder_error_write_norm"
    ONLINE_MLP_AUTOENCODER_ERROR_WRITE_TO_FINAL_RESIDUAL_GRAD = (
        "online_mlp_autoencoder_error_write_to_final_residual_grad"
    )
    ATTN_WRITE = "attn_write"
    ATTN_WRITE_SUM_HEADS = "attn_write_sum_heads"
    MLP_WRITE = "mlp_write"
    ONLINE_AUTOENCODER_WRITE = "online_autoencoder_write"
    ATTN_WEIGHTED_VALUE = "attn_weighted_value"
    PREVIOUS_LAYER_RESID_POST_MLP = "previous_layer_resid_post_mlp"
    MLP_WRITE_TO_FINAL_ACTIVATION_RESIDUAL_GRAD = "mlp_write_to_final_activation_residual_grad"
    UNFLATTENED_ATTN_WRITE_TO_FINAL_ACTIVATION_RESIDUAL_GRAD = (
        "unflattened_attn_write_to_final_activation_residual_grad"
    )
    ONLINE_AUTOENCODER_WRITE_TO_FINAL_ACTIVATION_RESIDUAL_GRAD = (
        "online_autoencoder_write_to_final_activation_residual_grad"
    )
    RESID_POST_EMBEDDING_PROJ_TO_FINAL_ACTIVATION_RESIDUAL_GRAD = (
        "resid_post_embedding_proj_to_final_activation_residual_grad"
    )
    # DSTs using gradient of autoencoder latent wrt input
    AUTOENCODER_LATENT_GRAD_WRT_RESIDUAL_INPUT = "autoencoder_latent_grad_wrt_residual_input"
    AUTOENCODER_LATENT_GRAD_WRT_MLP_POST_ACT_INPUT = (
        "autoencoder_latent_grad_wrt_mlp_post_act_input"
    )
    ATTN_WRITE_TO_LATENT = "attn_write_to_latent"
    ATTN_WRITE_TO_LATENT_SUMMED_OVER_HEADS = "attn_write_to_latent_summed_over_heads"
    FLATTENED_ATTN_WRITE_TO_LATENT_SUMMED_OVER_HEADS = (
        "flattened_attn_write_to_latent_summed_over_heads"
    )
    FLATTENED_ATTN_WRITE_TO_LATENT_SUMMED_OVER_HEADS_BATCHED = (
        "flattened_attn_write_to_latent_summed_over_heads_batched"
    )
    ATTN_WRITE_TO_LATENT_PER_SEQUENCE_TOKEN = "attn_write_to_latent_per_sequence_token"
    ATTN_WRITE_TO_LATENT_PER_SEQUENCE_TOKEN_BATCHED = (
        "attn_write_to_latent_per_sequence_token_batched"
    )
    TOKEN_ATTRIBUTION = "token_attribution"
    SINGLE_NODE_WRITE = "single_node_write"
    GRAD_OF_SINGLE_SUBNODE_ATTRIBUTION = "grad_of_single_subnode_attribution"
    ATTN_OUT_EDGE_ATTRIBUTION = "attn_out_edge_attribution"
    MLP_OUT_EDGE_ATTRIBUTION = "mlp_out_edge_attribution"
    ONLINE_AUTOENCODER_OUT_EDGE_ATTRIBUTION = "online_autoencoder_out_edge_attribution"
    ATTN_QUERY_IN_EDGE_ATTRIBUTION = "attn_query_in_edge_attribution"
    ATTN_KEY_IN_EDGE_ATTRIBUTION = "attn_key_in_edge_attribution"
    ATTN_VALUE_IN_EDGE_ATTRIBUTION = "attn_value_in_edge_attribution"
    MLP_IN_EDGE_ATTRIBUTION = "mlp_in_edge_attribution"
    ONLINE_AUTOENCODER_IN_EDGE_ATTRIBUTION = "online_autoencoder_in_edge_attribution"
    TOKEN_OUT_EDGE_ATTRIBUTION = "token_out_edge_attribution"
    SINGLE_NODE_WRITE_TO_FINAL_RESIDUAL_GRAD = "single_node_write_to_final_residual_grad"
    # compute the pre-activation of the node as if the token embedding vector were the input to the layer
    VOCAB_TOKEN_WRITE_TO_INPUT_DIRECTION = "vocab_token_write_to_input_direction"
    # this is a placeholder DST which has one value per layer and is always 1.0. It is used
    # for threading n/a values through activation server code, e.g. in the case of "activations"
    # of token/position embedding nodes.
    ALWAYS_ONE = "always_one"
    ATTN_QUERY_IN_EDGE_ACTIVATION = "attn_query_in_edge_activation"
    ATTN_KEY_IN_EDGE_ACTIVATION = "attn_key_in_edge_activation"
    MLP_IN_EDGE_ACTIVATION = "mlp_in_edge_activation"
    ONLINE_AUTOENCODER_IN_EDGE_ACTIVATION = "online_autoencoder_in_edge_activation"
    MLP_AUTOENCODER_LATENT = "mlp_autoencoder_latent"
    MLP_AUTOENCODER_WRITE_NORM = "mlp_autoencoder_write_norm"
    ONLINE_MLP_AUTOENCODER_LATENT = "online_mlp_autoencoder_latent"
    ONLINE_MLP_AUTOENCODER_WRITE = "online_mlp_autoencoder_write"
    ONLINE_MLP_AUTOENCODER_WRITE_NORM = "online_mlp_autoencoder_write_norm"
    ONLINE_MLP_AUTOENCODER_ACT_TIMES_GRAD = "online_mlp_autoencoder_act_times_grad"
    ONLINE_MLP_AUTOENCODER_WRITE_TO_FINAL_RESIDUAL_GRAD = (
        "online_mlp_autoencoder_write_to_final_residual_grad"
    )
    ONLINE_MLP_AUTOENCODER_WRITE_TO_FINAL_ACTIVATION_RESIDUAL_GRAD = (
        "online_mlp_autoencoder_write_to_final_activation_residual_grad"
    )
    ATTENTION_AUTOENCODER_LATENT = "attention_autoencoder_latent"
    ATTENTION_AUTOENCODER_WRITE_NORM = "attention_autoencoder_write_norm"
    ONLINE_ATTENTION_AUTOENCODER_LATENT = "online_attention_autoencoder_latent"
    ONLINE_ATTENTION_AUTOENCODER_WRITE = "online_attention_autoencoder_write"
    ONLINE_ATTENTION_AUTOENCODER_WRITE_NORM = "online_attention_autoencoder_write_norm"
    ONLINE_ATTENTION_AUTOENCODER_ACT_TIMES_GRAD = "online_attention_autoencoder_act_times_grad"
    ONLINE_ATTENTION_AUTOENCODER_WRITE_TO_FINAL_RESIDUAL_GRAD = (
        "online_attention_autoencoder_write_to_final_residual_grad"
    )
    ONLINE_ATTENTION_AUTOENCODER_WRITE_TO_FINAL_ACTIVATION_RESIDUAL_GRAD = (
        "online_attention_autoencoder_write_to_final_activation_residual_grad"
    )
    @property
    def is_raw_activation_type(self) -> bool:
        return self in {
            DerivedScalarType.RESID_POST_EMBEDDING,
            DerivedScalarType.RESID_DELTA_MLP,
            DerivedScalarType.RESID_POST_MLP,
            DerivedScalarType.MLP_PRE_ACT,
            DerivedScalarType.MLP_POST_ACT,
            DerivedScalarType.ATTN_QUERY,
            DerivedScalarType.ATTN_KEY,
            DerivedScalarType.ATTN_VALUE,
            DerivedScalarType.ATTN_QK_LOGITS,
            DerivedScalarType.ATTN_QK_PROBS,
            DerivedScalarType.ATTN_WEIGHTED_SUM_OF_VALUES,
            DerivedScalarType.RESID_DELTA_ATTN,
            DerivedScalarType.RESID_POST_ATTN,
            DerivedScalarType.RESID_FINAL_LAYER_NORM_SCALE,
            DerivedScalarType.ATTN_INPUT_LAYER_NORM_SCALE,
            DerivedScalarType.MLP_INPUT_LAYER_NORM_SCALE,
            DerivedScalarType.LOGITS,
        }
    @property
    def is_autoencoder_latent(self) -> bool:
        return self in {
            DerivedScalarType.AUTOENCODER_LATENT,
            DerivedScalarType.ONLINE_AUTOENCODER_LATENT,
            DerivedScalarType.MLP_AUTOENCODER_LATENT,
            DerivedScalarType.MLP_AUTOENCODER_WRITE_NORM,
            DerivedScalarType.ONLINE_MLP_AUTOENCODER_LATENT,
            DerivedScalarType.ONLINE_MLP_AUTOENCODER_WRITE,
            DerivedScalarType.ONLINE_MLP_AUTOENCODER_WRITE_NORM,
            DerivedScalarType.ONLINE_MLP_AUTOENCODER_ACT_TIMES_GRAD,
            DerivedScalarType.ONLINE_MLP_AUTOENCODER_WRITE_TO_FINAL_RESIDUAL_GRAD,
            DerivedScalarType.ONLINE_MLP_AUTOENCODER_WRITE_TO_FINAL_ACTIVATION_RESIDUAL_GRAD,
            DerivedScalarType.ATTENTION_AUTOENCODER_LATENT,
            DerivedScalarType.ATTENTION_AUTOENCODER_WRITE_NORM,
            DerivedScalarType.ONLINE_ATTENTION_AUTOENCODER_LATENT,
            DerivedScalarType.ONLINE_ATTENTION_AUTOENCODER_WRITE,
            DerivedScalarType.ONLINE_ATTENTION_AUTOENCODER_WRITE_NORM,
            DerivedScalarType.ONLINE_ATTENTION_AUTOENCODER_ACT_TIMES_GRAD,
            DerivedScalarType.ONLINE_ATTENTION_AUTOENCODER_WRITE_TO_FINAL_RESIDUAL_GRAD,
            DerivedScalarType.ONLINE_ATTENTION_AUTOENCODER_WRITE_TO_FINAL_ACTIVATION_RESIDUAL_GRAD,
        }
    def update_from_autoencoder_node_type(self, node_type: NodeType | None) -> "DerivedScalarType":
        """
        When multiple autoencoders are used, the DST needs to be specific to the autoencoder type.
        This function updates the DST to be specific to the autoencoder type:
            - NodeType.AUTOENCODER_LATENT: default autoencoder, used when no specific autoencoder type is specified
            - NodeType.MLP_AUTOENCODER_LATENT: autoencoder trained on activations from an MLP layer
            - NodeType.ATTENTION_AUTOENCODER_LATENT: autoencoder trained on activations from an Attention layer
        """
        node_type = node_type or NodeType.AUTOENCODER_LATENT
        assert node_type.is_autoencoder_latent
        new_dst_by_node_type = {
            DerivedScalarType.AUTOENCODER_LATENT: {
                NodeType.AUTOENCODER_LATENT: DerivedScalarType.AUTOENCODER_LATENT,
                NodeType.MLP_AUTOENCODER_LATENT: DerivedScalarType.MLP_AUTOENCODER_LATENT,
                NodeType.ATTENTION_AUTOENCODER_LATENT: DerivedScalarType.ATTENTION_AUTOENCODER_LATENT,
            },
            DerivedScalarType.ONLINE_AUTOENCODER_LATENT: {
                NodeType.AUTOENCODER_LATENT: DerivedScalarType.ONLINE_AUTOENCODER_LATENT,
                NodeType.MLP_AUTOENCODER_LATENT: DerivedScalarType.ONLINE_MLP_AUTOENCODER_LATENT,
                NodeType.ATTENTION_AUTOENCODER_LATENT: DerivedScalarType.ONLINE_ATTENTION_AUTOENCODER_LATENT,
            },
            DerivedScalarType.ONLINE_AUTOENCODER_ACT_TIMES_GRAD: {
                NodeType.AUTOENCODER_LATENT: DerivedScalarType.ONLINE_AUTOENCODER_ACT_TIMES_GRAD,
                NodeType.MLP_AUTOENCODER_LATENT: DerivedScalarType.ONLINE_MLP_AUTOENCODER_ACT_TIMES_GRAD,
                NodeType.ATTENTION_AUTOENCODER_LATENT: DerivedScalarType.ONLINE_ATTENTION_AUTOENCODER_ACT_TIMES_GRAD,
            },
            DerivedScalarType.ONLINE_AUTOENCODER_WRITE_TO_FINAL_RESIDUAL_GRAD: {
                NodeType.AUTOENCODER_LATENT: DerivedScalarType.ONLINE_AUTOENCODER_WRITE_TO_FINAL_RESIDUAL_GRAD,
                NodeType.MLP_AUTOENCODER_LATENT: DerivedScalarType.ONLINE_MLP_AUTOENCODER_WRITE_TO_FINAL_RESIDUAL_GRAD,
                NodeType.ATTENTION_AUTOENCODER_LATENT: DerivedScalarType.ONLINE_ATTENTION_AUTOENCODER_WRITE_TO_FINAL_RESIDUAL_GRAD,
            },
            DerivedScalarType.ONLINE_AUTOENCODER_WRITE_TO_FINAL_ACTIVATION_RESIDUAL_GRAD: {
                NodeType.AUTOENCODER_LATENT: DerivedScalarType.ONLINE_AUTOENCODER_WRITE_TO_FINAL_ACTIVATION_RESIDUAL_GRAD,
                NodeType.MLP_AUTOENCODER_LATENT: DerivedScalarType.ONLINE_MLP_AUTOENCODER_WRITE_TO_FINAL_ACTIVATION_RESIDUAL_GRAD,
                NodeType.ATTENTION_AUTOENCODER_LATENT: DerivedScalarType.ONLINE_ATTENTION_AUTOENCODER_WRITE_TO_FINAL_ACTIVATION_RESIDUAL_GRAD,
            },
            DerivedScalarType.ONLINE_AUTOENCODER_WRITE: {
                NodeType.AUTOENCODER_LATENT: DerivedScalarType.ONLINE_AUTOENCODER_WRITE,
                NodeType.MLP_AUTOENCODER_LATENT: DerivedScalarType.ONLINE_MLP_AUTOENCODER_WRITE,
                NodeType.ATTENTION_AUTOENCODER_LATENT: DerivedScalarType.ONLINE_ATTENTION_AUTOENCODER_WRITE,
            },
            DerivedScalarType.ONLINE_AUTOENCODER_WRITE_NORM: {
                NodeType.AUTOENCODER_LATENT: DerivedScalarType.ONLINE_AUTOENCODER_WRITE_NORM,
                NodeType.MLP_AUTOENCODER_LATENT: DerivedScalarType.ONLINE_MLP_AUTOENCODER_WRITE_NORM,
                NodeType.ATTENTION_AUTOENCODER_LATENT: DerivedScalarType.ONLINE_ATTENTION_AUTOENCODER_WRITE_NORM,
            },
            DerivedScalarType.AUTOENCODER_WRITE_NORM: {
                NodeType.AUTOENCODER_LATENT: DerivedScalarType.AUTOENCODER_WRITE_NORM,
                NodeType.MLP_AUTOENCODER_LATENT: DerivedScalarType.MLP_AUTOENCODER_WRITE_NORM,
                NodeType.ATTENTION_AUTOENCODER_LATENT: DerivedScalarType.ATTENTION_AUTOENCODER_WRITE_NORM,
            },
        }[self]
        return new_dst_by_node_type[node_type]
    @property
    def node_type(self) -> NodeType:
        """
        The last index of a tensor of derived scalars can correspond to a type of object in the
        network called a 'node'. This can be an MLP neuron, an attention head, an autoencoder
        latent, etc. If we don't yet have a name for the last dimension of a derived scalar type,
        this throws an error.
        """
        if self.is_autoencoder_latent:
            if "mlp" in self.value:
                return NodeType.MLP_AUTOENCODER_LATENT
            elif "attention" in self.value:
                return NodeType.ATTENTION_AUTOENCODER_LATENT
            else:
                return NodeType.AUTOENCODER_LATENT
        last_dimension = self.shape_spec_per_token_sequence[-1]
        if last_dimension in node_type_by_dimension:
            return node_type_by_dimension[last_dimension]
        else:
            raise NotImplementedError(f"Unknown node type for {last_dimension=}")
    @classmethod
    def from_activation_location_type(
        cls, activation_location_type: ActivationLocationType
    ) -> "DerivedScalarType":
        if activation_location_type.name in direct_mapping_alt_and_dst:
            return getattr(DerivedScalarType, activation_location_type.name)
        else:
            raise ValueError(
                f"{activation_location_type} does not have a corresponding DerivedScalarType"
            )
    def to_activation_location_type(self) -> ActivationLocationType:
        if self.name in direct_mapping_alt_and_dst:
            return getattr(ActivationLocationType, self.name)
        elif self == DerivedScalarType.RESID_DELTA_MLP_FROM_MLP_POST_ACT:
            return ActivationLocationType.RESID_DELTA_MLP
        else:
            raise ValueError(f"{self=} does not have a corresponding ActivationLocationType")
    @property
    def shape_spec_per_token_sequence(self) -> tuple[Dimension, ...]:
        shape_spec_per_token_sequence = shape_spec_per_token_sequence_by_dst[self]
        if self.is_raw_activation_type:
            activation_shape_spec_per_token_sequence = (
                self.to_activation_location_type().shape_spec_per_token_sequence
            )
            assert shape_spec_per_token_sequence == activation_shape_spec_per_token_sequence, (
                f"{shape_spec_per_token_sequence=} != "
                f"{activation_shape_spec_per_token_sequence=}"
            )
        return shape_spec_per_token_sequence
    @property
    def ndim_per_token_sequence(self) -> int:
        return len(self.shape_spec_per_token_sequence)
    @property
    def sequence_dim_is_sequence_token_pairs(self) -> bool:
        # If True, the sequence dimension is a flattened representation of lower triangle of 2D
        # attention matrix.
        return self in {
            DerivedScalarType.ATTN_WRITE_NORM,
            DerivedScalarType.FLATTENED_ATTN_POST_SOFTMAX,
            DerivedScalarType.ATTN_ACT_TIMES_GRAD,
        }
    @property
    def requires_grad_for_forward_pass(self) -> bool:
        return self in {
            DerivedScalarType.MLP_ACT_TIMES_GRAD,
            DerivedScalarType.MLP_WRITE_TO_FINAL_RESIDUAL_GRAD,
            DerivedScalarType.ATTN_ACT_TIMES_GRAD,
            DerivedScalarType.ATTN_WRITE_TO_FINAL_RESIDUAL_GRAD,
            DerivedScalarType.ATTN_ACT_TIMES_GRAD_PER_SEQUENCE_TOKEN,
            DerivedScalarType.ATTN_WRITE_TO_FINAL_RESIDUAL_GRAD_PER_SEQUENCE_TOKEN,
            DerivedScalarType.UNFLATTENED_ATTN_ACT_TIMES_GRAD,
            DerivedScalarType.UNFLATTENED_ATTN_WRITE_TO_FINAL_RESIDUAL_GRAD,
            DerivedScalarType.ONLINE_AUTOENCODER_ACT_TIMES_GRAD,
            DerivedScalarType.ONLINE_MLP_AUTOENCODER_ACT_TIMES_GRAD,
            DerivedScalarType.ONLINE_ATTENTION_AUTOENCODER_ACT_TIMES_GRAD,
            DerivedScalarType.ONLINE_AUTOENCODER_WRITE_TO_FINAL_RESIDUAL_GRAD,
            DerivedScalarType.ONLINE_MLP_AUTOENCODER_WRITE_TO_FINAL_RESIDUAL_GRAD,
            DerivedScalarType.ONLINE_ATTENTION_AUTOENCODER_WRITE_TO_FINAL_RESIDUAL_GRAD,
            DerivedScalarType.RESID_POST_EMBEDDING_PROJ_TO_FINAL_RESIDUAL_GRAD,
            DerivedScalarType.RESID_POST_MLP_PROJ_TO_FINAL_RESIDUAL_GRAD,
            DerivedScalarType.MLP_LAYER_WRITE_TO_FINAL_RESIDUAL_GRAD,
            DerivedScalarType.RESID_POST_ATTN_PROJ_TO_FINAL_RESIDUAL_GRAD,
            DerivedScalarType.ATTN_LAYER_WRITE_TO_FINAL_RESIDUAL_GRAD,
            DerivedScalarType.ONLINE_MLP_AUTOENCODER_ERROR_ACT_TIMES_GRAD,
            DerivedScalarType.ONLINE_MLP_AUTOENCODER_ERROR_WRITE_TO_FINAL_RESIDUAL_GRAD,
            DerivedScalarType.TOKEN_ATTRIBUTION,
            DerivedScalarType.GRAD_OF_SINGLE_SUBNODE_ATTRIBUTION,
            DerivedScalarType.ATTN_OUT_EDGE_ATTRIBUTION,
            DerivedScalarType.MLP_OUT_EDGE_ATTRIBUTION,
            DerivedScalarType.ONLINE_AUTOENCODER_OUT_EDGE_ATTRIBUTION,
            DerivedScalarType.ATTN_QUERY_IN_EDGE_ATTRIBUTION,
            DerivedScalarType.ATTN_KEY_IN_EDGE_ATTRIBUTION,
            DerivedScalarType.ATTN_VALUE_IN_EDGE_ATTRIBUTION,
            DerivedScalarType.MLP_IN_EDGE_ATTRIBUTION,
            DerivedScalarType.ONLINE_AUTOENCODER_IN_EDGE_ATTRIBUTION,
            DerivedScalarType.TOKEN_OUT_EDGE_ATTRIBUTION,
            DerivedScalarType.SINGLE_NODE_WRITE_TO_FINAL_RESIDUAL_GRAD,
        }
    def requires_grad_for_pass_type(self, pass_type: PassType) -> bool:
        if pass_type == PassType.BACKWARD:
            return True
        else:
            return self.requires_grad_for_forward_pass
    @property
    def location_within_layer(self) -> LocationWithinLayer | None:
        # returns the LocationWithinLayer if it's inferrable from the DST alone
        # None means ambiguous based on this information; must be specified by DstConfig
        if self.is_raw_activation_type:
            activation_location_type = self.to_activation_location_type()
            return activation_location_type.location_within_layer
        else:
            return self.node_type.location_within_layer
    @property
    def has_no_layers(self) -> bool:
        if self.is_raw_activation_type:
            activation_location_type = self.to_activation_location_type()
            return activation_location_type.has_no_layers
        else:
            raise NotImplementedError(
                "has_no_layers not implemented for nontrivial DSTs (i.e. with non-raw activations)"
            )
    def __repr__(self) -> str:
        return f"'{self.value}'"
# ActivationLocationType and DerivedScalarType with the same name and a direct one-to-one mapping.
# Note that the names are the same, but the values can be different, for example:
# DerivedScalarType.ATTN_QUERY = "attn_query" and ActivationLocationType.ATTN_QUERY = "attn.q"
direct_mapping_alt_and_dst = [
    "LOGITS",
    "RESID_POST_EMBEDDING",
    "MLP_PRE_ACT",
    "MLP_POST_ACT",
    "RESID_DELTA_MLP",
    "RESID_POST_MLP",
    "ATTN_QUERY",
    "ATTN_KEY",
    "ATTN_VALUE",
    "ATTN_QK_LOGITS",
    "ATTN_QK_PROBS",
    "ATTN_WEIGHTED_SUM_OF_VALUES",
    "RESID_DELTA_ATTN",
    "RESID_POST_ATTN",
    "ONLINE_AUTOENCODER_LATENT",
    "ONLINE_MLP_AUTOENCODER_LATENT",
    "ONLINE_ATTENTION_AUTOENCODER_LATENT",
    "ONLINE_MLP_AUTOENCODER_ERROR",
    "ONLINE_RESIDUAL_MLP_AUTOENCODER_ERROR",
    "ONLINE_RESIDUAL_ATTENTION_AUTOENCODER_ERROR",
    "MLP_INPUT_LAYER_NORM_SCALE",
    "ATTN_INPUT_LAYER_NORM_SCALE",
    "RESID_FINAL_LAYER_NORM_SCALE",
]
shape_spec_per_token_sequence_by_dst: dict[DerivedScalarType, tuple[Dimension, ...]] = {
    DerivedScalarType.RESID_POST_EMBEDDING: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.RESIDUAL_STREAM_CHANNELS,
    ),
    DerivedScalarType.RESID_POST_MLP: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.RESIDUAL_STREAM_CHANNELS,
    ),
    DerivedScalarType.RESID_POST_ATTN: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.RESIDUAL_STREAM_CHANNELS,
    ),
    DerivedScalarType.MLP_INPUT_LAYER_NORM_SCALE: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.SINGLETON,
    ),
    DerivedScalarType.ATTN_INPUT_LAYER_NORM_SCALE: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.SINGLETON,
    ),
    DerivedScalarType.RESID_FINAL_LAYER_NORM_SCALE: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.SINGLETON,
    ),
    DerivedScalarType.LOGITS: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.VOCAB_SIZE,
    ),
    DerivedScalarType.ATTN_QUERY: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.ATTN_HEADS,
        Dimension.QUERY_AND_KEY_CHANNELS,
    ),
    DerivedScalarType.ATTN_KEY: (
        Dimension.ATTENDED_TO_SEQUENCE_TOKENS,
        Dimension.ATTN_HEADS,
        Dimension.QUERY_AND_KEY_CHANNELS,
    ),
    DerivedScalarType.ATTN_VALUE: (
        Dimension.ATTENDED_TO_SEQUENCE_TOKENS,
        Dimension.ATTN_HEADS,
        Dimension.VALUE_CHANNELS,
    ),
    DerivedScalarType.ATTN_QK_LOGITS: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.ATTENDED_TO_SEQUENCE_TOKENS,
        Dimension.ATTN_HEADS,
    ),
    DerivedScalarType.ATTN_QK_PROBS: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.ATTENDED_TO_SEQUENCE_TOKENS,
        Dimension.ATTN_HEADS,
    ),
    DerivedScalarType.ATTN_WEIGHTED_SUM_OF_VALUES: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.ATTN_HEADS,
        Dimension.VALUE_CHANNELS,
    ),
    DerivedScalarType.RESID_DELTA_ATTN: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.RESIDUAL_STREAM_CHANNELS,
    ),
    DerivedScalarType.MLP_PRE_ACT: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.MLP_ACTS,
    ),
    DerivedScalarType.MLP_POST_ACT: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.MLP_ACTS,
    ),
    DerivedScalarType.ATTN_WRITE_NORM: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.ATTN_HEADS,
    ),
    DerivedScalarType.FLATTENED_ATTN_POST_SOFTMAX: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.ATTN_HEADS,
    ),
    DerivedScalarType.RESID_DELTA_MLP: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.RESIDUAL_STREAM_CHANNELS,
    ),
    DerivedScalarType.RESID_DELTA_MLP_FROM_MLP_POST_ACT: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.RESIDUAL_STREAM_CHANNELS,
    ),
    DerivedScalarType.ATTN_ACT_TIMES_GRAD: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.ATTN_HEADS,
    ),
    DerivedScalarType.MLP_WRITE_NORM: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.MLP_ACTS,
    ),
    DerivedScalarType.MLP_ACT_TIMES_GRAD: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.MLP_ACTS,
    ),
    DerivedScalarType.AUTOENCODER_LATENT: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.AUTOENCODER_LATENTS,
    ),
    DerivedScalarType.MLP_AUTOENCODER_LATENT: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.AUTOENCODER_LATENTS,
    ),
    DerivedScalarType.ATTENTION_AUTOENCODER_LATENT: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.AUTOENCODER_LATENTS,
    ),
    DerivedScalarType.ONLINE_AUTOENCODER_LATENT: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.AUTOENCODER_LATENTS,
    ),
    DerivedScalarType.ONLINE_MLP_AUTOENCODER_LATENT: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.AUTOENCODER_LATENTS,
    ),
    DerivedScalarType.ONLINE_ATTENTION_AUTOENCODER_LATENT: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.AUTOENCODER_LATENTS,
    ),
    DerivedScalarType.AUTOENCODER_WRITE_NORM: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.AUTOENCODER_LATENTS,
    ),
    DerivedScalarType.MLP_AUTOENCODER_WRITE_NORM: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.AUTOENCODER_LATENTS,
    ),
    DerivedScalarType.ATTENTION_AUTOENCODER_WRITE_NORM: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.AUTOENCODER_LATENTS,
    ),
    DerivedScalarType.MLP_WRITE_TO_FINAL_RESIDUAL_GRAD: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.MLP_ACTS,
    ),
    DerivedScalarType.ATTN_WRITE_NORM_PER_SEQUENCE_TOKEN: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.ATTN_HEADS,
    ),
    DerivedScalarType.ATTN_WRITE_TO_FINAL_RESIDUAL_GRAD_PER_SEQUENCE_TOKEN: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.ATTN_HEADS,
    ),
    DerivedScalarType.ATTN_ACT_TIMES_GRAD_PER_SEQUENCE_TOKEN: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.ATTN_HEADS,
    ),
    DerivedScalarType.RESID_POST_EMBEDDING_NORM: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.SINGLETON,  # denotes dim=1 always (i.e. one value per layer)
    ),
    DerivedScalarType.RESID_POST_MLP_NORM: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.SINGLETON,
    ),
    DerivedScalarType.MLP_LAYER_WRITE_NORM: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.SINGLETON,
    ),
    DerivedScalarType.RESID_POST_ATTN_NORM: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.SINGLETON,
    ),
    DerivedScalarType.ATTN_LAYER_WRITE_NORM: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.SINGLETON,
    ),
    DerivedScalarType.RESID_POST_EMBEDDING_PROJ_TO_FINAL_RESIDUAL_GRAD: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.SINGLETON,
    ),
    DerivedScalarType.RESID_POST_MLP_PROJ_TO_FINAL_RESIDUAL_GRAD: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.SINGLETON,
    ),
    DerivedScalarType.MLP_LAYER_WRITE_TO_FINAL_RESIDUAL_GRAD: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.SINGLETON,
    ),
    DerivedScalarType.RESID_POST_ATTN_PROJ_TO_FINAL_RESIDUAL_GRAD: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.SINGLETON,
    ),
    DerivedScalarType.ATTN_LAYER_WRITE_TO_FINAL_RESIDUAL_GRAD: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.SINGLETON,
    ),
    DerivedScalarType.UNFLATTENED_ATTN_ACT_TIMES_GRAD: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.ATTENDED_TO_SEQUENCE_TOKENS,
        Dimension.ATTN_HEADS,
    ),
    DerivedScalarType.UNFLATTENED_ATTN_WRITE_NORM: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.ATTENDED_TO_SEQUENCE_TOKENS,
        Dimension.ATTN_HEADS,
    ),
    DerivedScalarType.UNFLATTENED_ATTN_WRITE_TO_FINAL_RESIDUAL_GRAD: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.ATTENDED_TO_SEQUENCE_TOKENS,
        Dimension.ATTN_HEADS,
    ),
    DerivedScalarType.ATTN_WRITE_TO_FINAL_RESIDUAL_GRAD: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.ATTN_HEADS,
    ),
    DerivedScalarType.ONLINE_AUTOENCODER_ACT_TIMES_GRAD: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.AUTOENCODER_LATENTS,
    ),
    DerivedScalarType.ONLINE_MLP_AUTOENCODER_ACT_TIMES_GRAD: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.AUTOENCODER_LATENTS,
    ),
    DerivedScalarType.ONLINE_ATTENTION_AUTOENCODER_ACT_TIMES_GRAD: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.AUTOENCODER_LATENTS,
    ),
    DerivedScalarType.ONLINE_AUTOENCODER_WRITE_NORM: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.AUTOENCODER_LATENTS,
    ),
    DerivedScalarType.ONLINE_MLP_AUTOENCODER_WRITE_NORM: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.AUTOENCODER_LATENTS,
    ),
    DerivedScalarType.ONLINE_ATTENTION_AUTOENCODER_WRITE_NORM: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.AUTOENCODER_LATENTS,
    ),
    DerivedScalarType.ONLINE_AUTOENCODER_WRITE_TO_FINAL_RESIDUAL_GRAD: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.AUTOENCODER_LATENTS,
    ),
    DerivedScalarType.ONLINE_MLP_AUTOENCODER_WRITE_TO_FINAL_RESIDUAL_GRAD: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.AUTOENCODER_LATENTS,
    ),
    DerivedScalarType.ONLINE_ATTENTION_AUTOENCODER_WRITE_TO_FINAL_RESIDUAL_GRAD: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.AUTOENCODER_LATENTS,
    ),
    DerivedScalarType.ONLINE_MLP_AUTOENCODER_ERROR: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.MLP_ACTS,
    ),
    DerivedScalarType.ONLINE_RESIDUAL_MLP_AUTOENCODER_ERROR: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.RESIDUAL_STREAM_CHANNELS,
    ),
    DerivedScalarType.ONLINE_RESIDUAL_ATTENTION_AUTOENCODER_ERROR: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.RESIDUAL_STREAM_CHANNELS,
    ),
    DerivedScalarType.ONLINE_MLP_AUTOENCODER_ERROR_ACT_TIMES_GRAD: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.MLP_ACTS,
    ),
    DerivedScalarType.ONLINE_MLP_AUTOENCODER_ERROR_WRITE_NORM: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.MLP_ACTS,
    ),
    DerivedScalarType.ONLINE_MLP_AUTOENCODER_ERROR_WRITE_TO_FINAL_RESIDUAL_GRAD: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.MLP_ACTS,
    ),
    DerivedScalarType.ATTN_WRITE: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.ATTENDED_TO_SEQUENCE_TOKENS,
        Dimension.ATTN_HEADS,
        Dimension.RESIDUAL_STREAM_CHANNELS,
    ),
    DerivedScalarType.ATTN_WRITE_SUM_HEADS: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.ATTENDED_TO_SEQUENCE_TOKENS,
        Dimension.RESIDUAL_STREAM_CHANNELS,
    ),
    DerivedScalarType.MLP_WRITE: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.MLP_ACTS,
        Dimension.RESIDUAL_STREAM_CHANNELS,
    ),
    DerivedScalarType.ONLINE_AUTOENCODER_WRITE: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.AUTOENCODER_LATENTS,
        Dimension.RESIDUAL_STREAM_CHANNELS,
    ),
    DerivedScalarType.ONLINE_MLP_AUTOENCODER_WRITE: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.AUTOENCODER_LATENTS,
        Dimension.RESIDUAL_STREAM_CHANNELS,
    ),
    DerivedScalarType.ONLINE_ATTENTION_AUTOENCODER_WRITE: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.AUTOENCODER_LATENTS,
        Dimension.RESIDUAL_STREAM_CHANNELS,
    ),
    DerivedScalarType.ATTN_WEIGHTED_VALUE: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.ATTENDED_TO_SEQUENCE_TOKENS,
        Dimension.ATTN_HEADS,
        Dimension.VALUE_CHANNELS,
    ),
    DerivedScalarType.PREVIOUS_LAYER_RESID_POST_MLP: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.RESIDUAL_STREAM_CHANNELS,
    ),
    DerivedScalarType.MLP_WRITE_TO_FINAL_ACTIVATION_RESIDUAL_GRAD: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.MLP_ACTS,
    ),
    DerivedScalarType.UNFLATTENED_ATTN_WRITE_TO_FINAL_ACTIVATION_RESIDUAL_GRAD: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.ATTENDED_TO_SEQUENCE_TOKENS,
        Dimension.ATTN_HEADS,
    ),
    DerivedScalarType.ONLINE_AUTOENCODER_WRITE_TO_FINAL_ACTIVATION_RESIDUAL_GRAD: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.AUTOENCODER_LATENTS,
    ),
    DerivedScalarType.ONLINE_MLP_AUTOENCODER_WRITE_TO_FINAL_ACTIVATION_RESIDUAL_GRAD: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.AUTOENCODER_LATENTS,
    ),
    DerivedScalarType.ONLINE_ATTENTION_AUTOENCODER_WRITE_TO_FINAL_ACTIVATION_RESIDUAL_GRAD: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.AUTOENCODER_LATENTS,
    ),
    DerivedScalarType.RESID_POST_EMBEDDING_PROJ_TO_FINAL_ACTIVATION_RESIDUAL_GRAD: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.SINGLETON,
    ),
    DerivedScalarType.AUTOENCODER_LATENT_GRAD_WRT_RESIDUAL_INPUT: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.RESIDUAL_STREAM_CHANNELS,
    ),
    DerivedScalarType.AUTOENCODER_LATENT_GRAD_WRT_MLP_POST_ACT_INPUT: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.MLP_ACTS,
    ),
    DerivedScalarType.ATTN_WRITE_TO_LATENT: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.ATTENDED_TO_SEQUENCE_TOKENS,
        Dimension.ATTN_HEADS,
    ),
    DerivedScalarType.ATTN_WRITE_TO_LATENT_SUMMED_OVER_HEADS: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.ATTENDED_TO_SEQUENCE_TOKENS,
        Dimension.SINGLETON,
    ),
    DerivedScalarType.FLATTENED_ATTN_WRITE_TO_LATENT_SUMMED_OVER_HEADS: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.SINGLETON,
    ),
    DerivedScalarType.FLATTENED_ATTN_WRITE_TO_LATENT_SUMMED_OVER_HEADS_BATCHED: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.AUTOENCODER_LATENTS,
    ),
    DerivedScalarType.ATTN_WRITE_TO_LATENT_PER_SEQUENCE_TOKEN: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.ATTN_HEADS,
    ),
    DerivedScalarType.ATTN_WRITE_TO_LATENT_PER_SEQUENCE_TOKEN_BATCHED: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.ATTN_HEADS,
        Dimension.AUTOENCODER_LATENTS,
    ),
    DerivedScalarType.TOKEN_ATTRIBUTION: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.SINGLETON,
    ),
    DerivedScalarType.SINGLE_NODE_WRITE: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.RESIDUAL_STREAM_CHANNELS,
    ),
    DerivedScalarType.GRAD_OF_SINGLE_SUBNODE_ATTRIBUTION: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.RESIDUAL_STREAM_CHANNELS,
    ),
    DerivedScalarType.ATTN_OUT_EDGE_ATTRIBUTION: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.ATTENDED_TO_SEQUENCE_TOKENS,
        Dimension.ATTN_HEADS,
    ),
    DerivedScalarType.MLP_OUT_EDGE_ATTRIBUTION: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.MLP_ACTS,
    ),
    DerivedScalarType.ONLINE_AUTOENCODER_OUT_EDGE_ATTRIBUTION: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.AUTOENCODER_LATENTS,
    ),
    # Note that although the attn query is per sequence token, and not per attended to sequence
    # token, we can separately consider edges at each (sequence token, attended to sequence token)
    # pair since the edge attribution can depend on the attended to sequence token. Analogously for
    # key and value below.
    DerivedScalarType.ATTN_QUERY_IN_EDGE_ATTRIBUTION: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.ATTENDED_TO_SEQUENCE_TOKENS,
        Dimension.ATTN_HEADS,
    ),
    DerivedScalarType.ATTN_KEY_IN_EDGE_ATTRIBUTION: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.ATTENDED_TO_SEQUENCE_TOKENS,
        Dimension.ATTN_HEADS,
    ),
    DerivedScalarType.ATTN_VALUE_IN_EDGE_ATTRIBUTION: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.ATTENDED_TO_SEQUENCE_TOKENS,
        Dimension.ATTN_HEADS,
    ),
    DerivedScalarType.MLP_IN_EDGE_ATTRIBUTION: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.MLP_ACTS,
    ),
    DerivedScalarType.ONLINE_AUTOENCODER_IN_EDGE_ATTRIBUTION: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.AUTOENCODER_LATENTS,
    ),
    DerivedScalarType.TOKEN_OUT_EDGE_ATTRIBUTION: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.SINGLETON,
    ),
    DerivedScalarType.SINGLE_NODE_WRITE_TO_FINAL_RESIDUAL_GRAD: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.SINGLETON,
    ),
    DerivedScalarType.VOCAB_TOKEN_WRITE_TO_INPUT_DIRECTION: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.VOCAB_SIZE,
    ),
    DerivedScalarType.ALWAYS_ONE: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.SINGLETON,
    ),
    # Note that although the attn query is per sequence token, and not per attended to sequence
    # token, we can separately consider edges at each (sequence token, attended to sequence token
    # pair since the edge activation can depend on the attended to sequence token. Analogously for
    # key and value below.
    DerivedScalarType.ATTN_QUERY_IN_EDGE_ACTIVATION: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.ATTENDED_TO_SEQUENCE_TOKENS,
        Dimension.ATTN_HEADS,
    ),
    DerivedScalarType.ATTN_KEY_IN_EDGE_ACTIVATION: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.ATTENDED_TO_SEQUENCE_TOKENS,
        Dimension.ATTN_HEADS,
    ),
    DerivedScalarType.MLP_IN_EDGE_ACTIVATION: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.MLP_ACTS,
    ),
    DerivedScalarType.ONLINE_AUTOENCODER_IN_EDGE_ACTIVATION: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.AUTOENCODER_LATENTS,
    ),
}

================
File: neuron_explainer/activations/derived_scalars/direct_effects.py
================
"""
"Direct effects" of one node on another, or on the loss, are defined by first computing the gradient
of the downstream node's activation with respect to the residual stream immediately preceding the
downstream node. We then compute the inner product of this gradient with the write vector of the
upstream node. If the upstream node is in the residual stream basis, then it is considered to be its
own "write vector" for this purpose.
This file contains code for performing the computation described above, for upstream nodes of
various types.
"""
import dataclasses
from typing import Callable
import torch
from neuron_explainer.activations.derived_scalars.config import TraceConfig
from neuron_explainer.activations.derived_scalars.derived_scalar_types import DerivedScalarType
from neuron_explainer.activations.derived_scalars.indexing import AttnSubNodeIndex, NodeIndex
from neuron_explainer.activations.derived_scalars.locations import (
    ConstantLayerIndexer,
    get_previous_residual_dst_for_node_type,
    precedes_final_layer,
)
from neuron_explainer.activations.derived_scalars.raw_activations import (
    check_write_tensor_device_matches,
)
from neuron_explainer.activations.derived_scalars.reconstituted import make_apply_attn_V_act
from neuron_explainer.activations.derived_scalars.reconstituter_class import (
    Reconstituter,
    make_no_backward_pass_scalar_source_for_final_residual_grad,
)
from neuron_explainer.activations.derived_scalars.scalar_deriver import (
    DerivedScalarSource,
    DstConfig,
    RawScalarSource,
    ScalarDeriver,
    ScalarSource,
)
from neuron_explainer.activations.derived_scalars.write_tensors import (
    get_attn_write_tensor_by_layer_index,
    get_autoencoder_write_tensor_by_layer_index,
    get_mlp_write_tensor_by_layer_index,
)
from neuron_explainer.models.model_component_registry import (
    ActivationLocationType,
    LayerIndex,
    NodeType,
    PassType,
)
from neuron_explainer.models.model_context import ModelContext
def make_write_to_direction_tensor_fn(
    node_type: NodeType,
    write_tensor_by_layer_index: dict[LayerIndex, torch.Tensor] | dict[int, torch.Tensor] | None,
    layer_precedes_direction_layer_fn: Callable[[LayerIndex], bool],
) -> Callable[[torch.Tensor, torch.Tensor, LayerIndex, PassType], torch.Tensor]:
    """
    To convert an "activation" tensor to a "projection of write to direction" tensor, we need to convert the
    "activation" to the residual stream basis (using a write tensor) if it is not already, and then project to
    the direction of interest. This function constructs the appropriate tensor operation to perform this projection
    based on the node_type of the activation tensor (passed as an argument). The write_tensor_by_layer_index argument
    defines the conversion to the residual stream basis, and the layer_precedes_direction_layer_fn argument is assumed
    to return True iff the derived scalar at a given layer index is upstream of the direction of interest.
    """
    match node_type:
        case NodeType.RESIDUAL_STREAM_CHANNEL:
            assert write_tensor_by_layer_index is None
            def inner_product_with_residual(
                residual: torch.Tensor,
                direction: torch.Tensor,
                layer_index: LayerIndex,
                pass_type: PassType,
            ) -> torch.Tensor:  # (num_sequence_tokens, 1)
                assert pass_type == PassType.FORWARD
                assert residual.ndim == 2
                assert residual.shape == direction.shape
                if layer_precedes_direction_layer_fn(layer_index):
                    return torch.einsum("td,td->t", residual, direction)[
                        :, None
                    ]  # sum over residual stream channels
                else:
                    return torch.zeros_like(residual[:, 0:1])
            return inner_product_with_residual
        case NodeType.MLP_NEURON | NodeType.AUTOENCODER_LATENT | NodeType.MLP_AUTOENCODER_LATENT | NodeType.ATTENTION_AUTOENCODER_LATENT:
            assert write_tensor_by_layer_index is not None
            def multiply_by_projection_to_direction(
                activations: torch.Tensor,
                direction: torch.Tensor,
                layer_index: LayerIndex,
                pass_type: PassType,
            ) -> (
                torch.Tensor
            ):  # (num_sequence_tokens, num_activations [i.e. num_neurons, num_latents])
                assert layer_index is not None
                assert layer_index in write_tensor_by_layer_index
                assert pass_type == PassType.FORWARD
                assert activations.ndim == direction.ndim == 2
                if layer_precedes_direction_layer_fn(layer_index):
                    write_projection = torch.einsum(
                        "ao,to->ta", write_tensor_by_layer_index[layer_index], direction
                    )
                    return activations * write_projection
                else:
                    return torch.zeros_like(activations)
            return multiply_by_projection_to_direction
        case NodeType.V_CHANNEL:
            assert write_tensor_by_layer_index is not None
            def attn_write_to_residual_direction_tensor_calculate_derived_scalar_fn(
                attn_weighted_values: torch.Tensor,
                direction: torch.Tensor,
                layer_index: LayerIndex,
                pass_type: PassType,
            ) -> (
                torch.Tensor
            ):  # (num_sequence_tokens, num_heads) or (num_sequence_tokens, num_attended_to_sequence_tokens, num_heads)
                assert layer_index is not None
                assert layer_index in write_tensor_by_layer_index
                assert pass_type == PassType.FORWARD
                # one or two token dimensions, one head dimension, one value channel dimension
                assert attn_weighted_values.ndim in {3, 4}
                if layer_precedes_direction_layer_fn(layer_index):
                    return compute_attn_write_to_residual_direction_from_attn_weighted_values(
                        attn_weighted_values=attn_weighted_values,
                        residual_direction=direction,
                        W_O=write_tensor_by_layer_index[layer_index],
                        pass_type=pass_type,
                    )  # TODO: consider splitting into two cases, once we have separate node_types
                else:
                    return torch.zeros_like(attn_weighted_values[..., 0])  # sum over last dimension
            return attn_write_to_residual_direction_tensor_calculate_derived_scalar_fn
        case _:
            raise NotImplementedError(
                f"make_write_to_direction_tensor_fn not implemented for {node_type=}"
            )
def compute_attn_write_to_residual_direction_from_attn_weighted_values(
    attn_weighted_values: torch.Tensor,
    residual_direction: torch.Tensor,
    W_O: torch.Tensor,  # hdo
    pass_type: PassType,
) -> torch.Tensor:
    assert (
        pass_type == PassType.FORWARD
    ), "only forward pass implemented for now for attn write norm from weighted sum of values"
    if attn_weighted_values.ndim == 3:
        num_sequence_tokens, nheads, d_head = attn_weighted_values.shape
    else:
        assert attn_weighted_values.ndim == 4
        (
            num_sequence_tokens,
            num_attended_to_sequence_tokens,
            nheads,
            d_head,
        ) = attn_weighted_values.shape
    assert residual_direction.shape[0] == num_sequence_tokens
    _, d_model = residual_direction.shape
    assert W_O.shape == (nheads, d_head, d_model)
    W_O = W_O.to(residual_direction.dtype)
    Wo_projection = torch.einsum("hdo,to->thd", W_O, residual_direction)
    if attn_weighted_values.ndim == 3:
        v_times_Wo_projection = torch.einsum(
            "thd,thd->th", attn_weighted_values, Wo_projection
        )  # optionally either one or two token dimensions
    else:
        assert attn_weighted_values.ndim == 4
        v_times_Wo_projection = torch.einsum(
            "tuhd,thd->tuh", attn_weighted_values, Wo_projection
        )  # optionally either one or two token dimensions
    assert (v_times_Wo_projection.shape[0], v_times_Wo_projection.shape[-1]) == (
        num_sequence_tokens,
        nheads,
    )
    return v_times_Wo_projection
def convert_scalar_deriver_to_write_to_direction_with_write_tensor(
    scalar_deriver: ScalarDeriver,
    write_tensor_by_layer_index: dict[LayerIndex, torch.Tensor] | dict[int, torch.Tensor] | None,
    direction_scalar_source: ScalarSource,
    output_dst: DerivedScalarType,
) -> ScalarDeriver:
    """Takes as input a scalar deriver for a scalar activation fully defining a write direction
    (e.g. MLP activation or autoencoder but not post-softmax attention) and a scalar deriver for a direction
    in the residual stream basis. Multiplies each activation by its associated write vector and projects to the direction
    of interest."""
    if write_tensor_by_layer_index is not None:
        check_write_tensor_device_matches(
            scalar_deriver,
            write_tensor_by_layer_index,
        )
    def derived_scalar_precedes_direction_layer(layer_index: LayerIndex) -> bool:
        return precedes_final_layer(
            final_residual_location_within_layer=direction_scalar_source.location_within_layer,
            final_residual_layer_index=direction_scalar_source.layer_index,
            derived_scalar_location_within_layer=scalar_deriver.location_within_layer,
            derived_scalar_layer_index=layer_index,
        )
    write_to_direction_tensor_fn = make_write_to_direction_tensor_fn(
        node_type=scalar_deriver.dst.node_type,
        write_tensor_by_layer_index=write_tensor_by_layer_index,
        layer_precedes_direction_layer_fn=derived_scalar_precedes_direction_layer,
    )
    return scalar_deriver.apply_layerwise_transform_fn_to_output_and_other_tensor(
        write_to_direction_tensor_fn,
        pass_type_to_transform=PassType.FORWARD,
        output_dst=output_dst,
        other_scalar_source=direction_scalar_source,
    )
def make_final_residual_grad_scalar_source(
    dst_config: DstConfig,
    use_backward_pass: bool,
) -> ScalarSource:
    """
    Many DSTs depend on the residual stream gradient at the last point in the forward pass before the point
    from which the backward pass is run. There are two ways of deriving this residual stream gradient.
    Background on backward passes:
    By default, the backward pass is run starting from some scalar function of the transformer's
    output logits. In this case, the last relevant point in the forward pass is at the very last
    residual stream location in the network (pre- final layer norm).
    A backward pass can also be run from an arbitrary activation in the network. In this case, the
    last relevant point in the forward pass is at the residual stream location immediately preceding
    the layer index of the activation from which the backward pass is run (pre- layer norm for that
    layer).
    The DstConfig object specifies whether the backward pass is the default (trace_config=None)
    or from an activation (trace_config=TraceConfig(node_index=NodeIndex(),...)).
    Note that if all you care about for a particular DST is the gradient at the last point in the forward pass
    (i.e. the first point in the backward pass), then running the full backward pass is actually wasteful.
    If you need to compute gradients with respect to many different activations, it's best just to run the very
    first part of the backward pass if possible. This is what use_backward_pass=False does.
    Two ways of deriving the residual stream gradient:
     - use_backward_pass=True: assume a literal backward pass has been run, outside the DST setup, as specified
        by the DstConfig object. In this case, you can directly use the "raw" residual stream gradient at a location
        inferrable from dst_config.trace_config.
     - use_backward_pass=False: (specific to the case where trace_config is not None)
        do not make assumptions about the literal backward pass that has been run. Take
        the residual stream **activations** (the forward pass) at the location implied by
        dst_config.trace_config. Recompute the activation specified from those residual
        stream activations, and run a small backward pass on the activation, back to those residual stream
        activations.
    """
    if use_backward_pass:
        return make_backward_pass_scalar_source_for_final_residual_grad(dst_config)
    else:
        return make_no_backward_pass_scalar_source_for_final_residual_grad(dst_config)
def make_backward_pass_scalar_source_for_final_residual_grad(
    dst_config: DstConfig,
) -> ScalarSource:
    """Called by other make_scalar_deriver functions; not needed as a derived scalar on its own.
    Note that the dst_config is not used for the (temporary) ScalarDeriver that is returned
    by this function. This determines the config needed for a final_residual_grad scalar deriver, based on
    the config of the scalar deriver for the activation which will be multiplied by the final residual grad.
    """
    if (
        dst_config.trace_config is not None
        and dst_config.trace_config.node_type.is_autoencoder_latent
    ):
        autoencoder_dst = dst_config.get_autoencoder_dst(dst_config.trace_config.node_type)
    else:
        autoencoder_dst = None
    return make_backward_pass_scalar_source_for_final_residual_grad_helper(
        n_layers=dst_config.get_n_layers(),
        trace_config=dst_config.trace_config,
        autoencoder_dst=autoencoder_dst,
    )
def make_backward_pass_scalar_source_for_fake_final_residual_grad(
    dst_config: DstConfig,
) -> ScalarSource:
    """Called by other make_scalar_deriver functions; not needed as a derived scalar on its own.
    Note that the dst_config is not used for the (temporary) ScalarDeriver that is returned
    by this function. This determines the config needed for a final_fake_residual_grad scalar deriver, based on
    the config of the scalar deriver for the activation which will be multiplied by the final fake residual grad.
    The gradient is "fake" in the sense that a real backward pass is run from a later point in the network, but the
    gradient is assumed to be ablated such that a real gradient of interest can be computed at the residual stream
    immediately preceding the layer_index of dst_config.activation_index_for_fake_grad.
    """
    assert dst_config.activation_index_for_fake_grad is not None
    if (
        dst_config.trace_config is not None
        and dst_config.trace_config.node_type.is_autoencoder_latent
    ):
        autoencoder_dst = dst_config.get_autoencoder_dst(dst_config.trace_config.node_type)
    else:
        autoencoder_dst = None
    return make_backward_pass_scalar_source_for_final_residual_grad_helper(
        n_layers=dst_config.get_n_layers(),
        trace_config=TraceConfig.from_activation_index(
            activation_index=dst_config.activation_index_for_fake_grad
        ),
        autoencoder_dst=autoencoder_dst,
    )
def make_backward_pass_scalar_source_for_final_residual_grad_helper(
    n_layers: int,  # total layers in model
    trace_config: TraceConfig | None,
    autoencoder_dst: DerivedScalarType | None,
) -> ScalarSource:
    """
    Returns the location of the last residual stream location prior to the layer norm preceding the location from
    which .backward() is being computed
    """
    # lazily avoid circular import
    from neuron_explainer.activations.derived_scalars.make_scalar_derivers import (
        make_scalar_deriver,
    )
    if trace_config is None:
        return RawScalarSource(
            activation_location_type=ActivationLocationType.RESID_POST_MLP,
            pass_type=PassType.BACKWARD,
            layer_indexer=ConstantLayerIndexer(n_layers - 1),
        )
    else:
        layer_index = trace_config.layer_index
        assert layer_index is not None
        residual_dst = get_previous_residual_dst_for_node_type(
            node_type=trace_config.node_type,
            autoencoder_dst=autoencoder_dst,
        )
        return DerivedScalarSource(
            scalar_deriver=make_scalar_deriver(
                residual_dst, DstConfig(layer_indices=[layer_index], derive_gradients=True)
            ),
            pass_type=PassType.BACKWARD,
            layer_indexer=ConstantLayerIndexer(layer_index),
        )
def convert_scalar_deriver_to_write_to_final_residual_grad(
    scalar_deriver: ScalarDeriver,
    output_dst: DerivedScalarType,
    use_existing_backward_pass_for_final_residual_grad: bool,
) -> ScalarDeriver:
    direction_scalar_source = make_final_residual_grad_scalar_source(
        scalar_deriver.dst_config, use_existing_backward_pass_for_final_residual_grad
    )
    return convert_scalar_deriver_to_write_to_direction(
        scalar_deriver=scalar_deriver,
        direction_scalar_source=direction_scalar_source,
        output_dst=output_dst,
    )
def convert_scalar_deriver_to_write_to_direction(
    scalar_deriver: ScalarDeriver,
    direction_scalar_source: ScalarSource,
    output_dst: DerivedScalarType,
) -> ScalarDeriver:
    model_context = scalar_deriver.dst_config.get_model_context()
    layer_indices = scalar_deriver.dst_config.layer_indices or list(range(model_context.n_layers))
    node_type = scalar_deriver.dst.node_type
    match node_type:
        case NodeType.RESIDUAL_STREAM_CHANNEL:
            write_tensor_by_layer_index: dict[LayerIndex, torch.Tensor] | None = None
        case NodeType.MLP_NEURON:
            write_tensor_by_layer_index = get_mlp_write_tensor_by_layer_index(
                model_context=model_context,
                layer_indices=layer_indices,
            )
        case NodeType.V_CHANNEL:
            write_tensor_by_layer_index = get_attn_write_tensor_by_layer_index(
                model_context=model_context,
                layer_indices=layer_indices,
            )
        case (
            NodeType.AUTOENCODER_LATENT
            | NodeType.MLP_AUTOENCODER_LATENT
            | NodeType.ATTENTION_AUTOENCODER_LATENT
        ):
            autoencoder_context = scalar_deriver.dst_config.get_autoencoder_context(node_type)
            assert autoencoder_context is not None
            write_tensor_by_layer_index = get_autoencoder_write_tensor_by_layer_index(
                model_context=model_context,
                autoencoder_context=autoencoder_context,
            )
        case _:
            raise NotImplementedError(
                f"convert_scalar_deriver_to_write_to_direction not implemented for {node_type=}"
            )
    return convert_scalar_deriver_to_write_to_direction_with_write_tensor(
        scalar_deriver=scalar_deriver,
        write_tensor_by_layer_index=write_tensor_by_layer_index,
        direction_scalar_source=direction_scalar_source,
        output_dst=output_dst,
    )
def make_reconstituted_attention_direct_effect_fn(
    model_context: ModelContext,
    layer_indices: list[int] | None,
    detach_layer_norm_scale: bool,
) -> Callable[[torch.Tensor, torch.Tensor, LayerIndex, PassType], torch.Tensor]:
    apply_attn_V_act = make_apply_attn_V_act(
        transformer=model_context.get_or_create_model(),
        q_k_or_v=ActivationLocationType.ATTN_VALUE,
        detach_layer_norm_scale=detach_layer_norm_scale,
    )
    write_tensor_by_layer_index = get_attn_write_tensor_by_layer_index(
        model_context=model_context,
        layer_indices=layer_indices,
    )
    def direct_effect_fn(
        resid: torch.Tensor,
        grad: torch.Tensor,
        layer_index: LayerIndex,
        pass_type: PassType,
    ) -> torch.Tensor:
        # grad is a d_model-dimensional vector
        attn, V = apply_attn_V_act(resid, layer_index, pass_type)
        attn_weighted_V = torch.einsum("qkh,khd->qkhd", attn, V)
        grad_proj_to_V = torch.einsum("hdo,qo->qhd", write_tensor_by_layer_index[layer_index], grad)
        # grad is w/r/t (attn_weighted_V summed over k, or ATTN_WEIGHTED_SUM_OF_VALUES)
        return torch.einsum("qkhd,qhd->qkh", attn_weighted_V, grad_proj_to_V)
    return direct_effect_fn
class AttentionDirectEffectReconstituter(Reconstituter):
    """Reconstitute an attention head's write to a particular direction"""
    requires_other_scalar_source = True
    node_type = NodeType.ATTENTION_HEAD
    def __init__(
        self,
        model_context: ModelContext,
        layer_indices: list[int] | None,
        detach_layer_norm_scale: bool,
    ):
        super().__init__()
        self._reconstitute_activations_fn = make_reconstituted_attention_direct_effect_fn(
            model_context=model_context,
            layer_indices=layer_indices,
            detach_layer_norm_scale=detach_layer_norm_scale,
        )
        self._layer_indices = layer_indices
        self.residual_dst = get_previous_residual_dst_for_node_type(
            node_type=self.node_type,
            autoencoder_dst=None,
        )
    def reconstitute_activations(
        self,
        resid: torch.Tensor,
        grad: torch.Tensor | None,
        layer_index: LayerIndex,
        pass_type: PassType,
    ) -> torch.Tensor:
        assert pass_type == PassType.FORWARD
        assert grad is not None
        return self._reconstitute_activations_fn(
            resid,
            grad,
            layer_index,
            pass_type,
        )
    def make_other_scalar_source(self, dst_config: DstConfig) -> ScalarSource:
        # make_backward_pass_scalar_source_for_final_residual_grad
        # does not use most of the fields of dst_config; just
        # get_n_layers(), get_autoencoder_dst(), and trace_config
        return make_backward_pass_scalar_source_for_final_residual_grad(dst_config)
    def _check_node_index(self, node_index: NodeIndex) -> None:
        assert node_index.node_type == self.node_type
        assert node_index.pass_type == PassType.FORWARD
        assert node_index.layer_index is not None
        # self._layer_indices = None -> support all layer_indices; otherwise only a subset
        # of layer indices are loaded
        assert self._layer_indices is None or node_index.layer_index in self._layer_indices
        if isinstance(node_index, AttnSubNodeIndex):
            assert node_index.q_k_or_v == ActivationLocationType.ATTN_VALUE
    def make_scalar_hook_for_node_index(
        self, node_index: NodeIndex
    ) -> Callable[[torch.Tensor], torch.Tensor]:
        self._check_node_index(node_index)
        assert node_index.ndim == 0
        def get_activation_from_layer_activations(layer_activations: torch.Tensor) -> torch.Tensor:
            return layer_activations[node_index.tensor_indices]
        return get_activation_from_layer_activations
    def make_gradient_scalar_deriver_for_node_index(
        self,
        node_index: NodeIndex,
        dst_config: DstConfig,
        output_dst: DerivedScalarType | None = None,
    ) -> ScalarDeriver:
        self._check_node_index(node_index)
        assert node_index.layer_index is not None
        dst_config_for_layer = dataclasses.replace(
            dst_config,
            layer_indices=[node_index.layer_index],
        )
        scalar_hook = self.make_scalar_hook_for_node_index(node_index)
        return self.make_gradient_scalar_deriver(
            scalar_hook=scalar_hook,
            dst_config=dst_config_for_layer,
            output_dst=output_dst,
        )
    def make_gradient_scalar_source_for_node_index(
        self,
        node_index: NodeIndex,
        dst_config: DstConfig,
        output_dst: DerivedScalarType | None = None,
    ) -> DerivedScalarSource:
        scalar_hook = self.make_scalar_hook_for_node_index(node_index)
        gradient_scalar_deriver = self.make_gradient_scalar_deriver(
            scalar_hook=scalar_hook,
            dst_config=dst_config,
            output_dst=output_dst,
        )
        assert node_index.layer_index is not None
        return DerivedScalarSource(
            scalar_deriver=gradient_scalar_deriver,
            pass_type=PassType.FORWARD,
            layer_indexer=ConstantLayerIndexer(node_index.layer_index),
        )

================
File: neuron_explainer/activations/derived_scalars/edge_activation.py
================
"""This file defines ScalarDerivers for efficiently computing the direct effect of a single upstream node
on many downstream nodes."""
from typing import Callable
from neuron_explainer.activations.derived_scalars.derived_scalar_types import DerivedScalarType
from neuron_explainer.activations.derived_scalars.node_write import make_node_write_scalar_source
from neuron_explainer.activations.derived_scalars.reconstituter_class import ActivationReconstituter
from neuron_explainer.activations.derived_scalars.scalar_deriver import (
    DstConfig,
    ScalarDeriver,
    ScalarSource,
)
from neuron_explainer.models.model_component_registry import ActivationLocationType
from neuron_explainer.models.model_context import StandardModelContext
def convert_node_write_scalar_deriver_to_in_edge_activation(
    node_write_scalar_source: ScalarSource,
    output_dst: DerivedScalarType,
    dst_config: DstConfig,
    downstream_activation_location_type: ActivationLocationType,
    downstream_q_or_k: ActivationLocationType | None,
) -> ScalarDeriver:
    """Converts a scalar deriver for a write vector from some upstream node type to a scalar deriver for
    in edge activation for downstream nodes of some type (MLP, autoencoder, or attention head). In the
    case of attention heads, this is split up by subnode (Q or K)."""
    model_context = dst_config.get_model_context()
    autoencoder_context = dst_config.get_autoencoder_context()
    assert isinstance(model_context, StandardModelContext)
    transformer = model_context.get_or_create_model()
    reconstituter = ActivationReconstituter.from_activation_location_type(
        transformer=transformer,
        autoencoder_context=autoencoder_context,
        activation_location_type=downstream_activation_location_type,
        q_or_k=downstream_q_or_k,
    )
    return reconstituter.make_jvp_scalar_deriver(
        write_scalar_source=node_write_scalar_source,
        dst_config=dst_config,
        output_dst=output_dst,
    )
def make_in_edge_activation_scalar_deriver_factory(
    activation_location_type: ActivationLocationType,
    q_or_k: ActivationLocationType | None = None,
) -> Callable[[DstConfig], ScalarDeriver]:
    """Returns a function that creates a scalar deriver for the edge attribution from arbitrary node
    to the specified downstream activation location type / sub activation location type (MLP post act,
    autoencoder latent, attention head Q or K).
    """
    sub_node_type_to_output_dst = {
        (ActivationLocationType.MLP_POST_ACT, None): DerivedScalarType.MLP_IN_EDGE_ACTIVATION,
        (
            ActivationLocationType.ONLINE_AUTOENCODER_LATENT,
            None,
        ): DerivedScalarType.ONLINE_AUTOENCODER_IN_EDGE_ACTIVATION,
        (
            ActivationLocationType.ATTN_QK_PROBS,
            ActivationLocationType.ATTN_QUERY,
        ): DerivedScalarType.ATTN_QUERY_IN_EDGE_ACTIVATION,
        (
            ActivationLocationType.ATTN_QK_PROBS,
            ActivationLocationType.ATTN_KEY,
        ): DerivedScalarType.ATTN_KEY_IN_EDGE_ACTIVATION,
    }
    output_dst = sub_node_type_to_output_dst[(activation_location_type, q_or_k)]
    def make_in_edge_activation_scalar_deriver(dst_config: DstConfig) -> ScalarDeriver:
        node_write_scalar_source = make_node_write_scalar_source(dst_config)
        return convert_node_write_scalar_deriver_to_in_edge_activation(
            node_write_scalar_source=node_write_scalar_source,
            output_dst=output_dst,
            dst_config=dst_config,
            downstream_activation_location_type=activation_location_type,
            downstream_q_or_k=q_or_k,
        )
    return make_in_edge_activation_scalar_deriver

================
File: neuron_explainer/activations/derived_scalars/edge_attribution.py
================
"""
This file contains functions for computing the importance of edges in a transformer computation
graph.
Edges are taken to go from an upstream node (defined to be an MLP neuron, autoencoder latent, or
attention head at a specific token or token pair) to a downstream "subnode" (defined to be an MLP
neuron, autoencoder latent, or attention head Q, K, or V at a specific token or token pair).
Notice that we consider Q, K, V subnodes for attention separately for the downstream partner of the
edge, but lump them together for the upstream partner.
Note that the inputs to an attention head node are specified as either Q, K, or V-mediated, while
the outputs are specified merely as originating from the attention head node.
'Importance' of an edge is defined using act * grad, or 'attribution'.
See eq. 2 of https://arxiv.org/pdf/2310.10348.pdf for more context, but here briefly:
The attribution of an edge can be computed as:
dLoss/d"EdgeActivation" * "EdgeActivation"
 = dLoss/dDownstreamSubNodeActivation * DownstreamSubNodeActivation/UpstreamNodeActivation * UpstreamNodeActivation
 = dLoss/dDownstreamSubNodeActivation * dDownstreamSubNodeActivation/dResidualStream * UpstreamNodeWriteToResidualStream
where the Write terms indicate (d_model,) vectors per token or per token pair, dX/dY indicates the
total derivative of X with respect to Y, and X/Y indicates the partial derivative. "Total
derivatives" are also known as "gradients", while "partial derivatives" are also known as "direct
writes" to gradient directions (i.e. dLoss/dDownstreamSubNodeActivation is the gradient at the
downstream node, while DownstreamSubNodeActivation/UpstreamNodeActivation is the "direct write"
from the upstream to the downstream node). "EdgeActivation" is the considered to be the direct
effect of the upstream node's activation being patched to the downstream subnode's input. The
"Activation" of a node or subnode is considered to be any sufficient statistic for determining that
node's effect on downstream model components (e.g. the activation of a single MLP neuron, or all
d_head channels of an attention query at a particular pair of tokens). "ResidualStream" refers to
the residual stream just before the downstream subnode in question (edges correspond to direct
writes between nodes).
The strategy within this file is:
1. construct a function to compute
[dLoss/dDownstreamSubNodeActivation * dDownstreamSubNodeActivation(ResidualStream)](ResidualStream)
:=DownstreamSubNodeAttribution(ResidualStream)
with a stopgrad on the dLoss/dDownstreamSubNodeActivation term, for ONE OR MORE downstream subnodes.
This is flexible for use with one or more downstream subnodes to support reuse in two contexts: many
upstream to one downstream node, or one upstream to many downstream nodes.
(make_reconstituted_attribution_fn, which is used to construct AttributionReconstituter)
### MANY-UPSTREAM-TO-ONE-DOWNSTREAM CASE ###
2. construct a function to compute dDownstreamSubNodeAttribution/dResidualStream for JUST ONE
downstream subnode (using AttributionReconstituter)
3. construct a ScalarDeriver by taking the inner product of MANY upstream nodes' write vectors with
the gradient of the attribution of ONE downstream subnode
(convert_scalar_deriver_to_out_edge_attribution)
### ONE-UPSTREAM-TO-MANY-DOWNSTREAM CASE ###
4. construct a function to compute dDownstreamSubNodeAttribution/dResidualStream * WriteVector for
MANY downstream subnodes and ONE upstream node (using AttributionReconstituter)
5. construct a ScalarDeriver for the write vector of a single upstream node (note that this write
vector is per token, even if the upstream node is per token pair; in this case it will be the
contribution of just one (sequence token, attended to token) pair to the sequence token)
(make_node_write_scalar_deriver)
6. convert this ScalarDeriver to a ScalarDeriver for the in edge attribution of many downstream
subnodes, originating from one upstream node
(convert_node_write_scalar_deriver_to_in_edge_attribution,
make_in_edge_attribution_scalar_deriver_factory)
An AttributionReconstituter object is used to reconstruct the attribution of the downstream node(s).
The attribution of the edge is computed by taking derivatives of the downstream node attribution
with respect to the residual stream (where derivatives are either gradients, in the case where there
is one downstream node, or Jacobians, in the case where there are many downstream nodes).
"""
import dataclasses
from typing import Callable
import torch
from neuron_explainer.activations.derived_scalars.attention import (
    make_attn_weighted_value_scalar_deriver,
)
from neuron_explainer.activations.derived_scalars.autoencoder import (
    make_online_autoencoder_latent_scalar_deriver_factory,
)
from neuron_explainer.activations.derived_scalars.derived_scalar_types import DerivedScalarType
from neuron_explainer.activations.derived_scalars.direct_effects import (
    convert_scalar_deriver_to_write_to_direction,
    convert_scalar_deriver_to_write_to_final_residual_grad,
)
from neuron_explainer.activations.derived_scalars.indexing import (
    AttentionTraceType,
    AttnSubNodeIndex,
    NodeIndex,
    PreOrPostAct,
)
from neuron_explainer.activations.derived_scalars.locations import (
    ConstantLayerIndexer,
    IdentityLayerIndexer,
    get_previous_residual_dst_for_node_type,
)
from neuron_explainer.activations.derived_scalars.mlp import get_base_mlp_scalar_deriver
from neuron_explainer.activations.derived_scalars.node_write import (
    make_node_write_scalar_deriver,
    make_node_write_scalar_source,
)
from neuron_explainer.activations.derived_scalars.raw_activations import (
    make_scalar_deriver_factory_for_activation_location_type,
)
from neuron_explainer.activations.derived_scalars.reconstituted import (
    make_apply_attn_V_act,
    make_reconstituted_activation_fn,
)
from neuron_explainer.activations.derived_scalars.reconstituter_class import Reconstituter
from neuron_explainer.activations.derived_scalars.scalar_deriver import (
    DerivedScalarSource,
    DstConfig,
    RawScalarSource,
    ScalarDeriver,
    ScalarSource,
)
from neuron_explainer.models.autoencoder_context import AutoencoderContext
from neuron_explainer.models.model_component_registry import (
    ActivationLocationType,
    LayerIndex,
    NodeType,
    PassType,
)
from neuron_explainer.models.model_context import StandardModelContext
from neuron_explainer.models.transformer import Transformer
def get_activation_location_type_for_node_type(
    node_type: NodeType, q_k_or_v: ActivationLocationType | None
) -> ActivationLocationType:
    """This returns the activation location associated with a node of a given type, and
    specifying Q, K, or V in the case of attention. This returns an activation location type
    that is sufficient to determine that node's effect on the residual stream (post-softmax
    in the case of Q, K; ATTN_WEIGHTED_SUM_OF_VALUES in the case of V)"""
    match node_type:
        case NodeType.ATTENTION_HEAD:
            assert q_k_or_v is not None
            match q_k_or_v:
                case ActivationLocationType.ATTN_VALUE:
                    return ActivationLocationType.ATTN_WEIGHTED_SUM_OF_VALUES
                case ActivationLocationType.ATTN_QUERY | ActivationLocationType.ATTN_KEY:
                    return ActivationLocationType.ATTN_QK_PROBS
                case _:
                    raise NotImplementedError(q_k_or_v)
        case NodeType.MLP_NEURON:
            assert q_k_or_v is None
            return ActivationLocationType.MLP_POST_ACT
        case NodeType.AUTOENCODER_LATENT:
            assert q_k_or_v is None
            return ActivationLocationType.ONLINE_AUTOENCODER_LATENT
        case NodeType.MLP_AUTOENCODER_LATENT:
            assert q_k_or_v is None
            return ActivationLocationType.ONLINE_MLP_AUTOENCODER_LATENT
        case NodeType.ATTENTION_AUTOENCODER_LATENT:
            assert q_k_or_v is None
            return ActivationLocationType.ONLINE_ATTENTION_AUTOENCODER_LATENT
        case _:
            raise NotImplementedError(node_type)
def make_reconstituted_attribution_fn(
    transformer: Transformer,
    autoencoder_context: AutoencoderContext | None,
    node_type: NodeType,  # the type of the node of interest
    q_k_or_v: (
        ActivationLocationType | None
    ),  # if node_type is ATTENTION_HEAD, this specifies Q, K, or V
    detach_layer_norm_scale: bool,
) -> Callable[[torch.Tensor, torch.Tensor, LayerIndex, PassType], torch.Tensor]:
    """The 'attribution' of a node is taken to be the product of the node's gradient and the node's activation.
    This returns a function to compute the attribution of attention heads (specifically mediated by Q, K, or V), MLP activations,
    or autoencoder activations. The input expected by that function is the residual stream just before the node in question.
    The function returned can be used for further analysis, e.g. computing the gradient of the attribution with respect to the
    input residual stream.
    Note that this can be used to compute the attribution of one OR many downstream subnodes, depending on the tensor_indices_for_grad
    (an empty tensor_indices_for_grad corresponds to the entire layer worth of activations)."""
    assert (q_k_or_v is None) == (
        node_type != NodeType.ATTENTION_HEAD
    )  # for these functions, we require q_k_or_v to be
    # specified if node_type is ATTENTION_HEAD
    match node_type:
        case NodeType.ATTENTION_HEAD:
            match q_k_or_v:
                case ActivationLocationType.ATTN_QUERY | ActivationLocationType.ATTN_KEY:
                    # in all cases but attn value, the attribution fn is the hadamard product of the activation and the gradient
                    # NOTE: q_k_or_v = None covers all non-attention activations
                    match q_k_or_v:
                        case ActivationLocationType.ATTN_QUERY:
                            attention_trace_type = AttentionTraceType.Q
                        case ActivationLocationType.ATTN_KEY:
                            attention_trace_type = AttentionTraceType.K
                        case None:
                            attention_trace_type = AttentionTraceType.QK
                        case _:
                            raise NotImplementedError(q_k_or_v)
                    activation_fn = make_reconstituted_activation_fn(
                        transformer=transformer,
                        autoencoder_context=autoencoder_context,
                        node_type=node_type,
                        pre_or_post_act=PreOrPostAct.POST,
                        detach_layer_norm_scale=detach_layer_norm_scale,
                        attention_trace_type=attention_trace_type,
                    )
                    def attribution_fn(
                        resid: torch.Tensor,
                        grad: torch.Tensor,
                        layer_index: LayerIndex,
                        pass_type: PassType,
                    ) -> torch.Tensor:
                        activation = activation_fn(resid, layer_index, pass_type)
                        assert activation.shape == grad.shape, (
                            activation.shape,
                            grad.shape,
                        )
                        return activation * grad
                case ActivationLocationType.ATTN_VALUE:
                    assert (
                        get_activation_location_type_for_node_type(node_type, q_k_or_v)
                        == ActivationLocationType.ATTN_WEIGHTED_SUM_OF_VALUES
                    )
                    apply_attn_V_act = make_apply_attn_V_act(
                        transformer=transformer,
                        q_k_or_v=q_k_or_v,
                        detach_layer_norm_scale=detach_layer_norm_scale,
                    )
                    def attribution_fn(
                        resid: torch.Tensor,
                        grad: torch.Tensor,
                        layer_index: LayerIndex,
                        pass_type: PassType,
                    ) -> torch.Tensor:
                        attn, V = apply_attn_V_act(resid, layer_index, pass_type)
                        attn_weighted_V = torch.einsum("qkh,khd->qkhd", attn, V)
                        # grad is w/r/t (attn_weighted_V summed over k, or ATTN_WEIGHTED_SUM_OF_VALUES)
                        return torch.einsum("qkhd,qhd->qkh", attn_weighted_V, grad)
                case _:
                    raise NotImplementedError(q_k_or_v)
        case _:
            activation_fn = make_reconstituted_activation_fn(
                transformer=transformer,
                autoencoder_context=autoencoder_context,
                node_type=node_type,
                pre_or_post_act=PreOrPostAct.POST,
                detach_layer_norm_scale=detach_layer_norm_scale,
                attention_trace_type=None,
            )
            def attribution_fn(
                resid: torch.Tensor,
                grad: torch.Tensor,
                layer_index: LayerIndex,
                pass_type: PassType,
            ) -> torch.Tensor:
                activation = activation_fn(resid, layer_index, pass_type)
                assert activation.shape == grad.shape, (
                    activation.shape,
                    grad.shape,
                )
                return activation * grad
    return attribution_fn
class AttributionReconstituter(Reconstituter):
    """Reconstitute MLP, autoencoder, or attention node attribution (act * grad at node).
    Attention nodes are required to be split into Q, K, or V subnodes."""
    requires_other_scalar_source = True
    def __init__(
        self,
        transformer: Transformer,
        autoencoder_context: AutoencoderContext | None,
        node_type: NodeType,
        q_k_or_v: ActivationLocationType | None,
        detach_layer_norm_scale: bool,
    ):
        super().__init__()
        self._reconstitute_activations_fn = make_reconstituted_attribution_fn(
            transformer=transformer,
            autoencoder_context=autoencoder_context,
            node_type=node_type,
            q_k_or_v=q_k_or_v,
            detach_layer_norm_scale=detach_layer_norm_scale,
        )
        self.node_type = node_type
        self.q_k_or_v = q_k_or_v
        self.residual_dst = get_previous_residual_dst_for_node_type(
            node_type=node_type,
            autoencoder_dst=autoencoder_context.dst if autoencoder_context is not None else None,
        )
    def reconstitute_activations(
        self,
        resid: torch.Tensor,
        grad: torch.Tensor | None,
        layer_index: LayerIndex,
        pass_type: PassType,
    ) -> torch.Tensor:
        assert pass_type == PassType.FORWARD
        assert grad is not None
        return self._reconstitute_activations_fn(
            resid,
            grad,
            layer_index,
            pass_type,
        )
    def make_other_scalar_source(self, _unused_dst_config: DstConfig) -> ScalarSource:
        activation_location_type = get_activation_location_type_for_node_type(
            node_type=self.node_type,
            q_k_or_v=self.q_k_or_v,
        )
        return RawScalarSource(
            activation_location_type=activation_location_type,
            pass_type=PassType.BACKWARD,
            layer_indexer=IdentityLayerIndexer(),
        )  # this provides the 'grad' argument required by reconstitute_activations
    def _check_node_index(self, node_index: NodeIndex) -> None:
        assert node_index.node_type == self.node_type
        assert node_index.pass_type == PassType.FORWARD
        assert node_index.layer_index is not None
        if node_index.node_type == NodeType.ATTENTION_HEAD:
            assert isinstance(node_index, AttnSubNodeIndex)
            assert node_index.q_k_or_v == self.q_k_or_v
    def make_scalar_hook_for_node_index(
        self, node_index: NodeIndex
    ) -> Callable[[torch.Tensor], torch.Tensor]:
        self._check_node_index(node_index)
        def get_activation_from_layer_activations(layer_activations: torch.Tensor) -> torch.Tensor:
            return layer_activations[node_index.tensor_indices]
        return get_activation_from_layer_activations
    def make_gradient_scalar_deriver_for_node_index(
        self,
        node_index: NodeIndex,
        dst_config: DstConfig,
        output_dst: DerivedScalarType | None = None,
    ) -> ScalarDeriver:
        self._check_node_index(node_index)
        assert node_index.layer_index is not None
        dst_config_for_layer = dataclasses.replace(
            dst_config,
            layer_indices=[node_index.layer_index],
        )
        scalar_hook = self.make_scalar_hook_for_node_index(node_index)
        return self.make_gradient_scalar_deriver(
            scalar_hook=scalar_hook,
            dst_config=dst_config_for_layer,
            output_dst=output_dst,
        )
    def make_gradient_scalar_source_for_node_index(
        self,
        node_index: NodeIndex,
        dst_config: DstConfig,
        output_dst: DerivedScalarType | None = None,
    ) -> DerivedScalarSource:
        scalar_hook = self.make_scalar_hook_for_node_index(node_index)
        gradient_scalar_deriver = self.make_gradient_scalar_deriver(
            scalar_hook=scalar_hook,
            dst_config=dst_config,
            output_dst=output_dst,
        )
        assert node_index.layer_index is not None
        return DerivedScalarSource(
            scalar_deriver=gradient_scalar_deriver,
            pass_type=PassType.FORWARD,
            layer_indexer=ConstantLayerIndexer(node_index.layer_index),
        )
def _make_attribution_reconstituter_for_one_downstream_node(
    dst_config: DstConfig,
) -> AttributionReconstituter:
    # in the case of computing attribution of edges from many upstream to one downstream node, the dst_config
    # contains the information necessary to construct the Reconstituter. This is because the activation being
    # reconstituted corresponds to dst_config.node_index_for_attribution
    node_index_for_attribution = dst_config.node_index_for_attribution
    assert node_index_for_attribution is not None
    node_type = node_index_for_attribution.node_type
    if isinstance(node_index_for_attribution, AttnSubNodeIndex):
        q_k_or_v = node_index_for_attribution.q_k_or_v
    else:
        q_k_or_v = None
    assert (node_type == NodeType.ATTENTION_HEAD) == (q_k_or_v is not None)
    model_context = dst_config.get_model_context()
    transformer = model_context.get_or_create_model()
    autoencoder_context = dst_config.get_autoencoder_context()
    return AttributionReconstituter(
        transformer=transformer,
        autoencoder_context=autoencoder_context,
        node_type=node_type,
        q_k_or_v=q_k_or_v,
        detach_layer_norm_scale=dst_config.detach_layer_norm_scale_for_attribution,
    )
### MANY-UPSTREAM-TO-ONE-DOWNSTREAM CASE ###
def make_grad_of_downstream_subnode_attribution_scalar_deriver(
    dst_config: DstConfig,
) -> ScalarDeriver:
    """Computes the gradient with respect to the preceding residual stream of the downstream subnode's attribution
    (d(Activation(ResidualStream) * Gradient)/dResidualStream), with a stopgrad on the "Gradient" term.
    """
    node_index = dst_config.node_index_for_attribution
    assert node_index is not None
    reconstituter = _make_attribution_reconstituter_for_one_downstream_node(dst_config)
    return reconstituter.make_gradient_scalar_deriver_for_node_index(
        node_index=node_index,
        dst_config=dst_config,
        output_dst=DerivedScalarType.GRAD_OF_SINGLE_SUBNODE_ATTRIBUTION,
    )
def convert_scalar_deriver_to_out_edge_attributions(
    scalar_deriver: ScalarDeriver,
    output_dst: DerivedScalarType,
) -> ScalarDeriver:
    """Converts a scalar deriver for an activation of some kind to a scalar deriver for the
    attribution of edges going out from that activation to the node specified by
    trace_config (which can be autoencoder, MLP, or attention head-- and in the case of
    attention head, specifically the edge going to Q, K, or V)."""
    reconstituter = _make_attribution_reconstituter_for_one_downstream_node(
        scalar_deriver.dst_config,
    )
    node_index = scalar_deriver.dst_config.node_index_for_attribution
    assert node_index is not None
    attribution_grad_scalar_source = reconstituter.make_gradient_scalar_source_for_node_index(
        node_index=node_index,
        dst_config=scalar_deriver.dst_config,
        output_dst=DerivedScalarType.GRAD_OF_SINGLE_SUBNODE_ATTRIBUTION,
    )
    return convert_scalar_deriver_to_write_to_direction(
        scalar_deriver=scalar_deriver,
        direction_scalar_source=attribution_grad_scalar_source,
        output_dst=output_dst,
    )
def make_attn_out_edge_attribution_scalar_deriver(
    dst_config: DstConfig,
) -> ScalarDeriver:
    """Returns a scalar deriver for the attention value weighted by the post-softmax
    attention between each pair of tokens."""
    attn_weighted_value_scalar_deriver = make_attn_weighted_value_scalar_deriver(dst_config)
    return convert_scalar_deriver_to_out_edge_attributions(
        scalar_deriver=attn_weighted_value_scalar_deriver,
        output_dst=DerivedScalarType.ATTN_OUT_EDGE_ATTRIBUTION,
    )
def make_mlp_out_edge_attribution_scalar_deriver(
    dst_config: DstConfig,
) -> ScalarDeriver:
    """Returns a scalar deriver for the edge attribution of the MLP output layer at each token."""
    scalar_deriver = get_base_mlp_scalar_deriver(
        dst_config=dst_config,
    )
    return convert_scalar_deriver_to_out_edge_attributions(
        scalar_deriver=scalar_deriver,
        output_dst=DerivedScalarType.MLP_OUT_EDGE_ATTRIBUTION,
    )
def make_online_autoencoder_out_edge_attribution_scalar_deriver(
    dst_config: DstConfig,
    node_type: NodeType | None = None,
) -> ScalarDeriver:
    """Returns a scalar deriver for the edge attribution of the MLP output layer at each token."""
    scalar_deriver = make_online_autoencoder_latent_scalar_deriver_factory(node_type)(dst_config)
    return convert_scalar_deriver_to_out_edge_attributions(
        scalar_deriver=scalar_deriver,
        output_dst=DerivedScalarType.ONLINE_AUTOENCODER_OUT_EDGE_ATTRIBUTION,
    )
def make_token_out_edge_attribution_scalar_deriver(dst_config: DstConfig) -> ScalarDeriver:
    """This computes an attribution value for the edge from each token in the sequence to a particular
    downstream node."""
    node_index = dst_config.node_index_for_attribution
    assert node_index is not None
    reconstituter = _make_attribution_reconstituter_for_one_downstream_node(dst_config)
    emb_scalar_deriver = make_scalar_deriver_factory_for_activation_location_type(
        activation_location_type=ActivationLocationType.RESID_POST_EMBEDDING,
    )(dst_config)
    attribution_grad_scalar_source = reconstituter.make_gradient_scalar_source_for_node_index(
        node_index=node_index,
        dst_config=emb_scalar_deriver.dst_config,
        output_dst=DerivedScalarType.GRAD_OF_SINGLE_SUBNODE_ATTRIBUTION,
    )
    return convert_scalar_deriver_to_write_to_direction(
        scalar_deriver=emb_scalar_deriver,
        direction_scalar_source=attribution_grad_scalar_source,
        output_dst=DerivedScalarType.TOKEN_OUT_EDGE_ATTRIBUTION,
    )
### ONE-UPSTREAM-TO-MANY-DOWNSTREAM CASE ###
def convert_node_write_scalar_deriver_to_in_edge_attribution(
    node_write_scalar_source: ScalarSource,
    output_dst: DerivedScalarType,
    dst_config: DstConfig,
    downstream_node_type: NodeType,
    downstream_q_k_or_v: ActivationLocationType | None,
) -> ScalarDeriver:
    """Converts a scalar deriver for a write vector from some upstream node type to a scalar deriver for
    in edge attribution for downstream nodes of some type (MLP, autoencoder, or attention head)."""
    model_context = dst_config.get_model_context()
    autoencoder_context = dst_config.get_autoencoder_context()
    assert isinstance(model_context, StandardModelContext)
    transformer = model_context.get_or_create_model()
    reconstituter = AttributionReconstituter(
        transformer=transformer,
        autoencoder_context=autoencoder_context,
        node_type=downstream_node_type,
        q_k_or_v=downstream_q_k_or_v,
        detach_layer_norm_scale=dst_config.detach_layer_norm_scale_for_attribution,
    )
    return reconstituter.make_jvp_scalar_deriver(
        write_scalar_source=node_write_scalar_source,
        dst_config=dst_config,
        output_dst=output_dst,
    )
def make_in_edge_attribution_scalar_deriver_factory(
    node_type_for_attribution: NodeType,
    q_k_or_v_for_attribution: ActivationLocationType | None = None,
) -> Callable[[DstConfig], ScalarDeriver]:
    """Returns a function that creates a scalar deriver for the edge attribution from arbitrary node
    to the specified downstream node type / sub node type (MLP, autoencoder, or attention head Q, K, or V).
    """
    sub_node_type_to_output_dst = {
        (NodeType.MLP_NEURON, None): DerivedScalarType.MLP_IN_EDGE_ATTRIBUTION,
        (
            NodeType.AUTOENCODER_LATENT,
            None,
        ): DerivedScalarType.ONLINE_AUTOENCODER_IN_EDGE_ATTRIBUTION,
        (
            NodeType.ATTENTION_HEAD,
            ActivationLocationType.ATTN_QUERY,
        ): DerivedScalarType.ATTN_QUERY_IN_EDGE_ATTRIBUTION,
        (
            NodeType.ATTENTION_HEAD,
            ActivationLocationType.ATTN_KEY,
        ): DerivedScalarType.ATTN_KEY_IN_EDGE_ATTRIBUTION,
        (
            NodeType.ATTENTION_HEAD,
            ActivationLocationType.ATTN_VALUE,
        ): DerivedScalarType.ATTN_VALUE_IN_EDGE_ATTRIBUTION,
    }
    output_dst = sub_node_type_to_output_dst[(node_type_for_attribution, q_k_or_v_for_attribution)]
    def make_in_edge_attribution_scalar_deriver(dst_config: DstConfig) -> ScalarDeriver:
        node_write_scalar_source = make_node_write_scalar_source(dst_config)
        return convert_node_write_scalar_deriver_to_in_edge_attribution(
            node_write_scalar_source=node_write_scalar_source,
            output_dst=output_dst,
            dst_config=dst_config,
            downstream_node_type=node_type_for_attribution,
            downstream_q_k_or_v=q_k_or_v_for_attribution,
        )
    return make_in_edge_attribution_scalar_deriver
def make_node_write_to_final_residual_grad_scalar_deriver(dst_config: DstConfig) -> ScalarDeriver:
    """Returns a scalar deriver for the write vector from some upstream node type
    (MLP, autoencoder, or attention head) to the final residual grad. This can be used to compute
    the edge attribution of the edge from that node to the loss itself."""
    node_write_scalar_deriver = make_node_write_scalar_deriver(
        dst_config
    )  # TODO: figure out how to thread
    # the correct layer through to the final residual grad scalar source
    return convert_scalar_deriver_to_write_to_final_residual_grad(
        node_write_scalar_deriver,
        output_dst=DerivedScalarType.SINGLE_NODE_WRITE_TO_FINAL_RESIDUAL_GRAD,
        use_existing_backward_pass_for_final_residual_grad=True,
    )

================
File: neuron_explainer/activations/derived_scalars/indexing.py
================
"""
This file contains classes for referring to individual nodes (e.g. attention heads), activations
(e.g. attention post-softmax), or derived scalars (e.g. attention head write norm) from a forward
pass. DerivedScalarIndex can be used to index into a DerivedScalarStore.
These classes have a parallel structure to each other. One node index can be associated with
multiple activation indices and derived scalar indices. Derived scalar indices can be associated
with more types of scalars that aren't instantiated as 'activations' in the forward pass as
implemented.
Mirrored versions of these classes are used to refer to the same objects, but in a way that can be
transmitted via pydantic response and request data types for communication with a server. Changes
applied to mirrored dataclasses must be applied also to their unmirrored versions, and vice versa.
"""
import dataclasses
from dataclasses import dataclass
from enum import Enum, unique
from typing import Any, Literal, Union
from neuron_explainer.activations.derived_scalars.derived_scalar_types import DerivedScalarType
from neuron_explainer.models.model_component_registry import (
    ActivationLocationType,
    Dimension,
    LayerIndex,
    NodeType,
    PassType,
)
from neuron_explainer.pydantic import CamelCaseBaseModel, HashableBaseModel, immutable
DETACH_LAYER_NORM_SCALE = (
    True  # this sets default behavior for whether to detach layer norm scale everywhere
    # TODO: if all goes well, have this be hard-coded to True, and remove the plumbing
)
@dataclass(frozen=True)
class DerivedScalarIndex:
    """
    Indexes into a DerivedScalarStore and returns a tensor of activations specified by indices.
    """
    dst: DerivedScalarType
    tensor_indices: tuple[
        int | None, ...
    ]  # the indices of the activation tensor (not including layer_index)
    # elements of indices correspond to the elements of
    # scalar_deriver.shape_of_activation_per_token_spec
    # e.g. MLP activations might have shape (n_tokens, n_neurons).
    # an element of indices is None -> apply slice(None) for that dimension
    layer_index: LayerIndex  # the layer_index of the activation, if applicable
    pass_type: PassType
    @property
    def tensor_index_by_dim(self) -> dict[Dimension, int | None]:
        tensor_indices_list = list(self.tensor_indices)
        assert len(tensor_indices_list) <= len(self.dst.shape_spec_per_token_sequence), (
            f"Too many tensor indices {tensor_indices_list} for "
            f"{self.dst.shape_spec_per_token_sequence=}"
        )
        tensor_indices_list.extend(
            [None] * (len(self.dst.shape_spec_per_token_sequence) - len(self.tensor_indices))
        )
        return dict(zip(self.dst.shape_spec_per_token_sequence, tensor_indices_list))
    @classmethod
    def from_node_index(
        cls,
        node_index: "NodeIndex | MirroredNodeIndex",
        dst: DerivedScalarType,
    ) -> "DerivedScalarIndex":
        # with the extra information of what dst is desired (subject to the constraint
        # that it must share the same node_type), we can convert a NodeIndex to a DerivedScalarIndex
        assert (
            node_index.node_type == dst.node_type
        ), f"Node type does not match with the derived scalar type: {node_index.node_type=}, {dst=}"
        return cls(
            dst=dst,
            layer_index=node_index.layer_index,
            tensor_indices=node_index.tensor_indices,
            pass_type=node_index.pass_type,
        )
@immutable
class MirroredDerivedScalarIndex(HashableBaseModel):
    dst: DerivedScalarType
    tensor_indices: tuple[int | None, ...]
    layer_index: LayerIndex
    pass_type: PassType
    @classmethod
    def from_ds_index(cls, ds_index: DerivedScalarIndex) -> "MirroredDerivedScalarIndex":
        return cls(
            dst=ds_index.dst,
            layer_index=ds_index.layer_index,
            tensor_indices=ds_index.tensor_indices,
            pass_type=ds_index.pass_type,
        )
    def to_ds_index(self) -> DerivedScalarIndex:
        return DerivedScalarIndex(
            dst=self.dst,
            layer_index=self.layer_index,
            tensor_indices=self.tensor_indices,
            pass_type=self.pass_type,
        )
AllOrOneIndex = Union[int, Literal["All"]]
AllOrOneIndices = tuple[AllOrOneIndex, ...]
@dataclass(frozen=True)
class ActivationIndex:
    """
    This is parallel to DerivedScalarIndex, but specifically for ActivationLocationType's, not for more general DerivedScalarType's.
    """
    activation_location_type: ActivationLocationType
    tensor_indices: AllOrOneIndices
    layer_index: LayerIndex
    pass_type: PassType
    @property
    def tensor_index_by_dim(self) -> dict[Dimension, AllOrOneIndex]:
        # copied from DerivedScalarIndex; TODO: ActivationIndex and DerivedScalarIndex inherit from a shared base class,
        # and perhaps likewise with DerivedScalarType and ActivationLocationType?
        tensor_indices_list = list(self.tensor_indices)
        assert len(tensor_indices_list) <= len(
            self.activation_location_type.shape_spec_per_token_sequence
        ), (
            f"Too many tensor indices {tensor_indices_list} for "
            f"{self.activation_location_type.shape_spec_per_token_sequence=}"
        )
        tensor_indices_list.extend(
            ["All"]
            * (
                len(self.activation_location_type.shape_spec_per_token_sequence)
                - len(self.tensor_indices)
            )
        )
        assert len(tensor_indices_list) == len(
            self.activation_location_type.shape_spec_per_token_sequence
        )
        return dict(
            zip(
                self.activation_location_type.shape_spec_per_token_sequence,
                tensor_indices_list,
            )
        )
    @classmethod
    def from_node_index(
        cls,
        node_index: "NodeIndex | MirroredNodeIndex",
        activation_location_type: ActivationLocationType,
    ) -> "ActivationIndex":
        # with the extra information of what activation_location_type is desired (subject to the constraint
        # that it must share the same node_type), we can convert a NodeIndex to an ActivationIndex
        assert (
            node_index.node_type == activation_location_type.node_type
        ), f"Node type does not match with the derived scalar type: {node_index.node_type=}, {activation_location_type=}"
        return cls(
            activation_location_type=activation_location_type,
            layer_index=node_index.layer_index,
            tensor_indices=make_all_or_one_from_tensor_indices(node_index.tensor_indices),
            pass_type=node_index.pass_type,
        )
    @property
    def ndim(self) -> int:
        return compute_indexed_tensor_ndim(
            activation_location_type=self.activation_location_type,
            tensor_indices=self.tensor_indices,
        )
    def with_updates(self, **kwargs: Any) -> "ActivationIndex":
        """Given new values for fields of this ActivationIndex, return a new ActivationIndex instance with those
        fields updated"""
        return dataclasses.replace(self, **kwargs)
def make_all_or_one_from_tensor_indices(tensor_indices: tuple[int | None, ...]) -> AllOrOneIndices:
    return tuple("All" if tensor_index is None else tensor_index for tensor_index in tensor_indices)
def make_tensor_indices_from_all_or_one_indices(
    all_or_one_indices: AllOrOneIndices,
) -> tuple[int | None, ...]:
    return tuple(
        None if all_or_one_index == "All" else all_or_one_index
        for all_or_one_index in all_or_one_indices
    )
def compute_indexed_tensor_ndim(
    activation_location_type: ActivationLocationType,
    tensor_indices: AllOrOneIndices | tuple[int | None, ...],
) -> int:
    """Returns the dimensionality of a tensor of the given ActivationLocationType after being indexed by tensor_indices.
    int dimensions are removed from the resulting tensor."""
    ndim = activation_location_type.ndim_per_token_sequence - len(
        [tensor_index for tensor_index in tensor_indices if tensor_index not in {"All", None}]
    )
    assert ndim >= 0
    return ndim
def make_python_slice_from_tensor_indices(
    tensor_indices: tuple[int | None, ...]
) -> tuple[slice | int, ...]:
    return make_python_slice_from_all_or_one_indices(
        make_all_or_one_from_tensor_indices(tensor_indices)
    )
def make_python_slice_from_all_or_one_indices(
    all_or_one_indices: AllOrOneIndices,
) -> tuple[slice | int, ...]:
    return tuple(
        slice(None) if all_or_one_index == "All" else all_or_one_index
        for all_or_one_index in all_or_one_indices
    )
@immutable
class MirroredActivationIndex(HashableBaseModel):
    activation_location_type: ActivationLocationType
    tensor_indices: AllOrOneIndices
    layer_index: LayerIndex
    pass_type: PassType
    @classmethod
    def from_activation_index(cls, activation_index: ActivationIndex) -> "MirroredActivationIndex":
        return cls(
            activation_location_type=activation_index.activation_location_type,
            layer_index=activation_index.layer_index,
            tensor_indices=activation_index.tensor_indices,
            pass_type=activation_index.pass_type,
        )
    def to_activation_index(self) -> ActivationIndex:
        return ActivationIndex(
            activation_location_type=self.activation_location_type,
            layer_index=self.layer_index,
            tensor_indices=self.tensor_indices,
            pass_type=self.pass_type,
        )
@dataclass(frozen=True)
class NodeIndex:
    """
    This is parallel to DerivedScalarIndex, but refers to the NodeType associated with a
    DerivedScalarType, rather than the DerivedScalarType itself. This is for situations in
    which multiple derived scalars are computed for the same node.
    """
    node_type: NodeType
    tensor_indices: tuple[int | None, ...]
    layer_index: LayerIndex
    pass_type: PassType
    @classmethod
    def from_ds_index(
        cls,
        ds_index: DerivedScalarIndex,
    ) -> "NodeIndex":
        return cls(
            node_type=ds_index.dst.node_type,
            layer_index=ds_index.layer_index,
            tensor_indices=ds_index.tensor_indices,
            pass_type=ds_index.pass_type,
        )
    @classmethod
    def from_activation_index(
        cls,
        activation_index: ActivationIndex,
    ) -> "NodeIndex":
        return cls(
            node_type=activation_index.activation_location_type.node_type,
            layer_index=activation_index.layer_index,
            tensor_indices=make_tensor_indices_from_all_or_one_indices(
                activation_index.tensor_indices
            ),
            pass_type=activation_index.pass_type,
        )
    def with_updates(self, **kwargs: Any) -> "NodeIndex":
        """Given new values for fields of this NodeIndex, return a new NodeIndex instance with those
        fields updated"""
        return dataclasses.replace(self, **kwargs)
    @property
    def ndim(self) -> int:
        match self.node_type:
            case NodeType.ATTENTION_HEAD:
                reference_activation_location_type = ActivationLocationType.ATTN_QK_PROBS
            case NodeType.MLP_NEURON:
                reference_activation_location_type = ActivationLocationType.MLP_POST_ACT
            case NodeType.AUTOENCODER_LATENT:
                reference_activation_location_type = (
                    ActivationLocationType.ONLINE_AUTOENCODER_LATENT
                )
            case NodeType.MLP_AUTOENCODER_LATENT:
                reference_activation_location_type = (
                    ActivationLocationType.ONLINE_MLP_AUTOENCODER_LATENT
                )
            case NodeType.ATTENTION_AUTOENCODER_LATENT:
                reference_activation_location_type = (
                    ActivationLocationType.ONLINE_ATTENTION_AUTOENCODER_LATENT
                )
            case NodeType.RESIDUAL_STREAM_CHANNEL:
                reference_activation_location_type = ActivationLocationType.RESID_POST_MLP
            case _:
                raise NotImplementedError(f"Node type {self.node_type} not supported")
        return compute_indexed_tensor_ndim(
            activation_location_type=reference_activation_location_type,
            tensor_indices=self.tensor_indices,
        )
    def to_subnode_index(self, q_k_or_v: ActivationLocationType) -> "AttnSubNodeIndex":
        assert (
            self.node_type == NodeType.ATTENTION_HEAD
        ), f"Node type {self.node_type} is not NodeType.ATTENTION_HEAD"
        return AttnSubNodeIndex(
            node_type=self.node_type,
            layer_index=self.layer_index,
            tensor_indices=self.tensor_indices,
            pass_type=self.pass_type,
            q_k_or_v=q_k_or_v,
        )
@immutable
class MirroredNodeIndex(HashableBaseModel):
    """This class mirrors the fields of NodeIndex without default values."""
    node_type: NodeType
    tensor_indices: tuple[int | None, ...]
    layer_index: LayerIndex
    pass_type: PassType
    @classmethod
    def from_node_index(cls, node_index: NodeIndex) -> "MirroredNodeIndex":
        """
        Note that this conversion may lose information, specifically if the if the NodeIndex
        is an instance of a subclass of NodeIndex such as AttnSubNodeIndex.
        """
        return cls(
            node_type=node_index.node_type,
            layer_index=node_index.layer_index,
            tensor_indices=node_index.tensor_indices,
            pass_type=node_index.pass_type,
        )
    def to_node_index(self) -> NodeIndex:
        return NodeIndex(
            node_type=self.node_type,
            layer_index=self.layer_index,
            tensor_indices=self.tensor_indices,
            pass_type=self.pass_type,
        )
@dataclass(frozen=True)
class AttnSubNodeIndex(NodeIndex):
    """A NodeIndex that contains an extra piece of metadata, q_k_or_v,
    which specifies whether the input to an attention head node should
    be restricted to the portion going through the query, key, or value"""
    q_k_or_v: ActivationLocationType
    def __post_init__(self) -> None:
        assert (
            self.node_type == NodeType.ATTENTION_HEAD
        ), f"Node type {self.node_type} is not NodeType.ATTENTION_HEAD"
        assert self.q_k_or_v in {
            ActivationLocationType.ATTN_QUERY,
            ActivationLocationType.ATTN_KEY,
            ActivationLocationType.ATTN_VALUE,
        }
# TODO: consider subsuming this and the above into NodeIndex/ActivationIndex respectively
@dataclass(frozen=True)
class AttnSubActivationIndex(ActivationIndex):
    """An ActivationIndex that contains an extra piece of metadata, q_or_k,
    which specifies whether the input to an attention head node should
    be restricted to the portion going through the query or key"""
    q_or_k: ActivationLocationType
    def __post_init__(self) -> None:
        assert self.activation_location_type.node_type == NodeType.ATTENTION_HEAD
        assert self.q_or_k in {
            ActivationLocationType.ATTN_QUERY,
            ActivationLocationType.ATTN_KEY,
        }
@immutable
class AblationSpec(CamelCaseBaseModel):
    """A specification for performing ablation on a model."""
    index: MirroredActivationIndex
    value: float
@unique
class AttentionTraceType(Enum):
    Q = "Q"
    K = "K"
    QK = "QK"
    """Q times K"""
    V = "V"
    """Allow gradient to flow through value vector; the attention write * gradient with respect to
    some downstream node or the loss provides the scalar which is backpropagated"""
@immutable
class NodeAblation(CamelCaseBaseModel):
    """A specification for tracing an upstream node.
    This data structure is used by the client. The server converts it to an AblationSpec.
    """
    node_index: MirroredNodeIndex
    value: float
class PreOrPostAct(str, Enum):
    """Specifies whether to trace from pre- or post-nonlinearity"""
    PRE = "pre"
    POST = "post"
@dataclass(frozen=True)
class TraceConfig:
    """This specifies a node from which to compute a backward pass, along with whether to trace from
    pre- or post-nonlinearity, which subnodes to flow the gradient through in the case of an attention node,
    and whether to detach the layer norm scale just before the activation (i.e. whether to flow gradients
    through the layer norm scale parameter)."""
    node_index: NodeIndex
    pre_or_post_act: PreOrPostAct
    detach_layer_norm_scale: bool
    attention_trace_type: AttentionTraceType | None = None  # applies only to attention heads
    downstream_trace_config: "TraceConfig | None" = (
        None  # applies only to attention heads with attention_trace_type == AttentionTraceType.V
    )
    def __post_init__(self) -> None:
        if self.node_index.node_type != NodeType.ATTENTION_HEAD:
            assert self.attention_trace_type is None
        if self.attention_trace_type != AttentionTraceType.V:
            # only tracing through V supports a downstream node
            assert self.downstream_trace_config is None
        else:
            if self.downstream_trace_config is not None:
                # repeatedly tracing through V is not allowed; all other types of
                # downstream trace configs are fine
                assert self.downstream_trace_config.attention_trace_type != AttentionTraceType.V
            # cfg is None -> a loss (function of logits) is assumed to be defined
    @property
    def node_type(self) -> NodeType:
        return self.node_index.node_type
    @property
    def tensor_indices(self) -> AllOrOneIndices:
        return make_all_or_one_from_tensor_indices(self.node_index.tensor_indices)
    @property
    def layer_index(self) -> LayerIndex:
        return self.node_index.layer_index
    @property
    def pass_type(self) -> PassType:
        return self.node_index.pass_type
    @property
    def ndim(self) -> int:
        return self.node_index.ndim
    def with_updated_index(
        self,
        **kwargs: Any,
    ) -> "TraceConfig":
        return dataclasses.replace(
            self,
            node_index=self.node_index.with_updates(**kwargs),
        )
    @classmethod
    def from_activation_index(
        cls,
        activation_index: ActivationIndex,
        detach_layer_norm_scale: bool = DETACH_LAYER_NORM_SCALE,
    ) -> "TraceConfig":
        node_index = NodeIndex.from_activation_index(activation_index)
        match activation_index.activation_location_type:
            case ActivationLocationType.MLP_PRE_ACT | ActivationLocationType.ATTN_QK_LOGITS:
                pre_or_post_act = PreOrPostAct.PRE
            case (
                ActivationLocationType.MLP_POST_ACT
                | ActivationLocationType.ATTN_QK_PROBS
                | ActivationLocationType.ONLINE_AUTOENCODER_LATENT
            ):
                pre_or_post_act = PreOrPostAct.POST
            case _:
                raise ValueError(
                    f"ActivationLocationType {activation_index.activation_location_type} not supported"
                )
        match node_index.node_type:
            case NodeType.ATTENTION_HEAD:
                attention_trace_type: AttentionTraceType | None = AttentionTraceType.QK
            case _:
                attention_trace_type = None
        downstream_trace_config = None
        return cls(
            node_index=node_index,
            pre_or_post_act=pre_or_post_act,
            detach_layer_norm_scale=detach_layer_norm_scale,
            attention_trace_type=attention_trace_type,
            downstream_trace_config=downstream_trace_config,
        )
@immutable
class MirroredTraceConfig(HashableBaseModel):
    node_index: MirroredNodeIndex
    pre_or_post_act: PreOrPostAct
    detach_layer_norm_scale: bool
    attention_trace_type: AttentionTraceType | None = None  # applies only to attention heads
    downstream_trace_config: "MirroredTraceConfig | None" = (
        None  # applies only to attention heads with attention_trace_type == AttentionTraceType.V
    )
    def to_trace_config(self) -> TraceConfig:
        downstream_trace_config = (
            self.downstream_trace_config.to_trace_config()
            if self.downstream_trace_config is not None
            else None
        )
        return TraceConfig(
            node_index=self.node_index.to_node_index(),
            pre_or_post_act=self.pre_or_post_act,
            detach_layer_norm_scale=self.detach_layer_norm_scale,
            attention_trace_type=self.attention_trace_type,
            downstream_trace_config=downstream_trace_config,
        )
    @classmethod
    def from_trace_config(cls, trace_config: TraceConfig) -> "MirroredTraceConfig":
        mirrored_downstream_trace_config = (
            cls.from_trace_config(trace_config.downstream_trace_config)
            if trace_config.downstream_trace_config is not None
            else None
        )
        return cls(
            node_index=MirroredNodeIndex.from_node_index(trace_config.node_index),
            pre_or_post_act=trace_config.pre_or_post_act,
            detach_layer_norm_scale=trace_config.detach_layer_norm_scale,
            attention_trace_type=trace_config.attention_trace_type,
            downstream_trace_config=mirrored_downstream_trace_config,
        )
@immutable
class NodeToTrace(CamelCaseBaseModel):
    """A specification for tracing a node.
    This data structure is used by the client. The server converts it to an activation index and
    an ablation spec.
    In the case of tracing through attention value, there can be up to two NodeToTrace
    objects: one upstream and one downstream. First, a gradient is computed with respect to the
    downstream node. Then, the direct effect of the upstream (attention) node on that downstream
    node is computed. Then, the gradient is computed with respect to that direct effect, propagated
    through V
    """
    node_index: MirroredNodeIndex
    attention_trace_type: AttentionTraceType | None
    downstream_trace_config: MirroredTraceConfig | None

================
File: neuron_explainer/activations/derived_scalars/locations.py
================
"""
This file contains code related to specifying the locations of derived scalars, and their inputs,
within the residual stream.
"""
from abc import ABC, abstractmethod
from typing import Literal, Sequence
from neuron_explainer.activations.derived_scalars.config import DstConfig
from neuron_explainer.activations.derived_scalars.derived_scalar_types import DerivedScalarType
from neuron_explainer.activations.derived_scalars.indexing import ActivationIndex
from neuron_explainer.models.model_component_registry import (
    ActivationLocationType,
    LayerIndex,
    LocationWithinLayer,
    NodeType,
    PassType,
)
class LayerIndexer(ABC):
    """A LayerIndexer is a function that maps from a list of indices in an original ActivationsAndMetadata object
    and a list of indices in a reindexed ActivationsAndMetadata object. It can do things like:
    - replace the activations at every layer with a reference to the activations at a single layer
    - replace the activations at every layer with a reference to a single activation from an ActivationLocationType
    that doesn't use layers (e.g. residual stream post embedding)
    - replace the activations at every layer with a reference to the activations one layer earlier
    - keep the activations at every layer the same
    DST computation typically acts on the activations at the same layer for each of several layers in an ActivationsAndMetadata object.
    When the computation requires activations from multiple distinct layers to compute the result for a given layer, this class
    handles the remapping so that downstream code can act on each layer independently."""
    @abstractmethod
    def __call__(self, layer_indices: list[LayerIndex]) -> Sequence[LayerIndex | Literal["Dummy"]]:
        # given a list of layer indices to an original ActivationsAndMetadata object, return a list of layer indices
        # with which to index the activations_by_layer_index of the original object in order to obtain the reindexed
        # activations_by_layer_index of the new object
        # int refers to a normal layer index
        # None refers to an activation with no layer index (e.g. embeddings)
        # "Dummy" is used when the reindexed
        # ActivationsAndMetadata object does not require the activation from the original object at that layer index,
        # for example if it's the input to a derived scalar computation that doesn't require every activation at every
        # layer index
        pass
class IdentityLayerIndexer(LayerIndexer):
    """Sometimes computing derived scalar D at layer L requires Scalar S from layer L, and Scalar T from layer L. In this case no changes are needed
    to the layer indices of the activations_by_layer_index in the ActivationsAndMetadata object. This is used for such cases (it does a no-op).
    """
    def __init__(self) -> None:
        pass
    def __call__(self, layer_indices: list[LayerIndex]) -> list[LayerIndex]:
        return layer_indices
    def __repr__(self) -> str:
        return "IdentityLayerIndexer()"
class OffsetLayerIndexer(LayerIndexer):
    """Sometimes computing derived scalar D at layer L requires Scalar S from layer L, and Scalar T from layer L-1.
    This is used for populating an ActivationsAndMetadata object at each layer index with references to the activations at the previous layer index.
    """
    def __init__(self, layer_index_offset: int) -> None:
        self.layer_index_offset = layer_index_offset
    def __call__(self, layer_indices: list[LayerIndex]) -> list[LayerIndex | Literal["Dummy"]]:
        def _dummy_if_invalid(
            layer_index: LayerIndex, valid_indices: set[LayerIndex]
        ) -> LayerIndex | Literal["Dummy"]:
            if layer_index in valid_indices:
                return layer_index
            else:
                # this value represents the fact that the layer index is not needed for this computation
                # callers are free to use a dummy tensor in place of the activations at this layer index,
                # knowing that downstream DST calculations are intended to be independent of the activation
                # tensor provided at this layer index
                return "Dummy"
        assert all(layer_index is not None for layer_index in layer_indices)
        # source_layer_indices satisfy:
        # target_layer_indices = layer_indices
        # for target_layer_index, source_layer_index in zip(target_layer_indices, source_layer_indices):
        #    target_activations_by_layer_index[target_layer_index] = source_activations_by_layer_index[source_layer_index] # (or a dummy tensor, if the index is "Dummy")
        source_layer_indices = [layer_index + self.layer_index_offset for layer_index in layer_indices]  # type: ignore
        # 'invalid' layer indices are those not in starting_layer_indices; starting_layer_indices mapped to unneeded layer indices are considered "Unneeded"
        return [
            _dummy_if_invalid(source_layer_index, set(layer_indices))
            for source_layer_index in source_layer_indices
        ]
    def __repr__(self) -> str:
        return f"OffsetLayerIndexer(layer_index_offset={self.layer_index_offset})"
class StaticLayerIndexer(LayerIndexer, ABC):
    """A subset of LayerIndexers have a single layer_index associated with them. This gives those LayerIndexers a
    common abstract interface."""
    layer_index: LayerIndex
    def __call__(self, layer_indices: list[LayerIndex]) -> list[LayerIndex]:
        # this says to use the same activation tensor at every layer index requested; each layer index
        # is mapped to the same constant (or None) layer index
        return [self.layer_index for _ in layer_indices]
class ConstantLayerIndexer(StaticLayerIndexer):
    """Sometimes computing derived scalar D at layer L requires Scalar S from layer L, and Scalar T from layer C (independent of L).
    This is used for populating an ActivationsAndMetadata object with references to the same activation tensor (from layer C) at every layer index L.
    """
    def __init__(self, constant_layer_index: int) -> None:
        self.layer_index = constant_layer_index
    def __repr__(self) -> str:
        return f"ConstantLayerIndexer(constant_layer_index={self.layer_index})"
class NoLayersLayerIndexer(StaticLayerIndexer):
    """Sometimes computing derived scalar D at layer L requires Scalar S from layer L, and Scalar T which doesn't have layers.
    This is used for populating an ActivationsAndMetadata object with references to the same activation tensor (from a location type with no layers, i.e.
    at the index None) at every layer index L."""
    def __init__(self) -> None:
        self.layer_index = None
    def __repr__(self) -> str:
        return "NoLayersLayerIndexer()"
DEFAULT_LAYER_INDEXER = IdentityLayerIndexer()
def precedes_final_layer(
    derived_scalar_location_within_layer: LocationWithinLayer | None,
    derived_scalar_layer_index: LayerIndex,
    final_residual_location_within_layer: LocationWithinLayer | None,
    final_residual_layer_index: LayerIndex,
) -> bool:
    """Returns True if the derived scalar at a given layer_index precedes the final residual stream derived scalar
    at a specified layer_index"""
    # return True if the derived scalar at a given layer_index precedes the final residual stream layer_index
    if derived_scalar_layer_index is None:
        return True  # activations with no layer_index are assumed to precede
        # all activations with layer_index; note that according to current conventions
        # this is true for all residual stream activations (not true e.g. for token logits)
    elif final_residual_layer_index is None:
        assert derived_scalar_layer_index is not None
        return False  # activations with layer_index precede activations with no layer_index
    elif derived_scalar_layer_index < final_residual_layer_index:
        return True
    elif derived_scalar_layer_index == final_residual_layer_index:
        if derived_scalar_location_within_layer is None:
            raise ValueError(
                "derived_scalar_location_within_layer must be provided in case of equal layer indices"
            )
        if final_residual_location_within_layer is None:
            raise ValueError(
                "final_residual_location_within_layer must be provided in case of equal layer indices"
            )
        if derived_scalar_location_within_layer < final_residual_location_within_layer:
            # location_within_layer inherits from int; therefore they are straightforwardly comparable
            return True
        else:
            return False
    else:
        assert derived_scalar_layer_index > final_residual_layer_index
        return False
def get_location_within_layer_for_dst(
    dst: DerivedScalarType,
    dst_config: DstConfig,
) -> LocationWithinLayer | None:
    """Determines the location within a layer for DSTs which are not associated with an activation
    location type, or whose location within a layer depends on information in the DstConfig (e.g.
    autoencoder related DSTs). Defining new direct write related DSTs may require additional entries
    here."""
    if dst.location_within_layer is not None:
        # this might be determinable from the DST alone, in which case return it right away
        return dst.location_within_layer
    else:
        match dst.node_type:
            case (
                NodeType.AUTOENCODER_LATENT
                | NodeType.MLP_AUTOENCODER_LATENT
                | NodeType.ATTENTION_AUTOENCODER_LATENT
            ):
                autoencoder_context = dst_config.get_autoencoder_context(dst.node_type)
                if autoencoder_context is not None:
                    return autoencoder_context.dst.location_within_layer
                else:
                    return None
            case NodeType.RESIDUAL_STREAM_CHANNEL:
                match dst:
                    case DerivedScalarType.ATTN_WRITE:
                        return LocationWithinLayer.ATTN
                    case DerivedScalarType.PREVIOUS_LAYER_RESID_POST_MLP:
                        return LocationWithinLayer.END_OF_PREV_LAYER
                    case _:
                        return None
            case _:
                return None
def get_previous_residual_dst_for_node_type(
    node_type: NodeType,
    autoencoder_dst: DerivedScalarType | None,
) -> DerivedScalarType:
    """This function returns the DerivedScalarType of the residual stream that precedes the node
    type specified. autoencoder_context is only required if node_type is NodeType.ONLINE_AUTOENCODER.
    """
    match node_type:
        case NodeType.ATTENTION_HEAD:
            return DerivedScalarType.PREVIOUS_LAYER_RESID_POST_MLP
        case NodeType.MLP_NEURON:
            return DerivedScalarType.RESID_POST_ATTN
        case (
            NodeType.AUTOENCODER_LATENT
            | NodeType.MLP_AUTOENCODER_LATENT
            | NodeType.ATTENTION_AUTOENCODER_LATENT
        ):
            assert autoencoder_dst is not None, node_type
            match autoencoder_dst.node_type:
                case NodeType.RESIDUAL_STREAM_CHANNEL:
                    match autoencoder_dst:
                        case DerivedScalarType.RESID_DELTA_ATTN:
                            return get_previous_residual_dst_for_node_type(
                                node_type=NodeType.ATTENTION_HEAD,
                                autoencoder_dst=None,
                            )
                        case DerivedScalarType.RESID_DELTA_MLP:
                            return get_previous_residual_dst_for_node_type(
                                node_type=NodeType.MLP_NEURON,
                                autoencoder_dst=None,
                            )
                        case _:
                            raise NotImplementedError(autoencoder_dst)
                case _:
                    return get_previous_residual_dst_for_node_type(
                        node_type=autoencoder_dst.node_type,
                        autoencoder_dst=None,
                    )
        case _:
            raise NotImplementedError(node_type)
def get_activation_index_for_residual_dst(
    dst: DerivedScalarType,
    layer_index: int,
) -> ActivationIndex:
    """
    This returns an ActivationIndex corresponding to a residual stream activation location
    at a given layer_index; handles the indexing logic in the case of PREVIOUS_LAYER_RESID_POST_MLP.
    The ActivationIndex returned corresponds to the entire residual stream activation tensor for the
    layer.
    """
    assert dst.node_type == NodeType.RESIDUAL_STREAM_CHANNEL
    match dst:
        case DerivedScalarType.PREVIOUS_LAYER_RESID_POST_MLP:
            if layer_index == 0:
                return ActivationIndex(
                    activation_location_type=ActivationLocationType.RESID_POST_EMBEDDING,
                    layer_index=None,
                    tensor_indices=(),
                    pass_type=PassType.FORWARD,
                )
            else:
                return ActivationIndex(
                    activation_location_type=ActivationLocationType.RESID_POST_MLP,
                    layer_index=layer_index - 1,
                    tensor_indices=(),
                    pass_type=PassType.FORWARD,
                )
        case DerivedScalarType.RESID_POST_MLP:
            return ActivationIndex(
                activation_location_type=ActivationLocationType.RESID_POST_MLP,
                layer_index=layer_index,
                tensor_indices=(),
                pass_type=PassType.FORWARD,
            )
        case DerivedScalarType.RESID_POST_ATTN:
            return ActivationIndex(
                activation_location_type=ActivationLocationType.RESID_POST_ATTN,
                layer_index=layer_index,
                tensor_indices=(),
                pass_type=PassType.FORWARD,
            )
        case _:
            raise NotImplementedError(dst)

================
File: neuron_explainer/activations/derived_scalars/logprobs.py
================
from typing import Callable
import torch
from neuron_explainer.activations.derived_scalars.derived_scalar_types import DerivedScalarType
from neuron_explainer.activations.derived_scalars.indexing import ActivationIndex, NodeIndex
from neuron_explainer.activations.derived_scalars.reconstituted import (
    make_apply_logits,
    make_apply_logprobs,
)
from neuron_explainer.activations.derived_scalars.reconstituter_class import Reconstituter
from neuron_explainer.models.model_component_registry import LayerIndex, NodeType, PassType
from neuron_explainer.models.model_context import ModelContext
class LogProbReconstituter(Reconstituter):
    """Reconstitute vocab token logprobs from final residual stream location. Can be used e.g. to compute
    effect of residual stream writes on token logprobs, rather than logits."""
    residual_dst: DerivedScalarType = DerivedScalarType.RESID_POST_MLP
    requires_other_scalar_source: bool = False
    def __init__(
        self,
        model_context: ModelContext,
        detach_layer_norm_scale: bool,
    ):
        super().__init__()
        self._model_context = model_context
        self.detach_layer_norm_scale = detach_layer_norm_scale
        transformer = self._model_context.get_or_create_model()
        self._reconstitute_activations_fn = make_apply_logprobs(
            transformer=transformer,
            detach_layer_norm_scale=self.detach_layer_norm_scale,
        )
    def reconstitute_activations(
        self,
        resid: torch.Tensor,
        other_arg: torch.Tensor | None,
        layer_index: LayerIndex,
        pass_type: PassType,
    ) -> torch.Tensor:
        assert other_arg is None
        assert layer_index == self._model_context.n_layers - 1
        assert pass_type == PassType.FORWARD
        return self._reconstitute_activations_fn(resid)
class LogitReconstituter(Reconstituter):
    """Reconstitute vocab token logprobs from final residual stream location. Can be used e.g. to compute
    effect of residual stream writes on token logprobs, rather than logits."""
    residual_dst: DerivedScalarType = DerivedScalarType.RESID_POST_MLP
    requires_other_scalar_source: bool = False
    def __init__(
        self,
        model_context: ModelContext,
        detach_layer_norm_scale: bool,
    ):
        super().__init__()
        self._model_context = model_context
        self.detach_layer_norm_scale = detach_layer_norm_scale
        transformer = self._model_context.get_or_create_model()
        self._reconstitute_activations_fn = make_apply_logits(
            transformer=transformer,
            detach_layer_norm_scale=self.detach_layer_norm_scale,
        )
    def reconstitute_activations(
        self,
        resid: torch.Tensor,
        other_arg: torch.Tensor | None,
        layer_index: LayerIndex,
        pass_type: PassType,
    ) -> torch.Tensor:
        assert other_arg is None
        assert layer_index == self._model_context.n_layers - 1
        assert pass_type == PassType.FORWARD
        return self._reconstitute_activations_fn(resid)
    def get_residual_activation_index(self) -> ActivationIndex:
        # this contains only the information that we're interested in the final residual stream layer
        dummy_node_index = NodeIndex(
            node_type=NodeType.LAYER,
            layer_index=self._model_context.n_layers - 1,
            tensor_indices=(),
            pass_type=PassType.FORWARD,
        )
        return self.get_residual_activation_index_for_node_index(
            node_index=dummy_node_index,
        )
    def make_reconstitute_gradient_of_loss_fn(
        self,
        loss_fn: Callable[[torch.Tensor], torch.Tensor],
    ) -> Callable[[torch.Tensor], torch.Tensor]:
        def scalar_hook(
            resid: torch.Tensor,
        ) -> torch.Tensor:
            # loss fn expects a batch dimension
            return loss_fn(resid.unsqueeze(0)).squeeze(0)
        def reconstitute_gradient(
            resid: torch.Tensor,
        ) -> torch.Tensor:
            return self.reconstitute_gradient(
                resid=resid,
                other_arg=None,
                layer_index=self._model_context.n_layers - 1,
                pass_type=PassType.FORWARD,
                scalar_hook=scalar_hook,
            )
        return reconstitute_gradient

================
File: neuron_explainer/activations/derived_scalars/make_scalar_derivers.py
================
"""
This file contains functions for generating a ScalarDeriver based on a DerivedScalarType and a
DerivedScalarTypeConfig (`make_scalar_deriver`) or for convenience based on just a HookLocationType
(`make_scalar_deriver_for_hook_location_type`). It calls make_scalar_deriver_... functions defined
in other files within derived_scalars/.
"""
from typing import Callable
from neuron_explainer.activations.derived_scalars.attention import (
    make_attn_act_times_grad_per_sequence_token_scalar_deriver,
    make_attn_weighted_value_scalar_deriver,
    make_attn_write_norm_per_sequence_token_scalar_deriver,
    make_attn_write_norm_scalar_deriver,
    make_attn_write_scalar_deriver,
    make_attn_write_sum_heads_scalar_deriver,
    make_attn_write_to_final_residual_grad_per_sequence_token_scalar_deriver,
    make_attn_write_to_latent_per_sequence_token_batched_scalar_deriver,
    make_attn_write_to_latent_per_sequence_token_scalar_deriver,
    make_attn_write_to_latent_scalar_deriver,
    make_attn_write_to_latent_summed_over_heads_scalar_deriver,
    make_flattened_attn_post_softmax_act_times_grad_scalar_deriver,
    make_flattened_attn_post_softmax_scalar_deriver,
    make_flattened_attn_write_to_final_residual_grad_scalar_deriver,
    make_flattened_attn_write_to_latent_summed_over_heads_batched_scalar_deriver,
    make_flattened_attn_write_to_latent_summed_over_heads_scalar_deriver,
    make_unflattened_attn_write_norm_scalar_deriver,
    make_unflattened_attn_write_to_final_activation_residual_grad_scalar_deriver,
    make_unflattened_attn_write_to_final_residual_grad_scalar_deriver,
)
from neuron_explainer.activations.derived_scalars.autoencoder import (
    make_autoencoder_latent_grad_wrt_mlp_post_act_input_scalar_deriver,
    make_autoencoder_latent_grad_wrt_residual_input_scalar_deriver,
    make_autoencoder_latent_scalar_deriver_factory,
    make_autoencoder_write_norm_scalar_deriver_factory,
    make_online_autoencoder_act_times_grad_scalar_deriver_factory,
    make_online_autoencoder_error_scalar_deriver_factory,
    make_online_autoencoder_latent_scalar_deriver_factory,
    make_online_autoencoder_latentwise_write_scalar_deriver_factory,
    make_online_autoencoder_write_norm_scalar_deriver_factory,
    make_online_autoencoder_write_to_final_activation_residual_grad_scalar_deriver_factory,
    make_online_autoencoder_write_to_final_residual_grad_scalar_deriver_factory,
    make_online_mlp_autoencoder_error_act_times_grad_scalar_deriver,
    make_online_mlp_autoencoder_error_write_norm_scalar_deriver,
    make_online_mlp_autoencoder_error_write_to_final_residual_grad_scalar_deriver,
)
from neuron_explainer.activations.derived_scalars.config import DstConfig
from neuron_explainer.activations.derived_scalars.derived_scalar_types import DerivedScalarType
from neuron_explainer.activations.derived_scalars.edge_activation import (
    make_in_edge_activation_scalar_deriver_factory,
)
from neuron_explainer.activations.derived_scalars.edge_attribution import (
    make_attn_out_edge_attribution_scalar_deriver,
    make_grad_of_downstream_subnode_attribution_scalar_deriver,
    make_in_edge_attribution_scalar_deriver_factory,
    make_mlp_out_edge_attribution_scalar_deriver,
    make_node_write_scalar_deriver,
    make_node_write_to_final_residual_grad_scalar_deriver,
    make_online_autoencoder_out_edge_attribution_scalar_deriver,
    make_token_out_edge_attribution_scalar_deriver,
)
from neuron_explainer.activations.derived_scalars.mlp import (
    make_mlp_neuronwise_write_scalar_deriver,
    make_mlp_write_norm_scalar_deriver,
    make_mlp_write_to_final_activation_residual_grad_scalar_deriver,
    make_mlp_write_to_final_residual_grad_scalar_deriver,
    make_resid_delta_mlp_from_mlp_post_act_scalar_deriver,
)
from neuron_explainer.activations.derived_scalars.raw_activations import (
    make_scalar_deriver_factory_for_act_times_grad,
    make_scalar_deriver_factory_for_activation_location_type,
    make_truncate_to_expected_shape_scalar_deriver_factory_for_dst,
)
from neuron_explainer.activations.derived_scalars.residual import (
    make_previous_layer_resid_post_mlp_scalar_deriver,
    make_residual_norm_scalar_deriver_factory_for_activation_location_type,
    make_residual_projection_to_final_residual_grad_scalar_deriver_factory_for_activation_location_type,
    make_token_attribution_scalar_deriver,
    make_unity_scalar_deriver,
    make_vocab_token_write_to_input_direction_scalar_deriver,
)
from neuron_explainer.activations.derived_scalars.scalar_deriver import ScalarDeriver
from neuron_explainer.models.model_component_registry import ActivationLocationType, NodeType
### REGISTRY; ADD NEW TYPES HERE, AND ALSO IN ENUM IN scalar_deriver.py ###
# This contains a function to generate each implemented derived scalar type. Called by
# make_scalar_deriver below.
_DERIVED_SCALAR_TYPE_REGISTRY: dict[DerivedScalarType, Callable[[DstConfig], ScalarDeriver]] = {
    DerivedScalarType.RESID_POST_EMBEDDING: make_scalar_deriver_factory_for_activation_location_type(
        ActivationLocationType.RESID_POST_EMBEDDING
    ),
    DerivedScalarType.RESID_POST_MLP: make_scalar_deriver_factory_for_activation_location_type(
        ActivationLocationType.RESID_POST_MLP
    ),
    DerivedScalarType.RESID_POST_ATTN: make_scalar_deriver_factory_for_activation_location_type(
        ActivationLocationType.RESID_POST_ATTN
    ),
    DerivedScalarType.RESID_FINAL_LAYER_NORM_SCALE: make_scalar_deriver_factory_for_activation_location_type(
        ActivationLocationType.RESID_FINAL_LAYER_NORM_SCALE
    ),
    DerivedScalarType.ATTN_INPUT_LAYER_NORM_SCALE: make_scalar_deriver_factory_for_activation_location_type(
        ActivationLocationType.ATTN_INPUT_LAYER_NORM_SCALE
    ),
    DerivedScalarType.MLP_INPUT_LAYER_NORM_SCALE: make_scalar_deriver_factory_for_activation_location_type(
        ActivationLocationType.MLP_INPUT_LAYER_NORM_SCALE
    ),
    DerivedScalarType.LOGITS: make_truncate_to_expected_shape_scalar_deriver_factory_for_dst(
        DerivedScalarType.LOGITS
    ),
    DerivedScalarType.MLP_PRE_ACT: make_scalar_deriver_factory_for_activation_location_type(
        ActivationLocationType.MLP_PRE_ACT
    ),
    DerivedScalarType.MLP_POST_ACT: make_scalar_deriver_factory_for_activation_location_type(
        ActivationLocationType.MLP_POST_ACT
    ),
    DerivedScalarType.ATTN_QUERY: make_scalar_deriver_factory_for_activation_location_type(
        ActivationLocationType.ATTN_QUERY
    ),
    DerivedScalarType.ATTN_KEY: make_scalar_deriver_factory_for_activation_location_type(
        ActivationLocationType.ATTN_KEY
    ),
    DerivedScalarType.ATTN_VALUE: make_scalar_deriver_factory_for_activation_location_type(
        ActivationLocationType.ATTN_VALUE
    ),
    DerivedScalarType.ATTN_QK_LOGITS: make_scalar_deriver_factory_for_activation_location_type(
        ActivationLocationType.ATTN_QK_LOGITS
    ),
    DerivedScalarType.ATTN_QK_PROBS: make_scalar_deriver_factory_for_activation_location_type(
        ActivationLocationType.ATTN_QK_PROBS
    ),
    DerivedScalarType.ATTN_WEIGHTED_SUM_OF_VALUES: make_scalar_deriver_factory_for_activation_location_type(
        ActivationLocationType.ATTN_WEIGHTED_SUM_OF_VALUES
    ),
    DerivedScalarType.RESID_DELTA_ATTN: make_scalar_deriver_factory_for_activation_location_type(
        ActivationLocationType.RESID_DELTA_ATTN
    ),
    DerivedScalarType.ATTN_WRITE_NORM: make_attn_write_norm_scalar_deriver,
    DerivedScalarType.FLATTENED_ATTN_POST_SOFTMAX: make_flattened_attn_post_softmax_scalar_deriver,
    DerivedScalarType.ATTN_ACT_TIMES_GRAD: make_flattened_attn_post_softmax_act_times_grad_scalar_deriver,
    DerivedScalarType.RESID_DELTA_MLP: make_scalar_deriver_factory_for_activation_location_type(
        ActivationLocationType.RESID_DELTA_MLP,
    ),
    DerivedScalarType.RESID_DELTA_MLP_FROM_MLP_POST_ACT: make_resid_delta_mlp_from_mlp_post_act_scalar_deriver,
    DerivedScalarType.MLP_WRITE_NORM: make_mlp_write_norm_scalar_deriver,
    DerivedScalarType.MLP_ACT_TIMES_GRAD: make_scalar_deriver_factory_for_act_times_grad(
        ActivationLocationType.MLP_POST_ACT,
        DerivedScalarType.MLP_ACT_TIMES_GRAD,
    ),
    DerivedScalarType.MLP_WRITE_TO_FINAL_RESIDUAL_GRAD: make_mlp_write_to_final_residual_grad_scalar_deriver,
    DerivedScalarType.ATTN_WRITE_NORM_PER_SEQUENCE_TOKEN: make_attn_write_norm_per_sequence_token_scalar_deriver,
    DerivedScalarType.ATTN_WRITE_TO_FINAL_RESIDUAL_GRAD_PER_SEQUENCE_TOKEN: make_attn_write_to_final_residual_grad_per_sequence_token_scalar_deriver,
    DerivedScalarType.ATTN_ACT_TIMES_GRAD_PER_SEQUENCE_TOKEN: make_attn_act_times_grad_per_sequence_token_scalar_deriver,
    DerivedScalarType.RESID_POST_EMBEDDING_NORM: make_residual_norm_scalar_deriver_factory_for_activation_location_type(
        ActivationLocationType.RESID_POST_EMBEDDING
    ),
    DerivedScalarType.RESID_POST_MLP_NORM: make_residual_norm_scalar_deriver_factory_for_activation_location_type(
        ActivationLocationType.RESID_POST_MLP
    ),
    DerivedScalarType.RESID_POST_ATTN_NORM: make_residual_norm_scalar_deriver_factory_for_activation_location_type(
        ActivationLocationType.RESID_POST_ATTN
    ),
    DerivedScalarType.MLP_LAYER_WRITE_NORM: make_residual_norm_scalar_deriver_factory_for_activation_location_type(
        ActivationLocationType.RESID_DELTA_MLP
    ),
    DerivedScalarType.ATTN_LAYER_WRITE_NORM: make_residual_norm_scalar_deriver_factory_for_activation_location_type(
        ActivationLocationType.RESID_DELTA_ATTN
    ),
    DerivedScalarType.RESID_POST_EMBEDDING_PROJ_TO_FINAL_RESIDUAL_GRAD: make_residual_projection_to_final_residual_grad_scalar_deriver_factory_for_activation_location_type(
        ActivationLocationType.RESID_POST_EMBEDDING,
        use_existing_backward_pass_for_final_residual_grad=True,
    ),
    DerivedScalarType.RESID_POST_MLP_PROJ_TO_FINAL_RESIDUAL_GRAD: make_residual_projection_to_final_residual_grad_scalar_deriver_factory_for_activation_location_type(
        ActivationLocationType.RESID_POST_MLP,
        use_existing_backward_pass_for_final_residual_grad=True,
    ),
    DerivedScalarType.RESID_POST_ATTN_PROJ_TO_FINAL_RESIDUAL_GRAD: make_residual_projection_to_final_residual_grad_scalar_deriver_factory_for_activation_location_type(
        ActivationLocationType.RESID_POST_ATTN,
        use_existing_backward_pass_for_final_residual_grad=True,
    ),
    DerivedScalarType.MLP_LAYER_WRITE_TO_FINAL_RESIDUAL_GRAD: make_residual_projection_to_final_residual_grad_scalar_deriver_factory_for_activation_location_type(
        ActivationLocationType.RESID_DELTA_MLP,
        use_existing_backward_pass_for_final_residual_grad=True,
    ),
    DerivedScalarType.ATTN_LAYER_WRITE_TO_FINAL_RESIDUAL_GRAD: make_residual_projection_to_final_residual_grad_scalar_deriver_factory_for_activation_location_type(
        ActivationLocationType.RESID_DELTA_ATTN,
        use_existing_backward_pass_for_final_residual_grad=True,
    ),
    DerivedScalarType.UNFLATTENED_ATTN_ACT_TIMES_GRAD: make_scalar_deriver_factory_for_act_times_grad(
        ActivationLocationType.ATTN_QK_PROBS,
        DerivedScalarType.UNFLATTENED_ATTN_ACT_TIMES_GRAD,
    ),
    DerivedScalarType.UNFLATTENED_ATTN_WRITE_NORM: make_unflattened_attn_write_norm_scalar_deriver,
    DerivedScalarType.UNFLATTENED_ATTN_WRITE_TO_FINAL_RESIDUAL_GRAD: make_unflattened_attn_write_to_final_residual_grad_scalar_deriver,
    DerivedScalarType.ATTN_WRITE_TO_FINAL_RESIDUAL_GRAD: make_flattened_attn_write_to_final_residual_grad_scalar_deriver,
    DerivedScalarType.ONLINE_MLP_AUTOENCODER_ERROR: make_online_autoencoder_error_scalar_deriver_factory(
        ActivationLocationType.ONLINE_MLP_AUTOENCODER_ERROR
    ),
    DerivedScalarType.ONLINE_RESIDUAL_MLP_AUTOENCODER_ERROR: make_online_autoencoder_error_scalar_deriver_factory(
        ActivationLocationType.ONLINE_RESIDUAL_MLP_AUTOENCODER_ERROR
    ),
    DerivedScalarType.ONLINE_RESIDUAL_ATTENTION_AUTOENCODER_ERROR: make_online_autoencoder_error_scalar_deriver_factory(
        ActivationLocationType.ONLINE_RESIDUAL_ATTENTION_AUTOENCODER_ERROR
    ),
    DerivedScalarType.ONLINE_MLP_AUTOENCODER_ERROR_ACT_TIMES_GRAD: make_online_mlp_autoencoder_error_act_times_grad_scalar_deriver,
    DerivedScalarType.ONLINE_MLP_AUTOENCODER_ERROR_WRITE_NORM: make_online_mlp_autoencoder_error_write_norm_scalar_deriver,
    DerivedScalarType.ONLINE_MLP_AUTOENCODER_ERROR_WRITE_TO_FINAL_RESIDUAL_GRAD: make_online_mlp_autoencoder_error_write_to_final_residual_grad_scalar_deriver,
    DerivedScalarType.ATTN_WRITE: make_attn_write_scalar_deriver,
    DerivedScalarType.ATTN_WRITE_SUM_HEADS: make_attn_write_sum_heads_scalar_deriver,
    DerivedScalarType.MLP_WRITE: make_mlp_neuronwise_write_scalar_deriver,
    DerivedScalarType.ATTN_WEIGHTED_VALUE: make_attn_weighted_value_scalar_deriver,
    DerivedScalarType.PREVIOUS_LAYER_RESID_POST_MLP: make_previous_layer_resid_post_mlp_scalar_deriver,
    DerivedScalarType.MLP_WRITE_TO_FINAL_ACTIVATION_RESIDUAL_GRAD: make_mlp_write_to_final_activation_residual_grad_scalar_deriver,
    DerivedScalarType.UNFLATTENED_ATTN_WRITE_TO_FINAL_ACTIVATION_RESIDUAL_GRAD: make_unflattened_attn_write_to_final_activation_residual_grad_scalar_deriver,
    DerivedScalarType.RESID_POST_EMBEDDING_PROJ_TO_FINAL_ACTIVATION_RESIDUAL_GRAD: make_residual_projection_to_final_residual_grad_scalar_deriver_factory_for_activation_location_type(
        ActivationLocationType.RESID_POST_EMBEDDING,
        use_existing_backward_pass_for_final_residual_grad=False,
    ),
    DerivedScalarType.AUTOENCODER_LATENT_GRAD_WRT_RESIDUAL_INPUT: make_autoencoder_latent_grad_wrt_residual_input_scalar_deriver,
    DerivedScalarType.AUTOENCODER_LATENT_GRAD_WRT_MLP_POST_ACT_INPUT: make_autoencoder_latent_grad_wrt_mlp_post_act_input_scalar_deriver,
    DerivedScalarType.ATTN_WRITE_TO_LATENT: make_attn_write_to_latent_scalar_deriver,
    DerivedScalarType.ATTN_WRITE_TO_LATENT_SUMMED_OVER_HEADS: make_attn_write_to_latent_summed_over_heads_scalar_deriver,
    DerivedScalarType.FLATTENED_ATTN_WRITE_TO_LATENT_SUMMED_OVER_HEADS: make_flattened_attn_write_to_latent_summed_over_heads_scalar_deriver,
    DerivedScalarType.FLATTENED_ATTN_WRITE_TO_LATENT_SUMMED_OVER_HEADS_BATCHED: make_flattened_attn_write_to_latent_summed_over_heads_batched_scalar_deriver,
    DerivedScalarType.ATTN_WRITE_TO_LATENT_PER_SEQUENCE_TOKEN: make_attn_write_to_latent_per_sequence_token_scalar_deriver,
    DerivedScalarType.ATTN_WRITE_TO_LATENT_PER_SEQUENCE_TOKEN_BATCHED: make_attn_write_to_latent_per_sequence_token_batched_scalar_deriver,
    DerivedScalarType.TOKEN_ATTRIBUTION: make_token_attribution_scalar_deriver,
    DerivedScalarType.SINGLE_NODE_WRITE: make_node_write_scalar_deriver,
    DerivedScalarType.GRAD_OF_SINGLE_SUBNODE_ATTRIBUTION: make_grad_of_downstream_subnode_attribution_scalar_deriver,
    DerivedScalarType.ATTN_OUT_EDGE_ATTRIBUTION: make_attn_out_edge_attribution_scalar_deriver,
    DerivedScalarType.MLP_OUT_EDGE_ATTRIBUTION: make_mlp_out_edge_attribution_scalar_deriver,
    DerivedScalarType.ONLINE_AUTOENCODER_OUT_EDGE_ATTRIBUTION: make_online_autoencoder_out_edge_attribution_scalar_deriver,
    DerivedScalarType.ATTN_QUERY_IN_EDGE_ATTRIBUTION: make_in_edge_attribution_scalar_deriver_factory(
        NodeType.ATTENTION_HEAD, ActivationLocationType.ATTN_QUERY
    ),
    DerivedScalarType.ATTN_KEY_IN_EDGE_ATTRIBUTION: make_in_edge_attribution_scalar_deriver_factory(
        NodeType.ATTENTION_HEAD, ActivationLocationType.ATTN_KEY
    ),
    DerivedScalarType.ATTN_VALUE_IN_EDGE_ATTRIBUTION: make_in_edge_attribution_scalar_deriver_factory(
        NodeType.ATTENTION_HEAD, ActivationLocationType.ATTN_VALUE
    ),
    DerivedScalarType.MLP_IN_EDGE_ATTRIBUTION: make_in_edge_attribution_scalar_deriver_factory(
        NodeType.MLP_NEURON
    ),
    DerivedScalarType.ONLINE_AUTOENCODER_IN_EDGE_ATTRIBUTION: make_in_edge_attribution_scalar_deriver_factory(
        NodeType.AUTOENCODER_LATENT
    ),
    DerivedScalarType.SINGLE_NODE_WRITE_TO_FINAL_RESIDUAL_GRAD: make_node_write_to_final_residual_grad_scalar_deriver,
    DerivedScalarType.TOKEN_OUT_EDGE_ATTRIBUTION: make_token_out_edge_attribution_scalar_deriver,
    DerivedScalarType.VOCAB_TOKEN_WRITE_TO_INPUT_DIRECTION: make_vocab_token_write_to_input_direction_scalar_deriver,
    DerivedScalarType.ALWAYS_ONE: make_unity_scalar_deriver,
    DerivedScalarType.ATTN_QUERY_IN_EDGE_ACTIVATION: make_in_edge_activation_scalar_deriver_factory(
        ActivationLocationType.ATTN_QK_PROBS, ActivationLocationType.ATTN_QUERY
    ),
    DerivedScalarType.ATTN_KEY_IN_EDGE_ACTIVATION: make_in_edge_activation_scalar_deriver_factory(
        ActivationLocationType.ATTN_QK_PROBS, ActivationLocationType.ATTN_KEY
    ),
    DerivedScalarType.MLP_IN_EDGE_ACTIVATION: make_in_edge_activation_scalar_deriver_factory(
        ActivationLocationType.MLP_POST_ACT,
    ),
    DerivedScalarType.ONLINE_AUTOENCODER_IN_EDGE_ACTIVATION: make_in_edge_activation_scalar_deriver_factory(
        ActivationLocationType.ONLINE_AUTOENCODER_LATENT,
    ),
    DerivedScalarType.AUTOENCODER_LATENT: make_autoencoder_latent_scalar_deriver_factory(
        NodeType.AUTOENCODER_LATENT
    ),
    DerivedScalarType.MLP_AUTOENCODER_LATENT: make_autoencoder_latent_scalar_deriver_factory(
        NodeType.MLP_AUTOENCODER_LATENT
    ),
    DerivedScalarType.ATTENTION_AUTOENCODER_LATENT: make_autoencoder_latent_scalar_deriver_factory(
        NodeType.ATTENTION_AUTOENCODER_LATENT
    ),
    DerivedScalarType.ONLINE_AUTOENCODER_LATENT: make_online_autoencoder_latent_scalar_deriver_factory(
        NodeType.AUTOENCODER_LATENT
    ),
    DerivedScalarType.ONLINE_MLP_AUTOENCODER_LATENT: make_online_autoencoder_latent_scalar_deriver_factory(
        NodeType.MLP_AUTOENCODER_LATENT
    ),
    DerivedScalarType.ONLINE_ATTENTION_AUTOENCODER_LATENT: make_online_autoencoder_latent_scalar_deriver_factory(
        NodeType.ATTENTION_AUTOENCODER_LATENT
    ),
    DerivedScalarType.ONLINE_AUTOENCODER_ACT_TIMES_GRAD: make_online_autoencoder_act_times_grad_scalar_deriver_factory(
        NodeType.AUTOENCODER_LATENT
    ),
    DerivedScalarType.ONLINE_MLP_AUTOENCODER_ACT_TIMES_GRAD: make_online_autoencoder_act_times_grad_scalar_deriver_factory(
        NodeType.MLP_AUTOENCODER_LATENT
    ),
    DerivedScalarType.ONLINE_ATTENTION_AUTOENCODER_ACT_TIMES_GRAD: make_online_autoencoder_act_times_grad_scalar_deriver_factory(
        NodeType.ATTENTION_AUTOENCODER_LATENT
    ),
    DerivedScalarType.ONLINE_AUTOENCODER_WRITE_TO_FINAL_RESIDUAL_GRAD: make_online_autoencoder_write_to_final_residual_grad_scalar_deriver_factory(
        NodeType.AUTOENCODER_LATENT
    ),
    DerivedScalarType.ONLINE_MLP_AUTOENCODER_WRITE_TO_FINAL_RESIDUAL_GRAD: make_online_autoencoder_write_to_final_residual_grad_scalar_deriver_factory(
        NodeType.MLP_AUTOENCODER_LATENT
    ),
    DerivedScalarType.ONLINE_ATTENTION_AUTOENCODER_WRITE_TO_FINAL_RESIDUAL_GRAD: make_online_autoencoder_write_to_final_residual_grad_scalar_deriver_factory(
        NodeType.ATTENTION_AUTOENCODER_LATENT
    ),
    DerivedScalarType.ONLINE_AUTOENCODER_WRITE_TO_FINAL_ACTIVATION_RESIDUAL_GRAD: make_online_autoencoder_write_to_final_activation_residual_grad_scalar_deriver_factory(
        NodeType.AUTOENCODER_LATENT
    ),
    DerivedScalarType.ONLINE_MLP_AUTOENCODER_WRITE_TO_FINAL_ACTIVATION_RESIDUAL_GRAD: make_online_autoencoder_write_to_final_activation_residual_grad_scalar_deriver_factory(
        NodeType.MLP_AUTOENCODER_LATENT
    ),
    DerivedScalarType.ONLINE_ATTENTION_AUTOENCODER_WRITE_TO_FINAL_ACTIVATION_RESIDUAL_GRAD: make_online_autoencoder_write_to_final_activation_residual_grad_scalar_deriver_factory(
        NodeType.ATTENTION_AUTOENCODER_LATENT
    ),
    DerivedScalarType.ONLINE_AUTOENCODER_WRITE: make_online_autoencoder_latentwise_write_scalar_deriver_factory(
        NodeType.AUTOENCODER_LATENT
    ),
    DerivedScalarType.ONLINE_MLP_AUTOENCODER_WRITE: make_online_autoencoder_latentwise_write_scalar_deriver_factory(
        NodeType.MLP_AUTOENCODER_LATENT
    ),
    DerivedScalarType.ONLINE_ATTENTION_AUTOENCODER_WRITE: make_online_autoencoder_latentwise_write_scalar_deriver_factory(
        NodeType.ATTENTION_AUTOENCODER_LATENT
    ),
    DerivedScalarType.ONLINE_AUTOENCODER_WRITE_NORM: make_online_autoencoder_write_norm_scalar_deriver_factory(
        NodeType.AUTOENCODER_LATENT
    ),
    DerivedScalarType.ONLINE_MLP_AUTOENCODER_WRITE_NORM: make_online_autoencoder_write_norm_scalar_deriver_factory(
        NodeType.MLP_AUTOENCODER_LATENT
    ),
    DerivedScalarType.ONLINE_ATTENTION_AUTOENCODER_WRITE_NORM: make_online_autoencoder_write_norm_scalar_deriver_factory(
        NodeType.ATTENTION_AUTOENCODER_LATENT
    ),
    DerivedScalarType.AUTOENCODER_WRITE_NORM: make_autoencoder_write_norm_scalar_deriver_factory(
        NodeType.AUTOENCODER_LATENT
    ),
    DerivedScalarType.MLP_AUTOENCODER_WRITE_NORM: make_autoencoder_write_norm_scalar_deriver_factory(
        NodeType.MLP_AUTOENCODER_LATENT
    ),
    DerivedScalarType.ATTENTION_AUTOENCODER_WRITE_NORM: make_autoencoder_write_norm_scalar_deriver_factory(
        NodeType.ATTENTION_AUTOENCODER_LATENT
    ),
}
def make_scalar_deriver(
    dst: DerivedScalarType,
    dst_config: DstConfig,
) -> ScalarDeriver:
    """The model name and layer indices of interest might or might not need to be specified
    based on the dst. In particular, if the dst
    is also a HookLocationType, then the model name and layer indices are not needed."""
    assert dst in _DERIVED_SCALAR_TYPE_REGISTRY, f"Unknown {dst=}"
    # this is derived from one or more HookLocationTypes, via the function
    # specified in the registry
    make_scalar_deriver_fn = _DERIVED_SCALAR_TYPE_REGISTRY[dst]
    return make_scalar_deriver_fn(dst_config)
def make_scalar_deriver_for_activation_location_type(
    activation_location_type: ActivationLocationType,
    derive_gradients: bool = False,
) -> ScalarDeriver:
    return make_scalar_deriver(
        DerivedScalarType.from_activation_location_type(activation_location_type),
        dst_config=DstConfig(
            derive_gradients=derive_gradients,
        ),
    )

================
File: neuron_explainer/activations/derived_scalars/mlp.py
================
from neuron_explainer.activations.derived_scalars.derived_scalar_types import DerivedScalarType
from neuron_explainer.activations.derived_scalars.direct_effects import (
    convert_scalar_deriver_to_write_to_final_residual_grad,
)
from neuron_explainer.activations.derived_scalars.raw_activations import (
    convert_scalar_deriver_to_write,
    convert_scalar_deriver_to_write_norm,
    convert_scalar_deriver_to_write_vector,
    make_scalar_deriver_factory_for_activation_location_type,
)
from neuron_explainer.activations.derived_scalars.scalar_deriver import DstConfig, ScalarDeriver
from neuron_explainer.activations.derived_scalars.write_tensors import (
    get_mlp_write_tensor_by_layer_index,
)
from neuron_explainer.models.model_component_registry import ActivationLocationType
"""This file contains code to compute derived scalars related to MLP activations (typically an
MLP activation multiplied by some other value, such as the norm of that MLP neuron's output weight vector,
or the gradient of the loss with respect to that MLP activation)."""
def get_base_mlp_scalar_deriver(
    dst_config: DstConfig,
) -> ScalarDeriver:
    """Returns a scalar deriver for the MLP activations."""
    return make_scalar_deriver_factory_for_activation_location_type(
        activation_location_type=ActivationLocationType.MLP_POST_ACT,
    )(dst_config)
def make_mlp_write_norm_scalar_deriver(
    dst_config: DstConfig,
) -> ScalarDeriver:
    """Returns a scalar deriver for the write norm for each
    MLP neuron at each token."""
    model_context = dst_config.get_model_context()
    layer_indices = dst_config.layer_indices or list(range(model_context.n_layers))
    scalar_deriver = get_base_mlp_scalar_deriver(
        dst_config=dst_config,
    )
    W_out_by_layer_index = get_mlp_write_tensor_by_layer_index(
        model_context=model_context, layer_indices=layer_indices
    )
    return convert_scalar_deriver_to_write_norm(
        scalar_deriver=scalar_deriver,
        write_tensor_by_layer_index=W_out_by_layer_index,
        output_dst=DerivedScalarType.MLP_WRITE_NORM,
    )
def make_mlp_write_to_final_residual_grad_scalar_deriver(
    dst_config: DstConfig,
) -> ScalarDeriver:
    scalar_deriver = get_base_mlp_scalar_deriver(
        dst_config=dst_config,
    )
    return convert_scalar_deriver_to_write_to_final_residual_grad(
        scalar_deriver=scalar_deriver,
        output_dst=DerivedScalarType.MLP_WRITE_TO_FINAL_RESIDUAL_GRAD,
        use_existing_backward_pass_for_final_residual_grad=True,
    )
def make_mlp_write_to_final_activation_residual_grad_scalar_deriver(
    dst_config: DstConfig,
) -> ScalarDeriver:
    scalar_deriver = get_base_mlp_scalar_deriver(
        dst_config=dst_config,
    )
    return convert_scalar_deriver_to_write_to_final_residual_grad(
        scalar_deriver=scalar_deriver,
        output_dst=DerivedScalarType.MLP_WRITE_TO_FINAL_ACTIVATION_RESIDUAL_GRAD,
        use_existing_backward_pass_for_final_residual_grad=False,
    )
def make_resid_delta_mlp_from_mlp_post_act_scalar_deriver(
    dst_config: DstConfig,
) -> ScalarDeriver:
    """Returns a scalar deriver for the write vector for the MLP layer at each token."""
    model_context = dst_config.get_model_context()
    layer_indices = dst_config.layer_indices or list(range(model_context.n_layers))
    scalar_deriver = get_base_mlp_scalar_deriver(
        dst_config=dst_config,
    )
    W_out_by_layer_index = get_mlp_write_tensor_by_layer_index(
        model_context=model_context, layer_indices=layer_indices
    )
    return convert_scalar_deriver_to_write(
        scalar_deriver=scalar_deriver,
        write_tensor_by_layer_index=W_out_by_layer_index,
        output_dst=DerivedScalarType.RESID_DELTA_MLP_FROM_MLP_POST_ACT,
    )
def make_mlp_neuronwise_write_scalar_deriver(
    dst_config: DstConfig,
) -> ScalarDeriver:
    """Returns a scalar deriver for the write vector of each MLP neuron at each token."""
    model_context = dst_config.get_model_context()
    layer_indices = dst_config.layer_indices or list(range(model_context.n_layers))
    scalar_deriver = get_base_mlp_scalar_deriver(
        dst_config=dst_config,
    )
    W_out_by_layer_index = get_mlp_write_tensor_by_layer_index(
        model_context=model_context, layer_indices=layer_indices
    )
    return convert_scalar_deriver_to_write_vector(
        scalar_deriver=scalar_deriver,
        write_tensor_by_layer_index=W_out_by_layer_index,
        output_dst=DerivedScalarType.MLP_WRITE,
    )

================
File: neuron_explainer/activations/derived_scalars/multi_group.py
================
"""
This file contains code for computing top-k operations on multiple "groups" of derived
scalars, where a group is defined as having comparable units, and each group contains derived
scalars with the same set of NodeTypes. This is used e.g. for determining the elements to display
on a TDB node table.
"""
from typing import Callable
import torch
from neuron_explainer.activation_server.requests_and_responses import GroupId
from neuron_explainer.activations.derived_scalars.activations_and_metadata import RawActivationStore
from neuron_explainer.activations.derived_scalars.config import DstConfig
from neuron_explainer.activations.derived_scalars.derived_scalar_store import DerivedScalarStore
from neuron_explainer.activations.derived_scalars.derived_scalar_types import DerivedScalarType
from neuron_explainer.activations.derived_scalars.indexing import NodeIndex
from neuron_explainer.activations.derived_scalars.make_scalar_derivers import make_scalar_deriver
from neuron_explainer.activations.derived_scalars.scalar_deriver import ScalarDeriver
from neuron_explainer.models.model_component_registry import (
    ActivationLocationTypeAndPassType,
    NodeType,
)
class MultiGroupDerivedScalarStore:
    """
    This contains multiple DerivedScalarStores, each of which is taken to have comparable units
    among different derived scalars in the same DerivedScalarStore. For example, MLP_WRITE_NORM
    and ATTN_WRITE_NORM. The groups of derived scalars are associated with GroupIds. This supports
    a topk operation, which
    1. computes the top k derived scalars within each group, and associates each of the top values
    to a NodeIndex
    2. takes the union over all top NodeIndex objects for in any group
    3. computes the derived scalars for each group, for each NodeIndex in that union
    4. returns the list[NodeIndices] and a dict[GroupId, list[derived scalar values]]
    """
    def __init__(
        self,
        derived_scalars_by_group_id: dict[GroupId, DerivedScalarStore],
        exclude_bottom_k_by_group_id: dict[GroupId, bool] | None = None,
    ):
        self.derived_scalars_by_group_id = derived_scalars_by_group_id
        for ds_store in derived_scalars_by_group_id.values():
            assert (
                ds_store.node_types == self.node_types
            )  # all DerivedScalarStores must contain the same node_types
        if exclude_bottom_k_by_group_id is not None:
            assert set(exclude_bottom_k_by_group_id.keys()) == set(
                derived_scalars_by_group_id.keys()
            )
            self.exclude_bottom_k_by_group_id = exclude_bottom_k_by_group_id
        else:
            self.exclude_bottom_k_by_group_id = {
                group_id: False for group_id in derived_scalars_by_group_id.keys()
            }
    def to_single_ds_store(self) -> DerivedScalarStore:
        assert len(self.derived_scalars_by_group_id) == 1
        assert list(self.derived_scalars_by_group_id.keys()) == [GroupId.SINGLETON]
        return list(self.derived_scalars_by_group_id.values())[0]
    @classmethod
    def derive_from_raw(
        cls,
        raw_activation_store: RawActivationStore,
        multi_group_scalar_derivers: "MultiGroupScalarDerivers",
    ) -> "MultiGroupDerivedScalarStore":
        derived_scalars_by_group_id = {
            group_id: DerivedScalarStore.derive_from_raw(raw_activation_store, scalar_derivers)
            for group_id, scalar_derivers in multi_group_scalar_derivers.scalar_derivers_by_group_id.items()
        }
        exclude_bottom_k_by_group_id = multi_group_scalar_derivers.exclude_bottom_k_by_group_id
        return cls(derived_scalars_by_group_id, exclude_bottom_k_by_group_id)
    @property
    def group_ids(self) -> set[GroupId]:
        return set(self.derived_scalars_by_group_id.keys())
    @property
    def node_types(self) -> set[NodeType]:
        return next(iter(self.derived_scalars_by_group_id.values())).node_types
    def get_ds_store(self, group_id: GroupId) -> DerivedScalarStore:
        assert group_id in self.derived_scalars_by_group_id, (
            group_id,
            self.derived_scalars_by_group_id.keys(),
        )
        return self.derived_scalars_by_group_id[group_id]
    def _topk_of_group(
        self,
        group_id: GroupId,
        top_and_bottom_k: int | None,
        transform_activations_fn: Callable[[torch.Tensor], torch.Tensor] | None = None,
        transform_indices_fn: Callable[[NodeIndex], NodeIndex] | None = None,
    ) -> tuple[torch.Tensor, list[NodeIndex]]:
        """
        Depending on whether self.exclude_bottom_k_by_group_id[group_id] is True or False, this will return either the top k activations
        or the top and bottom k activations, respectively; in the latter case, they are concatenated together
        """
        derived_scalar_store = self.get_ds_store(group_id)
        if transform_activations_fn is not None:
            derived_scalar_store = derived_scalar_store.apply_transform_fn_to_activations(
                transform_activations_fn
            )
        top_activations, top_ds_indices = derived_scalar_store.topk(top_and_bottom_k, largest=True)
        if not self.exclude_bottom_k_by_group_id[group_id]:
            bottom_activations, bottom_ds_indices = derived_scalar_store.topk(
                top_and_bottom_k, largest=False
            )
            top_activations = torch.cat([top_activations, bottom_activations], dim=0)
            top_ds_indices += bottom_ds_indices
        top_node_indices = [NodeIndex.from_ds_index(ds_index) for ds_index in top_ds_indices]
        if transform_indices_fn is not None:
            top_node_indices = [transform_indices_fn(node_index) for node_index in top_node_indices]
        return top_activations, top_node_indices
    def topk(
        self,
        top_and_bottom_k: int | None,
        transform_activations_fn: Callable[[torch.Tensor], torch.Tensor] | None = None,
        transform_indices_fn: Callable[[NodeIndex], NodeIndex] | None = None,
    ) -> tuple[list[NodeIndex], dict[GroupId, torch.Tensor]]:
        """
        Return all node indices which were in the top or bottom k of any group id (as applicable according to self.exclude_bottom_k_by_group_id)
        Also return the derived scalar values for each group id for each node index, in a dict
        """
        top_node_indices_list = []
        for group_id in self.derived_scalars_by_group_id.keys():
            _, top_node_indices = self._topk_of_group(
                group_id,
                top_and_bottom_k,
                transform_activations_fn,
                transform_indices_fn,
            )
            top_node_indices_list.extend(top_node_indices)
        # Create a list of all node indices across all groups, removing duplicates. We need to maintain
        # a set and a list because we need to maintain the original order (which is why we can't use the
        # list(set()) trick). We need to maintain the original order in order to make the output deterministic.
        # these are the indices of nodes that were in the top-k for at least one kind of derived scalar
        # in parallel construct a matching list of dicts of token information
        all_node_indices = []
        unique_indices = set()
        for node_index in top_node_indices_list:
            if node_index not in unique_indices:
                unique_indices.add(node_index)
                all_node_indices.append(node_index)
        # for nodes that were top-k in at least one kind of derived scalar, convert from the index of the node
        # to the index of the relevant derived scalar for each group id;
        # using the derived scalar indices, access the derived scalar values themselves for all relevant node
        # indices
        activations_by_group_id = self.get_derived_scalars_by_group_id_for_node_indices(
            all_node_indices
        )
        return all_node_indices, activations_by_group_id
    def get_derived_scalars_by_group_id_for_node_indices(
        self, node_indices: list[NodeIndex]
    ) -> dict[GroupId, torch.Tensor]:
        return {
            group_id: (
                self.get_ds_store(group_id).get_derived_scalars_for_node_indices(node_indices)
            )
            for group_id in self.group_ids
        }
class MultiGroupScalarDerivers:
    """
    This contains multiple ScalarDerivers categorized into groups. Each group of ScalarDerivers is taken
    to have outputs with common units (see MultiGroupDerivedScalarStore) docstring
    """
    def __init__(
        self,
        scalar_derivers_by_group_id: dict[GroupId, list[ScalarDeriver]],
        exclude_bottom_k_by_group_id: dict[GroupId, bool] | None = None,
    ):
        self.scalar_derivers_by_group_id = scalar_derivers_by_group_id
        if exclude_bottom_k_by_group_id is not None:
            assert set(exclude_bottom_k_by_group_id.keys()) == set(
                scalar_derivers_by_group_id.keys()
            )
            self.exclude_bottom_k_by_group_id = exclude_bottom_k_by_group_id
        else:
            self.exclude_bottom_k_by_group_id = {
                group_id: False for group_id in scalar_derivers_by_group_id.keys()
            }
    @classmethod
    def from_scalar_derivers(
        cls, scalar_derivers: list[ScalarDeriver]
    ) -> "MultiGroupScalarDerivers":
        return cls({GroupId.SINGLETON: scalar_derivers}, None)
    @classmethod
    def from_dst_and_config_list(
        cls, dst_and_config_list: list[tuple[DerivedScalarType, DstConfig]]
    ) -> "MultiGroupScalarDerivers":
        scalar_derivers = [
            make_scalar_deriver(
                dst=dst,
                dst_config=dst_config,
            )
            for dst, dst_config in dst_and_config_list
        ]
        return cls.from_scalar_derivers(scalar_derivers)
    @classmethod
    def from_dst_and_config_list_by_group_id(
        cls,
        dst_and_config_list_by_group_id: dict[GroupId, list[tuple[DerivedScalarType, DstConfig]]],
        exclude_bottom_k_by_group_id: dict[GroupId, bool] | None = None,
    ) -> "MultiGroupScalarDerivers":
        # NOTE: requires that all groups have the same node types, appearing in the same order within
        # each of dst_and_config_list_by_group_id's values
        node_types_by_group_id = {
            group_id: tuple(dst.node_type for dst, _ in dst_and_config_list)
            for group_id, dst_and_config_list in dst_and_config_list_by_group_id.items()
        }
        assert (
            len(set(node_types_by_group_id.values())) == 1
        ), node_types_by_group_id  # all groups must have the same node types
        scalar_derivers_by_group_id = {
            group_id: [
                make_scalar_deriver(
                    dst=dst,
                    dst_config=dst_config,
                )
                for dst, dst_config in dst_and_config_list
            ]
            for group_id, dst_and_config_list in dst_and_config_list_by_group_id.items()
        }
        return cls(scalar_derivers_by_group_id, exclude_bottom_k_by_group_id)
    @property
    def activation_location_type_and_pass_types(self) -> list[ActivationLocationTypeAndPassType]:
        return list(
            {
                sub_activation_location_type_and_pass_type
                for scalar_deriver_list in self.scalar_derivers_by_group_id.values()
                for scalar_deriver in scalar_deriver_list
                for sub_activation_location_type_and_pass_type in scalar_deriver.get_sub_activation_location_type_and_pass_types()
            }
        )
    @property
    def devices_for_raw_activations(self) -> list[torch.device]:
        return list(
            {
                scalar_deriver.device_for_raw_activations
                for scalar_deriver_list in self.scalar_derivers_by_group_id.values()
                for scalar_deriver in scalar_deriver_list
            }
        )

================
File: neuron_explainer/activations/derived_scalars/multi_pass_scalar_deriver.py
================
"""
MultiPassScalarDerivers extend the functionality of ScalarDerivers by specifying how to derive a
scalar from some combination of the activations in multiple prompts.
The pairs of identical interfaces are:
ScalarDeriver:MultiPassScalarDeriver
ScalarSource:MultiPassScalarSource
RawActivationStore:MultiPassRawActivationStore
Both sets of objects can be used in populating a DerivedScalarStore. The intention is that it should
be possible to swap
derived_scalar_store = DerivedScalarStore.derive_from_raw(
    multi_pass_raw_activation_store,
    multi_pass_scalar_derivers,
)
for
batched_derived_scalar_store = [
    DerivedScalarStore.derive_from_raw(
        raw_activation_store,
        scalar_derivers,
    )
    for scalar_derivers, raw_activation_store
    in zip(batched_scalar_derivers, batched_raw_activation_store)
]
in order to compute derived scalars combining activations across multiple prompts in a batch.
Probable TODO: make an ABC, from which both ScalarDeriver and MultiPassScalarDeriver inherit
"""
from abc import ABC, abstractmethod
from enum import Enum
from neuron_explainer.activations.derived_scalars import DerivedScalarType
from neuron_explainer.activations.derived_scalars.derived_scalar_store import RawActivationStore
from neuron_explainer.activations.derived_scalars.locations import LayerIndexer, StaticLayerIndexer
from neuron_explainer.activations.derived_scalars.scalar_deriver import (
    ActivationsAndMetadata,
    DerivedScalarTypeAndPassType,
    ScalarDeriver,
    ScalarSource,
)
from neuron_explainer.models.model_component_registry import (
    ActivationLocationType,
    ActivationLocationTypeAndPassType,
    LayerIndex,
    LocationWithinLayer,
    PassType,
)
class PromptId(Enum):
    MAIN = "main"
    BASELINE = "baseline"
class PromptCombo(Enum):
    MAIN = "main"
    BASELINE = "baseline"
    SUBTRACT_BASELINE = "subtract_baseline"
    @property
    def required_prompt_ids(self) -> tuple[PromptId, ...]:
        match self:
            case PromptCombo.MAIN:
                return (PromptId.MAIN,)
            case PromptCombo.BASELINE:
                return (PromptId.BASELINE,)
            case PromptCombo.SUBTRACT_BASELINE:
                return (PromptId.MAIN, PromptId.BASELINE)
            case _:
                raise NotImplementedError
    def compute(
        self, activations_by_prompt_id: dict[PromptId, ActivationsAndMetadata]
    ) -> ActivationsAndMetadata:
        match self:
            case PromptCombo.MAIN:
                assert len(activations_by_prompt_id) == 1
                main = activations_by_prompt_id.pop(PromptId.MAIN)
                return main
            case PromptCombo.BASELINE:
                assert len(activations_by_prompt_id) == 1
                baseline = activations_by_prompt_id.pop(PromptId.BASELINE)
                return baseline
            case PromptCombo.SUBTRACT_BASELINE:
                main = activations_by_prompt_id.pop(PromptId.MAIN)
                baseline = activations_by_prompt_id.pop(PromptId.BASELINE)
                assert len(activations_by_prompt_id) == 0
                return main - baseline
            case _:
                raise NotImplementedError
    def derive_from_raw(
        self,
        multi_pass_raw_activation_store: "MultiPassRawActivationStore",
        scalar_source: ScalarSource,
        desired_layer_indices: (
            list[LayerIndex] | None
        ),  # indicates layer indices to keep; None indicates keep all
    ) -> ActivationsAndMetadata:
        activations_by_prompt_id: dict[PromptId, ActivationsAndMetadata] = {}
        for prompt_id in self.required_prompt_ids:
            raw_activation_store = (
                multi_pass_raw_activation_store.raw_activation_store_by_prompt_id[prompt_id]
            )
            activations_by_prompt_id[prompt_id] = scalar_source.derive_from_raw(
                raw_activation_store, desired_layer_indices
            )
        return self.compute(activations_by_prompt_id)
class MultiPassScalarSource(ABC):
    pass_type: PassType
    layer_indexer: LayerIndexer
    @property
    @abstractmethod
    def exists_by_default(self) -> bool:
        # returns True if the activation is instantiated by default in a normal transformer forward pass
        # this is False for activations related to autoencoders or for non-trivial derived scalars
        pass
    @property
    @abstractmethod
    def dst(self) -> DerivedScalarType:
        pass
    @property
    def dst_and_pass_type(self) -> DerivedScalarTypeAndPassType:
        return DerivedScalarTypeAndPassType(
            self.dst,
            self.pass_type,
        )
    @property
    @abstractmethod
    def sub_activation_location_type_and_pass_types(
        self,
    ) -> tuple[ActivationLocationTypeAndPassType, ...]:
        pass
    @property
    @abstractmethod
    def location_within_layer(self) -> LocationWithinLayer | None:
        pass
    @property
    def layer_index(self) -> LayerIndex:
        """Convenience method to get the single layer index associated with this ScalarSource, if such a single layer index
        exists. Throws an error if it does not."""
        assert isinstance(self.layer_indexer, StaticLayerIndexer), (
            self.layer_indexer,
            "ScalarSource.layer_index should only be called for ScalarSource StaticLayerIndexer",
        )
        return self.layer_indexer.layer_index
    @abstractmethod
    def derive_from_raw(
        self,
        multi_pass_raw_activation_store: "MultiPassRawActivationStore",
        desired_layer_indices: (
            list[LayerIndex] | None
        ),  # indicates layer indices to keep; None indicates keep all
    ) -> ActivationsAndMetadata:
        """
        See scalar_deriver.ScalarSource.derive_from_raw for explanation.
        """
        pass
class SinglePromptComboScalarSource(MultiPassScalarSource):
    """
    A SinglePromptComboScalarSource can be computed using some function
    derived_scalar_A = f(derived_scalar_A_from_one_prompt[, derived_scalar_A_from_another_prompt, ...])
    This is distinct from a MixedScalarSource, which is computed using some function of derived
    scalars from SinglePromptComboScalarSources or other MixedScalarSources. For example, a
    MixedScalarSource might be computed using some function:
    derived_scalar_A = f(
        g(sub_derived_scalar_B_from_one_prompt[, sub_derived_scalar_B_from_another_prompt, ...]),
        h(sub_derived_scalar_C_from_one_prompt[, sub_derived_scalar_C_from_another_prompt, ...]),
    )
    """
    scalar_source: ScalarSource
    prompt_combo: PromptCombo
    def __init__(self, scalar_source: ScalarSource, prompt_combo: PromptCombo):
        self.scalar_source = scalar_source
        self.prompt_combo = prompt_combo
    @property
    def exists_by_default(self) -> bool:
        return self.scalar_source.exists_by_default
    @property
    def dst(self) -> DerivedScalarType:
        return self.scalar_source.dst
    @property
    def sub_activation_location_type_and_pass_types(
        self,
    ) -> tuple[ActivationLocationTypeAndPassType, ...]:
        return self.scalar_source.sub_activation_location_type_and_pass_types
    @property
    def location_within_layer(self) -> LocationWithinLayer | None:
        return self.scalar_source.location_within_layer
    def derive_from_raw(
        self,
        multi_pass_raw_activation_store: "MultiPassRawActivationStore",
        desired_layer_indices: (
            list[LayerIndex] | None
        ),  # indicates layer indices to keep; None indicates keep all
    ) -> ActivationsAndMetadata:
        return self.prompt_combo.derive_from_raw(
            multi_pass_raw_activation_store, self.scalar_source, desired_layer_indices
        )
class MixedScalarSource(MultiPassScalarSource):
    multi_pass_scalar_deriver: "MultiPassScalarDeriver"
    pass_type: PassType
    layer_indexer: LayerIndexer
    def __init__(
        self,
        multi_pass_scalar_deriver: "MultiPassScalarDeriver",
        pass_type: PassType,
        layer_indexer: LayerIndexer,
    ):
        self.multi_pass_scalar_deriver = multi_pass_scalar_deriver
        self.pass_type = pass_type
        self.layer_indexer = layer_indexer
    @property
    def exists_by_default(self) -> bool:
        return False
    @property
    def dst(self) -> DerivedScalarType:
        return self.multi_pass_scalar_deriver.dst
    @property
    def sub_activation_location_type_and_pass_types(
        self,
    ) -> tuple[ActivationLocationTypeAndPassType, ...]:
        return self.multi_pass_scalar_deriver.get_sub_activation_location_type_and_pass_types()
    @property
    def location_within_layer(self) -> LocationWithinLayer | None:
        return self.multi_pass_scalar_deriver.scalar_deriver.location_within_layer
    def derive_from_raw(
        self,
        multi_pass_raw_activation_store: "MultiPassRawActivationStore",
        desired_layer_indices: (
            list[LayerIndex] | None
        ),  # indicates layer indices to keep; None indicates keep all
    ) -> ActivationsAndMetadata:
        return self.multi_pass_scalar_deriver.derive_from_raw(
            multi_pass_raw_activation_store, self.pass_type
        ).apply_layer_indexer(self.layer_indexer, desired_layer_indices)
class MultiPassScalarDeriver:
    scalar_deriver: ScalarDeriver
    sub_scalar_sources: tuple[MultiPassScalarSource, ...]
    def __init__(
        self, scalar_deriver: ScalarDeriver, sub_scalar_sources: tuple[MultiPassScalarSource, ...]
    ):
        self.scalar_deriver = scalar_deriver
        self.sub_scalar_sources = sub_scalar_sources
        assert [
            sub_scalar_source.dst_and_pass_type for sub_scalar_source in sub_scalar_sources
        ] == list(scalar_deriver.get_sub_dst_and_pass_types())
    @classmethod
    def from_scalar_deriver_and_sub_prompt_combos(
        cls,
        scalar_deriver: ScalarDeriver,
        sub_prompt_combos: tuple[PromptCombo, ...],
    ) -> "MultiPassScalarDeriver":
        assert len(scalar_deriver.sub_scalar_sources) == len(sub_prompt_combos)
        sub_scalar_sources = tuple(
            [
                SinglePromptComboScalarSource(scalar_source, prompt_combo)
                for scalar_source, prompt_combo in zip(
                    scalar_deriver.sub_scalar_sources, sub_prompt_combos
                )
            ]
        )
        return cls(scalar_deriver, sub_scalar_sources)
    @property
    def dst(self) -> DerivedScalarType:
        return self.scalar_deriver.dst
    @property
    def derivable_pass_types(self) -> tuple[PassType, ...]:
        return self.scalar_deriver.derivable_pass_types
    def activations_and_metadata_calculate_derived_scalar_fn(
        self, activation_data_tuple: tuple[ActivationsAndMetadata, ...], pass_type: PassType
    ) -> ActivationsAndMetadata:
        return self.scalar_deriver.activations_and_metadata_calculate_derived_scalar_fn(
            activation_data_tuple, pass_type
        )
    def get_sub_activation_location_type_and_pass_types(
        self,
    ) -> tuple[ActivationLocationTypeAndPassType, ...]:
        return self.scalar_deriver.get_sub_activation_location_type_and_pass_types()
    def derive_from_raw(
        self,
        multi_pass_raw_activation_store: "MultiPassRawActivationStore",
        pass_type: PassType,
    ) -> ActivationsAndMetadata:
        activations_list = []
        desired_layer_indices = None
        for scalar_source in self.sub_scalar_sources:
            activations = scalar_source.derive_from_raw(
                multi_pass_raw_activation_store, desired_layer_indices
            )
            activations_list.append(activations)
            if len(activations_list) == 1:
                # match the layer_indices of the first activations_and_metadata object
                desired_layer_indices = list(activations_list[0].layer_indices)
        return self.activations_and_metadata_calculate_derived_scalar_fn(
            tuple(activations_list),
            pass_type,
        )
# TODO: Run PromptCombo.derive_from_raw(scalar_source, raw_activation_store) as a part of
# MultiPassScalarSource.derive_from_raw(raw_activation_store)
class MultiPassRawActivationStore:
    raw_activation_store_by_prompt_id: dict[PromptId, RawActivationStore]
    def get_activations_and_metadata(
        self,
        prompt_id: PromptId,
        activation_location_type: ActivationLocationType,
        pass_type: PassType,
    ) -> ActivationsAndMetadata:
        return self.raw_activation_store_by_prompt_id[prompt_id].get_activations_and_metadata(
            activation_location_type, pass_type
        )

================
File: neuron_explainer/activations/derived_scalars/node_write.py
================
"""This file defines ScalarDerivers for a single node's residual stream write vector."""
import dataclasses
import torch
from neuron_explainer.activations.derived_scalars.attention import (
    make_attn_weighted_value_scalar_deriver,
)
from neuron_explainer.activations.derived_scalars.autoencoder import (
    make_online_autoencoder_latent_scalar_deriver_factory,
)
from neuron_explainer.activations.derived_scalars.derived_scalar_types import DerivedScalarType
from neuron_explainer.activations.derived_scalars.indexing import (
    make_python_slice_from_tensor_indices,
)
from neuron_explainer.activations.derived_scalars.locations import ConstantLayerIndexer
from neuron_explainer.activations.derived_scalars.mlp import get_base_mlp_scalar_deriver
from neuron_explainer.activations.derived_scalars.postprocessing import ResidualWriteConverter
from neuron_explainer.activations.derived_scalars.scalar_deriver import (
    DerivedScalarSource,
    DstConfig,
    ScalarDeriver,
)
from neuron_explainer.models.autoencoder_context import MultiAutoencoderContext
from neuron_explainer.models.model_component_registry import NodeType, PassType
def make_node_write_scalar_deriver(
    dst_config: DstConfig,
) -> ScalarDeriver:
    """Returns a scalar deriver for the write vector from some upstream node type (MLP, autoencoder, or attention head)."""
    node_index = dst_config.node_index_for_attribution
    assert node_index is not None
    assert node_index.layer_index is not None
    model_context = dst_config.get_model_context()
    autoencoder_context = dst_config.get_autoencoder_context()
    multi_autoencoder_context = MultiAutoencoderContext.from_context_or_multi_context(
        autoencoder_context
    )
    residual_write_converter = ResidualWriteConverter(
        model_context=model_context,
        multi_autoencoder_context=multi_autoencoder_context,
    )  # though called a Postprocessor, this converter is being used as part of the computation of this DST
    # It knows how to generate a residual stream write vector for a single node, and skips out on generating
    # residual stream write vectors for the entire layer worth of nodes, which is a much bigger/unnecessary matmul.
    dst_config_for_attribution = dataclasses.replace(
        dst_config,
        layer_indices=[node_index.layer_index],
    )
    match node_index.node_type:
        case NodeType.ATTENTION_HEAD:
            activation_scalar_deriver = make_attn_weighted_value_scalar_deriver(
                dst_config=dst_config_for_attribution,
            )
        case NodeType.MLP_NEURON:
            activation_scalar_deriver = get_base_mlp_scalar_deriver(
                dst_config=dst_config_for_attribution,
            )
        case (
            NodeType.AUTOENCODER_LATENT
            | NodeType.MLP_AUTOENCODER_LATENT
            | NodeType.ATTENTION_AUTOENCODER_LATENT
        ):
            activation_scalar_deriver = make_online_autoencoder_latent_scalar_deriver_factory(
                node_index.node_type
            )(dst_config_for_attribution)
    ds_index = residual_write_converter.convert_node_index_to_ds_index(node_index)
    sequence_token_index = ds_index.tensor_indices[0]
    slices_for_ds_index = make_python_slice_from_tensor_indices(ds_index.tensor_indices)
    def convert_activations_to_node_write(
        activations: torch.Tensor,
        layer_index: int | None,
        pass_type: PassType,
    ) -> torch.Tensor:
        assert pass_type == PassType.FORWARD
        assert layer_index == node_index.layer_index
        single_node_write = residual_write_converter.postprocess_tensor(
            ds_index,
            activations[slices_for_ds_index],
        )
        num_sequence_tokens = activations.shape[0]
        single_node_write_with_zeros = torch.zeros(
            (num_sequence_tokens,) + single_node_write.shape, device=single_node_write.device
        )
        single_node_write_with_zeros[sequence_token_index, :] = single_node_write
        return single_node_write_with_zeros
    return activation_scalar_deriver.apply_layerwise_transform_fn_to_output(
        convert_activations_to_node_write,
        pass_type_to_transform=PassType.FORWARD,
        output_dst=DerivedScalarType.SINGLE_NODE_WRITE,
    )
def make_node_write_scalar_source(
    dst_config: DstConfig,
) -> DerivedScalarSource:
    assert dst_config.node_index_for_attribution is not None
    layer_index = dst_config.node_index_for_attribution.layer_index
    assert layer_index is not None
    node_write_scalar_deriver = make_node_write_scalar_deriver(dst_config)
    return DerivedScalarSource(
        scalar_deriver=node_write_scalar_deriver,
        pass_type=PassType.FORWARD,
        layer_indexer=ConstantLayerIndexer(layer_index),
    )

================
File: neuron_explainer/activations/derived_scalars/postprocessing.py
================
from abc import ABC, abstractmethod
from typing import Any
import torch
from neuron_explainer.activations.derived_scalars.derived_scalar_store import DerivedScalarStore
from neuron_explainer.activations.derived_scalars.indexing import (
    DETACH_LAYER_NORM_SCALE,
    AttnSubNodeIndex,
    DerivedScalarIndex,
    MirroredNodeIndex,
    NodeIndex,
    PreOrPostAct,
    TraceConfig,
)
from neuron_explainer.activations.derived_scalars.locations import (
    get_previous_residual_dst_for_node_type,
)
from neuron_explainer.activations.derived_scalars.reconstituter_class import (
    make_reconstituted_gradient_fn,
)
from neuron_explainer.activations.derived_scalars.scalar_deriver import DerivedScalarType, DstConfig
from neuron_explainer.models.autoencoder_context import (
    AutoencoderContext,
    MultiAutoencoderContext,
    get_decoder_weight,
)
from neuron_explainer.models.model_component_registry import (
    ActivationLocationType,
    Dimension,
    NodeType,
    PassType,
    WeightLocationType,
)
from neuron_explainer.models.model_context import (
    ModelContext,
    StandardModelContext,
    get_embedding,
    get_unembedding_with_ln_gain,
)
class DerivedScalarPostprocessor(ABC):
    """
    A parent class for objects that perform postprocessing on specific tensors of derived scalars.
    This postprocessing in general is assumed to require model weights, hence ModelContext. Optionally,
    it might also require autoencoder weights, hence AutoencoderContext.
    The important logic is in the postprocess() function, which takes a ds_index, and a value
    that was produced using ds_store[ds_index] for some presumed ds_store.
    This produces the postprocessed value. Both the value and the metadata in ds_index might be
    required for performing the computation (e.g. the indices might be used to specify what part of a weight
    tensor is required for performing the computation).
    """
    _input_dst_by_node_type: dict[NodeType, DerivedScalarType]
    # TODO: this should really match derived scalar types based on the compatibility of their indexing prefixes, rather
    # than based on their node_types. Possibly this could take the form of: _input_dst_by_dimensions,
    # and check whether a given derived scalar type's dimensions are a prefix of the input derived scalar type's dimensions.
    # this could avoid the need for the _maybe_convert_input_node_type() method.
    def _extract_tensor_for_postprocessing(
        self,
        node_index: NodeIndex | MirroredNodeIndex,
        ds_store: DerivedScalarStore,
    ) -> tuple[DerivedScalarIndex, torch.Tensor, dict[str, Any]]:
        """
        Finds the ds_index (asserted to be unique) in ds_store that is compatible with node_index,
        (using self.convert_node_index_to_ds_index()),
        and returns the ds_index and the corresponding derived_scalars tensor from ds_store, as well as
        any additional kwargs required by self.postprocess_tensor().
        This allows callers to access derived scalars from ds_store without having to check the derived
        scalar types in the store.
        """
        if isinstance(node_index, MirroredNodeIndex):
            node_index = MirroredNodeIndex.to_node_index(node_index)
        assert isinstance(node_index, NodeIndex), f"{node_index=}"
        assert node_index.pass_type in ds_store.pass_types, (
            f"Pass type {node_index.pass_type} not supported by this DerivedScalarStore; "
            f"supported pass types are {ds_store.pass_types}"
        )
        ds_index = self.convert_node_index_to_ds_index(node_index)
        kwargs = self.get_postprocess_tensor_kwargs(node_index, ds_store)
        return ds_index, ds_store[ds_index], kwargs
    @abstractmethod
    def convert_node_index_to_ds_index(self, node_index: NodeIndex) -> DerivedScalarIndex:
        """For a specified node index, return the corresponding ds_index to submit as arguments to postprocess_tensor()"""
        pass
    def get_postprocess_tensor_kwargs(
        self, node_index: NodeIndex, ds_store: DerivedScalarStore
    ) -> dict[str, Any]:
        """
        Returns a dictionary of keyword arguments that should be passed to postprocess_tensor() for the given node index.
        Varies based on child class, and can be empty.
        """
        return {}
    def postprocess(
        self,
        node_index: NodeIndex | MirroredNodeIndex,
        ds_store: DerivedScalarStore,
    ) -> torch.Tensor:
        """
        The primary function of each child class; takes a node index and a DerivedScalarStore assumed to contain
        the DerivedScalarTypeAndPassType compatible with that node_type, and
        returns a postprocessed value. The postprocessing steps in general depend on any fields of the ds_index,
        as well as additional kwargs defined in self.get_postprocess_tensor_kwargs().
        """
        ds_index, derived_scalars, kwargs = self._extract_tensor_for_postprocessing(
            node_index, ds_store
        )
        return self.postprocess_tensor(ds_index, derived_scalars, **kwargs)
    def postprocess_multiple_nodes(
        self,
        node_indices: list[NodeIndex],
        ds_store: DerivedScalarStore,
    ) -> list[torch.Tensor]:
        """
        A default implementation for postprocessing multiple nodes at once, which calls postprocess() for each node.
        This can be overridden for performance reasons if a more efficient implementation is possible for a given
        DerivedScalarPostprocessor.
        """
        return [self.postprocess(node_index, ds_store) for node_index in node_indices]
    @abstractmethod
    def postprocess_tensor(
        self,
        ds_index: DerivedScalarIndex,
        derived_scalars: torch.Tensor,
        **kwargs: Any,
    ) -> torch.Tensor:
        """
        An alternative entry point for postprocessing, which takes a ds_index and a derived_scalars tensor;
        use this if you do not have access to a full DerivedScalarStore.
        """
        ...
    def get_input_dst_and_config_list(
        self,
        requested_dst_and_config_list: list[tuple[DerivedScalarType, DstConfig]],
    ) -> list[tuple[DerivedScalarType, DstConfig]]:
        """
        This matches the nodes reflected in the requested derived scalar types to the nodes supported by the postprocessor,
        and returns a list of derived scalar types and configurations that should be collected into a DerivedScalarStore to
        be passed to postprocess().
        """
        requested_dsts = [dst for dst, _ in requested_dst_and_config_list]
        dst_configs = [dst_config for _, dst_config in requested_dst_and_config_list]
        requested_node_types = [dst.node_type for dst in requested_dsts]
        assert len(requested_node_types) == len(
            set(requested_node_types)
        ), "Requested derived scalar types must have unique node types"
        input_dsts_and_configs = []
        for i, node_type in enumerate(requested_node_types):
            dst = self._input_dst_by_node_type.get(node_type)
            if dst is not None:
                input_dsts_and_configs.append((dst, dst_configs[i]))
        return input_dsts_and_configs + self.get_constitutive_dst_and_config_list()
    def get_constitutive_dst_and_config_list(self) -> list[tuple[DerivedScalarType, DstConfig]]:
        """
        Returns a list of derived scalar types and configurations that should be collected into a DerivedScalarStore to
        be passed to postprocess(), no matter what the requested derived scalar types are. Varies based on the child
        class, and can be empty.
        """
        return []
class ResidualWriteConverter(DerivedScalarPostprocessor):
    """
    Converts activations to a direction in residual stream space, using write tensors. Valid activations
    are MLP_POST_ACT and ATTN_WEIGHTED_VALUE (equal to post-softmax attention * value), and ONLINE_AUTOENCODER_LATENT
    """
    """
    input dsts and node types accepted by this converter match except in the case of attention heads;
    this is because we require more information (i.e. the entire value vector) to compute token space writes
    node_type == NodeType.V_CHANNEL is a piece of metadata saying that the last index of a derived scalar
    corresponds to a single dimension in v-space, or equivalently
    that if you index all but the last index of the derived scalar, you get a vector in the v-space basis
    """
    _input_dst_by_node_type: dict[NodeType, DerivedScalarType] = {
        NodeType.MLP_NEURON: DerivedScalarType.MLP_POST_ACT,
        NodeType.ATTENTION_HEAD: DerivedScalarType.ATTN_WEIGHTED_VALUE,
        NodeType.LAYER: DerivedScalarType.RESID_POST_EMBEDDING,
    }
    def __init__(
        self,
        model_context: ModelContext,
        multi_autoencoder_context: MultiAutoencoderContext | AutoencoderContext | None,
    ):
        self._model_context = model_context
        self._multi_autoencoder_context = MultiAutoencoderContext.from_context_or_multi_context(
            multi_autoencoder_context
        )
        if self._multi_autoencoder_context is not None:
            if (
                NodeType.MLP_AUTOENCODER_LATENT
                in self._multi_autoencoder_context.autoencoder_context_by_node_type
            ):
                self._input_dst_by_node_type[
                    NodeType.MLP_AUTOENCODER_LATENT
                ] = DerivedScalarType.ONLINE_MLP_AUTOENCODER_LATENT
            if (
                NodeType.ATTENTION_AUTOENCODER_LATENT
                in self._multi_autoencoder_context.autoencoder_context_by_node_type
            ):
                self._input_dst_by_node_type[
                    NodeType.ATTENTION_AUTOENCODER_LATENT
                ] = DerivedScalarType.ONLINE_ATTENTION_AUTOENCODER_LATENT
            if self._multi_autoencoder_context.has_single_autoencoder_context:
                self._input_dst_by_node_type[
                    NodeType.AUTOENCODER_LATENT
                ] = DerivedScalarType.ONLINE_AUTOENCODER_LATENT
    def convert_node_index_to_ds_index(self, node_index: NodeIndex) -> DerivedScalarIndex:
        dst_for_write = self._input_dst_by_node_type[node_index.node_type]
        supported_dsts = list(self._input_dst_by_node_type.values())
        assert dst_for_write in supported_dsts, (
            f"Node type {node_index.node_type} not supported by this DerivedScalarStore; "
            f"supported node types are {supported_dsts}"
        )
        if node_index.node_type == NodeType.LAYER:
            # remove the final, singleton dimension, which is not in the converted derived scalar type
            assert len(node_index.tensor_indices) == 2
            assert node_index.tensor_indices[1] == 0
            updated_tensor_indices: tuple[int | None, ...] = node_index.tensor_indices[:-1]
        else:
            updated_tensor_indices = node_index.tensor_indices
        ds_index = DerivedScalarIndex.from_node_index(
            node_index.with_updates(
                node_type=dst_for_write.node_type, tensor_indices=updated_tensor_indices
            ),
            dst_for_write,
        )
        return ds_index
    def _maybe_decode(
        self, ds_index: DerivedScalarIndex, derived_scalars: torch.Tensor
    ) -> torch.Tensor:
        """decodes if ds_index.dst is an autoencoder latent type, and if so, returns the
        derived_scalars decoded by the decoder weight for the corresponding autoencoder. Otherwise, returns the
        derived_scalars unchanged."""
        if ds_index.dst.is_autoencoder_latent:
            assert self._multi_autoencoder_context is not None
            assert derived_scalars.ndim == 0
            autoencoder = self._multi_autoencoder_context.get_autoencoder(
                ds_index.layer_index, node_type=ds_index.dst.node_type
            )
            assert Dimension.AUTOENCODER_LATENTS in ds_index.tensor_index_by_dim
            indices_for_decoder = (
                ds_index.tensor_index_by_dim[Dimension.AUTOENCODER_LATENTS],
                None,
            )
            slices_for_decoder: tuple[slice | int | None, ...] = tuple(
                slice(None) if index is None else index for index in indices_for_decoder
            )
            decoder_weight = get_decoder_weight(autoencoder)[slices_for_decoder]
            assert decoder_weight.ndim == 1
            derived_scalars = derived_scalars * decoder_weight
        return derived_scalars
    def _get_output_weight(self, ds_index: DerivedScalarIndex) -> torch.Tensor:
        if ds_index.dst.is_autoencoder_latent:
            assert self._multi_autoencoder_context is not None
            autoencoder_context = self._multi_autoencoder_context.get_autoencoder_context(
                ds_index.dst.node_type
            )
            assert autoencoder_context is not None
            output_dst = autoencoder_context.dst
        elif ds_index.dst.node_type == NodeType.ATTENTION_HEAD:
            output_dst = DerivedScalarType.ATTN_WEIGHTED_VALUE
        else:
            output_dst = ds_index.dst
        _output_weight_by_dst: dict[DerivedScalarType, WeightLocationType] = {
            DerivedScalarType.MLP_POST_ACT: WeightLocationType.MLP_TO_RESIDUAL,
            DerivedScalarType.ATTN_WEIGHTED_VALUE: WeightLocationType.ATTN_TO_RESIDUAL,
        }
        assert output_dst in _output_weight_by_dst, f"{output_dst} must be in output weight dict"
        output_weight_location_type = _output_weight_by_dst[output_dst]
        weight_shape_spec = output_weight_location_type.shape_spec
        weight_tensor_indices = tuple(
            [ds_index.tensor_index_by_dim.get(dim, None) for dim in weight_shape_spec]
        )
        weight_tensor_slices: tuple[slice | int | None, ...] = tuple(
            [slice(None) if index is None else index for index in weight_tensor_indices]
        )
        return self._model_context.get_weight(output_weight_location_type, ds_index.layer_index)[
            weight_tensor_slices
        ]
    def postprocess_tensor(
        self, ds_index: DerivedScalarIndex, derived_scalars: torch.Tensor, **kwargs: Any
    ) -> torch.Tensor:
        # TODO: rationalize the setup for choosing the raw activations device by getting it from DerivedScalarTypeConfig,
        # rather than permitting it as an argument to ScalarDeriver __init__.
        # TODO: Derived scalar tensors sometimes haven't been detached yet! We work around that
        # by detaching them here, but we should really just make sure they're always detached.
        assert len(kwargs) == 0, f"Unexpected kwargs: {kwargs}"
        derived_scalars = derived_scalars.to(self._model_context.device).detach()
        # input can be either a scalar or a vector. In the case of e.g. attention heads,
        # a vector worth of information is required to reconstruct the write to the residual stream
        assert derived_scalars.ndim in {0, 1}
        # 1. if an autoencoder latent, return the equivalent model activations; otherwise
        # return the derived scalar unchanged
        derived_scalars = self._maybe_decode(ds_index, derived_scalars)
        # 2. find the output dst
        if ds_index.dst.is_autoencoder_latent:
            assert self._multi_autoencoder_context is not None
            autoencoder_context = self._multi_autoencoder_context.get_autoencoder_context(
                ds_index.dst.node_type
            )
            assert autoencoder_context is not None
            output_dst = autoencoder_context.dst
        else:
            output_dst = ds_index.dst
        # 3. convert from model activations to the residual stream write, unless it is already a residual stream write
        if output_dst.node_type == NodeType.RESIDUAL_STREAM_CHANNEL:
            assert derived_scalars.ndim == 1, f"{ds_index=}, {derived_scalars.shape=}"
            return derived_scalars
        else:
            output_weight = self._get_output_weight(ds_index)
            if derived_scalars.ndim == 0:
                assert (
                    output_dst.node_type == NodeType.MLP_NEURON
                ), f"1-d activation expected only for MLP neurons, not {output_dst.node_type}"
                derived_scalars = derived_scalars.unsqueeze(0)
                assert output_weight.ndim == 1, f"{output_weight.shape=}"
                output_weight = output_weight.unsqueeze(0)
            else:
                assert derived_scalars.ndim == 1
                assert output_weight.ndim == 2
            return torch.einsum("a,ad->d", derived_scalars, output_weight)
class TokenWriteConverter(DerivedScalarPostprocessor):
    """
    Converts activations to a direction in token space, using the unembedding matrix. Valid activations
    are MLP_POST_ACT and ATTN_WEIGHTED_VALUE (equal to post-softmax attention * value), and ONLINE_AUTOENCODER_LATENT
    """
    def __init__(
        self,
        model_context: ModelContext,
        multi_autoencoder_context: MultiAutoencoderContext | AutoencoderContext | None = None,
    ):
        self._model_context = model_context
        self._multi_autoencoder_context = MultiAutoencoderContext.from_context_or_multi_context(
            multi_autoencoder_context
        )
        self._residual_write_converter = ResidualWriteConverter(
            model_context, multi_autoencoder_context
        )
        self._input_dst_by_node_type = self._residual_write_converter._input_dst_by_node_type
        self._unemb_with_ln_gain = get_unembedding_with_ln_gain(self._model_context)
    def convert_node_index_to_ds_index(self, node_index: NodeIndex) -> DerivedScalarIndex:
        return self._residual_write_converter.convert_node_index_to_ds_index(node_index)
    def postprocess_tensor(
        self, ds_index: DerivedScalarIndex, derived_scalars: torch.Tensor, **kwargs: Any
    ) -> torch.Tensor:
        residual_write = self._residual_write_converter.postprocess_tensor(
            ds_index, derived_scalars, **kwargs
        )
        # 3. convert from the residual stream write to the token-space write
        unembedded_output = torch.einsum("d,dv->v", residual_write, self._unemb_with_ln_gain)
        # 4. subtract the mean, since logprobs are invariant to adding a constant to all logits
        mean_subtracted_unembedded_output = unembedded_output - unembedded_output.mean()
        return mean_subtracted_unembedded_output
    def postprocess_multiple_nodes(
        self,
        node_indices: list[NodeIndex],
        ds_store: DerivedScalarStore,
    ) -> list[torch.Tensor]:
        """
        First, postprocesses all the nodes with the residual write converter, then concatenates them
        and applies the unembedding matrix to the concatenated tensor.
        """
        residual_writes = self._residual_write_converter.postprocess_multiple_nodes(
            node_indices, ds_store
        )
        concatenated_residual_writes = torch.stack(residual_writes, dim=0)
        unembedded_output = torch.einsum(
            "nd,dv->nv", concatenated_residual_writes, self._unemb_with_ln_gain
        )
        mean_subtracted_unembedded_output = unembedded_output - unembedded_output.mean(
            dim=1, keepdim=True
        )
        split_unembedded_output = torch.split(mean_subtracted_unembedded_output, 1, dim=0)
        list_of_tensors = [tensor.squeeze(0) for tensor in split_unembedded_output]
        return list_of_tensors
def _get_residual_stream_tensor_indices_for_node(node_index: NodeIndex) -> tuple[int]:
    """For a given node index defining a point from which the gradient will be computed, this identifies the token
    indices at which the gradient immediately before the node will be nonzero. For attention, in order for there to
    be exactly one such token index, the gradient is computed through one of query/key/value, with a stopgrad
    through the others. Depending on which of query/key/value is used, the token index will be either the query token
    index or the key/value token index. For MLP neurons, the token index will be the token index of the neuron.
    """
    # tensor_indices are expected to be tuple[int, ...], even if length 1
    match node_index.node_type:
        case NodeType.ATTENTION_HEAD:
            # in the case of attention head reads, there are several possible ways to interpret the "read" direction
            # - the gradient through just the query (at the query token)
            # - the gradient through just the key (at the key/value token)
            # - the gradient with respect to some function of the attention write, e.g. the attention write norm,
            # through just the value (at the key/value token)
            assert isinstance(node_index, AttnSubNodeIndex)
            assert len(node_index.tensor_indices) == 3
            match node_index.q_k_or_v:
                case ActivationLocationType.ATTN_QUERY:
                    tensor_index = node_index.tensor_indices[1]  # just the query token index
                case ActivationLocationType.ATTN_KEY | ActivationLocationType.ATTN_VALUE:
                    tensor_index = node_index.tensor_indices[0]  # just the key/value token index
                case _:
                    raise ValueError(f"Unexpected q_k_or_v: {node_index.q_k_or_v}")
        case (
            NodeType.MLP_NEURON
            | NodeType.AUTOENCODER_LATENT
            | NodeType.MLP_AUTOENCODER_LATENT
            | NodeType.ATTENTION_AUTOENCODER_LATENT
        ):
            assert len(node_index.tensor_indices) == 2
            tensor_index = node_index.tensor_indices[0]  # just the token index
        case _:
            raise ValueError(f"Node type {node_index.node_type} not supported")
    assert isinstance(tensor_index, int), (tensor_index, type(tensor_index))
    return (tensor_index,)
class ResidualReadConverter(DerivedScalarPostprocessor):
    """
    Converts activations to a gradient direction in residual stream space, by taking functions that recompute those
    activations from the residual stream, and compute a backward pass on them. Valid activations
    are PREVIOUS_LAYER_RESID_POST_MLP and RESID_POST_ATTN (the DSTs corresponding to residual stream locations that
    precede attention heads and MLP neurons, respectively)
    """
    def __init__(
        self,
        model_context: ModelContext,
        multi_autoencoder_context: MultiAutoencoderContext | AutoencoderContext | None = None,
    ):
        assert isinstance(model_context, StandardModelContext)
        self._transformer = model_context.get_or_create_model()
        self._device = model_context.device
        self._multi_autoencoder_context = MultiAutoencoderContext.from_context_or_multi_context(
            multi_autoencoder_context
        )
        # TODO: support attention heads; this will require specifying q, k or v in the make_reconstituted_gradient_fn
        self._input_dst_by_node_type: dict[NodeType, DerivedScalarType] = {}
        self._input_dst_by_node_type[NodeType.MLP_NEURON] = get_previous_residual_dst_for_node_type(
            NodeType.MLP_NEURON, None
        )
        self._input_dst_by_node_type[
            NodeType.ATTENTION_HEAD
        ] = get_previous_residual_dst_for_node_type(NodeType.ATTENTION_HEAD, None)
        if self._multi_autoencoder_context is not None:
            # add the autoencoders listed in the multi_autoencoder_context, using their node types
            for (
                node_type,
                autoencoder_context,
            ) in self._multi_autoencoder_context.autoencoder_context_by_node_type.items():
                self._input_dst_by_node_type[node_type] = get_previous_residual_dst_for_node_type(
                    node_type, autoencoder_context.dst
                )
            # if there is only one autoencoder context, also add the "default" node type for backwards compatibility
            if self._multi_autoencoder_context.has_single_autoencoder_context:
                autoencoder_context = (
                    self._multi_autoencoder_context.get_single_autoencoder_context()
                )
                self._input_dst_by_node_type[
                    NodeType.AUTOENCODER_LATENT
                ] = get_previous_residual_dst_for_node_type(
                    NodeType.AUTOENCODER_LATENT, autoencoder_context.dst
                )
    def convert_node_index_to_ds_index(self, node_index: NodeIndex) -> DerivedScalarIndex:
        if node_index.node_type == NodeType.ATTENTION_HEAD:
            # see _get_residual_stream_tensor_indices_for_node for more information
            # TODO: finish supporting attention heads
            assert isinstance(node_index, AttnSubNodeIndex), (
                node_index.node_type,
                type(node_index),
            )
            assert node_index.q_k_or_v in {
                ActivationLocationType.ATTN_QUERY,
                ActivationLocationType.ATTN_KEY,
            }
        dst_for_computing_grad = self._input_dst_by_node_type[node_index.node_type]
        supported_dsts = list(self._input_dst_by_node_type.values())
        assert dst_for_computing_grad in supported_dsts, (
            f"Node type {node_index.node_type} not supported by this DerivedScalarStore; "
            f"supported node types are {supported_dsts}"
        )
        updated_tensor_indices = _get_residual_stream_tensor_indices_for_node(node_index)
        # note: derived scalar indices do not have q_k_or_v associated to them, so we remove this field
        updated_node_index = NodeIndex(
            node_type=dst_for_computing_grad.node_type,
            # Remove the activation index; the entire residual stream will be needed for computing
            # the gradient.
            tensor_indices=updated_tensor_indices,
            layer_index=node_index.layer_index,
            pass_type=node_index.pass_type,
        )
        return DerivedScalarIndex.from_node_index(
            updated_node_index,
            dst_for_computing_grad,
        )
    def get_postprocess_tensor_kwargs(
        self, node_index: NodeIndex, _unused_ds_store: DerivedScalarStore
    ) -> dict[str, Any]:
        return {"node_index": node_index}
    def postprocess_tensor(
        self, ds_index: DerivedScalarIndex, derived_scalars: torch.Tensor, **kwargs: Any
    ) -> torch.Tensor:
        # TODO: rationalize the setup for choosing the raw activations device by getting it from DerivedScalarTypeConfig,
        # rather than permitting it as an argument to ScalarDeriver __init__.
        # TODO: Derived scalar tensors sometimes haven't been detached yet! We work around that
        # by detaching them here, but we should really just make sure they're always detached.
        node_index = kwargs.pop("node_index")
        assert len(kwargs) == 0, f"Unexpected kwargs: {kwargs}"
        assert (
            ds_index.pass_type == PassType.FORWARD
        ), "Residual read converter only supports forward pass"
        derived_scalars = derived_scalars.to(self._device).detach()
        # input should be a residual stream write (1-d)
        assert derived_scalars.ndim == 1
        node_index_with_singleton_first_dim = node_index.with_updates(
            tensor_indices=(0,) + node_index.tensor_indices[1:]
        )
        trace_config = TraceConfig(
            node_index=node_index_with_singleton_first_dim,
            pre_or_post_act=PreOrPostAct.PRE,
            detach_layer_norm_scale=DETACH_LAYER_NORM_SCALE,
        )
        # 1. create the function that computes the residual stream gradient
        if trace_config.node_type.is_autoencoder_latent:
            assert self._multi_autoencoder_context is not None
            autoencoder_context = self._multi_autoencoder_context.get_autoencoder_context(
                trace_config.node_type
            )
            assert autoencoder_context is not None
        else:
            autoencoder_context = None
        reconstitute_gradient = make_reconstituted_gradient_fn(
            transformer=self._transformer,
            autoencoder_context=autoencoder_context,
            trace_config=trace_config,
        )
        # 2. apply the function to the residual stream vector to get the residual stream gradient ("read" vector)
        residual_read = reconstitute_gradient(
            derived_scalars[None], ds_index.layer_index, PassType.FORWARD
        )[
            0
        ]  # add and then remove token dimension for compat with reconstitute_gradient
        return residual_read
class TokenReadConverter(DerivedScalarPostprocessor):
    """
    Converts activations to a direction in token space, by computing a gradient as in ResidualReadConverter,
    and projecting it into token space using the embedding matrix. Valid activations are
    PREVIOUS_LAYER_RESID_POST_MLP and RESID_POST_ATTN (the DSTs corresponding to residual stream locations that
    precede attention heads and MLP neurons, respectively)
    """
    def __init__(
        self,
        model_context: ModelContext,
        multi_autoencoder_context: MultiAutoencoderContext | AutoencoderContext | None = None,
    ):
        self._model_context = model_context
        self._multi_autoencoder_context = MultiAutoencoderContext.from_context_or_multi_context(
            multi_autoencoder_context
        )
        self._residual_read_converter = ResidualReadConverter(
            model_context, multi_autoencoder_context
        )
        self._input_dst_by_node_type = self._residual_read_converter._input_dst_by_node_type
        self._emb = get_embedding(self._model_context)
    def convert_node_index_to_ds_index(self, node_index: NodeIndex) -> DerivedScalarIndex:
        return self._residual_read_converter.convert_node_index_to_ds_index(node_index)
    def get_postprocess_tensor_kwargs(
        self, node_index: NodeIndex, _unused_ds_store: DerivedScalarStore
    ) -> dict[str, Any]:
        return self._residual_read_converter.get_postprocess_tensor_kwargs(
            node_index, _unused_ds_store
        )
    def postprocess_tensor(
        self, ds_index: DerivedScalarIndex, derived_scalars: torch.Tensor, **kwargs: Any
    ) -> torch.Tensor:
        residual_read = self._residual_read_converter.postprocess_tensor(
            ds_index, derived_scalars, **kwargs
        )
        # 3. convert from the residual stream read to the token-space read
        return torch.einsum("d,vd->v", residual_read, self._emb)
    def postprocess_multiple_nodes(
        self,
        node_indices: list[NodeIndex],
        ds_store: DerivedScalarStore,
    ) -> list[torch.Tensor]:
        """
        First, postprocesses all the nodes with the residual read converter, then concatenates them
        and applies the embedding matrix to the concatenated tensor.
        """
        residual_reads = self._residual_read_converter.postprocess_multiple_nodes(
            node_indices, ds_store
        )
        concatenated_residual_reads = torch.stack(residual_reads, dim=0)
        embedded_output = torch.einsum("nd,vd->nv", concatenated_residual_reads, self._emb)
        split_unembedded_output = torch.split(embedded_output, 1, dim=0)
        list_of_tensors = [tensor.squeeze(0) for tensor in split_unembedded_output]
        return list_of_tensors
class TokenPairAttributionConverter(DerivedScalarPostprocessor):
    """
    Converts activations of an attention-write autoencoder, to compute attribution to each token pair.
    """
    _input_dst_by_node_type: dict[NodeType, DerivedScalarType] = {
        NodeType.ATTENTION_AUTOENCODER_LATENT: DerivedScalarType.ATTENTION_AUTOENCODER_LATENT,
    }
    def __init__(
        self,
        model_context: ModelContext,
        multi_autoencoder_context: MultiAutoencoderContext | AutoencoderContext | None,
        num_tokens_attended_to: int,
    ):
        self._model_context = model_context
        self._multi_autoencoder_context = MultiAutoencoderContext.from_context_or_multi_context(
            multi_autoencoder_context
        )
        self.num_tokens_attended_to = num_tokens_attended_to
    def postprocess(
        self,
        node_index: NodeIndex | MirroredNodeIndex,
        ds_store: DerivedScalarStore,
    ) -> torch.Tensor:
        if node_index.node_type not in self._input_dst_by_node_type:
            raise ValueError(f"Node type {node_index.node_type} not supported")
        elif self._multi_autoencoder_context is not None:
            autoencoder_context = self._multi_autoencoder_context.get_autoencoder_context(
                node_index.node_type
            )
            if autoencoder_context is None:
                raise ValueError(
                    f"No autoencoder context found for node type {node_index.node_type}."
                )
            if autoencoder_context.dst != DerivedScalarType.RESID_DELTA_ATTN:
                raise ValueError(
                    "Autoencoder context found, but derived scalar type is not RESID_DELTA_ATTN."
                )
        # otherwise proceed
        ds_index, derived_scalars, kwargs = self._extract_tensor_for_postprocessing(
            node_index, ds_store
        )
        return self.postprocess_tensor(ds_index, derived_scalars, **kwargs)
    def convert_node_index_to_ds_index(self, node_index: NodeIndex) -> DerivedScalarIndex:
        dst = self._input_dst_by_node_type[node_index.node_type]
        ds_index = DerivedScalarIndex.from_node_index(
            node_index.with_updates(
                node_type=dst.node_type, tensor_indices=node_index.tensor_indices
            ),
            dst,
        )
        return ds_index
    def postprocess_tensor(
        self, ds_index: DerivedScalarIndex, derived_scalars: torch.Tensor, **kwargs: Any
    ) -> torch.Tensor:
        from neuron_explainer.activations.derived_scalars.autoencoder import (
            make_autoencoder_activation_fn_derivative,
            make_autoencoder_pre_act_encoder_derivative,
        )
        attn_write_sum_heads = kwargs.pop("attn_write_sum_heads")
        assert len(kwargs) == 0, f"Unexpected kwargs: {kwargs}"
        layer_index = ds_index.layer_index
        token_index, latent_index = ds_index.tensor_indices
        assert self._multi_autoencoder_context is not None
        autoencoder_context = self._multi_autoencoder_context.get_autoencoder_context(
            NodeType.ATTENTION_AUTOENCODER_LATENT
        )
        assert autoencoder_context is not None
        assert layer_index is not None
        # compute the activation function derivative
        activation_fn_derivative = make_autoencoder_activation_fn_derivative(
            autoencoder_context, layer_index
        )
        latent_activation = derived_scalars  # (,)
        d_latent_d_pre_act = activation_fn_derivative(latent_activation)  # (,)
        if d_latent_d_pre_act == 0:
            raise ValueError("Latent is inactive.")
        # compute the encoder derivative
        pre_act_encoder_derivative = make_autoencoder_pre_act_encoder_derivative(
            autoencoder_context, layer_index, latent_index=latent_index
        )
        n_tokens_attended_to, d_model = attn_write_sum_heads.shape  # already indexed by token_index
        projection = pre_act_encoder_derivative(attn_write_sum_heads)  # (n_tokens_attended_to, 1)
        projection = projection[:, 0]  # (n_tokens_attended_to,)
        direct_write_to_latents = projection * d_latent_d_pre_act  # (n_tokens_attended_to, )
        # make sure the result has one dimension, because we use zero-dimension when the postprocessor
        # is not supported (return torch.tensor(torch.nan))
        assert direct_write_to_latents.ndim >= 1
        return direct_write_to_latents
    def get_constitutive_dst_and_config_list(self) -> list[tuple[DerivedScalarType, DstConfig]]:
        return [
            (
                DerivedScalarType.ATTN_WRITE_SUM_HEADS,
                DstConfig(
                    model_context=self._model_context,
                ),
            )
        ]
    def get_postprocess_tensor_kwargs(
        self, node_index: NodeIndex, ds_store: DerivedScalarStore
    ) -> dict[str, Any]:
        sequence_token_index = node_index.tensor_indices[0]
        layer_index = node_index.layer_index
        attn_write_sum_heads = ds_store[
            DerivedScalarIndex(
                dst=DerivedScalarType.ATTN_WRITE_SUM_HEADS,
                layer_index=layer_index,
                pass_type=PassType.FORWARD,
                tensor_indices=(sequence_token_index, None),
            )
        ]
        return {"attn_write_sum_heads": attn_write_sum_heads}

================
File: neuron_explainer/activations/derived_scalars/raw_activations.py
================
"""
This file contains code to make scalar derivers for scalar types that are 1:1 with an
ActivationLocationType.
"""
from functools import partial
from typing import Callable
import torch
from neuron_explainer.activations.derived_scalars.derived_scalar_types import DerivedScalarType
from neuron_explainer.activations.derived_scalars.locations import (
    IdentityLayerIndexer,
    LayerIndexer,
    NoLayersLayerIndexer,
)
from neuron_explainer.activations.derived_scalars.scalar_deriver import (
    DstConfig,
    PassType,
    RawScalarSource,
    ScalarDeriver,
)
from neuron_explainer.models.model_component_registry import (
    ActivationLocationType,
    LayerIndex,
    NodeType,
)
def get_scalar_sources_for_activation_location_types(
    activation_location_type: ActivationLocationType,
    derive_gradients: bool,
) -> tuple[RawScalarSource, ...]:
    if activation_location_type.has_no_layers:
        layer_indexer: LayerIndexer = NoLayersLayerIndexer()
    else:
        layer_indexer = IdentityLayerIndexer()
    if derive_gradients:
        return (
            RawScalarSource(
                activation_location_type=activation_location_type,
                pass_type=PassType.FORWARD,
                layer_indexer=layer_indexer,
            ),
            RawScalarSource(
                activation_location_type=activation_location_type,
                pass_type=PassType.BACKWARD,
                layer_indexer=layer_indexer,
            ),
        )
    else:
        return (
            RawScalarSource(
                activation_location_type=activation_location_type,
                pass_type=PassType.FORWARD,
                layer_indexer=layer_indexer,
            ),
        )
def no_op_tensor_calculate_derived_scalar_fn(
    raw_activation_data_tuple: tuple[torch.Tensor, ...],
    layer_index: LayerIndex,
    pass_type: PassType,
) -> torch.Tensor:
    """
    This either:
    converts a length 1 tuple of tensors into
    a single tensor; pass_type is asserted to be PassType.FORWARD
    or
    converts a length 2 tuple of tensors, one for the forward pass and one for the backward pass,
    into the appropriate one of those two objects, depending on the pass_type argument.
    """
    if len(raw_activation_data_tuple) == 1:
        # in this case, only the activations at the relevant ActivationLocationType have been loaded from disk
        assert pass_type == PassType.FORWARD
        raw_activation_data = raw_activation_data_tuple[0]
        return raw_activation_data
    elif len(raw_activation_data_tuple) == 2:
        # in this case, both the activations and gradients at the relevant ActivationLocationType have been loaded from disk
        raw_activation_data, raw_gradient_data = raw_activation_data_tuple
        if pass_type == PassType.FORWARD:
            return raw_activation_data
        elif pass_type == PassType.BACKWARD:
            return raw_gradient_data
        else:
            raise ValueError(f"Unknown {pass_type=}")
    else:
        raise ValueError(f"Unknown {raw_activation_data_tuple=}")
def make_scalar_deriver_factory_for_activation_location_type(
    activation_location_type: ActivationLocationType,
) -> Callable[[DstConfig], ScalarDeriver]:
    """
    This is for DerivedScalarType's 1:1 with a ActivationLocationType, which can be generated from
    just the ActivationLocationType and no additional information.
    """
    def make_scalar_deriver_fn(
        dst_config: DstConfig,
    ) -> ScalarDeriver:
        sub_scalar_sources = get_scalar_sources_for_activation_location_types(
            activation_location_type, dst_config.derive_gradients
        )
        return ScalarDeriver(
            dst=DerivedScalarType.from_activation_location_type(activation_location_type),
            dst_config=dst_config,
            sub_scalar_sources=sub_scalar_sources,
            tensor_calculate_derived_scalar_fn=no_op_tensor_calculate_derived_scalar_fn,
        )
    return make_scalar_deriver_fn
def make_scalar_deriver_factory_for_act_times_grad(
    activation_location_type: ActivationLocationType,
    dst: DerivedScalarType,
) -> Callable[[DstConfig], ScalarDeriver]:
    def make_act_times_grad_scalar_deriver(
        dst_config: DstConfig,
    ) -> ScalarDeriver:
        assert not dst_config.derive_gradients, "Gradients not defined for act times grad"
        if activation_location_type.has_no_layers:
            layer_indexer: LayerIndexer = NoLayersLayerIndexer()
        else:
            layer_indexer = IdentityLayerIndexer()
        sub_scalar_sources = (
            RawScalarSource(
                activation_location_type=activation_location_type,
                pass_type=PassType.FORWARD,
                layer_indexer=layer_indexer,
            ),  # activations
            RawScalarSource(
                activation_location_type=activation_location_type,
                pass_type=PassType.BACKWARD,
                layer_indexer=layer_indexer,
            ),  # gradients
        )
        def _act_times_grad_tensor_calculate_derived_scalar_fn(
            raw_activation_data_tuple: tuple[torch.Tensor, ...],
            layer_index: LayerIndex,
            pass_type: PassType,
        ) -> torch.Tensor:
            assert pass_type == PassType.FORWARD, "Backward pass not defined for act times grad"
            assert len(raw_activation_data_tuple) == 2
            raw_activation_data, raw_gradient_data = raw_activation_data_tuple
            return raw_activation_data * raw_gradient_data
        return ScalarDeriver(
            dst=dst,
            dst_config=dst_config,
            sub_scalar_sources=sub_scalar_sources,
            tensor_calculate_derived_scalar_fn=_act_times_grad_tensor_calculate_derived_scalar_fn,
        )
    return make_act_times_grad_scalar_deriver
def check_write_tensor_device_matches(
    scalar_deriver: ScalarDeriver,
    write_tensor_by_layer_index: dict[LayerIndex, torch.Tensor] | dict[int, torch.Tensor],
) -> None:
    write_matrix_device = next(iter(write_tensor_by_layer_index.values())).device
    assert scalar_deriver.device_for_raw_activations == write_matrix_device, (
        scalar_deriver.dst,
        scalar_deriver.device_for_raw_activations,
        write_matrix_device,
    )
def convert_scalar_deriver_to_write_norm(
    scalar_deriver: ScalarDeriver,
    write_tensor_by_layer_index: dict[LayerIndex, torch.Tensor] | dict[int, torch.Tensor],
    output_dst: DerivedScalarType,
) -> ScalarDeriver:
    """
    Converts a scalar deriver for a scalar type that is 1:1 with an ActivationLocationType to a
    scalar deriver for the write norm for each neuron at each token.
    """
    check_write_tensor_device_matches(
        scalar_deriver,
        write_tensor_by_layer_index,
    )
    write_norm_by_layer_index = {
        layer_index_: write_tensor_by_layer_index[layer_index_].norm(dim=-1)  # type: ignore
        for layer_index_ in write_tensor_by_layer_index.keys()
    }
    def multiply_by_write_norm(
        activations: torch.Tensor,
        layer_index: LayerIndex,
        pass_type: PassType,
    ) -> torch.Tensor:
        assert pass_type == PassType.FORWARD, "Backward pass not defined for write norm"
        assert (
            layer_index in write_tensor_by_layer_index
        ), f"{layer_index=} not in {write_tensor_by_layer_index.keys()=} for {output_dst=}"
        return activations * write_norm_by_layer_index[layer_index]
    return scalar_deriver.apply_layerwise_transform_fn_to_output(
        multiply_by_write_norm,
        pass_type_to_transform=PassType.FORWARD,
        output_dst=output_dst,
    )
def convert_scalar_deriver_to_write(
    scalar_deriver: ScalarDeriver,
    write_tensor_by_layer_index: dict[LayerIndex, torch.Tensor] | dict[int, torch.Tensor],
    output_dst: DerivedScalarType,
) -> ScalarDeriver:
    """Converts a scalar deriver for a scalar type that is 1:1 with an ActivationLocationType
    to a scalar deriver for the write vector of the layer at each token."""
    check_write_tensor_device_matches(
        scalar_deriver,
        write_tensor_by_layer_index,
    )
    def multiply_by_write(
        activations: torch.Tensor,
        layer_index: LayerIndex,
        pass_type: PassType,
    ) -> torch.Tensor:
        assert pass_type == PassType.FORWARD, "Backward pass not defined for write"
        assert (
            layer_index in write_tensor_by_layer_index
        ), f"{layer_index=} not in {write_tensor_by_layer_index.keys()=}"
        return torch.einsum(
            "ta,ao->to",
            activations,
            write_tensor_by_layer_index[layer_index],  # type: ignore
        )
    return scalar_deriver.apply_layerwise_transform_fn_to_output(
        multiply_by_write,
        pass_type_to_transform=PassType.FORWARD,
        output_dst=output_dst,
    )
def convert_scalar_deriver_to_write_vector(
    scalar_deriver: ScalarDeriver,
    write_tensor_by_layer_index: dict[LayerIndex, torch.Tensor] | dict[int, torch.Tensor],
    output_dst: DerivedScalarType,
) -> ScalarDeriver:
    """
    Converts a scalar deriver for a scalar type that is 1:1 with an ActivationLocationType to a
    scalar deriver for the write vector of the layer at each token. Must be a scalar type that is
    related to the residual stream basis by a straightforward matmul (e.g. MLP post-activations are
    related to the residual stream basis by WeightLocationType.MLP_TO_RESIDUAL).
    """
    check_write_tensor_device_matches(
        scalar_deriver,
        write_tensor_by_layer_index,
    )
    assert scalar_deriver.dst.node_type in {
        NodeType.MLP_NEURON,
        NodeType.V_CHANNEL,
        NodeType.AUTOENCODER_LATENT,
        NodeType.MLP_AUTOENCODER_LATENT,
        NodeType.ATTENTION_AUTOENCODER_LATENT,
    }
    def multiply_by_write_vector(
        activations: torch.Tensor,
        layer_index: LayerIndex,
        pass_type: PassType,
    ) -> torch.Tensor:
        assert pass_type == PassType.FORWARD, "Backward pass not defined for write"
        assert (
            layer_index in write_tensor_by_layer_index
        ), f"{layer_index=} not in {write_tensor_by_layer_index.keys()=}"
        return torch.einsum(
            "ta,ao->tao",
            activations,
            write_tensor_by_layer_index[layer_index],  # type: ignore
        )
    return scalar_deriver.apply_layerwise_transform_fn_to_output(
        multiply_by_write_vector,
        pass_type_to_transform=PassType.FORWARD,
        output_dst=output_dst,
    )
def truncate_to_expected_shape(
    tensor: torch.Tensor,
    expected_shape: list[int | None],
) -> torch.Tensor:
    """
    This asserts that the tensor has the expected shape, and optionally truncates it to that shape.
    None in expected_shape means that dimension is not checked.
    """
    if expected_shape is None:
        return tensor
    for dim, real_size, expected_size in zip(
        range(len(expected_shape)), tensor.shape, expected_shape
    ):
        if expected_size is not None:
            assert (
                real_size >= expected_size
            ), f"Dimension {dim} of tensor has size {real_size} but expected size {expected_size}"
            tensor = tensor.narrow(dim, 0, expected_size)
    return tensor
def truncate_to_expected_shape_tensor_calculate_derived_scalar_fn(
    raw_activation_data_tuple: tuple[torch.Tensor, ...],
    layer_index: LayerIndex,
    pass_type: PassType,
    expected_shape: list[int | None],
) -> torch.Tensor:
    """This either:
    converts a length 1 tuple of tensors into
    a single tensor; pass_type is asserted to be PassType.FORWARD
    or
    converts a length 2 tuple of tensors, one for the forward pass and one for the backward pass,
    into the appropriate one of those two objects, depending on the pass_type argument."""
    if len(raw_activation_data_tuple) == 1:
        # in this case, only the activations at the relevant ActivationLocationType have been loaded from disk
        assert pass_type == PassType.FORWARD
        raw_activation_data = raw_activation_data_tuple[0]
        raw_activation_data = truncate_to_expected_shape(
            raw_activation_data,
            expected_shape,
        )
        return raw_activation_data
    elif len(raw_activation_data_tuple) == 2:
        # in this case, both the activations and gradients at the relevant ActivationLocationType have been loaded from disk
        raw_activation_data, raw_gradient_data = raw_activation_data_tuple
        if pass_type == PassType.FORWARD:
            raw_activation_data = truncate_to_expected_shape(
                raw_activation_data,
                expected_shape,
            )
            return raw_activation_data
        elif pass_type == PassType.BACKWARD:
            raw_gradient_data = truncate_to_expected_shape(
                raw_gradient_data,
                expected_shape,
            )
            return raw_gradient_data
        else:
            raise ValueError(f"Unknown {pass_type=}")
    else:
        raise ValueError(f"Unknown {raw_activation_data_tuple=}")
# TODO: this entire function should be simplified or deleted?
# Can possibly just use make_scalar_deriver_factory_for_activation_location_type(ActivationLocationType.LOGITS)
# in the one place it is called
def make_truncate_to_expected_shape_scalar_deriver_factory_for_dst(
    dst: DerivedScalarType,
) -> Callable[[DstConfig], ScalarDeriver]:
    """
    This is for DerivedScalarType's 1:1 with a ActivationLocationType, which can be generated from
    just the ActivationLocationType and no additional information.
    """
    untruncated_activation_location_type_by_truncated_dst = {
        DerivedScalarType.LOGITS: ActivationLocationType.LOGITS,
    }
    assert (
        dst in untruncated_activation_location_type_by_truncated_dst
    ), f"No untruncated ActivationLocationType for this DerivedScalarType: {dst}"
    activation_location_type = untruncated_activation_location_type_by_truncated_dst[dst]
    def make_scalar_deriver_fn(
        dst_config: DstConfig,
    ) -> ScalarDeriver:
        sub_scalar_sources = get_scalar_sources_for_activation_location_types(
            activation_location_type, dst_config.derive_gradients
        )
        model_context = dst_config.get_model_context()
        expected_dimensions = dst.shape_spec_per_token_sequence
        expected_shape: list[int | None] = []
        for dimension in expected_dimensions:
            if dimension.is_model_intrinsic:
                expected_shape.append(model_context.get_dim_size(dimension))
            else:
                expected_shape.append(None)
        return ScalarDeriver(
            dst=dst,
            dst_config=dst_config,
            sub_scalar_sources=sub_scalar_sources,
            tensor_calculate_derived_scalar_fn=partial(
                truncate_to_expected_shape_tensor_calculate_derived_scalar_fn,
                expected_shape=expected_shape,
            ),
        )
    return make_scalar_deriver_fn

================
File: neuron_explainer/activations/derived_scalars/README.md
================
# Derived scalars

**Derived scalar types (DSTs)** provide a shared interface for processing activations that are directly instantiated during the forward pass (corresponding to `ActivationLocationTypes`; for example, post-softmax attention), and functions of those activations which are useful to look at but not directly instantiated during the forward pass (for example, the norm of the attention write vector).

DSTs are intended to be as flexible as possible across models, architectures, and use cases (online processing or batch scripts); the cost of that is that there are many abstractions. The good news is that once you're familiar with the setup, it's pretty quick and easy to compute them and to define new ones.

> [!NOTE]
> In many places in the `neuron_explainer` codebase, **"dst"** is used in place of **"derived_scalar_type"**, or **"ds"** in place of **"derived_scalar"**, for conciseness.

---

## Core concepts

### Key classes

The key classes to understand are:

#### `DerivedScalarType`

- An enum containing names for all the derived scalars that have been defined.

#### `PassType`

- Forward or backward pass; derived scalars can depend on either forward pass activations, or backward pass gradients.

#### `ActivationsAndMetadata`

- A container for a set of pytorch tensors, corresponding to a certain `DerivedScalarType` and `PassType` at every layer index in the model for which it is defined.

#### `ScalarDeriver`

- An object specified by a certain `DerivedScalarType`, containing the necessary information to compute derived scalars for one or both `PassTypes`; specification often requires additional info in the `DstConfig`.

#### `DstConfig`

- A dataclass specifying any information required in constructing the `ScalarDeriver` beyond the `DerivedScalarType`. 

  - For example, DSTs that use weight tensors must know what model they are being computed for, so that the correct weights can be accessed. 

#### `ScalarSource`

- An object specifying the inputs expected by a `ScalarDeriver`. This consists of either an `ActivationLocationType` and `PassType`, or a `ScalarDeriver` and `PassType` (for the case where a `ScalarDeriver`'s inputs themselves require a `ScalarDeriver` to compute) plus a `LayerIndexer`. These two types are referred to as `RawScalarSource` and `DerivedScalarSource` respectively. 

  - **"Raw"** in this name refers to an activation is literally instantiated during a transformer forward/backward pass, and can be extracted using a hook at a particular line of code. 

  - **"Derived"** refers to a quantity that can be computed from activations instantiated during a forward/backward pass (a superset of "Raw" activations).

#### `LayerIndexer`

- Defines a transformation to be applied to an `ActivationsAndMetadata` object such that each layer index of the output is the appropriate layer index for computing a given derived scalar. 

  - For example, sometimes a derived scalar D at layer L (for L in `0...num_layers-1`) is computed by operating on activation A from layer L together with activation B from layer L0 (constant). In this case, we would apply a `ConstantLayerIndexer` to B, such that the activations of the `ActivationsAndMetadata` passed to the scalar deriver are from layer L0 of B, no matter the value of L.

#### `RawActivationStore`

- Contains `ActivationsAndMetadata` stored from a forward and possibly a backward pass, separated by `ActivationLocationType` and `PassType`.

#### `DerivedScalarStore`

- Contains `ActivationsAndMetadata` computed from a `RawActivationStore`, separated by `DerivedScalarType` and `PassType`.

---

## Computation process

Derived scalars are computed as follows:

1. User specifies `ScalarDeriver` objects using (`DerivedScalarType`, `DstConfig`) tuples, and constructs them.

2. For each `ScalarDeriver`, `scalar_deriver.sub_activation_location_type_and_pass_types` lets the user know the list of `ActivationLocationType` and `PassTypes` that it will require.

3. User populates a `RawActivationStore` with these `ActivationLocationTypes` and `PassTypes`, whether by reading activations from disk, or computing fresh activations from a forward and backward pass on a running LM. 

4. User runs `derived_scalar_store = DerivedScalarStore.derive_from_raw(raw_activation_store, scalar_derivers)`

    - (under the hood) for each `ScalarDeriver`, run `derived_scalar_activations_and_metadata = scalar_deriver.derive_from_raw(raw_activation_store, pass_type)`

        - (under the hood) for each of the `ScalarSource` objects in `scalar_deriver.sub_scalar_sources`, run `scalar_source_activations_and_metadata = sub_scalar_source.derive_from_raw(raw_activation_store)`.

            - (under the hood) this either gets an `ActivationsAndMetadata` object by `ActivationLocationType` and `PassType` directly from `raw_activation_store`, or derives it using `sub_scalar_source.scalar_deriver.derive_from_raw(raw_activation_store, pass_type)`, then applies `sub_scalar_source.layer_indexer`.

        - (under the hood) run `derived_scalar_activations_and_metadata = scalar_deriver.activations_and_metadata_calculate_derived_scalar_fn(scalar_source_activations_and_metadata_tuple, pass_type)`.

            - (under the hood) run `derived_scalar_tensor = scalar_deriver.tensor_calculate_derived_scalar_fn(scalar_source_tensor_tuple, layer_index, pass_type)` for each `layer_index`, which together populate the `ActivationsAndMetadata` object.

    - (under the hood) the `ActivationsAndMetadata` objects together populate the `DerivedScalarStore`.

Optionally, the outermost loop may be skipped if only a single `derived_scalar_activations_and_metadata` object is required.

> The most bare-bones function that shows the steps outlined above is in [activation_server/derived_scalar_computation:get_derived_scalars_for_prompt](../../../neuron_explainer/activation_server/derived_scalar_computation.py)

---

## How to define a new `ScalarDeriver`

When defining a new `ScalarDeriver` that does not correspond to an `ActivationLocationType`, the user provides:

1. The information necessary to compute the derived scalar. 

    This includes:

   1. The `ScalarSources` it expects

   2. The `tensor_calculate_derived_scalar_fn`, which takes tensors as arguments corresponding to the `ScalarSources` as well as the `layer_index` and `pass_type`. 

        - This lives in a function called `make_..._scalar_deriver`. These can be specified implicitly, if the new scalar can be derived from a transformation on one or more pre-existing derived scalars. 

        - The `make_..._scalar_deriver` functions are associated to DSTs in [derived_scalars/make_scalar_derivers.py](../../../neuron_explainer/activations/derived_scalars/make_scalar_derivers.py).

2. A specification of the shape of the output, in terms of `Dimension` objects. 

   - This lives in [derived_scalars/derived_scalar_types.py](../../../neuron_explainer/activations/derived_scalars/derived_scalar_types.py)

> `ScalarDeriver` is meant to be the primary class used to refer to activations once they are READ from disk. 
>
> When WRITTEN to disk, the primary class used is `ActivationLocationType`, since we always want to save the least processed form of the data possible.

---

## How to define a new `DerivedScalarType`

1. Add a new `DerivedScalarType` to the Enum in [scalar_deriver.py](../../../neuron_explainer/activations/derived_scalars/scalar_deriver.py).

2. Add a specification of its intended shape per token sequence to `shape_spec_per_token_sequence_by_dst` (e.g. does it contain a separate dimension for attended-to sequence tokens? Is it per-attention head or per-MLP neuron?)

3. In a separate file (possibly an existing file, if related DSTs have been defined already) write a `make_..._scalar_deriver` function. This function takes a `DstConfig` and returns a `ScalarDeriver` object. 

   - The core of this object is `tensor_calculate_derived_scalar_fn`, which takes as input a tuple of tensors corresponding to an existing activation location type and pass type or DST and pass type, and returns a tensor corresponding to the new DST. 

   - To construct `tensor_calculate_derived_scalar_fn`, you might require some metadata (for example, a `ModelContext` object which gives the ability to load model weights from disk). If your DST requires a new piece of metadata, add it to `DstConfig` as an optional argument. 

   - In addition to `calculate_derived_scalar_fn`, you must also specify which ScalarSources are required to compute this `DerivedScalarType` (`sub_scalar_sources`)

4. Once the `make_..._scalar_deriver` function is done, add a row like this to the registry in [make_scalar_derivers.py](../../../neuron_explainer/activations/derived_scalars/make_scalar_derivers.py):

    ```py
    DerivedScalarType.NEW_DST: make_new_dst_scalar_deriver,
    ```

---

## Examples

  For a simple example of a `make_..._scalar_deriver` function, see `make_mlp_write_norm_scalar_deriver` in [mlp.py](../../../neuron_explainer/activations/derived_scalars/mlp.py)

### Usage example 

```py
import torch

from neuron_explainer.activation_server.derived_scalar_computation import (
    get_derived_scalars_for_prompt,
    maybe_construct_loss_fn_for_backward_pass,
)
from neuron_explainer.activation_server.requests_and_responses import LossFnConfig, LossFnName
from neuron_explainer.activations.derived_scalars.config import DstConfig
from neuron_explainer.activations.derived_scalars.derived_scalar_types import DerivedScalarType
from neuron_explainer.models.model_context import StandardModelContext

prompt = "This is a test"

# This object contains model metadata and has methods for loading weights. It also has a 
# method to spin up a transformer for running a forward and backward pass.
model_context = StandardModelContext(model_name="gpt2-small", device=torch.device("cuda:0"))

# These are the derived scalars of interest; these correspond to direct writes from
# MLP neurons and attention heads to the gradient at the final residual
# stream layer ("direct effects" on the loss).
dst_list = [
    DerivedScalarType.MLP_WRITE_TO_FINAL_RESIDUAL_GRAD,
    DerivedScalarType.UNFLATTENED_ATTN_WRITE_TO_FINAL_RESIDUAL_GRAD,
]

# The derived scalars require model weights and metadata (such as the number of layers)
# for their computation. Nothing else needs to be specified.
dst_config = DstConfig(
    model_context=model_context,
)
dst_and_config_list = [(dst, dst_config) for dst in dst_list]

# This specifies how the backward pass will be run (outside the DST framework)
loss_fn_for_backward_pass = maybe_construct_loss_fn_for_backward_pass(
    model_context=model_context,
    config=LossFnConfig(
        name=LossFnName.LOGIT_DIFF,
        target_tokens=["."],
        distractor_tokens=["!"],
    ),
)

# This returns a DerivedScalarStore containing the DSTs specified, as well as a RawActivationStore
# containing the activations that were required to compute those DSTs.
ds_store, _, raw_store = get_derived_scalars_for_prompt(
    model_context=model_context,
    prompt=prompt,
    loss_fn_for_backward_pass=loss_fn_for_backward_pass,
    dst_and_config_list=dst_and_config_list,
)

# This returns the top 10 largest derived scalar values, across all the types specified, as well 
# as identifiers for the location of each within the model (i.e. which neuron or attention head 
# they correspond to, and at which token or token pair)
values, indices = ds_store.topk(10)

```

================
File: neuron_explainer/activations/derived_scalars/reconstituted.py
================
from typing import Any, Callable
import torch
from neuron_explainer.activations.derived_scalars.derived_scalar_types import DerivedScalarType
from neuron_explainer.activations.derived_scalars.indexing import AttentionTraceType, PreOrPostAct
from neuron_explainer.models import Autoencoder
from neuron_explainer.models.autoencoder_context import AutoencoderContext
from neuron_explainer.models.hooks import AttentionHooks, NormalizationHooks, TransformerHooks
from neuron_explainer.models.model_component_registry import (
    ActivationLocationType,
    LayerIndex,
    NodeType,
    PassType,
)
from neuron_explainer.models.transformer import Norm, Transformer, TransformerLayer
# scalar derivers that take residual stream as input and
# reconstitute activations such as attention post softmax and mlp post activations
def detach_hook(x: torch.Tensor, *args: Any, **kwargs: Any) -> torch.Tensor:
    return x.detach()
def make_hook_getter() -> tuple[Callable[..., Any], Callable[[], Any]]:
    """
    Returns a hook to append, and a function to retrieve the value of the hook.
    The retrieve function must be called after the hook has been called.
    """
    retrieve = {}
    def hook(x: torch.Tensor, *args: Any, **kwargs: Any) -> torch.Tensor:
        retrieve["value"] = x
        return x
    return hook, lambda: retrieve["value"]
def zero_batch_dim_hook(x: torch.Tensor, *args: Any, **kwargs: Any) -> torch.Tensor:
    """This hook can be applied before unnecessary computations to save compute."""
    return x[:0, ...]
def apply_layer_norm(
    x: torch.Tensor, norm_module: Norm, detach_layer_norm_scale: bool
) -> torch.Tensor:
    hooks = NormalizationHooks()
    if detach_layer_norm_scale:
        hooks = hooks.append_to_path("scale.fwd", detach_hook)
    return norm_module(x, hooks=hooks)
def add_q_k_or_v_detach_hook(
    hooks: AttentionHooks, q_k_or_v: ActivationLocationType | None
) -> None:
    """If q_k_or_v is None, leave everything attached. If q_k_or_v is not None,
    then leave only the corresponding tensor (Q, K, or V) attached."""
    if q_k_or_v is None:
        return
    if q_k_or_v != ActivationLocationType.ATTN_QUERY:
        hooks.q.append_fwd(detach_hook)
    if q_k_or_v != ActivationLocationType.ATTN_KEY:
        hooks.k.append_fwd(detach_hook)
    if q_k_or_v != ActivationLocationType.ATTN_VALUE:
        hooks.v.append_fwd(detach_hook)
def apply_attn_pre_softmax(
    transformer_layer: TransformerLayer,
    q_k_or_v: ActivationLocationType | None,
    resid_post_mlp: torch.Tensor,
    detach_layer_norm_scale: bool,
) -> torch.Tensor:
    attn_input = apply_layer_norm(
        resid_post_mlp.unsqueeze(0),
        transformer_layer.ln_1,
        detach_layer_norm_scale=detach_layer_norm_scale,
    )  # add batch dimension
    hooks = AttentionHooks()
    add_q_k_or_v_detach_hook(hooks, q_k_or_v)
    get_hook, get_attn = make_hook_getter()
    hooks.qk_logits.append_fwd(get_hook)
    # avoid v_out expense
    hooks.v.append_fwd(zero_batch_dim_hook)
    hooks.qk_logits.append_fwd(zero_batch_dim_hook)
    transformer_layer.attn.forward(attn_input, hooks=hooks)
    # remove batch dimension
    return get_attn()[0]
def apply_mlp_act(
    transformer_layer: TransformerLayer,
    resid_post_attn: torch.Tensor,
    detach_layer_norm_scale: bool,
) -> torch.Tensor:
    pre_act = apply_mlp_pre_act(transformer_layer, resid_post_attn, detach_layer_norm_scale)
    post_act = transformer_layer.mlp.act(pre_act)
    return post_act
def apply_mlp_pre_act(
    transformer_layer: TransformerLayer,
    resid_post_attn: torch.Tensor,
    detach_layer_norm_scale: bool,
) -> torch.Tensor:
    post_ln_mlp = apply_layer_norm(
        resid_post_attn.unsqueeze(0),
        transformer_layer.ln_2,
        detach_layer_norm_scale=detach_layer_norm_scale,
    )  # add batch dimension
    pre_act = transformer_layer.mlp.in_layer(post_ln_mlp)
    return pre_act.squeeze(0)  # remove batch dimension
def apply_autoencoder_pre_latent(
    transformer_layer: TransformerLayer,
    autoencoder: Autoencoder,
    resid: torch.Tensor,
    autoencoder_dst: DerivedScalarType,
    detach_layer_norm_scale: bool,
    latent_slice: slice = slice(None),
) -> torch.Tensor:
    """
    Given the residual stream activations preceding an autoencoder to be
    applied to a given DST, first compute the activations of the DST (`to_be_encoded`)
    and then apply the autoencoder to these activations (NOT INCLUDING the autoencoder nonlinearity),
    and return the result.
    """
    match autoencoder_dst:
        case DerivedScalarType.MLP_POST_ACT:
            to_be_encoded = apply_mlp_act(
                transformer_layer,
                resid,
                detach_layer_norm_scale=detach_layer_norm_scale,
            )
        case DerivedScalarType.RESID_DELTA_ATTN:
            to_be_encoded = apply_resid_delta_attn(
                transformer_layer,
                resid,
                detach_layer_norm_scale=detach_layer_norm_scale,
            )
        case DerivedScalarType.RESID_DELTA_MLP:
            to_be_encoded = apply_resid_delta_mlp(
                transformer_layer,
                resid,
                detach_layer_norm_scale=detach_layer_norm_scale,
            )
        case _:
            raise NotImplementedError(autoencoder_dst.node_type)
    return autoencoder.encode_pre_act(to_be_encoded, latent_slice=latent_slice)
def apply_autoencoder_latent(
    transformer_layer: TransformerLayer,
    autoencoder: Autoencoder,
    resid: torch.Tensor,
    autoencoder_dst: DerivedScalarType,
    detach_layer_norm_scale: bool,
) -> torch.Tensor:
    """
    Given the residual stream activations preceding an autoencoder to be
    applied to a given DST, first compute the activations of the DST (`to_be_encoded`)
    and then apply the autoencoder to these activations (INCLUDING the autoencoder nonlinearity),
    and return the result.
    """
    pre_latent = apply_autoencoder_pre_latent(
        transformer_layer,
        autoencoder,
        resid,
        autoencoder_dst,
        detach_layer_norm_scale=detach_layer_norm_scale,
    )
    return autoencoder.activation(pre_latent)
def apply_resid_delta_attn(
    transformer_layer: TransformerLayer, resid_post_mlp: torch.Tensor, detach_layer_norm_scale: bool
) -> torch.Tensor:
    """
    Compute resid_delta_attn (the output of an attention layer) from the residual stream
    just before the layer
    """
    X = resid_post_mlp.unsqueeze(0)
    hooks = TransformerHooks()
    if detach_layer_norm_scale:
        hooks = hooks.append_to_path("resid.torso.ln_attn.scale.fwd", detach_hook)
    # empty hooks and KV cache to match type signature of transformer_layer methods
    # second output is kv_cache, which is not used here
    attn_delta, _ = transformer_layer.attn_block(X, kv_cache=None, pad=None, hooks=hooks)
    return attn_delta.squeeze(0)
def apply_resid_delta_mlp(
    transformer_layer: TransformerLayer,
    resid_post_attn: torch.Tensor,
    detach_layer_norm_scale: bool,
) -> torch.Tensor:
    """
    Compute resid_delta_mlp (the output of an MLP layer) from the residual stream
    just before the layer
    """
    X = resid_post_attn.unsqueeze(0)
    hooks = TransformerHooks()
    if detach_layer_norm_scale:
        hooks = hooks.append_to_path("resid.torso.ln_mlp.scale.fwd", detach_hook)
    # empty hooks to match type signature of transformer_layer methods
    mlp_delta = transformer_layer.mlp_block(X, hooks=hooks)
    return mlp_delta.squeeze(0)
def make_reconstituted_activation_fn(
    transformer: Transformer,
    autoencoder_context: AutoencoderContext | None,
    node_type: NodeType,
    pre_or_post_act: PreOrPostAct | None,
    detach_layer_norm_scale: bool,
    attention_trace_type: AttentionTraceType | None,
) -> Callable[[torch.Tensor, LayerIndex, PassType], torch.Tensor]:
    match node_type:
        case NodeType.ATTENTION_HEAD:
            match attention_trace_type:
                case AttentionTraceType.QK:
                    q_or_k = None
                case AttentionTraceType.Q:
                    q_or_k = ActivationLocationType.ATTN_QUERY
                case AttentionTraceType.K:
                    q_or_k = ActivationLocationType.ATTN_KEY
                case None:
                    raise ValueError(
                        "attention_trace_type must be specified for attention activations"
                    )
            match pre_or_post_act:
                case PreOrPostAct.PRE:
                    def act_fn(
                        resid: torch.Tensor,
                        layer_index: int | None,
                        pass_type: PassType,
                    ) -> torch.Tensor:
                        assert pass_type == PassType.FORWARD
                        assert layer_index is not None
                        return apply_attn_pre_softmax(
                            transformer_layer=transformer.xf_layers[layer_index],
                            q_k_or_v=q_or_k,
                            resid_post_mlp=resid,
                            detach_layer_norm_scale=detach_layer_norm_scale,
                        )
                case PreOrPostAct.POST:
                    apply_attn_V_act = make_apply_attn_V_act(
                        transformer=transformer,
                        q_k_or_v=q_or_k,
                        detach_layer_norm_scale=detach_layer_norm_scale,
                    )  # returns attn, V
                    def act_fn(
                        resid: torch.Tensor,
                        layer_index: LayerIndex,
                        pass_type: PassType,
                    ) -> torch.Tensor:
                        assert pass_type == PassType.FORWARD
                        assert layer_index is not None
                        return apply_attn_V_act(
                            resid,
                            layer_index,
                            pass_type,
                        )[
                            0
                        ]  # returns attn
                case _:
                    raise NotImplementedError(pre_or_post_act)
        case NodeType.MLP_NEURON:
            match pre_or_post_act:
                case PreOrPostAct.PRE:
                    def act_fn(
                        resid: torch.Tensor,
                        layer_index: int | None,
                        pass_type: PassType,
                    ) -> torch.Tensor:
                        assert pass_type == PassType.FORWARD
                        assert layer_index is not None
                        return apply_mlp_pre_act(
                            transformer_layer=transformer.xf_layers[layer_index],
                            resid_post_attn=resid,
                            detach_layer_norm_scale=detach_layer_norm_scale,
                        )
                case PreOrPostAct.POST:
                    def act_fn(
                        resid: torch.Tensor,
                        layer_index: LayerIndex,
                        pass_type: PassType,
                    ) -> torch.Tensor:
                        assert pass_type == PassType.FORWARD
                        assert layer_index is not None
                        return apply_mlp_act(
                            transformer_layer=transformer.xf_layers[layer_index],
                            resid_post_attn=resid,
                            detach_layer_norm_scale=detach_layer_norm_scale,
                        )
                case _:
                    raise NotImplementedError(pre_or_post_act)
        case (
            NodeType.AUTOENCODER_LATENT
            | NodeType.MLP_AUTOENCODER_LATENT
            | NodeType.ATTENTION_AUTOENCODER_LATENT
        ):
            assert autoencoder_context is not None
            match pre_or_post_act:
                case PreOrPostAct.PRE:
                    def act_fn(
                        resid: torch.Tensor,
                        layer_index: int | None,
                        pass_type: PassType,
                    ) -> torch.Tensor:
                        assert pass_type == PassType.FORWARD
                        assert layer_index is not None
                        return apply_autoencoder_pre_latent(
                            transformer_layer=transformer.xf_layers[layer_index],
                            autoencoder=autoencoder_context.get_autoencoder(layer_index),
                            resid=resid,
                            autoencoder_dst=autoencoder_context.dst,
                            detach_layer_norm_scale=detach_layer_norm_scale,
                        )
                case PreOrPostAct.POST:
                    def act_fn(
                        resid: torch.Tensor,
                        layer_index: LayerIndex,
                        pass_type: PassType,
                    ) -> torch.Tensor:
                        assert pass_type == PassType.FORWARD
                        assert layer_index is not None
                        return apply_autoencoder_latent(
                            transformer_layer=transformer.xf_layers[layer_index],
                            autoencoder=autoencoder_context.get_autoencoder(layer_index),
                            resid=resid,
                            autoencoder_dst=autoencoder_context.dst,
                            detach_layer_norm_scale=detach_layer_norm_scale,
                        )
                case _:
                    raise NotImplementedError(pre_or_post_act)
        case _:
            raise NotImplementedError(node_type)
    return act_fn
def make_apply_attn_V_act(
    transformer: Transformer,
    q_k_or_v: ActivationLocationType | None,
    detach_layer_norm_scale: bool,
) -> Callable[[torch.Tensor, LayerIndex, PassType], tuple[torch.Tensor, torch.Tensor]]:
    """Used in functions that require reconstituting some or all of the attention head
    operation. Supports specifying a stop grad through all but one of Q, K, and V; or
    if q_k_or_v is None, then all of Q, K, and V are backprop'ed through."""
    def apply_attn_V_act(
        resid: torch.Tensor,
        layer_index: LayerIndex,
        pass_type: PassType,
    ) -> tuple[torch.Tensor, torch.Tensor]:
        assert pass_type == PassType.FORWARD
        transformer_layer = transformer.xf_layers[layer_index]
        attn_input = apply_layer_norm(
            resid.unsqueeze(0),
            transformer_layer.ln_1,
            detach_layer_norm_scale=detach_layer_norm_scale,
        )  # add batch dimension
        hooks = AttentionHooks()
        add_q_k_or_v_detach_hook(hooks, q_k_or_v)
        get_hook, get_v = make_hook_getter()
        hooks.v.append_fwd(get_hook)
        get_hook, get_attn = make_hook_getter()
        hooks.qk_probs.append_fwd(get_hook)
        # avoid v_out expense
        hooks.v.append_fwd(zero_batch_dim_hook)
        hooks.qk_probs.append_fwd(zero_batch_dim_hook)
        transformer_layer.attn.forward(attn_input, hooks=hooks)
        # remove batch dimensions
        return get_attn()[0], get_v()[0]
    return apply_attn_V_act
def make_apply_logits(
    transformer: Transformer,
    detach_layer_norm_scale: bool,
) -> Callable[[torch.Tensor], torch.Tensor]:
    def apply_logits(
        resid_post_mlp: torch.Tensor,
    ) -> torch.Tensor:
        """
        Input: (n_sequence_tokens, d_model) residual stream post-mlp activations at final layer.
        Output: (n_sequence_tokens, n_vocab) logprobs for each token in the sequence.
        """
        post_ln_f = apply_layer_norm(
            resid_post_mlp.unsqueeze(0),
            transformer.final_ln,
            detach_layer_norm_scale=detach_layer_norm_scale,
        )  # add batch dimension
        return transformer.unembed(post_ln_f).squeeze(0)  # remove batch dimension
    return apply_logits
def make_apply_logprobs(
    transformer: Transformer,
    detach_layer_norm_scale: bool,
) -> Callable[[torch.Tensor], torch.Tensor]:
    def apply_logprobs(
        resid_post_mlp: torch.Tensor,
    ) -> torch.Tensor:
        """
        Input: (n_sequence_tokens, d_model) residual stream post-mlp activations at final layer.
        Output: (n_sequence_tokens, n_vocab) logprobs for each token in the sequence.
        """
        logits = make_apply_logits(transformer, detach_layer_norm_scale)(resid_post_mlp)
        return torch.log_softmax(logits, dim=-1)
    return apply_logprobs
def make_apply_autoencoder(
    autoencoder_context: AutoencoderContext,
    use_no_grad: bool = True,  # use True to avoid keeping gradient info for autoencoder;
    # TODO: consider deleting in favor of universal non-gradient-keeping at the outside of ScalarDeriver base functions
) -> Callable[[torch.Tensor, LayerIndex], torch.Tensor]:
    """
    Returns a function that takes a tensor of activations and returns a tensor of the autoencoder
    latent representation of each token.
    """
    # TODO(sbills): Resolve the circular import between this file and attention.py.
    from neuron_explainer.activations.derived_scalars.attention import make_reshape_fn
    # reshape activations to be (n_tokens, n_inputs)
    dst = autoencoder_context.dst
    reshape_fn = make_reshape_fn(dst)
    def apply_autoencoder(raw_activations: torch.Tensor, layer_index: LayerIndex) -> torch.Tensor:
        assert (
            layer_index in autoencoder_context.layer_indices
        ), f"Layer index {layer_index} not in {autoencoder_context.layer_indices}"
        autoencoder = autoencoder_context.get_autoencoder(layer_index)
        latent_activations = autoencoder.encode(reshape_fn(raw_activations))
        return latent_activations  # shape (n_tokens, n_latents)
    if use_no_grad:
        apply_autoencoder = torch.no_grad()(apply_autoencoder)
    return apply_autoencoder

================
File: neuron_explainer/activations/derived_scalars/reconstituter_class.py
================
"""
(My understanding [Dan]; may have some names or details wrong):
This file contains tools for running forward-mode and backward-mode auto-differentiation
(gradients and Jacobian vector products) on tensor -> tensor functions, and generating scalar
derivers using those derivatives.
Child classes of Reconstituter can be created for classes of tensor->tensor functions operating
on residual stream activations, for example recomputing activations of interest from the residual
stream, or recomputing activation and multiplying by the gradient.
Motivation:
Jacobians of functions are slow to work with naively; when operating on vectors of dimension n, they
involve multiplying many nxn matrices together. Backprop through multiple functions is faster than
multiplying the Jacobian of each function, since it's effectively multiplying an nxn matrix by a
length n vector at each step. Jacobian vector products are like backprop, but in the forward
direction. Starting from a vector defined at an early point in the network, you can multiply that
length n vector by the nxn Jacobian at each step of processing, analogous to backprop.
Gradients are useful for computing direct writes from many upstream nodes to a single downstream
node in parallel. Jacobian vector products are useful for computing direct writes to many downstream
nodes in parallel from a single upstream node. The Reconstituter class is meant to make both easier.
"""
import dataclasses
from abc import ABC, abstractmethod
from functools import partial
from typing import Callable
import torch
from torch.func import jvp as torch_jvp
from neuron_explainer.activations.derived_scalars.derived_scalar_types import DerivedScalarType
from neuron_explainer.activations.derived_scalars.indexing import (
    DETACH_LAYER_NORM_SCALE,
    ActivationIndex,
    AttentionTraceType,
    NodeIndex,
    PreOrPostAct,
    TraceConfig,
)
from neuron_explainer.activations.derived_scalars.locations import (
    ConstantLayerIndexer,
    IdentityLayerIndexer,
    get_activation_index_for_residual_dst,
    get_previous_residual_dst_for_node_type,
    precedes_final_layer,
)
from neuron_explainer.activations.derived_scalars.reconstituted import (
    make_apply_autoencoder,
    make_reconstituted_activation_fn,
)
from neuron_explainer.activations.derived_scalars.scalar_deriver import (
    DerivedScalarSource,
    DstConfig,
    ScalarDeriver,
    ScalarSource,
)
from neuron_explainer.activations.derived_scalars.utils import detach_and_clone
from neuron_explainer.models.autoencoder_context import AutoencoderContext
from neuron_explainer.models.model_component_registry import (
    ActivationLocationType,
    LayerIndex,
    NodeType,
    PassType,
)
from neuron_explainer.models.transformer import Transformer
def compute_gradient_of_scalar_valued_fn_wrt_activations(
    scalar_valued_fn: Callable[[torch.Tensor], torch.Tensor],
    resid: torch.Tensor,
) -> torch.Tensor:
    """
    scalar_valued_fn takes a vector input and returns a scalar. This function evaluates the gradient
    of the scalar_valued_fn with respect to the vector input, at the vector specified by resid.
    """
    scalar_result = scalar_valued_fn(resid)
    assert scalar_result.shape == (), scalar_result.shape
    scalar_result.backward()
    assert resid.grad is not None
    return resid.grad.detach()
def compute_jvp_of_vector_valued_fn_wrt_activations(
    vector_valued_fn: Callable[[torch.Tensor], torch.Tensor],
    resid: torch.Tensor,
    write_vector: torch.Tensor,
) -> torch.Tensor:
    """
    vector_valued_fn takes a vector input and returns a vector. This function evaluates the jacobian
    of the vector_valued_fn with respect to the vector input, at the vector specified by resid, and
    then multiplies the jacobian by write_vector. pytorch's jvp function is used to perform this
    efficiently (without instantiating the full jacobian matrix). This can be considered a "forward
    prop" or the multiplication of a vector by the derivative of a computation graph in the forward
    direction (rather than the more common reverse direction)
    """
    jacobian_vector_product = torch_jvp(
        vector_valued_fn,
        (resid,),
        (write_vector,),
    )[1]
    return jacobian_vector_product
class Reconstituter(ABC):
    """
    This base class has at its core a tensor -> tensor function, reconstitute_activations,
    that computes some set of activations from the residual stream.
    It can compute gradients of that function with respect to the residual stream, using a scalar
    hook to convert the output of reconstitute_activations to a scalar (`reconstitute_gradient`).
    It can also compute Jacobian-vector products (JVPs) of the Jacobian of reconstitute_activations
    with respect to the residual stream, and some write vector (`reconstitute_jvp`).
    It can also be used to generate ScalarDerivers for the original activation, gradient and JVP
    computations (`make_activation_scalar_deriver`, `make_gradient_scalar_deriver`,
    `make_jvp_scalar_deriver`).
    """
    residual_dst: DerivedScalarType
    requires_other_scalar_source: bool
    _input_activation_shape: tuple[int, ...] | None
    def __init__(self) -> None:
        self._input_activation_shape = None
    @abstractmethod
    def reconstitute_activations(
        self,
        resid: torch.Tensor,
        other_arg: torch.Tensor | None,
        layer_index: LayerIndex,
        pass_type: PassType,
    ) -> torch.Tensor:
        """Must be implemented by subclasses. This function takes the residual stream as input,
        and returns the (tensor of) activations."""
        pass
    def make_other_scalar_source(self, dst_config: DstConfig) -> ScalarSource:
        """If requires_other_scalar_source is True, this function must be implemented by subclasses.
        Otherwise, it is not used. This generates a ScalarSource for a second scalar used by
        reconstitute_activations."""
        raise NotImplementedError
    def make_residual_scalar_deriver(self, dst_config: DstConfig) -> ScalarDeriver:
        from neuron_explainer.activations.derived_scalars.make_scalar_derivers import (  # lazy to avoid circular import
            make_scalar_deriver,
        )
        assert self.residual_dst.node_type == NodeType.RESIDUAL_STREAM_CHANNEL
        return make_scalar_deriver(
            dst=self.residual_dst,
            dst_config=dst_config,
        )
    def get_residual_activation_index_for_node_index(
        self, node_index: NodeIndex
    ) -> ActivationIndex:
        """
        Given a node index of interest, return the activation index corresponding to the preceding
        residual stream location for that node index. The activation index corresponds to the entire
        residual stream activation tensor for that layer.
        """
        layer_index = node_index.layer_index
        assert layer_index is not None
        return get_activation_index_for_residual_dst(
            dst=self.residual_dst,
            layer_index=layer_index,
        )
    def get_residual_activation_index_for_trace_config(
        self, trace_config: TraceConfig
    ) -> ActivationIndex:
        """
        Given a node index of interest, return the activation index corresponding to the preceding
        residual stream location for that node index. The activation index corresponds to the entire
        residual stream activation tensor for that layer.
        """
        layer_index = trace_config.layer_index
        assert layer_index is not None
        return get_activation_index_for_residual_dst(
            dst=self.residual_dst,
            layer_index=layer_index,
        )
    def _check_other_arg(self, other_arg: torch.Tensor | None) -> None:
        if self.requires_other_scalar_source:
            assert other_arg is not None
        else:
            assert other_arg is None
    def vector_reshape_hook(self, input_tensor: torch.Tensor) -> torch.Tensor:
        """
        Some activations are >1-d tensors per token, but the torch JVP function expects
        1-d tensor -> 1-d tensor functions as input. This reshapes the input tensor to be 1-d,
        and stores the shape for inverting the reshape later.
        """
        self._input_activation_shape = input_tensor.shape
        return input_tensor.reshape(input_tensor.shape[0], -1)
    def vector_unreshape_hook(self, output_vector: torch.Tensor) -> torch.Tensor:
        assert self._input_activation_shape is not None, "must call vector_reshape_hook first"
        reshaped = output_vector.reshape(self._input_activation_shape)
        self._input_activation_shape = None
        return reshaped
    def reconstitute_gradient(
        self,
        resid: torch.Tensor,
        other_arg: torch.Tensor | None,
        layer_index: LayerIndex,
        pass_type: PassType,
        scalar_hook: Callable[[torch.Tensor], torch.Tensor],
    ) -> torch.Tensor:
        assert pass_type == PassType.FORWARD
        self._check_other_arg(other_arg)
        resid = detach_and_clone(resid, requires_grad=True)
        if other_arg is not None:
            other_arg = detach_and_clone(other_arg, requires_grad=False)
        def reconstitute_scalar_activation(resid: torch.Tensor) -> torch.Tensor:
            return scalar_hook(
                self.reconstitute_activations(
                    resid,
                    other_arg,
                    layer_index=layer_index,
                    pass_type=pass_type,
                )
            )
        return compute_gradient_of_scalar_valued_fn_wrt_activations(
            scalar_valued_fn=reconstitute_scalar_activation,
            resid=resid,
        )
    def reconstitute_jvp(
        self,
        resid: torch.Tensor,
        other_arg: torch.Tensor | None,
        write_vector: torch.Tensor,
        layer_index: LayerIndex,
        pass_type: PassType,
    ) -> torch.Tensor:
        assert pass_type == PassType.FORWARD
        self._check_other_arg(other_arg)
        resid = detach_and_clone(resid, requires_grad=True)
        if other_arg is not None:
            other_arg = detach_and_clone(other_arg, requires_grad=False)
        def reconstitute_vector_activation(resid: torch.Tensor) -> torch.Tensor:
            return self.vector_reshape_hook(
                self.reconstitute_activations(
                    resid,
                    other_arg,
                    layer_index=layer_index,
                    pass_type=pass_type,
                )
            )
        return self.vector_unreshape_hook(
            compute_jvp_of_vector_valued_fn_wrt_activations(
                vector_valued_fn=reconstitute_vector_activation,
                resid=resid,
                write_vector=write_vector,
            )
        )
    def make_activation_scalar_deriver(
        self, dst_config: DstConfig, output_dst: DerivedScalarType
    ) -> ScalarDeriver:
        residual_scalar_deriver = self.make_residual_scalar_deriver(dst_config)
        if self.requires_other_scalar_source:
            def reconstitute_activations_2_arg(
                resid: torch.Tensor,
                other: torch.Tensor,
                layer_index: LayerIndex,
                pass_type: PassType,
            ) -> torch.Tensor:
                return self.reconstitute_activations(
                    resid,
                    other,
                    layer_index=layer_index,
                    pass_type=pass_type,
                )
            other_scalar_source = self.make_other_scalar_source(dst_config)
            return residual_scalar_deriver.apply_layerwise_transform_fn_to_output_and_other_tensor(
                layerwise_transform_fn=reconstitute_activations_2_arg,
                pass_type_to_transform=PassType.FORWARD,
                other_scalar_source=other_scalar_source,
                output_dst=output_dst,
            )
        else:
            def reconstitute_activations(
                resid: torch.Tensor,
                layer_index: LayerIndex,
                pass_type: PassType,
            ) -> torch.Tensor:
                return self.reconstitute_activations(
                    resid,
                    None,
                    layer_index=layer_index,
                    pass_type=pass_type,
                )
            return residual_scalar_deriver.apply_layerwise_transform_fn_to_output(
                layerwise_transform_fn=reconstitute_activations,
                pass_type_to_transform=PassType.FORWARD,
                output_dst=output_dst,
            )
    def make_gradient_scalar_deriver(
        self,
        scalar_hook: Callable[[torch.Tensor], torch.Tensor],
        dst_config: DstConfig,
        output_dst: (
            DerivedScalarType | None
        ) = None,  # sometimes, we are OK with not defining a new DST for gradient
        # directions, as they are often not used as standalone scalar derivers
    ) -> ScalarDeriver:
        residual_scalar_deriver = self.make_residual_scalar_deriver(dst_config)
        output_dst = output_dst or residual_scalar_deriver.dst
        assert output_dst is not None
        assert output_dst.node_type == NodeType.RESIDUAL_STREAM_CHANNEL
        if self.requires_other_scalar_source:
            def reconstitute_gradient_2_arg(
                resid: torch.Tensor,
                other: torch.Tensor,
                layer_index: LayerIndex,
                pass_type: PassType,
            ) -> torch.Tensor:
                return self.reconstitute_gradient(
                    resid,
                    other,
                    layer_index=layer_index,
                    pass_type=pass_type,
                    scalar_hook=scalar_hook,
                )
            other_scalar_source = self.make_other_scalar_source(dst_config)
            return residual_scalar_deriver.apply_layerwise_transform_fn_to_output_and_other_tensor(
                layerwise_transform_fn=reconstitute_gradient_2_arg,
                pass_type_to_transform=PassType.FORWARD,
                other_scalar_source=other_scalar_source,
                output_dst=output_dst,
            )
        else:
            def reconstitute_gradient(
                resid: torch.Tensor,
                layer_index: LayerIndex,
                pass_type: PassType,
            ) -> torch.Tensor:
                return self.reconstitute_gradient(
                    resid,
                    None,
                    layer_index=layer_index,
                    pass_type=pass_type,
                    scalar_hook=scalar_hook,
                )
            return residual_scalar_deriver.apply_layerwise_transform_fn_to_output(
                layerwise_transform_fn=reconstitute_gradient,
                pass_type_to_transform=PassType.FORWARD,
                output_dst=output_dst,
            )
    def make_jvp_scalar_deriver(
        self,
        write_scalar_source: ScalarSource,
        dst_config: DstConfig,
        output_dst: DerivedScalarType,
    ) -> ScalarDeriver:
        residual_scalar_deriver = self.make_residual_scalar_deriver(dst_config)
        write_precedes_jacobian_layer = partial(
            precedes_final_layer,
            derived_scalar_location_within_layer=write_scalar_source.location_within_layer,
            derived_scalar_layer_index=write_scalar_source.layer_index,
            final_residual_location_within_layer=residual_scalar_deriver.location_within_layer,
        )
        if self.requires_other_scalar_source:
            def reconstitute_jvp_tuple_arg(
                activation_data_tuple: tuple[torch.Tensor, ...],
                layer_index: LayerIndex,
                pass_type: PassType,
            ) -> torch.Tensor:
                resid, other_arg, write_vector = activation_data_tuple
                jacobian_vector_product = self.reconstitute_jvp(
                    resid,
                    other_arg,
                    write_vector=write_vector,
                    layer_index=layer_index,
                    pass_type=pass_type,
                )
                if write_precedes_jacobian_layer(final_residual_layer_index=layer_index):
                    return jacobian_vector_product
                else:
                    # wasteful, but we require the shape to be correct
                    return torch.zeros_like(jacobian_vector_product)
            resid_scalar_source = DerivedScalarSource(
                scalar_deriver=residual_scalar_deriver,
                pass_type=PassType.FORWARD,
                layer_indexer=IdentityLayerIndexer(),
            )
            other_scalar_source = self.make_other_scalar_source(dst_config)
            return ScalarDeriver(
                dst=output_dst,
                dst_config=dst_config,
                tensor_calculate_derived_scalar_fn=reconstitute_jvp_tuple_arg,
                sub_scalar_sources=(resid_scalar_source, other_scalar_source, write_scalar_source),
            )
        else:
            def reconstitute_jvp(
                resid: torch.Tensor,
                write_vector: torch.Tensor,
                layer_index: LayerIndex,
                pass_type: PassType,
            ) -> torch.Tensor:
                jacobian_vector_product = self.reconstitute_jvp(
                    resid,
                    None,
                    write_vector=write_vector,
                    layer_index=layer_index,
                    pass_type=pass_type,
                )
                if write_precedes_jacobian_layer(final_residual_layer_index=layer_index):
                    return jacobian_vector_product
                else:
                    # wasteful, but we require the shape to be correct
                    return torch.zeros_like(jacobian_vector_product)
            return residual_scalar_deriver.apply_layerwise_transform_fn_to_output_and_other_tensor(
                layerwise_transform_fn=reconstitute_jvp,
                pass_type_to_transform=PassType.FORWARD,
                output_dst=output_dst,
                other_scalar_source=write_scalar_source,
            )
class ActivationReconstituter(Reconstituter):
    """Reconstitute MLP, autoencoder, or attention post-softmax activations."""
    requires_other_scalar_source = False
    def __init__(
        self,
        transformer: Transformer,
        autoencoder_context: AutoencoderContext | None,
        node_type: NodeType,
        pre_or_post_act: PreOrPostAct,
        detach_layer_norm_scale: bool,
        attention_trace_type: AttentionTraceType | None = None,
    ):
        super().__init__()
        self._reconstitute_activations_fn = make_reconstituted_activation_fn(
            transformer=transformer,
            autoencoder_context=autoencoder_context,
            node_type=node_type,
            pre_or_post_act=pre_or_post_act,
            detach_layer_norm_scale=detach_layer_norm_scale,
            attention_trace_type=attention_trace_type,
        )
        self._node_type = node_type
        self._pre_or_post_act = pre_or_post_act
        self._attention_trace_type = attention_trace_type
        self.residual_dst = get_previous_residual_dst_for_node_type(
            node_type=node_type,
            autoencoder_dst=autoencoder_context.dst if autoencoder_context is not None else None,
        )
    @classmethod
    def from_trace_config(
        cls,
        transformer: Transformer,
        autoencoder_context: AutoencoderContext | None,
        trace_config: TraceConfig,
    ) -> "ActivationReconstituter":
        return cls(
            transformer=transformer,
            autoencoder_context=autoencoder_context,
            node_type=trace_config.node_type,
            pre_or_post_act=trace_config.pre_or_post_act,
            attention_trace_type=trace_config.attention_trace_type,
            detach_layer_norm_scale=trace_config.detach_layer_norm_scale,
        )
    @classmethod
    def from_activation_location_type(
        cls,
        transformer: Transformer,
        autoencoder_context: AutoencoderContext | None,
        activation_location_type: ActivationLocationType,
        q_or_k: ActivationLocationType | None,
    ) -> "ActivationReconstituter":
        match activation_location_type:
            case ActivationLocationType.MLP_PRE_ACT:
                node_type = NodeType.MLP_NEURON
                pre_or_post_act = PreOrPostAct.PRE
            case ActivationLocationType.MLP_POST_ACT:
                node_type = NodeType.MLP_NEURON
                pre_or_post_act = PreOrPostAct.POST
            case ActivationLocationType.ATTN_QK_LOGITS:
                node_type = NodeType.ATTENTION_HEAD
                pre_or_post_act = PreOrPostAct.PRE
            case ActivationLocationType.ATTN_QK_PROBS:
                node_type = NodeType.ATTENTION_HEAD
                pre_or_post_act = PreOrPostAct.POST
            case ActivationLocationType.ONLINE_AUTOENCODER_LATENT:
                node_type = NodeType.AUTOENCODER_LATENT
                pre_or_post_act = PreOrPostAct.POST
            case ActivationLocationType.ONLINE_MLP_AUTOENCODER_LATENT:
                node_type = NodeType.MLP_AUTOENCODER_LATENT
                pre_or_post_act = PreOrPostAct.POST
            case ActivationLocationType.ONLINE_ATTENTION_AUTOENCODER_LATENT:
                node_type = NodeType.ATTENTION_AUTOENCODER_LATENT
                pre_or_post_act = PreOrPostAct.POST
            case _:
                raise ValueError(
                    f"Unsupported activation_location_type: {activation_location_type}"
                )
        if node_type == NodeType.ATTENTION_HEAD:
            match q_or_k:
                case ActivationLocationType.ATTN_QUERY:
                    attention_trace_type = AttentionTraceType.Q
                case ActivationLocationType.ATTN_KEY:
                    attention_trace_type = AttentionTraceType.K
                case None:
                    attention_trace_type = AttentionTraceType.QK
                case _:
                    raise ValueError(f"Unsupported q_or_k: {q_or_k}")
        else:
            attention_trace_type = None
        return cls(
            transformer=transformer,
            autoencoder_context=autoencoder_context,
            node_type=node_type,
            pre_or_post_act=pre_or_post_act,
            attention_trace_type=attention_trace_type,
            detach_layer_norm_scale=DETACH_LAYER_NORM_SCALE,
        )
    def reconstitute_activations(
        self,
        resid: torch.Tensor,
        other_arg: torch.Tensor | None,
        layer_index: LayerIndex,
        pass_type: PassType,
    ) -> torch.Tensor:
        assert pass_type == PassType.FORWARD
        assert other_arg is None
        return self._reconstitute_activations_fn(
            resid,
            layer_index,
            pass_type,
        )
    def make_scalar_hook_for_trace_config(
        self,
        trace_config: TraceConfig,
    ) -> Callable[[torch.Tensor], torch.Tensor]:
        assert trace_config.node_type == self._node_type
        assert trace_config.pre_or_post_act == self._pre_or_post_act
        assert trace_config.attention_trace_type == self._attention_trace_type
        assert trace_config.pass_type == PassType.FORWARD
        assert trace_config.layer_index is not None
        assert trace_config.ndim == 0
        def get_activation_from_layer_activations(layer_activations: torch.Tensor) -> torch.Tensor:
            assert all(
                isinstance(index, int) for index in trace_config.tensor_indices
            ), "All indices in trace_config.tensor_indices must be integers."
            return layer_activations[trace_config.tensor_indices]  # type: ignore
        return get_activation_from_layer_activations
    def make_gradient_scalar_deriver_for_trace_config(
        self,
        trace_config: TraceConfig,
        dst_config: DstConfig,
        output_dst: DerivedScalarType | None = None,
    ) -> ScalarDeriver:
        scalar_hook = self.make_scalar_hook_for_trace_config(trace_config)
        assert trace_config.layer_index is not None
        dst_config_for_layer = dataclasses.replace(
            dst_config,
            layer_indices=[trace_config.layer_index],
        )
        return self.make_gradient_scalar_deriver(
            scalar_hook=scalar_hook,
            dst_config=dst_config_for_layer,
            output_dst=output_dst,
        )
    def make_gradient_scalar_source_for_trace_config(
        self,
        trace_config: TraceConfig,
        dst_config: DstConfig,
        output_dst: DerivedScalarType | None = None,
    ) -> DerivedScalarSource:
        assert trace_config.layer_index is not None
        gradient_scalar_deriver = self.make_gradient_scalar_deriver_for_trace_config(
            trace_config=trace_config,
            dst_config=dst_config,
            output_dst=output_dst,
        )
        return DerivedScalarSource(
            scalar_deriver=gradient_scalar_deriver,
            pass_type=PassType.FORWARD,
            layer_indexer=ConstantLayerIndexer(trace_config.layer_index),
        )
    def make_reconstitute_gradient_fn_for_trace_config(
        self,
        trace_config: TraceConfig,
    ) -> Callable[[torch.Tensor, LayerIndex, PassType], torch.Tensor]:
        scalar_hook = self.make_scalar_hook_for_trace_config(trace_config)
        def reconstitute_gradient(
            resid: torch.Tensor, layer_index: LayerIndex, pass_type: PassType
        ) -> torch.Tensor:
            assert pass_type == PassType.FORWARD
            assert layer_index == trace_config.layer_index
            return self.reconstitute_gradient(
                resid,
                None,
                layer_index=layer_index,
                pass_type=pass_type,
                scalar_hook=scalar_hook,
            )
        return reconstitute_gradient
    def make_reconstitute_activation_fn_for_trace_config(
        self,
        trace_config: TraceConfig,
    ) -> Callable[[torch.Tensor], torch.Tensor]:
        scalar_hook = self.make_scalar_hook_for_trace_config(trace_config)
        def reconstitute_activation(
            resid: torch.Tensor,
        ) -> torch.Tensor:
            return scalar_hook(
                self.reconstitute_activations(
                    resid=resid,
                    other_arg=None,
                    layer_index=trace_config.layer_index,
                    pass_type=trace_config.pass_type,
                )
            )
        return reconstitute_activation
def make_no_backward_pass_scalar_source_for_final_residual_grad(
    dst_config: DstConfig,
) -> DerivedScalarSource:
    """This ScalarDeriver takes a residual stream as input and reconstitutes the gradient of the
    scalar valued function specified by dst_config.trace_config, with respect to the
    residual stream."""
    trace_config = dst_config.trace_config
    assert trace_config is not None
    transformer = dst_config.get_or_create_model()
    autoencoder_context = dst_config.get_autoencoder_context()
    autoencoder_dst = autoencoder_context.dst if autoencoder_context is not None else None
    assert trace_config.layer_index is not None
    dst_config_for_layer = dataclasses.replace(
        dst_config,
        layer_indices=[trace_config.layer_index],
    )
    reconstituter = ActivationReconstituter.from_trace_config(
        transformer=transformer,
        autoencoder_context=autoencoder_context,
        trace_config=trace_config,
    )
    return reconstituter.make_gradient_scalar_source_for_trace_config(
        trace_config=trace_config,
        dst_config=dst_config_for_layer,
    )
def make_reconstituted_gradient_fn(
    transformer: Transformer,
    autoencoder_context: AutoencoderContext | None,
    trace_config: TraceConfig,
) -> Callable[[torch.Tensor, LayerIndex, PassType], torch.Tensor]:
    """
    Define a function which takes in a 2-d residual stream tensor (along with layer index and pass
    type) and returns the gradient with respect to trace_config at those residual stream values.
    This function is asserted to be applied at the layer index and pass type specified by
    trace_config.
    """
    assert trace_config.layer_index is not None
    reconstituter = ActivationReconstituter.from_trace_config(
        transformer=transformer,
        autoencoder_context=autoencoder_context,
        trace_config=trace_config,
    )
    return reconstituter.make_reconstitute_gradient_fn_for_trace_config(
        trace_config=trace_config,
    )
class WriteLatentReconstituter(Reconstituter):
    """Reconstitute autoencoder latents from RESID_DELTA_ATTN or RESID_DELTA_MLP."""
    requires_other_scalar_source = False
    def __init__(
        self,
        autoencoder_context: AutoencoderContext,
    ):
        super().__init__()
        self._reconstitute_activations_fn = make_apply_autoencoder(
            autoencoder_context=autoencoder_context,
            use_no_grad=False,
        )
        self.residual_dst = autoencoder_context.dst
        assert self.residual_dst in {
            DerivedScalarType.RESID_DELTA_ATTN,
            DerivedScalarType.RESID_DELTA_MLP,
        }
        assert self.residual_dst.node_type == NodeType.RESIDUAL_STREAM_CHANNEL
    def reconstitute_activations(
        self,
        resid: torch.Tensor,
        other_arg: torch.Tensor | None,
        layer_index: LayerIndex,
        pass_type: PassType,
    ) -> torch.Tensor:
        assert pass_type == PassType.FORWARD
        assert other_arg is None
        return self._reconstitute_activations_fn(
            resid,
            layer_index,
        )
    def make_scalar_hook_for_latent_index(
        self, latent_index: NodeIndex
    ) -> Callable[[torch.Tensor], torch.Tensor]:
        assert latent_index.pass_type == PassType.FORWARD
        assert latent_index.layer_index is not None
        assert latent_index.ndim == 1
        assert latent_index.tensor_indices[0] is None
        assert isinstance(latent_index.tensor_indices[1], int)
        latent_index_for_grad = latent_index.tensor_indices[1]
        def get_activation_from_layer_activations(layer_activations: torch.Tensor) -> torch.Tensor:
            return layer_activations[:, latent_index_for_grad].sum(dim=0)  # sum over tokens
        return get_activation_from_layer_activations
    def make_gradient_scalar_deriver_for_latent_index(
        self,
        latent_index: NodeIndex,
        dst_config: DstConfig,
        output_dst: DerivedScalarType | None = None,
    ) -> ScalarDeriver:
        scalar_hook = self.make_scalar_hook_for_latent_index(latent_index)
        return self.make_gradient_scalar_deriver(
            scalar_hook=scalar_hook,
            dst_config=dst_config,
            output_dst=output_dst,
        )
    def make_gradient_scalar_source_for_latent_index(
        self,
        latent_index: NodeIndex,
        dst_config: DstConfig,
        output_dst: DerivedScalarType | None = None,
    ) -> DerivedScalarSource:
        gradient_scalar_deriver = self.make_gradient_scalar_deriver_for_latent_index(
            latent_index=latent_index,
            dst_config=dst_config,
            output_dst=output_dst,
        )
        assert latent_index.layer_index is not None
        return DerivedScalarSource(
            scalar_deriver=gradient_scalar_deriver,
            pass_type=PassType.FORWARD,
            layer_indexer=ConstantLayerIndexer(latent_index.layer_index),
        )

================
File: neuron_explainer/activations/derived_scalars/residual.py
================
"""This file contains code to compute derived scalars related to activations in the residual stream basis."""
from typing import Callable
import torch
from neuron_explainer.activations.derived_scalars.derived_scalar_types import DerivedScalarType
from neuron_explainer.activations.derived_scalars.direct_effects import (
    convert_scalar_deriver_to_write_to_final_residual_grad,
)
from neuron_explainer.activations.derived_scalars.indexing import DETACH_LAYER_NORM_SCALE
from neuron_explainer.activations.derived_scalars.locations import (
    NoLayersLayerIndexer,
    OffsetLayerIndexer,
)
from neuron_explainer.activations.derived_scalars.raw_activations import (
    get_scalar_sources_for_activation_location_types,
    make_scalar_deriver_factory_for_act_times_grad,
    make_scalar_deriver_factory_for_activation_location_type,
)
from neuron_explainer.activations.derived_scalars.reconstituted import (
    apply_autoencoder_pre_latent,
    apply_mlp_pre_act,
)
from neuron_explainer.activations.derived_scalars.scalar_deriver import (
    DstConfig,
    PassType,
    RawScalarSource,
    ScalarDeriver,
)
from neuron_explainer.models.model_component_registry import (
    ActivationLocationType,
    Dimension,
    LayerIndex,
    NodeType,
)
from neuron_explainer.models.model_context import StandardModelContext, get_embedding
_residual_norm_dst_by_activation_location_type: dict[ActivationLocationType, DerivedScalarType] = {
    ActivationLocationType.RESID_POST_EMBEDDING: DerivedScalarType.RESID_POST_EMBEDDING_NORM,
    ActivationLocationType.RESID_POST_MLP: DerivedScalarType.RESID_POST_MLP_NORM,
    ActivationLocationType.RESID_DELTA_MLP: DerivedScalarType.MLP_LAYER_WRITE_NORM,
    ActivationLocationType.RESID_POST_ATTN: DerivedScalarType.RESID_POST_ATTN_NORM,
    ActivationLocationType.RESID_DELTA_ATTN: DerivedScalarType.ATTN_LAYER_WRITE_NORM,
}
def make_previous_layer_resid_post_mlp_scalar_deriver(
    dst_config: DstConfig,
) -> ScalarDeriver:
    """Several DSTs require use of the residual stream activations immediately preceding some particular set of activations.
    If the activations are MLP activations, this is simple: just the resid.post_attn activations in the same layer. If the
    activations are attention activations, this is a bit more complicated: resid.post_mlp in the previous layer if layer_index > 0,
    or resid.post_emb (which has no layers) if layer_index == 0.
    As a result, this gives the residual stream activations just prior to attention with layer_index k at entry k of
    activations_by_layer_index.
    This ScalarDeriver is for the latter case."""
    def get_scalar_sources(pass_type: PassType) -> tuple[RawScalarSource, RawScalarSource]:
        return (
            RawScalarSource(
                activation_location_type=ActivationLocationType.RESID_POST_MLP,
                pass_type=pass_type,
                layer_indexer=OffsetLayerIndexer(-1),
            ),  # resid.post_mlp
            RawScalarSource(
                activation_location_type=ActivationLocationType.RESID_POST_EMBEDDING,
                pass_type=pass_type,
                layer_indexer=NoLayersLayerIndexer(),
            ),  # resid.post_emb
        )
    if dst_config.derive_gradients:
        sub_scalar_sources: tuple[RawScalarSource, ...] = get_scalar_sources(
            PassType.FORWARD
        ) + get_scalar_sources(PassType.BACKWARD)
    else:
        sub_scalar_sources = get_scalar_sources(PassType.FORWARD)
    def tensor_calculate_derived_scalar_fn(
        raw_activation_data_tuple: tuple[torch.Tensor, ...],
        layer_index: LayerIndex,
        pass_type: PassType,
    ) -> torch.Tensor:
        if len(raw_activation_data_tuple) == 2:
            assert pass_type == PassType.FORWARD
            resid_post_mlp, resid_post_emb = raw_activation_data_tuple
        else:
            assert len(raw_activation_data_tuple) == 4
            _, _, resid_post_mlp, resid_post_emb = raw_activation_data_tuple
        assert layer_index is not None
        if layer_index == 0:
            return resid_post_emb
        else:
            assert layer_index > 0
            return resid_post_mlp
    return ScalarDeriver(
        dst=DerivedScalarType.PREVIOUS_LAYER_RESID_POST_MLP,
        dst_config=dst_config,
        sub_scalar_sources=sub_scalar_sources,
        tensor_calculate_derived_scalar_fn=tensor_calculate_derived_scalar_fn,
    )
def make_residual_norm_scalar_deriver_factory_for_activation_location_type(
    activation_location_type: ActivationLocationType,
) -> Callable[[DstConfig], ScalarDeriver]:
    """this is for DerivedScalarType's 1:1 with a ActivationLocationType, which can be generated from just the ActivationLocationType
    and no additional information"""
    dst = _residual_norm_dst_by_activation_location_type[activation_location_type]
    assert activation_location_type.shape_spec_per_token_sequence == (
        Dimension.SEQUENCE_TOKENS,
        Dimension.RESIDUAL_STREAM_CHANNELS,
    ), f"Residual norm only defined for residual stream activations, not {activation_location_type=} with shape {activation_location_type.shape_spec_per_token_sequence=}"
    def make_scalar_deriver_fn(
        dst_config: DstConfig,
    ) -> ScalarDeriver:
        sub_scalar_sources = get_scalar_sources_for_activation_location_types(
            activation_location_type, dst_config.derive_gradients
        )
        def compute_norm(
            raw_activation_data_tuple: tuple[torch.Tensor, ...],
            layer_index: LayerIndex,
            pass_type: PassType,
        ) -> torch.Tensor:
            assert len(raw_activation_data_tuple) == 1
            raw_activation_data = raw_activation_data_tuple[0]
            return raw_activation_data.norm(dim=-1)[:, None]  # singleton final dimension
        return ScalarDeriver(
            dst=dst,
            dst_config=dst_config,
            sub_scalar_sources=sub_scalar_sources,
            tensor_calculate_derived_scalar_fn=compute_norm,
        )
    return make_scalar_deriver_fn
def make_token_attribution_scalar_deriver(dst_config: DstConfig) -> ScalarDeriver:
    """This computes an attribution value for each token in the sequence, the inner product of act and grad for the embedding.
    This corresponds to the estimated importance of this token to the final prediction."""
    activation_location_type = ActivationLocationType.RESID_POST_EMBEDDING
    dst = DerivedScalarType.TOKEN_ATTRIBUTION
    act_times_grad_scalar_deriver = make_scalar_deriver_factory_for_act_times_grad(
        activation_location_type=activation_location_type,
        dst=dst,
    )(dst_config)
    def sum_over_last_dim(tensor: torch.Tensor) -> torch.Tensor:
        return tensor.sum(dim=-1)[:, None]  # singleton final dim
    return act_times_grad_scalar_deriver.apply_transform_fn_to_output(
        sum_over_last_dim, pass_type_to_transform=PassType.FORWARD, output_dst=dst
    )
_residual_projection_to_final_residual_grad_dst_by_activation_location_type: dict[
    ActivationLocationType, DerivedScalarType
] = {
    ActivationLocationType.RESID_POST_EMBEDDING: DerivedScalarType.RESID_POST_EMBEDDING_PROJ_TO_FINAL_RESIDUAL_GRAD,
    ActivationLocationType.RESID_POST_MLP: DerivedScalarType.RESID_POST_MLP_PROJ_TO_FINAL_RESIDUAL_GRAD,
    ActivationLocationType.RESID_DELTA_MLP: DerivedScalarType.MLP_LAYER_WRITE_TO_FINAL_RESIDUAL_GRAD,
    ActivationLocationType.RESID_POST_ATTN: DerivedScalarType.RESID_POST_ATTN_PROJ_TO_FINAL_RESIDUAL_GRAD,
    ActivationLocationType.RESID_DELTA_ATTN: DerivedScalarType.ATTN_LAYER_WRITE_TO_FINAL_RESIDUAL_GRAD,
}
_residual_projection_to_final_activation_residual_grad_dst_by_activation_location_type: dict[
    ActivationLocationType, DerivedScalarType
] = {
    ActivationLocationType.RESID_POST_EMBEDDING: DerivedScalarType.RESID_POST_EMBEDDING_PROJ_TO_FINAL_ACTIVATION_RESIDUAL_GRAD,
}
def make_residual_projection_to_final_residual_grad_scalar_deriver_factory_for_activation_location_type(
    activation_location_type: ActivationLocationType,
    use_existing_backward_pass_for_final_residual_grad: bool,
) -> Callable[[DstConfig], ScalarDeriver]:
    """this is for DerivedScalarType's 1:1 with a ActivationLocationType, which can be generated from just the ActivationLocationType
    and no additional information"""
    if use_existing_backward_pass_for_final_residual_grad:
        dst = _residual_projection_to_final_residual_grad_dst_by_activation_location_type[
            activation_location_type
        ]
    else:
        dst = (
            _residual_projection_to_final_activation_residual_grad_dst_by_activation_location_type[
                activation_location_type
            ]
        )
    assert activation_location_type.shape_spec_per_token_sequence == (
        Dimension.SEQUENCE_TOKENS,
        Dimension.RESIDUAL_STREAM_CHANNELS,
    ), f"Residual norm only defined for residual stream activations, not {activation_location_type=} with shape {activation_location_type.shape_spec_per_token_sequence=}"
    def make_scalar_deriver_fn(
        dst_config: DstConfig,
    ) -> ScalarDeriver:
        residual_scalar_deriver = make_scalar_deriver_factory_for_activation_location_type(
            activation_location_type
        )(dst_config)
        return convert_scalar_deriver_to_write_to_final_residual_grad(
            scalar_deriver=residual_scalar_deriver,
            use_existing_backward_pass_for_final_residual_grad=use_existing_backward_pass_for_final_residual_grad,
            output_dst=dst,
        )
    return make_scalar_deriver_fn
def make_vocab_token_write_to_input_direction_scalar_deriver(
    dst_config: DstConfig,
) -> ScalarDeriver:
    """
    This computes the inner product between each token's embedding vector, and the input
    weight for a particular autoencoder latent or MLP neuron. The MLP neuron case
    straightforwardly applies the logit-lens setup: Embedding * InputWeightWithLayerNormGain.
    For the MLP autoencoder case (so far only implemented for MLP latents), we first convert
    to the MLP neuron basis using MlpLayer(Embedding), then multiply by InputWeight (there is
    no associated layer norm gain). MlpLayer(Embedding) * InputWeight corresponds to the
    autoencoder latent pre-activation in the case where the residual stream vector were exactly
    the token embedding vector.
    """
    dst = DerivedScalarType.VOCAB_TOKEN_WRITE_TO_INPUT_DIRECTION
    activation_index = dst_config.activation_index_for_fake_grad
    assert activation_index is not None
    layer_index = activation_index.layer_index
    assert layer_index is not None
    activation_location_type = activation_index.activation_location_type
    neuron_index = activation_index.tensor_indices[1]
    assert isinstance(neuron_index, int)
    model_context = dst_config.get_model_context()
    autoencoder_context = dst_config.get_autoencoder_context(NodeType.MLP_AUTOENCODER_LATENT)
    assert isinstance(model_context, StandardModelContext)
    transformer = model_context.get_or_create_model()
    transformer_layer = transformer.xf_layers[layer_index]
    assert activation_location_type in (
        ActivationLocationType.MLP_POST_ACT,
        ActivationLocationType.ONLINE_AUTOENCODER_LATENT,
        ActivationLocationType.ONLINE_MLP_AUTOENCODER_LATENT,
    ), f"Virtual input weight not defined for {activation_location_type=}"
    emb = get_embedding(model_context)  # resulting shape: n_vocab, d_model
    match activation_location_type:
        # only fwd pass is used, so detaching layer norm scale is not relevant
        case ActivationLocationType.MLP_POST_ACT:
            token_vector = apply_mlp_pre_act(
                transformer_layer, emb, detach_layer_norm_scale=DETACH_LAYER_NORM_SCALE
            )[:, neuron_index]
            # ^ resulting shape: n_vocab,
        case (
            ActivationLocationType.ONLINE_MLP_AUTOENCODER_LATENT
            | ActivationLocationType.ONLINE_AUTOENCODER_LATENT
        ):
            assert autoencoder_context is not None
            autoencoder = autoencoder_context.get_autoencoder(layer_index)
            # directly applies MLP layer + autoencoder to all token embedding
            # vectors
            token_vector = apply_autoencoder_pre_latent(
                transformer_layer,
                autoencoder,
                emb,
                autoencoder_dst=autoencoder_context.dst,
                detach_layer_norm_scale=DETACH_LAYER_NORM_SCALE,
                # index the latent to avoid creating a matrix of shape (n_vocab, n_latents)
                latent_slice=slice(neuron_index, neuron_index + 1),
            )[:, 0]
            # indexed by zero because we already sliced to a single neuron
            # resulting shape: n_vocab,
        case _:
            raise NotImplementedError(activation_location_type)
    def replace_with_token_vector(
        resid_post_emb: torch.Tensor,
    ) -> torch.Tensor:
        # resid_post_emb only used for shape
        return token_vector.unsqueeze(0).expand(  # resulting shape: 1, n_vocab
            resid_post_emb.shape[0], -1
        )  # resulting shape: n_sequence_tokens, n_vocab
    resid_post_emb_scalar_deriver = make_scalar_deriver_factory_for_activation_location_type(
        ActivationLocationType.RESID_POST_EMBEDDING
    )(dst_config)
    return resid_post_emb_scalar_deriver.apply_transform_fn_to_output(
        replace_with_token_vector, pass_type_to_transform=PassType.FORWARD, output_dst=dst
    )
def make_unity_scalar_deriver(
    dst_config: DstConfig,
) -> ScalarDeriver:
    resid_post_emb_scalar_deriver = make_scalar_deriver_factory_for_activation_location_type(
        ActivationLocationType.RESID_POST_EMBEDDING
    )(dst_config)
    dst = DerivedScalarType.ALWAYS_ONE
    def convert_to_unity(tensor: torch.Tensor) -> torch.Tensor:
        return torch.ones_like(tensor[:, 0:1])  # (n_sequence_tokens, 1)
    return resid_post_emb_scalar_deriver.apply_transform_fn_to_output(
        convert_to_unity, pass_type_to_transform=PassType.FORWARD, output_dst=dst
    )

================
File: neuron_explainer/activations/derived_scalars/scalar_deriver.py
================
"""This file contains the primary code for the ScalarDeriver class."""
import dataclasses
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import Any, Callable
import torch
from neuron_explainer.activations.derived_scalars.activations_and_metadata import (
    ActivationsAndMetadata,
    RawActivationStore,
)
from neuron_explainer.activations.derived_scalars.config import DstConfig
from neuron_explainer.activations.derived_scalars.derived_scalar_types import DerivedScalarType
from neuron_explainer.activations.derived_scalars.locations import (
    DEFAULT_LAYER_INDEXER,
    LayerIndexer,
    NoLayersLayerIndexer,
    StaticLayerIndexer,
    get_location_within_layer_for_dst,
)
from neuron_explainer.models.model_component_registry import (
    ActivationLocationType,
    ActivationLocationTypeAndPassType,
    Dimension,
    LayerIndex,
    LocationWithinLayer,
    PassType,
)
### SHARED CODE FOR DERIVING SCALARS FROM ACTIVATIONS ###
@dataclass(frozen=True)
class DerivedScalarTypeAndPassType:
    dst: DerivedScalarType
    pass_type: PassType
class ScalarSource(ABC):
    pass_type: PassType
    layer_indexer: LayerIndexer
    @property
    @abstractmethod
    def exists_by_default(self) -> bool:
        # returns True if the activation is instantiated by default in a normal transformer forward pass
        # this is False for activations related to autoencoders or for non-trivial derived scalars
        pass
    @property
    @abstractmethod
    def dst(self) -> DerivedScalarType:
        pass
    @property
    def dst_and_pass_type(self) -> "DerivedScalarTypeAndPassType":
        return DerivedScalarTypeAndPassType(
            self.dst,
            self.pass_type,
        )
    @property
    @abstractmethod
    def sub_activation_location_type_and_pass_types(
        self,
    ) -> tuple[ActivationLocationTypeAndPassType, ...]:
        pass
    @property
    @abstractmethod
    def location_within_layer(self) -> LocationWithinLayer | None:
        pass
    @property
    def layer_index(self) -> LayerIndex:
        """Convenience method to get the single layer index associated with this ScalarSource, if such a single layer index
        exists. Throws an error if it does not."""
        assert isinstance(self.layer_indexer, StaticLayerIndexer), (
            self.layer_indexer,
            "ScalarSource.layer_index should only be called for ScalarSource StaticLayerIndexer",
        )
        return self.layer_indexer.layer_index
    @abstractmethod
    def derive_from_raw(
        self,
        raw_activation_store: RawActivationStore,
        desired_layer_indices: (
            list[LayerIndex] | None
        ),  # indicates layer indices to keep; None indicates keep all
    ) -> ActivationsAndMetadata:
        """Given raw activations, derive the scalar value. desired_layer_indices is a list of layer indices to include in the output; None indicates all layers,
        while [None] indicates activations not indexed by layers (e.g. from the embedding)."""
        pass
# note that this class, inheriting from ActivationLocationTypeAndPassType, becomes a
# base class. This needs to be a separate object from ActivationLocationTypeAndPassType,
# and located within this file, because ScalarSource needs to know about DerivedScalarTypes,
# which are defined within the derived_scalars/ directory
class RawScalarSource(ActivationLocationTypeAndPassType, ScalarSource):
    def __init__(
        self,
        activation_location_type: ActivationLocationType,
        pass_type: PassType,
        layer_indexer: LayerIndexer = DEFAULT_LAYER_INDEXER,
    ) -> None:
        super().__init__(activation_location_type, pass_type)
        self.layer_indexer = layer_indexer
        if activation_location_type.has_no_layers:
            assert isinstance(layer_indexer, NoLayersLayerIndexer), self
    @property
    def dst(self) -> DerivedScalarType:
        return DerivedScalarType.from_activation_location_type(self.activation_location_type)
    @property
    def sub_activation_location_type_and_pass_types(
        self,
    ) -> tuple[ActivationLocationTypeAndPassType, ...]:
        return (self.activation_location_type_and_pass_type,)
    @property
    def exists_by_default(self) -> bool:
        return self.activation_location_type.exists_by_default
    @property
    def location_within_layer(self) -> LocationWithinLayer | None:
        return self.activation_location_type.location_within_layer
    @property
    def activation_location_type_and_pass_type(self) -> ActivationLocationTypeAndPassType:
        return ActivationLocationTypeAndPassType(self.activation_location_type, self.pass_type)
    def derive_from_raw(
        self,
        raw_activation_store: RawActivationStore,
        desired_layer_indices: (
            list[LayerIndex] | None
        ),  # indicates layer indices to keep; None indicates keep all
    ) -> ActivationsAndMetadata:
        return raw_activation_store.get_activations_and_metadata(
            self.activation_location_type,
            self.pass_type,
        ).apply_layer_indexer(self.layer_indexer, desired_layer_indices)
class DerivedScalarSource(ScalarSource):
    scalar_deriver: "ScalarDeriver"
    def __init__(
        self,
        scalar_deriver: "ScalarDeriver",
        pass_type: PassType,
        layer_indexer: LayerIndexer = DEFAULT_LAYER_INDEXER,
    ) -> None:
        self.scalar_deriver = scalar_deriver
        self.pass_type = pass_type
        self.layer_indexer = layer_indexer
    @property
    def exists_by_default(self) -> bool:
        return False
    @property
    def dst(self) -> DerivedScalarType:
        return self.scalar_deriver.dst
    @property
    def sub_activation_location_type_and_pass_types(
        self,
    ) -> tuple[ActivationLocationTypeAndPassType, ...]:
        return self.scalar_deriver.get_sub_activation_location_type_and_pass_types()
    @property
    def location_within_layer(self) -> LocationWithinLayer | None:
        return self.scalar_deriver.location_within_layer
    def derive_from_raw(
        self,
        raw_activation_store: RawActivationStore,
        desired_layer_indices: (
            list[LayerIndex] | None
        ),  # indicates layer indices to keep; None indicates keep all
    ) -> ActivationsAndMetadata:
        return self.scalar_deriver.derive_from_raw(
            raw_activation_store, self.pass_type
        ).apply_layer_indexer(self.layer_indexer, desired_layer_indices=desired_layer_indices)
@dataclass(frozen=True)
class ScalarDeriver:
    """Contains the information necessary for specifying some function of one or more activations,
    (this function can be as simple as the identity function). This includes: what activations are required
    to compute it; a function that takes in ActivationsAndMetadata for each of those activations and
    returns a ActivationsAndMetadata for the derived scalar; and a function that
    returns the shape you expect the derived scalar to have for each token (e.g. one float per attention head,
    one float per layer, etc.).
    The function for computing this derived scalar on the forward pass can be different from the function for computing
    its gradient on the backward pass, so the pass type must also be an argument to the function that computes the scalar.
    A HookLocationType describes the type of activation that is saved during inference, and a ScalarDeriver describes the
    type of "derived" scalar computed from those activations after they are read from disk. In the simplest case, a
    derived scalar can be computed directly from the saved activations with an identity transformation (e.g. a single MLP
    activation is saved during inference, and the derived scalar is the same MLP activation)."""
    dst: DerivedScalarType
    """
    Dataclass with fields needed to construct a ScalarDeriver for this DerivedScalarType; e.g. derived scalars
    computed using model weights will require at minimum the model_name to load the weights.
    """
    dst_config: DstConfig
    """
    Contains ActivationLocationTypes or other ScalarDerivers, and corresponding pass directions (forward or backward) that are
    required to compute this derived scalar type. These are loaded from disk and passed to the
    tensor_calculate_derived_scalar_fn as a single tuple argument.
    """
    sub_scalar_sources: tuple[ScalarSource, ...]
    """
    A function that takes a tuple of tensors, a layer index, and a pass type, and returns
    a tensor containing the derived scalar values. layer_index can be None in case of activation
    location types that don't have layer indices, like embeddings.
    """
    tensor_calculate_derived_scalar_fn: Callable[
        [tuple[torch.Tensor, ...], LayerIndex, PassType], torch.Tensor
    ]
    """In cases where a ScalarDeriver is a transform applied to another scalar deriver, the location within a layer associated
    with the resulting scalar deriver is taken to be the same as the location within a layer associated with the original scalar deriver.
    See definition of LocationWithinLayer in model_component_registry.py for more details."""
    _specified_location_within_layer: LocationWithinLayer | None = None
    @property
    def device_for_raw_activations(self) -> torch.device:
        """Which device to read raw activations onto."""
        return self.dst_config.get_device()
    @property
    def shape_of_activation_per_token_spec(self) -> tuple[Dimension, ...]:
        # first dimension is num_sequence_tokens; this can be either the literal number of tokens in a sequence or
        # the number of token pairs in a sequence
        return self.dst.shape_spec_per_token_sequence[1:]
    @property
    def location_within_layer(self) -> LocationWithinLayer | None:
        """An activation location type at a topologically equivalent point in the network, in terms of which
        residual stream locations precede and follow it."""
        specified_location_within_layer = self._specified_location_within_layer
        dst_location_within_layer = get_location_within_layer_for_dst(self.dst, self.dst_config)
        if specified_location_within_layer is not None and dst_location_within_layer is not None:
            assert specified_location_within_layer == dst_location_within_layer
        consensus_location_within_layer = (
            specified_location_within_layer or dst_location_within_layer
        )
        return consensus_location_within_layer
    def _check_dst_and_pass_types(
        self, activation_data_tuple: tuple[ActivationsAndMetadata, ...]
    ) -> None:
        """Check that the derived scalar types and pass types of the raw activations match
        the order of the dsts and pass types in self.get_sub_dst_and_pass_types().
        """
        assert len(activation_data_tuple) == len(self.get_sub_dst_and_pass_types()), (
            [activation_data.dst for activation_data in activation_data_tuple],
            [
                sub_dst_and_pass_type.dst
                for sub_dst_and_pass_type in self.get_sub_dst_and_pass_types()
            ],
        )
        for activation_data, sub_dst_and_pass_type in zip(
            activation_data_tuple, self.get_sub_dst_and_pass_types()
        ):
            assert (
                activation_data.dst == sub_dst_and_pass_type.dst
            ), f"{activation_data.dst=}, {sub_dst_and_pass_type.dst=}"
            assert activation_data.pass_type == sub_dst_and_pass_type.pass_type, (
                f"{self.dst=}, "
                f"{activation_data.dst=}, "
                f"{activation_data.pass_type=}, {sub_dst_and_pass_type.pass_type=}"
            )
            assert activation_data.pass_type == sub_dst_and_pass_type.pass_type
        return
    def activations_and_metadata_calculate_derived_scalar_fn(
        self, activation_data_tuple: tuple[ActivationsAndMetadata, ...], pass_type: PassType
    ) -> ActivationsAndMetadata:
        self._check_dst_and_pass_types(activation_data_tuple)
        for activation_data in activation_data_tuple:
            assert len(activation_data.activations_by_layer_index) > 0, (
                f"{activation_data.activations_by_layer_index=}"
                f"{activation_data.dst=}"
                f"{activation_data.pass_type=}"
            )
        activation_data = activation_data_tuple[0]
        filtered_activation_data = activation_data.filter_layers(
            layer_indices=self.dst_config.layer_indices
        )
        if len(activation_data_tuple) == 1:
            def _calculate_derived_scalar_fn(
                activations: torch.Tensor,
                layer_index: LayerIndex,
            ) -> torch.Tensor:
                return self.tensor_calculate_derived_scalar_fn(
                    (activations,), layer_index, pass_type
                )
            return filtered_activation_data.apply_layerwise_transform_fn_to_activations(
                layerwise_transform_fn=_calculate_derived_scalar_fn,
                output_dst=self.dst,
                output_pass_type=pass_type,
            )
        elif len(activation_data_tuple) >= 2:
            def _calculate_multi_arg_derived_scalar_fn(
                *args: torch.Tensor,
                layer_index: LayerIndex,
            ) -> torch.Tensor:
                return self.tensor_calculate_derived_scalar_fn(tuple(args), layer_index, pass_type)
            other_filtered_activation_data_tuple = tuple(
                activation_data.filter_layers(layer_indices=self.dst_config.layer_indices)
                for activation_data in activation_data_tuple[1:]
            )
            return filtered_activation_data.apply_layerwise_transform_fn_to_multiple_activations(
                # care should be taken in a dictionary comprehension of callables that the
                # variables (i.e. layer_index) are bound at time of creation, not at time of execution
                # partial accomplishes this
                layerwise_transform_fn=_calculate_multi_arg_derived_scalar_fn,
                others=other_filtered_activation_data_tuple,
                output_dst=self.dst,
                output_pass_type=pass_type,
            )
        else:
            raise NotImplementedError(
                f"ScalarDeriver.activations_and_metadata_calculate_derived_scalar_fn not implemented for "
                f"{len(activation_data_tuple)=}"
            )
    def derive_from_raw(
        self,
        raw_activation_store: RawActivationStore,
        pass_type: PassType,
    ) -> ActivationsAndMetadata:
        desired_layer_indices = None
        sub_activations_list = []
        for sub_scalar_source in self.get_sub_scalar_sources():
            sub_activation_data = sub_scalar_source.derive_from_raw(
                raw_activation_store, desired_layer_indices=desired_layer_indices
            )
            sub_activations_list.append(sub_activation_data)
            if len(sub_activations_list) == 1:
                desired_layer_indices = list(sub_activations_list[0].layer_indices)
        return self.activations_and_metadata_calculate_derived_scalar_fn(
            tuple(sub_activations_list), pass_type
        )
    def to_serializable_dict(self) -> dict[str, Any]:
        return {
            "dst": self.dst,
            "dst_config": self.dst_config,
        }
    def get_sub_dst_and_pass_types(self) -> tuple[DerivedScalarTypeAndPassType, ...]:
        return tuple(
            sub_scalar_source.dst_and_pass_type for sub_scalar_source in self.sub_scalar_sources
        )
    def get_sub_scalar_sources(self) -> tuple[ScalarSource, ...]:
        return self.sub_scalar_sources
    def get_sub_activation_location_type_and_pass_types(
        self,
    ) -> tuple[ActivationLocationTypeAndPassType, ...]:
        sub_activation_location_type_and_pass_types_list = []
        for scalar_source in self.get_sub_scalar_sources():
            sub_activation_location_type_and_pass_types_list.extend(
                list(scalar_source.sub_activation_location_type_and_pass_types)
            )
        return tuple(sub_activation_location_type_and_pass_types_list)
    @property
    def n_input_tensors(self) -> int:
        # the number of arguments expected by the top-level function. Note that this is not necessarily
        # the same as the number of sub_activation_location_type_and_pass_types; some of these might be
        # consumed by lower-level ScalarDerivers, and combined into single tensors passed to the top-level
        # function.
        return len(self.get_sub_dst_and_pass_types())
    @property
    def n_total_required_tensors(self) -> int:
        # the number of tensors required to compute the derived scalar, including those that are
        # passed to lower-level ScalarDerivers
        return len(self.get_sub_activation_location_type_and_pass_types())
    @property
    def derivable_pass_types(self) -> tuple[PassType, ...]:
        # ScalarDerivers are configurable to support either only computing a
        # scalar on the forward pass, or computing it on both the forward and
        # the backward pass. Supporting the backward pass requires more kinds
        # of raw activations in general.
        if self.dst_config.derive_gradients:
            return (PassType.FORWARD, PassType.BACKWARD)
        else:
            return (PassType.FORWARD,)
    def apply_transform_fn_to_output(
        self,
        transform_fn: Callable[[torch.Tensor], torch.Tensor],
        pass_type_to_transform: PassType,
        output_dst: DerivedScalarType,
    ) -> "ScalarDeriver":
        """Converts one ScalarDeriver to another, by applying a tensor -> tensor function to the output.
        The tensor -> tensor function takes a tensor, a layer index, and a pass type, and returns a tensor,
        so that it can depend on layer and pass type."""
        def layerwise_transform_fn(
            tensor: torch.Tensor,
            layer_index: LayerIndex,
            pass_type: PassType,
        ) -> torch.Tensor:
            return transform_fn(tensor)
        return self.apply_layerwise_transform_fn_to_output(
            layerwise_transform_fn=layerwise_transform_fn,
            pass_type_to_transform=pass_type_to_transform,
            output_dst=output_dst,
        )
    def apply_layerwise_transform_fn_to_output(
        self,
        layerwise_transform_fn: Callable[[torch.Tensor, LayerIndex, PassType], torch.Tensor],
        pass_type_to_transform: PassType,
        output_dst: DerivedScalarType,
    ) -> "ScalarDeriver":
        """Converts one ScalarDeriver to another, by applying a tensor -> tensor function to the output.
        The tensor -> tensor function takes a tensor, a layer index, and a pass type, and returns a tensor,
        so that it can depend on layer and pass type."""
        sub_scalar_sources = (DerivedScalarSource(self, pass_type=pass_type_to_transform),)
        def tensor_calculate_derived_scalar_fn(
            activation_data_tuple: tuple[torch.Tensor, ...],
            layer_index: LayerIndex,
            pass_type: PassType,
        ) -> torch.Tensor:
            assert len(activation_data_tuple) == 1
            return layerwise_transform_fn(activation_data_tuple[0], layer_index, pass_type)
        return dataclasses.replace(
            self,
            dst=output_dst,
            sub_scalar_sources=sub_scalar_sources,
            tensor_calculate_derived_scalar_fn=tensor_calculate_derived_scalar_fn,
            _specified_location_within_layer=self.location_within_layer,
        )
    def apply_layerwise_transform_fn_to_output_and_other_tensor(
        self,
        layerwise_transform_fn: Callable[..., torch.Tensor],
        pass_type_to_transform: PassType,
        output_dst: DerivedScalarType,
        other_scalar_source: ScalarSource,
    ) -> "ScalarDeriver":
        """Converts one ScalarDeriver to another, by applying a two tensor -> tensor function to the output + an additional activation tensor.
        The tensor -> tensor function takes two tensors, a layer index, and a pass type, and returns a tensor,
        so that it can depend on layer and pass type."""
        sub_scalar_sources = (
            DerivedScalarSource(
                self, pass_type=pass_type_to_transform, layer_indexer=DEFAULT_LAYER_INDEXER
            ),
            other_scalar_source,
        )
        def tensor_calculate_derived_scalar_fn(
            activation_data_tuple: tuple[torch.Tensor, ...],
            layer_index: LayerIndex,
            pass_type: PassType,
        ) -> torch.Tensor:
            assert len(activation_data_tuple) == 2, [t.shape for t in activation_data_tuple]
            return layerwise_transform_fn(*activation_data_tuple, layer_index, pass_type)
        return dataclasses.replace(
            self,
            dst=output_dst,
            sub_scalar_sources=sub_scalar_sources,
            tensor_calculate_derived_scalar_fn=tensor_calculate_derived_scalar_fn,
            _specified_location_within_layer=self.location_within_layer,
        )

================
File: neuron_explainer/activations/derived_scalars/tests/test_attention.py
================
import pytest
import torch
from neuron_explainer.activations.derived_scalars.attention import (
    flatten_lower_triangle,
    unflatten_lower_triangle,
    unflatten_lower_triangle_and_sum_columns,
)
@pytest.mark.parametrize("extra_dim", [[], [2], [2, 3]])
@pytest.mark.parametrize("N", [63, 64, 65])
def test_unflatten_lower_triangle(extra_dim: list[int], N: int) -> None:
    """Test that unflatten_lower_triangle is the inverse of flatten_lower_triangle."""
    # Create a random tensor of shape ... x M x N
    M = 64
    original_tensor = torch.rand(extra_dim + [M, N])
    # Set all elements above the lower triangular to 0
    lower_triangular_mask = torch.tril(torch.ones(M, N)).bool()
    original_tensor[..., ~lower_triangular_mask] = 0
    # Apply flatten_lower_triangle to the original tensor
    flattened = flatten_lower_triangle(original_tensor)
    assert flattened.shape == tuple(extra_dim + [lower_triangular_mask.sum()])
    # Apply unflatten_lower_triangle to the flattened tensor
    reconstructed_tensor = unflatten_lower_triangle(flattened, M, N)
    assert torch.allclose(original_tensor, reconstructed_tensor)
@pytest.mark.parametrize("extra_dim", [[], [2], [2, 3]])
@pytest.mark.parametrize("N", [63, 64, 65])
def test_unflatten_lower_triangle_and_sum_columns(extra_dim: list[int], N: int) -> None:
    """Test unflatten_lower_triangle_and_sum_columns(...) is equal to unflatten_lower_triangle(...).sum(-1)."""
    # Create a random flattened tensor
    M = 64
    num_elements = int(torch.tril(torch.ones(M, N)).bool().sum().item())
    flattened = torch.rand(extra_dim + [num_elements])
    # apply unflatten_lower_triangle_and_sum_columns
    result = unflatten_lower_triangle_and_sum_columns(flattened, M, N)
    # apply unflatten_lower_triangle and sum(-1)
    reconstructed = unflatten_lower_triangle(flattened, M, N)
    reference = reconstructed.sum(dim=-1)
    assert torch.allclose(result, reference)

================
File: neuron_explainer/activations/derived_scalars/tests/test_derived_scalar_store.py
================
import torch
from neuron_explainer.activations.derived_scalars.derived_scalar_store import (
    DerivedScalarIndex,
    DerivedScalarStore,
    RawActivationStore,
)
from neuron_explainer.activations.derived_scalars.make_scalar_derivers import make_scalar_deriver
from neuron_explainer.activations.derived_scalars.scalar_deriver import (
    ActivationsAndMetadata,
    DerivedScalarType,
    DstConfig,
)
from neuron_explainer.models.model_component_registry import (
    ActivationLocationTypeAndPassType,
    Dimension,
    LayerIndex,
    PassType,
)
from neuron_explainer.models.model_context import ModelContext
def test_derived_scalar_store() -> None:
    activations_by_layer_index: dict[LayerIndex, torch.Tensor] = {
        5: torch.tensor([float(i) for i in range(1, 49, 2)]).view(2, 3, 4),
        6: torch.tensor([float(i) for i in range(2, 50, 2)]).view(2, 3, 4),
    }
    dst = DerivedScalarType.ATTN_QK_PROBS
    ds_store = DerivedScalarStore.from_list(
        [
            ActivationsAndMetadata(
                activations_by_layer_index=activations_by_layer_index,
                dst=dst,
                pass_type=PassType.FORWARD,
            ),
        ]
    )
    layer_5_total = 576
    assert sum(list(range(1, 49, 2))) == layer_5_total
    layer_6_total = 600
    assert sum(list(range(2, 50, 2))) == layer_6_total
    grand_total = layer_5_total + layer_6_total
    assert ds_store.sum() == float(grand_total)
    # Test __getitem__ method
    ds_index = DerivedScalarIndex(
        dst=dst, layer_index=5, tensor_indices=(0, None, None), pass_type=PassType.FORWARD
    )
    result_tensor = ds_store[ds_index]
    expected_first_layer_result: torch.Tensor = activations_by_layer_index[5][0]
    assert isinstance(result_tensor, torch.Tensor)
    assert torch.allclose(
        result_tensor, expected_first_layer_result
    ), f"DerivedScalarStore __getitem__ method failed: {result_tensor} != {expected_first_layer_result}"
    # Test topk method
    top_values, top_indices = ds_store.topk(2)
    assert torch.allclose(
        top_values, torch.tensor([48.0, 47.0])
    ), f"DerivedScalarStore topk method failed: {top_values} != {torch.tensor([48, 46])}"
    assert len(top_indices) == 2
    assert all(
        [
            top_indices[0]
            == DerivedScalarIndex(
                dst=dst, layer_index=6, tensor_indices=(1, 2, 3), pass_type=PassType.FORWARD
            ),
            top_indices[1]
            == DerivedScalarIndex(
                dst=dst, layer_index=5, tensor_indices=(1, 2, 3), pass_type=PassType.FORWARD
            ),
        ]
    )
    # Test apply_transform_fn method
    transform_fn = lambda x: x * 2
    transformed_ds_store = ds_store.apply_transform_fn_to_activations(transform_fn)
    result_tensor = transformed_ds_store[ds_index]
    assert isinstance(result_tensor, torch.Tensor)
    assert torch.allclose(
        result_tensor, activations_by_layer_index[5][0] * 2
    ), "DerivedScalarStore apply_transform_fn method failed"
def test_multi_layer_ds_store() -> None:
    full_activations_by_layer_index: dict[LayerIndex, torch.Tensor] = {
        5: torch.tensor([float(i) for i in range(1, 49, 2)]).view(2, 3, 4),
        6: torch.tensor([float(i) for i in range(2, 50, 2)]).view(2, 3, 4),
    }
    top_values: dict[tuple[int, ...], torch.Tensor] = {}
    top_indices: dict[tuple[int, ...], list[DerivedScalarIndex]] = {}
    for keys_to_include in [(5,), (5, 6), (6,)]:
        activations_by_layer_index: dict[LayerIndex, torch.Tensor] = {
            k: full_activations_by_layer_index[k] for k in keys_to_include
        }
        # define DerivedScalarStore
        ds_store = DerivedScalarStore.from_list(
            [
                ActivationsAndMetadata(
                    activations_by_layer_index=activations_by_layer_index,
                    dst=DerivedScalarType.ATTN_QK_PROBS,
                    pass_type=PassType.FORWARD,
                ),
            ]
        )
        top_values[keys_to_include], top_indices[keys_to_include] = ds_store.topk(2)
        max_value, max_index = ds_store.max()
        assert max_value == top_values[keys_to_include][0]
        assert max_index == top_indices[keys_to_include][0]
    assert torch.allclose(top_values[(5, 6)], torch.tensor([48.0, 47.0]))
    assert torch.allclose(top_values[(5,)], torch.tensor([47.0, 45.0]))
    assert torch.allclose(top_values[(6,)], torch.tensor([48.0, 46.0]))
    assert top_indices[(5, 6)][0] == top_indices[(6,)][0]
    assert top_indices[(5, 6)][1] == top_indices[(5,)][0]
def test_create_ds_store() -> None:
    model_context = ModelContext.from_model_type("gpt2-small", device=torch.device("cpu"))
    num_tokens = 5
    input_shape_by_dim = {
        Dimension.SEQUENCE_TOKENS: num_tokens,
        Dimension.ATTENDED_TO_SEQUENCE_TOKENS: num_tokens,
    }
    dsts = [
        # DerivedScalarType.ATTN_WRITE_NORM,
        DerivedScalarType.MLP_WRITE_NORM,
        # DerivedScalarType.MLP_POST_ACT,
    ]
    def get_shape_for_dim(dim: Dimension) -> int:
        if dim.is_sequence_token_dimension:
            return input_shape_by_dim[dim]
        else:
            return model_context.get_dim_size(dim)
    all_layer_indices = [5, 6]
    all_activations_by_location_type_and_pass_type: (
        dict[ActivationLocationTypeAndPassType, dict[LayerIndex, torch.Tensor]] | None
    ) = None
    def get_subset_of_layer_indices(
        all_activations_by_location_type_and_pass_type: dict[
            ActivationLocationTypeAndPassType, dict[LayerIndex, torch.Tensor]
        ],
        layer_indices: list[LayerIndex],
    ) -> dict[ActivationLocationTypeAndPassType, dict[LayerIndex, torch.Tensor]]:
        return {
            activation_location_type_and_pass_type: {
                layer_index: activations_by_layer_index[layer_index]
                for layer_index in layer_indices
            }
            for activation_location_type_and_pass_type, activations_by_layer_index in all_activations_by_location_type_and_pass_type.items()
        }
    ds_store_by_layer_indices: dict[tuple[int, ...], DerivedScalarStore] = {}
    current_layer_indices_list = [(5, 6), (5,), (6,)]
    for current_layer_indices in current_layer_indices_list:
        dst_config = DstConfig(
            model_context=model_context,
            layer_indices=list(current_layer_indices),
        )
        scalar_derivers = [
            make_scalar_deriver(
                dst=dst,
                dst_config=dst_config,
            )
            for dst in dsts
        ]
        if all_activations_by_location_type_and_pass_type is None:
            # define the raw activations
            all_activations_by_location_type_and_pass_type = {}
            activation_location_type_and_pass_types: list[ActivationLocationTypeAndPassType] = []
            for scalar_deriver in scalar_derivers:
                activation_location_type_and_pass_types.extend(
                    scalar_deriver.get_sub_activation_location_type_and_pass_types()
                )
            activation_location_type_and_pass_types = list(
                set(activation_location_type_and_pass_types)
            )
            for activation_location_type_and_pass_type in activation_location_type_and_pass_types:
                shape_spec = (
                    activation_location_type_and_pass_type.activation_location_type.shape_spec_per_token_sequence
                )
                activation_shape = tuple(get_shape_for_dim(dim) for dim in shape_spec)
                activations_by_layer_index: dict[LayerIndex, torch.Tensor] = {
                    layer_index: torch.randn(activation_shape) for layer_index in all_layer_indices
                }
                all_activations_by_location_type_and_pass_type.update(
                    {activation_location_type_and_pass_type: activations_by_layer_index}
                )
        activations_by_location_type_and_pass_type = get_subset_of_layer_indices(
            all_activations_by_location_type_and_pass_type=all_activations_by_location_type_and_pass_type,
            layer_indices=list(current_layer_indices),
        )
        raw_activation_store = RawActivationStore.from_nested_dict_of_activations(
            activations_by_location_type_and_pass_type
        )
        ds_store_by_layer_indices[current_layer_indices] = DerivedScalarStore.derive_from_raw(
            raw_activation_store=raw_activation_store, scalar_derivers=scalar_derivers
        )
    assert (
        ds_store_by_layer_indices[(5, 6)].sum()
        == ds_store_by_layer_indices[(5,)].sum() + ds_store_by_layer_indices[(6,)].sum()
    )

================
File: neuron_explainer/activations/derived_scalars/tests/test_derived_scalar_types.py
================
import pytest
import torch
from neuron_explainer.activations.derived_scalars.attention import (
    flatten_lower_triangle,
    make_reshape_fn,
)
from neuron_explainer.activations.derived_scalars.autoencoder import (
    make_autoencoder_latent_scalar_deriver_factory,
    make_autoencoder_write_norm_scalar_deriver_factory,
)
from neuron_explainer.activations.derived_scalars.scalar_deriver import (
    ActivationsAndMetadata,
    DerivedScalarType,
    DstConfig,
    ScalarSource,
)
from neuron_explainer.activations.derived_scalars.tests.utils import (
    get_activation_shape,
    get_autoencoder_test_path,
)
from neuron_explainer.file_utils import CustomFileHandler, file_exists
from neuron_explainer.models import Autoencoder
from neuron_explainer.models.autoencoder_context import AutoencoderConfig, AutoencoderContext
from neuron_explainer.models.model_component_registry import NodeType, PassType
from neuron_explainer.models.model_context import ModelContext, get_default_device
def test_flatten_lower_triangle() -> None:
    """This tests code which turns the lower triangular entries tensor (that is, entries with
    i <= j for i and j indexing the last two dimensions) into a tensor flattened along the last
    two dimensions."""
    # Test the function with a (2, 2, 4, 4) tensor
    t = torch.arange(1, 65).reshape(2, 2, 4, 4)
    flattened_t = flatten_lower_triangle(t)
    expected_t = torch.tensor(
        [
            [[1, 5, 6, 9, 10, 11, 13, 14, 15, 16], [17, 21, 22, 25, 26, 27, 29, 30, 31, 32]],
            [[33, 37, 38, 41, 42, 43, 45, 46, 47, 48], [49, 53, 54, 57, 58, 59, 61, 62, 63, 64]],
        ],
        dtype=torch.int64,
    )
    assert flattened_t.shape == (2, 2, 10), f"Got shape {flattened_t.shape}, expected (2, 2, 10)"
    assert torch.allclose(
        flattened_t, expected_t
    ), "Flattened tensor values do not match expected values"
    # Test the function with a (3, 2, 2, 4, 4) tensor
    t = torch.arange(1, 33).reshape(2, 4, 4)
    flattened_t = flatten_lower_triangle(t)
    expected_t = torch.tensor(
        [[1, 5, 6, 9, 10, 11, 13, 14, 15, 16], [17, 21, 22, 25, 26, 27, 29, 30, 31, 32]],
        dtype=torch.int64,
    )
    assert flattened_t.shape == (2, 10), f"Got shape {flattened_t.shape}, expected (2, 2, 10)"
    assert torch.allclose(
        flattened_t, expected_t
    ), "Flattened tensor values do not match expected values"
def test_dst_values_equal_to_names() -> None:
    message = "DerivedScalarType values should be equal to their names (lowercased)."
    for dst in DerivedScalarType:
        assert dst.value == dst.name.lower(), message
def make_fake_activations_and_metadata_tuple(
    sub_scalar_sources: tuple[ScalarSource, ...],
    model_context: ModelContext,
    n_tokens: int,
    n_latents: int | None = None,
) -> tuple[ActivationsAndMetadata, ...]:
    activations_and_metadata_list = []
    desired_layer_indices = None
    for sub_scalar_source in sub_scalar_sources:
        dst_and_pass_type = sub_scalar_source.dst_and_pass_type
        layer_indexer = sub_scalar_source.layer_indexer
        this_dst = dst_and_pass_type.dst
        activation_shape = get_activation_shape(this_dst, model_context, n_tokens, n_latents)
        activations_and_metadata = ActivationsAndMetadata(
            activations_by_layer_index={
                layer_index: torch.randn(activation_shape, device=model_context.device)
                for layer_index in range(model_context.n_layers)
            },
            pass_type=PassType.FORWARD,
            dst=this_dst,
        ).apply_layer_indexer(layer_indexer, desired_layer_indices)
        activations_and_metadata_list += [activations_and_metadata]
        if len(activations_and_metadata_list) == 1:
            # match the layer_indices of the first activations_and_metadata object
            desired_layer_indices = list(activations_and_metadata_list[0].layer_indices)
    return tuple(activations_and_metadata_list)
def _get_autoencoder_test_path_maybe_saving_new_autoencoder(
    dst: DerivedScalarType,
    model_context: ModelContext,
) -> str:
    """Return the path to a test autoencoder. If the autoencoder does not exist, create it."""
    autoencoder_path = get_autoencoder_test_path(dst)
    if file_exists(autoencoder_path):
        return autoencoder_path
    # create autoencoder
    activation_shape = get_activation_shape(dst, model_context)
    input_tensor = torch.zeros(activation_shape)
    reshape_fn = make_reshape_fn(dst)
    reshaped_input_tensor = reshape_fn(input_tensor)
    print(f"autoencoder_input_tensor.shape: {reshaped_input_tensor.shape}")
    n_inputs = reshaped_input_tensor.shape[1]
    n_latents = 6
    autoencoder = Autoencoder(n_latents, n_inputs)
    print({autoencoder.pre_bias.shape})
    # change bias to have non-zero latent activations, to have non-zero gradients in test
    autoencoder.latent_bias.data[:] = 100.0
    # save autoencoder
    with CustomFileHandler(autoencoder_path, "wb") as f:
        print(f"Saving autoencoder to {autoencoder_path}")
        torch.save(autoencoder.state_dict(), f)
    return autoencoder_path
@pytest.mark.parametrize(
    "dst",
    [
        DerivedScalarType.MLP_POST_ACT,
        DerivedScalarType.RESID_DELTA_MLP_FROM_MLP_POST_ACT,
        DerivedScalarType.ATTN_WEIGHTED_SUM_OF_VALUES,
        DerivedScalarType.RESID_DELTA_ATTN,
        DerivedScalarType.ATTN_WEIGHTED_VALUE,
        DerivedScalarType.ATTN_WRITE,
    ],
)
def test_autoencoder_scalar_deriver(
    dst: DerivedScalarType,
) -> None:
    model_name = "gpt2-small"
    device = get_default_device()
    model_context = ModelContext.from_model_type(model_name, device=device)
    layer_indices = list(range(model_context.n_layers))
    n_tokens = 10
    # create autoencoder config
    autoencoder_path = _get_autoencoder_test_path_maybe_saving_new_autoencoder(dst, model_context)
    autoencoder_config = AutoencoderConfig(
        dst=dst,
        autoencoder_path_by_layer_index={
            layer_index: autoencoder_path for layer_index in layer_indices
        },
    )
    autoencoder_context = AutoencoderContext(autoencoder_config=autoencoder_config, device=device)
    autoencoder_context.warmup()
    n_latents = autoencoder_context.num_autoencoder_directions
    # Test
    for make_scalar_deriver_factory, generic_dst in zip(
        [
            make_autoencoder_latent_scalar_deriver_factory,
            make_autoencoder_write_norm_scalar_deriver_factory,
        ],
        [
            DerivedScalarType.AUTOENCODER_LATENT,
            DerivedScalarType.AUTOENCODER_WRITE_NORM,
        ],
    ):
        if generic_dst == DerivedScalarType.AUTOENCODER_WRITE_NORM and (
            dst.node_type != NodeType.RESIDUAL_STREAM_CHANNEL
        ):
            continue
        # create scalar deriver
        dst_config = DstConfig(
            layer_indices=layer_indices,
            model_name=model_name,
            autoencoder_context=autoencoder_context,
        )
        scalar_deriver = make_scalar_deriver_factory(autoencoder_context.autoencoder_node_type)(
            dst_config
        )
        # create fake dataset of activations
        activations_and_metadata_tuple = make_fake_activations_and_metadata_tuple(
            scalar_deriver.get_sub_scalar_sources(), model_context, n_tokens, n_latents
        )
        # calculate derived scalar
        new_activations_and_metadata = (
            scalar_deriver.activations_and_metadata_calculate_derived_scalar_fn(
                activations_and_metadata_tuple, PassType.FORWARD
            )
        )
        assert list(new_activations_and_metadata.activations_by_layer_index.keys()) == layer_indices
        assert (
            new_activations_and_metadata.activations_by_layer_index[layer_indices[0]].shape[1]
            == n_latents
        )
        # saved memory by not storing gradients -> is_leaf
        assert new_activations_and_metadata.activations_by_layer_index[layer_indices[0]].is_leaf
        node_specific_dst = generic_dst.update_from_autoencoder_node_type(
            autoencoder_context.autoencoder_node_type
        )
        assert new_activations_and_metadata.dst == node_specific_dst

================
File: neuron_explainer/activations/derived_scalars/tests/utils.py
================
from neuron_explainer.activations.derived_scalars.derived_scalar_types import DerivedScalarType
from neuron_explainer.models.model_component_registry import Dimension
from neuron_explainer.models.model_context import ModelContext, get_default_device
get_testing_device = get_default_device  # keep for compatibility
def get_autoencoder_test_path(
    dst: DerivedScalarType,
) -> str:
    """Return the path to a test autoencoder."""
    name = f"{dst.value}.pt"
    return f"https://openaipublic.blob.core.windows.net/neuron-explainer/test-data/autoencoder_test_state_dicts/{name}"
def get_activation_shape(
    dst: DerivedScalarType,
    model_context: ModelContext,
    n_tokens: int = 10,
    n_latents: int | None = None,
) -> tuple[int, ...]:
    """Return the shape of activations"""
    activation_shape = []
    assert dst.shape_spec_per_token_sequence[0].is_sequence_token_dimension
    if dst in [
        DerivedScalarType.ATTN_WRITE_NORM,
        DerivedScalarType.FLATTENED_ATTN_POST_SOFTMAX,
        DerivedScalarType.ATTN_ACT_TIMES_GRAD,
        DerivedScalarType.ATTN_WRITE_TO_FINAL_RESIDUAL_GRAD,
    ]:
        # first dimension is token pairs
        activation_shape.append(n_tokens * (n_tokens + 1) // 2)
    else:
        activation_shape.append(n_tokens)
    for dimension in dst.shape_spec_per_token_sequence[1:]:
        if dimension == Dimension.SINGLETON:
            activation_shape.append(1)
        elif dimension.is_model_intrinsic:
            activation_shape.append(model_context.get_dim_size(dimension))
        elif dimension.is_sequence_token_dimension:
            activation_shape.append(n_tokens)
        elif dimension.is_parameterized_dimension:
            assert n_latents is not None
            activation_shape.append(n_latents)
        else:
            raise ValueError(f"Unsupported dimension: {dimension}")
    print(f"{dst}: {activation_shape}")
    return tuple(activation_shape)

================
File: neuron_explainer/activations/derived_scalars/tokens.py
================
"""
This file contains utilities and a class for converting token-space vectors to and from a pydantic
base class summarizing them in terms of the extremal entries in the vector and their associated
tokens. The class can be used in InteractiveModel responses.
"""
import math
import torch
from pydantic import validator
from neuron_explainer.activations.derived_scalars.least_common_tokens import (
    LEAST_COMMON_GPT2_TOKEN_STRS,
)
from neuron_explainer.models.model_context import ModelContext
from neuron_explainer.pydantic import CamelCaseBaseModel, immutable
@immutable
class TokenAndRawScalar(CamelCaseBaseModel):
    token: str
    scalar: float
    @validator("scalar")
    def check_scalar(cls, scalar: float) -> float:
        assert math.isfinite(scalar), "Scalar value must be a finite number"
        return scalar
@immutable
class TokenAndScalar(TokenAndRawScalar):
    normalized_scalar: float
    @validator("normalized_scalar")
    def check_normalized_scalar(cls, normalized_scalar: float) -> float:
        assert math.isfinite(normalized_scalar), "Normalized scalar value must be a finite number"
        return normalized_scalar
@immutable
class TopTokens(CamelCaseBaseModel):
    """
    Contains two lists of tokens and associated scalars: one for the highest-scoring tokens and one
    for the lowest-scoring tokens, according to some way of scoring tokens. For example, this could
    be used to represent the top upvoted and downvoted "logit lens" tokens. An instance of this
    class is scoped to a single node. The set of tokens eligible for scoring is typically just the
    model's entire vocabulary. Each list is sorted from largest to smallest absolute value for the
    associated scalar.
    """
    top: list[TokenAndScalar]
    bottom: list[TokenAndScalar]
def package_top_t_tokens(
    model_context: ModelContext,
    top_t_upvoted_token_ints_tensor: torch.Tensor,
    top_t_upvoted_token_weights_tensor: torch.Tensor,
    norm_top_t_upvoted_token_weights_tensor: torch.Tensor,
) -> list[list[TokenAndScalar]]:
    """
    Convert tensors of top t upvoted token ints, weights, and normalized weights into a list of
    lists of TokenAndScalar, one list per node.
    """
    n_nodes, n_tokens = top_t_upvoted_token_ints_tensor.shape
    top_t_upvoted_token_strings = [
        model_context.decode_token_list(top_t_upvoted_token_ints_tensor[i].tolist())
        for i in range(top_t_upvoted_token_ints_tensor.shape[0])
    ]
    top_t_upvoted_token_weights = top_t_upvoted_token_weights_tensor.tolist()
    norm_top_t_upvoted_token_weights = norm_top_t_upvoted_token_weights_tensor.tolist()
    token_and_weight_data_for_all_nodes = []
    # for each row of the tensor, zip the results into a list of TokenAndRawScalar for the relevant node
    for node_index in range(n_nodes):
        token_and_weight_data_for_this_node = []
        # zip the results into a list of TokenAndRawScalar for this node
        for token_index in range(n_tokens):
            token_and_weight_data_for_this_node.append(
                TokenAndScalar(
                    token=top_t_upvoted_token_strings[node_index][token_index],
                    scalar=top_t_upvoted_token_weights[node_index][token_index],
                    normalized_scalar=norm_top_t_upvoted_token_weights[node_index][token_index],
                )
            )
        token_and_weight_data_for_all_nodes.append(token_and_weight_data_for_this_node)
    return token_and_weight_data_for_all_nodes
def get_top_t_tokens_maybe_excluding_least_common(
    token_writes_tensor: torch.Tensor,
    top_t_tokens: int,
    largest: bool,
    least_common_tokens_as_ints: list[int] | None,
) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Return the top t tokens and their weights, optionally excluding the least common token ints,
    passed as an argument. Sorted by largest or smallest absolute value, as specified by the
    'largest' argument.
    """
    if least_common_tokens_as_ints is not None:
        token_writes_tensor[:, least_common_tokens_as_ints] = (
            float("-inf") if largest else float("inf")
        )
    (
        top_t_upvoted_token_weights_tensor,
        top_t_upvoted_token_ints_tensor,
    ) = token_writes_tensor.topk(k=top_t_tokens, largest=largest)
    assert torch.isfinite(
        top_t_upvoted_token_weights_tensor
    ).all(), "Top token weights should only contain finite values"
    return top_t_upvoted_token_weights_tensor, top_t_upvoted_token_ints_tensor
def get_most_upvoted_and_downvoted_tokens_for_nodes(
    model_context: ModelContext,
    token_writes_tensor: torch.Tensor,
    top_t_tokens: int,
    flip_upvoted_and_downvoted: bool = False,
) -> list[TopTokens]:
    """
    Convert a 2D token_writes_tensor to the most positive (upvoted) and most negative (downvoted) vocab tokens per row,
    with weights corresponding to how upvoted or downvoted each token is. Return a list (indexed by row index) of
    TopTokens, each of which contains a list of TokenAndScalar for the most upvoted tokens and
    a list of TokenAndScalar for the most downvoted tokens.
    Note that the scalars in TokenAndScalar are referred to as 'weights', despite being held in an object called
    TokenAndScalar. The weights returned here include normalized versions (normalized to max(abs(weight))).
    """
    if model_context.get_encoding().name == "gpt2":
        # for GPT-2, we exclude tokens string-matching to the least common tokens
        # from the top_t tokens displayed
        least_common_tokens_as_ints = model_context.encode_token_str_list(
            LEAST_COMMON_GPT2_TOKEN_STRS
        )
    else:
        least_common_tokens_as_ints = None
    (
        top_t_upvoted_token_weights_tensor,
        top_t_upvoted_token_ints_tensor,
    ) = get_top_t_tokens_maybe_excluding_least_common(
        token_writes_tensor,
        top_t_tokens,
        largest=True,
        least_common_tokens_as_ints=least_common_tokens_as_ints,
    )
    (
        top_t_downvoted_token_weights_tensor,
        top_t_downvoted_token_ints_tensor,
    ) = get_top_t_tokens_maybe_excluding_least_common(
        token_writes_tensor,
        top_t_tokens,
        largest=False,
        least_common_tokens_as_ints=least_common_tokens_as_ints,
    )
    max_abs_token_writes_tensor = torch.max(
        top_t_upvoted_token_weights_tensor[:, 0:1].abs(),
        top_t_downvoted_token_weights_tensor[:, 0:1].abs(),
    )
    def safe_divide(
        numerator_tensor: torch.Tensor, denominator_tensor: torch.Tensor
    ) -> torch.Tensor:
        assert torch.isfinite(
            numerator_tensor
        ).all(), "Numerator tensor should only contain finite values"
        assert torch.isfinite(
            denominator_tensor
        ).all(), "Denominator tensor should only contain finite values"
        return torch.where(
            denominator_tensor == 0,
            torch.zeros_like(numerator_tensor),
            numerator_tensor / denominator_tensor,
        )
    norm_top_t_upvoted_token_weights_tensor = safe_divide(
        top_t_upvoted_token_weights_tensor, max_abs_token_writes_tensor
    )
    norm_top_t_downvoted_token_weights_tensor = safe_divide(
        top_t_downvoted_token_weights_tensor, max_abs_token_writes_tensor
    )
    normalized_most_upvoted_tokens_for_all_nodes = package_top_t_tokens(
        model_context,
        top_t_upvoted_token_ints_tensor,
        top_t_upvoted_token_weights_tensor,
        norm_top_t_upvoted_token_weights_tensor,
    )
    normalized_most_downvoted_tokens_for_all_nodes = package_top_t_tokens(
        model_context,
        top_t_downvoted_token_ints_tensor,
        top_t_downvoted_token_weights_tensor,
        norm_top_t_downvoted_token_weights_tensor,
    )
    # zip the results into a list of TopTokens
    top_tokens_list = []
    for node_index in range(len(normalized_most_upvoted_tokens_for_all_nodes)):
        if flip_upvoted_and_downvoted:
            normalized_most_upvoted_tokens_for_node = (
                normalized_most_downvoted_tokens_for_all_nodes[node_index]
            )
            normalized_most_downvoted_tokens_for_node = (
                normalized_most_upvoted_tokens_for_all_nodes[node_index]
            )
        else:
            normalized_most_upvoted_tokens_for_node = normalized_most_upvoted_tokens_for_all_nodes[
                node_index
            ]
            normalized_most_downvoted_tokens_for_node = (
                normalized_most_downvoted_tokens_for_all_nodes[node_index]
            )
        top_tokens_list.append(
            TopTokens(
                top=normalized_most_upvoted_tokens_for_node,
                bottom=normalized_most_downvoted_tokens_for_node,
            )
        )
    assert len(top_tokens_list) == token_writes_tensor.shape[0]
    return top_tokens_list

================
File: neuron_explainer/activations/derived_scalars/utils.py
================
import torch
def detach_and_clone(x: torch.Tensor, requires_grad: bool) -> torch.Tensor:
    """In some cases, a derived scalar may be computed by applying a function to
    some activations, and running .backward() on the output, with some tensors
    desired to be backprop'ed through and some not. This function is for that:
    it detaches and clones the input tensor such that it doesn't interfere with
    other places those activations are used, and so that the gradient information
    is cleared. It then sets requires_grad to the desired value based on whether this
    activation should be backprop'ed through."""
    return x.detach().clone().requires_grad_(requires_grad)

================
File: neuron_explainer/activations/derived_scalars/write_tensors.py
================
import torch
from neuron_explainer.activations.derived_scalars.derived_scalar_types import DerivedScalarType
from neuron_explainer.models.autoencoder_context import (
    AutoencoderContext,
    get_autoencoder_output_weight_by_layer_index,
)
from neuron_explainer.models.model_component_registry import (
    LayerIndex,
    NodeType,
    WeightLocationType,
)
from neuron_explainer.models.model_context import ModelContext
def get_attn_write_tensor_by_layer_index(
    model_context: ModelContext,
    layer_indices: list[int] | None,
) -> dict[LayerIndex, torch.Tensor]:
    """Returns a dictionary mapping layer index to the write weight matrix for that layer."""
    if layer_indices is None:
        layer_indices = list(range(model_context.n_layers))
    W_out_by_layer_index: dict[LayerIndex, torch.Tensor] = {
        layer_index: model_context.get_weight(
            location_type=WeightLocationType.ATTN_TO_RESIDUAL,
            layer=layer_index,
            device=model_context.device,
        )  # shape (n_heads, d_head, d_model)
        for layer_index in layer_indices
    }
    return W_out_by_layer_index
def get_mlp_write_tensor_by_layer_index(
    model_context: ModelContext, layer_indices: list[int] | None
) -> dict[LayerIndex, torch.Tensor]:
    if layer_indices is None:
        layer_indices = list(range(model_context.n_layers))
    W_out_location_type = WeightLocationType.MLP_TO_RESIDUAL
    W_out_by_layer_index: dict[LayerIndex, torch.Tensor] = {
        layer_index: model_context.get_weight(
            location_type=W_out_location_type,
            layer=layer_index,
            device=model_context.device,
        )  # shape (d_ff, d_model)
        for layer_index in layer_indices
    }
    return W_out_by_layer_index
def _assert_non_none(x: LayerIndex) -> int:
    assert x is not None
    return x
def get_autoencoder_write_tensor_by_layer_index(
    autoencoder_context: AutoencoderContext,
    model_context: ModelContext,
) -> dict[LayerIndex, torch.Tensor]:
    if autoencoder_context.dst == DerivedScalarType.MLP_POST_ACT:
        autoencoder_output_weight_by_layer_index = get_autoencoder_output_weight_by_layer_index(
            autoencoder_context
        )
        W_out_by_layer_index = get_mlp_write_tensor_by_layer_index_with_autoencoder_context(
            autoencoder_context, model_context
        )
        return {
            _assert_non_none(layer_index): torch.einsum(
                "an,nd->ad",
                autoencoder_output_weight_by_layer_index[layer_index],
                W_out_by_layer_index[_assert_non_none(layer_index)],
            )
            for layer_index in autoencoder_context.layer_indices
        }
    else:
        assert (
            autoencoder_context.dst.node_type == NodeType.RESIDUAL_STREAM_CHANNEL
        ), autoencoder_context.dst
        return get_autoencoder_output_weight_by_layer_index(autoencoder_context)
def get_mlp_write_tensor_by_layer_index_with_autoencoder_context(
    autoencoder_context: AutoencoderContext,
    model_context: ModelContext,
) -> dict[int, torch.Tensor]:
    assert all(layer_index is not None for layer_index in autoencoder_context.layer_indices)
    layer_indices: list[int] = list(autoencoder_context.layer_indices)  # type: ignore
    write_tensor_by_layer_index = get_mlp_write_tensor_by_layer_index(
        model_context=model_context, layer_indices=layer_indices
    )
    return {
        _assert_non_none(layer_index): write_tensor_by_layer_index[layer_index]
        for layer_index in autoencoder_context.layer_indices
    }

================
File: neuron_explainer/activations/hook_graph.py
================
"""
This module contains classes for injecting hooks into a Transformer using the
ActivationLocationType and PassType ontology. These produce activation location types that would not
have otherwise existed. For example, the AutoencoderHookGraph is necessary for the
ActivationLocationType.ONLINE_AUTOENCODER_LATENT location to exist.
"""
from abc import ABC
from copy import deepcopy
from typing import Any, Callable, Mapping, cast
import torch
from neuron_explainer.activations.derived_scalars.derived_scalar_types import DerivedScalarType
from neuron_explainer.models.autoencoder_context import AutoencoderContext
from neuron_explainer.models.hooks import (
    AtLayers,
    AutoencoderHooks,
    HookCollection,
    Hooks,
    TransformerHooks,
)
from neuron_explainer.models.inference_engine_type_registry import (
    InferenceEngineType,
    get_hook_location_type_for_activation_location_type,
    standard_model_activation_location_types,
)
from neuron_explainer.models.model_component_registry import (
    ActivationLocationType,
    ActivationLocationTypeAndPassType,
    LayerIndex,
    PassType,
)
def unflatten(f: Callable) -> Callable:
    def _f(x: torch.Tensor) -> torch.Tensor:
        return f(x.reshape(-1, x.shape[-1])).reshape(x.shape[0], x.shape[1], -1)
    return _f
def _append_to_hook_collection_using_string_list(
    hook_collection: HookCollection, string_list: list[str], hook: Callable
) -> None:
    assert len(string_list) > 0
    assert (
        string_list[0] in hook_collection.all_hooks
    ), f"string_list: {string_list}, hook_collection: {hook_collection}"
    sub_hook_collection = hook_collection.all_hooks[string_list[0]]
    if len(string_list) == 1:
        assert isinstance(
            sub_hook_collection, Hooks
        ), f"string_list: {string_list}, hook_collection: {type(hook_collection)}, sub_hook_collection: {type(sub_hook_collection)}"
        sub_hook_collection.append(hook)
    else:
        assert isinstance(
            sub_hook_collection, HookCollection
        ), f"string_list: {string_list}, hook_collection: {type(hook_collection)}, sub_hook_collection: {type(sub_hook_collection)}"
        _append_to_hook_collection_using_string_list(sub_hook_collection, string_list[1:], hook)
def _append_to_hook_collection_using_activation_location_type_and_pass_type(
    hook_collection: HookCollection,
    activation_location_type_and_pass_type: ActivationLocationTypeAndPassType,
    hook: Callable,
    append_to_fwd2: bool = False,
) -> None:
    activation_location_type = activation_location_type_and_pass_type.activation_location_type
    pass_type = activation_location_type_and_pass_type.pass_type
    standard_model_hook_location_type = get_hook_location_type_for_activation_location_type(
        activation_location_type, inference_engine_type=InferenceEngineType.STANDARD
    )
    if (
        "resid" in standard_model_hook_location_type
        and "post_emb" not in standard_model_hook_location_type
        and "ln_f" not in standard_model_hook_location_type
        and "post_ln_f" not in standard_model_hook_location_type
    ):
        # an extra "torso" is needed for the residual location types
        standard_model_hook_location_type = standard_model_hook_location_type.replace(
            "resid", "resid/torso"
        )
    string_list = standard_model_hook_location_type.split("/")  # e.g. ["mlp", "post_act"]
    if append_to_fwd2:
        assert pass_type == PassType.FORWARD
        string_list += ["fwd2"]  # called after all "fwd" and "bwd" hooks
    else:
        string_list += [_pass_type_hc_name_by_hook_pass_type[pass_type]]
    _append_to_hook_collection_using_string_list(hook_collection, string_list, hook)
_pass_type_hc_name_by_hook_pass_type: dict[PassType, str] = {
    PassType.FORWARD: "fwd",
    PassType.BACKWARD: "bwd",
}
class PerLayerHookCollection(HookCollection):
    """
    Organizes HookCollections by layer; supports e.g. appending to the same location within
    each per-layer HookCollection by supplying a callable to apply_fn_to_all_layers, to do that
    appending.
    """
    def __init__(self, hook_collection_by_layer: Mapping[LayerIndex, HookCollection]) -> None:
        super().__init__()
        for layer in hook_collection_by_layer.keys():
            self.add_subhooks(layer, hook_collection_by_layer[layer])
    def __call__(self, x: torch.Tensor, *, layer: LayerIndex = None, **kwargs: Any) -> torch.Tensor:
        if layer in self.all_hooks:
            return self.all_hooks[layer](x, layer=layer, **kwargs)
        else:
            return x
    def append_to_all_layers_using_string_list(
        self, string_list: list[str], hook: Callable
    ) -> None:
        for layer in self.all_hooks.keys():
            _append_to_hook_collection_using_string_list(self.all_hooks[layer], string_list, hook)
    def __deepcopy__(self, memo: dict) -> "PerLayerHookCollection":
        # can't use deepcopy because of __getattr__
        hook_collection_by_layer = self.all_hooks
        new = self.__class__(self.all_hooks)
        new.all_hooks = deepcopy(self.all_hooks)
        return new
class HookGraph(ABC):
    """
    This is a wrapper for HookCollection objects that supports
    1. adding hooks at points specified using activation_location_type + pass_type and optionally layer_indices
    2. adding subgraphs that are themselves HookGraphs, in such a way that activation_location_types within the
    subgraph remain accessible by the same activation_location_type + pass_type + layer_indices interface
    """
    hook_collection: HookCollection
    activation_locations: set[ActivationLocationType]
    subgraph_by_name: dict[str, "InjectableHookGraph"]
    def __call__(self, x: torch.Tensor, *args: Any, **kwargs: Any) -> torch.Tensor:
        return self.hook_collection(x, *args, **kwargs)  # type: ignore
    def append(
        self,
        activation_location_type_and_pass_type: ActivationLocationTypeAndPassType,
        hook: Callable,
        layer_indices: int | list[int] | None = None,
        append_to_fwd2: bool = False,
    ) -> None:
        pass
    def inject_subgraph(
        self,
        # activation_location_type_and_pass_type: ActivationLocationTypeAndPassType,
        subgraph: "InjectableHookGraph",
        name: str,
        layer_indices: int | list[int] | None = None,
    ) -> None:
        activation_location_type_and_pass_type = subgraph.at_activation_location_type_and_pass_type
        activation_location_type = activation_location_type_and_pass_type.activation_location_type
        pass_type = activation_location_type_and_pass_type.pass_type
        assert (
            activation_location_type in self.activation_locations
        ), f"{activation_location_type} not in {self.activation_locations}"
        assert name not in self.subgraph_by_name
        # assert no overlap between activation locations of self and graph
        assert not self.activation_locations.intersection(subgraph.activation_locations), (
            self.activation_locations,
            subgraph.activation_locations,
        )
        self.append(
            activation_location_type_and_pass_type=activation_location_type_and_pass_type,
            hook=subgraph,
            layer_indices=layer_indices,
            append_to_fwd2=True,  # we inject the subgraph after the forward and backward hooks
        )
        self.subgraph_by_name[name] = subgraph
        self.activation_locations = self.activation_locations.union(subgraph.activation_locations)
class InjectableHookGraph(HookGraph):
    """
    This is a HookGraph that can be injected into another HookGraph. It contains one extra piece
    of information: the activation_location_type_and_pass_type where it is to be injected.
    """
    at_activation_location_type_and_pass_type: ActivationLocationTypeAndPassType
class TransformerHookGraph(HookGraph):
    """
    This is a HookGraph that specifically wraps TransformerHooks. It can be used with the Transformer.forward()
    function call using the transformer_graph.as_transformer_hooks() method.
    """
    def __init__(self) -> None:
        self.hook_collection = TransformerHooks()
        self.subgraph_by_name: dict[str, InjectableHookGraph] = {}
        self.activation_locations = standard_model_activation_location_types
    def append(
        self,
        activation_location_type_and_pass_type: ActivationLocationTypeAndPassType,
        hook: Callable,
        layer_indices: int | list[int] | None = None,
        append_to_fwd2: bool = False,
    ) -> None:
        activation_location_type = activation_location_type_and_pass_type.activation_location_type
        pass_type = activation_location_type_and_pass_type.pass_type
        if layer_indices is not None:
            assert (
                not activation_location_type.has_no_layers
            ), f"activation_location_type: {activation_location_type}, layer_indices: {layer_indices}"
            hook = AtLayers(layer_indices).append(hook)
        assert (
            activation_location_type in self.activation_locations
        ), f"{activation_location_type} not in {self.activation_locations}"
        if activation_location_type in standard_model_activation_location_types:
            _append_to_hook_collection_using_activation_location_type_and_pass_type(
                self.hook_collection,
                activation_location_type_and_pass_type,
                hook,
                append_to_fwd2,
            )
        else:
            for name in self.subgraph_by_name.keys():
                if activation_location_type in self.subgraph_by_name[name].activation_locations:
                    self.subgraph_by_name[name].append(activation_location_type_and_pass_type, hook)
    def as_transformer_hooks(self) -> TransformerHooks:
        return cast(TransformerHooks, self.hook_collection)
class AutoencoderHookGraph(InjectableHookGraph):
    """
    This is a HookGraph that specifically wraps a PerLayerHookCollection of AutoencoderHooks (in general, one per layer).
    """
    def __init__(
        self, autoencoder_context: AutoencoderContext, is_one_of_multiple_autoencoders: bool = False
    ) -> None:
        autoencoder_hooks_by_layer_index: dict[LayerIndex, AutoencoderHooks] = {}
        layer_indices = autoencoder_context.layer_indices or [None]
        for layer_index in layer_indices:
            autoencoder = autoencoder_context.get_autoencoder(layer_index)
            autoencoder_hooks_by_layer_index[layer_index] = AutoencoderHooks(
                encode=unflatten(autoencoder.encode),
                decode=unflatten(autoencoder.decode),
                add_error=True,
            )
        if not autoencoder_context.dst.is_raw_activation_type:
            raise NotImplementedError(
                "AutoencoderHookGraph only supports raw activation types for now."
            )
        self.at_activation_location_type_and_pass_type = ActivationLocationTypeAndPassType(
            autoencoder_context.dst.to_activation_location_type(), PassType.FORWARD
        )
        self.hook_collection = PerLayerHookCollection(autoencoder_hooks_by_layer_index)
        self.location_hc_name_by_activation_location_type = (
            self.get_location_hc_name_by_activation_location_type(
                autoencoder_context.dst, is_one_of_multiple_autoencoders
            )
        )
        self.activation_locations = set(self.location_hc_name_by_activation_location_type.keys())
        self.autoencoder_context = autoencoder_context
    def append(
        self,
        activation_location_type_and_pass_type: ActivationLocationTypeAndPassType,
        hook: Callable,
        layer_indices: int | list[int] | None = None,
        append_to_fwd2: bool = False,
    ) -> None:
        activation_location_type = activation_location_type_and_pass_type.activation_location_type
        pass_type = activation_location_type_and_pass_type.pass_type
        if layer_indices is not None:
            assert (
                not activation_location_type.has_no_layers
            ), f"activation_location_type: {activation_location_type}, layer_indices: {layer_indices}"
            hook = AtLayers(layer_indices).append(hook)
        assert (
            activation_location_type in self.activation_locations
        ), f"{activation_location_type} not in {self.activation_locations}"
        assert pass_type in _pass_type_hc_name_by_hook_pass_type
        string_list = [
            self.location_hc_name_by_activation_location_type[activation_location_type],
            _pass_type_hc_name_by_hook_pass_type[pass_type],
        ]
        self.hook_collection.append_to_all_layers_using_string_list(string_list, hook)
    # note: hc = hook_collection
    def get_location_hc_name_by_activation_location_type(
        self, dst: DerivedScalarType, is_one_of_multiple_autoencoders: bool
    ) -> dict[ActivationLocationType, str]:
        latent_alt_by_dst = {
            DerivedScalarType.MLP_POST_ACT: ActivationLocationType.ONLINE_MLP_AUTOENCODER_LATENT,
            DerivedScalarType.RESID_DELTA_MLP: ActivationLocationType.ONLINE_MLP_AUTOENCODER_LATENT,
            DerivedScalarType.RESID_DELTA_ATTN: ActivationLocationType.ONLINE_ATTENTION_AUTOENCODER_LATENT,
        }
        error_alt_by_dst = {
            DerivedScalarType.MLP_POST_ACT: ActivationLocationType.ONLINE_MLP_AUTOENCODER_ERROR,
            DerivedScalarType.RESID_DELTA_MLP: ActivationLocationType.ONLINE_RESIDUAL_MLP_AUTOENCODER_ERROR,
            DerivedScalarType.RESID_DELTA_ATTN: ActivationLocationType.ONLINE_RESIDUAL_ATTENTION_AUTOENCODER_ERROR,
        }
        location_hc_name_by_alt = {
            latent_alt_by_dst[dst]: "latents",
            error_alt_by_dst[dst]: "error",
        }
        if not is_one_of_multiple_autoencoders:
            # if there is only one autoencoder, we also add the "ONLINE_AUTOENCODER_LATENT" location for backward compatibility
            generic_latent_alt = ActivationLocationType.ONLINE_AUTOENCODER_LATENT
            location_hc_name_by_alt[generic_latent_alt] = "latents"
        return location_hc_name_by_alt

================
File: neuron_explainer/activations/test_attention_utils.py
================
from neuron_explainer.activations.attention_utils import (
    _inverse_triangular_number,
    convert_flattened_index_to_unflattened_index,
    get_attended_to_sequence_length_per_sequence_token,
    get_max_num_attended_to_sequence_tokens,
)
def _simulate_num_activations(
    num_sequence_tokens: int, max_num_attended_to_sequence_tokens: int
) -> int:
    num_activations_per_token = list(range(1, max_num_attended_to_sequence_tokens + 1)) + [
        max_num_attended_to_sequence_tokens
        for _ in range(num_sequence_tokens - max_num_attended_to_sequence_tokens)
    ]
    num_activations = sum(num_activations_per_token)
    return num_activations
def test_inverse_triangular_number() -> None:
    for m in range(5):
        n = m * (m + 1) // 2
        assert _inverse_triangular_number(n) == m
def test_get_max_num_attended_to_sequence_tokens() -> None:
    num_sequence_tokens = 100
    for max_num_attended_to_sequence_tokens in [50, 100]:
        num_activations = _simulate_num_activations(
            num_sequence_tokens, max_num_attended_to_sequence_tokens
        )
        assert (
            get_max_num_attended_to_sequence_tokens(num_sequence_tokens, num_activations)
            == max_num_attended_to_sequence_tokens
        )
        attended_to_sequence_lengths = get_attended_to_sequence_length_per_sequence_token(
            num_sequence_tokens, max_num_attended_to_sequence_tokens
        )
        assert sum(attended_to_sequence_lengths) == num_activations, (
            sum(attended_to_sequence_lengths),
            num_activations,
        )
def test_convert_flattened_index_to_unflattened_index() -> None:
    possible_max_num_attended_to_sequence_tokens = 9
    num_sequence_tokens = 17
    assert possible_max_num_attended_to_sequence_tokens < num_sequence_tokens
    for max_num_attended_to_sequence_tokens in [
        possible_max_num_attended_to_sequence_tokens,
        num_sequence_tokens,
    ]:
        attended_to_sequence_lengths = get_attended_to_sequence_length_per_sequence_token(
            num_sequence_tokens, max_num_attended_to_sequence_tokens
        )
        num_activations = sum(attended_to_sequence_lengths)
        flat_indices = list(range(num_activations))
        flat_indices_split_by_sequence_token = []
        for attended_to_sequence_length in attended_to_sequence_lengths:
            flat_indices_split_by_sequence_token.append(flat_indices[:attended_to_sequence_length])
            flat_indices = flat_indices[attended_to_sequence_length:]
        for flat_index in list(range(num_activations)):
            if max_num_attended_to_sequence_tokens == num_sequence_tokens:
                unflattened_i, unflattened_j = convert_flattened_index_to_unflattened_index(
                    flat_index
                )
            else:
                unflattened_i, unflattened_j = convert_flattened_index_to_unflattened_index(
                    flat_index,
                    num_sequence_tokens=num_sequence_tokens,
                    num_activations=num_activations,
                )
            assert unflattened_i < num_sequence_tokens
            assert unflattened_j < len(flat_indices_split_by_sequence_token[unflattened_i])
            assert (
                flat_indices_split_by_sequence_token[unflattened_i][unflattened_j] == flat_index
            ), (
                flat_indices_split_by_sequence_token[unflattened_i][unflattened_j],
                flat_index,
            )

================
File: neuron_explainer/api_client.py
================
import asyncio
import contextlib
import os
import random
import traceback
from functools import wraps
from typing import Any
import httpx
import orjson
API_KEY = os.getenv("OPENAI_API_KEY")
assert API_KEY, "Please set the OPENAI_API_KEY environment variable"
API_HTTP_HEADERS = {
    "Content-Type": "application/json",
    "Authorization": "Bearer " + API_KEY,
}
BASE_API_URL = "https://api.openai.com/v1"
def async_exponential_backoff(retry_on=lambda err: True):
    """
    Returns a decorator which retries the wrapped function as long as the specified retry_on
    function returns True for the exception, applying exponential backoff with jitter after
    failures, up to a retry limit.
    """
    init_delay_s = 1
    max_delay_s = 10
    backoff_multiplier = 2.0
    jitter = 0.2
    def decorate(f):
        @wraps(f)
        async def f_retry(self, *args, **kwargs):
            max_tries = 200
            delay_s = init_delay_s
            for i in range(max_tries):
                try:
                    return await f(self, *args, **kwargs)
                except Exception as err:
                    if i == max_tries - 1:
                        print(f"Exceeded max tries ({max_tries}) on HTTP request")
                        raise
                    if not retry_on(err):
                        print("Unretryable error on HTTP request")
                        raise
                    jittered_delay = random.uniform(delay_s * (1 - jitter), delay_s * (1 + jitter))
                    await asyncio.sleep(jittered_delay)
                    delay_s = min(delay_s * backoff_multiplier, max_delay_s)
        return f_retry
    return decorate
def is_api_error(err: Exception) -> bool:
    if isinstance(err, httpx.HTTPStatusError):
        response = err.response
        error_data = response.json().get("error", {})
        error_message = error_data.get("message")
        if response.status_code in [400, 404, 415]:
            if error_data.get("type") == "idempotency_error":
                print(f"Retrying after idempotency error: {error_message} ({response.url})")
                return True
            else:
                # Invalid request
                return False
        else:
            print(f"Retrying after API error: {error_message} ({response.url})")
            return True
    elif isinstance(err, httpx.ConnectError):
        print(f"Retrying after connection error... ({err.request.url})")
        return True
    elif isinstance(err, httpx.TimeoutException):
        print(f"Retrying after a timeout error... ({err.request.url})")
        return True
    elif isinstance(err, httpx.ReadError):
        print(f"Retrying after a read error... ({err.request.url})")
        return True
    print(f"Retrying after an unexpected error: {repr(err)}")
    traceback.print_tb(err.__traceback__)
    return True
class ApiClient:
    """
    Performs inference using the OpenAI API. Supports response caching and concurrency limits."
    Cache is useful for rerunning code in jupyter notebooks without repeating identical requests
    """
    def __init__(
        self,
        model_name: str,
        max_concurrent: int | None = None,
        # If set, no more than this number of HTTP requests will be made concurrently.
        cache: bool = False,
    ):
        self.model_name = model_name
        if max_concurrent is not None:
            self.concurrency_check: asyncio.Semaphore | None = asyncio.Semaphore(max_concurrent)
        else:
            self.concurrency_check = None
        if cache:
            self.cache = {}
        else:
            self.cache = None
    @async_exponential_backoff(retry_on=is_api_error)
    async def async_generate(self, timeout: int | None = None, **kwargs: Any) -> dict:
        if self.cache is not None:
            key = orjson.dumps(kwargs)
            if key in self.cache:
                return self.cache[key]
        async with contextlib.AsyncExitStack() as stack:
            if self.concurrency_check is not None:
                await stack.enter_async_context(self.concurrency_check)
            http_client = await stack.enter_async_context(httpx.AsyncClient(timeout=timeout))
            # If the request has a "messages" key, it should be sent to the /chat/completions
            # endpoint. Otherwise, it should be sent to the /completions endpoint.
            url = BASE_API_URL + ("/chat/completions" if "messages" in kwargs else "/completions")
            kwargs["model"] = self.model_name
            response = await http_client.post(url, headers=API_HTTP_HEADERS, json=kwargs)
        # Response json has useful information but the exception doesn't include it, so print it out then reraise
        try:
            response.raise_for_status()
        except Exception as e:
            print(response.json())
            raise e
        if self.cache is not None:
            self.cache[key] = response.json()
        return response.json()
if __name__ == "__main__":
    async def main() -> None:
        client = ApiClient(model_name="gpt-3.5-turbo", max_concurrent=1)
        print(
            await client.async_generate(prompt="Why did the chicken cross the road?", max_tokens=9)
        )
    asyncio.run(main())

================
File: neuron_explainer/explanations/attention_head_scoring.py
================
"""Uses API calls to score attention head explanations."""
from __future__ import annotations
import random
from typing import Any
import numpy as np
from sklearn.metrics import roc_auc_score
from neuron_explainer.activations.activations import (
    ActivationRecord,
    ActivationRecordSliceParams,
    load_neuron,
)
from neuron_explainer.activations.attention_utils import (
    convert_flattened_index_to_unflattened_index,
)
from neuron_explainer.api_client import ApiClient
from neuron_explainer.explanations.explainer import (
    ATTENTION_EXPLANATION_PREFIX,
    ContextSize,
    format_attention_head_token_pair_string,
)
from neuron_explainer.explanations.explanations import (
    AttentionSimulation,
    ScoredAttentionSimulation,
)
from neuron_explainer.explanations.few_shot_examples import ATTENTION_HEAD_FEW_SHOT_EXAMPLES
from neuron_explainer.explanations.prompt_builder import (
    ChatMessage,
    PromptBuilder,
    PromptFormat,
    Role,
)
class AttentionHeadOneAtATimeScorer:
    def __init__(
        self,
        model_name: str,
        prompt_format: PromptFormat = PromptFormat.CHAT_MESSAGES,
        # This parameter lets us adjust the length of the prompt when we're generating explanations
        # using older models with shorter context windows. In the future we can use it to experiment
        # with longer context windows.
        context_size: ContextSize = ContextSize.FOUR_K,
        repeat_strongly_attending_pairs: bool = False,
        max_concurrent: int | None = 10,
        cache: bool = False,
    ):
        assert (
            prompt_format == PromptFormat.CHAT_MESSAGES
        ), f"Unhandled prompt format {prompt_format}"
        self.prompt_format = prompt_format
        self.context_size = context_size
        self.client = ApiClient(model_name=model_name, max_concurrent=max_concurrent, cache=cache)
        self.repeat_strongly_attending_pairs = repeat_strongly_attending_pairs
    async def score_explanation(
        self,
        explanation: str,
        activation_records: list[ActivationRecord],
        max_activation: float,
        # The number of high and low activating token pairs to sample for simulation
        num_activations_for_scoring: int = 5,
        # The activation threshold below which a token pair is eligible for sampling
        # as a low activating pair.
        low_activation_threshold: float = 0.1,
    ) -> ScoredAttentionSimulation:
        """Score explanations based on how well they predict attention between
        top attending token pairs and random low attending token pairs."""
        # Use the activation records to generate a set of pairs for scoring.
        # 10 pairs: the five top activating pairs, and five randomly chosen pairs
        # where the activations are below 0.1 * the max value.
        candidates = []
        for i, record in enumerate(activation_records):
            sorted_activation_flat_indices = np.argsort(record.activations)[::-1]
            sorted_vals = [record.activations[idx] for idx in sorted_activation_flat_indices]
            coordinates = [
                convert_flattened_index_to_unflattened_index(flat_index)
                for flat_index in sorted_activation_flat_indices
            ]
            candidates.extend([(i, val, coords) for val, coords in zip(sorted_vals, coordinates)])
        top_activation_coordinates = [
            (candidate[0], candidate[2])
            for candidate in sorted(candidates, key=lambda x: x[1], reverse=True)
        ][:num_activations_for_scoring]
        filtered_low_activation_coordinates = [
            (candidate[0], candidate[2])
            for candidate in candidates
            if candidate[1] < low_activation_threshold * max_activation
        ]
        selected_low_activation_coordinates = random.sample(
            filtered_low_activation_coordinates,
            min(len(filtered_low_activation_coordinates), num_activations_for_scoring),
        )
        attention_simulations = []
        true_labels = [1 for _ in range(len(top_activation_coordinates))] + [
            0 for _ in range(len(selected_low_activation_coordinates))
        ]
        # No need to shuffle because the model only sees one at a time anyway.
        for coords, label in zip(
            top_activation_coordinates + selected_low_activation_coordinates, true_labels
        ):
            activation_record = activation_records[coords[0]]
            # for each pair, generate a prompt where the model is asked to predict if the token pair has a strong
            # or weak activation.
            prompt = self.make_token_pair_prompt(explanation, activation_record.tokens, coords[1])
            assert isinstance(prompt, list)
            assert isinstance(prompt[0], dict)  # Really a ChatMessage
            generate_kwargs: dict[str, Any] = {
                # Using a timeout prevents the scorer from hanging if the API server is overloaded.
                "timeout": 60,
                "n": 1,
                "max_tokens": 1,  # we only want to sample one token.
                "logprobs": True,
                "top_logprobs": 15,
                "messages": prompt,
            }
            response = await self.client.async_generate(**generate_kwargs)
            assert len(response["choices"]) == 1
            # from the response, extract the logit values for "0" (for weak) and "1" (for strong) to obtain
            # a float.
            choice = response["choices"][0]
            # for whatever reason `choice["logprobs"]["top_logprobs"]` is a list of dicts
            logprobs_dicts = choice["logprobs"]["content"][0]["top_logprobs"]
            extracted_probs = {d["token"]: d["logprob"] for d in logprobs_dicts}
            zero_prob = np.exp(extracted_probs["0"]) if "0" in extracted_probs else 0.0
            one_prob = np.exp(extracted_probs["1"]) if "1" in extracted_probs else 0.0
            total_prob = zero_prob + one_prob
            # The score is 0 * normalized probability of "0" + 1 * normalized probability of "1", which
            # reduces to just the normalized probability of "1".
            normalized_one_prob = one_prob / total_prob
            # print(f"zero_prob: {zero_prob/total_prob}, one_prob: {normalized_one_prob}")
            attention_simulations.append(
                AttentionSimulation(
                    tokens=activation_record.tokens,
                    token_pair_coords=coords[1],
                    token_pair_label=label,
                    simulation_prediction=normalized_one_prob,
                )
            )
        assert (
            len(attention_simulations)
            == len(true_labels)
            == len(top_activation_coordinates) + len(selected_low_activation_coordinates)
        )
        # ROC AUC awards a perfect score to explanations that order all of the scores
        # for pairs labeled "1" above the scores for pairs labeled "0" (even if the scores
        # for pairs labeled "0" are well above 0).
        score = roc_auc_score(
            y_true=true_labels, y_score=[sim.simulation_prediction for sim in attention_simulations]
        )
        return ScoredAttentionSimulation(
            attention_simulations=attention_simulations,
            roc_auc_score=score,
        )
    def make_token_pair_prompt(
        self, explanation: str, tokens: list[str], coords: tuple[int, int]
    ) -> str | list[ChatMessage]:
        """
        Create a prompt to send to the API to simulate the model predicting whether a token pair
        has a strong attention write norm according to the given explanation.
        """
        prompt_builder = PromptBuilder()
        prompt_builder.add_message(
            Role.SYSTEM,
            "We're studying attention heads in a neural network. Each head looks at every pair of tokens "
            "in a short token sequence and activates for pairs of tokens that fit what it is looking for. "
            "Attention heads always attend from a token to a token earlier in the sequence (or from a "
            "token to itself). We will display a token sequence and indicate a particular token pair within "
            'that sequence. The "to" token of the pair will be marked with double asterisks (e.g., **token**) '
            'and the "from" token will be marked with double square brackets (e.g., [[token]]). If the token pair '
            "consists of a token paired with itself, it will be marked with both (e.g., [[**token**]]) and "
            "no other token in the sequence will be marked. We present an explanation of what the "
            "attention head is looking for. Output 1 if the head activates for the token pair, and 0 otherwise.",
        )
        num_few_shot = 0
        for few_shot_example in ATTENTION_HEAD_FEW_SHOT_EXAMPLES:
            if not few_shot_example.simulation_examples:
                continue
            for simulation_example in few_shot_example.simulation_examples:
                self._add_per_token_pair_attention_simulation_prompt(
                    prompt_builder=prompt_builder,
                    tokens=few_shot_example.token_pair_examples[
                        simulation_example.token_pair_example_index
                    ].tokens,
                    explanation=few_shot_example.explanation,
                    simulation_coords=simulation_example.token_pair_coordinates,
                    index=num_few_shot,
                    label=simulation_example.label,
                )
                num_few_shot += 1
        self._add_per_token_pair_attention_simulation_prompt(
            prompt_builder=prompt_builder,
            tokens=tokens,
            explanation=explanation,
            simulation_coords=coords,
            index=num_few_shot,
            label=None,
        )
        return prompt_builder.build(self.prompt_format)
    def _add_per_token_pair_attention_simulation_prompt(
        self,
        prompt_builder: PromptBuilder,
        tokens: list[str],
        explanation: str,
        simulation_coords: tuple[int, int],
        index: int,
        label: int | None,  # None means this is the end of the full prompt.
    ) -> None:
        user_message = f"""
Example {index + 1}
Explanation: {ATTENTION_EXPLANATION_PREFIX} {explanation.strip()}
Sequence:\n{format_attention_head_token_pair_string(tokens, simulation_coords)}"""
        if self.repeat_strongly_attending_pairs:
            user_message += (
                f"\nThe same token pair, presented as (to_token, from_token): "
                f"({tokens[simulation_coords[1]]}, {tokens[simulation_coords[0]]})"
            )
        user_message += (
            f"\nPrediction of whether attention head {index + 1} activates on the token pair: "
        )
        prompt_builder.add_message(Role.USER, user_message)
        if label is not None:
            prompt_builder.add_message(Role.ASSISTANT, f"{label}")
if __name__ == "__main__":
    # Example usage
    async def main() -> None:
        scorer = AttentionHeadOneAtATimeScorer("gpt-4o")
        explanation = "attends from tokens to the first token in the sequence"
        attention_head = load_neuron(
            "https://openaipublic.blob.core.windows.net/neuron-explainer/gpt2_small/attn_write_norm/collated_activations_by_token_pair",
            "0",
            "5",
        )
        train_records = attention_head.train_activation_records(
            activation_record_slice_params=ActivationRecordSliceParams(n_examples_per_split=5)
        )
        scored_simulation = await scorer.score_explanation(
            explanation, train_records, max([max(record.activations) for record in train_records])
        )
        print(scored_simulation.roc_auc_score)
    import asyncio
    asyncio.run(main())

================
File: neuron_explainer/explanations/calibrated_simulator.py
================
"""
Code for calibrating simulations of neuron behavior. Calibration refers to a process of mapping from
a space of predicted activation values (e.g. [0, 10]) to the real activation distribution for a
neuron.
See http://go/neuron_explanation_methodology for description of calibration step. Necessary for
simulating neurons in the context of ablate-to-simulation, but can be skipped when using correlation
scoring. (Calibration may still improve quality for scoring, at least for non-linear calibration
methods.)
"""
from __future__ import annotations
import asyncio
from abc import abstractmethod
from typing import Sequence
import numpy as np
from sklearn import linear_model
from neuron_explainer.activations.activations import ActivationRecord
from neuron_explainer.explanations.explanations import ActivationScale
from neuron_explainer.explanations.simulator import NeuronSimulator, SequenceSimulation
class CalibratedNeuronSimulator(NeuronSimulator):
    """
    Wrap a NeuronSimulator and calibrate it to map from the predicted activation space to the
    actual neuron activation space.
    """
    def __init__(self, uncalibrated_simulator: NeuronSimulator):
        self.uncalibrated_simulator = uncalibrated_simulator
    @classmethod
    async def create(
        cls,
        uncalibrated_simulator: NeuronSimulator,
        calibration_activation_records: Sequence[ActivationRecord],
    ) -> CalibratedNeuronSimulator:
        """
        Create and calibrate a calibrated simulator (so initialization and calibration can be done
        in one call).
        """
        calibrated_simulator = cls(uncalibrated_simulator)
        await calibrated_simulator.calibrate(calibration_activation_records)
        return calibrated_simulator
    async def calibrate(self, calibration_activation_records: Sequence[ActivationRecord]) -> None:
        """
        Determine parameters to map from the predicted activation space to the real neuron
        activation space, based on a calibration set.
        Use when simulated sequences haven't already been produced on the calibration set.
        """
        simulations = await asyncio.gather(
            *[
                self.uncalibrated_simulator.simulate(activations.tokens)
                for activations in calibration_activation_records
            ]
        )
        self.calibrate_from_simulations(calibration_activation_records, simulations)
    def calibrate_from_simulations(
        self,
        calibration_activation_records: Sequence[ActivationRecord],
        simulations: Sequence[SequenceSimulation],
    ) -> None:
        """
        Determine parameters to map from the predicted activation space to the real neuron
        activation space, based on a calibration set.
        Use when simulated sequences have already been produced on the calibration set.
        """
        flattened_activations = []
        flattened_simulated_activations: list[float] = []
        for activations, simulation in zip(calibration_activation_records, simulations):
            flattened_activations.extend(activations.activations)
            flattened_simulated_activations.extend(simulation.expected_activations)
        self._calibrate_from_flattened_activations(
            np.array(flattened_activations), np.array(flattened_simulated_activations)
        )
    @abstractmethod
    def _calibrate_from_flattened_activations(
        self,
        true_activations: np.ndarray,
        uncalibrated_activations: np.ndarray,
    ) -> None:
        """
        Determine parameters to map from the predicted activation space to the real neuron
        activation space, based on a calibration set.
        Take numpy arrays of all true activations and all uncalibrated activations on the
        calibration set over all sequences.
        """
    @abstractmethod
    def apply_calibration(self, values: Sequence[float]) -> list[float]:
        """Apply the learned calibration to a sequence of values."""
    async def simulate(self, tokens: Sequence[str]) -> SequenceSimulation:
        uncalibrated_seq_simulation = await self.uncalibrated_simulator.simulate(tokens)
        calibrated_activations = self.apply_calibration(
            uncalibrated_seq_simulation.expected_activations
        )
        calibrated_distribution_values = [
            self.apply_calibration(dv) for dv in uncalibrated_seq_simulation.distribution_values
        ]
        return SequenceSimulation(
            activation_scale=ActivationScale.NEURON_ACTIVATIONS,
            tokens=uncalibrated_seq_simulation.tokens,
            expected_activations=calibrated_activations,
            distribution_values=calibrated_distribution_values,
            distribution_probabilities=uncalibrated_seq_simulation.distribution_probabilities,
            uncalibrated_simulation=uncalibrated_seq_simulation,
        )
class UncalibratedNeuronSimulator(CalibratedNeuronSimulator):
    """Pass through the activations without trying to calibrate."""
    def __init__(self, uncalibrated_simulator: NeuronSimulator):
        super().__init__(uncalibrated_simulator)
    async def calibrate(self, calibration_activation_records: Sequence[ActivationRecord]) -> None:
        pass
    def _calibrate_from_flattened_activations(
        self,
        true_activations: np.ndarray,
        uncalibrated_activations: np.ndarray,
    ) -> None:
        pass
    def apply_calibration(self, values: Sequence[float]) -> list[float]:
        return values if isinstance(values, list) else list(values)
class LinearCalibratedNeuronSimulator(CalibratedNeuronSimulator):
    """Find a linear mapping from uncalibrated activations to true activations.
    Should not change ev_correlation_score because it is invariant to linear transformations.
    """
    def __init__(self, uncalibrated_simulator: NeuronSimulator):
        super().__init__(uncalibrated_simulator)
        self._regression: linear_model.LinearRegression | None = None
    def _calibrate_from_flattened_activations(
        self,
        true_activations: np.ndarray,
        uncalibrated_activations: np.ndarray,
    ) -> None:
        self._regression = linear_model.LinearRegression()
        self._regression.fit(uncalibrated_activations.reshape(-1, 1), true_activations)
    def apply_calibration(self, values: Sequence[float]) -> list[float]:
        if self._regression is None:
            raise ValueError("Must call calibrate() before apply_calibration")
        if len(values) == 0:
            return []
        return self._regression.predict(np.reshape(np.array(values), (-1, 1))).tolist()
class PercentileMatchingCalibratedNeuronSimulator(CalibratedNeuronSimulator):
    """
    Map the nth percentile of the uncalibrated activations to the nth percentile of the true
    activations for all n.
    This will match the distribution of true activations on the calibration set, but will be
    overconfident outside of the calibration set.
    """
    def __init__(self, uncalibrated_simulator: NeuronSimulator):
        super().__init__(uncalibrated_simulator)
        self._uncalibrated_activations: np.ndarray | None = None
        self._true_activations: np.ndarray | None = None
    def _calibrate_from_flattened_activations(
        self,
        true_activations: np.ndarray,
        uncalibrated_activations: np.ndarray,
    ) -> None:
        self._uncalibrated_activations = np.sort(uncalibrated_activations)
        self._true_activations = np.sort(true_activations)
    def apply_calibration(self, values: Sequence[float]) -> list[float]:
        if self._true_activations is None or self._uncalibrated_activations is None:
            raise ValueError("Must call calibrate() before apply_calibration")
        if len(values) == 0:
            return []
        return np.interp(
            np.array(values), self._uncalibrated_activations, self._true_activations
        ).tolist()

================
File: neuron_explainer/explanations/explainer.py
================
"""Uses API calls to generate explanations of neuron behavior."""
from __future__ import annotations
import logging
import re
from abc import ABC, abstractmethod
from enum import Enum
from typing import Any, Sequence
import numpy as np
from neuron_explainer.activations.activation_records import (
    calculate_max_activation,
    format_activation_records,
    non_zero_activation_proportion,
)
from neuron_explainer.activations.activations import ActivationRecord
from neuron_explainer.activations.attention_utils import (
    convert_flattened_index_to_unflattened_index,
)
from neuron_explainer.api_client import ApiClient
from neuron_explainer.explanations.few_shot_examples import (
    ATTENTION_HEAD_FEW_SHOT_EXAMPLES,
    AttentionTokenPairExample,
    FewShotExampleSet,
)
from neuron_explainer.explanations.prompt_builder import (
    ChatMessage,
    PromptBuilder,
    PromptFormat,
    Role,
)
logger = logging.getLogger(__name__)
EXPLANATION_PREFIX = "this neuron activates for"
ATTENTION_EXPLANATION_PREFIX = "this attention head"
ATTENTION_SEQUENCE_SEPARATOR = "<|sequence_separator|>"
def _split_numbered_list(text: str) -> list[str]:
    """Split a numbered list into a list of strings."""
    lines = re.split(r"\n\d+\.", text)
    # Strip the leading whitespace from each line.
    return [line.lstrip() for line in lines]
class ContextSize(int, Enum):
    TWO_K = 2049
    FOUR_K = 4097
    @classmethod
    def from_int(cls, i: int) -> ContextSize:
        for context_size in cls:
            if context_size.value == i:
                return context_size
        raise ValueError(f"{i} is not a valid ContextSize")
class NeuronExplainer(ABC):
    """
    Abstract base class for Explainer classes that generate explanations from subclass-specific
    input data.
    """
    def __init__(
        self,
        model_name: str,
        prompt_format: PromptFormat = PromptFormat.CHAT_MESSAGES,
        # This parameter lets us adjust the length of the prompt when we're generating explanations
        # using older models with shorter context windows. In the future we can use it to experiment
        # with longer context windows.
        context_size: ContextSize = ContextSize.FOUR_K,
        max_concurrent: int | None = 10,
        cache: bool = False,
    ):
        self.prompt_format = prompt_format
        self.context_size = context_size
        self.client = ApiClient(model_name=model_name, max_concurrent=max_concurrent, cache=cache)
    async def generate_explanations(
        self,
        *,
        num_samples: int = 1,
        max_tokens: int = 60,
        temperature: float = 1.0,
        top_p: float = 1.0,
        **prompt_kwargs: Any,
    ) -> list[Any]:
        """Generate explanations based on subclass-specific input data."""
        prompt = self.make_explanation_prompt(max_tokens_for_completion=max_tokens, **prompt_kwargs)
        generate_kwargs: dict[str, Any] = {
            # Using a timeout prevents the explainer from hanging if the API server is overloaded.
            "timeout": 60,
            "n": num_samples,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "top_p": top_p,
        }
        if self.prompt_format == PromptFormat.CHAT_MESSAGES:
            assert isinstance(prompt, list)
            assert isinstance(prompt[0], dict)  # Really a ChatMessage
            generate_kwargs["messages"] = prompt
        else:
            assert isinstance(prompt, str)
            generate_kwargs["prompt"] = prompt
        response = await self.client.async_generate(**generate_kwargs)
        logger.debug("response in generate_explanations is %s", response)
        if self.prompt_format == PromptFormat.CHAT_MESSAGES:
            explanations = [x["message"]["content"] for x in response["choices"]]
        elif self.prompt_format in [PromptFormat.NONE, PromptFormat.INSTRUCTION_FOLLOWING]:
            explanations = [x["text"] for x in response["choices"]]
        else:
            raise ValueError(f"Unhandled prompt format {self.prompt_format}")
        return self.postprocess_explanations(explanations, prompt_kwargs)
    @abstractmethod
    def make_explanation_prompt(self, **kwargs: Any) -> str | list[ChatMessage]:
        """
        Create a prompt to send to the API to generate one or more explanations.
        A prompt can be a simple string, or a list of ChatMessages, depending on the PromptFormat
        used by this instance.
        """
        ...
    def postprocess_explanations(
        self, completions: list[str], prompt_kwargs: dict[str, Any]
    ) -> list[Any]:
        """Postprocess the completions returned by the API into a list of explanations."""
        return completions  # no-op by default
    def _prompt_is_too_long(
        self, prompt_builder: PromptBuilder, max_tokens_for_completion: int
    ) -> bool:
        # We'll get a context size error if the prompt itself plus the maximum number of tokens for
        # the completion is longer than the context size.
        prompt_length = prompt_builder.prompt_length_in_tokens(self.prompt_format)
        if prompt_length + max_tokens_for_completion > self.context_size.value:
            print(
                f"Prompt is too long: {prompt_length} + {max_tokens_for_completion} > "
                f"{self.context_size.value}"
            )
            return True
        return False
class TokenActivationPairExplainer(NeuronExplainer):
    """
    Generate explanations of neuron behavior using a prompt with lists of token/activation pairs.
    """
    def __init__(
        self,
        model_name: str,
        prompt_format: PromptFormat = PromptFormat.CHAT_MESSAGES,
        # This parameter lets us adjust the length of the prompt when we're generating explanations
        # using older models with shorter context windows. In the future we can use it to experiment
        # with 8k+ context windows.
        context_size: ContextSize = ContextSize.FOUR_K,
        few_shot_example_set: FewShotExampleSet = FewShotExampleSet.ORIGINAL,
        repeat_non_zero_activations: bool = False,
        max_concurrent: int | None = 10,
        cache: bool = False,
    ):
        super().__init__(
            model_name=model_name,
            prompt_format=prompt_format,
            max_concurrent=max_concurrent,
            cache=cache,
        )
        self.context_size = context_size
        self.few_shot_example_set = few_shot_example_set
        self.repeat_non_zero_activations = repeat_non_zero_activations
    def make_explanation_prompt(self, **kwargs: Any) -> str | list[ChatMessage]:
        original_kwargs = kwargs.copy()
        all_activation_records: Sequence[ActivationRecord] = kwargs.pop("all_activations")
        max_activation: float = kwargs.pop("max_activation")
        kwargs.setdefault("numbered_list_of_n_explanations", None)
        numbered_list_of_n_explanations: int | None = kwargs.pop("numbered_list_of_n_explanations")
        if numbered_list_of_n_explanations is not None:
            assert numbered_list_of_n_explanations > 0, numbered_list_of_n_explanations
        # This parameter lets us dynamically shrink the prompt if our initial attempt to create it
        # results in something that's too long. It's only implemented for the 4k context size.
        kwargs.setdefault("omit_n_activation_records", 0)
        omit_n_activation_records: int = kwargs.pop("omit_n_activation_records")
        max_tokens_for_completion: int = kwargs.pop("max_tokens_for_completion")
        assert not kwargs, f"Unexpected kwargs: {kwargs}"
        prompt_builder = PromptBuilder()
        prompt_builder.add_message(
            Role.SYSTEM,
            "We're studying neurons in a neural network. Each neuron looks for some particular "
            "thing in a short document. Look at the parts of the document the neuron activates for "
            "and summarize in a single sentence what the neuron is looking for. Don't list "
            "examples of words.\n\nThe activation format is token<tab>activation. Activation "
            "values range from 0 to 10. A neuron finding what it's looking for is represented by a "
            "non-zero activation value. The higher the activation value, the stronger the match.",
        )
        few_shot_examples = self.few_shot_example_set.get_examples()
        num_omitted_activation_records = 0
        for i, few_shot_example in enumerate(few_shot_examples):
            few_shot_activation_records = few_shot_example.activation_records
            if self.context_size == ContextSize.TWO_K:
                # If we're using a 2k context window, we only have room for one activation record
                # per few-shot example. (Two few-shot examples with one activation record each seems
                # to work better than one few-shot example with two activation records, in local
                # testing.)
                few_shot_activation_records = few_shot_activation_records[:1]
            elif (
                self.context_size == ContextSize.FOUR_K
                and num_omitted_activation_records < omit_n_activation_records
            ):
                # Drop the last activation record for this few-shot example to save tokens, assuming
                # there are at least two activation records.
                if len(few_shot_activation_records) > 1:
                    print(f"Warning: omitting activation record from few-shot example {i}")
                    few_shot_activation_records = few_shot_activation_records[:-1]
                    num_omitted_activation_records += 1
            self._add_per_neuron_explanation_prompt(
                prompt_builder,
                few_shot_activation_records,
                i,
                calculate_max_activation(few_shot_example.activation_records),
                numbered_list_of_n_explanations=numbered_list_of_n_explanations,
                explanation=few_shot_example.explanation,
            )
        self._add_per_neuron_explanation_prompt(
            prompt_builder,
            # If we're using a 2k context window, we only have room for two of the activation
            # records.
            (
                all_activation_records[:2]
                if self.context_size == ContextSize.TWO_K
                else all_activation_records
            ),
            len(few_shot_examples),
            max_activation,
            numbered_list_of_n_explanations=numbered_list_of_n_explanations,
            explanation=None,
        )
        # If the prompt is too long *and* we omitted the specified number of activation records, try
        # again, omitting one more. (If we didn't make the specified number of omissions, we're out
        # of opportunities to omit records, so we just return the prompt as-is.)
        if (
            self._prompt_is_too_long(prompt_builder, max_tokens_for_completion)
            and num_omitted_activation_records == omit_n_activation_records
        ):
            original_kwargs["omit_n_activation_records"] = omit_n_activation_records + 1
            return self.make_explanation_prompt(**original_kwargs)
        return prompt_builder.build(self.prompt_format)
    def _add_per_neuron_explanation_prompt(
        self,
        prompt_builder: PromptBuilder,
        activation_records: Sequence[ActivationRecord],
        index: int,
        max_activation: float,
        # When set, this indicates that the prompt should solicit a numbered list of the given
        # number of explanations, rather than a single explanation.
        numbered_list_of_n_explanations: int | None,
        explanation: str | None,  # None means this is the end of the full prompt.
    ) -> None:
        max_activation = calculate_max_activation(activation_records)
        user_message = f"""
Neuron {index + 1}
Activations:{format_activation_records(activation_records, max_activation, omit_zeros=False)}"""
        # We repeat the non-zero activations only if it was requested and if the proportion of
        # non-zero activations isn't too high.
        if (
            self.repeat_non_zero_activations
            and non_zero_activation_proportion(activation_records, max_activation) < 0.2
        ):
            user_message += (
                f"\nSame activations, but with all zeros filtered out:"
                f"{format_activation_records(activation_records, max_activation, omit_zeros=True)}"
            )
        if numbered_list_of_n_explanations is None:
            user_message += f"\nExplanation of neuron {index + 1} behavior:"
            assistant_message = ""
            # For the IF format, we want <|endofprompt|> to come before the explanation prefix.
            if self.prompt_format == PromptFormat.INSTRUCTION_FOLLOWING:
                assistant_message += f" {EXPLANATION_PREFIX}"
            else:
                user_message += f" {EXPLANATION_PREFIX}"
            prompt_builder.add_message(Role.USER, user_message)
            if explanation is not None:
                assistant_message += f" {explanation}."
            if assistant_message:
                prompt_builder.add_message(Role.ASSISTANT, assistant_message)
        else:
            if explanation is None:
                # For the final neuron, we solicit a numbered list of explanations.
                prompt_builder.add_message(
                    Role.USER,
                    f"""\nHere are {numbered_list_of_n_explanations} possible explanations for neuron {index + 1} behavior, each beginning with "{EXPLANATION_PREFIX}":\n1. {EXPLANATION_PREFIX}""",
                )
            else:
                # For the few-shot examples, we only present one explanation, but we present it as a
                # numbered list.
                prompt_builder.add_message(
                    Role.USER,
                    f"""\nHere is 1 possible explanation for neuron {index + 1} behavior, beginning with "{EXPLANATION_PREFIX}":\n1. {EXPLANATION_PREFIX}""",
                )
                prompt_builder.add_message(Role.ASSISTANT, f" {explanation}.")
    def postprocess_explanations(
        self, completions: list[str], prompt_kwargs: dict[str, Any]
    ) -> list[Any]:
        """Postprocess the explanations returned by the API"""
        numbered_list_of_n_explanations = prompt_kwargs.get("numbered_list_of_n_explanations")
        if numbered_list_of_n_explanations is None:
            return completions
        else:
            all_explanations = []
            for completion in completions:
                for explanation in _split_numbered_list(completion):
                    if explanation.startswith(EXPLANATION_PREFIX):
                        explanation = explanation[len(EXPLANATION_PREFIX) :]
                    all_explanations.append(explanation.strip())
            return all_explanations
def format_attention_head_token_pairs(
    token_pair_examples: list[AttentionTokenPairExample], omit_zeros: bool = False
) -> str:
    if omit_zeros:
        return ", ".join(
            [
                ", ".join(
                    [
                        f"({example.tokens[coords[1]]}, {example.tokens[coords[0]]})"
                        for coords in example.token_pair_coordinates
                    ]
                )
                for example in token_pair_examples
            ]
        )
    else:
        return f"\n{ATTENTION_SEQUENCE_SEPARATOR}\n".join(
            [
                f"\n{ATTENTION_SEQUENCE_SEPARATOR}\n".join(
                    [
                        f"{format_attention_head_token_pair_string(example.tokens, coords)}"
                        for coords in example.token_pair_coordinates
                    ]
                )
                for example in token_pair_examples
            ]
        )
def format_attention_head_token_pair_string(
    token_list: list[str], pair_coordinates: tuple[int, int]
) -> str:
    def format_activated_token(i: int, token: str) -> str:
        if i == pair_coordinates[0] and i == pair_coordinates[1]:
            return f"[[**{token}**]]"  # from and to
        if i == pair_coordinates[0]:
            return f"[[{token}]]"  # from
        if i == pair_coordinates[1]:
            return f"**{token}**"  # to
        return token
    return "".join([format_activated_token(i, token) for i, token in enumerate(token_list)])
def get_top_attention_coordinates(
    activation_records: list[ActivationRecord], top_k: int = 5
) -> list[tuple[int, float, tuple[int, int]]]:
    candidates = []
    for i, record in enumerate(activation_records):
        top_activation_flat_indices = np.argsort(record.activations)[::-1][:top_k]
        top_vals: list[float] = [record.activations[idx] for idx in top_activation_flat_indices]
        top_coordinates = [
            convert_flattened_index_to_unflattened_index(flat_index)
            for flat_index in top_activation_flat_indices
        ]
        candidates.extend(
            [(i, top_val, coords) for top_val, coords in zip(top_vals, top_coordinates)]
        )
    return sorted(candidates, key=lambda x: x[1], reverse=True)[:top_k]
class AttentionHeadExplainer(NeuronExplainer):
    """
    Generate explanations of attention head behavior using a prompt with lists of
    strongly attending to/from token pairs.
    Takes in NeuronRecord's corresponding to a single attention head. Extracts strongly
    activating to/from token pairs.
    """
    def __init__(
        self,
        model_name: str,
        prompt_format: PromptFormat = PromptFormat.CHAT_MESSAGES,
        # This parameter lets us adjust the length of the prompt when we're generating explanations
        # using older models with shorter context windows. In the future we can use it to experiment
        # with 8k+ context windows.
        context_size: ContextSize = ContextSize.FOUR_K,
        repeat_strongly_attending_pairs: bool = False,
        max_concurrent: int | None = 10,
        cache: bool = False,
    ):
        super().__init__(
            model_name=model_name,
            prompt_format=prompt_format,
            max_concurrent=max_concurrent,
            cache=cache,
        )
        assert (
            context_size != ContextSize.TWO_K
        ), "2k context size not supported for attention explanation"
        self.context_size = context_size
        self.repeat_strongly_attending_pairs = repeat_strongly_attending_pairs
    def make_explanation_prompt(self, **kwargs: Any) -> str | list[ChatMessage]:
        original_kwargs = kwargs.copy()
        all_activation_records: list[ActivationRecord] = kwargs.pop("all_activations")
        # This parameter lets us dynamically shrink the prompt if our initial attempt to create it
        # results in something that's too long.
        kwargs.setdefault("omit_n_token_pair_examples", 0)
        omit_n_token_pair_examples: int = kwargs.pop("omit_n_token_pair_examples")
        max_tokens_for_completion: int = kwargs.pop("max_tokens_for_completion")
        kwargs.setdefault("num_top_pairs_to_display", 0)
        num_top_pairs_to_display: int = kwargs.pop("num_top_pairs_to_display")
        assert not kwargs, f"Unexpected kwargs: {kwargs}"
        prompt_builder = PromptBuilder()
        prompt_builder.add_message(
            Role.SYSTEM,
            "We're studying attention heads in a neural network. Each head looks at every pair of tokens "
            "in a short token sequence and activates for pairs of tokens that fit what it is looking for. "
            "Attention heads always attend from a token to a token earlier in the sequence (or from a "
            'token to itself). We will display multiple instances of sequences with the "to" token '
            'surrounded by double asterisks (e.g., **token**) and the "from" token surrounded by double '
            "square brackets (e.g., [[token]]). If a token attends from itself to itself, it will be "
            "surrounded by both (e.g., [[**token**]]). Look at the pairs of tokens the head activates for "
            "and summarize in a single sentence what pattern the head is looking for. We do not display "
            "every activating pair of tokens in a sequence; you must generalize from limited examples. "
            "Remember, the head always attends to tokens earlier in the sentence (marked with ** **) from "
            "tokens later in the sentence (marked with [[ ]]), except when the head attends from a token to "
            'itself (marked with [[** **]]). The explanation takes the form: "This attention head attends '
            "to {pattern of tokens marked with ** **, which appear earlier} from {pattern of tokens marked with "
            '[[ ]], which appear later}." The explanation does not include any of the markers (** **, [[ ]]), '
            f"as these are just for your reference. Sequences are separated by `{ATTENTION_SEQUENCE_SEPARATOR}`.",
        )
        num_omitted_token_pair_examples = 0
        for i, few_shot_example in enumerate(ATTENTION_HEAD_FEW_SHOT_EXAMPLES):
            few_shot_token_pair_examples = few_shot_example.token_pair_examples
            if num_omitted_token_pair_examples < omit_n_token_pair_examples:
                # Drop the last activation record for this few-shot example to save tokens, assuming
                # there are at least two activation records.
                if len(few_shot_token_pair_examples) > 1:
                    print(f"Warning: omitting activation record from few-shot example {i}")
                    few_shot_token_pair_examples = few_shot_token_pair_examples[:-1]
                    num_omitted_token_pair_examples += 1
            few_shot_explanation: str = few_shot_example.explanation
            self._add_per_head_explanation_prompt(
                prompt_builder,
                few_shot_token_pair_examples,
                i,
                explanation=few_shot_explanation,
            )
        # each element is (record_index, attention value, (from_token_index, to_token_index))
        coords = get_top_attention_coordinates(
            all_activation_records, top_k=num_top_pairs_to_display
        )
        prompt_examples = {}
        for record_index, _, (from_token_index, to_token_index) in coords:
            if record_index not in prompt_examples:
                prompt_examples[record_index] = AttentionTokenPairExample(
                    tokens=all_activation_records[record_index].tokens,
                    token_pair_coordinates=[(from_token_index, to_token_index)],
                )
            else:
                prompt_examples[record_index].token_pair_coordinates.append(
                    (from_token_index, to_token_index)
                )
        current_head_token_pair_examples = list(prompt_examples.values())
        self._add_per_head_explanation_prompt(
            prompt_builder,
            current_head_token_pair_examples,
            len(ATTENTION_HEAD_FEW_SHOT_EXAMPLES),
            explanation=None,
        )
        # If the prompt is too long *and* we omitted the specified number of activation records, try
        # again, omitting one more. (If we didn't make the specified number of omissions, we're out
        # of opportunities to omit records, so we just return the prompt as-is.)
        if (
            self._prompt_is_too_long(prompt_builder, max_tokens_for_completion)
            and num_omitted_token_pair_examples == omit_n_token_pair_examples
        ):
            original_kwargs["omit_n_token_pair_examples"] = omit_n_token_pair_examples + 1
            return self.make_explanation_prompt(**original_kwargs)
        return prompt_builder.build(self.prompt_format)
    def _add_per_head_explanation_prompt(
        self,
        prompt_builder: PromptBuilder,
        token_pair_examples: list[
            AttentionTokenPairExample
        ],  # each dict has keys "tokens" and "token_pair_coordinates"
        index: int,
        explanation: str | None,  # None means this is the end of the full prompt.
    ) -> None:
        user_message = f"""
Attention head {index + 1}
Activations:\n{format_attention_head_token_pairs(token_pair_examples, omit_zeros=False)}"""
        if self.repeat_strongly_attending_pairs:
            user_message += (
                f"\nThe same list of strongly activating token pairs, presented as (to_token, from_token):"
                f"{format_attention_head_token_pairs(token_pair_examples, omit_zeros=True)}"
            )
        user_message += f"\nExplanation of attention head {index + 1} behavior:"
        assistant_message = ""
        # For the IF format, we want <|endofprompt|> to come before the explanation prefix.
        if self.prompt_format == PromptFormat.INSTRUCTION_FOLLOWING:
            assistant_message += f" {ATTENTION_EXPLANATION_PREFIX}"
        else:
            user_message += f" {ATTENTION_EXPLANATION_PREFIX}"
        prompt_builder.add_message(Role.USER, user_message)
        if explanation is not None:
            assistant_message += f" {explanation}."
        if assistant_message:
            prompt_builder.add_message(Role.ASSISTANT, assistant_message)

================
File: neuron_explainer/explanations/explanations.py
================
# Dataclasses and enums for storing neuron explanations, their scores, and related data. Also,
# related helper functions.
from __future__ import annotations
import json
import math
import os.path as osp
from dataclasses import dataclass
from enum import Enum
from typing import Any
from neuron_explainer.activations.activations import NeuronId
from neuron_explainer.fast_dataclasses import FastDataclass, loads, register_dataclass
from neuron_explainer.file_utils import CustomFileHandler, file_exists, read_single_async
class ActivationScale(str, Enum):
    """Which "units" are stored in the expected_activations/distribution_values fields of a
    SequenceSimulation.
    This enum identifies whether the values represent real activations of the neuron or something
    else. Different scales are not necessarily related by a linear transformation.
    """
    NEURON_ACTIVATIONS = "neuron_activations"
    """Values represent real activations of the neuron."""
    SIMULATED_NORMALIZED_ACTIVATIONS = "simulated_normalized_activations"
    """
    Values represent simulated activations of the neuron, normalized to the range [0, 10]. This
    scale is arbitrary and should not be interpreted as a neuron activation.
    """
    HUMAN_PREDICTED_ACTIVATIONS = "human_predicted_activations"
    """
    Values represent human predictions of the neuron's activation, normalized to the range [0, 2]:
        0=not active
        1=weakly/possibly active
        2=strongly/definitely active
    Not used at present.
    """
@register_dataclass
@dataclass
class SequenceSimulation(FastDataclass):
    """The result of a simulation of neuron activations on one text sequence."""
    tokens: list[str]
    """The sequence of tokens that was simulated."""
    expected_activations: list[float]
    """Expected value of the possibly-normalized activation for each token in the sequence."""
    activation_scale: ActivationScale
    """What scale is used for values in the expected_activations field."""
    distribution_values: list[list[float]]
    """
    For each token in the sequence, a list of values from the discrete distribution of activations
    produced from simulation. Tokens will be included here if and only if they are in the top K=15
    tokens predicted by the simulator, and excluded otherwise.
    May be transformed to another unit by calibration. When we simulate a neuron, we produce a
    discrete distribution with values in the arbitrary discretized space of the neuron, e.g. 10%
    chance of 0, 70% chance of 1, 20% chance of 2. Which we store as distribution_values =
    [0, 1, 2], distribution_probabilities = [0.1, 0.7, 0.2]. When we tranform the distribution to
    the real activation units, we can correspondingly tranform the values of this distribution
    to get a distribution in the units of the neuron. e.g. if the mapping from the discretized space
    to the real activation unit of the neuron is f(x) = x/2, then the distribution becomes 10%
    chance of 0, 70% chance of 0.5, 20% chance of 1. Which we store as distribution_values =
    [0, 0.5, 1], distribution_probabilities = [0.1, 0.7, 0.2].
    """
    distribution_probabilities: list[list[float]]
    """
    For each token in the sequence, the probability of the corresponding value in
    distribution_values.
    """
    uncalibrated_simulation: "SequenceSimulation" | None = None
    """The result of the simulation before calibration."""
SequenceSimulation.field_renamed("unit", "activation_scale")
SequenceSimulation.field_deleted("response")
@register_dataclass
@dataclass
class ScoredSequenceSimulation(FastDataclass):
    """
    SequenceSimulation result with a score (for that sequence only) and ground truth activations.
    """
    sequence_simulation: SequenceSimulation
    """The result of a simulation of neuron activations."""
    true_activations: list[float]
    """Ground truth activations on the sequence (not normalized)"""
    ev_correlation_score: float
    """
    Correlation coefficient between the expected values of the normalized activations from the
    simulation and the unnormalized true activations of the neuron on the text sequence.
    """
    rsquared_score: float | None = None
    """R^2 of the simulated activations."""
    absolute_dev_explained_score: float | None = None
    """
    Score based on absolute difference between real and simulated activations.
    absolute_dev_explained_score = 1 - mean(abs(real-predicted))/ mean(abs(real))
    """
    def __eq__(self, other: Any) -> bool:
        if not isinstance(other, ScoredSequenceSimulation):
            return False
        if len(self.__dict__.keys()) != len(other.__dict__.keys()):
            return False
        # Since NaN != NaN in Python, we need to make an exception for this case when checking for equality
        # of two ScoredSequenceSimulation objects.
        for field_name in self.__dict__.keys():
            if field_name not in other.__dict__:
                return False
            self_val, other_val = self.__dict__[field_name], other.__dict__[field_name]
            if self_val != other_val:
                if not (
                    isinstance(self_val, float)
                    and math.isnan(self_val)
                    and isinstance(other_val, float)
                    and math.isnan(other_val)
                ):
                    return False
        return True
ScoredSequenceSimulation.field_renamed("simulation", "sequence_simulation")
@register_dataclass
@dataclass
class ScoredSimulation(FastDataclass):
    """Result of scoring a neuron simulation on multiple sequences."""
    scored_sequence_simulations: list[ScoredSequenceSimulation]
    """ScoredSequenceSimulation for each sequence"""
    ev_correlation_score: float | None = None
    """
    Correlation coefficient between the expected values of the normalized activations from the
    simulation and the unnormalized true activations on a dataset created from all score_results.
    (Note that this is not equivalent to averaging across sequences.)
    """
    rsquared_score: float | None = None
    """R^2 of the simulated activations."""
    absolute_dev_explained_score: float | None = None
    """
    Score based on absolute difference between real and simulated activations.
    absolute_dev_explained_score = 1 - mean(abs(real-predicted))/ mean(abs(real)).
    """
    def get_preferred_score(self) -> float | None:
        """
        This method may return None in cases where the score is undefined, for example if the
        normalized activations were all zero, yielding a correlation coefficient of NaN.
        """
        return self.ev_correlation_score
@register_dataclass
@dataclass
class ScoredExplanation(FastDataclass):
    """Simulator parameters and the results of scoring it on multiple sequences"""
    explanation: str
    scored_simulation: ScoredSimulation
    """Result of scoring the neuron simulator on multiple sequences."""
    def get_preferred_score(self) -> float | None:
        """
        This method may return None in cases where the score is undefined, for example if the
        normalized activations were all zero, yielding a correlation coefficient of NaN.
        """
        return self.scored_simulation.get_preferred_score()
ScoredExplanation.was_previously_named("ScoredExplanationOrBaseline")
ScoredExplanation.field_renamed("explanation_or_baseline", "explanation")
@register_dataclass
@dataclass
class NeuronSimulationResults(FastDataclass):
    """Simulation results and scores for a neuron."""
    neuron_id: NeuronId
    scored_explanations: list[ScoredExplanation]
NeuronSimulationResults.field_renamed("scored_explanation_or_baseline_list", "scored_explanations")
@register_dataclass
@dataclass
class AttentionSimulation(FastDataclass):
    tokens: list[str]
    token_pair_coords: tuple[int, int]
    """The coordinates of the token pair that we're simulating attention for."""
    token_pair_label: int
    """Either 0 or 1 for negative or positive label, respectively."""
    simulation_prediction: float
    """The predicted label for the token pair from the attention simulator."""
@register_dataclass
@dataclass
class ScoredAttentionSimulation(FastDataclass):
    """Result of scoring an attention head simulation on multiple sequences."""
    attention_simulations: list[AttentionSimulation]
    """ScoredSequenceSimulation for each sequence"""
    roc_auc_score: float | None = None
    """
    Area under the ROC curve for the attention predictions. Each AttentionSimulation is
    essentially a single binary classification.
    """
    def get_preferred_score(self) -> float | None:
        return self.roc_auc_score
@register_dataclass
@dataclass
class ScoredAttentionExplanation(FastDataclass):
    """Simulator parameters and the results of scoring it on multiple sequences"""
    explanation: str
    scored_attention_simulation: ScoredAttentionSimulation
    """Result of scoring the neuron simulator on multiple sequences."""
    def get_preferred_score(self) -> float | None:
        """
        This method may return None in cases where the score is undefined, for example if the
        normalized activations were all zero, yielding a correlation coefficient of NaN.
        """
        return self.scored_attention_simulation.get_preferred_score()
@register_dataclass
@dataclass
class AttentionSimulationResults(FastDataclass):
    """Simulation results and scores for an attention head."""
    # Typing this as NeuronId is not ideal but I'm not sure if we want to rename the type to something more general.
    attention_head_id: NeuronId
    scored_explanations: list[ScoredAttentionExplanation]
AttentionSimulationResults.field_renamed("attention_id", "attention_head_id")
def load_neuron_explanations(
    explanations_path: str, layer_index: str | int, neuron_index: str | int
) -> NeuronSimulationResults | None:
    """Load scored explanations for the specified neuron."""
    file = osp.join(explanations_path, str(layer_index), f"{neuron_index}.jsonl")
    if not file_exists(file):
        return None
    with CustomFileHandler(file) as f:
        for line in f:
            return loads(line)
    return None
async def load_neuron_explanations_async(
    explanations_path: str, layer_index: str | int, neuron_index: str | int
) -> NeuronSimulationResults | None:
    """Load scored explanations for the specified neuron, asynchronously."""
    return await read_explanation_file(
        osp.join(explanations_path, str(layer_index), f"{neuron_index}.jsonl")
    )
async def read_file(filename: str) -> str | None:
    """Read the contents of the given file as a string, asynchronously. File can be a
    local file or a remote file."""
    try:
        raw_contents = await read_single_async(filename)
    except FileNotFoundError:
        return None
    lines = []
    for line in raw_contents.decode("utf-8").split("\n"):
        if len(line) > 0:
            lines.append(line)
    assert len(lines) == 1, filename
    return lines[0]
async def read_explanation_file(explanation_filename: str) -> NeuronSimulationResults | None:
    """Load scored explanations from the given filename, asynchronously."""
    line = await read_file(explanation_filename)
    return loads(line) if line is not None else None
async def read_json_file(filename: str) -> dict | None:
    """Read the contents of the given file as a JSON object, asynchronously."""
    line = await read_file(filename)
    return json.loads(line) if line is not None else None

================
File: neuron_explainer/explanations/few_shot_examples.py
================
# Few-shot examples for generating and simulating neuron explanations.
from __future__ import annotations
from dataclasses import dataclass
from enum import Enum
from neuron_explainer.activations.activations import ActivationRecord
from neuron_explainer.fast_dataclasses import FastDataclass
@dataclass
class Example(FastDataclass):
    activation_records: list[ActivationRecord]
    explanation: str
    first_revealed_activation_indices: list[int]
    """
    For each activation record, the index of the first token for which the activation value in the
    prompt should be an actual number rather than "unknown".
    Examples all start with the activations rendered as "unknown", then transition to revealing
    specific normalized activation values. The goal is to lead the model to predict that activation
    sequences will eventually transition to predicting specific activation values instead of just
    "unknown". This lets us cheat and get predictions of activation values for every token in a
    single round of inference by having the activations in the sequence we're predicting always be
    "unknown" in the prompt: the model will always think that maybe the next token will be a real
    activation.
    """
    token_index_to_score: int | None = None
    """
    If the prompt is used as an example for one-token-at-a-time scoring, this is the index of the
    token to score.
    """
class FewShotExampleSet(Enum):
    """Determines which few-shot examples to use when sampling explanations."""
    ORIGINAL = "original"
    COLANGV2 = "colangv2"
    TEST = "test"
    @classmethod
    def from_string(cls, string: str) -> FewShotExampleSet:
        for example_set in FewShotExampleSet:
            if example_set.value == string:
                return example_set
        raise ValueError(f"Unrecognized example set: {string}")
    def get_examples(self) -> list[Example]:
        """Returns regular examples for use in a few-shot prompt."""
        if self is FewShotExampleSet.ORIGINAL:
            return ORIGINAL_EXAMPLES
        elif self is FewShotExampleSet.COLANGV2:
            return COLANGV2_EXAMPLES
        elif self is FewShotExampleSet.TEST:
            return TEST_EXAMPLES
        else:
            raise ValueError(f"Unhandled example set: {self}")
    def get_single_token_prediction_example(self) -> Example:
        """
        Returns an example suitable for use in a subprompt for predicting a single token's
        normalized activation, for use with the "one token at a time" scoring approach.
        """
        if self is FewShotExampleSet.COLANGV2:
            return COLANGV2_SINGLE_TOKEN_EXAMPLE
        elif self is FewShotExampleSet.TEST:
            return TEST_SINGLE_TOKEN_EXAMPLE
        else:
            raise ValueError(f"Unhandled example set: {self}")
TEST_EXAMPLES = [
    Example(
        activation_records=[
            ActivationRecord(
                tokens=["a", "b", "c"],
                activations=[1.0, 0.0, 0.0],
            ),
            ActivationRecord(
                tokens=["d", "e", "f"],
                activations=[0.0, 1.0, 0.0],
            ),
        ],
        explanation="vowels",
        first_revealed_activation_indices=[0, 1],
    ),
]
TEST_SINGLE_TOKEN_EXAMPLE = Example(
    activation_records=[
        ActivationRecord(
            activations=[0.0, 0.0, 1.0],
            tokens=["g", "h", "i"],
        ),
    ],
    first_revealed_activation_indices=[],
    token_index_to_score=2,
    explanation="test explanation",
)
ORIGINAL_EXAMPLES = [
    Example(
        activation_records=[
            ActivationRecord(
                tokens=[
                    "t",
                    "urt",
                    "ur",
                    "ro",
                    " is",
                    " fab",
                    "ulously",
                    " funny",
                    " and",
                    " over",
                    " the",
                    " top",
                    " as",
                    " a",
                    " '",
                    "very",
                    " sneaky",
                    "'",
                    " but",
                    "ler",
                    " who",
                    " excel",
                    "s",
                    " in",
                    " the",
                    " art",
                    " of",
                    " impossible",
                    " disappearing",
                    "/",
                    "re",
                    "app",
                    "earing",
                    " acts",
                ],
                activations=[
                    -0.71,
                    -1.85,
                    -2.39,
                    -2.58,
                    -1.34,
                    -1.92,
                    -1.69,
                    -0.84,
                    -1.25,
                    -1.75,
                    -1.42,
                    -1.47,
                    -1.51,
                    -0.8,
                    -1.89,
                    -1.56,
                    -1.63,
                    0.44,
                    -1.87,
                    -2.55,
                    -2.09,
                    -1.76,
                    -1.33,
                    -0.88,
                    -1.63,
                    -2.39,
                    -2.63,
                    -0.99,
                    2.83,
                    -1.11,
                    -1.19,
                    -1.33,
                    4.24,
                    -1.51,
                ],
            ),
            ActivationRecord(
                tokens=[
                    "esc",
                    "aping",
                    " the",
                    " studio",
                    " ,",
                    " pic",
                    "col",
                    "i",
                    " is",
                    " warm",
                    "ly",
                    " affecting",
                    " and",
                    " so",
                    " is",
                    " this",
                    " ad",
                    "roit",
                    "ly",
                    " minimalist",
                    " movie",
                    " .",
                ],
                activations=[
                    -0.69,
                    4.12,
                    1.83,
                    -2.28,
                    -0.28,
                    -0.79,
                    -2.2,
                    -2.03,
                    -1.77,
                    -1.71,
                    -2.44,
                    1.6,
                    -1,
                    -0.38,
                    -1.93,
                    -2.09,
                    -1.63,
                    -1.94,
                    -1.82,
                    -1.64,
                    -1.32,
                    -1.92,
                ],
            ),
        ],
        first_revealed_activation_indices=[10, 3],
        explanation="present tense verbs ending in 'ing'",
    ),
    Example(
        activation_records=[
            ActivationRecord(
                tokens=[
                    "as",
                    " sac",
                    "char",
                    "ine",
                    " movies",
                    " go",
                    " ,",
                    " this",
                    " is",
                    " likely",
                    " to",
                    " cause",
                    " massive",
                    " cardiac",
                    " arrest",
                    " if",
                    " taken",
                    " in",
                    " large",
                    " doses",
                    " .",
                ],
                activations=[
                    -0.14,
                    -1.37,
                    -0.68,
                    -2.27,
                    -1.46,
                    -1.11,
                    -0.9,
                    -2.48,
                    -2.07,
                    -3.49,
                    -2.16,
                    -1.79,
                    -0.23,
                    -0.04,
                    4.46,
                    -1.02,
                    -2.26,
                    -2.95,
                    -1.49,
                    -1.46,
                    -0.6,
                ],
            ),
            ActivationRecord(
                tokens=[
                    "shot",
                    " perhaps",
                    " '",
                    "art",
                    "istically",
                    "'",
                    " with",
                    " handheld",
                    " cameras",
                    " and",
                    " apparently",
                    " no",
                    " movie",
                    " lights",
                    " by",
                    " jo",
                    "aquin",
                    " b",
                    "aca",
                    "-",
                    "as",
                    "ay",
                    " ,",
                    " the",
                    " low",
                    "-",
                    "budget",
                    " production",
                    " swings",
                    " annoy",
                    "ingly",
                    " between",
                    " vert",
                    "igo",
                    " and",
                    " opacity",
                    " .",
                ],
                activations=[
                    -0.09,
                    -3.53,
                    -0.72,
                    -2.36,
                    -1.05,
                    -1.12,
                    -2.49,
                    -2.14,
                    -1.98,
                    -1.59,
                    -2.62,
                    -2,
                    -2.73,
                    -2.87,
                    -3.23,
                    -1.11,
                    -2.23,
                    -0.97,
                    -2.28,
                    -2.37,
                    -1.5,
                    -2.81,
                    -1.73,
                    -3.14,
                    -2.61,
                    -1.7,
                    -3.08,
                    -4,
                    -0.71,
                    -2.48,
                    -1.39,
                    -1.96,
                    -1.09,
                    4.37,
                    -0.74,
                    -0.5,
                    -0.62,
                ],
            ),
        ],
        first_revealed_activation_indices=[5, 20],
        explanation="words related to physical medical conditions",
    ),
    Example(
        activation_records=[
            # The sense of togetherness in our town is strong.
            ActivationRecord(
                tokens=[
                    "the",
                    " sense",
                    " of",
                    " together",
                    "ness",
                    " in",
                    " our",
                    " town",
                    " is",
                    " strong",
                    " .",
                ],
                activations=[
                    0,
                    0,
                    0,
                    1,
                    2,
                    0,
                    0.23,
                    0.5,
                    0,
                    0,
                    0,
                ],
            ),
            ActivationRecord(
                tokens=[
                    "a",
                    " buoy",
                    "ant",
                    " romantic",
                    " comedy",
                    " about",
                    " friendship",
                    " ,",
                    " love",
                    " ,",
                    " and",
                    " the",
                    " truth",
                    " that",
                    " we",
                    "'re",
                    " all",
                    " in",
                    " this",
                    " together",
                    " .",
                ],
                activations=[
                    -0.15,
                    -2.33,
                    -1.4,
                    -2.17,
                    -2.53,
                    -0.85,
                    0.23,
                    -1.89,
                    0.09,
                    -0.47,
                    -0.5,
                    -0.58,
                    -0.87,
                    0.22,
                    0.58,
                    1.34,
                    0.98,
                    2.21,
                    2.84,
                    1.7,
                    -0.89,
                ],
            ),
        ],
        first_revealed_activation_indices=[0, 10],
        explanation="phrases related to community",
    ),
]
COLANGV2_EXAMPLES = [
    Example(
        activation_records=[
            ActivationRecord(
                tokens=[
                    "The",
                    " editors",
                    " of",
                    " Bi",
                    "opol",
                    "ym",
                    "ers",
                    " are",
                    " delighted",
                    " to",
                    " present",
                    " the",
                    " ",
                    "201",
                    "8",
                    " Murray",
                    " Goodman",
                    " Memorial",
                    " Prize",
                    " to",
                    " Professor",
                    " David",
                    " N",
                    ".",
                    " Ber",
                    "atan",
                    " in",
                    " recognition",
                    " of",
                    " his",
                    " seminal",
                    " contributions",
                    " to",
                    " bi",
                    "oph",
                    "ysics",
                    " and",
                    " their",
                    " impact",
                    " on",
                    " our",
                    " understanding",
                    " of",
                    " charge",
                    " transport",
                    " in",
                    " biom",
                    "olecules",
                    ".\n\n",
                    "In",
                    "aug",
                    "ur",
                    "ated",
                    " in",
                    " ",
                    "200",
                    "7",
                    " in",
                    " honor",
                    " of",
                    " the",
                    " Bi",
                    "opol",
                    "ym",
                    "ers",
                    " Found",
                    "ing",
                    " Editor",
                    ",",
                    " the",
                    " prize",
                    " is",
                    " awarded",
                    " for",
                    " outstanding",
                    " accomplishments",
                ],
                activations=[
                    0,
                    0.01,
                    0.01,
                    0,
                    0,
                    0,
                    -0.01,
                    0,
                    -0.01,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0.04,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    3.39,
                    0.12,
                    0,
                    -0.01,
                    0,
                    0,
                    0,
                    0,
                    -0,
                    0,
                    -0,
                    0,
                    0,
                    -0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    -0,
                    0,
                    0,
                    -0.01,
                    0,
                    0.41,
                    0,
                    0,
                    0,
                    -0.01,
                    0,
                    0,
                    0,
                    0,
                    0,
                ],
            ),
            # We sometimes exceed the max context size when this is included :(
            # We can uncomment this if we start using an 8k context size.
            # ActivationRecord(
            #     tokens=[
            #         " We",
            #         " are",
            #         " proud",
            #         " of",
            #         " our",
            #         " national",
            #         " achievements",
            #         " in",
            #         " mastering",
            #         " all",
            #         " aspects",
            #         " of",
            #         " the",
            #         " fuel",
            #         " cycle",
            #         ".",
            #         " The",
            #         " current",
            #         " international",
            #         " interest",
            #         " in",
            #         " closing",
            #         " the",
            #         " fuel",
            #         " cycle",
            #         " is",
            #         " a",
            #         " vind",
            #         "ication",
            #         " of",
            #         " Dr",
            #         ".",
            #         " B",
            #         "hab",
            #         "ha",
            #         "s",
            #         " pioneering",
            #         " vision",
            #         " and",
            #         " genius",
            #     ],
            #     activations=[
            #         -0,
            #         -0,
            #         0,
            #         -0,
            #         -0,
            #         0,
            #         0,
            #         0,
            #         -0,
            #         0,
            #         0,
            #         -0,
            #         0,
            #         -0.01,
            #         0,
            #         0,
            #         -0,
            #         -0,
            #         0,
            #         0,
            #         0,
            #         -0,
            #         -0,
            #         -0.01,
            #         0,
            #         0,
            #         -0,
            #         0,
            #         0,
            #         0,
            #         0,
            #         0,
            #         -0,
            #         0,
            #         0,
            #         0,
            #         2.15,
            #         0,
            #         0,
            #         0.03,
            #     ],
            # ),
        ],
        first_revealed_activation_indices=[7],  # , 19],
        explanation="language related to something being groundbreaking",
    ),
    Example(
        activation_records=[
            ActivationRecord(
                tokens=[
                    '{"',
                    "widget",
                    "Class",
                    '":"',
                    "Variant",
                    "Matrix",
                    "Widget",
                    '","',
                    "back",
                    "order",
                    "Message",
                    '":"',
                    "Back",
                    "ordered",
                    '","',
                    "back",
                    "order",
                    "Message",
                    "Single",
                    "Variant",
                    '":"',
                    "This",
                    " item",
                    " is",
                    " back",
                    "ordered",
                    '.","',
                    "ordered",
                    "Selection",
                    '":',
                    "true",
                    ',"',
                    "product",
                    "Variant",
                    "Id",
                    '":',
                    "0",
                    ',"',
                    "variant",
                    "Id",
                    "Field",
                    '":"',
                    "product",
                    "196",
                    "39",
                    "_V",
                    "ariant",
                    "Id",
                    '","',
                    "back",
                    "order",
                    "To",
                    "Message",
                    "Single",
                    "Variant",
                    '":"',
                    "This",
                    " item",
                    " is",
                    " back",
                    "ordered",
                    " and",
                    " is",
                    " expected",
                    " by",
                    " {",
                    "0",
                    "}.",
                    '","',
                    "low",
                    "Price",
                    '":',
                    "999",
                    "9",
                    ".",
                    "0",
                    ',"',
                    "attribute",
                    "Indexes",
                    '":[',
                    '],"',
                    "productId",
                    '":',
                    "196",
                    "39",
                    ',"',
                    "price",
                    "V",
                    "ariance",
                    '":',
                    "true",
                    ',"',
                ],
                activations=[
                    -0.03,
                    0,
                    0,
                    0,
                    4.2,
                    0,
                    0,
                    0,
                    0,
                    -0,
                    0,
                    -0,
                    0,
                    0,
                    0,
                    0,
                    -0.01,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    -0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    -0.03,
                    0,
                    0,
                    0,
                    0,
                    -0.02,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    -0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    -0,
                    -0,
                    0,
                    0,
                    0,
                    0.01,
                    -0.01,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    -0.02,
                    0,
                    0,
                    0,
                    0,
                    0,
                    1.24,
                    0,
                    0,
                    0,
                ],
            ),
            ActivationRecord(
                tokens=[
                    "A",
                    " regular",
                    " look",
                    " at",
                    " the",
                    " ups",
                    " and",
                    " downs",
                    " of",
                    " variant",
                    " covers",
                    " in",
                    " the",
                    " comics",
                    " industry",
                    "\n\n",
                    "Here",
                    " are",
                    " the",
                    " Lego",
                    " variant",
                    " sketch",
                    " covers",
                    " by",
                    " Leon",
                    "el",
                    " Cast",
                    "ell",
                    "ani",
                    " for",
                    " a",
                    " variety",
                    " of",
                    " Marvel",
                    " titles",
                    ",",
                ],
                activations=[
                    0,
                    -0.01,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    6.52,
                    0,
                    0,
                    0,
                    -0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    1.62,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    0,
                    -0,
                    0,
                    0,
                    0,
                    -0,
                    0,
                ],
            ),
        ],
        first_revealed_activation_indices=[2, 8],
        explanation="the word variant and other words with the same vari root",
    ),
]
COLANGV2_SINGLE_TOKEN_EXAMPLE = Example(
    activation_records=[
        ActivationRecord(
            tokens=[
                "B",
                "10",
                " ",
                "111",
                " MON",
                "DAY",
                ",",
                " F",
                "EB",
                "RU",
                "ARY",
                " ",
                "11",
                ",",
                " ",
                "201",
                "9",
                " DON",
                "ATE",
                "fake higher scoring token",  # See below.
            ],
            activations=[
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0,
                0.37,
                # This fake activation makes the previous token's activation normalize to 8, which
                # might help address overconfidence in "10" activations for the one-token-at-a-time
                # scoring prompt. This value and the associated token don't actually appear anywhere
                # in the prompt.
                0.45,
            ],
        ),
    ],
    first_revealed_activation_indices=[],
    token_index_to_score=18,
    explanation="instances of the token 'ate' as part of another word",
)
@dataclass
class AttentionSimulationExample(FastDataclass):
    token_pair_example_index: int
    token_pair_coordinates: tuple[int, int]
    label: int
@dataclass
class AttentionTokenPairExample(FastDataclass):
    tokens: list[str]
    token_pair_coordinates: list[tuple[int, int]]
@dataclass
class AttentionHeadFewShotExample(FastDataclass):
    token_pair_examples: list[AttentionTokenPairExample]
    explanation: str
    simulation_examples: list[AttentionSimulationExample] | None = None
ATTENTION_HEAD_FEW_SHOT_EXAMPLES: list[AttentionHeadFewShotExample] = [
    # gpt2-xl, layer 1, head 1
    AttentionHeadFewShotExample(
        token_pair_examples=[
            AttentionTokenPairExample(
                tokens=[
                    " dreams",
                    " of",
                    " a",
                    " future",
                    " like",
                    " her",
                    " biggest",
                    " idol",
                    ",",
                    " who",
                    " was",
                    " also",
                    " born",
                    " visually",
                    " impaired",
                    ".",
                    "\n",
                    "\n",
                    '"',
                    "My",
                    " ultimate",
                    " dream",
                    " would",
                    " be",
                    " to",
                    " sing",
                    " at",
                    " Carol",
                    "s",
                    " [",
                    "by",
                    " Candle",
                    "light",
                    "]",
                    " and",
                    " to",
                    " become",
                    " a",
                    " famous",
                    " musician",
                    " like",
                    " Andrea",
                    " Bo",
                    "cell",
                    "i",
                    " ...",
                    " and",
                    " to",
                    " show",
                    " people",
                    " that",
                    " if",
                    " you",
                    " have",
                    " a",
                    " disability",
                    " it",
                    " doesn",
                    "'t",
                    " matter",
                    ',"',
                    " she",
                    " said",
                    ".",
                ],
                # 45 = "attended from", 33 = "attended to"
                token_pair_coordinates=[(45, 33)],
            ),
            AttentionTokenPairExample(
                tokens=[
                    "omes",
                    " Ever",
                    " Sequ",
                    "enced",
                    "]",
                    "\n",
                    "\n",
                    "One",
                    " mystery",
                    " of",
                    " cat",
                    " development",
                    " is",
                    " how",
                    " cats",
                    " have",
                    " come",
                    " to",
                    " have",
                    " such",
                    " varied",
                    " coats",
                    ",",
                    " from",
                    " solid",
                    " colours",
                    " to",
                    ' "',
                    "mac",
                    "ke",
                    "rel",
                    '"',
                    " tab",
                    "by",
                    " patterns",
                    " of",
                    " thin",
                    " vertical",
                    " stripes",
                    ".",
                    " The",
                    " researchers",
                    " were",
                    " particularly",
                    " interested",
                    " in",
                    " what",
                    " turns",
                    " the",
                    " mac",
                    "ke",
                    "rel",
                    " pattern",
                    " into",
                    " a",
                    ' "',
                    "bl",
                    "ot",
                    "ched",
                    '"',
                    " tab",
                    "by",
                    " pattern",
                    ",",
                ],
                token_pair_coordinates=[(5, 4)],
            ),
            AttentionTokenPairExample(
                tokens=[
                    ",",
                    " 6",
                    ",",
                    " 8",
                    ",",
                    " 4",
                    "]",
                    "':",
                    "rb",
                    ".",
                    "sort",
                    ".",
                    "slice",
                    "(",
                    "1",
                    ",",
                    "2",
                    ");",
                    " #",
                    " More",
                    " advanced",
                    ",",
                    " this",
                    " is",
                    " Ruby",
                    "'s",
                    " map",
                    " and",
                    " each",
                    "_",
                    "with",
                    "_",
                    "index",
                    " #",
                    " This",
                    " shows",
                    " the",
                    " :",
                    "rb",
                    " post",
                    "fix",
                    "-",
                    "operator",
                    " sugar",
                    " instead",
                    " of",
                    " EV",
                    "AL",
                    ' "[',
                    "1",
                    ",",
                    "2",
                    ",",
                    "3",
                    ",",
                    "4",
                    "]",
                    '":',
                    "rb",
                    " .",
                    "map",
                    "(",
                    "->",
                    " $",
                ],
                token_pair_coordinates=[(7, 6)],
            ),
            AttentionTokenPairExample(
                tokens=[
                    " him",
                    " a",
                    " W",
                    "N",
                    " [",
                    "white",
                    " nationalist",
                    "]",
                    " until",
                    " there",
                    " is",
                    " an",
                    " indication",
                    " as",
                    " such",
                    "...",
                    " The",
                    " fact",
                    " that",
                    " he",
                    " targeted",
                    " a",
                    " church",
                    " gives",
                    " me",
                    " an",
                    " ink",
                    "ling",
                    " that",
                    " it",
                    " was",
                    " religion",
                    "-",
                    "related",
                    ',"',
                    " wrote",
                    " White",
                    "Virgin",
                    "ian",
                    ".",
                    "\n",
                    "\n",
                    '"',
                    "Yep",
                    ",",
                    " bad",
                    " news",
                    " for",
                    " gun",
                    " rights",
                    " advocates",
                    " as",
                    " well",
                    ',"',
                    " wrote",
                    " math",
                    "the",
                    "ory",
                    "l",
                    "over",
                    "2008",
                    ".",
                    ' "',
                    "Another",
                ],
                token_pair_coordinates=[(15, 7)],
            ),
            AttentionTokenPairExample(
                tokens=[
                    "23",
                    "]",
                    "\n",
                    "\n",
                    "While",
                    " preparing",
                    " to",
                    " take",
                    " the",
                    " fight",
                    " to",
                    " Prim",
                    "ord",
                    "us",
                    ",",
                    " B",
                    "alth",
                    "azar",
                    " learned",
                    " about",
                    " T",
                    "aim",
                    "i",
                    "'s",
                    " machine",
                    " and",
                    " how",
                    " it",
                    " could",
                    " supposedly",
                    " kill",
                    " two",
                    " Elder",
                    " Dragons",
                    " with",
                    " a",
                    " single",
                    " blow",
                    ",",
                    " which",
                    " p",
                    "iqu",
                    "ed",
                    " his",
                    " interest",
                    ".",
                    " This",
                    " piece",
                    " of",
                    " news",
                    ",",
                    " as",
                    " well",
                    " as",
                    " Mar",
                    "j",
                    "ory",
                    "'s",
                    " sudden",
                    " departure",
                    " from",
                    " his",
                    " side",
                    " which",
                ],
                token_pair_coordinates=[(3, 1)],
            ),
        ],
        explanation="attends to the latest closing square bracket from arbitrary subsequent tokens",
    ),
    # gpt2-xl, layer 2, head 8
    AttentionHeadFewShotExample(
        simulation_examples=[
            AttentionSimulationExample(
                token_pair_example_index=0,
                token_pair_coordinates=(63, 15),
                label=0,
            ),
            AttentionSimulationExample(
                token_pair_example_index=0,
                token_pair_coordinates=(50, 15),
                label=1,
            ),
        ],
        token_pair_examples=[
            AttentionTokenPairExample(
                tokens=[
                    " he",
                    " said",
                    ".",
                    ' "',
                    "Coming",
                    " off",
                    " winning",
                    " the",
                    " year",
                    " before",
                    ",",
                    " I",
                    " love",
                    " playing",
                    " links",
                    " golf",
                    ",",
                    " and",
                    " I",
                    " love",
                    " playing",
                    " the",
                    " week",
                    " before",
                    " a",
                    " major",
                    ".",
                    " It",
                    " was",
                    " tough",
                    " to",
                    " miss",
                    " it",
                    ".",
                    " I",
                    "'m",
                    " just",
                    " glad",
                    " to",
                    " be",
                    " back",
                    '."',
                    "\n",
                    "\n",
                    "F",
                    "owler",
                    " out",
                    "played",
                    " his",
                    " partners",
                    " Rory",
                    " Mc",
                    "Il",
                    "roy",
                    " (",
                    "74",
                    ")",
                    " and",
                    " Hen",
                    "rik",
                    " St",
                    "enson",
                    " (",
                    "72",
                ],
                token_pair_coordinates=[(50, 15)],
            ),
            AttentionTokenPairExample(
                tokens=[
                    " Club",
                    ":",
                    "\n",
                    "\n",
                    "1",
                    ".",
                    " World",
                    " renowned",
                    " golf",
                    " course",
                    "\n",
                    "\n",
                    "2",
                    ".",
                    " Vern",
                    " Mor",
                    "com",
                    " designed",
                    " golf",
                    " course",
                    "\n",
                    "\n",
                    "3",
                    ".",
                    " Great",
                    " family",
                    " holiday",
                    " destination",
                    "\n",
                    "\n",
                    "4",
                    ".",
                    " Play",
                    " amid",
                    " our",
                    " resident",
                    " Eastern",
                    " Grey",
                    " k",
                    "ang",
                    "aroo",
                    " population",
                    "\n",
                    "\n",
                    "5",
                    ".",
                    " Terr",
                    "ific",
                    " friendly",
                    " staff",
                    "\n",
                    "\n",
                    "6",
                    ".",
                    " Natural",
                    " pictures",
                    "que",
                    " bush",
                    " setting",
                    "\n",
                    "\n",
                    "7",
                    ".",
                    " L",
                ],
                token_pair_coordinates=[(9, 8)],
            ),
            AttentionTokenPairExample(
                tokens=[
                    "615",
                    " rpm",
                    " on",
                    " average",
                    ").",
                    " As",
                    " a",
                    " result",
                    ",",
                    " each",
                    " player",
                    " was",
                    " hitting",
                    " longer",
                    " drives",
                    " on",
                    " their",
                    " best",
                    " shots",
                    ",",
                    " while",
                    " achieving",
                    " a",
                    " stra",
                    "ighter",
                    " ball",
                    " flight",
                    " that",
                    " was",
                    " less",
                    " affected",
                    " by",
                    " wind",
                    ".",
                    "\n",
                    "\n",
                    "Every",
                    " Golf",
                    "WR",
                    "X",
                    " Member",
                    " gained",
                    " yard",
                    "age",
                    " with",
                    " a",
                    " new",
                    " Taylor",
                    "Made",
                    " driver",
                    ";",
                    " the",
                    " largest",
                    " distance",
                    " gain",
                    " was",
                    " an",
                    " impressive",
                    " +",
                    "10",
                    ".",
                    "1",
                    " yards",
                    ",",
                ],
                token_pair_coordinates=[(47, 37)],
            ),
            AttentionTokenPairExample(
                tokens=[
                    " of",
                    " being",
                    "?",
                    " Well",
                    ",",
                    " having",
                    " perfected",
                    " the",
                    " art",
                    " of",
                    " swimming",
                    ",",
                    " Phelps",
                    " has",
                    " moved",
                    " on",
                    " to",
                    " another",
                    " cherished",
                    " summer",
                    " past",
                    "ime",
                    " ",
                    " golf",
                    ".",
                    " Here",
                    " he",
                    " is",
                    " participating",
                    " in",
                    " the",
                    " Dun",
                    "hill",
                    " Links",
                    " Championship",
                    " at",
                    " Kings",
                    "b",
                    "arn",
                    "s",
                    " in",
                    " Scotland",
                    " today",
                    ".",
                    " The",
                    " greens",
                    " over",
                    " there",
                    " are",
                    " really",
                    " big",
                    ",",
                    " so",
                    " the",
                    " opportunity",
                    " for",
                    " 50",
                    "-",
                    "yard",
                    " put",
                    "ts",
                    " exist",
                    ".",
                    " Of",
                ],
                token_pair_coordinates=[(45, 23)],
            ),
            AttentionTokenPairExample(
                tokens=[
                    "OTUS",
                    " is",
                    " getting",
                    " to",
                    " see",
                    " aliens",
                    ".",
                    "\n",
                    "\n",
                    "RELATED",
                    ":",
                    " Barack",
                    " Obama",
                    " joins",
                    " second",
                    " D",
                    ".",
                    "C",
                    ".-",
                    "area",
                    " golf",
                    " club",
                    "\n",
                    "\n",
                    '"',
                    "He",
                    " goes",
                    ",",
                    " '",
                    "they",
                    "'re",
                    " freaking",
                    " crazy",
                    " looking",
                    ".'",
                    " And",
                    " then",
                    " he",
                    " walks",
                    " up",
                    ",",
                    " makes",
                    " his",
                    " put",
                    "t",
                    ",",
                    " turns",
                    " back",
                    ",",
                    " walks",
                    " off",
                    " the",
                    " green",
                    ",",
                    " leaves",
                    " it",
                    " at",
                    " that",
                    " and",
                    " gives",
                    " me",
                    " a",
                    " wink",
                    ',"',
                ],
                token_pair_coordinates=[(52, 20)],
            ),
        ],
        explanation='attends to the token "golf" from golf-related tokens',
    ),
    # gpt2-xl, layer 1, head 10
    AttentionHeadFewShotExample(
        simulation_examples=[
            AttentionSimulationExample(
                token_pair_example_index=0,
                token_pair_coordinates=(37, 36),
                label=0,
            ),
            AttentionSimulationExample(
                token_pair_example_index=0,
                token_pair_coordinates=(14, 12),
                label=1,
            ),
        ],
        token_pair_examples=[
            AttentionTokenPairExample(
                tokens=[
                    " security",
                    " by",
                    " requiring",
                    " the",
                    " user",
                    " to",
                    " enter",
                    " a",
                    " numeric",
                    " code",
                    " sent",
                    " to",
                    " his",
                    " or",
                    " her",
                    " cellphone",
                    " in",
                    " addition",
                    " to",
                    " a",
                    " password",
                    ".",
                    " A",
                    " lot",
                    " of",
                    " websites",
                    " have",
                    " offered",
                    " this",
                    " feature",
                    " for",
                    " years",
                    ",",
                    " but",
                    " Int",
                    "uit",
                    " just",
                    " made",
                    " it",
                    " widely",
                    " available",
                    " earlier",
                    " this",
                    " year",
                    ".",
                    "\n",
                    "\n",
                    '"',
                    "When",
                    " you",
                    " give",
                    " your",
                    " most",
                    " sensitive",
                    " data",
                    " and",
                    " that",
                    " of",
                    " your",
                    " family",
                    " to",
                    " a",
                    " company",
                    ",",
                ],
                token_pair_coordinates=[(14, 12)],
            ),
            AttentionTokenPairExample(
                tokens=[
                    " 3",
                    " months",
                    ",",
                    " they",
                    " separated",
                    " the",
                    " men",
                    " and",
                    " women",
                    " here",
                    ".",
                    " I",
                    " don",
                    "'t",
                    " know",
                    " where",
                    " they",
                    " took",
                    " the",
                    " men",
                    " and",
                    " the",
                    " children",
                    ",",
                    " but",
                    " they",
                    " took",
                    " us",
                    " women",
                    " to",
                    " Syria",
                    ".",
                    " They",
                    " kept",
                    " us",
                    " in",
                    " an",
                    " underground",
                    " prison",
                    ".",
                    " My",
                    " only",
                    " wish",
                    " is",
                    " that",
                    " my",
                    " children",
                    " and",
                    " husband",
                    " escape",
                    " ISIS",
                    ".",
                    " They",
                    " brought",
                    " us",
                    " here",
                    " from",
                    " Raqqa",
                    ",",
                    " my",
                    " sisters",
                    " from",
                    " the",
                    " PKK",
                ],
                token_pair_coordinates=[(8, 6)],
            ),
            AttentionTokenPairExample(
                tokens=[
                    " an",
                    " emphasis",
                    " on",
                    " the",
                    " pursuit",
                    " of",
                    " power",
                    " despite",
                    " interpersonal",
                    " costs",
                    '."',
                    "\n",
                    "\n",
                    "The",
                    " study",
                    ",",
                    " which",
                    " involved",
                    " over",
                    " 600",
                    " young",
                    " men",
                    " and",
                    " women",
                    ",",
                    " makes",
                    " a",
                    " strong",
                    " case",
                    " for",
                    " assessing",
                    " such",
                    " traits",
                    " as",
                    ' "',
                    "r",
                    "uth",
                    "less",
                    " ambition",
                    ',"',
                    ' "',
                    "dis",
                    "comfort",
                    " with",
                    " leadership",
                    '"',
                    " and",
                    ' "',
                    "hub",
                    "rist",
                    "ic",
                    " pride",
                    '"',
                    " to",
                    " understand",
                    " psychopath",
                    "ologies",
                    ".",
                    "\n",
                    "\n",
                    "The",
                    " researchers",
                    " looked",
                    " at",
                ],
                token_pair_coordinates=[(23, 21)],
            ),
            AttentionTokenPairExample(
                tokens=[
                    " 4",
                    " hours",
                    ".",
                    " These",
                    " results",
                    ",",
                    " differ",
                    " between",
                    " men",
                    " and",
                    " women",
                    ",",
                    " however",
                    ".",
                    " We",
                    " can",
                    " see",
                    " that",
                    " although",
                    " both",
                    " groups",
                    " have",
                    " a",
                    " large",
                    " cluster",
                    " of",
                    " people",
                    " at",
                    " exactly",
                    " 40",
                    " hours",
                    " per",
                    " week",
                    ",",
                    " there",
                    " are",
                    " more",
                    " men",
                    " reporting",
                    " hours",
                    " above",
                    " 40",
                    ",",
                    " whereas",
                    " there",
                    " are",
                    " more",
                    " women",
                    " reporting",
                    " hours",
                    " below",
                    " 40",
                    ".",
                    " Result",
                    " 3",
                    ":",
                    " Male",
                    " Hours",
                    " Work",
                    "ed",
                    " [",
                    "Info",
                    "]",
                    " Owner",
                ],
                token_pair_coordinates=[(10, 8)],
            ),
            AttentionTokenPairExample(
                tokens=[
                    " they",
                    " were",
                    " perceived",
                    " as",
                    " more",
                    " emotional",
                    ",",
                    " which",
                    " made",
                    " participants",
                    " more",
                    " confident",
                    " in",
                    " their",
                    " own",
                    " opinion",
                    '."',
                    "\n",
                    "\n",
                    "Ms",
                    " Sal",
                    "erno",
                    " said",
                    " both",
                    " men",
                    " and",
                    " women",
                    " reacted",
                    " in",
                    " the",
                    " same",
                    " way",
                    " to",
                    " women",
                    " expressing",
                    " themselves",
                    " angrily",
                    ".",
                    "\n",
                    "\n",
                    '"',
                    "Particip",
                    "ants",
                    " confidence",
                    " in",
                    " their",
                    " own",
                    " verdict",
                    " dropped",
                    " significantly",
                    " after",
                    " male",
                    " hold",
                    "outs",
                    " expressed",
                    " anger",
                    ',"',
                    " the",
                    " paper",
                    "'s",
                    " findings",
                    " stated",
                    ".",
                    "\n",
                ],
                token_pair_coordinates=[(26, 24)],
            ),
        ],
        explanation="attends to male-related tokens from paired female-related tokens",
    ),
    # gpt2-xl, layer 1, head 3
    AttentionHeadFewShotExample(
        token_pair_examples=[
            AttentionTokenPairExample(
                tokens=[
                    "\n",
                    "********************************",
                    "************",
                    "***",
                    "\n",
                    "\n",
                    "V",
                    "iet",
                    "namese",
                    " Ministry",
                    " of",
                    " Foreign",
                    " Affairs",
                    " spokesperson",
                    " Le",
                    " Hai",
                    " Bin",
                    "h",
                    " is",
                    " seen",
                    " in",
                    " this",
                    " file",
                    " photo",
                    ".",
                    " .",
                    " Tu",
                    "oi",
                    " Tre",
                    "\n",
                    "\n",
                    "The",
                    " Ministry",
                    " of",
                    " Foreign",
                    " Affairs",
                    " has",
                    " ordered",
                    " a",
                    " thorough",
                    " investigation",
                    " into",
                    " a",
                    " case",
                    " in",
                    " which",
                    " a",
                    " Vietnamese",
                    " fisherman",
                    " was",
                    " shot",
                    " dead",
                    " on",
                    " his",
                    " boat",
                    " in",
                    " Vietnam",
                    "'s",
                    " Tru",
                    "ong",
                    " Sa",
                    " (",
                    "Spr",
                    "at",
                ],
                token_pair_coordinates=[(1, 1)],
            ),
            AttentionTokenPairExample(
                tokens=[
                    " J",
                    "okin",
                    "en",
                    " tells",
                    " a",
                    " much",
                    " different",
                    " story",
                    ".",
                    " He",
                    " almost",
                    " sounded",
                    " like",
                    " a",
                    " pitch",
                    "man",
                    ".",
                    "\n",
                    "\n",
                    '"',
                    "All",
                    " the",
                    " staff",
                    ",",
                    " team",
                    " service",
                    " guys",
                    ",",
                    " all",
                    " the",
                    " trainers",
                    ",",
                    " they",
                    "'re",
                    " unbelievable",
                    " guys",
                    ',"',
                    " said",
                    " J",
                    "okin",
                    "en",
                    ".",
                    ' "',
                    "It",
                    "'s",
                    " not",
                    " just",
                    " the",
                    " players",
                    ",",
                    " it",
                    "'s",
                    " the",
                    " staff",
                    " around",
                    " the",
                    " team",
                    ".",
                    " I",
                    " feel",
                    " really",
                    " bad",
                    " for",
                    " them",
                ],
                token_pair_coordinates=[(1, 1)],
            ),
            AttentionTokenPairExample(
                tokens=[
                    " a",
                    " Pv",
                    "E",
                    " game",
                    ",",
                    " we",
                    " probably",
                    " would",
                    " use",
                    " it",
                    " but",
                    " based",
                    " on",
                    " the",
                    " tests",
                    " we",
                    "'ve",
                    " run",
                    " on",
                    " it",
                    ",",
                    " that",
                    " wouldn",
                    "'t",
                    " be",
                    " our",
                    " first",
                    " choice",
                    " for",
                    " a",
                    " live",
                    " R",
                    "v",
                    "R",
                    " game",
                    ".",
                    " Now",
                    ",",
                    " could",
                    " we",
                    " use",
                    " it",
                    " for",
                    " prototyp",
                    "ing",
                    "?",
                    " Yep",
                    ",",
                    " we",
                    " are",
                    " already",
                    " doing",
                    " that",
                    ".",
                    " Second",
                    ",",
                    " as",
                    " to",
                    " other",
                    " engines",
                    " there",
                    " are",
                    " both",
                    " financial",
                ],
                token_pair_coordinates=[(1, 1)],
            ),
            AttentionTokenPairExample(
                tokens=[
                    "-",
                    "tun",
                    "er",
                    " is",
                    " also",
                    " custom",
                    "isable",
                    " for",
                    " hassle",
                    "-",
                    "free",
                    " experimentation",
                    ".",
                    " The",
                    " St",
                    "rix",
                    " X",
                    "399",
                    "-",
                    "E",
                    " Gaming",
                    " takes",
                    " up",
                    " to",
                    " three",
                    " double",
                    "-",
                    "wide",
                    " cards",
                    " in",
                    " SLI",
                    " or",
                    " Cross",
                    "Fire",
                    "X",
                    ".",
                    " Primary",
                    " graphics",
                    " slots",
                    " are",
                    " protected",
                    " by",
                    " Safe",
                    "Slot",
                    " from",
                    " damages",
                    " that",
                    " heavy",
                    " GPU",
                    " cool",
                    "ers",
                    " can",
                    " potentially",
                    " cause",
                    ".",
                    "\n",
                    "\n",
                    "Personal",
                    "ised",
                    " RGB",
                    " lighting",
                    " is",
                    " made",
                    " possible",
                ],
                token_pair_coordinates=[(1, 1)],
            ),
            AttentionTokenPairExample(
                tokens=[
                    " to",
                    " abolish",
                    " such",
                    " a",
                    " complex",
                    "?",
                    " Are",
                    " there",
                    " ways",
                    " ve",
                    "gans",
                    " can",
                    " eat",
                    " more",
                    " sustain",
                    "ably",
                    "?",
                    " What",
                    " are",
                    " some",
                    " of",
                    " the",
                    " health",
                    " challenges",
                    " for",
                    " new",
                    " ve",
                    "gans",
                    ",",
                    " and",
                    " how",
                    " can",
                    " we",
                    " raise",
                    " awareness",
                    " of",
                    " these",
                    " issues",
                    " so",
                    " that",
                    ",",
                    " for",
                    " instance",
                    ",",
                    " medical",
                    " professionals",
                    " are",
                    " more",
                    " supportive",
                    " of",
                    " vegan",
                    "ism",
                    "?",
                    "\n",
                    "\n",
                    "Moreover",
                    ",",
                    " it",
                    " is",
                    " essential",
                    " that",
                    " ve",
                    "gans",
                    " differentiate",
                ],
                token_pair_coordinates=[(1, 1)],
            ),
        ],
        explanation="attends from the second token in the sequence to the second token in the sequence",
    ),
]

================
File: neuron_explainer/explanations/prompt_builder.py
================
from __future__ import annotations
from enum import Enum
from typing import TypedDict
import tiktoken
class Role(str, Enum):
    """See https://platform.openai.com/docs/guides/chat"""
    SYSTEM = "system"
    USER = "user"
    ASSISTANT = "assistant"
ChatMessage = TypedDict(
    "ChatMessage",
    {
        "role": Role,
        "content": str,
    },
)
class PromptFormat(str, Enum):
    """
    Different ways of formatting the components of a prompt into the format accepted by the relevant
    API server endpoint.
    """
    NONE = "none"
    """Suitable for use with models that don't use special tokens for instructions."""
    INSTRUCTION_FOLLOWING = "instruction_following"
    """Suitable for IF models that use <|endofprompt|>."""
    CHAT_MESSAGES = "chat_messages"
    """
    Suitable for ChatGPT models that use a structured turn-taking role+content format. Generates a
    list of ChatMessage dicts that can be sent to the /chat/completions endpoint.
    """
    @classmethod
    def from_string(cls, s: str) -> PromptFormat:
        for prompt_format in cls:
            if prompt_format.value == s:
                return prompt_format
        raise ValueError(f"{s} is not a valid PromptFormat")
class PromptBuilder:
    """Class for accumulating components of a prompt and then formatting them into an output."""
    def __init__(self, allow_extra_system_messages: bool = False) -> None:
        """The `allow_extra_system_messages` instance variable allows the caller to specify that the prompt
        should be allowed to contain system messages after the very first one."""
        self._messages: list[ChatMessage] = []
        self._allow_extra_system_messages = allow_extra_system_messages
    def add_message(self, role: Role, message: str) -> None:
        self._messages.append(ChatMessage(role=role, content=message))
    def prompt_length_in_tokens(self, prompt_format: PromptFormat) -> int:
        # TODO(sbills): Make the model/encoding configurable. This implementation assumes GPT-4.
        encoding = tiktoken.get_encoding("cl100k_base")
        if prompt_format == PromptFormat.CHAT_MESSAGES:
            # Approximately-correct implementation adapted from this documentation:
            # https://platform.openai.com/docs/guides/chat/introduction
            num_tokens = 0
            for message in self._messages:
                num_tokens += (
                    4  # every message follows <|im_start|>{role/name}\n{content}<|im_end|>\n
                )
                num_tokens += len(encoding.encode(message["content"], allowed_special="all"))
            num_tokens += 2  # every reply is primed with <|im_start|>assistant
            return num_tokens
        else:
            prompt_str = self.build(prompt_format)
            assert isinstance(prompt_str, str)
            return len(encoding.encode(prompt_str, allowed_special="all"))
    def build(self, prompt_format: PromptFormat) -> str | list[ChatMessage]:
        """
        Validates the messages added so far (reasonable alternation of assistant vs. user, etc.)
        and returns either a regular string (maybe with <|endofprompt|> tokens) or a list of
        ChatMessages suitable for use with the /chat/completions endpoint.
        """
        # Create a deep copy of the messages so we can modify it and so that the caller can't
        # modify the internal state of this object.
        messages = [message.copy() for message in self._messages]
        expected_next_role = Role.SYSTEM
        for message in messages:
            role = message["role"]
            assert role == expected_next_role or (
                self._allow_extra_system_messages and role == Role.SYSTEM
            ), f"Expected message from {expected_next_role} but got message from {role}"
            if role == Role.SYSTEM:
                expected_next_role = Role.USER
            elif role == Role.USER:
                expected_next_role = Role.ASSISTANT
            elif role == Role.ASSISTANT:
                expected_next_role = Role.USER
        if prompt_format == PromptFormat.INSTRUCTION_FOLLOWING:
            last_user_message = None
            for message in messages:
                if message["role"] == Role.USER:
                    last_user_message = message
            assert last_user_message is not None
            last_user_message["content"] += "<|endofprompt|>"
        if prompt_format == PromptFormat.CHAT_MESSAGES:
            return messages
        elif prompt_format in [PromptFormat.NONE, PromptFormat.INSTRUCTION_FOLLOWING]:
            return "".join(message["content"] for message in messages)
        else:
            raise ValueError(f"Unknown prompt format: {prompt_format}")

================
File: neuron_explainer/explanations/scoring.py
================
from __future__ import annotations
import asyncio
import logging
from typing import Any, Callable, Coroutine, Sequence
import numpy as np
from neuron_explainer.activations.activations import ActivationRecord
from neuron_explainer.api_client import ApiClient
from neuron_explainer.explanations.calibrated_simulator import (
    CalibratedNeuronSimulator,
    LinearCalibratedNeuronSimulator,
    UncalibratedNeuronSimulator,
)
from neuron_explainer.explanations.explanations import (
    ScoredSequenceSimulation,
    ScoredSimulation,
    SequenceSimulation,
)
from neuron_explainer.explanations.simulator import (
    ExplanationNeuronSimulator,
    LogprobFreeExplanationTokenSimulator,
    NeuronSimulator,
)
def flatten_list(list_of_lists: Sequence[Sequence[Any]]) -> list[Any]:
    return [item for sublist in list_of_lists for item in sublist]
def correlation_score(
    real_activations: Sequence[float] | np.ndarray,
    predicted_activations: Sequence[float] | np.ndarray,
) -> float:
    score = np.corrcoef(real_activations, predicted_activations)[0, 1]
    if np.isnan(score):
        return 0.0
    return score
def score_from_simulation(
    real_activations: ActivationRecord,
    simulation: SequenceSimulation,
    score_function: Callable[[Sequence[float] | np.ndarray, Sequence[float] | np.ndarray], float],
) -> float:
    return score_function(real_activations.activations, simulation.expected_activations)
def rsquared_score_from_sequences(
    real_activations: Sequence[float] | np.ndarray,
    predicted_activations: Sequence[float] | np.ndarray,
) -> float:
    return float(
        1
        - np.mean(np.square(np.array(real_activations) - np.array(predicted_activations)))
        / np.mean(np.square(np.array(real_activations)))
    )
def absolute_dev_explained_score_from_sequences(
    real_activations: Sequence[float] | np.ndarray,
    predicted_activations: Sequence[float] | np.ndarray,
) -> float:
    return float(
        1
        - np.mean(np.abs(np.array(real_activations) - np.array(predicted_activations)))
        / np.mean(np.abs(np.array(real_activations)))
    )
async def make_uncalibrated_explanation_simulator(
    explanation: str,
    client: ApiClient,
    **kwargs: Any,
) -> CalibratedNeuronSimulator:
    """Make a simulator that doesn't apply any calibration."""
    simulator = LogprobFreeExplanationTokenSimulator(client, explanation, **kwargs)
    calibrated_simulator = UncalibratedNeuronSimulator(simulator)
    return calibrated_simulator
async def make_explanation_simulator(
    explanation: str,
    calibration_activation_records: Sequence[ActivationRecord],
    client: ApiClient,
    calibrated_simulator_class: type[CalibratedNeuronSimulator] = LinearCalibratedNeuronSimulator,
    **kwargs: Any,
) -> CalibratedNeuronSimulator:
    """
    Make a simulator that uses an explanation to predict activations and calibrates it on the given
    activation records.
    """
    simulator = ExplanationNeuronSimulator(client, explanation, **kwargs)
    calibrated_simulator = calibrated_simulator_class(simulator)
    await calibrated_simulator.calibrate(calibration_activation_records)
    return calibrated_simulator
async def _simulate_and_score_sequence(
    simulator: NeuronSimulator, activations: ActivationRecord
) -> ScoredSequenceSimulation:
    """Score an explanation of a neuron by how well it predicts activations on a sentence."""
    sequence_simulation = await simulator.simulate(activations.tokens)
    logging.debug(sequence_simulation)
    rsquared_score = score_from_simulation(
        activations, sequence_simulation, rsquared_score_from_sequences
    )
    absolute_dev_explained_score = score_from_simulation(
        activations, sequence_simulation, absolute_dev_explained_score_from_sequences
    )
    scored_sequence_simulation = ScoredSequenceSimulation(
        sequence_simulation=sequence_simulation,
        true_activations=activations.activations,
        ev_correlation_score=score_from_simulation(
            activations, sequence_simulation, correlation_score
        ),
        rsquared_score=rsquared_score,
        absolute_dev_explained_score=absolute_dev_explained_score,
    )
    return scored_sequence_simulation
def aggregate_scored_sequence_simulations(
    scored_sequence_simulations: list[ScoredSequenceSimulation],
) -> ScoredSimulation:
    """
    Aggregate a list of scored sequence simulations. The logic for doing this is non-trivial for EV
    scores, since we want to calculate the correlation over all activations from all sequences at
    once rather than simply averaging per-sequence correlations.
    """
    all_true_activations: list[float] = []
    all_expected_values: list[float] = []
    for scored_sequence_simulation in scored_sequence_simulations:
        all_true_activations.extend(scored_sequence_simulation.true_activations or [])
        all_expected_values.extend(
            scored_sequence_simulation.sequence_simulation.expected_activations
        )
    ev_correlation_score = (
        correlation_score(all_true_activations, all_expected_values)
        if len(all_true_activations) > 0
        else None
    )
    rsquared_score = rsquared_score_from_sequences(all_true_activations, all_expected_values)
    absolute_dev_explained_score = absolute_dev_explained_score_from_sequences(
        all_true_activations, all_expected_values
    )
    return ScoredSimulation(
        scored_sequence_simulations=scored_sequence_simulations,
        ev_correlation_score=ev_correlation_score,
        rsquared_score=rsquared_score,
        absolute_dev_explained_score=absolute_dev_explained_score,
    )
async def simulate_and_score(
    simulator: NeuronSimulator,
    activation_records: Sequence[ActivationRecord],
) -> ScoredSimulation:
    """
    Score an explanation of a neuron by how well it predicts activations on the given text
    sequences.
    """
    scored_sequence_simulations = await asyncio.gather(
        *[
            _simulate_and_score_sequence(
                simulator,
                activation_record,
            )
            for activation_record in activation_records
        ]
    )
    return aggregate_scored_sequence_simulations(scored_sequence_simulations)
async def make_simulator_and_score(
    make_simulator: Coroutine[None, None, NeuronSimulator],
    activation_records: Sequence[ActivationRecord],
) -> ScoredSimulation:
    """Chain together creating the simulator and using it to score activation records."""
    simulator = await make_simulator
    return await simulate_and_score(simulator, activation_records)

================
File: neuron_explainer/explanations/simulator.py
================
"""Uses API calls to simulate neuron activations based on an explanation."""
from __future__ import annotations
import asyncio
import json
import logging
from abc import ABC, abstractmethod
from collections import OrderedDict
from enum import Enum
from typing import Any, Sequence
import numpy as np
from neuron_explainer.activations.activation_records import (
    calculate_max_activation,
    format_activation_records,
    format_sequences_for_simulation,
    normalize_activations,
)
from neuron_explainer.activations.activations import ActivationRecord
from neuron_explainer.api_client import ApiClient
from neuron_explainer.explanations.explainer import EXPLANATION_PREFIX
from neuron_explainer.explanations.explanations import ActivationScale, SequenceSimulation
from neuron_explainer.explanations.few_shot_examples import FewShotExampleSet
from neuron_explainer.explanations.prompt_builder import (
    ChatMessage,
    PromptBuilder,
    PromptFormat,
    Role,
)
logger = logging.getLogger(__name__)
# Our prompts use normalized activation values, which map any range of positive activations to the
# integers from 0 to 10.
MAX_NORMALIZED_ACTIVATION = 10
VALID_ACTIVATION_TOKENS_ORDERED = [str(i) for i in range(MAX_NORMALIZED_ACTIVATION + 1)]
VALID_ACTIVATION_TOKENS = set(VALID_ACTIVATION_TOKENS_ORDERED)
class SimulationType(str, Enum):
    """How to simulate neuron activations. Values correspond to subclasses of NeuronSimulator."""
    ALL_AT_ONCE = "all_at_once"
    """
    Use a single prompt with <unknown> tokens; calculate EVs using logprobs.
    Implemented by ExplanationNeuronSimulator.
    """
    ONE_AT_A_TIME = "one_at_a_time"
    """
    Use a separate prompt for each token being simulated; calculate EVs using logprobs.
    Implemented by ExplanationTokenByTokenSimulator.
    """
    @classmethod
    def from_string(cls, s: str) -> SimulationType:
        for simulation_type in SimulationType:
            if simulation_type.value == s:
                return simulation_type
        raise ValueError(f"Invalid simulation type: {s}")
def compute_expected_value(
    norm_probabilities_by_distribution_value: OrderedDict[int, float]
) -> float:
    """
    Given a map from distribution values (integers on the range [0, 10]) to normalized
    probabilities, return an expected value for the distribution.
    """
    return np.dot(
        np.array(list(norm_probabilities_by_distribution_value.keys())),
        np.array(list(norm_probabilities_by_distribution_value.values())),
    )
def parse_top_logprobs(top_logprobs: dict[str, float]) -> OrderedDict[int, float]:
    """
    Given a map from tokens to logprobs, return a map from distribution values (integers on the
    range [0, 10]) to unnormalized probabilities (in the sense that they may not sum to 1).
    """
    probabilities_by_distribution_value = OrderedDict()
    for token, logprob in top_logprobs.items():
        if token in VALID_ACTIVATION_TOKENS:
            token_as_int = int(token)
            probabilities_by_distribution_value[token_as_int] = np.exp(logprob)
    return probabilities_by_distribution_value
def compute_predicted_activation_stats_for_token(
    top_logprobs: dict[str, float],
) -> tuple[OrderedDict[int, float], float]:
    probabilities_by_distribution_value = parse_top_logprobs(top_logprobs)
    total_p_of_distribution_values = sum(probabilities_by_distribution_value.values())
    norm_probabilities_by_distribution_value = OrderedDict(
        {
            distribution_value: p / total_p_of_distribution_values
            for distribution_value, p in probabilities_by_distribution_value.items()
        }
    )
    expected_value = compute_expected_value(norm_probabilities_by_distribution_value)
    return (
        norm_probabilities_by_distribution_value,
        expected_value,
    )
# Adapted from tether/tether/core/encoder.py.
def convert_to_byte_array(s: str) -> bytearray:
    byte_array = bytearray()
    assert s.startswith("bytes:"), s
    s = s[6:]
    while len(s) > 0:
        if s[0] == "\\":
            # Hex encoding.
            assert s[1] == "x"
            assert len(s) >= 4
            byte_array.append(int(s[2:4], 16))
            s = s[4:]
        else:
            # Regular ascii encoding.
            byte_array.append(ord(s[0]))
            s = s[1:]
    return byte_array
def handle_byte_encoding(
    response_tokens: Sequence[str], merged_response_index: int
) -> tuple[str, int]:
    """
    Handle the case where the current token is a sequence of bytes. This may involve merging
    multiple response tokens into a single token.
    """
    response_token = response_tokens[merged_response_index]
    if response_token.startswith("bytes:"):
        byte_array = bytearray()
        while True:
            byte_array = convert_to_byte_array(response_token) + byte_array
            try:
                # If we can decode the byte array as utf-8, then we're done.
                response_token = byte_array.decode("utf-8")
                break
            except UnicodeDecodeError:
                # If not, then we need to merge the previous response token into the byte
                # array.
                merged_response_index -= 1
                response_token = response_tokens[merged_response_index]
    return response_token, merged_response_index
def was_token_split(current_token: str, response_tokens: Sequence[str], start_index: int) -> bool:
    """
    Return whether current_token (a token from the subject model) was split into multiple tokens by
    the simulator model (as represented by the tokens in response_tokens). start_index is the index
    in response_tokens at which to begin looking backward to form a complete token. It is usually
    the first token *before* the delimiter that separates the token from the normalized activation,
    barring some unusual cases.
    This mainly happens if the subject model uses a different tokenizer than the simulator model.
    But it can also happen in cases where Unicode characters are split. This function handles both
    cases.
    """
    merged_response_tokens = ""
    merged_response_index = start_index
    while len(merged_response_tokens) < len(current_token):
        response_token = response_tokens[merged_response_index]
        response_token, merged_response_index = handle_byte_encoding(
            response_tokens, merged_response_index
        )
        merged_response_tokens = response_token + merged_response_tokens
        merged_response_index -= 1
    # It's possible that merged_response_tokens is longer than current_token at this point,
    # since the between-lines delimiter may have been merged into the original token. But it
    # should always be the case that merged_response_tokens ends with current_token.
    assert merged_response_tokens.endswith(current_token)
    num_merged_tokens = start_index - merged_response_index
    token_was_split = num_merged_tokens > 1
    if token_was_split:
        logger.debug(
            "Warning: token from the subject model was split into 2+ tokens by the simulator model."
        )
    return token_was_split
def parse_simulation_response(
    response: dict[str, Any],
    prompt_format: PromptFormat,
    tokens: Sequence[str],
) -> SequenceSimulation:
    """
    Parse an API response to a simulation prompt.
    Args:
        response: response from the API
        prompt_format: how the prompt was formatted
        tokens: list of tokens as strings in the sequence where the neuron is being simulated
    """
    choice = response["choices"][0]
    if prompt_format == PromptFormat.CHAT_MESSAGES:
        text = choice["message"]["content"]
    elif prompt_format in [
        PromptFormat.NONE,
        PromptFormat.INSTRUCTION_FOLLOWING,
    ]:
        text = choice["text"]
    else:
        raise ValueError(f"Unhandled prompt format {prompt_format}")
    response_tokens = choice["logprobs"]["tokens"]
    top_logprobs = choice["logprobs"]["top_logprobs"]
    token_text_offset = choice["logprobs"]["text_offset"]
    # This only works because the sequence "<start>" tokenizes into multiple tokens if it appears in
    # a text sequence in the prompt.
    scoring_start = text.rfind("<start>")
    expected_values = []
    original_sequence_tokens: list[str] = []
    distribution_values: list[list[float]] = []
    distribution_probabilities: list[list[float]] = []
    for i in range(2, len(response_tokens)):
        if len(original_sequence_tokens) == len(tokens):
            # Make sure we haven't hit some sort of off-by-one error.
            # TODO(sbills): Generalize this to handle different tokenizers.
            reached_end = response_tokens[i + 1] == "<" and response_tokens[i + 2] == "end"
            assert reached_end, f"{response_tokens[i-3:i+3]}"
            break
        if token_text_offset[i] >= scoring_start:
            # We're looking for the first token after a tab. This token should be the text
            # "unknown" if hide_activations=True or a normalized activation (0-10) otherwise.
            # If it isn't, that means that the tab is not appearing as a delimiter, but rather
            # as a token, in which case we should move on to the next response token.
            if response_tokens[i - 1] == "\t":
                if response_tokens[i] != "unknown":
                    logger.debug("Ignoring tab token that is not followed by an 'unknown' token.")
                    continue
                # j represents the index of the token in a "token<tab>activation" line, barring
                # one of the unusual cases handled below.
                j = i - 2
                current_token = tokens[len(original_sequence_tokens)]
                if current_token == response_tokens[j] or was_token_split(
                    current_token, response_tokens, j
                ):
                    # We're in the normal case where the tokenization didn't throw off the
                    # formatting or in the token-was-split case, which we handle the usual way.
                    current_top_logprobs = top_logprobs[i]
                    (
                        norm_probabilities_by_distribution_value,
                        expected_value,
                    ) = compute_predicted_activation_stats_for_token(
                        current_top_logprobs,
                    )
                    current_distribution_values = list(
                        norm_probabilities_by_distribution_value.keys()
                    )
                    current_distribution_probabilities = list(
                        norm_probabilities_by_distribution_value.values()
                    )
                else:
                    # We're in a case where the tokenization resulted in a newline being folded into
                    # the token. We can't do our usual prediction of activation stats for the token,
                    # since the model did not observe the original token. Instead, we use dummy
                    # values. See the TODO elsewhere in this file about coming up with a better
                    # prompt format that avoids this situation.
                    newline_folded_into_token = "\n" in response_tokens[j]
                    assert (
                        newline_folded_into_token
                    ), f"`{current_token=}` {response_tokens[j-3:j+3]=}"
                    logger.debug(
                        "Warning: newline before a token<tab>activation line was folded into the token"
                    )
                    current_distribution_values = []
                    current_distribution_probabilities = []
                    expected_value = 0.0
                original_sequence_tokens.append(current_token)
                # These values are ints, but for backward compatibility we store them as floats.
                distribution_values.append([float(v) for v in current_distribution_values])
                distribution_probabilities.append(current_distribution_probabilities)
                expected_values.append(expected_value)
    return SequenceSimulation(
        activation_scale=ActivationScale.SIMULATED_NORMALIZED_ACTIVATIONS,
        expected_activations=expected_values,
        distribution_values=distribution_values,
        distribution_probabilities=distribution_probabilities,
        tokens=original_sequence_tokens,
    )
class NeuronSimulator(ABC):
    """Abstract base class for simulating neuron behavior."""
    @abstractmethod
    async def simulate(self, tokens: Sequence[str]) -> SequenceSimulation:
        """Simulate the behavior of a neuron based on an explanation."""
        ...
class ExplanationNeuronSimulator(NeuronSimulator):
    """
    Simulate neuron behavior based on an explanation.
    This class uses a few-shot prompt with examples of other explanations and activations. This
    prompt allows us to score all of the tokens at once using a nifty trick involving logprobs.
    """
    def __init__(
        self,
        client: ApiClient,
        explanation: str,
        few_shot_example_set: FewShotExampleSet = FewShotExampleSet.ORIGINAL,
        prompt_format: PromptFormat = PromptFormat.CHAT_MESSAGES,
    ):
        self.client = client
        self.explanation = explanation
        self.few_shot_example_set = few_shot_example_set
        self.prompt_format = prompt_format
    async def simulate(
        self,
        tokens: Sequence[str],
    ) -> SequenceSimulation:
        prompt = self.make_simulation_prompt(tokens)
        generate_kwargs: dict[str, Any] = {
            "max_tokens": 0,
            "echo": True,
            "logprobs": 15,
            "timeout": 10,
        }
        # We can't use the CHAT_MESSAGES prompt for scoring, since it only works with the production API endpoint
        # and production no longer returns logprobs. A simulator method which doesn't require logprobs is a WIP.
        assert self.prompt_format != PromptFormat.CHAT_MESSAGES
        assert isinstance(prompt, str)
        generate_kwargs["prompt"] = prompt
        response = await self.client.async_generate(**generate_kwargs)
        logger.debug("response in score_explanation_by_activations is %s", response)
        result = parse_simulation_response(response, self.prompt_format, tokens)
        logger.debug("result in score_explanation_by_activations is %s", result)
        return result
    # TODO(sbills): The current token<tab>activation format can result in improper tokenization.
    # In particular, if the token is itself a tab, we may get a single "\t\t" token rather than two
    # "\t" tokens. Consider using a separator that does not appear in any multi-character tokens.
    def make_simulation_prompt(self, tokens: Sequence[str]) -> str | list[ChatMessage]:
        """Create a few-shot prompt for predicting neuron activations for the given tokens."""
        # TODO(sbills): The prompts in this file are subtly different from the ones in explainer.py.
        # Consider reconciling them.
        prompt_builder = PromptBuilder()
        prompt_builder.add_message(
            Role.SYSTEM,
            """We're studying neurons in a neural network.
Each neuron looks for some particular thing in a short document.
Look at summary of what the neuron does, and try to predict how it will fire on each token.
The activation format is token<tab>activation, activations go from 0 to 10, "unknown" indicates an unknown activation. Most activations will be 0.
""",
        )
        few_shot_examples = self.few_shot_example_set.get_examples()
        for i, example in enumerate(few_shot_examples):
            prompt_builder.add_message(
                Role.USER,
                f"\n\nNeuron {i + 1}\nExplanation of neuron {i + 1} behavior: {EXPLANATION_PREFIX} "
                f"{example.explanation}",
            )
            formatted_activation_records = format_activation_records(
                example.activation_records,
                calculate_max_activation(example.activation_records),
                start_indices=example.first_revealed_activation_indices,
            )
            prompt_builder.add_message(
                Role.ASSISTANT, f"\nActivations: {formatted_activation_records}\n"
            )
        prompt_builder.add_message(
            Role.USER,
            f"\n\nNeuron {len(few_shot_examples) + 1}\nExplanation of neuron "
            f"{len(few_shot_examples) + 1} behavior: {EXPLANATION_PREFIX} "
            f"{self.explanation.strip()}",
        )
        prompt_builder.add_message(
            Role.ASSISTANT, f"\nActivations: {format_sequences_for_simulation([tokens])}"
        )
        return prompt_builder.build(self.prompt_format)
class ExplanationDummySimulator(NeuronSimulator):
    """
    A dummy class, returns all zero activations.
    """
    def __init__(
        self,
        client: ApiClient,
        explanation: str,
        **kwargs: Any,
    ) -> None:
        pass
    async def simulate(
        self,
        tokens: Sequence[str],
    ) -> SequenceSimulation:
        return SequenceSimulation(
            tokens=list(tokens),
            expected_activations=[0.0] * len(tokens),
            activation_scale=ActivationScale.SIMULATED_NORMALIZED_ACTIVATIONS,
            distribution_values=[[] for _ in tokens],
            distribution_probabilities=[[] for _ in tokens],
        )
class ExplanationTokenByTokenSimulator(NeuronSimulator):
    """
    Simulate neuron behavior based on an explanation.
    Unlike ExplanationNeuronSimulator, this class uses one few-shot prompt per token to calculate
    expected activations. This is slower. This class gets a one-token completion and calculates an
    expected value from that token's logprobs.
    """
    def __init__(
        self,
        client: ApiClient,
        explanation: str,
        few_shot_example_set: FewShotExampleSet = FewShotExampleSet.COLANGV2,
        prompt_format: PromptFormat = PromptFormat.INSTRUCTION_FOLLOWING,
    ):
        assert (
            few_shot_example_set != FewShotExampleSet.ORIGINAL
        ), "This simulator doesn't support the ORIGINAL few-shot example set."
        self.client = client
        self.explanation = explanation
        self.few_shot_example_set = few_shot_example_set
        self.prompt_format = prompt_format
    async def simulate(
        self,
        tokens: Sequence[str],
    ) -> SequenceSimulation:
        responses_by_token = await asyncio.gather(
            *[
                self._get_activation_stats_for_single_token(tokens, self.explanation, token_index)
                for token_index in range(len(tokens))
            ]
        )
        expected_values, distribution_values, distribution_probabilities = [], [], []
        for response in responses_by_token:
            activation_logprobs = response["choices"][0]["logprobs"]["top_logprobs"][0]
            (
                norm_probabilities_by_distribution_value,
                expected_value,
            ) = compute_predicted_activation_stats_for_token(
                activation_logprobs,
            )
            distribution_values.append(
                [float(v) for v in norm_probabilities_by_distribution_value.keys()]
            )
            distribution_probabilities.append(
                list(norm_probabilities_by_distribution_value.values())
            )
            expected_values.append(expected_value)
        result = SequenceSimulation(
            activation_scale=ActivationScale.SIMULATED_NORMALIZED_ACTIVATIONS,
            expected_activations=expected_values,
            distribution_values=distribution_values,
            distribution_probabilities=distribution_probabilities,
            tokens=list(tokens),  # SequenceSimulation expects List type
        )
        logger.debug("result in score_explanation_by_activations is %s", result)
        return result
    async def _get_activation_stats_for_single_token(
        self,
        tokens: Sequence[str],
        explanation: str,
        token_index_to_score: int,
    ) -> dict:
        prompt = self.make_single_token_simulation_prompt(
            tokens,
            explanation,
            token_index_to_score=token_index_to_score,
        )
        return await self.client.async_generate(
            prompt=prompt, max_tokens=1, echo=False, logprobs=15
        )
    def _add_single_token_simulation_subprompt(
        self,
        prompt_builder: PromptBuilder,
        activation_record: ActivationRecord,
        neuron_index: int,
        explanation: str,
        token_index_to_score: int,
        end_of_prompt: bool,
    ) -> None:
        trimmed_activation_record = ActivationRecord(
            tokens=activation_record.tokens[: token_index_to_score + 1],
            activations=activation_record.activations[: token_index_to_score + 1],
        )
        prompt_builder.add_message(
            Role.USER,
            f"""
Neuron {neuron_index}
Explanation of neuron {neuron_index} behavior: {EXPLANATION_PREFIX} {explanation.strip()}
Text:
{"".join(trimmed_activation_record.tokens)}
Last token in the text:
{trimmed_activation_record.tokens[-1]}
Last token activation, considering the token in the context in which it appeared in the text:
""",
        )
        if not end_of_prompt:
            normalized_activations = normalize_activations(
                trimmed_activation_record.activations, calculate_max_activation([activation_record])
            )
            prompt_builder.add_message(
                Role.ASSISTANT, str(normalized_activations[-1]) + ("" if end_of_prompt else "\n\n")
            )
    def make_single_token_simulation_prompt(
        self,
        tokens: Sequence[str],
        explanation: str,
        token_index_to_score: int,
    ) -> str | list[ChatMessage]:
        """Make a few-shot prompt for predicting the neuron's activation on a single token."""
        assert explanation != ""
        prompt_builder = PromptBuilder(allow_extra_system_messages=True)
        prompt_builder.add_message(
            Role.SYSTEM,
            """We're studying neurons in a neural network. Each neuron looks for some particular thing in a short document. Look at  an explanation of what the neuron does, and try to predict its activations on a particular token.
The activation format is token<tab>activation, and activations range from 0 to 10. Most activations will be 0.
""",
        )
        few_shot_examples = self.few_shot_example_set.get_examples()
        for i, example in enumerate(few_shot_examples):
            prompt_builder.add_message(
                Role.USER,
                f"Neuron {i + 1}\nExplanation of neuron {i + 1} behavior: {EXPLANATION_PREFIX} "
                f"{example.explanation}\n",
            )
            formatted_activation_records = format_activation_records(
                example.activation_records,
                calculate_max_activation(example.activation_records),
                start_indices=None,
            )
            prompt_builder.add_message(
                Role.ASSISTANT,
                f"Activations: {formatted_activation_records}\n\n",
            )
        prompt_builder.add_message(
            Role.SYSTEM,
            "Now, we're going predict the activation of a new neuron on a single token, "
            "following the same rules as the examples above. Activations still range from 0 to 10.",
        )
        single_token_example = self.few_shot_example_set.get_single_token_prediction_example()
        assert single_token_example.token_index_to_score is not None
        self._add_single_token_simulation_subprompt(
            prompt_builder,
            single_token_example.activation_records[0],
            len(few_shot_examples) + 1,
            explanation,
            token_index_to_score=single_token_example.token_index_to_score,
            end_of_prompt=False,
        )
        activation_record = ActivationRecord(
            tokens=list(tokens[: token_index_to_score + 1]),  # ActivationRecord expects List type.
            activations=[0.0] * len(tokens),
        )
        self._add_single_token_simulation_subprompt(
            prompt_builder,
            activation_record,
            len(few_shot_examples) + 2,
            explanation,
            token_index_to_score,
            end_of_prompt=True,
        )
        return prompt_builder.build(self.prompt_format)
def _parse_no_logprobs_completion_json(
    completion: str,
    tokens: Sequence[str],
) -> list[float]:
    """
    Parse a completion into a list of simulated activations. If the model did not faithfully
    reproduce the token sequence, return a list of 0s. If the model's activation for a token
    is not a number between 0 and 10 (inclusive), substitute 0.
    Args:
        completion: completion from the API
        tokens: list of tokens as strings in the sequence where the neuron is being simulated
    """
    zero_prediction: list[float] = [0.0] * len(tokens)
    try:
        completion_json: dict = json.loads(completion)
        if "activations" not in completion_json:
            logger.error(
                "The key 'activations' is not in the logprob free simulator response. Not a severe error, throw rate depends on how well the model can reproduce a particular JSON format."
            )
            return zero_prediction
        activations = completion_json["activations"]
        if len(activations) != len(tokens):
            return zero_prediction
        predicted_activations: list[float] = []
        # check that there is a token and activation value
        # no need to double check the token matches exactly
        for activation in activations:
            if "token" not in activation:
                predicted_activations.append(0)
                continue
            if "activation" not in activation:
                predicted_activations.append(0)
                continue
            # Ensure activation value is between 0-10 inclusive
            try:
                predicted_activation_float = float(activation["activation"])
                if (
                    predicted_activation_float < 0
                    or predicted_activation_float > MAX_NORMALIZED_ACTIVATION
                ):
                    predicted_activations.append(0.0)
                else:
                    predicted_activations.append(predicted_activation_float)
            except ValueError:
                predicted_activations.append(0)
            except TypeError:
                predicted_activations.append(0)
        logger.debug("predicted activations: %s", predicted_activations)
        return predicted_activations
    except json.JSONDecodeError:
        logger.error(
            "Error: the logprob free simulator response is not valid JSON. Not a severe error, throw rate depends on the model's ability to produce JSON."
        )
        return zero_prediction
def _format_record_for_logprob_free_simulation_json(
    explanation: str,
    activation_record: ActivationRecord,
    include_activations: bool = False,
) -> str:
    if include_activations:
        assert len(activation_record.tokens) == len(
            activation_record.activations
        ), f"{len(activation_record.tokens)=}, {len(activation_record.activations)=}"
    return json.dumps(
        {
            "to_find": explanation,
            "document": "".join(activation_record.tokens),
            "activations": [
                {
                    "token": token,
                    "activation": activation_record.activations[i] if include_activations else None,
                }
                for i, token in enumerate(activation_record.tokens)
            ],
        }
    )
class LogprobFreeExplanationTokenSimulator(NeuronSimulator):
    """
    Simulate neuron behavior based on an explanation.
    Unlike ExplanationNeuronSimulator and ExplanationTokenByTokenSimulator, this class does not rely on
    logprobs to calculate expected activations. Instead, it uses a few-shot prompt that displays all of the
    tokens at once, and requests that the model repeat the tokens with the activations appended. Sampling
    is with temperature = 0. Thus, the activations are deterministic. Also, each activation for a token
    is a function of all the activations that came previously and all of the tokens in the sequence, not
    just the current and previous tokens. In the case where the model does not faithfully reproduce the
    token sequence, the simulator will return a response where every predicted activation is 0.
    The tokens and activations in the prompt are formatted as a JSON object, which empirically improves
    the likelihood that the model will faithfully reproduce the token sequence.
    """
    def __init__(
        self,
        client: ApiClient,
        explanation: str,
        few_shot_example_set: FewShotExampleSet = FewShotExampleSet.COLANGV2,
        prompt_format: PromptFormat = PromptFormat.CHAT_MESSAGES,
    ):
        assert (
            few_shot_example_set != FewShotExampleSet.ORIGINAL
        ), "This simulator doesn't support the ORIGINAL few-shot example set."
        assert (
            prompt_format == PromptFormat.CHAT_MESSAGES
        ), "This simulator only supports the CHAT_MESSAGES prompt format."
        self.client = client
        self.explanation = explanation
        self.few_shot_example_set = few_shot_example_set
        self.prompt_format = prompt_format
    async def simulate(
        self,
        tokens: Sequence[str],
    ) -> SequenceSimulation:
        cleaned_tokens = []
        # Sanitize the token list to increase the chance that the model will faithfully reproduce it.
        for token in tokens:
            cleaned_tokens.append(
                token.replace("<|endoftext|>", "<|not_endoftext|>")
                .encode("ascii", errors="backslashreplace")
                .decode("ascii")
            )
        prompt = self.make_simulation_prompt(
            tokens,
            self.explanation,
        )
        response = await self.client.async_generate(messages=prompt, max_tokens=2000, temperature=0)
        assert len(response["choices"]) == 1
        choice = response["choices"][0]
        completion = choice["message"]["content"]
        predicted_activations = _parse_no_logprobs_completion_json(completion, cleaned_tokens)
        result = SequenceSimulation(
            activation_scale=ActivationScale.SIMULATED_NORMALIZED_ACTIVATIONS,
            expected_activations=predicted_activations,
            # Since the predicted activation is just a sampled token, we don't have a distribution.
            distribution_values=[],
            distribution_probabilities=[],
            tokens=list(tokens),  # SequenceSimulation expects List type
        )
        return result
    def make_simulation_prompt(
        self,
        tokens: Sequence[str],
        explanation: str,
    ) -> str | list[ChatMessage]:
        """Make a few-shot prompt for predicting the neuron's activations on a sequence.
        This prompt only gives the model one sequence per neuron in the few shot examples."""
        assert explanation != ""
        prompt_builder = PromptBuilder(allow_extra_system_messages=True)
        prompt_builder.add_message(
            Role.SYSTEM,
            """We're studying neurons in a neural network. Each neuron looks for certain things in a short document. Your task is to read the explanation of what the neuron does, and predict the neuron's activations for each token in the document.
For each document, you will see the full text of the document, then the tokens in the document with the activation left blank. You will print, in valid json, the exact same tokens verbatim, but with the activation values filled in according to the explanation. Pay special attention to the explanation's description of the context and order of tokens or words.
Fill out the activation values from 0 to 10. Please think carefully.";
""",
        )
        few_shot_examples = self.few_shot_example_set.get_examples()
        for example in few_shot_examples:
            prompt_builder.add_message(
                Role.USER,
                _format_record_for_logprob_free_simulation_json(
                    explanation=example.explanation,
                    activation_record=example.activation_records[0],
                    include_activations=False,
                ),
            )
            # Example of this few shot user message.
            """
            {
                "to_find": "hello",
                "document": "The",
                "activations": [
                    {
                        "token": "The",
                        "activation": null
                    },
                    ...
                ]
            }
            """
            prompt_builder.add_message(
                Role.ASSISTANT,
                _format_record_for_logprob_free_simulation_json(
                    explanation=example.explanation,
                    activation_record=example.activation_records[0],
                    include_activations=True,
                ),
            )
            # Example of this few shot assistant message:
            """
            {
                "to_find": "hello",
                "document": "The",
                "activations": [
                    {
                        "token": "The",
                        "activation": 10
                    },
                    ...
                ]
            }
            """
        prompt_builder.add_message(
            Role.USER,
            _format_record_for_logprob_free_simulation_json(
                explanation=explanation,
                activation_record=ActivationRecord(tokens=list(tokens), activations=[]),
                include_activations=False,
            ),
        )
        # Example of the final user message:
        """
        {
            "to_find": "hello",
            "document": "The",
            "activations": [
                {
                    "token": "The",
                    "activation": null
                },
                ...
            ]
        }
        """
        return prompt_builder.build(self.prompt_format)
if __name__ == "__main__":
    from neuron_explainer.activations.activations import load_neuron
    neuron = load_neuron(
        "https://openaipublic.blob.core.windows.net/neuron-explainer/data/collated-activations/",
        "21",
        "2932",
    )
    client = ApiClient(model_name="gpt-4o", max_concurrent=5)
    simulator = LogprobFreeExplanationTokenSimulator(
        client=client, explanation="Canada or things related to Canada"
    )
    result = asyncio.run(simulator.simulate(neuron.most_positive_activation_records[0].tokens))
    for token, real, activation in zip(
        result.tokens,
        neuron.most_positive_activation_records[0].activations,
        result.expected_activations,
    ):
        print(str(token), real, activation)

================
File: neuron_explainer/explanations/test_explainer.py
================
import asyncio
from typing import Any
from neuron_explainer.explanations.explainer import TokenActivationPairExplainer
from neuron_explainer.explanations.few_shot_examples import TEST_EXAMPLES, FewShotExampleSet
from neuron_explainer.explanations.prompt_builder import ChatMessage, PromptFormat, Role
def setup_module(unused_module: Any) -> None:
    # Make sure we have an event loop, since the attempt to create the Semaphore in
    # ApiClient will fail without it.
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
def test_if_formatting() -> None:
    expected_prompt = """We're studying neurons in a neural network. Each neuron looks for some particular thing in a short document. Look at the parts of the document the neuron activates for and summarize in a single sentence what the neuron is looking for. Don't list examples of words.
The activation format is token<tab>activation. Activation values range from 0 to 10. A neuron finding what it's looking for is represented by a non-zero activation value. The higher the activation value, the stronger the match.
Neuron 1
Activations:
<start>
a	10
b	0
c	0
<end>
<start>
d	0
e	10
f	0
<end>
Explanation of neuron 1 behavior: this neuron activates for vowels.
Neuron 2
Activations:
<start>
a	10
b	0
c	0
<end>
<start>
d	0
e	10
f	0
<end>
Explanation of neuron 2 behavior:<|endofprompt|> this neuron activates for"""
    explainer = TokenActivationPairExplainer(
        model_name="gpt-4o",
        prompt_format=PromptFormat.INSTRUCTION_FOLLOWING,
        few_shot_example_set=FewShotExampleSet.TEST,
    )
    prompt = explainer.make_explanation_prompt(
        all_activations=TEST_EXAMPLES[0].activation_records,
        max_activation=1.0,
        max_tokens_for_completion=20,
    )
    assert prompt == expected_prompt
def test_chat_format() -> None:
    expected_prompt = [
        ChatMessage(
            role=Role.SYSTEM,
            content="""We're studying neurons in a neural network. Each neuron looks for some particular thing in a short document. Look at the parts of the document the neuron activates for and summarize in a single sentence what the neuron is looking for. Don't list examples of words.
The activation format is token<tab>activation. Activation values range from 0 to 10. A neuron finding what it's looking for is represented by a non-zero activation value. The higher the activation value, the stronger the match.""",
        ),
        ChatMessage(
            role=Role.USER,
            content="""
Neuron 1
Activations:
<start>
a	10
b	0
c	0
<end>
<start>
d	0
e	10
f	0
<end>
Explanation of neuron 1 behavior: this neuron activates for""",
        ),
        ChatMessage(
            role=Role.ASSISTANT,
            content=" vowels.",
        ),
        ChatMessage(
            role=Role.USER,
            content="""
Neuron 2
Activations:
<start>
a	10
b	0
c	0
<end>
<start>
d	0
e	10
f	0
<end>
Explanation of neuron 2 behavior: this neuron activates for""",
        ),
    ]
    explainer = TokenActivationPairExplainer(
        model_name="gpt-4o",
        prompt_format=PromptFormat.CHAT_MESSAGES,
        few_shot_example_set=FewShotExampleSet.TEST,
    )
    prompt = explainer.make_explanation_prompt(
        all_activations=TEST_EXAMPLES[0].activation_records,
        max_activation=1.0,
        max_tokens_for_completion=20,
    )
    assert isinstance(prompt, list)
    assert isinstance(prompt[0], dict)  # Really a ChatMessage
    for actual_message, expected_message in zip(prompt, expected_prompt):
        assert actual_message["role"] == expected_message["role"]
        assert actual_message["content"] == expected_message["content"]
    assert prompt == expected_prompt

================
File: neuron_explainer/explanations/test_simulator.py
================
# ruff: noqa: W291
from neuron_explainer.explanations.few_shot_examples import FewShotExampleSet
from neuron_explainer.explanations.prompt_builder import ChatMessage, PromptFormat, Role
from neuron_explainer.explanations.simulator import (
    ExplanationNeuronSimulator,
    ExplanationTokenByTokenSimulator,
)
def test_make_explanation_simulation_prompt_if_format() -> None:
    expected_prompt = """We're studying neurons in a neural network.
Each neuron looks for some particular thing in a short document.
Look at summary of what the neuron does, and try to predict how it will fire on each token.
The activation format is token<tab>activation, activations go from 0 to 10, "unknown" indicates an unknown activation. Most activations will be 0.
Neuron 1
Explanation of neuron 1 behavior: this neuron activates for vowels
Activations: 
<start>
a	10
b	0
c	0
<end>
<start>
d	unknown
e	10
f	0
<end>
Neuron 2
Explanation of neuron 2 behavior: this neuron activates for EXPLANATION<|endofprompt|>
Activations: 
<start>
0	unknown
1	unknown
2	unknown
<end>
"""
    prompt = ExplanationNeuronSimulator(
        client=None,  # type: ignore
        explanation="EXPLANATION",
        few_shot_example_set=FewShotExampleSet.TEST,
        prompt_format=PromptFormat.INSTRUCTION_FOLLOWING,
    ).make_simulation_prompt(
        tokens=[str(x) for x in range(3)],
    )
    assert prompt == expected_prompt
def test_make_explanation_simulation_prompt_chat_format() -> None:
    expected_prompt = [
        ChatMessage(
            role=Role.SYSTEM,
            content="""We're studying neurons in a neural network.
Each neuron looks for some particular thing in a short document.
Look at summary of what the neuron does, and try to predict how it will fire on each token.
The activation format is token<tab>activation, activations go from 0 to 10, "unknown" indicates an unknown activation. Most activations will be 0.
""",
        ),
        ChatMessage(
            role=Role.USER,
            content="""
Neuron 1
Explanation of neuron 1 behavior: this neuron activates for vowels""",
        ),
        ChatMessage(
            role=Role.ASSISTANT,
            content="""
Activations: 
<start>
a	10
b	0
c	0
<end>
<start>
d	unknown
e	10
f	0
<end>
""",
        ),
        ChatMessage(
            role=Role.USER,
            content="""
Neuron 2
Explanation of neuron 2 behavior: this neuron activates for EXPLANATION""",
        ),
        ChatMessage(
            role=Role.ASSISTANT,
            content="""
Activations: 
<start>
0	unknown
1	unknown
2	unknown
<end>
""",
        ),
    ]
    prompt = ExplanationNeuronSimulator(
        client=None,  # type: ignore
        explanation="EXPLANATION",
        few_shot_example_set=FewShotExampleSet.TEST,
        prompt_format=PromptFormat.CHAT_MESSAGES,
    ).make_simulation_prompt(
        tokens=[str(x) for x in range(3)],
    )
    assert isinstance(prompt, list)
    assert isinstance(prompt[0], dict)  # Really a ChatMessage
    for actual_message, expected_message in zip(prompt, expected_prompt):
        assert actual_message["role"] == expected_message["role"]
        assert actual_message["content"] == expected_message["content"]
    assert prompt == expected_prompt
def test_make_token_by_token_simulation_prompt_if_format() -> None:
    expected_prompt = """We're studying neurons in a neural network. Each neuron looks for some particular thing in a short document. Look at  an explanation of what the neuron does, and try to predict its activations on a particular token.
The activation format is token<tab>activation, and activations range from 0 to 10. Most activations will be 0.
Neuron 1
Explanation of neuron 1 behavior: this neuron activates for vowels
Activations: 
<start>
a	10
b	0
c	0
<end>
<start>
d	0
e	10
f	0
<end>
Now, we're going predict the activation of a new neuron on a single token, following the same rules as the examples above. Activations still range from 0 to 10.
Neuron 2
Explanation of neuron 2 behavior: this neuron activates for numbers and nothing else
Text:
ghi
Last token in the text:
i
Last token activation, considering the token in the context in which it appeared in the text:
10
Neuron 3
Explanation of neuron 3 behavior: this neuron activates for numbers and nothing else
Text:
01
Last token in the text:
1
Last token activation, considering the token in the context in which it appeared in the text:
<|endofprompt|>"""
    prompt = ExplanationTokenByTokenSimulator(
        client=None,  # type: ignore
        explanation="EXPLANATION",
        few_shot_example_set=FewShotExampleSet.TEST,
        prompt_format=PromptFormat.INSTRUCTION_FOLLOWING,
    ).make_single_token_simulation_prompt(
        tokens=[str(x) for x in range(3)],
        explanation="numbers and nothing else",
        token_index_to_score=1,
    )
    assert prompt == expected_prompt
def test_make_token_by_token_simulation_prompt_chat_format() -> None:
    expected_prompt = [
        ChatMessage(
            role=Role.SYSTEM,
            content="""We're studying neurons in a neural network. Each neuron looks for some particular thing in a short document. Look at  an explanation of what the neuron does, and try to predict its activations on a particular token.
The activation format is token<tab>activation, and activations range from 0 to 10. Most activations will be 0.
""",
        ),
        ChatMessage(
            role=Role.USER,
            content="""Neuron 1
Explanation of neuron 1 behavior: this neuron activates for vowels
""",
        ),
        ChatMessage(
            role=Role.ASSISTANT,
            content="""Activations: 
<start>
a	10
b	0
c	0
<end>
<start>
d	0
e	10
f	0
<end>
""",
        ),
        ChatMessage(
            role=Role.SYSTEM,
            content="Now, we're going predict the activation of a new neuron on a single token, following the same rules as the examples above. Activations still range from 0 to 10.",
        ),
        ChatMessage(
            role=Role.USER,
            content="""
Neuron 2
Explanation of neuron 2 behavior: this neuron activates for numbers and nothing else
Text:
ghi
Last token in the text:
i
Last token activation, considering the token in the context in which it appeared in the text:
""",
        ),
        ChatMessage(
            role=Role.ASSISTANT,
            content="""10
""",
        ),
        ChatMessage(
            role=Role.USER,
            content="""
Neuron 3
Explanation of neuron 3 behavior: this neuron activates for numbers and nothing else
Text:
01
Last token in the text:
1
Last token activation, considering the token in the context in which it appeared in the text:
""",
        ),
    ]
    prompt = ExplanationTokenByTokenSimulator(
        client=None,  # type: ignore
        explanation="EXPLANATION",
        few_shot_example_set=FewShotExampleSet.TEST,
        prompt_format=PromptFormat.CHAT_MESSAGES,
    ).make_single_token_simulation_prompt(
        tokens=[str(x) for x in range(3)],
        explanation="numbers and nothing else",
        token_index_to_score=1,
    )
    assert isinstance(prompt, list)
    assert isinstance(prompt[0], dict)  # Really a ChatMessage
    for actual_message, expected_message in zip(prompt, expected_prompt):
        assert actual_message["role"] == expected_message["role"]
        assert actual_message["content"] == expected_message["content"]
    assert prompt == expected_prompt

================
File: neuron_explainer/fast_dataclasses/__init__.py
================
from .fast_dataclasses import FastDataclass, dumps, loads, register_dataclass
__all__ = ["FastDataclass", "dumps", "loads", "register_dataclass"]

================
File: neuron_explainer/fast_dataclasses/fast_dataclasses.py
================
# Utilities for dataclasses that are very fast to serialize and deserialize, with limited data
# validation. Fields must not be tuples, since they get serialized and then deserialized as lists.
#
# The unit tests for this library show how to use it.
import json
from dataclasses import dataclass, field, fields, is_dataclass
from functools import partial
from typing import Any
import orjson
dataclasses_by_name = {}
# TODO(williamrs): remove deserialization using fieldnames once all data is in the new format
dataclasses_by_fieldnames = {}
@dataclass
class FastDataclass:
    dataclass_name: str = field(init=False)
    def __post_init__(self) -> None:
        self.dataclass_name = self.__class__.__name__
    @classmethod
    def field_renamed(cls, old_name: str, new_name: str) -> None:
        if not hasattr(cls, "field_renames"):
            cls.field_renames = {}  # type: ignore
        assert old_name not in cls.field_renames, f"{old_name} already renamed"  # type: ignore
        existing_field_names = [f.name for f in fields(cls)]
        assert new_name in existing_field_names, f"{new_name} not in {existing_field_names}"
        assert old_name not in existing_field_names, f"{old_name} still in {existing_field_names}"
        cls.field_renames[old_name] = new_name  # type: ignore
    # Type checking is suppressed in these two functions because mypy doesn't like that we're
    # setting attributes at runtime. We need to do that because if we defined the attributes at the
    # class level, they'd be shared across subclasses.
    @classmethod
    def field_deleted(cls, field_name: str) -> None:
        if not hasattr(cls, "deleted_fields"):
            cls.deleted_fields = []  # type: ignore
        assert field_name not in cls.deleted_fields, f"{field_name} already deleted"  # type: ignore
        existing_field_names = [f.name for f in fields(cls)]
        assert field_name not in existing_field_names, f"{field_name} still in use"
        cls.deleted_fields.append(field_name)  # type: ignore
    @classmethod
    def was_previously_named(cls, old_name: str) -> None:
        assert old_name not in dataclasses_by_name, f"{old_name} still in use as a dataclass name"
        dataclasses_by_name[old_name] = cls
        name_set = frozenset(f.name for f in fields(cls) if f.name != "dataclass_name")
        dataclasses_by_fieldnames[name_set] = cls
def register_dataclass(cls):  # type: ignore
    assert is_dataclass(cls), "Only dataclasses can be registered."
    dataclasses_by_name[cls.__name__] = cls
    name_set = frozenset(f.name for f in fields(cls) if f.name != "dataclass_name")
    dataclasses_by_fieldnames[name_set] = cls
    return cls
def dumps(obj: Any) -> bytes:
    return orjson.dumps(obj, option=orjson.OPT_SERIALIZE_NUMPY)
def _object_hook(d: Any, backwards_compatible: bool = True) -> Any:
    # If d is a list, recurse.
    if isinstance(d, list):
        return [_object_hook(x, backwards_compatible=backwards_compatible) for x in d]
    # If d is not a dict, return it as is.
    if not isinstance(d, dict):
        return d
    cls = None
    if "dataclass_name" in d:
        if d["dataclass_name"] in dataclasses_by_name:
            cls = dataclasses_by_name[d["dataclass_name"]]
        else:
            assert backwards_compatible, (
                f"Dataclass {d['dataclass_name']} not found, set backwards_compatible=True if you "
                f"are okay with that."
            )
    # Load objects created without dataclass_name set.
    else:
        # Try our best to find a dataclass if backwards_compatible is True.
        if backwards_compatible:
            d_fields = frozenset(d.keys())
            if d_fields in dataclasses_by_fieldnames:
                cls = dataclasses_by_fieldnames[d_fields]
            elif len(d_fields) > 0:
                # Check if the fields are a subset of a dataclass (if the dataclass had extra fields
                # added since the data was created). Note that this will fail if fields were removed
                # from the dataclass.
                for key, possible_cls in dataclasses_by_fieldnames.items():
                    if d_fields.issubset(key):
                        cls = possible_cls
                        break
                else:
                    print(f"Could not find dataclass for {d_fields} {cls}")
    if cls is not None:
        # Apply renames and deletions.
        if hasattr(cls, "field_renames"):
            d = {cls.field_renames.get(k, k): v for k, v in d.items()}
        if hasattr(cls, "deleted_fields"):
            d = {k: v for k, v in d.items() if k not in cls.deleted_fields}
    new_d = {
        k: _object_hook(v, backwards_compatible=backwards_compatible)
        for k, v in d.items()
        if k != "dataclass_name"
    }
    if cls is not None:
        return cls(**new_d)
    else:
        return new_d
def loads(s: str | bytes, backwards_compatible: bool = True) -> Any:
    return json.loads(
        s,
        object_hook=partial(_object_hook, backwards_compatible=backwards_compatible),
    )

================
File: neuron_explainer/fast_dataclasses/test_fast_dataclasses.py
================
import json
from dataclasses import dataclass
import pytest
from .fast_dataclasses import FastDataclass, dumps, loads, register_dataclass
# Inheritance is a bit tricky with our setup. dataclass_name must be set for instances of these
# classes to serialize and deserialize correctly, but if it's given a default value, then subclasses
# can't have any fields that don't have default values, because of how constructors are generated
# for dataclasses (fields with no default value can't follow those with default values). To work
# around this, we set dataclass_name in __post_init__ on the base class, which is called after the
# constructor. The implementation does the right thing for both the base class and the subclass.
@register_dataclass
@dataclass
class DataclassC(FastDataclass):
    ints: list[int]
@register_dataclass
@dataclass
class DataclassC_ext(DataclassC):
    s: str
@register_dataclass
@dataclass
class DataclassB(FastDataclass):
    str_to_c: dict[str, DataclassC]
    cs: list[DataclassC]
@register_dataclass
@dataclass
class DataclassA(FastDataclass):
    floats: list[float]
    strings: list[str]
    bs: list[DataclassB]
@register_dataclass
@dataclass
class DataclassD(FastDataclass):
    s1: str
    s2: str = "default"
def test_dataclasses() -> None:
    a = DataclassA(
        floats=[1.0, 2.0],
        strings=["a", "b"],
        bs=[
            DataclassB(
                str_to_c={"a": DataclassC(ints=[1, 2]), "b": DataclassC(ints=[3, 4])},
                cs=[DataclassC(ints=[5, 6]), DataclassC_ext(ints=[7, 8], s="s")],
            ),
            DataclassB(
                str_to_c={"c": DataclassC_ext(ints=[9, 10], s="t"), "d": DataclassC(ints=[11, 12])},
                cs=[DataclassC(ints=[13, 14]), DataclassC(ints=[15, 16])],
            ),
        ],
    )
    assert loads(dumps(a)) == a
def test_c_and_c_ext() -> None:
    c_ext = DataclassC_ext(ints=[3, 4], s="s")
    assert loads(dumps(c_ext)) == c_ext
    c = DataclassC(ints=[1, 2])
    assert loads(dumps(c)) == c
def test_bad_serialized_data() -> None:
    assert type(loads(dumps(DataclassC(ints=[3, 4])))) == DataclassC
    assert type(loads('{"ints": [3, 4]}', backwards_compatible=False)) == dict
    assert type(loads('{"ints": [3, 4], "dataclass_name": "DataclassC"}')) == DataclassC
    with pytest.raises(TypeError):
        loads('{"ints": [3, 4], "bogus_extra_field": "foo", "dataclass_name": "DataclassC"}')
    with pytest.raises(TypeError):
        loads('{"ints_field_is_missing": [3, 4], "dataclass_name": "DataclassC"}')
    assert type(loads('{"s1": "test"}', backwards_compatible=False)) == dict
    assert type(loads('{"s1": "test"}', backwards_compatible=True)) == DataclassD
def test_renames_and_deletes() -> None:
    @register_dataclass
    @dataclass
    class FooV1(FastDataclass):
        a: int
        b: str
        c: float
    # Version with c renamed.
    @register_dataclass
    @dataclass
    class FooV2(FastDataclass):
        a: int
        b: str
        c_renamed: float
    # Version with c renamed and b deleted.
    @register_dataclass
    @dataclass
    class FooV3(FastDataclass):
        a: int
        c_renamed: float
    # Basic sanity checks.
    foo_v1 = FooV1(a=1, b="hello", c=3.14)
    foo_v1_bytes = dumps(foo_v1)
    assert loads(foo_v1_bytes) == foo_v1
    # Deserializing a FooV1 as a FooV2 should fail.
    foo_v1_dict = json.loads(foo_v1_bytes)
    # Change the dataclass_name so this will be interpreted as a FooV2.
    foo_v1_dict["dataclass_name"] = "FooV2"
    foo_v1_as_v2_bytes = dumps(foo_v1_dict)
    with pytest.raises(TypeError):
        loads(foo_v1_as_v2_bytes)
    # Register the field's rename, then try again. This time it should succeed.
    FooV2.field_renamed("c", "c_renamed")
    foo_v1_as_v2 = loads(foo_v1_as_v2_bytes)
    assert type(foo_v1_as_v2) == FooV2
    assert foo_v1_as_v2.a == foo_v1.a
    assert foo_v1_as_v2.b == foo_v1.b
    assert foo_v1_as_v2.c_renamed == foo_v1.c
    # Deserializing a FooV1 as a FooV3 should fail, even with the rename.
    FooV3.field_renamed("c", "c_renamed")
    # Change the dataclass_name so this will be interpreted as a FooV3.
    foo_v1_dict["dataclass_name"] = "FooV3"
    foo_v1_as_v3_bytes = dumps(foo_v1_dict)
    with pytest.raises(TypeError):
        loads(foo_v1_as_v3_bytes)
    # Register the field's deletion, then try again. This time it should succeed.
    FooV3.field_deleted("b")
    foo_v1_as_v3 = loads(foo_v1_as_v3_bytes)
    assert type(foo_v1_as_v3) == FooV3
    assert foo_v1_as_v3.a == foo_v1.a
    assert foo_v1_as_v3.c_renamed == foo_v1.c
def test_class_rename() -> None:
    @register_dataclass
    @dataclass
    class BarRenamed(FastDataclass):
        d: str
        e: int
    BarRenamed.was_previously_named("Bar")
    # We should be able to deserialize an old Bar as a BarRenamed.
    bar = BarRenamed(d="hello", e=1)
    bar_bytes = dumps(bar)
    bar_dict = json.loads(bar_bytes)
    # Change the dataclass_name so this looks like an old Bar.
    bar_dict["dataclass_name"] = "Bar"
    bar_as_bar_bytes = dumps(bar_dict)
    bar_as_bar_renamed = loads(bar_as_bar_bytes)
    assert type(bar_as_bar_renamed) == BarRenamed
    assert bar_as_bar_renamed.d == bar.d
    assert bar_as_bar_renamed.e == bar.e
def test_invalid_renames_and_deletions() -> None:
    @register_dataclass
    @dataclass
    class OldBaz(FastDataclass):
        a: int
        b: str
        c: float
    @register_dataclass
    @dataclass
    class Baz(FastDataclass):
        a: int
        b: str
        c: float
    # Renaming a field should fail if the new name doesn't exist.
    with pytest.raises(AssertionError):
        Baz.field_renamed("d", "d_renamed")
    # Renaming a field should fail if the old name is still in use.
    with pytest.raises(AssertionError):
        Baz.field_renamed("a", "b")
    # Deleting a field should fail if the name is still in use.
    with pytest.raises(AssertionError):
        Baz.field_deleted("a")
    # Renaming a dataclass should fail if the old name is still in use.
    with pytest.raises(AssertionError):
        Baz.was_previously_named("OldBaz")
    # Renaming the same field twice should fail.
    Baz.field_renamed("old_c", "c")
    with pytest.raises(AssertionError):
        Baz.field_renamed("old_c", "b")
    # Deleting a field twice should fail.
    Baz.field_deleted("z")
    with pytest.raises(AssertionError):
        Baz.field_deleted("z")

================
File: neuron_explainer/file_utils.py
================
import io
import os
import urllib.request
from io import IOBase
import aiohttp
def file_exists(filepath: str) -> bool:
    if filepath.startswith("https://"):
        try:
            urllib.request.urlopen(filepath)
            return True
        except urllib.error.HTTPError:
            return False
    else:
        # It's a local file.
        return os.path.exists(filepath)
class CustomFileHandler:
    def __init__(self, filepath: str, mode: str) -> None:
        self.filepath = filepath
        self.mode = mode
        self.file = None
    def __enter__(self) -> IOBase:
        assert not self.filepath.startswith("az://"), "Azure blob storage is not supported"
        if self.filepath.startswith("https://"):
            assert self.mode in ["r", "rb"], "Only read mode is supported for remote files"
            remote_data = urllib.request.urlopen(self.filepath)
            if "b" in self.mode:
                # Read the content into a BytesIO object for binary mode
                self.file = io.BytesIO(remote_data.read())
            else:
                # Decode the content and use StringIO for text mode (less common for torch.load)
                self.file = io.StringIO(remote_data.read().decode())
        else:
            # Create the subdirectories if they don't exist
            directory = os.path.dirname(self.filepath)
            os.makedirs(directory, exist_ok=True)
            self.file = open(self.filepath, self.mode)
            if "b" in self.mode:
                # Ensure the file is seekable; if not, read into a BytesIO object
                try:
                    self.file.seek(0)
                except io.UnsupportedOperation:
                    self.file.close()
                    with open(self.filepath, self.mode) as f:
                        self.file = io.BytesIO(f.read())
        return self.file
    def __exit__(self, exc_type, exc_val, exc_tb) -> bool:
        # Close the file if it's open
        if self.file is not None:
            self.file.close()
        # Propagate exceptions
        return False
async def read_single_async(filepath: str) -> bytes:
    if filepath.startswith("https://"):
        async with aiohttp.ClientSession() as session:
            async with session.get(filepath) as response:
                return await response.read()
    else:
        with open(filepath, "rb") as f:
            return f.read()
def copy_to_local_cache(src: str, dst: str) -> None:
    if not os.path.exists(os.path.dirname(dst)):
        os.makedirs(os.path.dirname(dst), exist_ok=True)
    if src.startswith("https://"):
        with urllib.request.urlopen(src) as response, open(dst, "wb") as out_file:
            data = response.read()  # Consider chunked reading for large files
            out_file.write(data)
    else:
        with open(src, "rb") as in_file, open(dst, "wb") as out_file:
            data = in_file.read()
            out_file.write(data)

================
File: neuron_explainer/models/__init__.py
================
from .autoencoder import Autoencoder
from .hooks import Hooks, TransformerHooks
from .transformer import Transformer, TransformerConfig

================
File: neuron_explainer/models/autoencoder_context.py
================
import os
from dataclasses import dataclass, field
from typing import Union
import torch
from neuron_explainer.activations.derived_scalars.derived_scalar_types import DerivedScalarType
from neuron_explainer.file_utils import copy_to_local_cache, file_exists
from neuron_explainer.models import Autoencoder
from neuron_explainer.models.model_component_registry import Dimension, LayerIndex, NodeType
@dataclass(frozen=True)
class AutoencoderSpec:
    """Parameters used in the construction of an AutoencoderConfig object. Seperate so we don't need to validate when constructed"""
    dst: DerivedScalarType
    autoencoder_path_by_layer_index: dict[LayerIndex, str]
@dataclass(frozen=True)
class AutoencoderConfig:
    """
    This class specifies a set of autoencoders to load from disk, for one or more layer indices.
    The activation location type indicates the type of activation that the autoencoder was trained
    on, and that will be fed into the autoencoder.
    """
    dst: DerivedScalarType
    autoencoder_path_by_layer_index: dict[LayerIndex, str]
    def __post_init__(self) -> None:
        assert len(self.autoencoder_path_by_layer_index) > 0
        if len(self.autoencoder_path_by_layer_index) > 1:
            assert (
                None not in self.autoencoder_path_by_layer_index.keys()
            ), "layer_indices must be [None], or a list of int layer indices"
    @classmethod
    def from_spec(cls, params: AutoencoderSpec) -> "AutoencoderConfig":
        return cls(
            dst=params.dst,
            autoencoder_path_by_layer_index=params.autoencoder_path_by_layer_index,
        )
@dataclass(frozen=True)
class AutoencoderContext:
    autoencoder_config: AutoencoderConfig
    device: torch.device
    _cached_autoencoders_by_path: dict[str, Autoencoder] = field(default_factory=dict)
    omit_dead_latents: bool = False
    """
    Omit dead latents to save memory. Only happens if self.warmup() is called. Because we omit the
    same number of latents from all autoencoders, we can only omit the smallest number of dead
    latents among all autoencoders.
    """
    @property
    def num_autoencoder_directions(self) -> int:
        """Note that this property might change after warmup() is called, if omit_dead_latents is True."""
        if len(self._cached_autoencoders_by_path) == 0:
            raise ValueError(
                "num_autoencoder_directions is not populated yet. Call warmup() first."
            )
        else:
            # all autoencoders have the same number of directions, so we can just check one
            first_autoencoder = next(iter(self._cached_autoencoders_by_path.values()))
            return first_autoencoder.latent_bias.shape[0]
    @property
    def _min_n_dead_latents(self) -> int:
        return min(
            count_dead_latents(autoencoder)
            for autoencoder in self._cached_autoencoders_by_path.values()
        )
    @property
    def dst(self) -> DerivedScalarType:
        return self.autoencoder_config.dst
    @property
    def layer_indices(self) -> set[LayerIndex]:
        return set(self.autoencoder_config.autoencoder_path_by_layer_index.keys())
    def get_autoencoder(self, layer_index: LayerIndex) -> Autoencoder:
        autoencoder_azure_path = self.autoencoder_config.autoencoder_path_by_layer_index.get(
            layer_index
        )
        if autoencoder_azure_path is None:
            raise ValueError(f"No autoencoder path for layer_index {layer_index}")
        else:
            if autoencoder_azure_path in self._cached_autoencoders_by_path:
                autoencoder = self._cached_autoencoders_by_path[autoencoder_azure_path]
            else:
                # Check if the autoencoder is cached on disk
                disk_cache_path = os.path.join(
                    "/tmp", autoencoder_azure_path.replace("https://", "")
                )
                if file_exists(disk_cache_path):
                    print(f"Loading autoencoder from disk cache: {disk_cache_path}")
                else:
                    print(f"Reading autoencoder from blob storage: {autoencoder_azure_path}")
                    copy_to_local_cache(autoencoder_azure_path, disk_cache_path)
                state_dict = torch.load(disk_cache_path, map_location=self.device)
                # released autoencoders are saved as a dict for better compatibility
                assert isinstance(state_dict, dict)
                autoencoder = Autoencoder.from_state_dict(state_dict, strict=False).to(self.device)
                self._cached_autoencoders_by_path[autoencoder_azure_path] = autoencoder
            # freeze the autoencoder
            for p in autoencoder.parameters():
                p.requires_grad = False
            return autoencoder
    def warmup(self) -> None:
        """Load all autoencoders into memory."""
        for layer_index in self.layer_indices:
            self.get_autoencoder(layer_index)
        # num_autoencoder_directions is always populated after warmup
        n_latents = self.num_autoencoder_directions
        if self.omit_dead_latents:
            # drop the dead latents to save memory, but keep the same number of directions for all autoencoders
            if self._min_n_dead_latents > 0:
                print(f"Omitting {self._min_n_dead_latents} dead latents from all autoencoders")
                n_latents_to_keep = n_latents - self._min_n_dead_latents
                for key, autoencoder in self._cached_autoencoders_by_path.items():
                    self._cached_autoencoders_by_path[key] = omit_least_active_latents(
                        autoencoder, n_latents_to_keep=n_latents_to_keep
                    )
    def get_parameterized_dimension_sizes(self) -> dict[Dimension, int]:
        """A dictionary specifying the size of the parameterized dimensions; for convenient use with ScalarDerivers"""
        return {
            Dimension.AUTOENCODER_LATENTS: self.num_autoencoder_directions,
        }
    @property
    def autoencoder_node_type(self) -> NodeType | None:
        return _autoencoder_node_type_by_input_dst.get(self.dst)
_autoencoder_node_type_by_input_dst = {
    # add more mappings as needed
    DerivedScalarType.MLP_POST_ACT: NodeType.MLP_AUTOENCODER_LATENT,
    DerivedScalarType.RESID_DELTA_MLP_FROM_MLP_POST_ACT: NodeType.MLP_AUTOENCODER_LATENT,
    DerivedScalarType.RESID_DELTA_MLP: NodeType.MLP_AUTOENCODER_LATENT,
    DerivedScalarType.RESID_DELTA_ATTN: NodeType.ATTENTION_AUTOENCODER_LATENT,
    DerivedScalarType.ATTN_WRITE: NodeType.ATTENTION_AUTOENCODER_LATENT,
}
@dataclass(frozen=True)
class MultiAutoencoderContext:
    autoencoder_context_by_node_type: dict[NodeType, AutoencoderContext]
    @classmethod
    def from_context_or_multi_context(
        cls,
        input: Union[AutoencoderContext, "MultiAutoencoderContext", None],
    ) -> Union["MultiAutoencoderContext", None]:
        if isinstance(input, AutoencoderContext):
            return cls.from_autoencoder_context_list([input])
        elif input is None:
            return None
        else:
            return input
    @classmethod
    def from_autoencoder_context_list(
        cls, autoencoder_context_list: list[AutoencoderContext]
    ) -> "MultiAutoencoderContext":
        # check if there are duplicate node types
        node_types = [
            _autoencoder_node_type_by_input_dst[autoencoder_context.dst]
            for autoencoder_context in autoencoder_context_list
        ]
        if len(node_types) != len(set(node_types)):
            raise ValueError(f"Cannot load two autoencoders with the same node type ({node_types})")
        return cls(
            autoencoder_context_by_node_type={
                _autoencoder_node_type_by_input_dst[autoencoder_context.dst]: autoencoder_context
                for autoencoder_context in autoencoder_context_list
            }
        )
    def get_autoencoder_context(
        self, node_type: NodeType | None = None
    ) -> AutoencoderContext | None:
        if node_type is None or node_type == NodeType.AUTOENCODER_LATENT:  # handle default case
            return self.get_single_autoencoder_context()
        else:
            return self.autoencoder_context_by_node_type.get(node_type, None)
    @property
    def has_single_autoencoder_context(self) -> bool:
        return len(self.autoencoder_context_by_node_type) == 1
    def get_single_autoencoder_context(self) -> AutoencoderContext:
        assert self.has_single_autoencoder_context
        return next(iter(self.autoencoder_context_by_node_type.values()))
    def get_autoencoder(
        self, layer_index: LayerIndex, node_type: NodeType | None = None
    ) -> Autoencoder:
        autoencoder_context = self.get_autoencoder_context(node_type)
        assert autoencoder_context is not None
        return autoencoder_context.get_autoencoder(layer_index)
    def warmup(self) -> None:
        """Load all autoencoders into memory."""
        for node_type, autoencoder_context in self.autoencoder_context_by_node_type.items():
            print(f"Warming up autoencoder {node_type}")
            autoencoder_context.warmup()
def get_decoder_weight(autoencoder: Autoencoder) -> torch.Tensor:
    return autoencoder.decoder.weight.T  # shape (n_latents, d_ff)
def get_autoencoder_output_weight_by_layer_index(
    autoencoder_context: AutoencoderContext,
) -> dict[LayerIndex, torch.Tensor]:
    return {
        layer_index: get_decoder_weight(
            autoencoder_context.get_autoencoder(layer_index)
        )  # shape (n_latents, d_ff)
        for layer_index in autoencoder_context.layer_indices
    }
ACTIVATION_FREQUENCY_THRESHOLD_FOR_DEAD_LATENTS = 1e-8
def count_dead_latents(autoencoder: Autoencoder) -> int:
    if hasattr(autoencoder, "latents_activation_frequency"):
        if torch.all(autoencoder.latents_activation_frequency == 0):
            raise ValueError("latents_activation_frequency is all zeros, all latents are dead.")
        dead_latents_mask = (
            autoencoder.latents_activation_frequency
            < ACTIVATION_FREQUENCY_THRESHOLD_FOR_DEAD_LATENTS
        )
        num_dead_latents = int(dead_latents_mask.sum().item())
        return num_dead_latents
    else:
        return 0
def omit_least_active_latents(
    autoencoder: Autoencoder,
    n_latents_to_keep: int,
    # if preserve_indices=True, ignore the stored activation frequencies, and keep the first indices.
    # this is to preserve latent indices compared to the original autoencoder.
    preserve_indices: bool = True,
) -> Autoencoder:
    n_latents_original = int(autoencoder.latent_bias.shape[0])
    if n_latents_to_keep >= n_latents_original:
        return autoencoder
    device: torch.device = autoencoder.encoder.weight.device
    # create the dead latent mask (True for live latents, False for dead latents)
    mask = torch.ones(n_latents_original, dtype=torch.bool, device=device)
    if preserve_indices or not hasattr(autoencoder, "latents_activation_frequency"):
        # drop the last latents
        mask[n_latents_to_keep:] = 0
    else:
        # drop the least active latents
        order = torch.argsort(autoencoder.latents_activation_frequency, descending=True)
        mask[order[n_latents_to_keep:]] = 0
    # apply the mask to a new autoencoder
    n_latents = int(mask.sum().item())
    d_model = autoencoder.pre_bias.shape[0]
    new_autoencoder = Autoencoder(n_latents, d_model).to(device)
    new_autoencoder.encoder.weight.data = autoencoder.encoder.weight[mask, :].clone()
    new_autoencoder.decoder.weight.data = autoencoder.decoder.weight[:, mask].clone()
    new_autoencoder.latent_bias.data = autoencoder.latent_bias[mask].clone()
    new_autoencoder.stats_last_nonzero.data = autoencoder.stats_last_nonzero[mask].clone()
    if hasattr(autoencoder, "latents_mean_square"):
        new_autoencoder.register_buffer(
            "latents_mean_square", torch.zeros(n_latents, dtype=torch.float)
        )
        new_autoencoder.latents_mean_square.data = autoencoder.latents_mean_square[mask].clone()
    if hasattr(autoencoder, "latents_activation_frequency"):
        new_autoencoder.register_buffer(
            "latents_activation_frequency", torch.ones(n_latents, dtype=torch.float)
        )
        new_autoencoder.latents_activation_frequency.data = (
            autoencoder.latents_activation_frequency[mask].clone()
        )
    return new_autoencoder

================
File: neuron_explainer/models/autoencoder.py
================
from typing import Callable
import torch
import torch.nn as nn
import torch.nn.functional as F
class Autoencoder(nn.Module):
    """Sparse autoencoder
    Implements:
        latents = activation(encoder(x - pre_bias) + latent_bias)
        recons = decoder(latents) + pre_bias
    """
    def __init__(
        self, n_latents: int, n_inputs: int, activation: Callable = nn.ReLU(), tied: bool = False
    ) -> None:
        """
        :param n_latents: dimension of the autoencoder latent
        :param n_inputs: dimensionality of the original data (e.g residual stream, number of MLP hidden units)
        :param activation: activation function
        :param tied: whether to tie the encoder and decoder weights
        """
        super().__init__()
        self.pre_bias = nn.Parameter(torch.zeros(n_inputs))
        self.encoder: nn.Module = nn.Linear(n_inputs, n_latents, bias=False)
        self.latent_bias = nn.Parameter(torch.zeros(n_latents))
        self.activation = activation
        if tied:
            self.decoder: nn.Linear | TiedTranspose = TiedTranspose(self.encoder)
        else:
            self.decoder = nn.Linear(n_latents, n_inputs, bias=False)
        self.stats_last_nonzero: torch.Tensor
        self.latents_activation_frequency: torch.Tensor
        self.latents_mean_square: torch.Tensor
        self.register_buffer("stats_last_nonzero", torch.zeros(n_latents, dtype=torch.long))
        self.register_buffer(
            "latents_activation_frequency", torch.ones(n_latents, dtype=torch.float)
        )
        self.register_buffer("latents_mean_square", torch.zeros(n_latents, dtype=torch.float))
    def encode_pre_act(self, x: torch.Tensor, latent_slice: slice = slice(None)) -> torch.Tensor:
        """
        :param x: input data (shape: [batch, n_inputs])
        :param latent_slice: slice of latents to compute
            Example: latent_slice = slice(0, 10) to compute only the first 10 latents.
        :return: autoencoder latents before activation (shape: [batch, n_latents])
        """
        x = x - self.pre_bias
        latents_pre_act = F.linear(
            x, self.encoder.weight[latent_slice], self.latent_bias[latent_slice]
        )
        return latents_pre_act
    def encode(self, x: torch.Tensor) -> torch.Tensor:
        """
        :param x: input data (shape: [batch, n_inputs])
        :return: autoencoder latents (shape: [batch, n_latents])
        """
        return self.activation(self.encode_pre_act(x))
    def decode(self, latents: torch.Tensor) -> torch.Tensor:
        """
        :param latents: autoencoder latents (shape: [batch, n_latents])
        :return: reconstructed data (shape: [batch, n_inputs])
        """
        return self.decoder(latents) + self.pre_bias
    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        :param x: input data (shape: [batch, n_inputs])
        :return:  autoencoder latents pre activation (shape: [batch, n_latents])
                  autoencoder latents (shape: [batch, n_latents])
                  reconstructed data (shape: [batch, n_inputs])
        """
        latents_pre_act = self.encode_pre_act(x)
        latents = self.activation(latents_pre_act)
        recons = self.decode(latents)
        # set all indices of self.stats_last_nonzero where (latents != 0) to 0
        self.stats_last_nonzero *= (latents == 0).all(dim=0).long()
        self.stats_last_nonzero += 1
        return latents_pre_act, latents, recons
    @classmethod
    def from_state_dict(
        cls, state_dict: dict[str, torch.Tensor], strict: bool = True
    ) -> "Autoencoder":
        n_latents, d_model = state_dict["encoder.weight"].shape
        autoencoder = cls(n_latents, d_model)
        # Retrieve activation
        activation_class_name = state_dict.pop("activation", "ReLU")
        activation_class = ACTIVATIONS_CLASSES.get(activation_class_name, nn.ReLU)
        activation_state_dict = state_dict.pop("activation_state_dict", {})
        if hasattr(activation_class, "from_state_dict"):
            autoencoder.activation = activation_class.from_state_dict(
                activation_state_dict, strict=strict
            )
        else:
            autoencoder.activation = activation_class()
            if hasattr(autoencoder.activation, "load_state_dict"):
                autoencoder.activation.load_state_dict(activation_state_dict, strict=strict)
        # Load remaining state dict
        autoencoder.load_state_dict(state_dict, strict=strict)
        return autoencoder
    def state_dict(self, destination=None, prefix="", keep_vars=False):
        sd = super().state_dict(destination, prefix, keep_vars)
        sd[prefix + "activation"] = self.activation.__class__.__name__
        if hasattr(self.activation, "state_dict"):
            sd[prefix + "activation_state_dict"] = self.activation.state_dict()
        return sd
class TiedTranspose(nn.Module):
    def __init__(self, linear: nn.Linear):
        super().__init__()
        self.linear = linear
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        assert self.linear.bias is None
        return F.linear(x, self.linear.weight.t(), None)
    @property
    def weight(self) -> torch.Tensor:
        return self.linear.weight.t()
    @property
    def bias(self) -> torch.Tensor:
        return self.linear.bias
class TopK(nn.Module):
    def __init__(self, k: int, postact_fn: Callable = nn.ReLU()) -> None:
        super().__init__()
        self.k = k
        self.postact_fn = postact_fn
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        topk = torch.topk(x, k=self.k, dim=-1)
        values = self.postact_fn(topk.values)
        # make all other values 0
        result = torch.zeros_like(x)
        result.scatter_(-1, topk.indices, values)
        return result
    def state_dict(self, destination=None, prefix="", keep_vars=False):
        state_dict = super().state_dict(destination, prefix, keep_vars)
        state_dict.update({prefix + "k": self.k, prefix + "postact_fn": self.postact_fn.__class__.__name__})
        return state_dict
    @classmethod
    def from_state_dict(cls, state_dict: dict[str, torch.Tensor], strict: bool = True) -> "TopK":
        k = state_dict["k"]
        postact_fn = ACTIVATIONS_CLASSES[state_dict["postact_fn"]]()
        return cls(k=k, postact_fn=postact_fn)
ACTIVATIONS_CLASSES = {
    "ReLU": nn.ReLU,
    "Identity": nn.Identity,
    "TopK": TopK,
}

================
File: neuron_explainer/models/hooks.py
================
from collections import OrderedDict
from copy import deepcopy
import torch
class Hooks:
    """A callable that is a sequence of callables"""
    def __init__(self):
        self._hooks = []
        self.bound_kwargs = {}
    def __call__(self, x, *args, **kwargs):
        for hook in self._hooks:
            x = hook(x, *args, **kwargs, **self.bound_kwargs)
        return x
    def append(self, hook):
        self._hooks.append(hook)
        return self
    def bind(self, **kwargs):
        for key, value in kwargs.items():
            if key in self.bound_kwargs:
                raise ValueError(f"Key {key} already bound")
            self.bound_kwargs[key] = value
        return self
    def unbind(self, keys: list):
        for key in keys:
            del self.bound_kwargs[key]
        return self
    def __repr__(self, indent=0, name=None):
        import inspect
        indent_str = " " * indent
        full_name = f"{name}" if name is not None else self.name
        if self.bound_kwargs:
            full_name += f" {self.bound_kwargs}"
        if self.is_empty():
            return f"{indent_str}{full_name}"
        def hook_repr(hook):
            if "indent" in inspect.signature(hook.__class__.__repr__).parameters:
                return hook.__repr__(indent=indent + 4)
            else:
                return indent_str + " " * 4 + repr(hook)
        hooks_repr = "\n".join(f"{hook_repr(hook)}" for hook in self._hooks)
        return f"{indent_str}{full_name}\n{hooks_repr}"
    @property
    def name(self):
        return self.__class__.__name__
    def is_empty(self):
        return len(self._hooks) == 0
# takes a gradient hook and makes into a forward pass hook
def grad_hook_wrapper(grad_hook):
    def fwd_hook(act, *args, **kwargs):
        class _IdentityWithGradHook(torch.autograd.Function):
            @staticmethod
            def forward(ctx, tensor):
                return tensor
            @staticmethod
            def backward(ctx, grad_output):
                grad_output = grad_hook(grad_output, *args, **kwargs)
                return grad_output
        return _IdentityWithGradHook.apply(act)
    return fwd_hook
class HookCollection:
    def __init__(self):
        self.all_hooks = OrderedDict()
    def bind(self, **kwargs):
        for hook in self.all_hooks.values():
            hook.bind(**kwargs)
        return self
    def unbind(self, keys):
        for hook in self.all_hooks.values():
            hook.unbind(keys)
        return self
    def append_all(self, hook):
        for hooks in self.all_hooks.values():
            try:
                hooks.append_all(hook)
            except AttributeError:
                hooks.append(hook)
        return self
    def append_to_path(self, hook_location_name, hook):
        """
        Adds a hook to a location in a nested hook collection with a dot-separated name.
        e.g. `self.append_to_path("resid.torso.post_mlp.fwd", hook)` adds `hook` to
        `self.all_hooks["resid"].all_hooks["torso"].all_hooks["post_mlp"].all_hooks["fwd"]`
        """
        hook_location_parts = hook_location_name.split(".", 1)  # split at first dot, if present
        top_level_location = hook_location_parts[0]
        assert top_level_location in self.all_hooks
        if len(hook_location_parts) == 1:  # no dot in path
            self.all_hooks[top_level_location].append(hook)
        else:  # at least one dot in path -> split outputs two parts
            sub_location = hook_location_parts[1]
            self.all_hooks[top_level_location].append_to_path(sub_location, hook)
        return self
    def __deepcopy__(self, memo):
        # can't use deepcopy because of __getattr__
        new = self.__class__()
        new.all_hooks = deepcopy(self.all_hooks)
        return new
    def add_subhooks(self, name, subhooks):
        self.all_hooks[name] = subhooks
        return self
    def __getattr__(self, name):
        if name in self.all_hooks:
            return self.all_hooks[name]
        else:
            raise AttributeError(f"HookCollection has no attribute {name}")
    def __repr__(self, indent=0, name=None):
        indent_str = " " * indent
        full_name = f"{name}" if name is not None else self.__class__.__name__
        prefix = f"{name}." if name is not None else ""
        hooks_repr = "\n".join(
            hook.__repr__(indent + 4, f"{prefix}{hook_name}")
            for hook_name, hook in self.all_hooks.items()
        )
        return f"{indent_str}{full_name}\n{hooks_repr}"
    def is_empty(self):
        return all(hook.is_empty() for hook in self.all_hooks.values())
class FwdBwdHooks(HookCollection):
    def __init__(self):
        super().__init__()
        # By default, all grad hooks are applied after all forward hooks.  This way,
        # the gradients are given for the final "hooked" output of the forward pass.
        # If you want gradients for an intermediate output, you should simply
        # append_fwd(from_grad_hook(hook)) at the appropriate time.
        self.add_subhooks("fwd", Hooks())
        self.add_subhooks("bwd", WrapperHooks(wrapper=grad_hook_wrapper))
        self.add_subhooks("fwd2", Hooks())
    def append_fwd(self, fwd_hook):
        self.fwd.append(fwd_hook)
        return self
    def append_bwd(self, bwd_hook):
        self.bwd.append(bwd_hook)
        return self
    def append_fwd2(self, fwd2_hook):
        self.fwd2.append(fwd2_hook)
        return self
    def __call__(self, x, *args, **kwargs):
        # hooks into fwd, then bwd, then fwd2
        x = self.fwd(x, *args, **kwargs)
        x = self.bwd(x, *args, **kwargs)
        x = self.fwd2(x, *args, **kwargs)
        return x
class MLPHooks(HookCollection):
    def __init__(self):
        super().__init__()
        self.add_subhooks("pre_act", FwdBwdHooks())
        self.add_subhooks("post_act", FwdBwdHooks())
class NormalizationHooks(HookCollection):
    def __init__(self):
        super().__init__()
        self.add_subhooks("post_mean_subtraction", FwdBwdHooks())
        self.add_subhooks("scale", FwdBwdHooks())
        self.add_subhooks("post_scale", FwdBwdHooks())
class AttentionHooks(HookCollection):
    def __init__(self):
        super().__init__()
        self.add_subhooks("q", FwdBwdHooks())
        self.add_subhooks("k", FwdBwdHooks())
        self.add_subhooks("v", FwdBwdHooks())
        self.add_subhooks("qk_logits", FwdBwdHooks())
        self.add_subhooks("qk_softmax_denominator", FwdBwdHooks())
        self.add_subhooks("qk_probs", FwdBwdHooks())
        self.add_subhooks("v_out", FwdBwdHooks())  # pre-final projection
class ResidualStreamTorsoHooks(HookCollection):
    def __init__(self):
        super().__init__()
        self.add_subhooks("post_ln_attn", FwdBwdHooks())
        self.add_subhooks("ln_attn", NormalizationHooks())
        self.add_subhooks("delta_attn", FwdBwdHooks())
        self.add_subhooks("post_attn", FwdBwdHooks())
        self.add_subhooks("ln_mlp", NormalizationHooks())
        self.add_subhooks("post_ln_mlp", FwdBwdHooks())
        self.add_subhooks("delta_mlp", FwdBwdHooks())
        self.add_subhooks("post_mlp", FwdBwdHooks())
class ResidualStreamHooks(HookCollection):
    def __init__(self):
        super().__init__()
        self.add_subhooks("post_emb", FwdBwdHooks())
        self.add_subhooks("torso", ResidualStreamTorsoHooks())
        self.add_subhooks("ln_f", NormalizationHooks())
        self.add_subhooks("post_ln_f", FwdBwdHooks())
class TransformerHooks(HookCollection):
    def __init__(self):
        super().__init__()
        self.add_subhooks("mlp", MLPHooks())
        self.add_subhooks("attn", AttentionHooks())
        self.add_subhooks("resid", ResidualStreamHooks())
        self.add_subhooks("logits", FwdBwdHooks())
class WrapperHooks(Hooks):
    def __init__(self, wrapper):
        self.wrapper = wrapper
        super().__init__()
    def append(self, fn):
        self._hooks.append(self.wrapper(fn))
class ConditionalHooks(Hooks):
    def __init__(self, condition):
        self.condition = condition
        super().__init__()
    def __call__(self, x, *args, **kwargs):
        cond = self.condition(x, *args, **kwargs)
        if cond:
            x = super().__call__(x, *args, **kwargs)
        return x
class AtLayers(ConditionalHooks):
    def __init__(self, at_layers: int | list[int]):
        if isinstance(at_layers, int):
            at_layers = [at_layers]
        self.at_layers = at_layers
        def at_layers_condition(x, *, layer, **kwargs):
            return layer in at_layers
        super().__init__(condition=at_layers_condition)
    @property
    def name(self):
        return f"{self.__class__.__name__}({self.at_layers})"
class AutoencoderHooks(HookCollection):
    """
    Hooks into the hidden dimension of an autoencoder.
    """
    def __init__(self, encode, decode, add_error=False):
        super().__init__()
        # hooks
        self.add_subhooks("latents", FwdBwdHooks())
        self.add_subhooks("reconstruction", FwdBwdHooks())
        self.add_subhooks("error", FwdBwdHooks())
        # autoencoder functions
        self.encode = encode
        self.decode = decode
        # if add_error is True, add the error to the reconstruction.
        self.add_error = add_error
    def __call__(self, x, *args, **kwargs):
        latents = self.encode(x)
        if self.add_error:
            # Here, the latents are cloned twice:
            # - the first clone is passed through `self.latents` and `self.reconstruction`
            # - the second clone is passed through `self.error`
            latents_to_hook = latents.clone()
            latents_to_error = latents.clone()
        else:
            latents_to_hook = latents
        latents_to_hook = self.latents(latents_to_hook, *args, **kwargs)  # call hooks
        reconstruction = self.decode(latents_to_hook)
        reconstruction = self.reconstruction(reconstruction, *args, **kwargs)  # call hooks
        if self.add_error:
            error = x - self.decode(latents_to_error)
            error = self.error(error, *args, **kwargs)  # call hooks
            return reconstruction + error
        else:
            error = x - reconstruction
            error = self.error(error, *args, **kwargs)  # call hooks
            return reconstruction
    def __deepcopy__(self, memo):
        # can't use deepcopy because of __getattr__
        new = self.__class__(self.encode, self.decode, self.add_error)
        new.all_hooks = deepcopy(self.all_hooks)
        return new

================
File: neuron_explainer/models/inference_engine_type_registry.py
================
# This file supports translation between universal representation and inference engine-specific
# representations. The current codebase only supports one inference engine, but that may change in
# the future.
from enum import Enum
from neuron_explainer.models.model_component_registry import ActivationLocationType, LayerIndex
# TODO: Consider using a stronger type here.
StandardModelHookLocationType = str
HookLocationType = StandardModelHookLocationType
class InferenceEngineType(str, Enum):
    STANDARD = "standard"
_standard_model_hook_location_type_by_activation_location_type: dict[
    ActivationLocationType, StandardModelHookLocationType
] = {
    ActivationLocationType.RESID_POST_EMBEDDING: "resid/post_emb",
    ActivationLocationType.RESID_POST_MLP: "resid/post_mlp",
    ActivationLocationType.MLP_PRE_ACT: "mlp/pre_act",
    ActivationLocationType.MLP_POST_ACT: "mlp/post_act",
    ActivationLocationType.ATTN_QUERY: "attn/q",
    ActivationLocationType.ATTN_KEY: "attn/k",
    ActivationLocationType.ATTN_VALUE: "attn/v",
    ActivationLocationType.ATTN_QK_LOGITS: "attn/qk_logits",
    ActivationLocationType.ATTN_QK_PROBS: "attn/qk_probs",
    ActivationLocationType.ATTN_WEIGHTED_SUM_OF_VALUES: "attn/v_out",
    ActivationLocationType.RESID_DELTA_ATTN: "resid/delta_attn",
    ActivationLocationType.RESID_DELTA_MLP: "resid/delta_mlp",
    ActivationLocationType.RESID_POST_ATTN: "resid/post_attn",
    ActivationLocationType.RESID_POST_MLP: "resid/post_mlp",
    ActivationLocationType.LOGITS: "logits",
    ActivationLocationType.RESID_FINAL_LAYER_NORM_SCALE: "resid/ln_f/scale",
    ActivationLocationType.ATTN_INPUT_LAYER_NORM_SCALE: "resid/ln_attn/scale",
    ActivationLocationType.MLP_INPUT_LAYER_NORM_SCALE: "resid/ln_mlp/scale",
}
_hook_location_type_by_activation_location_type_by_inference_engine_type: dict[
    InferenceEngineType, dict[ActivationLocationType, HookLocationType]
] = {
    InferenceEngineType.STANDARD: _standard_model_hook_location_type_by_activation_location_type,
}
_activation_location_type_by_hook_location_type_by_inference_engine_type: dict[
    InferenceEngineType, dict[HookLocationType, ActivationLocationType]
] = {
    inference_engine_type: {v: k for k, v in hook_location_type_by_activation_location_type.items()}
    for inference_engine_type, hook_location_type_by_activation_location_type in _hook_location_type_by_activation_location_type_by_inference_engine_type.items()
}
standard_model_activation_location_types: set[ActivationLocationType] = set(
    _hook_location_type_by_activation_location_type_by_inference_engine_type[
        InferenceEngineType.STANDARD
    ].keys()
)
def get_hook_location_type_for_activation_location_type(
    activation_location_type: ActivationLocationType, inference_engine_type: InferenceEngineType
) -> HookLocationType:
    assert (
        inference_engine_type
        in _hook_location_type_by_activation_location_type_by_inference_engine_type
    ), f"Unknown inference_engine_type {inference_engine_type}"
    return _hook_location_type_by_activation_location_type_by_inference_engine_type[
        inference_engine_type
    ][activation_location_type]
def get_activation_location_type_for_hook_location_type(
    hook_location_type: HookLocationType, inference_engine_type: InferenceEngineType
) -> ActivationLocationType:
    assert (
        inference_engine_type
        in _activation_location_type_by_hook_location_type_by_inference_engine_type
    ), f"Unknown inference_engine_type {inference_engine_type}"
    return _activation_location_type_by_hook_location_type_by_inference_engine_type[
        inference_engine_type
    ][hook_location_type]
def parse_standard_location_str(location: str) -> tuple[str, LayerIndex]:
    """
    Our transformer implementation uses location strings like "7/mlp/post_act" or "resid/post_emb"
    to capture both the location type and the layer index (if applicable). This function parses
    those strings, returning the location type and layer index. The location type is specific to our
    transformer implementation: call get_activation_location_type_for_hook_location_type to convert
    it into an ActivationLocationType.
    """
    if location[0].isdigit():
        # Example: 7/mlp/post_act
        return location[location.find("/") + 1 :], int(location.split("/")[0])
    else:
        # Example: resid/post_emb
        return location, None

================
File: neuron_explainer/models/model_component_registry.py
================
"""
This file contains Enums of three kinds of objects that are common across
LM architectures: weight locations, activation locations, and model dimensions.
The weight locations and activation locations are associated
to tuples of dimensions, to define their standardized shapes. This file is intended to
serve as a source of truth for those standards.
Abbreviations appearing in this file:
attn = attention
emb = embedding
ln = layer norm
ln_f = final layer norm
mlp = multi-layer perceptron
act = activation
resid = residual
q = query
k = key
v = value
qk = query-key inner product (with 1/sqrt(query_and_key_channels) scaling already applied)
"""
from dataclasses import dataclass
from enum import Enum, EnumMeta, auto, unique
from typing import Optional
class WeightLocationType(str, Enum):
    """These are the names of tensors that are expected to be found in model weights."""
    MLP_TO_HIDDEN = "mlp.to_hidden"
    MLP_TO_RESIDUAL = "mlp.to_residual"
    EMBEDDING = "embedding"
    UNEMBEDDING = "unembedding"
    POSITION_EMBEDDING = "position_embedding"
    ATTN_TO_QUERY = "attn.to_query"
    ATTN_TO_KEY = "attn.to_key"
    ATTN_TO_VALUE = "attn.to_value"
    ATTN_TO_RESIDUAL = "attn.to_residual"
    LAYER_NORM_GAIN_FINAL = "layer_norm_gain.final"
    LAYER_NORM_BIAS_FINAL = "layer_norm_bias.final"
    LAYER_NORM_GAIN_PRE_ATTN = "layer_norm_gain.pre_attn"
    LAYER_NORM_GAIN_PRE_MLP = "layer_norm_gain.pre_mlp"
    @property
    def is_mlp_specific(self) -> bool:
        return self in {
            WeightLocationType.MLP_TO_HIDDEN,
            WeightLocationType.MLP_TO_RESIDUAL,
            WeightLocationType.LAYER_NORM_GAIN_PRE_MLP,
        }
    @property
    def has_no_layers(self) -> bool:
        # if True, there is one of this type of weight tensor per model, and the layer index is set
        # as None wherever used (these occur at the very beginning or end of the model)
        # if False, there is one of this type of weight tensor per layer, and the tensor additionally
        # requires a layer index to specify
        return self in {
            WeightLocationType.EMBEDDING,
            WeightLocationType.UNEMBEDDING,
            WeightLocationType.POSITION_EMBEDDING,
            WeightLocationType.LAYER_NORM_GAIN_FINAL,
            WeightLocationType.LAYER_NORM_BIAS_FINAL,
        }
    @property
    def is_absolute_position_embedding_specific(self) -> bool:
        return self in {WeightLocationType.POSITION_EMBEDDING}
    @property
    def shape_spec(self) -> tuple["Dimension", ...]:
        return weight_shape_by_location_type[self]
class EnumMetaContains(EnumMeta):
    def __contains__(cls, item: object) -> bool:
        # enables the syntax "if item in enum:"
        return isinstance(item, cls) or item in cls._value2member_map_
LayerIndex = int | None  # None refers to an activation with no layer index (e.g. embeddings)
class LocationWithinLayer(int, Enum):
    """
    Coarsely specifies the location of a tensor within a layer. Each of the following is mapped to an
    int, and the ordering is from the beginning of the layer to the end.
    This is to be inferred for a scalar deriver based on information available to it.
    The level of granularity is enough to determine whether one node is upstream of a different node,
    but no more. For example, it doesn't distinguish between attention pre- and post-softmax, but it does
    distinguish between an attention head and the residual stream locations immediately before and after
    the attention layer.
    THESE VALUES ARE NOT INTENDED TO BE SERIALIZED.
    The strategy for inferring the LocationWithinLayer is to check
    (1) the node type of the ScalarDeriver
    (2) the activation location type of the ScalarDeriver (if it corresponds to a raw activation)
    (3) the dst of the ScalarDeriver
    (4) any other information available to the ScalarDeriver.
    Based on any one
    piece of information alone, it may be ambiguous what the location within layer is. For example,
    DerivedScalarType.RESID_POST_ATTN and DerivedScalarType.RESID_POST_MLP both correspond to the same
    node type, but post-attn appears earlier in the layer than post-mlp. So the location within layer
    is left ambiguous after checking the node, but clarified after checking the activation location type.
    """
    END_OF_PREV_LAYER = (
        auto()
    )  # auto() assigns increasing integer values to the enum values, starting from 1
    ATTN = auto()
    RESID_POST_ATTN = auto()
    MLP = auto()
    RESID_POST_MLP = auto()
class NodeType(str, Enum):
    """
    A "node" is defined as a model component associated with a scalar activation per
    token or per token pair. The canonical example is an MLP neuron. An activation
    for which the NodeType is defined has the node as the last dimension of the
    activation tensor.
    """
    ATTENTION_HEAD = "attention_head"
    QK_CHANNEL = "qk_channel"
    V_CHANNEL = "v_channel"
    MLP_NEURON = "mlp_neuron"
    AUTOENCODER_LATENT = "autoencoder_latent"
    MLP_AUTOENCODER_LATENT = "mlp_autoencoder_latent"
    ATTENTION_AUTOENCODER_LATENT = "attention_autoencoder_latent"
    # TODO: remove this hack, and make NodeType depend on the token dimensions
    AUTOENCODER_LATENT_BY_TOKEN_PAIR = "autoencoder_latent_by_token_pair"
    LAYER = "layer"
    RESIDUAL_STREAM_CHANNEL = "residual_stream_channel"
    VOCAB_TOKEN = "vocab_token"
    @property
    def location_within_layer(self) -> Optional["LocationWithinLayer"]:
        # this uses the information available to infer the location within a layer of a specific node_type.
        # It returns None in cases where the location within layer is ambiguous based on the information
        # provided; e.g. for residual stream node types, it might be post-attn or post-mlp.
        # (with activation location type
        # for additional clarification). It returns None in cases where the location within layer is ambiguous based on the information
        # provided; one example is for autoencoder latents, which might be based on any dst. In this case, further information from the
        # DSTConfig is used.
        # It throws an error if the node_type is *never* associated with a location within layer (e.g. vocab tokens)
        match self:
            case NodeType.MLP_NEURON:
                return LocationWithinLayer.MLP
            case NodeType.ATTENTION_HEAD | NodeType.QK_CHANNEL | NodeType.V_CHANNEL:
                return LocationWithinLayer.ATTN
            case (
                NodeType.RESIDUAL_STREAM_CHANNEL
                | NodeType.LAYER
                | NodeType.AUTOENCODER_LATENT
                | NodeType.MLP_AUTOENCODER_LATENT
                | NodeType.ATTENTION_AUTOENCODER_LATENT
                | NodeType.AUTOENCODER_LATENT_BY_TOKEN_PAIR
            ):
                # these node types are ambiguous based on the information provided
                return None
            case NodeType.VOCAB_TOKEN:
                # users should not be asking about the location within layer of vocab tokens; this indicates something's wrong
                raise ValueError("Vocab tokens don't have a location within layer")
            case _:
                raise NotImplementedError(f"Unknown node type {self=}")
    @property
    def is_autoencoder_latent(self) -> bool:
        return self in {
            NodeType.AUTOENCODER_LATENT,
            NodeType.MLP_AUTOENCODER_LATENT,
            NodeType.ATTENTION_AUTOENCODER_LATENT,
            NodeType.AUTOENCODER_LATENT_BY_TOKEN_PAIR,
        }
class ActivationLocationType(str, Enum, metaclass=EnumMetaContains):
    """These are the names of activations expected to be instantiated during a forward pass. All activations are
    pre-layer norm unless otherwise specified (RESID_POST_XYZ_LAYER_NORM)."""
    RESID_POST_EMBEDDING = "resid.post_emb"
    RESID_DELTA_ATTN = "resid.delta_attn"
    RESID_POST_ATTN = "resid.post_attn"
    RESID_DELTA_MLP = "resid.delta_mlp"
    RESID_POST_MLP = "resid.post_mlp"
    RESID_POST_MLP_LAYER_NORM = "resid.post_mlp_ln"
    RESID_POST_ATTN_LAYER_NORM = "resid.post_attn_ln"
    RESID_POST_FINAL_LAYER_NORM = "resid.post_ln_f"
    MLP_INPUT_LAYER_NORM_SCALE = "mlp_ln.scale"
    ATTN_INPUT_LAYER_NORM_SCALE = "attn_ln.scale"
    RESID_FINAL_LAYER_NORM_SCALE = "resid.ln_f.scale"
    ATTN_QUERY = "attn.q"
    ATTN_KEY = "attn.k"
    ATTN_VALUE = "attn.v"
    ATTN_QK_LOGITS = "attn.qk_logits"
    ATTN_QK_PROBS = "attn.qk_probs"
    ATTN_WEIGHTED_SUM_OF_VALUES = "attn.v_out"
    MLP_PRE_ACT = "mlp.pre_act"
    MLP_POST_ACT = "mlp.post_act"
    LOGITS = "logits"
    ONLINE_AUTOENCODER_LATENT = "online_autoencoder_latent"
    ONLINE_MLP_AUTOENCODER_LATENT = "online_mlp_autoencoder_latent"
    ONLINE_ATTENTION_AUTOENCODER_LATENT = "online_attention_autoencoder_latent"
    ONLINE_MLP_AUTOENCODER_ERROR = "online_mlp_autoencoder_error"
    ONLINE_RESIDUAL_MLP_AUTOENCODER_ERROR = "online_residual_mlp_autoencoder_error"
    ONLINE_RESIDUAL_ATTENTION_AUTOENCODER_ERROR = "online_residual_attention_autoencoder_error"
    @property
    def has_no_layers(self) -> bool:
        # if True, there is one of this type of activation tensor per model, and the layer index is set
        # as None wherever used (these occur at the very beginning or end of the model)
        # if False, there is one of this type of activation tensor per layer, and the tensor additionally
        # requires a layer index to specify
        return self in {
            ActivationLocationType.RESID_POST_EMBEDDING,
            ActivationLocationType.RESID_FINAL_LAYER_NORM_SCALE,
            ActivationLocationType.RESID_POST_FINAL_LAYER_NORM,
            ActivationLocationType.LOGITS,
        }
    @property
    def shape_spec_per_token_sequence(self) -> tuple["Dimension", ...]:
        return _activation_shape_per_token_sequence_by_location_type[self]
    @property
    def ndim_per_token_sequence(self) -> int:
        return len(self.shape_spec_per_token_sequence)
    @property
    def exists_by_default(self) -> bool:
        # this returns True if the activation is expected to exist by default in the model, and False if
        # it needs to be added using hooks
        return self not in {
            ActivationLocationType.ONLINE_AUTOENCODER_LATENT,
        }
    @property
    def node_type(self) -> NodeType:
        """The last index of an activation tensor can correspond to a type of object
        in the network called a 'node'. This can be an MLP neuron, an attention head, an autoencoder
        latent, etc. If we don't yet have a name for the last dimension of an activation tensor,
        this throws an error."""
        last_dimension = self.shape_spec_per_token_sequence[-1]
        if last_dimension in node_type_by_dimension:
            return node_type_by_dimension[last_dimension]
        else:
            raise NotImplementedError(f"Unknown node type for {last_dimension=}")
    @property
    def location_within_layer(self) -> LocationWithinLayer | None:
        # this uses the information available to infer the location within a layer of a specific node_type (with activation location type
        # for additional clarification). It returns None in cases where the location within layer is ambiguous based on the information
        # provided; one example is for autoencoder latents, which might be based on any dst. In this case, further information from the
        # DSTConfig is needed.
        # It throws an error if the node_type is not associated with a location within layer.
        if self.node_type.location_within_layer is None:
            if self.node_type == NodeType.RESIDUAL_STREAM_CHANNEL:
                if self == ActivationLocationType.RESID_POST_EMBEDDING:
                    return None
                elif self == ActivationLocationType.RESID_DELTA_ATTN:
                    return LocationWithinLayer.ATTN
                elif self == ActivationLocationType.RESID_POST_ATTN:
                    return LocationWithinLayer.RESID_POST_ATTN
                elif self == ActivationLocationType.RESID_DELTA_MLP:
                    return LocationWithinLayer.MLP
                elif self == ActivationLocationType.RESID_POST_MLP:
                    return LocationWithinLayer.RESID_POST_MLP
                else:
                    return None
            else:
                return None
        else:
            return self.node_type.location_within_layer
class Dimension(str, Enum):
    """Dimensions correspond to the names of dimensions of activation tensors, and can depend on the input,
    the model, or e.g. parameters of added subgraphs such as autoencoders.
    The dimensions below are taken to be 'per layer' wherever applicable.
    Dimensions associated with attention heads (e.g. value channels) are taken to be 'per attention head'.
    """
    SEQUENCE_TOKENS = "sequence_tokens"
    ATTENDED_TO_SEQUENCE_TOKENS = "attended_to_sequence_tokens"
    """These are the names of dimensions of activation tensors that are intrinsic to a model,
    and are not a consequence of a particular input sequence. The shape of activations will in general
    depend on these and on Dimension, above. The shape of weights will in general depend only on
    these."""
    # "context" refers to the number of tokens in the sequence being processed by the model
    # "max_context_length" refers to the maximum number of tokens that can be processed by the
    # model (relevant for models with absolute position embeddingsg)
    MAX_CONTEXT_LENGTH = "max_context_length"
    # "residual_stream_channels" means the same as "d_model"
    RESIDUAL_STREAM_CHANNELS = "residual_stream_channels"
    VOCAB_SIZE = "vocab_size"
    ATTN_HEADS = "attn_heads"
    QUERY_AND_KEY_CHANNELS = "query_and_key_channels"
    VALUE_CHANNELS = "value_channels"
    MLP_ACTS = "mlp_acts"
    LAYERS = "layers"
    SINGLETON = "singleton"  # always 1
    """These are the names of dimensions that are not intrinsic to a model's activations, but that in some
    way parameterize its activations (currently just including autoencoder latents, but in future possibly
    including other methods for finding useful directions within activations)."""
    AUTOENCODER_LATENTS = "autoencoder_latents"
    # TODO: remove this hack, and make NodeType depend on the token dimensions
    AUTOENCODER_LATENTS_BY_TOKEN_PAIR = "autoencoder_latents_by_token_pair"
    @property
    def is_sequence_token_dimension(self) -> bool:
        return self in {Dimension.SEQUENCE_TOKENS, Dimension.ATTENDED_TO_SEQUENCE_TOKENS}
    @property
    def is_parameterized_dimension(self) -> bool:
        return self in {Dimension.AUTOENCODER_LATENTS}
    @property
    def is_model_intrinsic(self) -> bool:
        """this is True for dimensions that depend only on the model. This is False for dimensions that depend on either (1) the input being processed, or (2) some parameterization
        of the model activations (e.g. autoencoder latents).
        """
        return not (self.is_parameterized_dimension or self.is_sequence_token_dimension)
node_type_by_dimension: dict[Dimension, NodeType] = {
    Dimension.MLP_ACTS: NodeType.MLP_NEURON,
    Dimension.ATTN_HEADS: NodeType.ATTENTION_HEAD,
    Dimension.QUERY_AND_KEY_CHANNELS: NodeType.QK_CHANNEL,
    Dimension.VALUE_CHANNELS: NodeType.V_CHANNEL,
    Dimension.SINGLETON: NodeType.LAYER,
    Dimension.RESIDUAL_STREAM_CHANNELS: NodeType.RESIDUAL_STREAM_CHANNEL,
    Dimension.VOCAB_SIZE: NodeType.VOCAB_TOKEN,
    Dimension.AUTOENCODER_LATENTS: NodeType.AUTOENCODER_LATENT,
    Dimension.AUTOENCODER_LATENTS_BY_TOKEN_PAIR: NodeType.AUTOENCODER_LATENT_BY_TOKEN_PAIR,
}
_activation_shape_per_token_sequence_by_location_type: dict[
    ActivationLocationType, tuple[Dimension, ...]
] = {
    # this is a standard convention for the shape of activation tensors per token sequence. All activations
    # by convention have either Dimension.SEQUENCE_TOKENS, Dimension.ATTENDED_TO_SEQUENCE_TOKENS,
    # or both, as their first dimension. The ordering of dimensions is generally:
    # 1. tokens
    # 2. dimensions with a privileged basis (e.g. attention heads, MLP neurons), descending in order of
    # hierarchy
    # 3. dimensions without a privileged basis (e.g. residual stream or attention head hidden dimension)
    ActivationLocationType.RESID_POST_EMBEDDING: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.RESIDUAL_STREAM_CHANNELS,
    ),
    ActivationLocationType.RESID_DELTA_ATTN: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.RESIDUAL_STREAM_CHANNELS,
    ),
    ActivationLocationType.RESID_POST_ATTN: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.RESIDUAL_STREAM_CHANNELS,
    ),
    ActivationLocationType.RESID_DELTA_MLP: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.RESIDUAL_STREAM_CHANNELS,
    ),
    ActivationLocationType.RESID_POST_MLP: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.RESIDUAL_STREAM_CHANNELS,
    ),
    ActivationLocationType.RESID_POST_MLP_LAYER_NORM: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.RESIDUAL_STREAM_CHANNELS,
    ),
    ActivationLocationType.RESID_POST_ATTN_LAYER_NORM: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.RESIDUAL_STREAM_CHANNELS,
    ),
    ActivationLocationType.RESID_POST_FINAL_LAYER_NORM: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.RESIDUAL_STREAM_CHANNELS,
    ),
    ActivationLocationType.MLP_INPUT_LAYER_NORM_SCALE: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.SINGLETON,
    ),
    ActivationLocationType.ATTN_INPUT_LAYER_NORM_SCALE: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.SINGLETON,
    ),
    ActivationLocationType.RESID_FINAL_LAYER_NORM_SCALE: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.SINGLETON,
    ),
    ActivationLocationType.ATTN_QUERY: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.ATTN_HEADS,
        Dimension.QUERY_AND_KEY_CHANNELS,
    ),
    ActivationLocationType.ATTN_KEY: (
        Dimension.ATTENDED_TO_SEQUENCE_TOKENS,
        Dimension.ATTN_HEADS,
        Dimension.QUERY_AND_KEY_CHANNELS,
    ),
    ActivationLocationType.ATTN_VALUE: (
        Dimension.ATTENDED_TO_SEQUENCE_TOKENS,
        Dimension.ATTN_HEADS,
        Dimension.VALUE_CHANNELS,
    ),
    ActivationLocationType.ATTN_QK_LOGITS: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.ATTENDED_TO_SEQUENCE_TOKENS,
        Dimension.ATTN_HEADS,
    ),
    ActivationLocationType.ATTN_QK_PROBS: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.ATTENDED_TO_SEQUENCE_TOKENS,
        Dimension.ATTN_HEADS,
    ),
    ActivationLocationType.ATTN_WEIGHTED_SUM_OF_VALUES: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.ATTN_HEADS,
        Dimension.VALUE_CHANNELS,
    ),
    ActivationLocationType.MLP_PRE_ACT: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.MLP_ACTS,
    ),
    ActivationLocationType.MLP_POST_ACT: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.MLP_ACTS,
    ),
    ActivationLocationType.ONLINE_AUTOENCODER_LATENT: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.AUTOENCODER_LATENTS,
    ),
    ActivationLocationType.ONLINE_MLP_AUTOENCODER_LATENT: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.AUTOENCODER_LATENTS,
    ),
    ActivationLocationType.ONLINE_ATTENTION_AUTOENCODER_LATENT: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.AUTOENCODER_LATENTS,
    ),
    ActivationLocationType.ONLINE_MLP_AUTOENCODER_ERROR: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.MLP_ACTS,
    ),
    ActivationLocationType.ONLINE_RESIDUAL_MLP_AUTOENCODER_ERROR: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.RESIDUAL_STREAM_CHANNELS,
    ),
    ActivationLocationType.ONLINE_RESIDUAL_ATTENTION_AUTOENCODER_ERROR: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.RESIDUAL_STREAM_CHANNELS,
    ),
    ActivationLocationType.LOGITS: (
        Dimension.SEQUENCE_TOKENS,
        Dimension.VOCAB_SIZE,
    ),
}
weight_shape_by_location_type: dict[WeightLocationType, tuple[Dimension, ...]] = {
    # this is a standard convention for the shape of weight tensors. All weights by convention have
    # the privileged basis at the top of the hierarchy first, if applicable (e.g. attention heads), and the
    # remaining dimensions are ordered: input, then output. Some tensors (e.g. biases, layernorm parameters) have only
    # a single dimension.
    WeightLocationType.POSITION_EMBEDDING: (
        Dimension.MAX_CONTEXT_LENGTH,
        Dimension.RESIDUAL_STREAM_CHANNELS,
    ),
    WeightLocationType.EMBEDDING: (
        Dimension.VOCAB_SIZE,
        Dimension.RESIDUAL_STREAM_CHANNELS,
    ),
    WeightLocationType.UNEMBEDDING: (
        Dimension.RESIDUAL_STREAM_CHANNELS,
        Dimension.VOCAB_SIZE,
    ),
    WeightLocationType.ATTN_TO_QUERY: (
        Dimension.ATTN_HEADS,
        Dimension.RESIDUAL_STREAM_CHANNELS,
        Dimension.QUERY_AND_KEY_CHANNELS,
    ),
    WeightLocationType.ATTN_TO_KEY: (
        Dimension.ATTN_HEADS,
        Dimension.RESIDUAL_STREAM_CHANNELS,
        Dimension.QUERY_AND_KEY_CHANNELS,
    ),
    WeightLocationType.ATTN_TO_VALUE: (
        Dimension.ATTN_HEADS,
        Dimension.RESIDUAL_STREAM_CHANNELS,
        Dimension.VALUE_CHANNELS,
    ),
    WeightLocationType.ATTN_TO_RESIDUAL: (
        Dimension.ATTN_HEADS,
        Dimension.VALUE_CHANNELS,
        Dimension.RESIDUAL_STREAM_CHANNELS,
    ),
    WeightLocationType.MLP_TO_HIDDEN: (
        Dimension.RESIDUAL_STREAM_CHANNELS,
        Dimension.MLP_ACTS,
    ),
    WeightLocationType.MLP_TO_RESIDUAL: (
        Dimension.MLP_ACTS,
        Dimension.RESIDUAL_STREAM_CHANNELS,
    ),
    WeightLocationType.LAYER_NORM_GAIN_PRE_MLP: (Dimension.RESIDUAL_STREAM_CHANNELS,),
    WeightLocationType.LAYER_NORM_GAIN_PRE_ATTN: (Dimension.RESIDUAL_STREAM_CHANNELS,),
    WeightLocationType.LAYER_NORM_GAIN_FINAL: (Dimension.RESIDUAL_STREAM_CHANNELS,),
    WeightLocationType.LAYER_NORM_BIAS_FINAL: (Dimension.RESIDUAL_STREAM_CHANNELS,),
}
def get_dimension_index_of_weight_location_type(
    weight_location_type: WeightLocationType, dimension: Dimension
) -> int:
    """Returns the index of a dimension within a weight tensor, and raises an error if the
    dimension is found 0 or >1 time (0 can happen, but >1 indicates a bug somewhere)."""
    assert weight_shape_by_location_type[weight_location_type].count(dimension) == 1
    return weight_shape_by_location_type[weight_location_type].index(dimension)
@unique
class PassType(str, Enum):
    FORWARD = "forward"
    BACKWARD = "backward"
    def __repr__(self) -> str:
        return f"'{self.value}'"
@dataclass(frozen=True)
class ActivationLocationTypeAndPassType:
    activation_location_type: ActivationLocationType
    pass_type: PassType
    @property
    def exists_by_default(self) -> bool:
        return self.activation_location_type.exists_by_default

================
File: neuron_explainer/models/model_context.py
================
import os
from abc import ABC, abstractmethod
from typing import Any
import tiktoken
import torch
import torch.nn as nn
from neuron_explainer.file_utils import CustomFileHandler
from neuron_explainer.models import Transformer, TransformerConfig
from neuron_explainer.models.inference_engine_type_registry import InferenceEngineType
from neuron_explainer.models.model_component_registry import (
    Dimension,
    LayerIndex,
    WeightLocationType,
    get_dimension_index_of_weight_location_type,
    weight_shape_by_location_type,
)
from neuron_explainer.models.model_registry import get_standard_model_spec
ALLOWED_SPECIAL_TOKENS = {"<|endoftext|>"}
class InvalidTokenException(Exception):
    pass
class ModelContext(ABC):
    def __init__(self, model_name: str, device: torch.device) -> None:
        self.model_name = model_name
        self.device = device
    # takes a WeightLocationType and optional layer
    # returns the tensor
    @abstractmethod
    def _get_weight_helper(
        self,
        location_type: WeightLocationType,
        layer: LayerIndex = None,
        device: torch.device | None = None,
    ) -> torch.Tensor:
        ...
    def get_weight(
        self,
        location_type: WeightLocationType,
        layer: LayerIndex = None,
        normalize_dim: Dimension | None = None,
        device: torch.device | None = None,
    ) -> torch.Tensor:
        """Returns the specified weights, with shape checking and optional normalization.
        Tensors returned by this method are not cloned, so please be sure not to perform in-place
        edits on them!
        """
        assert (
            location_type in weight_shape_by_location_type
        ), f"location_type_str {location_type} not found"
        weight = self._get_weight_helper(
            location_type=location_type, layer=layer, device=device or self.device
        )
        if normalize_dim is not None:
            weight = nn.functional.normalize(
                weight,
                dim=get_dimension_index_of_weight_location_type(location_type, normalize_dim),
            )
        weight_shape_spec = weight_shape_by_location_type[location_type]
        expected_shape = self.get_shape_from_spec(weight_shape_spec)
        assert (
            weight.shape == expected_shape
        ), f"Expected shape {expected_shape} but got {weight.shape}"
        # We don't want to return tensors that have gradients enabled, so we detach. Ideally we'd
        # also clone since we don't want callers to inadvertently edit the weights, but doing so
        # uses a lot of memory, so instead we just ask politely in the docstring.
        return weight.detach()
    # get Encoding -> call this in the base class
    @abstractmethod
    def get_encoding(self) -> tiktoken.Encoding:
        ...
    def encode(self, string: str) -> list[int]:
        return self.get_encoding().encode(string, allowed_special=ALLOWED_SPECIAL_TOKENS)
    def decode_token(self, token: int) -> str:
        return self.get_encoding().decode([token])
    def decode(self, token_list: list[int]) -> str:
        return self.get_encoding().decode(token_list)
    def encode_token_str(self, token_str: str) -> int:
        token_int_list = self.encode(token_str)
        if len(token_int_list) != 1:
            raise InvalidTokenException(
                f"'{token_str}' decoded to {token_int_list}; wanted exactly 1 token"
            )
        return token_int_list[0]
    @abstractmethod
    def get_dim_size(self, model_dimension_spec: Dimension) -> int:
        ...
    def get_shape_from_spec(self, shape_spec: tuple[Dimension, ...]) -> tuple[int, ...]:
        expected_shape: tuple[int, ...] = tuple(
            self.get_dim_size(dimension_spec) if dimension_spec != Dimension.SINGLETON else 1
            for dimension_spec in shape_spec
        )
        return expected_shape
    @abstractmethod
    def get_or_create_model(self) -> Transformer:
        """Returns an instantiated model which can be used to run forward passes.
        The first call to this method results in a new model being created. Subsequent calls return
        the same cached model instance.
        """
        ...
    def decode_token_list(self, token_list: list[int]) -> list[str]:
        return [self.decode_token(token=token) for token in token_list]
    def encode_token_str_list(self, token_str_list: list[str]) -> list[int]:
        return [self.encode_token_str(token_str=token_str) for token_str in token_str_list]
    @classmethod
    def from_model_type(
        cls,
        model_type: str,
        inference_engine_type: InferenceEngineType = InferenceEngineType.STANDARD,
        **kwargs: Any,
    ) -> "ModelContext":
        device = kwargs.pop("device", get_default_device())
        if inference_engine_type == InferenceEngineType.STANDARD:
            return StandardModelContext(model_name=model_type, device=device, **kwargs)
        else:
            raise ValueError(f"Unsupported inference_engine_type {inference_engine_type}")
    @property
    def n_neurons(self) -> int:
        return self.get_dim_size(Dimension.MLP_ACTS)
    @property
    def n_attention_heads(self) -> int:
        return self.get_dim_size(Dimension.ATTN_HEADS)
    @property
    def n_layers(self) -> int:
        return self.get_dim_size(Dimension.LAYERS)
    @property
    def n_residual_stream_channels(self) -> int:
        return self.get_dim_size(Dimension.RESIDUAL_STREAM_CHANNELS)
    @property
    def n_vocab(self) -> int:
        return self.get_dim_size(Dimension.VOCAB_SIZE)
    @property
    def n_context(self) -> int:
        return self.get_dim_size(Dimension.MAX_CONTEXT_LENGTH)
    @abstractmethod
    def get_model_config_as_dict(self) -> dict[str, Any]:
        ...
# Note: If you're seeing mysterious crashes while running on a MacBook, try switching from "mps" to
# "cpu".
def get_default_device() -> torch.device:
    # TODO: Figure out why test_interactive_model.py crashes on the "mps" backend, then remove
    # this workaround.
    is_pytest = "PYTEST_CURRENT_TEST" in os.environ
    if torch.cuda.is_available():
        return torch.device("cuda", 0)
    elif torch.backends.mps.is_available() and not is_pytest:
        return torch.device("mps", 0)
    else:
        return torch.device("cpu")
class StandardModelContext(ModelContext):
    def __init__(self, model_name: str, device: torch.device | None = None) -> None:
        if device is None:
            device = get_default_device()
        super().__init__(model_name=model_name, device=device)
        self._model_spec = get_standard_model_spec(self.model_name)
        self.load_path = self._model_spec.model_path
        self._config = TransformerConfig.load(f"{self.load_path}/config.json")
        # Once a transformer has been created via get_or_create_model, we cache it. Subsequent calls
        # to get_or_create_model return the cached instance.
        self._cached_transformer: Transformer | None = None
    @classmethod
    def from_model_type(
        cls,
        model_type: str,
        inference_engine_type: InferenceEngineType = InferenceEngineType.STANDARD,
        **kwargs: Any,
    ) -> ModelContext:  # specifically a StandardModelContext, but to satisfy mypy
        assert (
            inference_engine_type == InferenceEngineType.STANDARD
        ), "don't set a different inference_engine_type kwarg here"
        model_context = super().from_model_type(
            model_type=model_type, inference_engine_type=InferenceEngineType.STANDARD, **kwargs
        )
        assert isinstance(model_context, StandardModelContext)
        return model_context
    def get_dim_size(self, model_dimension_spec: Dimension) -> int:
        # TODO(sbills): This should really be a match statement.
        dimension_by_dimension_spec: dict[Dimension, int] = {
            Dimension.MAX_CONTEXT_LENGTH: self._config.ctx_window,
            Dimension.RESIDUAL_STREAM_CHANNELS: self._config.d_model,
            Dimension.VOCAB_SIZE: self.get_encoding().n_vocab,
            Dimension.ATTN_HEADS: self._config.n_heads,
            Dimension.QUERY_AND_KEY_CHANNELS: self._config.d_head_qk,
            Dimension.VALUE_CHANNELS: self._config.d_head_v,
            Dimension.MLP_ACTS: self._config.d_ff,
            Dimension.MLP_ACTS: self._config.d_ff,
            Dimension.LAYERS: self._config.n_layers,
        }
        return dimension_by_dimension_spec[model_dimension_spec]
    def _get_weight_helper(
        self,
        location_type: WeightLocationType,
        layer: LayerIndex = None,
        device: torch.device | None = None,
    ) -> torch.Tensor:
        info_by_type: dict[WeightLocationType, dict] = {
            WeightLocationType.MLP_TO_HIDDEN: dict(
                part=f"xf_layers.{layer}.mlp.in_layer.weight",
                reshape="hr->rh",
            ),
            WeightLocationType.MLP_TO_RESIDUAL: dict(
                part=f"xf_layers.{layer}.mlp.out_layer.weight",
                reshape="rh->hr",
            ),
            WeightLocationType.EMBEDDING: dict(
                part="tok_embed.weight",
            ),
            WeightLocationType.UNEMBEDDING: dict(
                part="unembed.weight",
                reshape="vr->rv",
            ),
            WeightLocationType.POSITION_EMBEDDING: dict(
                part="pos_embed.weight",
            ),
            WeightLocationType.ATTN_TO_QUERY: dict(
                part=f"xf_layers.{layer}.attn.q_proj.weight",
                split=(0, self._config.n_heads),
                reshape="hqr->hrq",
            ),
            WeightLocationType.ATTN_TO_KEY: dict(
                part=f"xf_layers.{layer}.attn.k_proj.weight",
                split=(0, self._config.n_heads),
                reshape="hkr->hrk",
            ),
            WeightLocationType.ATTN_TO_VALUE: dict(
                part=f"xf_layers.{layer}.attn.v_proj.weight",
                split=(0, self._config.n_heads),
                reshape="hvr->hrv",
            ),
            WeightLocationType.ATTN_TO_RESIDUAL: dict(
                part=f"xf_layers.{layer}.attn.out_proj.weight",
                split=(1, self._config.n_heads),
                reshape="rhv->hvr",
            ),
            WeightLocationType.LAYER_NORM_GAIN_FINAL: dict(
                part="final_ln.weight",
                broadcast=True,
            ),
            WeightLocationType.LAYER_NORM_BIAS_FINAL: dict(
                part="final_ln.bias",
            ),
            WeightLocationType.LAYER_NORM_GAIN_PRE_ATTN: dict(
                part=f"xf_layers.{layer}.ln_1.weight",
            ),
            WeightLocationType.LAYER_NORM_GAIN_PRE_MLP: dict(
                part=f"xf_layers.{layer}.ln_2.weight",
            ),
        }
        info = info_by_type.get(location_type)
        if info is None:
            raise NotImplementedError(f"Unsupported weight location type: {location_type}")
        part = info["part"]
        split = info.get("split")
        reshape = info.get("reshape")
        if self._cached_transformer is None:
            with CustomFileHandler(f"{self.load_path}/model_pieces/{part}.pt", "rb") as f:
                weight = torch.load(f, map_location=device or self.device)
        else:
            weight = self._cached_transformer.state_dict()[part].to(device or self.device)
        if split is not None:
            (dim_split, n_split) = split
            w_shape = list(weight.shape)
            w_shape_new = (
                w_shape[:dim_split]
                + [n_split, w_shape[dim_split] // n_split]
                + w_shape[dim_split + 1 :]
            )
            weight = weight.reshape(*w_shape_new)
        if reshape is not None:
            weight = torch.einsum(reshape, weight)
        # Some tensors are sometimes stored with a subset of dimensions and then broadcasted in the model
        # E.g. the final layer norm gain is stored as a scalar
        # Broadcast flag indicates that we should broadcast them to the expected shape
        broadcast = info.get("broadcast")
        if broadcast is True:
            expected_shape = self.get_shape_from_spec(weight_shape_by_location_type[location_type])
            weight = weight.expand(expected_shape)
        return weight
    def get_or_create_model(
        self,
        device: torch.device | None = None,
        simplify: bool = False,
    ) -> Transformer:
        if self._cached_transformer is None:
            self._cached_transformer = Transformer.load(
                self.load_path, simplify=simplify, device=device or self.device
            )
        return self._cached_transformer
    def get_encoding(self) -> tiktoken.Encoding:
        return tiktoken.get_encoding(self._config.enc)
    def get_model_config_as_dict(self) -> dict[str, Any]:
        return self._config.to_dict()
class StubModelContext(ModelContext):
    # TODO: maybe make a unified interface for the Config objects of ModelContext objects, and
    # have this be a StubConfig instead of a StubContext
    """This is a fake model context object for use in testing. It just works as a holder for
    a mapping from model dimension to size (int)."""
    def __init__(
        self,
        size_by_model_dimension_spec: dict[Dimension, int],
    ):
        super().__init__(model_name="stub", device=torch.device("cpu"))
        self._size_by_model_dimension_spec = size_by_model_dimension_spec
    def _get_weight_helper(
        self,
        location_type: WeightLocationType,
        layer: LayerIndex = None,
        device: torch.device | None = None,
    ) -> torch.Tensor:
        raise NotImplementedError
    def get_encoding(self) -> tiktoken.Encoding:
        raise NotImplementedError
    def get_or_create_model(self) -> Transformer:
        raise NotImplementedError
    def get_model_config_as_dict(self) -> dict[str, Any]:
        raise NotImplementedError
    def get_dim_size(self, model_dimension_spec: Dimension) -> int:
        if model_dimension_spec in self._size_by_model_dimension_spec:
            return self._size_by_model_dimension_spec[model_dimension_spec]
        else:
            raise NotImplementedError
# TODO: make this robust to whether the transformer is 'simplified' in our terminology
# once the .simplify() operation is extended to cover final layer norm gain
def get_unembedding_with_ln_gain(model_context: ModelContext) -> torch.Tensor:
    """
    returns an unembedding matrix multiplied by the layer norm gain (a d_model-dimensional vector)
    for the final layer
    """
    Unemb_without_ln_gain = model_context.get_weight(
        location_type=WeightLocationType.UNEMBEDDING,
        device=model_context.device,
    )
    ln_gain_final = model_context.get_weight(
        location_type=WeightLocationType.LAYER_NORM_GAIN_FINAL,
        device=model_context.device,
    )
    return torch.einsum("ov,o->ov", Unemb_without_ln_gain, ln_gain_final)
def get_embedding(model_context: ModelContext) -> torch.Tensor:
    """
    returns an embedding matrix. Note that there is no layer norm in between the embedding tensor and
    the residual stream
    """
    return model_context.get_weight(
        location_type=WeightLocationType.EMBEDDING,
        device=model_context.device,
    )

================
File: neuron_explainer/models/model_registry.py
================
from dataclasses import dataclass
import torch
from neuron_explainer.activations.derived_scalars import DerivedScalarType
from neuron_explainer.models import Transformer
from neuron_explainer.models.autoencoder_context import (
    AutoencoderConfig,
    AutoencoderContext,
    AutoencoderSpec,
)
@dataclass(frozen=True)
class StandardModelSpec:
    model_path: str  # checkpoint path
_MODEL_SPECS: dict[str, StandardModelSpec] = {
    # GPT-2 series
    "gpt2-small": StandardModelSpec(
        model_path="https://openaipublic.blob.core.windows.net/neuron-explainer/subject-models/gpt2/small"
    ),
    "gpt2-medium": StandardModelSpec(
        model_path="https://openaipublic.blob.core.windows.net/neuron-explainer/subject-models/gpt2/medium"
    ),
    "gpt2-large": StandardModelSpec(
        model_path="https://openaipublic.blob.core.windows.net/neuron-explainer/subject-models/gpt2/large"
    ),
    "gpt2-xl": StandardModelSpec(
        model_path="https://openaipublic.blob.core.windows.net/neuron-explainer/subject-models/gpt2/xl"
    ),
}
_AUTOENCODER_SPECS: dict[str, dict[str, AutoencoderSpec]] = {
    "gpt2-small": {
        # released December 2023
        "ae-mlp-post-act-v1": AutoencoderSpec(
            dst=DerivedScalarType.MLP_POST_ACT,
            autoencoder_path_by_layer_index={
                layer_index: f"https://openaipublic.blob.core.windows.net/sparse-autoencoder/gpt2-small/mlp_post_act/autoencoders/{layer_index}.pt"
                for layer_index in range(12)
            },
        ),
        "ae-resid-delta-mlp-v1": AutoencoderSpec(
            dst=DerivedScalarType.RESID_DELTA_MLP,
            autoencoder_path_by_layer_index={
                layer_index: f"https://openaipublic.blob.core.windows.net/sparse-autoencoder/gpt2-small/resid_delta_mlp/autoencoders/{layer_index}.pt"
                for layer_index in range(12)
            },
        ),
        # released March 2024
        "ae-mlp-post-act-v4": AutoencoderSpec(
            dst=DerivedScalarType.MLP_POST_ACT,
            autoencoder_path_by_layer_index={
                layer_index: f"https://openaipublic.blob.core.windows.net/sparse-autoencoder/gpt2-small/mlp_post_act_v4/autoencoders/{layer_index}.pt"
                for layer_index in range(12)
            },
        ),
        "ae-resid-delta-mlp-v4": AutoencoderSpec(
            dst=DerivedScalarType.RESID_DELTA_MLP,
            autoencoder_path_by_layer_index={
                layer_index: f"https://openaipublic.blob.core.windows.net/sparse-autoencoder/gpt2-small/resid_delta_mlp_v4/autoencoders/{layer_index}.pt"
                for layer_index in range(12)
            },
        ),
        "ae-resid-delta-attn-v4": AutoencoderSpec(
            dst=DerivedScalarType.RESID_DELTA_ATTN,
            autoencoder_path_by_layer_index={
                layer_index: f"https://openaipublic.blob.core.windows.net/sparse-autoencoder/gpt2-small/resid_delta_attn_v4/autoencoders/{layer_index}.pt"
                for layer_index in range(12)
            },
        ),
    },
}
def list_autoencoder_names(model_name: str = "gpt2-small") -> list[str]:
    return list(_AUTOENCODER_SPECS[model_name].keys())
def get_standard_model_spec(model_name: str) -> StandardModelSpec:
    return _MODEL_SPECS[model_name]
def load_standard_transformer(model_name: str, device: torch.device | None = None) -> Transformer:
    print(f"Loading standard model {model_name}...")
    model_spec = get_standard_model_spec(model_name)
    return load_standard_transformer_from_model_spec(model_spec, device=device)
def load_standard_transformer_from_model_spec(
    model_spec: StandardModelSpec, device: torch.device | None = None
) -> Transformer:
    return Transformer.load(
        model_spec.model_path,
        dtype=torch.float32,
        device=device,
    )
def make_autoencoder_context(
    model_name: str,
    autoencoder_name: str,
    device: torch.device,
    omit_dead_latents: bool = False,
) -> AutoencoderContext:
    try:
        autoencoder_spec = _AUTOENCODER_SPECS[model_name][autoencoder_name]
    except KeyError:
        raise ValueError(
            f"No autoencoder spec found for model {model_name} and autoencoder {autoencoder_name}. "
            f"Available autoencoders for model {model_name} are: {list(_AUTOENCODER_SPECS[model_name].keys())}"
        )
    autoencoder_config = AutoencoderConfig.from_spec(autoencoder_spec)
    autoencoder_context = AutoencoderContext(
        autoencoder_config=autoencoder_config,
        device=device,
        omit_dead_latents=omit_dead_latents,
    )
    return autoencoder_context

================
File: neuron_explainer/models/README.md
================
This directory contains:
- a simple transformer and autoencoder implementation, along with ways to hook their internals.
- the interfaces ModelContext and AutoencoderContext, which we use to wrap the simple implementations.

The rest of our codebase can work to some degree with other transformer implementations, so long as they are wrapped in ModelContext.

## Transformer

Example usage

```python
import torch
from neuron_explainer.models.hooks import TransformerHooks
from neuron_explainer.models.model_context import get_default_device
from neuron_explainer.models.transformer import Transformer

# load a saved model and cast all params to a dtype
device = get_default_device()
xf = Transformer.load("gpt2-small", dtype=torch.float32, device=device)

# forward pass
random_tokens = torch.randint(0, xf.n_vocab, (4, 128)).to(xf.device)
logits, kv_caches = xf(random_tokens)

# sample from the model
out = xf.sample("Hello, transformer!", num_tokens=10, top_p=0.5, temperature=1.0)
# out is a dict {"tokens": list[list[int]], "strings": list[str]}
print(out["strings"][0])

# create a hook to ablate the activations of the 300th neuron in the 3rd layer
def make_ablation_hook(at_layer, neuron):
    def ablate_neuron(xx, layer, **kwargs):
        if layer == at_layer:
            xx[..., neuron] = 0
        return xx

    return ablate_neuron

hooks = TransformerHooks()
hooks.mlp.post_act.append_fwd(make_ablation_hook(3, 300))

# sample from the model with the hooks
out = xf.sample("Hello, transformer!", hooks=hooks, num_tokens=10, top_p=0.5, temperature=1.0)
print(out["strings"][0])
```

The argument to `Transformer.load()` can be any folder.

## Sparse autoencoder

The `neuron_explainer.models.autoencoder` module implements a sparse autoencoder trained on the GPT-2 small model's activations.
The autoencoder's purpose is to expand the MLP layer activations into a larger number of dimensions,
providing an overcomplete basis of the MLP activation space. The learned dimensions have been
shown to be more interpretable than the original MLP dimensions.

The module is a slightly modified version of `https://github.com/openai/sparse_autoencoder`. It is included in this repository for convenience.

### Autoencoder settings

- Model used: "gpt2-small", 12 layers
- Autoencoder architecture: see [`autoencoder.py`](autoencoder.py)
- Autoencoder input: "mlp_post_act" (3072 dimensions), "resid_delta_mlp" (768 dimensions), or "resid_delta_attn" (768 dimensions)
- Number of autoencoder latents: 32768
- Number of training tokens: ~64M
- L1 regularization strength: 0.01 or 0.03 ("_v4")

### Trained autoencoder files

Trained autoencoder files (saved as torch state-dicts) are located at the following paths:
`https://openaipublic.blob.core.windows.net/sparse-autoencoder/gpt2-small/{autoencoder_input}{version}/autoencoders/{layer_index}.pt`

with the following parameters:
- `autoencoder_input` is in ["mlp_post_act", "resid_delta_mlp", "resid_delta_attn"]
- `version` is in ["", "_v4"] ("resid_delta_attn" only available for "_v4")
- `layer_index` is in range(12) (GPT-2 small)

### Collated activation datasets

Note: collated activation datasets located at `https://openaipublic.blob.core.windows.net/sparse-autoencoder/gpt2-small` are not compatible with Transformer Debugger. Use the following datasets instead:
`https://openaipublic.blob.core.windows.net/neuron-explainer/gpt2-small/autoencoder_latent/{autoencoder_input}{version}/collated-activations/{layer_index}/{latent_index}.json`

See [datasets.md](../../datasets.md) for more details.

### Example usage - with Transformer Debugger

- see [Neuron Viewer](../../neuron_viewer/README.md) for instructions on how to start a Neuron Viewer or a Transformer Debugger with autoencoders.
- see [Activation Server](../../neuron_explainer/activation_server/README.md) for instructions on how to start an Activation Server with autoencoders.


### Example usage - with transformer hooks

Autoencoders can be used outside of Transformer Debugger.
Here, we provide a simple example showing how to extract neuron/attention activations, and how to encode them with autoencoders.


```py
import torch

from neuron_explainer.file_utils import CustomFileHandler
from neuron_explainer.models.autoencoder import Autoencoder
from neuron_explainer.models.hooks import TransformerHooks
from neuron_explainer.models.model_context import get_default_device
from neuron_explainer.models.transformer import Transformer

# Load the autoencoder
layer_index = 0  # in range(12)
autoencoder_input = ["mlp_post_act", "resid_delta_mlp", "resid_delta_attn"][1]
version = ["", "_v4"][1]
filename = f"https://openaipublic.blob.core.windows.net/sparse-autoencoder/gpt2-small/{autoencoder_input}{version}/autoencoders/{layer_index}.pt"
with CustomFileHandler(filename, mode="rb") as f:
    print(f"Loading autoencoder..")
    state_dict = torch.load(f)
    autoencoder = Autoencoder.from_state_dict(state_dict, strict=False)

# Load the transformer
device = get_default_device()
print(f"Loading transformer..")
transformer = Transformer.load("gpt2/small", dtype=torch.float32, device=device)

# create hooks to store activations during the forward pass
def store_forward(xx, layer, **kwargs):
    activation_cache[layer] = xx.detach().clone()
    return xx

activation_cache = {}
hooks = TransformerHooks()
if autoencoder_input == "mlp_post_act":
    hooks.mlp.post_act.append_fwd(store_forward)
elif autoencoder_input == "resid_delta_mlp":
    hooks.resid.torso.delta_mlp.append_fwd(store_forward)
elif autoencoder_input == "resid_delta_attn":
    hooks.resid.torso.delta_attn.append_fwd(store_forward)

# Run the transformer and store activations
prompt = "What is essential is invisible to the"
tokens = transformer.enc.encode(prompt)  # (1, n_tokens)
print("tokenized prompt:", transformer.enc.decode_batch([[token] for token in tokens]))
with torch.no_grad():
    transformer(torch.tensor([tokens], device=device), hooks=hooks)
input_tensor = activation_cache[layer_index][0]  # (n_tokens, n_input_dimensions)

# Encode activations with the autoencoder
autoencoder.to(device)
with torch.no_grad():
    latent_activations = autoencoder.encode(input_tensor)  # (n_tokens, n_latents)
```

================
File: neuron_explainer/models/transformer.py
================
import json
import os.path as osp
import pickle
from concurrent.futures import ThreadPoolExecutor
from copy import deepcopy
from dataclasses import asdict, dataclass
from functools import cache
from typing import Any, Self, Union
import numpy as np
import tiktoken
import torch
import torch.nn as nn
from torch import Tensor
from torch.distributions.categorical import Categorical
from torch.utils.checkpoint import checkpoint
from neuron_explainer.file_utils import CustomFileHandler, copy_to_local_cache, file_exists
from neuron_explainer.models.hooks import (
    AttentionHooks,
    MLPHooks,
    NormalizationHooks,
    TransformerHooks,
)
# for static analysis
Device = Union[torch.device, str]
# NOTE: some code from this file related to attention, MLP, and layernorm operations is copy-pasted in
# neuron_explainer/activations/derived_scalars/reconstituted.py; if those operations change here, they should correspondingly
# be changed in that file.
class SerializableDataclass:
    def to_dict(self) -> dict:
        return asdict(self)
    @classmethod
    def from_dict(cls, d) -> Self:
        return cls(**d)
    def save(self, path: str) -> None:
        if path.endswith((".pkl", ".pickle")):
            with CustomFileHandler(path, "wb") as f:
                pickle.dump(self.to_dict(), f)
        elif path.endswith(".json"):
            with CustomFileHandler(path, "w") as f:
                json.dump(self.to_dict(), f)
        else:
            raise ValueError(f"Unknown file extension for {path}")
    @classmethod
    def load(cls, path: str) -> Self:
        if path.endswith((".pkl", ".pickle")):
            with CustomFileHandler(path, "rb") as f:
                return cls.from_dict(pickle.load(f))
        elif path.endswith(".json"):
            with CustomFileHandler(path, "r") as f:
                return cls.from_dict(json.load(f))
        else:
            raise ValueError(f"Unknown file extension for {path}")
@dataclass
class TransformerConfig(SerializableDataclass):
    enc: str = "gpt2"
    ctx_window: int = 1024
    d_model: int = 256
    n_layers: int = 2
    # attn
    m_attn: float = 1
    n_heads: int = 8
    # mlp
    m_mlp: float = 4
    @property
    def d_ff(self) -> int:
        return int(self.d_model * self.m_mlp)
    @property
    def d_attn_qk(self) -> int:
        return int(self.d_model * self.m_attn)
    @property
    def d_attn_v(self) -> int:
        return int(self.d_model * self.m_attn)
    @property
    def d_head_qk(self) -> int:
        return safe_div(self.d_attn_qk, self.n_heads)
    @property
    def d_head_v(self) -> int:
        return safe_div(self.d_attn_v, self.n_heads)
def default_device() -> torch.device:
    return torch.device("cuda" if torch.cuda.is_available() else "cpu")
def safe_div(numerator: int, denominator: int) -> int:
    assert numerator % denominator == 0
    return numerator // denominator
# ====================
# Attention utilities
# ====================
@cache
def causal_attn_mask(size: int, device: Device = "cpu") -> Tensor:
    return torch.tril(torch.ones(size, size)).bool().to(device)
def split_heads(Z: Tensor, n_heads: int) -> Tensor:
    batch, seq, d_attn = Z.shape
    return Z.reshape(batch, seq, n_heads, d_attn // n_heads)
def merge_heads(Z: Tensor) -> Tensor:
    batch, seq, n_heads, d_head = Z.shape
    return Z.reshape(batch, seq, n_heads * d_head)
# ===================================
# MLP utilities
# ===================================
def gelu(x: Tensor) -> Tensor:
    return 0.5 * x * (1 + torch.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * torch.pow(x, 3))))
    # return x * torch.sigmoid(1.702 * x)
# ========================================
# Sampling, padding and related utilities
# ========================================
def prep_input_and_right_pad_for_forward_pass(
    X: list[list[int]], device: Device = "cpu"
) -> tuple[Tensor, Tensor]:
    # Helper function. The two tensors returned by this function are suitable to be passed to
    # Transformer.forward.
    return prep_input_and_pad(X, "right", device)
def prep_input_and_pad(
    X: list[list[int]], pad_side: str, device: Device = "cpu"
) -> tuple[Tensor, Tensor]:
    # X is a list of tokenized prompts; prompts may have unequal lengths. This function will
    # left-pad X by putting "-1" in all the slots where a prompt is shorter than the longest prompt.
    # Then convert X into a tensor of int tokens. Then build the pad tensor by looking for the
    # "-1"s. Then fill the "-1"s in X with "0"s so the embedding layer doesn't get upset.
    max_len = max([len(prompt) for prompt in X])
    def pad(x):
        padding = [-1] * (max_len - len(x))
        if pad_side == "left":
            return padding + x
        elif pad_side == "right":
            return x + padding
        else:
            raise ValueError(f"pad_side must be 'left' or 'right', not {pad_side}")
    X_tensor = torch.LongTensor([pad(prompt) for prompt in X]).to(device)
    pad = X_tensor == -1
    X_tensor = torch.where(X_tensor == -1, 0, X_tensor)
    return X_tensor, pad
def prep_pos_from_pad_and_prev_lens(pad: Tensor, prev_lens: Tensor) -> Tensor:
    # pad has shape b x s, prev_lens has shape b x 1.
    # For position embedding, we need a tensor of shape (b x s) whose
    # entries are the positions of X in the sequence. When sampling with
    # prompts of unequal length, X is left padded with pad tokens. The
    # position tensor needs to take that into account.
    pos = torch.logical_not(pad).long().cumsum(dim=-1) - 1
    pos = torch.where(pos == -1, 0, pos)
    return pos + prev_lens
def nucleus_sample(logits: Tensor, top_p: float) -> Tensor:
    # top_p in [0,1] is the total probability mass of top outputs.
    # based on https://nn.labml.ai/sampling/nucleus.html
    # input shape: [..., n_vocab] -> output shape: [...]
    sorted_logits, idxs = torch.sort(logits, dim=-1, descending=True)
    sorted_probs = torch.softmax(sorted_logits, dim=-1)
    cum_probs = torch.cumsum(sorted_probs, dim=-1)
    # logic to ensure there is always at least one token with nonzero
    # probability when selecting nucleus.
    p0 = cum_probs[..., 0]
    top_p = torch.where(p0 > top_p, p0, top_p)[..., None]
    # sampling
    do_not_sample = cum_probs > top_p
    sorted_logits = sorted_logits.masked_fill(do_not_sample, float("-inf"))
    dist = Categorical(logits=sorted_logits)
    samples = dist.sample()
    tokens = idxs.gather(-1, samples.unsqueeze(-1)).squeeze(-1)
    return tokens
# ===============
# Layer Norm
# ===============
class Norm(nn.Module):
    """LayerNorm reimplementation with hooks."""
    def __init__(
        self,
        size: int,
        eps: float = 1e-5,
        device: Device | None = None,
        dtype: torch.dtype | None = None,
    ):
        super().__init__()
        kwargs = {"device": device, "dtype": dtype}
        self.size = size
        self.weight = nn.Parameter(torch.empty(size, **kwargs))  # type: ignore[arg-type]
        self.bias = nn.Parameter(torch.empty(size, **kwargs))  # type: ignore[arg-type]
        self.eps = eps
    def forward(self, x: Tensor, hooks: NormalizationHooks = None) -> Tensor:
        if hooks is None:
            hooks = NormalizationHooks()
        # always do norm in fp32
        orig_dtype = x.dtype
        x = x.float()
        x = x - x.mean(axis=-1, keepdim=True)  # [batch, pos, length]
        x = hooks.post_mean_subtraction(x)
        scale = torch.sqrt((x**2).mean(dim=-1, keepdim=True) + self.eps)
        scale = hooks.scale(scale)
        x = x / scale
        x = hooks.post_scale(x)
        ret = x * self.weight + self.bias
        return ret.to(orig_dtype)
def apply_layernorm_foldin(ln: Norm, linears: list[nn.Linear]) -> None:
    # folds in a layernorm weight/bias into the next linear layer.
    # ln(x) = W_ln * (x - x.mean())/(x.std()) + b_ln
    # linear(ln(x)) = W_linear * (W_ln * (x - x.mean())/(x.std()) + b_ln) + b_linear
    W_ln = ln.weight.float()
    b_ln = ln.bias.float()
    for linear in linears:
        W_linear = linear.weight.float()
        b_linear = linear.bias.float()
        W_composed = W_linear * W_ln[None, :]
        b_composed = None
        b_composed = b_linear + W_linear @ b_ln
        # should only copy after new weights are calculated
        linear.weight.data.copy_(W_composed)
        linear.bias.data.copy_(b_composed)
    ln.weight.data[:] = 1
    ln.bias.data[:] = 0
# ===========================================
# Attention layers and associated components
# ===========================================
@dataclass
class KeyValueCache:
    """KV cache to save on compute"""
    K_cache: Tensor | None = None  # b x s_old x d
    V_cache: Tensor | None = None  # b x s_old x d
    pad_cache: Tensor | None = None  # b x s_old
    def update(self, K: Tensor, V: Tensor, pad: Tensor):
        # K, V have shape: b x (s_new - s_old) x d
        # pad has shape: b x (s_new - s_old)
        new = self.K_cache is None
        self.K_cache = K if new else torch.cat([self.K_cache, K], dim=1)
        self.V_cache = V if new else torch.cat([self.V_cache, V], dim=1)
        self.pad_cache = pad if new else torch.cat([self.pad_cache, pad], dim=1)
        return self.K_cache, self.V_cache, self.pad_cache
class MultiHeadedDotProductSelfAttention(nn.Module):
    """A configurable multi-headed dot product attention layer."""
    def __init__(
        self,
        cfg: TransformerConfig,
        layer_idx: int,
        device: Device | None = None,
        dtype: torch.dtype | None = None,
    ):
        super().__init__()
        self.n_heads = cfg.n_heads
        # make layers
        kwargs = {"device": device, "dtype": dtype}
        self.q_proj = nn.Linear(cfg.d_model, cfg.d_attn_qk, **kwargs)
        self.k_proj = nn.Linear(cfg.d_model, cfg.d_attn_qk, **kwargs)
        self.v_proj = nn.Linear(cfg.d_model, cfg.d_attn_v, **kwargs)
        self.out_proj = nn.Linear(cfg.d_attn_v, cfg.d_model, **kwargs)
        self.qk_scale = 1 / np.sqrt(np.sqrt(cfg.d_head_qk))
        self.cfg = cfg
    def forward(
        self,
        X: Tensor,
        kv_cache: KeyValueCache | None = None,
        pad: Tensor | None = None,
        hooks: AttentionHooks = AttentionHooks(),
    ) -> tuple[Tensor, KeyValueCache]:
        Q = self.q_proj(X)
        K = self.k_proj(X)
        V = self.v_proj(X)
        # update KV cache
        if kv_cache is None:
            kv_cache = KeyValueCache()
        K, V, pad = kv_cache.update(K, V, pad)
        # split apart heads, rescale QK
        Q = split_heads(Q, self.n_heads) * self.qk_scale
        Q = hooks.q(Q)  # bshd
        K = split_heads(K, self.n_heads) * self.qk_scale
        K = hooks.k(K)  # bshd
        V = split_heads(V, self.n_heads)
        V = hooks.v(V)  # bshd
        # useful for calculations below
        n_queries, n_keys = Q.shape[1], K.shape[1]
        # softmax multi-headed dot product attention
        pre_softmax = torch.einsum("bqhd,bkhd -> bhqk", Q, K)
        # apply causal attention mask
        M = causal_attn_mask(n_keys, device=X.device)
        M = M[None, None, -n_queries:]  # make M broadcastable to batch, head
        pre_softmax = pre_softmax.masked_fill(torch.logical_not(M), float("-inf"))
        # apply pad mask
        if pad is not None and torch.any(pad):
            # we only mask out pad tokens for non-pad query tokens
            # (because masking all pad tokens => empty rows => NaNs later)
            pad_mask = torch.bitwise_xor(pad[:, None, :], pad[:, :, None])
            # make pad broadcastable on head dim, and slice for current queries only
            pad_mask = pad_mask[:, None, -n_queries:]
            # apply pad mask
            pre_softmax = pre_softmax.masked_fill(pad_mask, float("-inf"))
        pre_softmax = torch.einsum("bhqk->bqkh", pre_softmax)
        pre_softmax = hooks.qk_logits(pre_softmax)
        pre_softmax = pre_softmax.float()  # for numerical stability
        if hooks.qk_softmax_denominator.is_empty():
            attn = torch.softmax(pre_softmax, dim=-2)
        else:
            # factor out softmax in order to hook
            pre_softmax_max = torch.max(pre_softmax, -2, keepdim=True)[0].detach()
            numerator = torch.exp(pre_softmax - pre_softmax_max)
            denominator = numerator.sum(dim=-2, keepdim=True)
            denominator = hooks.qk_softmax_denominator(denominator)
            attn = numerator / denominator
        attn = attn.to(Q.dtype)
        attn = hooks.qk_probs(attn)
        out = torch.einsum("bqkh,bkhd->bqhd", attn, V)
        out = hooks.v_out(out)
        out = merge_heads(out)  # concatenate results from all heads
        # final output projection
        return self.out_proj(out), kv_cache
# =====================================
# MLP layers and associated components
# =====================================
class MLP(nn.Module):
    """An MLP for a transformer is a simple two-layer network."""
    def __init__(
        self, cfg: TransformerConfig, device: Device | None = None, dtype: torch.dtype | None = None
    ):
        super().__init__()
        kwargs = {"device": device, "dtype": dtype}
        self.in_layer = nn.Linear(cfg.d_model, cfg.d_ff, **kwargs)
        self.out_layer = nn.Linear(cfg.d_ff, cfg.d_model, **kwargs)
        self.act = gelu
    def forward(self, X: Tensor, hooks: MLPHooks = MLPHooks()) -> Tensor:
        pre = self.in_layer(X)
        pre = hooks.pre_act(pre)
        a = self.act(pre)
        a = hooks.post_act(a, out_layer=self.out_layer)
        out = self.out_layer(a)
        return out
# =============
# Transformers
# =============
class TransformerLayer(nn.Module):
    def __init__(
        self,
        cfg: TransformerConfig,
        layer_idx: int,
        device: Device | None = None,
        dtype: torch.dtype | None = None,
    ):
        super().__init__()
        kwargs = {"device": device, "dtype": dtype}
        self.cfg = cfg
        self.attn = MultiHeadedDotProductSelfAttention(cfg, layer_idx, **kwargs)
        self.mlp = MLP(cfg, **kwargs)
        self.ln_1 = Norm(cfg.d_model, **kwargs)
        self.ln_2 = Norm(cfg.d_model, **kwargs)
        self.layer_idx = layer_idx
    def simplify(self) -> None:
        ln_1_linears: list[Any] = [
            self.attn.q_proj,
            self.attn.k_proj,
            self.attn.v_proj,
        ]
        apply_layernorm_foldin(self.ln_1, ln_1_linears)
        ln_2_linears: list[Any] = [self.mlp.in_layer]
        apply_layernorm_foldin(self.ln_2, ln_2_linears)
    def attn_block(
        self, X: Tensor, kv_cache: KeyValueCache | None, pad: Tensor | None, hooks: TransformerHooks
    ) -> Tensor:
        ln_X = self.ln_1(X, hooks.resid.torso.ln_attn)
        ln_X = hooks.resid.torso.post_ln_attn(ln_X)
        attn_delta, kv_cache = self.attn(ln_X, kv_cache, pad, hooks.attn)
        attn_delta = hooks.resid.torso.delta_attn(attn_delta)
        return attn_delta, kv_cache
    def mlp_block(self, X: Tensor, hooks: TransformerHooks) -> Tensor:
        ln_X = self.ln_2(X, hooks.resid.torso.ln_mlp)
        ln_X = hooks.resid.torso.post_ln_mlp(ln_X)
        mlp_delta = self.mlp(ln_X, hooks.mlp)
        mlp_delta = hooks.resid.torso.delta_mlp(mlp_delta)
        return mlp_delta
    def forward(
        self,
        X: Tensor,
        kv_cache: KeyValueCache | None = None,
        pad: Tensor | None = None,
        hooks: TransformerHooks = TransformerHooks(),
    ) -> tuple[Tensor, KeyValueCache]:
        attn_delta, kv_cache = self.attn_block(X, kv_cache, pad, hooks)
        X = X + attn_delta
        X = hooks.resid.torso.post_attn(X)
        mlp_delta = self.mlp_block(X, hooks)
        X = X + mlp_delta
        X = hooks.resid.torso.post_mlp(X)
        return X, kv_cache
class HiddenState:
    """A hidden state for a transformer. Tracks prompt lengths and KV caches."""
    def __init__(self, n_layers: int):
        self.prev_lens = 0
        self.kv_caches = [None for _ in range(n_layers)]
    def set_prev_lens(self, prev_lens) -> None:
        self.prev_lens = prev_lens
    def __getitem__(self, idx: int):
        return self.kv_caches[idx]
    def __setitem__(self, idx: int, value: KeyValueCache | None):
        self.kv_caches[idx] = value
class Transformer(nn.Module):
    def __init__(
        self,
        cfg: TransformerConfig,
        # recomputing is optional, and it trades off compute for memory.
        recompute: bool = False,
        device: Device | None = None,
        dtype: torch.dtype | None = None,
    ):
        super().__init__()
        self.cfg = cfg
        self.enc = tiktoken.get_encoding(self.cfg.enc)
        self.n_vocab = self.enc.n_vocab
        self.recompute = recompute
        self.dtype = dtype
        # build network
        kwargs = {"device": device, "dtype": dtype}
        self.tok_embed = nn.Embedding(self.n_vocab, cfg.d_model, **kwargs)
        self.pos_embed = nn.Embedding(cfg.ctx_window, cfg.d_model, **kwargs)
        self.xf_layers = nn.ModuleList(
            [TransformerLayer(cfg, idx, **kwargs) for idx in range(cfg.n_layers)]
        )
        self.final_ln = Norm(cfg.d_model, **kwargs)
        self.unembed = nn.Linear(cfg.d_model, self.n_vocab, bias=False, **kwargs)
    def simplify(self):
        for xf_layer in self.xf_layers:
            xf_layer.simplify()
        # NOTE: we can't fold layer norm into unembedding layer
        # because it has no bias
        # apply_layernorm_foldin(self.final_ln, [self.unembed])
    @property
    def device(self) -> Device:
        return next(self.parameters()).device
    def set_recompute(self, recompute: bool) -> None:
        self.recompute = recompute
    def forward(
        self,
        tokens: Tensor,
        H: HiddenState | None = None,
        pad: Tensor | None = None,
        hooks: TransformerHooks = TransformerHooks(),
    ) -> tuple[Tensor, HiddenState]:
        """
        Forward pass through the transformer!
        During evaluation or first forward pass in sampling:
            X is expected to be a [batch_size x sequence_length]-shaped LongTensor of encoded prompts.
            H is expected to be None.
            pad is a [batch_size x sequence_length]-shaped boolean Tensor. "1"s mean "ignore this
            token". This parameter must be set if not all encoded prompts in X have the same length.
            Note that activations observed by hooks will include padded values.
        During sampling after first forward pass:
            X is expected to be the new part of the sequences (eg most recently sampled tokens).
            H is expected to have KV-caches of all Keys and Values for prior tokens.
            pad is expected to be None (new tokens are not pad tokens).
        Returns a tuple containing the resulting logits tensor and a new hidden state consisting of a KV cache.
        """
        X, H, pad, hooks = self.run_embed(tokens, H, pad, hooks)
        X, H, pad, hooks = self.run_torso(X, H, pad, hooks)
        return self.run_unembed(X, H, hooks)
    def run_embed(
        self,
        tokens: Tensor,
        H: HiddenState | None = None,
        pad: Tensor | None = None,
        hooks: TransformerHooks = TransformerHooks(),
    ) -> tuple[Tensor, HiddenState, Tensor | None, TransformerHooks]:
        assert tokens.dtype == torch.long, "tokens must be sequences of tokens."
        if H is None:
            H = HiddenState(self.cfg.n_layers)
        if pad is None:
            pad = torch.zeros_like(tokens, dtype=torch.bool)
        # embedding
        X = self.tok_embed(tokens)
        # position encoding logic to support sampling with prompts of unequal length.
        pos = prep_pos_from_pad_and_prev_lens(pad, H.prev_lens)
        seq_lens = (pos[:, -1] + 1).unsqueeze(-1)
        assert all(
            seq_lens <= self.cfg.ctx_window
        ), f"sequences must fit in the context window {self.cfg.ctx_window}."
        H.set_prev_lens(seq_lens)
        X = X + self.pos_embed(pos)
        X = hooks.resid.post_emb(X)
        return X, H, pad, hooks
    def run_torso(
        self,
        X: Tensor,
        H: HiddenState | None,
        pad: Tensor | None,
        hooks: TransformerHooks,
    ) -> tuple[Tensor, HiddenState, Tensor | None, TransformerHooks]:
        # transformer torso
        for i, xf_layer in enumerate(self.xf_layers):
            hooks_layer_i = deepcopy(hooks).bind(layer=i)
            if self.recompute:
                X, H[i] = checkpoint(xf_layer, X, H[i], pad, hooks_layer_i)
            else:
                X, H[i] = xf_layer(X, H[i], pad, hooks_layer_i)
        return X, H, pad, hooks
    def run_ln_f(
        self,
        X: Tensor,
        H: HiddenState | None,
        hooks: TransformerHooks,
    ) -> tuple[Tensor, HiddenState, TransformerHooks]:
        X = self.final_ln(X, hooks.resid.ln_f)
        X = hooks.resid.post_ln_f(X)
        return X, H, hooks
    def run_unembed(
        self,
        X: Tensor,
        H: HiddenState | None,
        hooks: TransformerHooks,
    ) -> tuple[Tensor, HiddenState]:
        # unembedding
        X, H, hooks = self.run_ln_f(X, H, hooks)
        X = self.unembed(X)
        X = hooks.logits(X)
        return X, H
    def sample(
        self,
        prompts: str | list[str] | list[int] | list[list[int]],
        num_tokens: int = 5,
        temperature: float = 1.0,
        top_p: float | None = None,
        hooks: TransformerHooks = TransformerHooks(),
    ) -> dict[str, Any]:
        """
        Sampling with the transformer!
        If top_p is set, then nucleus sampling is used.
        Otherwise, the sampling will be Categorical.
        If temperature=0, sampling is deterministic (and top_p is ignored).
        (Warning: when using torch.use_deterministic_algorithms(True),
        nucleus sampling will throw an error. It depends on torch.cumsum,
        which unfortunately has no deterministic implementation in torch.)
        Output is a dict {'tokens': list[list[int]], 'strings': list[str]}
        """
        prompts = [prompts] if isinstance(prompts, str) else prompts
        if isinstance(prompts[0], str):
            X: list[list[int]] = [self.enc.encode(prompt) for prompt in prompts]
        elif isinstance(prompts[0], int):
            X = [prompts]
        else:
            X = prompts
        X, pad = prep_input_and_pad(X, "left", self.device)
        H = None
        beta = 1 / max(temperature, 1e-10)
        out = {
            "tokens": [[] for _ in prompts],
            "strings": ["" for _ in prompts],
        }
        # sampling loop
        for _ in range(num_tokens):
            with torch.no_grad():
                # get logits
                Y, H = self.forward(X, H, pad, hooks=hooks)
                logits = Y[:, -1] * beta
                # sampling only works if logits are floats
                logits = logits.float()
                # perform sampling
                if temperature == 0:
                    tokens = torch.argmax(logits, dim=-1)
                elif top_p is not None:
                    tokens = nucleus_sample(logits, top_p)
                else:
                    tokens = Categorical(logits=logits).sample()
                X, pad = tokens.unsqueeze(-1), None
            for batch_idx, token in enumerate(tokens):
                out["tokens"][batch_idx].append(token.item())
                out["strings"][batch_idx] += self.enc.decode([token.item()])
        return out
    @classmethod
    def load(
        cls,
        name_or_path: str,
        device: Device | None = None,
        dtype: torch.dtype | None = None,
        simplify: bool = False,
        simplify_kwargs: dict[str, Any] | None = None,
    ) -> "Transformer":
        if name_or_path.startswith("https://"):
            path = name_or_path
        else:
            path = f"https://openaipublic.blob.core.windows.net/neuron-explainer/subject-models/{name_or_path.replace('-', '/')}"
        xf = cls.from_checkpoint(
            path,
            device=device,
            dtype=dtype,
        )
        if simplify:
            if simplify_kwargs is None:
                simplify_kwargs = {}
            xf.simplify(**simplify_kwargs)
        return xf
    def save_checkpoint(
        self,
        path: str,
    ) -> None:
        self.cfg.save(osp.join(path, "config.json"))
        pieces_path = osp.join(path, "model_pieces")
        for k, v in self.state_dict().items():
            with CustomFileHandler(osp.join(pieces_path, f"{k}.pt"), "wb") as f:
                torch.save(v, f)
    def load_state_from_checkpoint(
        self, path: str, device: Device | None = None, dtype: torch.dtype | None = None
    ):
        pieces_path = osp.join(path, "model_pieces")
        piece_names = set(self.state_dict().keys())
        piece_files = [f"{k}.pt" for k in piece_names]
        if dtype is not None:
            assert isinstance(dtype, torch.dtype), "Must provide valid dtype."
        device = device or self.device
        with ThreadPoolExecutor(max_workers=50) as executor:
            k_to_future = {
                fname[: -len(".pt")]: executor.submit(
                    _load_piece, osp.join(pieces_path, fname), device, dtype
                )
                for fname in piece_files
            }
            d = {k: future.result() for k, future in k_to_future.items()}
        self.load_state_dict(d)
    @classmethod
    def from_checkpoint(
        cls, path: str, device: Device | None = None, dtype: torch.dtype | None = None
    ) -> "Transformer":
        if device is None:
            device = default_device()
        cfg = TransformerConfig.load(osp.join(path, "config.json"))
        xf = cls(cfg, device=device, dtype=dtype)
        xf.load_state_from_checkpoint(path, device=device, dtype=dtype)
        return xf
def _load_piece(
    file_path: str, device: Device, dtype: torch.dtype | None
) -> tuple[str, torch.Tensor]:
    disk_cache_path = osp.join(
        "/tmp/neuron-explainer-model-pieces-cache", file_path.replace("https://", "")
    )
    if not file_exists(disk_cache_path):
        copy_to_local_cache(file_path, disk_cache_path)
    with CustomFileHandler(disk_cache_path, "rb") as f:
        t = torch.load(f, map_location=device)
        if dtype is not None:
            t = t.to(dtype)
    return t

================
File: neuron_explainer/pydantic/__init__.py
================
from .camel_case_base_model import CamelCaseBaseModel
from .hashable_base_model import HashableBaseModel
from .immutable import immutable
__all__ = ["CamelCaseBaseModel", "HashableBaseModel", "immutable"]

================
File: neuron_explainer/pydantic/camel_case_base_model.py
================
from pydantic import BaseModel
def to_camel(string: str) -> str:
    return "".join(word.capitalize() if i > 0 else word for i, word in enumerate(string.split("_")))
class CamelCaseBaseModel(BaseModel):
    """
    Base model that will automatically generate camelCase aliases for fields. Python code can use
    either snake_case or camelCase names. When Typescript code is generated, it will only use the
    camelCase names.
    """
    class Config:
        alias_generator = to_camel
        allow_population_by_field_name = True

================
File: neuron_explainer/pydantic/hashable_base_model.py
================
from .camel_case_base_model import CamelCaseBaseModel
class HashableBaseModel(CamelCaseBaseModel):
    def __hash__(self) -> int:
        values = tuple(getattr(self, name) for name in self.__annotations__.keys())
        # Convert lists to tuples.
        values = tuple(value if not isinstance(value, list) else tuple(value) for value in values)
        return hash(values)

================
File: neuron_explainer/pydantic/immutable.py
================
from typing import TypeVar
from pydantic import BaseConfig, BaseModel
T = TypeVar("T", bound=BaseModel)
def immutable(cls: type[T]) -> type[T]:
    """
    Makes a Pydantic model immutable.
    Annotate a Pydantic class with `@immutable` to prevent the values of its fields from being
    changed after an instance is constructed. (This only guarantees shallow immutability of course:
    fields may have their internal state change.)
    """
    class Config(BaseConfig):
        frozen: bool = True
    cls.Config = Config
    return cls

================
File: neuron_explainer/scripts/create_hf_test_data.py
================
from typing import Any
import click
import torch
from transformers import GPT2Tokenizer
from neuron_explainer.file_utils import copy_to_local_cache
from neuron_explainer.scripts.download_from_hf import get_hf_model
# ==============================
# Reference models for testing
# ==============================
ALL_MODELS = [
    "gpt2/small",
    "gpt2/medium",
    "gpt2/large",
    "gpt2/xl",
]
# test prompts to sample at temperature zero from
test_prompts = [
    "this is a test",
    "I'm sorry Dave, I'm afraid",
    "We're not strangers to love. You know the rules, and",
    "in the beginning",
    "buy now!",
    "Why did the chicken cross the road?",
]
# =======================================================
# Get the hf models and generate test data from those
# =======================================================
def create_hf_test_data(
    models: list[str],
    test_prompts: list[str],
    num_examples: int,
    seq_len: int,
    sample_len: int,
    last_n: int,
) -> dict:
    # for GPT2 models, seq len maxes out at 1024
    seq_len = min(seq_len, 1024)
    tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
    prompts = [tokenizer.encode(p, return_tensors="pt") for p in test_prompts]
    test_data = {}
    for model_name in models:
        print(f"Creating test data for {model_name}")
        model_data: dict[str, Any] = {}
        # prepare model
        model = get_hf_model(model_name)
        model.cuda()
        print(f"...loaded {model_name}...")
        # make test inputs and get logits
        with torch.no_grad():
            X = torch.randint(0, 50257, (num_examples, seq_len)).cuda()
            Y = model(X)
        X = X.cpu()
        logits = Y.logits.cpu()
        logits_at_inputs = logits.gather(-1, X.unsqueeze(-1)).squeeze(-1)
        logits_slice = logits[:, -last_n:].clone()
        model_data["inputs"] = X
        model_data["logits_at_inputs"] = logits_at_inputs
        model_data["logits_slice"] = logits_slice
        model_data["slice_last_n"] = last_n
        # generate temperature-zero samples
        samples = []
        for op, p in zip(test_prompts, prompts):
            p = p.cuda()
            tok1 = model.generate(p, max_length=sample_len + len(p[0]), temperature=0)
            tok2 = model.generate(p, max_length=sample_len + len(p[0]), temperature=0)
            str1 = tokenizer.decode(tok1[0])
            str2 = tokenizer.decode(tok2[0])
            assert (
                str1 == str2
            ), "HuggingFace temperature-zero generate was unexpectedly nondeterministic"
            # get tokens out as a list, then chop off the ones from the prompt
            tok1 = tok1[0].tolist()
            tok1 = tok1[len(p[0]) :]
            samples.append({"prompt": op, "completion": tokenizer.decode(tok1), "tokens": tok1})
        model_data["samples"] = samples
        test_data[model_name] = model_data
        # free up GPU memory
        model.cpu()
        del model
    return test_data
@click.command()
@click.option(
    "-dir",
    "--savedir",
    type=str,
    default="https://openaipublic.blob.core.windows.net/neuron-explainer/data/test-reference-data",
)
@click.option("-n", "--num_examples", type=int, default=4)
@click.option("-m", "--sample_len", type=int, default=50)
@click.option("-s", "--seq_len", type=int, default=1024)
@click.option("-l", "--last_n", type=int, default=100)
def make_and_save_test_data(
    savedir: str, num_examples: int, seq_len: int, sample_len: int, last_n: int
) -> None:
    test_data = create_hf_test_data(
        models=ALL_MODELS,
        test_prompts=test_prompts,
        num_examples=num_examples,
        seq_len=seq_len,
        sample_len=sample_len,
        last_n=last_n,
    )
    torch.save(test_data, "test_data.pt")
    copy_to_local_cache(src="test_data.pt", dst="/".join([savedir, "test_data.pt"]))
if __name__ == "__main__":
    make_and_save_test_data()

================
File: neuron_explainer/scripts/download_from_hf.py
================
import json
import os.path as osp
import click
import torch
from transformers import GPT2LMHeadModel
from neuron_explainer.file_utils import CustomFileHandler
from neuron_explainer.models.transformer import TransformerConfig
EXCLUDES = [".attn.bias", ".attn.masked_bias"]
ALL_MODELS = [
    "gpt2/small",
    "gpt2/medium",
    "gpt2/large",
    "gpt2/xl",
]
def get_hf_model(model_name: str) -> GPT2LMHeadModel:
    _, model_size = model_name.split("/")
    hf_name = "gpt2" if model_size == "small" else f"gpt2-{model_size}"
    model = GPT2LMHeadModel.from_pretrained(hf_name)
    return model
# ====================================
# Conversion from HuggingFace format
# ====================================
def convert(hf_sd: dict) -> dict:
    """convert state_dict from HuggingFace format to our format"""
    n_layers = max([int(k.split(".")[2]) for k in hf_sd.keys() if ".h." in k]) + 1
    hf_to_ours = dict()
    hf_to_ours["wte"] = "tok_embed"
    hf_to_ours["wpe"] = "pos_embed"
    hf_to_ours["ln_f"] = "final_ln"
    hf_to_ours["lm_head"] = "unembed"
    for i in range(n_layers):
        hf_to_ours[f"h.{i}"] = f"xf_layers.{i}"
    hf_to_ours["attn.c_attn"] = "attn.linear_qkv"
    hf_to_ours["attn.c_proj"] = "attn.out_proj"
    hf_to_ours["mlp.c_fc"] = "mlp.in_layer"
    hf_to_ours["mlp.c_proj"] = "mlp.out_layer"
    sd = dict()
    for k, v in hf_sd.items():
        if any(x in k for x in EXCLUDES):
            continue
        if "weight" in k and ("attn" in k or "mlp" in k):
            v = v.T
        k = k.replace("transformer.", "")
        for hf_part, part in hf_to_ours.items():
            k = k.replace(hf_part, part)
        if "attn.linear_qkv." in k:
            qproj, kproj, vproj = v.chunk(3, dim=0)
            sd[k.replace(".linear_qkv.", ".q_proj.")] = qproj
            sd[k.replace(".linear_qkv.", ".k_proj.")] = kproj
            sd[k.replace(".linear_qkv.", ".v_proj.")] = vproj
        else:
            sd[k] = v
    return sd
def download(model_name: str, save_dir: str) -> None:
    assert model_name in ALL_MODELS, f"Must use valid model size, not {model_name=}"
    print(f"Downloading and converting model {model_name} to {save_dir}...")
    print(f"Getting HuggingFace model {model_name}...")
    model = get_hf_model(model_name)
    hf_config = model.config
    base_config = dict(
        enc="gpt2",
        ctx_window=1024,
        # attn
        m_attn=1,
        # mlp
        m_mlp=4,
    )
    cfg = TransformerConfig(
        **base_config,  # type: ignore
        d_model=hf_config.n_embd,
        n_layers=hf_config.n_layer,
        n_heads=hf_config.n_head,
    )
    print("Converting state_dict...")
    sd = convert(model.state_dict())
    print(f"Saving model to {save_dir}...")
    # save to file with config
    pieces_path = osp.join(save_dir, model_name, "model_pieces")
    for k, v in sd.items():
        with CustomFileHandler(osp.join(pieces_path, f"{k}.pt"), "wb") as f:
            torch.save(v, f)
    fname_cfg = osp.join(save_dir, model_name, "config.json")
    with CustomFileHandler(fname_cfg, "w") as f:
        f.write(json.dumps(cfg.__dict__))
@click.command()
@click.argument("save_dir", type=click.Path(exists=False, file_okay=False))
def download_all(save_dir: str) -> None:
    for model_size in ALL_MODELS:
        download(model_size, save_dir)
if __name__ == "__main__":
    download_all()

================
File: neuron_explainer/tests/conftest.py
================
# This file defines fixtures for model tests, with a focus on expensive objects that are used across
# multiple test files. Fixtures are created once per session (i.e. `pytest` invocation), and are
# available to and reused across all test cases in the session. Fixtures are evaluated lazily.
# The filename uses the pytest convention.
import pytest
from neuron_explainer.activations.derived_scalars import DerivedScalarType
from neuron_explainer.activations.derived_scalars.tests.utils import get_autoencoder_test_path
from neuron_explainer.models.autoencoder_context import AutoencoderConfig, AutoencoderContext
from neuron_explainer.models.model_context import StandardModelContext, get_default_device
AUTOENCODER_TEST_DST = DerivedScalarType.MLP_POST_ACT
AUTOENCODER_TEST_PATH = get_autoencoder_test_path(AUTOENCODER_TEST_DST)
@pytest.fixture(scope="session")
def standard_model_context() -> StandardModelContext:
    standard_model_context = StandardModelContext.from_model_type(
        "gpt2-small", device=get_default_device()
    )
    assert isinstance(standard_model_context, StandardModelContext)
    return standard_model_context
@pytest.fixture(scope="session")
def standard_autoencoder_context(
    standard_model_context: StandardModelContext,
) -> AutoencoderContext:
    autoencoder_config = AutoencoderConfig(
        dst=AUTOENCODER_TEST_DST,
        autoencoder_path_by_layer_index={
            layer_index: AUTOENCODER_TEST_PATH
            for layer_index in range(standard_model_context.n_layers)
        },
    )
    return AutoencoderContext(
        autoencoder_config=autoencoder_config,
        device=standard_model_context.device,
    )

================
File: neuron_explainer/tests/test_activation_reconstituter.py
================
from typing import Any
import torch
from neuron_explainer.activations.derived_scalars.indexing import AttentionTraceType, PreOrPostAct
from neuron_explainer.activations.derived_scalars.reconstituter_class import ActivationReconstituter
from neuron_explainer.models.autoencoder_context import AutoencoderContext
from neuron_explainer.models.hooks import AtLayers, TransformerHooks
from neuron_explainer.models.model_component_registry import NodeType, PassType
from neuron_explainer.models.model_context import StandardModelContext
async def test_reconstituter_vs_transformer(
    standard_model_context: StandardModelContext,
    standard_autoencoder_context: AutoencoderContext,
) -> None:
    """
    This test compares the values of activations hooked during the transformer forward
    pass, with the same activations reconstituted from the preceding residual stream
    using an ActivationReconstituter object.
    """
    transformer = standard_model_context.get_or_create_model()
    prompt = "This is a test"
    input_token_ints = torch.tensor(
        standard_model_context.encode(prompt), device=standard_model_context.device
    ).unsqueeze(0)
    test_layer_index = 5
    settings_list: list[dict[str, Any]] = [
        {
            "act_location_type": "mlp.post_act",
            "resid_location_type": "resid.torso.post_attn",
            "node_type": NodeType.MLP_NEURON,
            "pre_or_post_act": PreOrPostAct.POST,
        },
        {
            "act_location_type": "mlp.pre_act",
            "resid_location_type": "resid.torso.post_attn",
            "node_type": NodeType.MLP_NEURON,
            "pre_or_post_act": PreOrPostAct.PRE,
        },
        {
            "act_location_type": "attn.qk_probs",
            "resid_location_type": "resid.torso.post_mlp",
            "node_type": NodeType.ATTENTION_HEAD,
            "pre_or_post_act": PreOrPostAct.POST,
        },
        {
            "act_location_type": "attn.qk_logits",
            "resid_location_type": "resid.torso.post_mlp",
            "node_type": NodeType.ATTENTION_HEAD,
            "pre_or_post_act": PreOrPostAct.PRE,
        },
    ]
    for settings in settings_list:
        act_location_type: str = settings["act_location_type"]
        resid_location_type: str = settings["resid_location_type"]
        node_type: NodeType = settings["node_type"]
        pre_or_post_act: PreOrPostAct = settings["pre_or_post_act"]
        act_layer_index = test_layer_index
        if node_type == NodeType.ATTENTION_HEAD:
            # attention is computed from the post-MLP residual stream in the previous layer
            resid_layer_index = act_layer_index - 1
            attention_trace_type = AttentionTraceType.QK  # irrelevant
        else:
            resid_layer_index = act_layer_index
            attention_trace_type = None  # irrelevant
        reconstituter = ActivationReconstituter(
            transformer=transformer,
            autoencoder_context=standard_autoencoder_context,
            node_type=node_type,
            pre_or_post_act=pre_or_post_act,
            detach_layer_norm_scale=True,  # irrelevant
            attention_trace_type=attention_trace_type,
        )
        hooks = TransformerHooks()
        stored_act = {}
        stored_resid = {}
        def act_saving_hook_fn(act: torch.Tensor, **kwargs: Any) -> torch.Tensor:
            stored_act["value"] = act[0]  # 0 batch index
            return act  # store the activation value for the latent in question
        def resid_saving_hook_fn(act: torch.Tensor, **kwargs: Any) -> torch.Tensor:
            stored_resid["value"] = act[0]  # 0 batch index
            return act  # store the activation value for the latent in question
        hooks = hooks.append_to_path(
            act_location_type + ".fwd",
            AtLayers([act_layer_index]).append(act_saving_hook_fn),
        ).append_to_path(
            resid_location_type + ".fwd",
            AtLayers([resid_layer_index]).append(resid_saving_hook_fn),
        )
        transformer(input_token_ints, hooks=hooks)
        original = stored_act["value"]
        reconstituted = reconstituter.reconstitute_activations(
            resid=stored_resid["value"],
            other_arg=None,
            layer_index=test_layer_index,
            pass_type=PassType.FORWARD,
        )
        assert original.shape == reconstituted.shape
        torch.testing.assert_close(
            original,
            reconstituted,
            msg=f"Failed for {settings['act_location_type']}",
        )

================
File: neuron_explainer/tests/test_against_data.py
================
import warnings
from dataclasses import dataclass
import pytest
import torch
from neuron_explainer.file_utils import CustomFileHandler, copy_to_local_cache, file_exists
from neuron_explainer.models import Transformer
SRC_TEST_DATA_FNAME = "https://openaipublic.blob.core.windows.net/neuron-explainer/test-data/test-reference-data/test_data.pt"
DST_TEST_DATA_FNAME = "/tmp/neuron_explainer_reference_test_data.pt"
REFERENCE_MODELS = [
    "gpt2/small",
    "gpt2/medium",
    "gpt2/large",
    "gpt2/xl",
]
@dataclass
class Tolerances:
    max_logits_diff: float
    mean_logits_diff: float
    max_kl: float
    mean_kl: float
    sampling_tolerance: int
DEFAULT_TOLERANCES = Tolerances(
    max_logits_diff=1e-3, mean_logits_diff=5e-5, max_kl=1e-8, mean_kl=1e-9, sampling_tolerance=1
)
def KL(logits1: torch.Tensor, logits2: torch.Tensor) -> torch.Tensor:
    p = torch.softmax(logits1.double(), dim=-1)
    lp1 = torch.log_softmax(logits1.double(), dim=-1)
    lp2 = torch.log_softmax(logits2.double(), dim=-1)
    kl = (p * (lp1 - lp2)).sum(dim=-1)
    return kl
@pytest.mark.parametrize("model_name", REFERENCE_MODELS)
def test_pretrained_models_against_reference_data(model_name: str) -> None:
    """
    Verify that our transformer is correctly loading pretrained models
    by checking their outputs on random data against reference data from huggingface models.
    """
    if not file_exists(DST_TEST_DATA_FNAME):
        copy_to_local_cache(SRC_TEST_DATA_FNAME, DST_TEST_DATA_FNAME)
    else:
        print(f"Test data already exists, reusing.  Run `rm {DST_TEST_DATA_FNAME}` to redownload.")
    with CustomFileHandler(DST_TEST_DATA_FNAME, "rb") as f:
        test_data = torch.load(f)
    data = test_data[model_name]
    print(f"testing model {model_name}")
    xf = Transformer.load(model_name, dtype=torch.float32)
    xf.train(mode=False)
    X = data["inputs"]
    print(f"running forward pass for {model_name}")
    with torch.no_grad():
        Y, _ = xf(X.to(xf.device))
        Y = Y.cpu()
    last_n = data["slice_last_n"]
    logits_slice = Y[:, -last_n:, :]
    target_logits_slice = data["logits_slice"][:, :, :]
    # =========================
    # Get reference tolerance
    # =========================
    tolerance = DEFAULT_TOLERANCES
    # =========================================
    # Look for absolute differences in logits
    # =========================================
    diff = logits_slice - target_logits_slice
    abs_diff = diff.abs()
    print(f"{model_name} | logits diff max: {abs_diff.max()}, diff mean: {abs_diff.mean()}")
    assert (
        abs_diff.max() < tolerance.max_logits_diff
    ), f"max logits diff exceeded {tolerance.max_logits_diff} for {model_name}"
    assert (
        abs_diff.mean() < tolerance.mean_logits_diff
    ), f"mean logits diff exceeded {tolerance.mean_logits_diff} for {model_name}"
    # =========================================
    # Ensure KL is small
    # =========================================
    kl = KL(target_logits_slice, logits_slice)
    print(f"{model_name} | kl max: {kl.max()}, kl mean: {kl.mean()}")
    assert kl.max() < tolerance.max_kl, f"max kl exceeded {tolerance.max_kl} for {model_name}"
    assert kl.mean() < tolerance.mean_kl, f"mean kl exceeded {tolerance.mean_kl} for {model_name}"
    # =========================================
    # Check temperature=0 sampling
    # =========================================
    prompts = [s["prompt"] for s in data["samples"]]
    print(f"sampling for {model_name}")
    results = []
    for idx, p in enumerate(prompts):
        target_tok = data["samples"][idx]["tokens"]
        o = xf.sample(p, temperature=0, num_tokens=len(target_tok))
        results.append(o["tokens"][0] == target_tok)
    assert (
        sum(results) >= len(prompts) - tolerance.sampling_tolerance
    ), f"temperature=0 sampling did not match for {model_name}"
    if sum(results) < len(prompts):
        warnings.warn(f"Not all samples were exact matches for {model_name}")

================
File: neuron_explainer/tests/test_all_dsts.py
================
import pytest
import torch
from neuron_explainer.activation_server.derived_scalar_computation import (
    get_derived_scalars_for_prompt,
)
from neuron_explainer.activations.derived_scalars import DerivedScalarType
from neuron_explainer.activations.derived_scalars.derived_scalar_store import DerivedScalarStore
from neuron_explainer.activations.derived_scalars.indexing import (
    ActivationIndex,
    AttnSubNodeIndex,
    NodeIndex,
    TraceConfig,
)
from neuron_explainer.activations.derived_scalars.scalar_deriver import DstConfig
from neuron_explainer.activations.derived_scalars.tests.utils import get_activation_shape
from neuron_explainer.models.autoencoder_context import AutoencoderContext
from neuron_explainer.models.model_component_registry import (
    ActivationLocationType,
    Dimension,
    NodeType,
    PassType,
)
from neuron_explainer.models.model_context import StandardModelContext
REFERENCE_DS_STORE_PATH_BY_GRAD_LOCATION = {
    "mlp": "https://openaipublic.blob.core.windows.net/neuron-explainer/test-data/reference_ds_stores/test_all_dsts_reference_ds_store.pt",
    "attn": "https://openaipublic.blob.core.windows.net/neuron-explainer/test-data/reference_ds_stores/attn/test_all_dsts_reference_ds_store.pt",
}
DETACH_LAYER_NORM_SCALE_FOR_TEST = (
    False  # this sets whether to detach layer norm scale when computing these DSTs.
)
# Likely the desired value is True going forward, but saved activations implicitly used False here (TODO).
@pytest.mark.parametrize("grad_location", ["mlp", "attn"])
def test_dsts_consistency(
    standard_model_context: StandardModelContext,
    standard_autoencoder_context: AutoencoderContext,
    grad_location: str,
) -> None:
    reference_ds_store_path = REFERENCE_DS_STORE_PATH_BY_GRAD_LOCATION[grad_location]
    prompt = "This is a test"
    n_tokens = len(standard_model_context.encode(prompt))
    grad_layer_index = 5
    token_index = 1
    attended_to_token_index = 1
    if grad_location == "mlp":
        activation_index_for_grad = ActivationIndex(
            activation_location_type=ActivationLocationType.MLP_POST_ACT,
            layer_index=grad_layer_index,
            tensor_indices=(token_index, 0),
            pass_type=PassType.FORWARD,
        )
    else:
        assert grad_location == "attn"
        activation_index_for_grad = ActivationIndex(
            activation_location_type=ActivationLocationType.ATTN_QK_PROBS,
            layer_index=grad_layer_index,
            tensor_indices=(token_index, attended_to_token_index, 0),
            pass_type=PassType.FORWARD,
        )
    # these require separate configs
    out_edge_attribution_dsts = [
        DerivedScalarType.GRAD_OF_SINGLE_SUBNODE_ATTRIBUTION,
        DerivedScalarType.ATTN_OUT_EDGE_ATTRIBUTION,
        DerivedScalarType.MLP_OUT_EDGE_ATTRIBUTION,
        DerivedScalarType.ONLINE_AUTOENCODER_OUT_EDGE_ATTRIBUTION,
        DerivedScalarType.TOKEN_OUT_EDGE_ATTRIBUTION,
    ]
    # these require separate configs
    in_edge_attribution_dsts = [
        DerivedScalarType.SINGLE_NODE_WRITE,
        DerivedScalarType.ATTN_QUERY_IN_EDGE_ATTRIBUTION,
        DerivedScalarType.ATTN_KEY_IN_EDGE_ATTRIBUTION,
        DerivedScalarType.ATTN_VALUE_IN_EDGE_ATTRIBUTION,
        DerivedScalarType.MLP_IN_EDGE_ATTRIBUTION,
        DerivedScalarType.ONLINE_AUTOENCODER_IN_EDGE_ATTRIBUTION,
        DerivedScalarType.SINGLE_NODE_WRITE_TO_FINAL_RESIDUAL_GRAD,
        DerivedScalarType.ATTN_QUERY_IN_EDGE_ACTIVATION,
        DerivedScalarType.ATTN_KEY_IN_EDGE_ACTIVATION,
        DerivedScalarType.MLP_IN_EDGE_ACTIVATION,
        DerivedScalarType.ONLINE_AUTOENCODER_IN_EDGE_ACTIVATION,
    ]
    dst_list = list(DerivedScalarType.__members__.values())
    disallow_list = (
        [
            # not present based on autoencoder settings (mlp_post_act autoencoder only)
            DerivedScalarType.ONLINE_RESIDUAL_MLP_AUTOENCODER_ERROR,
            DerivedScalarType.ONLINE_RESIDUAL_ATTENTION_AUTOENCODER_ERROR,
            DerivedScalarType.ATTENTION_AUTOENCODER_LATENT,
            DerivedScalarType.ATTENTION_AUTOENCODER_WRITE_NORM,
            DerivedScalarType.ONLINE_ATTENTION_AUTOENCODER_LATENT,
            DerivedScalarType.ONLINE_ATTENTION_AUTOENCODER_WRITE,
            DerivedScalarType.ONLINE_ATTENTION_AUTOENCODER_WRITE_NORM,
            DerivedScalarType.ONLINE_ATTENTION_AUTOENCODER_ACT_TIMES_GRAD,
            DerivedScalarType.ONLINE_ATTENTION_AUTOENCODER_WRITE_TO_FINAL_RESIDUAL_GRAD,
            DerivedScalarType.ONLINE_ATTENTION_AUTOENCODER_WRITE_TO_FINAL_ACTIVATION_RESIDUAL_GRAD,
            # requires a different autoencoder or a different activation index
            DerivedScalarType.AUTOENCODER_LATENT_GRAD_WRT_MLP_POST_ACT_INPUT,
            DerivedScalarType.AUTOENCODER_LATENT_GRAD_WRT_RESIDUAL_INPUT,
            DerivedScalarType.ATTN_WRITE_TO_LATENT,
            DerivedScalarType.ATTN_WRITE_TO_LATENT_SUMMED_OVER_HEADS,
            DerivedScalarType.FLATTENED_ATTN_WRITE_TO_LATENT_SUMMED_OVER_HEADS,
            DerivedScalarType.FLATTENED_ATTN_WRITE_TO_LATENT_SUMMED_OVER_HEADS_BATCHED,
            DerivedScalarType.ATTN_WRITE_TO_LATENT_PER_SEQUENCE_TOKEN,
            DerivedScalarType.ATTN_WRITE_TO_LATENT_PER_SEQUENCE_TOKEN_BATCHED,
            DerivedScalarType.VOCAB_TOKEN_WRITE_TO_INPUT_DIRECTION,
            DerivedScalarType.ALWAYS_ONE,
        ]
        + out_edge_attribution_dsts
        + in_edge_attribution_dsts
    )
    starting_dst_list = [dst for dst in dst_list if dst not in disallow_list]
    standard_autoencoder_context.warmup()
    dst_config = DstConfig(
        model_context=standard_model_context,
        autoencoder_context=standard_autoencoder_context,
        trace_config=TraceConfig.from_activation_index(
            activation_index_for_grad, detach_layer_norm_scale=DETACH_LAYER_NORM_SCALE_FOR_TEST
        ),
    )
    dst_config_by_dst = {dst: dst_config for dst in starting_dst_list}
    attribution_layer_index = 3
    assert attribution_layer_index < grad_layer_index - 1
    # edge attribution DSTs involve 3 locations in the network (from most downstream to most upstream):
    # 1. the location from which the backward pass is run
    # 2. the location of the downstream node of the edge whose attribution is being computed
    # 3. the location of the upstream node of the edge whose attribution is being computed
    # in the case where attribution_layer_index refers to the upstream node (3.), we need it to be at least 2 layers
    # earlier than the grad_layer_index (1.), so that there is at least one layer between the attribution layer and
    # the grad layer, for nodes in category (2.) to exist
    in_edge_attribution_dst_config = DstConfig(
        model_context=standard_model_context,
        autoencoder_context=standard_autoencoder_context,
        trace_config=TraceConfig.from_activation_index(
            activation_index_for_grad, detach_layer_norm_scale=DETACH_LAYER_NORM_SCALE_FOR_TEST
        ),
        node_index_for_attribution=NodeIndex(
            # must be an earlier layer than activation_index_for_grad
            node_type=NodeType.ATTENTION_HEAD,
            layer_index=attribution_layer_index,
            tensor_indices=(token_index, attended_to_token_index, 0),
            pass_type=PassType.FORWARD,  # note: does not test autoencoder, MLP nodes
        ),
        detach_layer_norm_scale_for_attribution=DETACH_LAYER_NORM_SCALE_FOR_TEST,
    )
    dst_config_by_dst.update(
        {dst: in_edge_attribution_dst_config for dst in in_edge_attribution_dsts}
    )
    out_edge_attribution_dst_config = DstConfig(
        model_context=standard_model_context,
        autoencoder_context=standard_autoencoder_context,
        trace_config=TraceConfig.from_activation_index(
            activation_index_for_grad, detach_layer_norm_scale=DETACH_LAYER_NORM_SCALE_FOR_TEST
        ),
        node_index_for_attribution=AttnSubNodeIndex(
            # must be an earlier layer than activation_index_for_grad
            node_type=NodeType.ATTENTION_HEAD,
            layer_index=attribution_layer_index,
            tensor_indices=(token_index, attended_to_token_index, 0),
            q_k_or_v=ActivationLocationType.ATTN_VALUE,
            pass_type=PassType.FORWARD,  # note: does not test autoencoder, MLP, Q/K subnodes
        ),
        detach_layer_norm_scale_for_attribution=DETACH_LAYER_NORM_SCALE_FOR_TEST,
    )
    dst_config_by_dst.update(
        {dst: out_edge_attribution_dst_config for dst in out_edge_attribution_dsts}
    )
    dst_list = list(dst_config_by_dst.keys())
    dst_and_config_list: list[tuple[DerivedScalarType, DstConfig | None]] = list(
        dst_config_by_dst.items()
    )
    current_ds_store, _, raw_store = get_derived_scalars_for_prompt(
        model_context=standard_model_context,
        prompt=prompt,
        trace_config=TraceConfig.from_activation_index(
            activation_index_for_grad, detach_layer_norm_scale=DETACH_LAYER_NORM_SCALE_FOR_TEST
        ),
        dst_and_config_list=dst_and_config_list,
        autoencoder_context=standard_autoencoder_context,
    )
    # check that the derived scalar is not all zeros, and has the correct shape
    for (
        dst,
        _pass_type,
    ), derived_scalar in current_ds_store.activations_and_metadata_by_dst_and_pass_type.items():
        if Dimension.AUTOENCODER_LATENTS in dst.shape_spec_per_token_sequence:
            n_latents = standard_autoencoder_context.num_autoencoder_directions
        else:
            n_latents = None
        assert derived_scalar.shape == get_activation_shape(
            dst, standard_model_context, n_tokens, n_latents
        ), f"{dst}: {derived_scalar.shape} != {get_activation_shape(dst, standard_model_context, n_tokens, n_latents)}"
        if dst in in_edge_attribution_dsts and dst not in {
            DerivedScalarType.SINGLE_NODE_WRITE,
            DerivedScalarType.SINGLE_NODE_WRITE_TO_FINAL_RESIDUAL_GRAD,
        }:
            last_tensor = derived_scalar.activations_by_layer_index[attribution_layer_index + 1]
            assert not torch.all(
                last_tensor[..., 0] == 0.0
            ), dst  # the first element is legitimately zero for these DSTs at or before attribution_layer_index
        else:
            first_tensor = next(iter(derived_scalar.activations_by_layer_index.values()))
            assert not torch.all(
                first_tensor[..., 0] == 0.0
            ), dst  # check only first element to limit memory
    # Next step: compare the DerivedScalarStore against a reference store in blob storage.
    # The goal is to notice if the derived scalar values change unexpectedly, which may
    # indicate a bug. Note that this test will also fail for PRs that fix existing bugs that
    # affect the derived scalar values!
    def truncate_acts(acts: torch.Tensor) -> torch.Tensor:
        for dim in range(acts.ndim):
            acts = acts.narrow(dim=dim, start=0, length=min(50, acts.size(dim)))
        return acts
    # Truncate all the activations in the store, in preparation for comparing them against the
    # reference data, which is also truncated. We truncate to avoid needing to save and load
    # excessively large tensors.
    current_ds_store = current_ds_store.apply_transform_fn_to_activations(truncate_acts)
    # If you need to update the reference data, do the following in exactly this order:
    #   1) Increment the version number in the reference data path to avoid changing the data
    #      used by the version of this test running on CI.
    #   2) Uncomment the "save_to_file" line below.
    #   3) Run the test.
    #   4) Comment the "save_to_file" line out again.
    #
    # Think carefully before doing this! Make sure you're confident that the new data is
    # correct. Note that changes to the autoencoder stored at AUTOENCODER_TEST_PATH will also
    # affect the derived scalar values.
    # current_ds_store.save_to_file(reference_ds_store_path)
    #
    # In case of a deleted DST, you can temporarily set skip_missing_dsts=True below to run the tests, confirming
    # that all other DSTs are still correct. Then, increment the version number in the reference data path as
    # described above, save the new data, and set skip_missing_dsts=False again.
    reference_ds_store = DerivedScalarStore.load_from_file(
        reference_ds_store_path, map_location=standard_model_context.device, skip_missing_dsts=False
    )
    current_dsts_and_pass_types = set(
        current_ds_store.activations_and_metadata_by_dst_and_pass_type.keys()
    )
    reference_dsts_and_pass_types = set(
        reference_ds_store.activations_and_metadata_by_dst_and_pass_type.keys()
    )
    something_has_failed = False
    failed_dsts_and_pass_types = []
    for dst_and_pass_type in current_dsts_and_pass_types.intersection(
        reference_dsts_and_pass_types
    ):
        print(f"Comparing {dst_and_pass_type}")
        current_activations_and_metadata = (
            current_ds_store.activations_and_metadata_by_dst_and_pass_type[dst_and_pass_type]
        )
        reference_activations_and_metadata = (
            reference_ds_store.activations_and_metadata_by_dst_and_pass_type[dst_and_pass_type]
        )
        # TODO(sbills): Refactor this to share code with the __eq__ method in
        # ActivationsAndMetadata.
        try:
            layer_index: int | None = None
            assert (
                current_activations_and_metadata.activations_by_layer_index.keys()
                == reference_activations_and_metadata.activations_by_layer_index.keys()
            ), (
                "current:",
                current_activations_and_metadata.activations_by_layer_index.keys(),
                "reference:",
                reference_activations_and_metadata.activations_by_layer_index.keys(),
            )
            for layer_index in current_activations_and_metadata.activations_by_layer_index.keys():
                # show the first value for every sequence token
                activations_ndim = current_activations_and_metadata.activations_by_layer_index[
                    layer_index
                ].ndim
                slices_to_show = tuple([slice(None)] + [0] * (activations_ndim - 1))
                assert torch.allclose(
                    current_activations_and_metadata.activations_by_layer_index[layer_index],
                    reference_activations_and_metadata.activations_by_layer_index[layer_index],
                    rtol=5e-4,
                    atol=5e-4,
                ), (
                    "current:",
                    current_activations_and_metadata.activations_by_layer_index[layer_index][
                        slices_to_show
                    ],
                    "reference:",
                    reference_activations_and_metadata.activations_by_layer_index[layer_index][
                        slices_to_show
                    ],
                )
        except AssertionError as e:
            print(f"Discrepancy in {dst_and_pass_type}, layer {layer_index}: {e}")
            something_has_failed = True
            failed_dsts_and_pass_types.append(dst_and_pass_type)
    assert not something_has_failed, (
        "Some derived scalar values have changed unexpectedly: "
        f"{failed_dsts_and_pass_types}\n"
        "See the comments in this test case for instructions on how to update the reference data."
    )
    # Calculate which dsts are missing from current_ds_store and which dsts are added to
    # current_ds_store, relative to reference_ds_store.
    missing_dsts = reference_ds_store.dsts - current_ds_store.dsts
    added_dsts = current_ds_store.dsts - reference_ds_store.dsts
    assert len(missing_dsts) == 0 and len(added_dsts) == 0, (
        f"The following DSTs have been removed relative to the reference data: {missing_dsts}\n"
        f"The following DSTs have been added relative to the reference data:   {added_dsts}\n"
        f"If these changes are intentional, update the reference data by following the process "
        f"documented in a comment in this test case."
    )

================
File: neuron_explainer/tests/test_emb_dsts.py
================
"""
This file contains a test checking that DerivedScalarStore values associated with the embedding are
nonzero in DerivedScalarStores using backward passes from early activations.
"""
import pytest
import torch
from _pytest.fixtures import FixtureRequest
from neuron_explainer.activation_server.derived_scalar_computation import (
    get_derived_scalars_for_prompt,
    maybe_construct_loss_fn_for_backward_pass,
)
from neuron_explainer.activation_server.requests_and_responses import (
    AblationSpec,
    GroupId,
    LossFnConfig,
    LossFnName,
)
from neuron_explainer.activations.derived_scalars import DerivedScalarType
from neuron_explainer.activations.derived_scalars.indexing import (
    ActivationIndex,
    MirroredActivationIndex,
    NodeIndex,
    TraceConfig,
)
from neuron_explainer.activations.derived_scalars.postprocessing import TokenReadConverter
from neuron_explainer.activations.derived_scalars.scalar_deriver import DstConfig
from neuron_explainer.models.autoencoder_context import AutoencoderContext
from neuron_explainer.models.model_component_registry import (
    ActivationLocationType,
    NodeType,
    PassType,
)
from neuron_explainer.models.model_context import ModelContext, StandardModelContext
@pytest.fixture(params=["mlp", "attn"])
def grad_location(request: FixtureRequest) -> str:
    return request.param
def test_emb_dsts(
    standard_model_context: StandardModelContext,
    grad_location: str,
) -> None:
    dst_list_by_group_id = {
        GroupId.ACT_TIMES_GRAD: [
            DerivedScalarType.UNFLATTENED_ATTN_ACT_TIMES_GRAD,
            DerivedScalarType.TOKEN_ATTRIBUTION,
            DerivedScalarType.MLP_ACT_TIMES_GRAD,
        ],
        GroupId.DIRECT_WRITE_TO_GRAD: [
            DerivedScalarType.UNFLATTENED_ATTN_WRITE_TO_FINAL_RESIDUAL_GRAD,
            DerivedScalarType.RESID_POST_EMBEDDING_PROJ_TO_FINAL_RESIDUAL_GRAD,
            DerivedScalarType.MLP_WRITE_TO_FINAL_RESIDUAL_GRAD,
        ],
    }
    for group_id in [GroupId.ACT_TIMES_GRAD, GroupId.DIRECT_WRITE_TO_GRAD]:
        prompt = "This is a test"
        n_tokens = len(standard_model_context.encode(prompt))
        autoencoder_context = None
        if grad_location == "mlp":
            activation_index_for_grad = ActivationIndex(
                activation_location_type=ActivationLocationType.MLP_POST_ACT,
                layer_index=0,
                tensor_indices=(1, 0),
                pass_type=PassType.FORWARD,
            )
        else:
            assert grad_location == "attn"
            activation_index_for_grad = ActivationIndex(
                activation_location_type=ActivationLocationType.ATTN_QK_PROBS,
                layer_index=0,
                tensor_indices=(3, 2, 0),
                pass_type=PassType.FORWARD,
            )
        dst_list = dst_list_by_group_id[group_id]
        dst_config = DstConfig(
            model_context=standard_model_context,
            autoencoder_context=autoencoder_context,
            trace_config=TraceConfig.from_activation_index(activation_index_for_grad),
        )
        dst_and_config_list: list[tuple[DerivedScalarType, DstConfig | None]] = [
            (dst, dst_config) for dst in dst_list
        ]
        current_ds_store, _, raw_store = get_derived_scalars_for_prompt(
            model_context=standard_model_context,
            autoencoder_context=autoencoder_context,
            prompt=prompt,
            trace_config=TraceConfig.from_activation_index(activation_index_for_grad),
            dst_and_config_list=dst_and_config_list,
        )
        vals, indices = current_ds_store.topk(50)
        emb_indices = [index for index in indices if index.dst.node_type == NodeType.LAYER]
        assert len(emb_indices) > 0
        zipped_vals_and_indices = list(zip(vals, indices))
        match grad_location:
            case "mlp":
                # at least one non-emb value should be nonzero
                assert any(
                    val != 0
                    for val, index in zipped_vals_and_indices
                    if index.dst.node_type != NodeType.LAYER
                )
            case "attn":
                # all non-emb values should be zero
                assert all(
                    val == 0
                    for val, index in zipped_vals_and_indices
                    if index.dst.node_type != NodeType.LAYER
                )
            case _:
                raise ValueError(f"Invalid grad_location: {grad_location}")
def _compute_top_token_ints_using_reconstituted_grad(
    model_context: ModelContext,
    autoencoder_context: AutoencoderContext | None,
    prompt: str,
    activation_location_type: ActivationLocationType,
    layer_index: int,
    tensor_indices: tuple[int, ...],
    top_tokens_to_check: int,
) -> list[int]:
    assert isinstance(model_context, StandardModelContext)
    activation_index_for_grad = ActivationIndex(
        activation_location_type=activation_location_type,
        layer_index=layer_index,
        tensor_indices=tensor_indices,
        pass_type=PassType.FORWARD,
    )
    dst_config = DstConfig(
        model_context=model_context,
        autoencoder_context=autoencoder_context,
        trace_config=TraceConfig.from_activation_index(activation_index_for_grad),
    )
    token_read_converter = TokenReadConverter(
        model_context=model_context, multi_autoencoder_context=autoencoder_context
    )
    dst_and_config_list: list[tuple[DerivedScalarType, DstConfig]] = [
        (
            DerivedScalarType.from_activation_location_type(
                activation_index_for_grad.activation_location_type
            ),
            dst_config,
        )
    ]
    post_dst_and_config_list = token_read_converter.get_input_dst_and_config_list(
        dst_and_config_list
    )
    ds_store, _, _ = get_derived_scalars_for_prompt(
        model_context=model_context,
        autoencoder_context=autoencoder_context,
        prompt=prompt,
        dst_and_config_list=dst_and_config_list + post_dst_and_config_list,  # type: ignore
    )
    node_index = NodeIndex.from_activation_index(activation_index_for_grad)
    token_vector = token_read_converter.postprocess(
        ds_store=ds_store,
        node_index=node_index,
    )
    topk = torch.topk(token_vector, top_tokens_to_check)
    top_token_ints_using_postprocess = topk.indices.tolist()
    return top_token_ints_using_postprocess
def _compute_top_and_bottom_token_ints_using_input_direction(
    model_context: ModelContext,
    autoencoder_context: AutoencoderContext | None,
    prompt: str,
    activation_location_type: ActivationLocationType,
    layer_index: int,
    tensor_indices: tuple[int, ...],
    top_tokens_to_check: int,
) -> tuple[list[int], list[int]]:
    assert isinstance(model_context, StandardModelContext)
    activation_index_for_fake_grad = ActivationIndex(
        activation_location_type=activation_location_type,
        layer_index=layer_index,
        tensor_indices=tensor_indices,
        pass_type=PassType.BACKWARD,
    )
    value_for_fake_grad = 1.0
    ablation_specs = [
        AblationSpec(
            index=MirroredActivationIndex.from_activation_index(activation_index_for_fake_grad),
            value=value_for_fake_grad,
        )
    ]
    dst_list = [DerivedScalarType.VOCAB_TOKEN_WRITE_TO_INPUT_DIRECTION]
    dst_config = DstConfig(
        model_context=model_context,
        autoencoder_context=autoencoder_context,
        activation_index_for_fake_grad=activation_index_for_fake_grad,
    )
    dst_and_config_list: list[tuple[DerivedScalarType, DstConfig | None]] = [
        (dst, dst_config) for dst in dst_list
    ]
    loss_fn_for_backward_pass = maybe_construct_loss_fn_for_backward_pass(
        model_context=model_context,
        config=LossFnConfig(
            name=LossFnName.ZERO,
        ),
    )
    current_ds_store, _, raw_store = get_derived_scalars_for_prompt(
        model_context=model_context,
        autoencoder_context=autoencoder_context,
        prompt=prompt,
        loss_fn_for_backward_pass=loss_fn_for_backward_pass,
        dst_and_config_list=dst_and_config_list,
        ablation_specs=ablation_specs,
    )
    top_tokens_to_check = 10
    # select just the first token
    values, indices = current_ds_store.apply_transform_fn_to_activations(lambda x: x[0:1]).topk(
        top_tokens_to_check
    )
    top_token_ints_using_fake_grad = [index.tensor_indices[1] for index in indices]
    values, indices = current_ds_store.apply_transform_fn_to_activations(lambda x: x[0:1]).topk(
        top_tokens_to_check, largest=False
    )
    bottom_token_ints_using_fake_grad = [index.tensor_indices[1] for index in indices]
    return top_token_ints_using_fake_grad, bottom_token_ints_using_fake_grad  # type: ignore
def test_vocab_token_write_to_mlp_similarity_and_autoencoder_smoke(
    standard_model_context: StandardModelContext,
    standard_autoencoder_context: AutoencoderContext,
) -> None:
    # for token write to MLP: for each neuron, check that the top tokens according to
    # VOCAB_TOKEN_WRITE_TO_INPUT_DIRECTION are "similar" (>=1 element overlap) to the top tokens
    # according to TokenReadConverter or that the bottom tokens
    # according to VOCAB_TOKEN_WRITE_TO_INPUT_DIRECTION are "similar" to the top tokens
    # according to TokenReadConverter.
    # We expect this DST to fix a sign flip in TokenReadConverter
    # caused by the MLP gradient sometimes being negative and sometimes being positive.
    prompt = "This is a test"
    top_tokens_to_check = 10
    activation_location_type = ActivationLocationType.MLP_POST_ACT
    layer_index = 5
    activation_index = 0
    for activation_index in [0, 1, 2, 3]:
        tensor_indices = (1, activation_index)  # token index doesn't matter
        for activation_location_type in {
            ActivationLocationType.MLP_POST_ACT,
            ActivationLocationType.ONLINE_AUTOENCODER_LATENT,
        }:
            (
                top_token_ints_using_input_direction,
                bottom_token_ints_using_input_direction,
            ) = _compute_top_and_bottom_token_ints_using_input_direction(
                model_context=standard_model_context,
                autoencoder_context=standard_autoencoder_context,
                prompt=prompt,
                activation_location_type=activation_location_type,
                layer_index=layer_index,
                tensor_indices=tensor_indices,
                top_tokens_to_check=top_tokens_to_check,
            )
            top_token_ints_using_postprocess = _compute_top_token_ints_using_reconstituted_grad(
                model_context=standard_model_context,
                autoencoder_context=standard_autoencoder_context,
                prompt=prompt,
                activation_location_type=activation_location_type,
                layer_index=layer_index,
                tensor_indices=tensor_indices,
                top_tokens_to_check=top_tokens_to_check,
            )
            if activation_location_type == ActivationLocationType.MLP_POST_ACT:
                top_overlap = len(
                    set(top_token_ints_using_input_direction)
                    & set(top_token_ints_using_postprocess)
                )
                bottom_overlap = len(
                    set(bottom_token_ints_using_input_direction)
                    & set(top_token_ints_using_postprocess)
                )
                if top_overlap >= 1:
                    assert bottom_overlap == 0
                else:
                    raise ValueError(
                        "Neither flipping nor unflipping worked\n"
                        f"Top tokens using input direction: {top_token_ints_using_input_direction}\n"
                        f"Bottom tokens using input direction: {bottom_token_ints_using_input_direction}\n"
                        f"Top tokens using postprocess: {top_token_ints_using_postprocess}\n"
                        f"{activation_location_type.value}, {layer_index}:{tensor_indices[-1]}\n"
                    )
                print("activation: ")
                print(activation_location_type.value, f"{layer_index}:{tensor_indices[-1]}")
                print("top tokens:")
                print(
                    standard_model_context.decode_token_list(top_token_ints_using_input_direction)
                )

================
File: neuron_explainer/tests/test_hooks.py
================
from copy import deepcopy
from functools import cache
import pytest
import torch
from neuron_explainer.models import Autoencoder, Transformer
from neuron_explainer.models.hooks import AtLayers, AutoencoderHooks, Hooks, TransformerHooks
def unflatten(f):
    def _f(x):
        return f(x.reshape(-1, x.shape[-1])).reshape(x.shape[0], x.shape[1], -1)
    return _f
@cache
def get_test_model():
    return Transformer.load("gpt2/small")
def test_forward_backward_hooks():
    """Test hooks with a simple cache saving function"""
    # create transformer
    model = get_test_model()
    cfg = model.cfg
    # create a save function to hook to the autoencoder
    forward_cache, backward_cache = {}, {}
    def store_forward(xx, layer, **kwargs):
        forward_cache[layer] = xx.detach().clone()
        return xx
    def store_backward(xx, layer, **kwargs):
        backward_cache[layer] = xx.detach().clone()
        return xx
    # create hooks
    hooks = TransformerHooks()
    hooks.mlp.post_act.append_fwd(store_forward)
    hooks.mlp.post_act.append_bwd(store_backward)
    # run the model
    input_tokens = torch.arange(5, device=model.device)[None, :]
    model.zero_grad()
    X, _ = model.forward(input_tokens, hooks=hooks)
    X.sum().backward()
    n_neurons = model.xf_layers[0].mlp.out_layer.weight.shape[1]
    # check that the forward were stored for all layers
    assert list(forward_cache.keys()) == list(range(cfg.n_layers))
    assert forward_cache[0].shape == (*input_tokens.shape, n_neurons)
    # check that the gradient were stored for all layers
    assert list(backward_cache.keys()) == list(range(cfg.n_layers))[::-1]
    assert backward_cache[0].shape == (*input_tokens.shape, n_neurons)
def test_autoencoder_hooks():
    """Test autoencoder hook with a simple cache saving function"""
    # create transformer and autoencoder
    n_latents = 10
    model = get_test_model()
    cfg = model.cfg
    autoencoder = Autoencoder(n_latents=n_latents, n_inputs=cfg.d_ff).to(model.device)
    # create a save function to hook to the autoencoder
    cache = {}
    def store_latents(latents, layer, **kwargs):
        cache[layer] = latents.detach().clone()
        return latents
    # create hooks
    ae_hooks = AutoencoderHooks(
        encode=unflatten(autoencoder.encode), decode=unflatten(autoencoder.decode)
    )
    ae_hooks.latents.append_fwd(store_latents)
    hooks = TransformerHooks()
    hooks.mlp.post_act.append_fwd(ae_hooks)
    # run the model
    input_tokens = torch.arange(5, device=model.device)[None, :]
    model.forward(input_tokens, hooks=hooks)
    # check that the latents were stored for all layers
    assert list(cache.keys()) == list(range(cfg.n_layers))
    assert cache[0].shape == (*input_tokens.shape, n_latents)
    # do the same, but only on a single layer
    layer_idx = 4
    cache = {}
    ae_hooks = AutoencoderHooks(
        encode=unflatten(autoencoder.encode), decode=unflatten(autoencoder.decode)
    )
    ae_hooks.latents.append_fwd(store_latents)
    hooks = TransformerHooks()
    hooks.mlp.post_act.append_fwd(AtLayers(layer_idx).append(ae_hooks))
    model.forward(input_tokens, hooks=hooks)
    assert list(cache.keys()) == [layer_idx]
    assert cache[layer_idx].shape == (*input_tokens.shape, n_latents)
    # do the same, but on a backward pass
    ae_hooks = AutoencoderHooks(
        encode=unflatten(autoencoder.encode), decode=unflatten(autoencoder.decode)
    )
    ae_hooks.latents.append_fwd(store_latents)
    hooks = TransformerHooks()
    hooks.mlp.post_act.append_bwd(AtLayers(layer_idx).append(ae_hooks))
    model.zero_grad()
    X, _ = model.forward(input_tokens, hooks=hooks)
    X.sum().backward()
    assert list(cache.keys()) == [layer_idx]
    assert cache[layer_idx].shape == (*input_tokens.shape, n_latents)
@pytest.mark.parametrize("add_error", [True, False])
def test_autoencoder_backward_hooks(add_error):
    """Test autoencoder hook, running a backward pass *from* a latent"""
    layer_idx = 4
    n_latents = 10
    model = get_test_model()
    cfg = model.cfg
    autoencoder = Autoencoder(n_latents=n_latents, n_inputs=cfg.d_ff).to(model.device)
    input_tokens = torch.arange(5, device=model.device)[None, :]
    cache = {}
    def store_latents_no_detach(latents, layer, **kwargs):
        cache[layer] = latents
        return latents
    ae_hooks = AutoencoderHooks(
        encode=unflatten(autoencoder.encode),
        decode=unflatten(autoencoder.decode),
        add_error=add_error,
    )
    ae_hooks.latents.append_fwd(store_latents_no_detach)
    hooks = TransformerHooks()
    hooks.mlp.post_act.append_fwd(AtLayers(layer_idx).append(ae_hooks))
    model.zero_grad()
    X, _ = model.forward(input_tokens, hooks=hooks)
    assert model.xf_layers[0].mlp.in_layer.weight.grad is None
    latents = cache[layer_idx]
    latents[:, 0].sum().backward()  # backward from the first latent
    assert model.xf_layers[0].mlp.in_layer.weight.grad is not None
@pytest.mark.parametrize("add_error", [True, False])
def test_autoencoder_hooks_ablation(add_error):
    """To test ablation, create an autoencoder with large weights at one latent, and ablate it."""
    # create transformer and autoencoder
    layer_idx = 4
    latent_idx = 1
    n_latents = 10
    model = get_test_model()
    cfg = model.cfg
    autoencoder = Autoencoder(n_latents=n_latents, n_inputs=cfg.d_ff).to(model.device)
    autoencoder.latent_bias.data[latent_idx] = +1000  # make sure the latent activates strongly
    input_tokens = torch.arange(5, device=model.device)[None, :]
    # create an ablation hook
    def ablate_latent(latents, layer, **kwargs):
        latents[:, :, latent_idx] = 0
        return latents
    # create a cache hook, to check downstream of the autoencoder hook
    def store_mlp_post_act(x, layer, **kwargs):
        cache[layer] = x.detach().clone().max().item()
        return x
    for ablate in [False, True]:
        cache = {}
        ae_hooks = AutoencoderHooks(
            encode=unflatten(autoencoder.encode),
            decode=unflatten(autoencoder.decode),
            add_error=add_error,
        )
        if ablate:
            ae_hooks.latents.append_fwd(ablate_latent)
        hooks = TransformerHooks()
        hooks.mlp.post_act.append_all(
            AtLayers(layer_idx).append(ae_hooks).append(store_mlp_post_act)
        )
        output, _ = model.forward(input_tokens, hooks=hooks)
        # if not add_error, the MLP post_act is large with no ablation
        # (because the latent activates strongly), and small with ablation
        # if add_error, the MLP post_act is small with no ablation
        # (because the autoencoder returns the identity), and large with ablation
        if ablate != add_error:
            assert cache[layer_idx] < 100
        else:
            assert cache[layer_idx] > 100
def test_autoencoder_hooks_add_error():
    """Test that the autoencoder hook modifies (or not) the graph correctly"""
    # create transformer and autoencoder
    layer_idx = 4
    n_latents = 10
    model = get_test_model()
    cfg = model.cfg
    autoencoder = Autoencoder(n_latents=n_latents, n_inputs=cfg.d_ff).to(model.device)
    input_tokens = torch.arange(5, device=model.device)[None, :]
    for add_error in [True, False]:
        # create hooks
        hooks = TransformerHooks()
        ae_hooks = AutoencoderHooks(
            encode=unflatten(autoencoder.encode),
            decode=unflatten(autoencoder.decode),
            add_error=add_error,
        )
        hooks.mlp.post_act.append_fwd(AtLayers(layer_idx).append(ae_hooks))
        # make sure to reset autoencoder gradients
        autoencoder.zero_grad()
        assert autoencoder.encoder.weight.grad is None
        # run forward and backward pass, with hooks
        model.zero_grad()
        output, _ = model.forward(input_tokens, hooks=hooks)
        output.sum().backward()
        grads = model.xf_layers[0].mlp.in_layer.weight.grad.clone()
        # run forward and backward pass, without hook
        model.zero_grad()
        output_nohook, _ = model.forward(input_tokens)
        output_nohook.sum().backward()
        grads_nohook = model.xf_layers[0].mlp.in_layer.weight.grad.clone()
        # check forward pass
        # if add_error:
        #     torch.testing.assert_allclose(output, output_nohook, atol=1e-3, rtol=1e-5)
        #     torch.testing.assert_allclose(grads, grads_nohook, atol=1., rtol=1e-1)
        assert add_error == torch.allclose(output, output_nohook, atol=1e-3, rtol=1e-5)
        # check backward pass
        assert add_error == torch.allclose(grads, grads_nohook, atol=1.0, rtol=1e-1)
        # check that we can get gradient on autoencoder latents in all cases
        assert autoencoder.encoder.weight.grad is not None
def test_hook_copy():
    hooks = TransformerHooks()
    hooks_copy = deepcopy(hooks)
    hooks.bind(layer=0)
    hooks_copy.bind(layer=1)  # does not raise
    hooks = Hooks()
    hooks_copy = deepcopy(hooks)
    hooks.bind(layer=0)
    hooks_copy.bind(layer=1)  # does not raise
    hooks = AtLayers(0)
    hooks_copy = deepcopy(hooks)
    assert hooks.condition == hooks_copy.condition
    autoencoder = Autoencoder(n_latents=10, n_inputs=5)
    hooks = AutoencoderHooks(
        encode=unflatten(autoencoder.encode), decode=unflatten(autoencoder.decode)
    )
    hooks_copy = deepcopy(hooks)
    assert hooks.encode == hooks_copy.encode

================
File: neuron_explainer/tests/test_interactive_model.py
================
import math
from typing import Any, TypeVar
import numpy as np
import pydantic
import torch
from neuron_explainer.activation_server.derived_scalar_computation import (
    LossFnConfig,
    LossFnName,
    get_derived_scalars_for_prompt,
    maybe_construct_loss_fn_for_backward_pass,
)
from neuron_explainer.activation_server.interactive_model import (
    AblationSpec,
    BatchedRequest,
    InferenceRequestSpec,
    InferenceSubRequest,
    InteractiveModel,
    MultipleTopKDerivedScalarsRequest,
    MultipleTopKDerivedScalarsRequestSpec,
    MultipleTopKDerivedScalarsResponseData,
    Tensor0D,
    Tensor1D,
    Tensor2D,
)
from neuron_explainer.activation_server.requests_and_responses import (
    BatchedResponse,
    BatchedTdbRequest,
    ComponentTypeForAttention,
    ComponentTypeForMlp,
    DerivedScalarsRequestSpec,
    GroupId,
    ProcessingRequestSpec,
    ScoredTokensRequestSpec,
    TdbRequestSpec,
    TokenScoringType,
)
from neuron_explainer.activations.derived_scalars import DerivedScalarType
from neuron_explainer.activations.derived_scalars.indexing import (
    ActivationIndex,
    AttentionTraceType,
    DerivedScalarIndex,
    MirroredActivationIndex,
    MirroredNodeIndex,
    MirroredTraceConfig,
    NodeToTrace,
    TraceConfig,
)
from neuron_explainer.activations.derived_scalars.postprocessing import TokenWriteConverter
from neuron_explainer.models.autoencoder_context import AutoencoderContext
from neuron_explainer.models.hooks import AtLayers, TransformerHooks
from neuron_explainer.models.model_component_registry import (
    ActivationLocationType,
    Dimension,
    NodeType,
    PassType,
)
from neuron_explainer.models.model_context import StandardModelContext
from neuron_explainer.tests.conftest import AUTOENCODER_TEST_DST
from neuron_explainer.tests.test_all_dsts import DETACH_LAYER_NORM_SCALE_FOR_TEST
dst_by_group_id_and_component_type: dict[GroupId, dict[str, DerivedScalarType]] = {
    GroupId.WRITE_NORM: {
        "mlp": DerivedScalarType.MLP_WRITE_NORM,
        "autoencoder": DerivedScalarType.ONLINE_AUTOENCODER_WRITE_NORM,
        "unflattened_attn": DerivedScalarType.UNFLATTENED_ATTN_WRITE_NORM,
        "per_token_attn": DerivedScalarType.ATTN_WRITE_NORM_PER_SEQUENCE_TOKEN,
    },
    GroupId.ACT_TIMES_GRAD: {
        "mlp": DerivedScalarType.MLP_ACT_TIMES_GRAD,
        "autoencoder": DerivedScalarType.ONLINE_AUTOENCODER_ACT_TIMES_GRAD,
        "unflattened_attn": DerivedScalarType.UNFLATTENED_ATTN_ACT_TIMES_GRAD,
        "per_token_attn": DerivedScalarType.ATTN_ACT_TIMES_GRAD_PER_SEQUENCE_TOKEN,
    },
    GroupId.DIRECT_WRITE_TO_GRAD: {
        "mlp": DerivedScalarType.MLP_WRITE_TO_FINAL_RESIDUAL_GRAD,
        "autoencoder": DerivedScalarType.ONLINE_AUTOENCODER_WRITE_TO_FINAL_RESIDUAL_GRAD,
        "unflattened_attn": DerivedScalarType.UNFLATTENED_ATTN_WRITE_TO_FINAL_RESIDUAL_GRAD,
        "per_token_attn": DerivedScalarType.ATTN_WRITE_TO_FINAL_RESIDUAL_GRAD_PER_SEQUENCE_TOKEN,
    },
    GroupId.ACTIVATION: {
        "mlp": DerivedScalarType.MLP_POST_ACT,
        "autoencoder": DerivedScalarType.ONLINE_AUTOENCODER_LATENT,
        "unflattened_attn": DerivedScalarType.ATTN_QK_PROBS,
    },
}
def make_dst_list_by_group_id(
    group_ids: list[GroupId],
    component_types: list[str],
) -> dict[GroupId, list[DerivedScalarType]]:
    dst_list_by_group_id: dict[GroupId, list[DerivedScalarType]] = {}
    for group_id in group_ids:
        assert group_id in dst_by_group_id_and_component_type
        dst_list_by_group_id[group_id] = []
        for component_type in component_types:
            assert component_type in dst_by_group_id_and_component_type[group_id]
            dst = dst_by_group_id_and_component_type[group_id][component_type]
            dst_list_by_group_id[group_id].append(dst)
    return dst_list_by_group_id
def assert_acts_within_epsilon(
    acts1: list[float], acts2: list[float], epsilon: float = 1e-3
) -> None:
    assert len(acts1) == len(acts2), "Activations lists are of different lengths"
    for a, b in zip(acts1, acts2):
        assert abs(a - b) < epsilon, f"Pair of activations differ by more than {epsilon}"
def assert_common_acts_within_epsilon(
    acts1: list[float],
    acts2: list[float],
    # indices1 must be a subset of indices2
    indices1: list[MirroredNodeIndex],
    indices2: list[MirroredNodeIndex],
    # some benign numeric inconsistencies appear to cause diffs just a bit over 1e-3
    epsilon: float = 2e-3,
) -> None:
    assert set(indices1).issubset(set(indices2)), f"{indices1=} is not a subset of {indices2=}"
    for i, node_index in enumerate(indices1):
        j = indices2.index(node_index)
        assert (
            abs(acts1[i] - acts2[j]) < epsilon
        ), f"Pair of activations differ by more than {epsilon}"
async def test_forward_and_backward_pass_request(
    standard_model_context: StandardModelContext,
    standard_autoencoder_context: AutoencoderContext,
) -> None:
    interactive_model = InteractiveModel.from_standard_model_context_and_autoencoder_context(
        standard_model_context,
        standard_autoencoder_context,
    )
    loss_fn_config = LossFnConfig(
        name=LossFnName.LOGIT_DIFF,
        target_tokens=["!"],
        distractor_tokens=["."],
    )
    # test per-sequence-token attention DSTs in multi-request context
    multi_top_k_request = MultipleTopKDerivedScalarsRequest(
        inference_request_spec=InferenceRequestSpec(
            prompt="Hello world",
            loss_fn_config=loss_fn_config,
        ),
        multiple_top_k_derived_scalars_request_spec=MultipleTopKDerivedScalarsRequestSpec(
            dst_list_by_group_id=make_dst_list_by_group_id(
                group_ids=[
                    GroupId.WRITE_NORM,
                    GroupId.ACT_TIMES_GRAD,
                    GroupId.DIRECT_WRITE_TO_GRAD,
                ],
                component_types=["mlp", "per_token_attn"],
            ),
            # dsts for each group ID are assumed to have defined node_type,
            # all node_types assumed to be distinct within a group_id, and all group_ids to
            # contain the same set of node_types.
            token_index=None,
            top_and_bottom_k=10,
            pass_type=PassType.FORWARD,
        ),
    )
    # test layer requests with backward pass from activation
    single_request = MultipleTopKDerivedScalarsRequest(
        inference_request_spec=InferenceRequestSpec(
            prompt="Hello world",
            loss_fn_config=None,
            trace_config=MirroredTraceConfig.from_trace_config(
                TraceConfig.from_activation_index(
                    ActivationIndex(
                        activation_location_type=ActivationLocationType.MLP_POST_ACT,
                        layer_index=5,
                        tensor_indices=(0, 0),
                        pass_type=PassType.FORWARD,
                    )
                )
            ),
        ),
        multiple_top_k_derived_scalars_request_spec=MultipleTopKDerivedScalarsRequestSpec(
            dst_list_by_group_id={
                GroupId.SINGLETON: [
                    DerivedScalarType.RESID_POST_ATTN_PROJ_TO_FINAL_RESIDUAL_GRAD,
                ]
            },
            # dsts for each group ID are assumed to have defined node_type,
            # all node_types assumed to be distinct within a group_id, and all group_ids to
            # contain the same set of node_types.
            token_index=None,
            top_and_bottom_k=100,
            pass_type=PassType.FORWARD,
        ),
    )
    single_response = await interactive_model.get_multiple_top_k_derived_scalars(single_request)
    # test layer requests
    single_request = MultipleTopKDerivedScalarsRequest(
        inference_request_spec=InferenceRequestSpec(
            prompt="Hello world",
            loss_fn_config=loss_fn_config,
        ),
        multiple_top_k_derived_scalars_request_spec=MultipleTopKDerivedScalarsRequestSpec(
            dst_list_by_group_id={
                GroupId.SINGLETON: [
                    DerivedScalarType.RESID_POST_ATTN_PROJ_TO_FINAL_RESIDUAL_GRAD,
                ]
            },
            # dsts for each group ID are assumed to have defined node_type,
            # all node_types assumed to be distinct within a group_id, and all group_ids to
            # contain the same set of node_types.
            token_index=None,
            top_and_bottom_k=100,
            pass_type=PassType.FORWARD,
        ),
    )
    single_response = await interactive_model.get_multiple_top_k_derived_scalars(single_request)
    # test individual requests, and then a combined multi-request
    # the individual requests should share the same inference request spec
    inference_request_spec = InferenceRequestSpec(
        prompt="Hello world",
        loss_fn_config=loss_fn_config,
    )
    # test vocab token requests
    vocab_token_request_spec = MultipleTopKDerivedScalarsRequestSpec(
        dst_list_by_group_id={
            GroupId.LOGITS: [DerivedScalarType.LOGITS],
        },
        # dsts for each group ID are assumed to have defined node_type,
        # all node_types assumed to be distinct within a group_id, and all group_ids to
        # contain the same set of node_types.
        token_index=1,
        top_and_bottom_k=10,
        pass_type=PassType.FORWARD,
    )
    request = MultipleTopKDerivedScalarsRequest(
        inference_request_spec=inference_request_spec,
        multiple_top_k_derived_scalars_request_spec=vocab_token_request_spec,
    )
    vocab_token_response = await interactive_model.get_multiple_top_k_derived_scalars(request)
    vocab_token_response_data_in_single = (
        vocab_token_response.multiple_top_k_derived_scalars_response_data
    )
    print(vocab_token_response_data_in_single.activations_by_group_id[GroupId.LOGITS])
    unflattened_attention_dst_request_spec = MultipleTopKDerivedScalarsRequestSpec(
        dst_list_by_group_id=make_dst_list_by_group_id(
            group_ids=[
                GroupId.WRITE_NORM,
                GroupId.ACT_TIMES_GRAD,
                GroupId.DIRECT_WRITE_TO_GRAD,
            ],
            component_types=["mlp", "unflattened_attn"],
        ),
        # dsts for each group ID are assumed to have defined node_type,
        # all node_types assumed to be distinct within a group_id, and all group_ids to
        # contain the same set of node_types.
        token_index=None,
        top_and_bottom_k=10,
        pass_type=PassType.FORWARD,
    )
    # test requests with unflattened attention DSTs
    multi_top_k_request = MultipleTopKDerivedScalarsRequest(
        inference_request_spec=inference_request_spec,
        multiple_top_k_derived_scalars_request_spec=unflattened_attention_dst_request_spec,
    )
    unflattened_attention_response = await interactive_model.get_multiple_top_k_derived_scalars(
        multi_top_k_request
    )
    unflattened_attention_response_data_in_single = (
        unflattened_attention_response.multiple_top_k_derived_scalars_response_data
    )
    # test request for vocab tokens and flattened attention DSTs combined
    batched_request = BatchedRequest(
        inference_sub_requests=[
            InferenceSubRequest(
                inference_request_spec=inference_request_spec,
                processing_request_spec_by_name={
                    "vocab_token": vocab_token_request_spec,
                    "unflattened_attention": unflattened_attention_dst_request_spec,
                },
            ),
        ],
    )
    batched_response = await interactive_model.handle_batched_request(batched_request)
    assert len(batched_response.inference_sub_responses) == 1
    response = batched_response.inference_sub_responses[0]
    vocab_token_response_data = response.processing_response_data_by_name["vocab_token"]
    assert isinstance(vocab_token_response_data, MultipleTopKDerivedScalarsResponseData)
    assert (
        vocab_token_response_data_in_single.activations_by_group_id[GroupId.LOGITS]
        == vocab_token_response_data.activations_by_group_id[GroupId.LOGITS]
    )
    unflattened_attention_response_data = response.processing_response_data_by_name[
        "unflattened_attention"
    ]
    assert isinstance(
        unflattened_attention_response_data,
        MultipleTopKDerivedScalarsResponseData,
    )
    assert (
        unflattened_attention_response_data_in_single.node_indices
        == unflattened_attention_response_data.node_indices
    )
    for group_id in unflattened_attention_response_data_in_single.activations_by_group_id:
        assert (
            unflattened_attention_response_data_in_single.activations_by_group_id[group_id]
            == unflattened_attention_response_data.activations_by_group_id[group_id]
        )
async def test_top_tokens_timing(
    standard_model_context: StandardModelContext,
    standard_autoencoder_context: AutoencoderContext,
) -> None:
    interactive_model = InteractiveModel.from_standard_model_context_and_autoencoder_context(
        standard_model_context,
        standard_autoencoder_context,
    )
    loss_fn_config = LossFnConfig(
        name=LossFnName.LOGIT_DIFF,
        target_tokens=["!"],
        distractor_tokens=["."],
    )
    # the individual requests should share the same inference request spec
    inference_request_spec = InferenceRequestSpec(
        prompt="Hello world",
        loss_fn_config=loss_fn_config,
    )
    top_and_bottom_k = 10
    num_tokens = 30
    multi_top_k_request_spec = MultipleTopKDerivedScalarsRequestSpec(
        dst_list_by_group_id=make_dst_list_by_group_id(
            group_ids=[
                GroupId.DIRECT_WRITE_TO_GRAD,
            ],
            component_types=["mlp"],
        ),
        # dsts for each group ID are assumed to have defined node_type,
        # all node_types assumed to be distinct within a group_id, and all group_ids to
        # contain the same set of node_types.
        token_index=None,
        top_and_bottom_k=top_and_bottom_k,
        pass_type=PassType.FORWARD,
    )
    write_tokens_request_spec = ScoredTokensRequestSpec(
        token_scoring_type=TokenScoringType.UPVOTED_OUTPUT_TOKENS,
        num_tokens=num_tokens,
        depends_on_spec_name="multi_top_k",
    )
    read_tokens_request_spec = ScoredTokensRequestSpec(
        token_scoring_type=TokenScoringType.INPUT_TOKENS_THAT_UPVOTE_MLP,
        num_tokens=num_tokens,
        depends_on_spec_name="multi_top_k",
    )
    # test request for vocab tokens and flattened attention DSTs combined
    batched_request = BatchedRequest(
        inference_sub_requests=[
            InferenceSubRequest(
                inference_request_spec=inference_request_spec,
                processing_request_spec_by_name={
                    "multi_top_k": multi_top_k_request_spec,
                    "write_tokens": write_tokens_request_spec,
                    "read_tokens": read_tokens_request_spec,
                },
            ),
        ],
    )
    batched_response = await interactive_model.handle_batched_request(batched_request)
async def test_postprocessing(
    standard_model_context: StandardModelContext,
    standard_autoencoder_context: AutoencoderContext,
) -> None:
    interactive_model = InteractiveModel.from_standard_model_context_and_autoencoder_context(
        standard_model_context,
        standard_autoencoder_context,
    )
    loss_fn_config = LossFnConfig(
        name=LossFnName.LOGIT_DIFF,
        target_tokens=["!"],
        distractor_tokens=["."],
    )
    for component_type in ["mlp", "autoencoder"]:
        # test per-sequence-token attention DSTs in multi-request context
        multi_top_k_request = MultipleTopKDerivedScalarsRequest(
            inference_request_spec=InferenceRequestSpec(
                prompt="Hello world",
                loss_fn_config=loss_fn_config,
            ),
            multiple_top_k_derived_scalars_request_spec=MultipleTopKDerivedScalarsRequestSpec(
                dst_list_by_group_id=make_dst_list_by_group_id(
                    group_ids=[GroupId.ACTIVATION],
                    component_types=[component_type, "unflattened_attn"],
                ),
                # dsts for each group ID are assumed to have defined node_type,
                # all node_types assumed to be distinct within a group_id, and all group_ids to
                # contain the same set of node_types.
                token_index=None,
                top_and_bottom_k=10,
                pass_type=PassType.FORWARD,
            ),
        )
        multi_response = await interactive_model.get_multiple_top_k_derived_scalars(
            multi_top_k_request
        )
        postprocessor = TokenWriteConverter(
            model_context=standard_model_context,
            multi_autoencoder_context=standard_autoencoder_context,
        )
        # get example ds_index and activation
        node_indices = multi_response.multiple_top_k_derived_scalars_response_data.node_indices
        for node_index_index in [0, 1]:
            print()
            activation = torch.tensor(
                multi_response.multiple_top_k_derived_scalars_response_data.activations_by_group_id[
                    GroupId.ACTIVATION
                ][node_index_index],
                device=standard_model_context.device,
            )
            node_index = node_indices[node_index_index]
            print(f"{node_index.node_type=}")
            dst_list = multi_top_k_request.multiple_top_k_derived_scalars_request_spec.dst_list_by_group_id[
                GroupId.ACTIVATION
            ]
            matching_dsts = [dst for dst in dst_list if dst.node_type == node_index.node_type]
            assert len(matching_dsts) == 1, f"{matching_dsts=}, {node_index=}, {dst_list=}"
            dst = matching_dsts[0]
            if dst != DerivedScalarType.ATTN_QK_PROBS:
                # for non-attention activations, the activation preserves enough information to reconstruct the token space write vector
                ds_index = DerivedScalarIndex.from_node_index(
                    node_index,
                    dst,
                )
                token_write = postprocessor.postprocess_tensor(ds_index, activation)
                print(f"{token_write.shape=}")
async def test_all_layer_autoencoders_request(
    standard_model_context: StandardModelContext,
    standard_autoencoder_context: AutoencoderContext,
) -> None:
    interactive_model = InteractiveModel.from_standard_model_context_and_autoencoder_context(
        standard_model_context,
        standard_autoencoder_context,
    )
    loss_fn_config = LossFnConfig(
        name=LossFnName.LOGIT_DIFF,
        target_tokens=["!"],
        distractor_tokens=["."],
    )
    single_request = MultipleTopKDerivedScalarsRequest(
        inference_request_spec=InferenceRequestSpec(
            prompt="Hello world",
            loss_fn_config=loss_fn_config,
        ),
        multiple_top_k_derived_scalars_request_spec=MultipleTopKDerivedScalarsRequestSpec(
            dst_list_by_group_id={
                GroupId.SINGLETON: [
                    DerivedScalarType.ONLINE_AUTOENCODER_LATENT,
                ]
            },
            # dsts for each group ID are assumed to have defined node_type,
            # all node_types assumed to be distinct within a group_id, and all group_ids to
            # contain the same set of node_types.
            token_index=None,
            top_and_bottom_k=100,
            pass_type=PassType.FORWARD,
            dimensions_to_keep_for_intermediate_sum=[
                Dimension.SEQUENCE_TOKENS,
                # Dimension.AUTOENCODER_LATENTS,
            ],
        ),
    )
    single_response = await interactive_model.get_multiple_top_k_derived_scalars(single_request)
    dims_to_keep_by_ndims: dict[int, list[Dimension]] = {
        2: [
            Dimension.SEQUENCE_TOKENS,
            Dimension.AUTOENCODER_LATENTS,
        ],
        1: [
            Dimension.SEQUENCE_TOKENS,
        ],
        0: [],
    }
    single_request_by_ndims = {}
    for ndims in dims_to_keep_by_ndims.keys():
        single_request_by_ndims[ndims] = MultipleTopKDerivedScalarsRequest(
            inference_request_spec=InferenceRequestSpec(
                prompt="Hello world",
                loss_fn_config=loss_fn_config,
            ),
            multiple_top_k_derived_scalars_request_spec=MultipleTopKDerivedScalarsRequestSpec(
                dst_list_by_group_id={
                    GroupId.SINGLETON: [
                        DerivedScalarType.ONLINE_AUTOENCODER_LATENT,
                    ]
                },
                # dsts for each group ID are assumed to have defined node_type,
                # all node_types assumed to be distinct within a group_id, and all group_ids to
                # contain the same set of node_types.
                token_index=None,
                top_and_bottom_k=100,
                pass_type=PassType.FORWARD,
                dimensions_to_keep_for_intermediate_sum=dims_to_keep_by_ndims[ndims],
            ),
        )
    single_response_by_ndims = {
        ndims: await interactive_model.get_multiple_top_k_derived_scalars(
            single_request_by_ndims[ndims]
        )
        for ndims in dims_to_keep_by_ndims.keys()
    }
    dtype_by_ndims = {
        ndims: {
            dst: type(
                single_response_by_ndims[
                    ndims
                ].multiple_top_k_derived_scalars_response_data.intermediate_sum_activations_by_dst_by_group_id[
                    GroupId.SINGLETON
                ][
                    dst
                ]
            )
            for dst in single_response_by_ndims[ndims]
            .multiple_top_k_derived_scalars_response_data.intermediate_sum_activations_by_dst_by_group_id[
                GroupId.SINGLETON
            ]
            .keys()
        }
        for ndims in dims_to_keep_by_ndims.keys()
    }
    for ndims in single_response_by_ndims.keys():
        single_response = single_response_by_ndims[ndims]
        sum_by_dst = single_response.multiple_top_k_derived_scalars_response_data.intermediate_sum_activations_by_dst_by_group_id[
            GroupId.SINGLETON
        ]
        for dst in sum_by_dst.values():
            if ndims == 2:
                assert isinstance(
                    dst, Tensor2D
                ), f"{dst=} for {dims_to_keep_by_ndims=}, {dtype_by_ndims=}"
            elif ndims == 1:
                assert isinstance(dst, Tensor1D)
            elif ndims == 0:
                assert isinstance(dst, Tensor0D)
            else:
                raise ValueError(f"Invalid ndims: {ndims}")
async def test_ablation_specs(standard_model_context: StandardModelContext) -> None:
    interactive_model = InteractiveModel.from_standard_model_context(standard_model_context)
    vocab_token_request_spec = MultipleTopKDerivedScalarsRequestSpec(
        dst_list_by_group_id={
            GroupId.LOGITS: [
                DerivedScalarType.LOGITS,
            ]
        },
        # dsts for each group ID are assumed to have defined node_type,
        # all node_types assumed to be distinct within a group_id, and all group_ids to
        # contain the same set of node_types.
        token_index=1,
        top_and_bottom_k=10,
        pass_type=PassType.FORWARD,
    )
    mlp_act_times_grad_request_spec = MultipleTopKDerivedScalarsRequestSpec(
        dst_list_by_group_id={
            GroupId.ACT_TIMES_GRAD: [
                DerivedScalarType.MLP_ACT_TIMES_GRAD,
            ]
        },
        token_index=None,
        top_and_bottom_k=100,
        pass_type=PassType.FORWARD,
    )
    attn_act_times_grad_request_spec = MultipleTopKDerivedScalarsRequestSpec(
        dst_list_by_group_id={
            GroupId.ACT_TIMES_GRAD: [
                DerivedScalarType.UNFLATTENED_ATTN_ACT_TIMES_GRAD,
            ]
        },
        token_index=None,
        top_and_bottom_k=100,
        pass_type=PassType.FORWARD,
    )
    zero_ablation_specs = [
        AblationSpec(
            index=MirroredActivationIndex(
                layer_index=0,
                activation_location_type=ActivationLocationType.MLP_POST_ACT,
                tensor_indices=("All", 0),
                pass_type=PassType.FORWARD,
            ),
            value=0.0,
        )
    ]
    ablation_specs_by_ablation_setting = {
        "clean": None,
        "ablated": zero_ablation_specs,
    }
    logits_by_ablation_setting = {}
    mlp_act_times_grad_by_ablation_setting = {}
    attn_act_times_grad_by_ablation_setting = {}
    for ablation_setting, ablation_specs in ablation_specs_by_ablation_setting.items():
        print(f"Testing ablation setting: {ablation_setting}")
        request_spec = InferenceRequestSpec(
            prompt="Hello world",
            ablation_specs=ablation_specs,
            loss_fn_config=LossFnConfig(
                name=LossFnName.LOGIT_DIFF,
                target_tokens=["!"],
                distractor_tokens=["."],
            ),
        )
        request_spec2 = InferenceRequestSpec(
            prompt="Goodbye",
            ablation_specs=ablation_specs,
            loss_fn_config=LossFnConfig(
                name=LossFnName.LOGIT_DIFF,
                target_tokens=["?"],
                distractor_tokens=[","],
            ),
        )
        batched_request = BatchedRequest(
            inference_sub_requests=[
                InferenceSubRequest(
                    inference_request_spec=request_spec,
                    processing_request_spec_by_name={
                        "vocab_token": vocab_token_request_spec,
                        "mlp_act_times_grad": mlp_act_times_grad_request_spec,
                        "attn_act_times_grad": attn_act_times_grad_request_spec,
                    },
                ),
                InferenceSubRequest(
                    inference_request_spec=request_spec2,
                    processing_request_spec_by_name={
                        "vocab_token": vocab_token_request_spec,
                        "mlp_act_times_grad": mlp_act_times_grad_request_spec,
                        "attn_act_times_grad": attn_act_times_grad_request_spec,
                    },
                ),
            ],
        )
        batched_response = await interactive_model.handle_batched_request(batched_request)
        # We add two requests, so we should get two responses. For now, only check
        # the validity of the first response though.
        assert len(batched_response.inference_sub_responses) == 2
        response = batched_response.inference_sub_responses[0]
        top_k = response.processing_response_data_by_name["vocab_token"]
        assert isinstance(top_k, MultipleTopKDerivedScalarsResponseData)
        logits_by_ablation_setting[ablation_setting] = top_k.activations_by_group_id[GroupId.LOGITS]
        top_k = response.processing_response_data_by_name["mlp_act_times_grad"]
        assert isinstance(top_k, MultipleTopKDerivedScalarsResponseData)
        mlp_act_times_grad_by_ablation_setting[ablation_setting] = (
            top_k.activations_by_group_id[GroupId.ACT_TIMES_GRAD],
            top_k.node_indices,
        )
        top_k = response.processing_response_data_by_name["attn_act_times_grad"]
        assert isinstance(top_k, MultipleTopKDerivedScalarsResponseData)
        attn_act_times_grad_by_ablation_setting[ablation_setting] = (
            top_k.activations_by_group_id[GroupId.ACT_TIMES_GRAD],
            top_k.node_indices,
        )
    # Check if the output logits changed
    assert not np.array_equal(
        logits_by_ablation_setting["clean"],
        logits_by_ablation_setting["ablated"],
    )
    # Check if the activations changed
    assert not np.array_equal(
        mlp_act_times_grad_by_ablation_setting["clean"][0],
        mlp_act_times_grad_by_ablation_setting["ablated"][0],
    )
    # Check if the attn activations changed
    assert not np.array_equal(
        attn_act_times_grad_by_ablation_setting["clean"][0],
        attn_act_times_grad_by_ablation_setting["ablated"][0],
    )
async def test_batched_ablation_equivalent_to_single_ablation(
    standard_model_context: StandardModelContext,
    standard_autoencoder_context: AutoencoderContext,
) -> None:
    interactive_model = InteractiveModel.from_standard_model_context_and_autoencoder_context(
        standard_model_context,
        standard_autoencoder_context,
    )
    processing_request_spec_by_name: dict[str, ProcessingRequestSpec] = {
        "topKComponents": MultipleTopKDerivedScalarsRequestSpec(
            dst_list_by_group_id={
                GroupId.WRITE_NORM: [
                    DerivedScalarType.UNFLATTENED_ATTN_WRITE_NORM,
                    DerivedScalarType.ONLINE_AUTOENCODER_WRITE_NORM,
                ],
            },
            top_and_bottom_k=4,
            token_index=None,
        ),
    }
    req_a = InferenceSubRequest(
        inference_request_spec=InferenceRequestSpec(
            prompt="<|endoftext|>Paris, France. Ottawa,",
            loss_fn_config=LossFnConfig(
                name=LossFnName.LOGIT_DIFF,
                target_tokens=[" Canada"],
                distractor_tokens=[" Germany"],
            ),
            ablation_specs=[
                AblationSpec(
                    index=MirroredActivationIndex(
                        activation_location_type=ActivationLocationType.ONLINE_AUTOENCODER_LATENT,
                        pass_type=PassType.FORWARD,
                        tensor_indices=(6, 2),
                        layer_index=8,
                    ),
                    value=0,
                ),
            ],
        ),
        processing_request_spec_by_name=processing_request_spec_by_name,
    )
    req_b = InferenceSubRequest(
        inference_request_spec=InferenceRequestSpec(
            prompt="<|endoftext|>Paris, France. Madrid,",
            loss_fn_config=LossFnConfig(
                name=LossFnName.LOGIT_DIFF,
                target_tokens=[" Canada"],
                distractor_tokens=[" Germany"],
            ),
            ablation_specs=[
                AblationSpec(
                    index=MirroredActivationIndex(
                        activation_location_type=ActivationLocationType.ONLINE_AUTOENCODER_LATENT,
                        pass_type=PassType.FORWARD,
                        tensor_indices=(5, 1),
                        layer_index=7,
                    ),
                    value=0,
                ),
            ],
        ),
        processing_request_spec_by_name=processing_request_spec_by_name,
    )
    def get_acts_and_indices_from_response(
        response: BatchedResponse, sub_resp_index: int = 0
    ) -> tuple[list[float], list[MirroredNodeIndex]]:
        sub_response = response.inference_sub_responses[sub_resp_index]
        sub_response_data = sub_response.processing_response_data_by_name["topKComponents"]
        assert isinstance(
            sub_response_data, MultipleTopKDerivedScalarsResponseData
        ), f"{sub_response_data=}"
        assert len(list(sub_response_data.activations_by_group_id.keys())) == 1
        return (
            sub_response_data.activations_by_group_id[GroupId.WRITE_NORM],
            sub_response_data.node_indices,
        )
    resp_a = await interactive_model.handle_batched_request(
        BatchedRequest(inference_sub_requests=[req_a])
    )
    a_acts, a_indices = get_acts_and_indices_from_response(resp_a)
    resp_b = await interactive_model.handle_batched_request(
        BatchedRequest(inference_sub_requests=[req_b])
    )
    b_acts, b_indices = get_acts_and_indices_from_response(resp_b)
    resp_ab = await interactive_model.handle_batched_request(
        BatchedRequest(inference_sub_requests=[req_a, req_b])
    )
    batched_a_acts, batched_a_indices = get_acts_and_indices_from_response(
        resp_ab, sub_resp_index=0
    )
    batched_b_acts, batched_b_indices = get_acts_and_indices_from_response(
        resp_ab, sub_resp_index=1
    )
    assert set(a_indices + b_indices) == set(
        batched_a_indices
    ), f"{a_indices=}, {b_indices=}, {batched_a_indices=}"
    assert_common_acts_within_epsilon(a_acts, batched_a_acts, a_indices, batched_a_indices)
    assert set(a_indices + b_indices) == set(
        batched_b_indices
    ), f"{a_indices=}, {b_indices=}, {batched_b_indices=}"
    assert_common_acts_within_epsilon(b_acts, batched_b_acts, b_indices, batched_b_indices)
async def test_activation_grabbing(
    standard_model_context: StandardModelContext,
    standard_autoencoder_context: AutoencoderContext,
) -> None:
    """Compute act-times-grad of all upstream nodes for a given node"""
    interactive_model = InteractiveModel.from_standard_model_context_and_autoencoder_context(
        standard_model_context,
        standard_autoencoder_context,
    )
    # specify from what points the backward pass will be performed; either from the loss or
    # from an activation, running each backward pass one after another
    layer_index_for_grad = 6
    inference_kwargs_by_backward_pass_setting: dict[str, dict[str, Any]] = {
        "loss": {
            "loss_fn_config": LossFnConfig(
                name=LossFnName.LOGIT_DIFF,
                target_tokens=["!"],
                distractor_tokens=["."],
            ),
            "trace_config": None,
        },
        "activation": {
            "loss_fn_config": None,
            "trace_config": MirroredTraceConfig.from_trace_config(
                TraceConfig.from_activation_index(
                    ActivationIndex(
                        layer_index=layer_index_for_grad,
                        activation_location_type=ActivationLocationType.MLP_POST_ACT,
                        tensor_indices=(0, 1),
                        pass_type=PassType.FORWARD,
                    ),
                    detach_layer_norm_scale=DETACH_LAYER_NORM_SCALE_FOR_TEST,
                )
            ),
        },
        "autoencoder_activation": {
            "loss_fn_config": None,
            "trace_config": MirroredTraceConfig.from_trace_config(
                TraceConfig.from_activation_index(
                    ActivationIndex(
                        layer_index=layer_index_for_grad,
                        activation_location_type=ActivationLocationType.ONLINE_AUTOENCODER_LATENT,
                        tensor_indices=(0, 1),
                        pass_type=PassType.FORWARD,
                    ),
                    detach_layer_norm_scale=DETACH_LAYER_NORM_SCALE_FOR_TEST,
                )
            ),
        },
    }
    # For each node, compute the gradient from this node to all upstream nodes, and select the
    # top-k upstream nodes with the largest activation-times-gradient.
    grads_by_backward_pass_setting = {}  # more precisely "act-times-grad"
    for backward_pass_setting in [
        "loss",
        "activation",
        "autoencoder_activation",
    ]:
        inference_kwargs: dict[str, Any] = inference_kwargs_by_backward_pass_setting[
            backward_pass_setting
        ]
        grad_requiring_dst_request_spec = MultipleTopKDerivedScalarsRequestSpec(
            dst_list_by_group_id=make_dst_list_by_group_id(
                group_ids=[
                    GroupId.ACT_TIMES_GRAD,
                ],
                component_types=[
                    "mlp",
                    "autoencoder",
                    "unflattened_attn",
                ],
            ),
            # dsts for each group ID are assumed to have defined node_type,
            # all node_types assumed to be distinct within a group_id, and all group_ids to
            # contain the same set of node_types.
            token_index=None,
            top_and_bottom_k=10,
            pass_type=PassType.FORWARD,
        )
        multiple_top_k_layers_request_spec = MultipleTopKDerivedScalarsRequestSpec(
            dst_list_by_group_id={
                GroupId.SINGLETON: [
                    DerivedScalarType.RESID_POST_ATTN_PROJ_TO_FINAL_RESIDUAL_GRAD,
                ]
            },
            # dsts for each group ID are assumed to have defined node_type,
            # all node_types assumed to be distinct within a group_id, and all group_ids to
            # contain the same set of node_types.
            token_index=None,
            top_and_bottom_k=100,
            pass_type=PassType.FORWARD,
        )
        print(f"Testing backward pass setting: {backward_pass_setting}")
        batched_request = BatchedRequest(
            inference_sub_requests=[
                InferenceSubRequest(
                    inference_request_spec=InferenceRequestSpec(
                        prompt="Hello world",
                        **inference_kwargs,
                    ),
                    processing_request_spec_by_name={
                        "multi_top_k": grad_requiring_dst_request_spec,
                        "layers": multiple_top_k_layers_request_spec,
                    },
                ),
                InferenceSubRequest(
                    inference_request_spec=InferenceRequestSpec(
                        prompt="Goodbye",
                        **inference_kwargs,
                    ),
                    processing_request_spec_by_name={
                        "multi_top_k": grad_requiring_dst_request_spec,
                        "layers": multiple_top_k_layers_request_spec,
                    },
                ),
            ],
        )
        assert (
            batched_request.inference_sub_requests[0].inference_request_spec.trace_config
            is not None
            or batched_request.inference_sub_requests[0].inference_request_spec.loss_fn_config
            is not None
        )
        batched_response = await interactive_model.handle_batched_request(batched_request)
        # We add two requests, so we should get two responses. For now, only check
        # the validity of the first response though.
        assert len(batched_response.inference_sub_responses) == 2
        response = batched_response.inference_sub_responses[0]
        multi_top_k = response.processing_response_data_by_name["multi_top_k"]
        assert isinstance(multi_top_k, MultipleTopKDerivedScalarsResponseData)
        grads_by_backward_pass_setting[backward_pass_setting] = multi_top_k.activations_by_group_id[
            GroupId.ACT_TIMES_GRAD
        ]
    # Check that the values are different for different backward pass settings
    assert not np.array_equal(
        grads_by_backward_pass_setting["loss"],
        grads_by_backward_pass_setting["activation"],
    )
    assert not np.array_equal(
        grads_by_backward_pass_setting["activation"],
        grads_by_backward_pass_setting["autoencoder_activation"],
    )
    # Checking the extremal values of each backward pass setting
    # Note: will need to change if anything about the requests is changed
    top_value_by_backward_pass_setting = {
        "loss": 162.00540161132812,
        "activation": 0.19626690447330475,
        "autoencoder_activation": 0.44310837984085083,
    }
    for backward_pass_setting in [
        "loss",
        "activation",
        "autoencoder_activation",
    ]:
        grads = grads_by_backward_pass_setting[backward_pass_setting]
        assert math.isclose(
            grads[0], top_value_by_backward_pass_setting[backward_pass_setting], rel_tol=1e-4
        ), f"{grads[0]} != {top_value_by_backward_pass_setting[backward_pass_setting]}"
async def test_batched_activation_grabbing(
    standard_model_context: StandardModelContext,
    standard_autoencoder_context: AutoencoderContext,
) -> None:
    interactive_model = InteractiveModel.from_standard_model_context_and_autoencoder_context(
        standard_model_context,
        standard_autoencoder_context,
    )
    # Derived scalars
    batched_request = BatchedRequest(
        inference_sub_requests=[
            InferenceSubRequest(
                inference_request_spec=InferenceRequestSpec(
                    prompt="Hello world",
                ),
                processing_request_spec_by_name={
                    "foobar": DerivedScalarsRequestSpec(
                        dst=DerivedScalarType.ONLINE_AUTOENCODER_LATENT,
                        layer_index=5,
                        activation_index=0,
                    ),
                },
            ),
            InferenceSubRequest(
                inference_request_spec=InferenceRequestSpec(
                    prompt="Goodbye",
                ),
                processing_request_spec_by_name={
                    "foobar2": DerivedScalarsRequestSpec(
                        dst=DerivedScalarType.ONLINE_AUTOENCODER_LATENT,
                        layer_index=6,
                        activation_index=0,
                    ),
                },
            ),
        ],
    )
    batched_response = await interactive_model.handle_batched_request(batched_request)
    assert len(batched_response.inference_sub_responses) == 2
    # Verify that Pydantic can serialize and deserialize the request
    raw_request = batched_request.dict()
    reconstituted_request = pydantic.parse_obj_as(BatchedRequest, raw_request)
    assert isinstance(reconstituted_request, BatchedRequest)
    assert len(reconstituted_request.inference_sub_requests) == 2
    # Activations
    batched_request = BatchedRequest(
        inference_sub_requests=[
            InferenceSubRequest(
                inference_request_spec=InferenceRequestSpec(
                    prompt="hello world my name is bob",
                ),
                processing_request_spec_by_name={
                    "foobar": DerivedScalarsRequestSpec(
                        dst=AUTOENCODER_TEST_DST,
                        layer_index=5,
                        activation_index=100,
                    ),
                },
            ),
            InferenceSubRequest(
                inference_request_spec=InferenceRequestSpec(
                    prompt="goodbye world i hope your good",
                ),
                processing_request_spec_by_name={
                    "foobar2": DerivedScalarsRequestSpec(
                        dst=AUTOENCODER_TEST_DST,
                        layer_index=6,
                        activation_index=101,
                    ),
                },
            ),
        ],
    )
    batched_response = await interactive_model.handle_batched_request(batched_request)
    assert len(batched_response.inference_sub_responses) == 2
async def test_multi_topk_multiple_different_layer_indices(
    standard_model_context: StandardModelContext,
) -> None:
    interactive_model = InteractiveModel.from_standard_model_context(standard_model_context)
    # test per-sequence-token attention DSTs in multi-request context
    multi_top_k_request1 = MultipleTopKDerivedScalarsRequest(
        inference_request_spec=InferenceRequestSpec(
            prompt="Hello world",
        ),
        multiple_top_k_derived_scalars_request_spec=MultipleTopKDerivedScalarsRequestSpec(
            dst_list_by_group_id=make_dst_list_by_group_id(
                group_ids=[GroupId.WRITE_NORM],
                component_types=["mlp"],
            ),
            token_index=None,
            top_and_bottom_k=100,
            pass_type=PassType.FORWARD,
        ),
    )
    multi_response1 = await interactive_model.get_multiple_top_k_derived_scalars(
        multi_top_k_request1
    )
    multi_top_k_request2 = multi_top_k_request1.copy()
    multi_response2 = await interactive_model.get_multiple_top_k_derived_scalars(
        multi_top_k_request2
    )
    data1 = multi_response1.multiple_top_k_derived_scalars_response_data
    data2 = multi_response2.multiple_top_k_derived_scalars_response_data
    for i1, node_indices1 in enumerate(data1.node_indices):
        for i2, node_indices2 in enumerate(data2.node_indices):
            if node_indices1 == node_indices2:
                value1 = data1.activations_by_group_id[GroupId.WRITE_NORM][i1]
                value2 = data2.activations_by_group_id[GroupId.WRITE_NORM][i2]
                assert_acts_within_epsilon(
                    [value1],
                    [value2],
                    epsilon=0.1,
                )
def test_get_derived_scalars_for_prompt(
    standard_model_context: StandardModelContext,
) -> None:
    prompt = "<|endoftext|>1+1="
    target = "2"
    loss_fn_name = LossFnName.LOGIT_DIFF
    distractor = "1"
    loss_fn = maybe_construct_loss_fn_for_backward_pass(
        model_context=standard_model_context,
        config=LossFnConfig(
            name=loss_fn_name,
            target_tokens=[target],
            distractor_tokens=[distractor],
        ),
    )
    (
        ds_store,
        inference_and_token_data,
        _,  # raw activation store; not used by current test
    ) = get_derived_scalars_for_prompt(
        model_context=standard_model_context,
        dst_and_config_list=[
            (DerivedScalarType.MLP_POST_ACT, None),  # None -> default config
            (DerivedScalarType.ATTN_QK_PROBS, None),
        ],
        prompt=prompt,
        loss_fn_for_backward_pass=loss_fn,
    )
async def test_tdb_request_vs_transformer(
    standard_model_context: StandardModelContext,
) -> None:
    """
    Test that the top and bottom output token logits returned by a BatchedTdbRequest
    match those straightforwardly output by the Transformer. Also test that the top
    MLP activation returned by the BatchedTdbRequest matches the activation value stored
    using a hook on the Transformer forward pass directly.
    This test compares the simplest way to get outputs and activations from a Transformer, with the
    way that is used in TDB.
    """
    transformer = standard_model_context.get_or_create_model()
    prompt = "This is a test"
    input_token_ints = torch.tensor(
        standard_model_context.encode(prompt), device=standard_model_context.device
    ).unsqueeze(0)
    logits, _ = transformer(input_token_ints)
    # these are the logits for the next token in the sequence, directly from the Transformer
    logits = logits[0, -1, : standard_model_context.n_vocab]
    tdb_request = TdbRequestSpec(
        prompt=prompt,
        target_tokens=["."],
        distractor_tokens=["!"],
        component_type_for_mlp=ComponentTypeForMlp.NEURON,
        component_type_for_attention=ComponentTypeForAttention.ATTENTION_HEAD,
        top_and_bottom_k_for_node_table=5,
        hide_early_layers_when_ablating=False,
        node_ablations=None,
        upstream_node_to_trace=None,
        downstream_node_to_trace=None,
    )
    batched_tdb_request = BatchedTdbRequest(sub_requests=[tdb_request])
    interactive_model = InteractiveModel.from_standard_model_context(standard_model_context)
    batched_tdb_response = await interactive_model.handle_batched_tdb_request(batched_tdb_request)
    output_token_logits_response = batched_tdb_response.inference_sub_responses[
        0
    ].processing_response_data_by_name["topOutputTokenLogits"]
    assert isinstance(output_token_logits_response, MultipleTopKDerivedScalarsResponseData)
    logit_activations = output_token_logits_response.activations_by_group_id[GroupId.LOGITS]
    logit_indices = output_token_logits_response.node_indices
    # these are activations and NodeIndex objects associated with the top and bottom predicted
    # tokens for the next token in the sequence
    vocab_token_indices = [index.tensor_indices[1] for index in logit_indices]
    top_k_logits = len(vocab_token_indices) // 2
    # first half are largest
    assert logits.topk(top_k_logits).indices.tolist() == vocab_token_indices[:top_k_logits]
    assert logits.topk(top_k_logits).values.tolist() == logit_activations[:top_k_logits]
    # second half are smallest
    assert (
        logits.topk(top_k_logits, largest=False).indices.tolist()
        == vocab_token_indices[-top_k_logits:][::-1]
    )
    assert (
        logits.topk(top_k_logits, largest=False).values.tolist()
        == logit_activations[-top_k_logits:][::-1]
    )
    top_k_components_response = batched_tdb_response.inference_sub_responses[
        0
    ].processing_response_data_by_name["topKComponents"]
    assert isinstance(top_k_components_response, MultipleTopKDerivedScalarsResponseData)
    top_node_indices = top_k_components_response.node_indices
    top_activations = top_k_components_response.activations_by_group_id[GroupId.ACTIVATION]
    mlp_neuron_index, mlp_activation = next(
        (
            (node_index, activation)
            for node_index, activation in zip(top_node_indices, top_activations)
            if node_index.node_type == NodeType.MLP_NEURON
        ),
        (None, None),
    )  # get the first MLP neuron index returned by the TDB request
    assert mlp_neuron_index is not None
    hooks = TransformerHooks()
    stored_activation = {}
    def saving_hook_fn(act: torch.Tensor, **kwargs: Any) -> torch.Tensor:
        stored_activation["act"] = act[
            (0,) + mlp_neuron_index.tensor_indices
        ].item()  # 0 batch index
        return act  # store the activation value for the MLP neuron in question
    hooks.append_to_path(
        "mlp.post_act.fwd",
        AtLayers([assert_not_none(mlp_neuron_index.layer_index)]).append(saving_hook_fn),
    )
    transformer(input_token_ints, hooks=hooks)
    # the hook should have populated the dict with the MLP neuron's activation value
    assert stored_activation["act"] == mlp_activation
async def test_tdb_request_with_autoencoder_vs_transformer(
    standard_model_context: StandardModelContext,
    standard_autoencoder_context: AutoencoderContext,
) -> None:
    """
    Test that the top MLP autoencoder latent activation returned by the BatchedTdbRequest
    matches the activation value stored using a hook on the Transformer forward pass directly.
    This test compares the simplest way to get activations from a Transformer, with the way that is
    used in TDB.
    """
    transformer = standard_model_context.get_or_create_model()
    prompt = "This is a test"
    input_token_ints = torch.tensor(
        standard_model_context.encode(prompt), device=standard_model_context.device
    ).unsqueeze(0)
    tdb_request = TdbRequestSpec(
        prompt=prompt,
        target_tokens=["."],
        distractor_tokens=["!"],
        component_type_for_mlp=ComponentTypeForMlp.AUTOENCODER_LATENT,
        component_type_for_attention=ComponentTypeForAttention.ATTENTION_HEAD,
        top_and_bottom_k_for_node_table=15,
        hide_early_layers_when_ablating=False,
        node_ablations=None,
        upstream_node_to_trace=None,
        downstream_node_to_trace=None,
    )
    batched_tdb_request = BatchedTdbRequest(sub_requests=[tdb_request])
    interactive_model = InteractiveModel.from_standard_model_context_and_autoencoder_context(
        standard_model_context, standard_autoencoder_context
    )
    batched_tdb_response = await interactive_model.handle_batched_tdb_request(batched_tdb_request)
    top_k_components_response = batched_tdb_response.inference_sub_responses[
        0
    ].processing_response_data_by_name["topKComponents"]
    assert isinstance(top_k_components_response, MultipleTopKDerivedScalarsResponseData)
    top_node_indices = top_k_components_response.node_indices
    top_activations = top_k_components_response.activations_by_group_id[GroupId.ACTIVATION]
    ae_index, ae_activation = next(
        (
            (node_index, activation)
            for node_index, activation in zip(top_node_indices, top_activations)
            if node_index.node_type == NodeType.MLP_AUTOENCODER_LATENT
        ),
        (None, None),
    )  # get the first latent index returned by the TDB request
    assert ae_index is not None
    hooks = TransformerHooks()
    stored_activation = {}
    def saving_hook_fn(act: torch.Tensor, **kwargs: Any) -> torch.Tensor:
        stored_activation["act"] = act[
            (0,) + ae_index.tensor_indices[0:1]
        ]  # 0 batch index, keep token index from ae_index
        return act  # store the activation value for the latent in question
    hooks.append_to_path(
        "mlp.post_act.fwd",
        AtLayers([assert_not_none(ae_index.layer_index)]).append(saving_hook_fn),
    )
    transformer(input_token_ints, hooks=hooks)
    autoencoder = standard_autoencoder_context.get_autoencoder(ae_index.layer_index)
    # the hook should have populated the dict with the latent's activation value
    assert ae_activation is not None
    assert math.isclose(
        autoencoder.encode(stored_activation["act"])[ae_index.tensor_indices[-1]],
        ae_activation,
        rel_tol=1e-4,
    )
async def test_tdb_request_act_times_grad_from_intermediate_node(
    standard_model_context: StandardModelContext,
    standard_autoencoder_context: AutoencoderContext,
) -> None:
    """
    Test that the top MLP autoencoder latent act * grad returned by the BatchedTdbRequest
    matches the activation value stored using a hook on the Transformer forward pass directly.
    This test compares the simplest way to get activations from a Transformer, with the
    way that is used in TDB.
    """
    transformer = standard_model_context.get_or_create_model()
    prompt = "This is a test"
    input_token_ints = torch.tensor(
        standard_model_context.encode(prompt), device=standard_model_context.device
    ).unsqueeze(0)
    node_index_for_bwd = MirroredNodeIndex(
        node_type=NodeType.MLP_NEURON,
        layer_index=5,
        tensor_indices=(1, 0),
        pass_type=PassType.FORWARD,
    )
    tdb_request = TdbRequestSpec(
        prompt=prompt,
        target_tokens=[],
        distractor_tokens=[],
        component_type_for_mlp=ComponentTypeForMlp.NEURON,
        component_type_for_attention=ComponentTypeForAttention.ATTENTION_HEAD,
        top_and_bottom_k_for_node_table=25,
        hide_early_layers_when_ablating=False,
        node_ablations=None,
        upstream_node_to_trace=NodeToTrace(
            node_index=node_index_for_bwd,
            attention_trace_type=None,
            downstream_trace_config=None,
        ),
        downstream_node_to_trace=None,
    )
    batched_tdb_request = BatchedTdbRequest(sub_requests=[tdb_request])
    interactive_model = InteractiveModel.from_standard_model_context_and_autoencoder_context(
        standard_model_context, standard_autoencoder_context
    )
    batched_tdb_response = await interactive_model.handle_batched_tdb_request(batched_tdb_request)
    top_k_components_response = batched_tdb_response.inference_sub_responses[
        0
    ].processing_response_data_by_name["topKComponents"]
    assert isinstance(top_k_components_response, MultipleTopKDerivedScalarsResponseData)
    top_node_indices = top_k_components_response.node_indices
    top_act_times_grads = top_k_components_response.activations_by_group_id[GroupId.ACT_TIMES_GRAD]
    top_acts = top_k_components_response.activations_by_group_id[GroupId.ACTIVATION]
    node_index, tdb_act, tdb_act_times_grad = next(
        (
            (top_node_index, top_act, top_act_times_grad)
            for top_node_index, top_act, top_act_times_grad in zip(
                top_node_indices, top_acts, top_act_times_grads
            )
            if top_node_index.node_type == NodeType.MLP_NEURON
        ),
        (None, None, None),
    )  # get the first latent index returned by the TDB request
    assert node_index is not None
    hooks = TransformerHooks()
    stored_act = {}
    stored_grad = {}
    stored_act_for_bwd = {}
    def act_saving_hook_fn(act: torch.Tensor, **kwargs: Any) -> torch.Tensor:
        stored_act["value"] = act[
            (0,) + node_index.tensor_indices
        ]  # 0 batch index, keep token index from node_index
        return act  # store the activation value for the latent in question
    def grad_saving_hook_fn(grad: torch.Tensor, **kwargs: Any) -> torch.Tensor:
        stored_grad["value"] = grad[(0,) + node_index.tensor_indices]
        return grad
    def act_grabbing_hook_fn(act: torch.Tensor, **kwargs: Any) -> torch.Tensor:
        stored_act_for_bwd["value"] = act[(0,) + node_index_for_bwd.tensor_indices]
        return act
    def detach_fn(act: torch.Tensor, **kwargs: Any) -> torch.Tensor:
        return act.detach()
    hooks = (
        hooks.append_to_path(
            "mlp.post_act.fwd",
            AtLayers([assert_not_none(node_index.layer_index)]).append(act_saving_hook_fn),
        )
        .append_to_path(
            "mlp.post_act.bwd",
            AtLayers([assert_not_none(node_index.layer_index)]).append(grad_saving_hook_fn),
        )
        .append_to_path(
            "resid.torso.ln_mlp.scale.fwd",
            AtLayers([assert_not_none(node_index_for_bwd.layer_index)]).append(detach_fn),
        )  # detach layer norm scale
        .append_to_path(
            "mlp.pre_act.fwd",
            AtLayers([assert_not_none(node_index_for_bwd.layer_index)]).append(
                act_grabbing_hook_fn
            ),
        )
    )
    transformer(input_token_ints, hooks=hooks)
    stored_act_for_bwd["value"].backward()
    assert isinstance(tdb_act, float)
    assert math.isclose(
        tdb_act,
        stored_act["value"].item(),
        rel_tol=1e-4,
    )
    assert isinstance(tdb_act_times_grad, float)
    assert math.isclose(
        tdb_act_times_grad,
        (stored_act["value"] * stored_grad["value"]).item(),
        rel_tol=1e-4,
    )
async def test_tdb_request_act_times_grad_from_intermediate_autoencoder_node(
    standard_model_context: StandardModelContext,
    standard_autoencoder_context: AutoencoderContext,
) -> None:
    """
    Test that the top attention act * grad returned by the BatchedTdbRequest when tracing
    from an intermediate autoencoder latent matches the activation value stored using hooks
    on the Transformer forward pass directly.
    This test compares the simplest way to get activations from a Transformer, with the
    way that is used in TDB.
    """
    transformer = standard_model_context.get_or_create_model()
    prompt = "This is a test"
    input_token_ints = torch.tensor(
        standard_model_context.encode(prompt), device=standard_model_context.device
    ).unsqueeze(0)
    node_index_for_bwd = MirroredNodeIndex(
        node_type=NodeType.AUTOENCODER_LATENT,
        layer_index=5,
        tensor_indices=(1, 0),
        pass_type=PassType.FORWARD,
    )
    tdb_request = TdbRequestSpec(
        prompt=prompt,
        target_tokens=[],
        distractor_tokens=[],
        component_type_for_mlp=ComponentTypeForMlp.AUTOENCODER_LATENT,
        component_type_for_attention=ComponentTypeForAttention.ATTENTION_HEAD,
        top_and_bottom_k_for_node_table=25,
        hide_early_layers_when_ablating=False,
        node_ablations=None,
        upstream_node_to_trace=NodeToTrace(
            node_index=node_index_for_bwd,
            attention_trace_type=None,
            downstream_trace_config=None,
        ),
        downstream_node_to_trace=None,
    )
    batched_tdb_request = BatchedTdbRequest(sub_requests=[tdb_request])
    interactive_model = InteractiveModel.from_standard_model_context_and_autoencoder_context(
        standard_model_context, standard_autoencoder_context
    )
    batched_tdb_response = await interactive_model.handle_batched_tdb_request(batched_tdb_request)
    top_k_components_response = batched_tdb_response.inference_sub_responses[
        0
    ].processing_response_data_by_name["topKComponents"]
    assert isinstance(top_k_components_response, MultipleTopKDerivedScalarsResponseData)
    top_node_indices = top_k_components_response.node_indices
    top_act_times_grads = top_k_components_response.activations_by_group_id[GroupId.ACT_TIMES_GRAD]
    top_acts = top_k_components_response.activations_by_group_id[GroupId.ACTIVATION]
    node_index, tdb_act, tdb_act_times_grad = next(
        (
            (top_node_index, top_act, top_act_times_grad)
            for top_node_index, top_act, top_act_times_grad in zip(
                top_node_indices, top_acts, top_act_times_grads
            )
            if top_node_index.node_type == NodeType.ATTENTION_HEAD and top_act_times_grad > 0.0
        ),
        (None, None, None),
    )  # get the first latent index returned by the TDB request
    assert node_index is not None
    hooks = TransformerHooks()
    stored_act = {}
    stored_grad = {}
    stored_act_for_bwd = {}
    def act_saving_hook_fn(act: torch.Tensor, **kwargs: Any) -> torch.Tensor:
        stored_act["value"] = act[
            (0,) + node_index.tensor_indices
        ]  # 0 batch index, keep token index from node_index
        return act  # store the activation value for the latent in question
    def grad_saving_hook_fn(grad: torch.Tensor, **kwargs: Any) -> torch.Tensor:
        stored_grad["value"] = grad[(0,) + node_index.tensor_indices]
        return grad
    autoencoder = standard_autoencoder_context.get_autoencoder(node_index_for_bwd.layer_index)
    def act_grabbing_hook_fn(act: torch.Tensor, **kwargs: Any) -> torch.Tensor:
        # apply autoencoder
        token_index_for_bwd, activation_index_for_bwd = node_index_for_bwd.tensor_indices
        assert isinstance(token_index_for_bwd, int)
        assert isinstance(activation_index_for_bwd, int)
        stored_act_for_bwd["value"] = autoencoder.encode_pre_act(
            act[0, token_index_for_bwd : token_index_for_bwd + 1]
        )[0, activation_index_for_bwd]
        return act
    def detach_fn(act: torch.Tensor, **kwargs: Any) -> torch.Tensor:
        return act.detach()
    hooks = (
        hooks.append_to_path(
            "attn.qk_probs.fwd",
            AtLayers([assert_not_none(node_index.layer_index)]).append(act_saving_hook_fn),
        )
        .append_to_path(
            "attn.qk_probs.bwd",
            AtLayers([assert_not_none(node_index.layer_index)]).append(grad_saving_hook_fn),
        )
        .append_to_path(
            "resid.torso.ln_mlp.scale.fwd",
            AtLayers([assert_not_none(node_index_for_bwd.layer_index)]).append(detach_fn),
        )  # detach layer norm scale
        .append_to_path(
            "mlp.post_act.fwd",  # autoencoder is applied to mlp.post_act
            AtLayers([assert_not_none(node_index_for_bwd.layer_index)]).append(
                act_grabbing_hook_fn
            ),
        )
    )
    transformer(input_token_ints, hooks=hooks)
    stored_act_for_bwd["value"].backward()
    assert isinstance(tdb_act, float)
    assert math.isclose(
        tdb_act,
        stored_act["value"].item(),
        rel_tol=1e-4,
    )
    assert isinstance(tdb_act_times_grad, float)
    assert math.isclose(
        tdb_act_times_grad,
        (stored_act["value"] * stored_grad["value"]).item(),
        rel_tol=1e-4,
    )
async def test_tdb_request_act_times_grad_from_intermediate_autoencoder_node_trace_through_k(
    standard_model_context: StandardModelContext,
    standard_autoencoder_context: AutoencoderContext,
) -> None:
    """
    Test that the top attention act * grad returned by the BatchedTdbRequest when tracing
    from an intermediate attention node through K matches the activation value stored using hooks
    on the Transformer forward pass directly.
    This test compares the simplest way to get activations from a Transformer, with the
    way that is used in TDB.
    """
    transformer = standard_model_context.get_or_create_model()
    prompt = "This is a test"
    input_token_ints = torch.tensor(
        standard_model_context.encode(prompt), device=standard_model_context.device
    ).unsqueeze(0)
    node_index_for_bwd = MirroredNodeIndex(
        node_type=NodeType.ATTENTION_HEAD,
        layer_index=5,
        tensor_indices=(1, 0, 0),
        pass_type=PassType.FORWARD,
    )
    tdb_request = TdbRequestSpec(
        prompt=prompt,
        target_tokens=[],
        distractor_tokens=[],
        component_type_for_mlp=ComponentTypeForMlp.AUTOENCODER_LATENT,
        component_type_for_attention=ComponentTypeForAttention.ATTENTION_HEAD,
        top_and_bottom_k_for_node_table=25,
        hide_early_layers_when_ablating=False,
        node_ablations=None,
        upstream_node_to_trace=NodeToTrace(
            node_index=node_index_for_bwd,
            attention_trace_type=AttentionTraceType.K,
            downstream_trace_config=None,
        ),
        downstream_node_to_trace=None,
    )
    batched_tdb_request = BatchedTdbRequest(sub_requests=[tdb_request])
    interactive_model = InteractiveModel.from_standard_model_context_and_autoencoder_context(
        standard_model_context, standard_autoencoder_context
    )
    batched_tdb_response = await interactive_model.handle_batched_tdb_request(batched_tdb_request)
    top_k_components_response = batched_tdb_response.inference_sub_responses[
        0
    ].processing_response_data_by_name["topKComponents"]
    assert isinstance(top_k_components_response, MultipleTopKDerivedScalarsResponseData)
    top_node_indices = top_k_components_response.node_indices
    top_act_times_grads = top_k_components_response.activations_by_group_id[GroupId.ACT_TIMES_GRAD]
    top_acts = top_k_components_response.activations_by_group_id[GroupId.ACTIVATION]
    node_index, tdb_act, tdb_act_times_grad = next(
        (
            (top_node_index, top_act, top_act_times_grad)
            for top_node_index, top_act, top_act_times_grad in zip(
                top_node_indices, top_acts, top_act_times_grads
            )
            if top_node_index.node_type == NodeType.ATTENTION_HEAD and top_act_times_grad > 0.0
        ),
        (None, None, None),
    )  # get the first latent index returned by the TDB request
    assert node_index is not None
    hooks = TransformerHooks()
    stored_act = {}
    stored_grad = {}
    stored_act_for_bwd = {}
    def act_saving_hook_fn(act: torch.Tensor, **kwargs: Any) -> torch.Tensor:
        stored_act["value"] = act[
            (0,) + node_index.tensor_indices
        ]  # 0 batch index, keep token index from node_index
        return act  # store the activation value for the latent in question
    def grad_saving_hook_fn(grad: torch.Tensor, **kwargs: Any) -> torch.Tensor:
        stored_grad["value"] = grad[(0,) + node_index.tensor_indices]
        return grad
    def act_grabbing_hook_fn(act: torch.Tensor, **kwargs: Any) -> torch.Tensor:
        # apply autoencoder
        stored_act_for_bwd["value"] = act[(0,) + node_index_for_bwd.tensor_indices]
        return act
    def detach_fn(act: torch.Tensor, **kwargs: Any) -> torch.Tensor:
        return act.detach()
    hooks = (
        hooks.append_to_path(
            "attn.qk_probs.fwd",
            AtLayers([assert_not_none(node_index.layer_index)]).append(act_saving_hook_fn),
        )
        .append_to_path(
            "attn.qk_probs.bwd",
            AtLayers([assert_not_none(node_index.layer_index)]).append(grad_saving_hook_fn),
        )
        .append_to_path(
            "resid.torso.ln_attn.scale.fwd",
            AtLayers([assert_not_none(node_index_for_bwd.layer_index)]).append(detach_fn),
        )  # detach layer norm scale
        .append_to_path(
            "attn.q.fwd",
            AtLayers([assert_not_none(node_index_for_bwd.layer_index)]).append(detach_fn),
        )  # detach layer norm scale
        .append_to_path(
            "attn.qk_logits.fwd",
            AtLayers([assert_not_none(node_index_for_bwd.layer_index)]).append(
                act_grabbing_hook_fn
            ),
        )
    )
    transformer(input_token_ints, hooks=hooks)
    stored_act_for_bwd["value"].backward()
    assert isinstance(tdb_act, float)
    assert math.isclose(
        tdb_act,
        stored_act["value"].item(),
        rel_tol=1e-4,
    )
    assert isinstance(tdb_act_times_grad, float)
    assert math.isclose(
        tdb_act_times_grad,
        (stored_act["value"] * stored_grad["value"]).item(),
        rel_tol=1e-4,
    )
T = TypeVar("T")
def assert_not_none(value: T | None) -> T:
    assert value is not None
    return value

================
File: neuron_explainer/tests/test_model_context_get_weight.py
================
import time
from typing import Any, Callable
import torch
from neuron_explainer.models.inference_engine_type_registry import InferenceEngineType
from neuron_explainer.models.model_component_registry import WeightLocationType
from neuron_explainer.models.model_context import ModelContext
def assert_all_eq(
    lst: list[Any],
    eq_fn: Callable[[Any, Any], bool] = lambda x, y: x == y,
    weight_location_type: WeightLocationType | None = None,
) -> Any:
    for i in range(1, len(lst)):
        assert eq_fn(lst[i], lst[0]), f"{lst[i]} != {lst[0]}; {weight_location_type=}; {i=}"
    return lst[0]
def test_model_context_weights() -> None:
    for model_name in ["gpt2-small"]:
        contexts = []
        standard_model_context = ModelContext.from_model_type(
            model_name,
            inference_engine_type=InferenceEngineType.STANDARD,
            device="cpu",
        )
        contexts.append(("standard", standard_model_context))
        standard_model_context_with_model = ModelContext.from_model_type(
            model_name,
            inference_engine_type=InferenceEngineType.STANDARD,
            device="cpu",
        )
        standard_model_context_with_model.get_or_create_model(simplify=False)  # type: ignore
        contexts.append(("standard_cached", standard_model_context_with_model))
        for weight_location_type in WeightLocationType:
            if not weight_location_type.has_no_layers:
                # just test layer 0 for now
                layer_index: int | None = 0
            else:
                layer_index = None
            weights = []
            for ctx_name, ctx in contexts:
                try:
                    t = time.time()
                    # Convert all weights to float32, since different contexts may use different
                    # dtypes by default. torch.allclose requires dtypes to match.
                    weight = ctx.get_weight(weight_location_type, layer_index).to(torch.float32)
                    print(f"{ctx_name} {weight.shape=} loaded in {time.time() - t:.2f}s")
                    weights.append(weight)
                except NotImplementedError:
                    print(f"{weight_location_type} not implemented in {ctx_name} context")
            if len(weights):
                assert_all_eq(
                    [weight.shape for weight in weights], lambda x, y: x == y, weight_location_type
                )
                assert_all_eq(
                    list(weights),
                    lambda x, y: torch.allclose(x, y, atol=1e-5, rtol=1e-3),
                    weight_location_type,
                )
            else:
                print(f"no weights found for {weight_location_type}")

================
File: neuron_explainer/tests/test_offline_autoencoder_dsts.py
================
import pytest
import torch
from neuron_explainer.activation_server.derived_scalar_computation import (
    get_derived_scalars_for_prompt,
)
from neuron_explainer.activations.derived_scalars import DerivedScalarType
from neuron_explainer.activations.derived_scalars.indexing import (
    ActivationIndex,
    DerivedScalarIndex,
    TraceConfig,
)
from neuron_explainer.activations.derived_scalars.scalar_deriver import DstConfig
from neuron_explainer.activations.derived_scalars.tests.utils import (
    get_activation_shape,
    get_autoencoder_test_path,
)
from neuron_explainer.models.autoencoder_context import AutoencoderConfig, AutoencoderContext
from neuron_explainer.models.model_component_registry import ActivationLocationType, PassType
from neuron_explainer.models.model_context import StandardModelContext
gradient_dst_by_input_dst = {
    DerivedScalarType.MLP_POST_ACT: DerivedScalarType.AUTOENCODER_LATENT_GRAD_WRT_MLP_POST_ACT_INPUT,
    DerivedScalarType.RESID_DELTA_MLP: DerivedScalarType.AUTOENCODER_LATENT_GRAD_WRT_RESIDUAL_INPUT,
    DerivedScalarType.RESID_DELTA_ATTN: DerivedScalarType.AUTOENCODER_LATENT_GRAD_WRT_RESIDUAL_INPUT,
}
tested_dsts = [
    DerivedScalarType.ATTN_WRITE_TO_LATENT,
    DerivedScalarType.ATTN_WRITE_TO_LATENT_SUMMED_OVER_HEADS,
    DerivedScalarType.ATTN_WRITE_TO_LATENT_PER_SEQUENCE_TOKEN,
]
@pytest.mark.parametrize("tested_dst", tested_dsts)
def test_backprop_to_attn_smoke(
    standard_model_context: StandardModelContext,
    tested_dst: DerivedScalarType,
) -> None:
    autoencoder_input_dst = DerivedScalarType.RESID_DELTA_ATTN
    latent_index_to_check = 3
    autoencoder_path = get_autoencoder_test_path(autoencoder_input_dst)
    prompt = "This is a test"
    n_tokens = len(standard_model_context.encode(prompt))
    layer_index = 5
    token_index_to_check = 1
    autoencoder_config = AutoencoderConfig(
        dst=autoencoder_input_dst,
        autoencoder_path_by_layer_index={
            layer_index: autoencoder_path for layer_index in range(standard_model_context.n_layers)
        },
    )
    autoencoder_context = AutoencoderContext(
        autoencoder_config=autoencoder_config, device=standard_model_context.device
    )
    # define which DST to derive
    dst_config_by_dst = {
        tested_dst: DstConfig(
            model_context=standard_model_context,
            autoencoder_config=autoencoder_config,
            trace_config=TraceConfig.from_activation_index(
                ActivationIndex(
                    activation_location_type=ActivationLocationType.ONLINE_AUTOENCODER_LATENT,
                    layer_index=layer_index,
                    tensor_indices=("All", latent_index_to_check),
                    pass_type=PassType.FORWARD,
                )
            ),
            layer_indices=[layer_index],
        ),
    }
    dst_and_config_list: list[tuple[DerivedScalarType, DstConfig]] = list(dst_config_by_dst.items())
    # run the model and compute the requested derived scalars
    ds_store, _, raw_store = get_derived_scalars_for_prompt(
        model_context=standard_model_context,
        prompt=prompt,
        trace_config=TraceConfig.from_activation_index(
            ActivationIndex(
                activation_location_type=ActivationLocationType.ONLINE_AUTOENCODER_LATENT,
                layer_index=layer_index,
                tensor_indices=(token_index_to_check, latent_index_to_check),
                pass_type=PassType.FORWARD,
            )
        ),
        dst_and_config_list=dst_and_config_list,  # type: ignore
        autoencoder_context=autoencoder_context,
    )
    # check that the derived scalar is not all zeros, and has the correct shape
    ds_index = DerivedScalarIndex(
        dst=tested_dst,
        pass_type=PassType.FORWARD,
        tensor_indices=(token_index_to_check,),
        layer_index=layer_index,
    )
    # The .to("cpu") shouldn't be necessary, but there's a strange Python crash we hit if we run on
    # a MacBook, which uses the "mps" device. This is a workaround.
    derived_scalar = ds_store[ds_index].to("cpu")
    assert (
        derived_scalar.shape
        == get_activation_shape(tested_dst, standard_model_context, n_tokens)[1:]
    )
    assert not torch.allclose(derived_scalar, torch.zeros_like(derived_scalar))
tested_dsts_batched = [
    DerivedScalarType.FLATTENED_ATTN_WRITE_TO_LATENT_SUMMED_OVER_HEADS_BATCHED,
    DerivedScalarType.ATTN_WRITE_TO_LATENT_PER_SEQUENCE_TOKEN_BATCHED,
]
@pytest.mark.parametrize("dst_batched", tested_dsts_batched)
def test_attn_write_to_latent_dsts_batched(
    standard_model_context: StandardModelContext,
    dst_batched: DerivedScalarType,
) -> None:
    autoencoder_input_dst = DerivedScalarType.RESID_DELTA_ATTN
    latent_index_to_check = 3
    autoencoder_path = get_autoencoder_test_path(autoencoder_input_dst)
    prompt = "This is a test"
    dst_not_batched = {
        DerivedScalarType.FLATTENED_ATTN_WRITE_TO_LATENT_SUMMED_OVER_HEADS_BATCHED: DerivedScalarType.FLATTENED_ATTN_WRITE_TO_LATENT_SUMMED_OVER_HEADS,
        DerivedScalarType.ATTN_WRITE_TO_LATENT_PER_SEQUENCE_TOKEN_BATCHED: DerivedScalarType.ATTN_WRITE_TO_LATENT_PER_SEQUENCE_TOKEN,
    }[dst_batched]
    layer_index = 5
    token_index_to_check = 1
    autoencoder_config = AutoencoderConfig(
        dst=autoencoder_input_dst,
        autoencoder_path_by_layer_index={
            layer_index: autoencoder_path for layer_index in range(standard_model_context.n_layers)
        },
    )
    autoencoder_context = AutoencoderContext(
        autoencoder_config=autoencoder_config, device=standard_model_context.device
    )
    autoencoder_context.warmup()
    # define which DST to derive
    dst_config_by_dst = {
        dst_batched: DstConfig(
            model_context=standard_model_context,
            autoencoder_config=autoencoder_config,
            layer_indices=[layer_index],
        ),
        dst_not_batched: DstConfig(
            model_context=standard_model_context,
            autoencoder_config=autoencoder_config,
            trace_config=TraceConfig.from_activation_index(
                ActivationIndex(
                    activation_location_type=ActivationLocationType.ONLINE_AUTOENCODER_LATENT,
                    layer_index=layer_index,
                    pass_type=PassType.FORWARD,
                    tensor_indices=("All", latent_index_to_check),
                )
            ),
            layer_indices=[layer_index],
        ),
    }
    dst_and_config_list: list[tuple[DerivedScalarType, DstConfig]] = list(dst_config_by_dst.items())
    # run the model and compute the requested derived scalars
    ds_store, _, raw_store = get_derived_scalars_for_prompt(
        model_context=standard_model_context,
        prompt=prompt,
        loss_fn_for_backward_pass=None,
        dst_and_config_list=dst_and_config_list,  # type: ignore
        autoencoder_context=autoencoder_context,
    )
    # compare the two outputs
    ds_index_batched = DerivedScalarIndex(
        dst=dst_batched,
        pass_type=PassType.FORWARD,
        tensor_indices=(token_index_to_check,),
        layer_index=layer_index,
    )
    ds_index_not_batched = DerivedScalarIndex(
        dst=dst_not_batched,
        pass_type=PassType.FORWARD,
        tensor_indices=(token_index_to_check,),
        layer_index=layer_index,
    )
    derived_scalar_batched = ds_store[ds_index_batched]
    derived_scalar_not_batched = ds_store[ds_index_not_batched]
    autoencoder_context.warmup()
    assert derived_scalar_batched.shape[-1] == autoencoder_context.num_autoencoder_directions
    assert torch.allclose(
        derived_scalar_batched[..., latent_index_to_check], derived_scalar_not_batched
    )

================
File: neuron_explainer/tests/test_online_autoencoder_dsts.py
================
from typing import Callable
import torch
from neuron_explainer.activation_server.derived_scalar_computation import (
    DerivedScalarComputationParams,
    compute_derived_scalar_groups_for_input_token_ints,
)
from neuron_explainer.activation_server.requests_and_responses import InferenceData
from neuron_explainer.activations.derived_scalars import DerivedScalarType
from neuron_explainer.activations.derived_scalars.derived_scalar_store import DerivedScalarStore
from neuron_explainer.activations.derived_scalars.indexing import AblationSpec, TraceConfig
from neuron_explainer.activations.derived_scalars.make_scalar_derivers import make_scalar_deriver
from neuron_explainer.activations.derived_scalars.multi_group import (
    MultiGroupDerivedScalarStore,
    MultiGroupScalarDerivers,
)
from neuron_explainer.activations.derived_scalars.scalar_deriver import (
    ActivationsAndMetadata,
    DstConfig,
    ScalarDeriver,
)
from neuron_explainer.models.autoencoder_context import AutoencoderContext, MultiAutoencoderContext
from neuron_explainer.models.model_component_registry import PassType
from neuron_explainer.models.model_context import StandardModelContext
from neuron_explainer.tests.conftest import AUTOENCODER_TEST_DST
def validate_activations_and_metadata(
    activations_and_metadata: ActivationsAndMetadata,
    dst: DerivedScalarType,
    pass_type: PassType,
    input_token_ints: list[list[int]],
) -> None:
    assert activations_and_metadata.dst == dst
    assert activations_and_metadata.pass_type == pass_type
    for layer_index in activations_and_metadata.activations_by_layer_index.keys():
        assert activations_and_metadata.activations_by_layer_index[layer_index].shape[0] == len(
            input_token_ints[0]
        )
def compute_derived_scalars_for_input_token_ints(
    model_context: StandardModelContext,
    input_token_ints: list[list[int]],
    scalar_derivers: list[ScalarDeriver],
    loss_fn_for_backward_pass: Callable[[torch.Tensor], torch.Tensor] | None,
    ablation_specs: list[AblationSpec] | None,
    autoencoder_context: MultiAutoencoderContext | AutoencoderContext | None,
    trace_config: TraceConfig | None,
) -> tuple[DerivedScalarStore, InferenceData]:
    """This function runs a forward pass on the given input tokens, with hooks added to the transformer to
    extract the activations needed to compute the scalars in scalar_derivers. It then returns a
    DerivedScalarStore containing the derived scalars for each token in the input."""
    multi_autoencoder_context = MultiAutoencoderContext.from_context_or_multi_context(
        autoencoder_context
    )
    multi_group_scalar_derivers_by_processing_step = {
        # "dummy" is a placeholder processing step name
        "dummy": MultiGroupScalarDerivers.from_scalar_derivers(scalar_derivers)
    }
    assert len(input_token_ints) == 1
    batched_ds_computation_params = [
        DerivedScalarComputationParams(
            input_token_ints=input_token_ints[0],
            multi_group_scalar_derivers_by_processing_step=multi_group_scalar_derivers_by_processing_step,
            device_for_raw_activations=model_context.device,
            loss_fn_for_backward_pass=loss_fn_for_backward_pass,
            ablation_specs=ablation_specs,
            trace_config=trace_config,
        )
    ]
    batched_multi_group_ds_store_by_processing_step: list[dict[str, MultiGroupDerivedScalarStore]]
    (
        batched_multi_group_ds_store_by_processing_step,
        batched_inference_data,
        _,
    ) = compute_derived_scalar_groups_for_input_token_ints(
        model_context=model_context,
        multi_autoencoder_context=multi_autoencoder_context,
        batched_ds_computation_params=batched_ds_computation_params,
    )
    return (
        batched_multi_group_ds_store_by_processing_step[0]["dummy"].to_single_ds_store(),
        batched_inference_data[0],
    )
def test_online_autoencoder_latent(
    standard_model_context: StandardModelContext,
    standard_autoencoder_context: AutoencoderContext,
) -> None:
    # SANITY CHECK ONLINE AUTOENCODER LATENT DST OUTPUTS
    autoencoder_config = standard_autoencoder_context.autoencoder_config
    dst_config = DstConfig(
        model_context=standard_model_context,
        autoencoder_config=autoencoder_config,
    )
    online_latent_scalar_deriver = make_scalar_deriver(
        DerivedScalarType.ONLINE_AUTOENCODER_LATENT, dst_config
    )
    mlp_layer_index = 5
    dst_config = DstConfig(
        model_context=standard_model_context,
    )
    mlp_scalar_deriver = make_scalar_deriver(
        AUTOENCODER_TEST_DST,
        dst_config,
    )
    scalar_derivers = [online_latent_scalar_deriver, mlp_scalar_deriver]
    input_token_ints = [[1, 2, 3, 4, 5]]
    ds_store_with_autoencoder, loss = compute_derived_scalars_for_input_token_ints(
        standard_model_context,
        input_token_ints,
        scalar_derivers,
        loss_fn_for_backward_pass=None,
        ablation_specs=None,
        autoencoder_context=standard_autoencoder_context,
        trace_config=None,
    )
    activations_and_metadata_by_dst_and_pass_type = (
        ds_store_with_autoencoder.activations_and_metadata_by_dst_and_pass_type
    )
    assert len(activations_and_metadata_by_dst_and_pass_type) == 2
    dst_and_pass_type = (DerivedScalarType.ONLINE_AUTOENCODER_LATENT, PassType.FORWARD)
    activations_and_metadata = activations_and_metadata_by_dst_and_pass_type[dst_and_pass_type]
    validate_activations_and_metadata(
        activations_and_metadata,
        DerivedScalarType.ONLINE_AUTOENCODER_LATENT,
        PassType.FORWARD,
        input_token_ints,
    )
    dst_and_pass_type = (AUTOENCODER_TEST_DST, PassType.FORWARD)
    mlp_activations_and_metadata_with_autoencoder = activations_and_metadata_by_dst_and_pass_type[
        dst_and_pass_type
    ]
    validate_activations_and_metadata(
        mlp_activations_and_metadata_with_autoencoder,
        AUTOENCODER_TEST_DST,
        PassType.FORWARD,
        input_token_ints,
    )
    # CHECK ONLINE AUTOENCODER HOOK DOES NOT AFFECT DOWNSTREAM ACTIVATIONS, BY COMPARING TO CASE WHERE HOOK
    # IS NOT INCLUDED
    scalar_derivers = [mlp_scalar_deriver]
    input_token_ints = [[1, 2, 3, 4, 5]]
    (
        ds_store_without_autoencoder,
        loss,
    ) = compute_derived_scalars_for_input_token_ints(
        standard_model_context,
        input_token_ints,
        scalar_derivers,
        loss_fn_for_backward_pass=None,
        ablation_specs=None,
        autoencoder_context=None,
        trace_config=None,
    )
    activations_and_metadata_by_dst_and_pass_type_without_autoencoder = (
        ds_store_without_autoencoder.activations_and_metadata_by_dst_and_pass_type
    )
    dst_and_pass_type = (AUTOENCODER_TEST_DST, PassType.FORWARD)
    mlp_activations_and_metadata_without_autoencoder = (
        activations_and_metadata_by_dst_and_pass_type_without_autoencoder[dst_and_pass_type]
    )
    validate_activations_and_metadata(
        mlp_activations_and_metadata_without_autoencoder,
        AUTOENCODER_TEST_DST,
        PassType.FORWARD,
        input_token_ints,
    )
    assert (
        mlp_activations_and_metadata_with_autoencoder.activations_by_layer_index[
            mlp_layer_index
        ].shape
        == mlp_activations_and_metadata_without_autoencoder.activations_by_layer_index[
            mlp_layer_index
        ].shape
    )
    assert torch.allclose(
        mlp_activations_and_metadata_with_autoencoder.activations_by_layer_index[mlp_layer_index],
        mlp_activations_and_metadata_without_autoencoder.activations_by_layer_index[
            mlp_layer_index
        ],
        rtol=1e-4,
        atol=1e-4,
    ), (
        mlp_activations_and_metadata_with_autoencoder.activations_by_layer_index[
            mlp_layer_index
        ].flatten()[:10],
        mlp_activations_and_metadata_without_autoencoder.activations_by_layer_index[
            mlp_layer_index
        ].flatten()[:10],
    )
def test_online_equal_to_offline(
    standard_model_context: StandardModelContext,
    standard_autoencoder_context: AutoencoderContext,
) -> None:
    # CHECK THAT ONLINE AND OFFLINE AUTOENCODER LATENT DSTS RESULT IN THE SAME ACTIVATIONS
    dst_config = DstConfig(
        model_context=standard_model_context,
        autoencoder_config=standard_autoencoder_context.autoencoder_config,
    )
    online_latent_scalar_deriver = make_scalar_deriver(
        DerivedScalarType.ONLINE_AUTOENCODER_LATENT, dst_config
    )
    offline_latent_scalar_deriver = make_scalar_deriver(
        DerivedScalarType.AUTOENCODER_LATENT, dst_config
    )
    scalar_derivers = [online_latent_scalar_deriver, offline_latent_scalar_deriver]
    input_token_ints = [[1, 2, 3, 4, 5]]
    ds_store, loss = compute_derived_scalars_for_input_token_ints(
        standard_model_context,
        input_token_ints,
        scalar_derivers,
        loss_fn_for_backward_pass=None,
        ablation_specs=None,
        autoencoder_context=standard_autoencoder_context,
        trace_config=None,
    )
    activations_and_metadata_by_dst_and_pass_type = (
        ds_store.activations_and_metadata_by_dst_and_pass_type
    )
    assert len(activations_and_metadata_by_dst_and_pass_type) == 2
    dst_and_pass_type = (DerivedScalarType.ONLINE_AUTOENCODER_LATENT, PassType.FORWARD)
    online_activations_and_metadata = activations_and_metadata_by_dst_and_pass_type[
        dst_and_pass_type
    ]
    validate_activations_and_metadata(
        online_activations_and_metadata,
        DerivedScalarType.ONLINE_AUTOENCODER_LATENT,
        PassType.FORWARD,
        input_token_ints,
    )
    dst_and_pass_type = (DerivedScalarType.AUTOENCODER_LATENT, PassType.FORWARD)
    offline_activations_and_metadata = activations_and_metadata_by_dst_and_pass_type[
        dst_and_pass_type
    ]
    validate_activations_and_metadata(
        offline_activations_and_metadata,
        DerivedScalarType.AUTOENCODER_LATENT,
        PassType.FORWARD,
        input_token_ints,
    )
    # check that online and offline activations are the same
    for layer_index in range(standard_model_context.n_layers):
        assert torch.allclose(
            online_activations_and_metadata.activations_by_layer_index[layer_index],
            offline_activations_and_metadata.activations_by_layer_index[layer_index],
        )
def test_autoencoder_dsts_together(
    standard_model_context: StandardModelContext,
    standard_autoencoder_context: AutoencoderContext,
) -> None:
    # test that autoencoder act * grad works together with other autoencoder dsts
    dst_config = DstConfig(
        model_context=standard_model_context,
        autoencoder_config=standard_autoencoder_context.autoencoder_config,
    )
    dsts = [
        DerivedScalarType.ONLINE_AUTOENCODER_LATENT,
        DerivedScalarType.ONLINE_AUTOENCODER_WRITE_NORM,
        DerivedScalarType.ONLINE_AUTOENCODER_ACT_TIMES_GRAD,
        DerivedScalarType.ONLINE_AUTOENCODER_WRITE_TO_FINAL_RESIDUAL_GRAD,
        DerivedScalarType.ONLINE_MLP_AUTOENCODER_ERROR,
        DerivedScalarType.ONLINE_MLP_AUTOENCODER_ERROR_ACT_TIMES_GRAD,
        DerivedScalarType.ONLINE_MLP_AUTOENCODER_ERROR_WRITE_TO_FINAL_RESIDUAL_GRAD,
    ]
    scalar_derivers = [make_scalar_deriver(dst, dst_config) for dst in dsts]
    def loss_fn_for_backward_pass(output_logits: torch.Tensor) -> torch.Tensor:
        assert output_logits.ndim == 3
        nbatch, ntoken, nlogit = output_logits.shape
        assert nbatch == 1
        target_vocab_token_ints = [0]
        anti_target_vocab_token_ints = [1]
        return (
            output_logits[:, -1, target_vocab_token_ints].mean(-1)
            - output_logits[:, -1, anti_target_vocab_token_ints].mean(-1)
        ).mean()  # difference between average logits for target and anti-target tokens
    input_token_ints = [[1, 2, 3, 4, 5]]
    ds_store, loss = compute_derived_scalars_for_input_token_ints(
        standard_model_context,
        input_token_ints,
        scalar_derivers,
        loss_fn_for_backward_pass=loss_fn_for_backward_pass,
        ablation_specs=None,
        autoencoder_context=standard_autoencoder_context,
        trace_config=None,
    )

================
File: neuron_explainer/tests/test_postprocessing.py
================
import torch
from neuron_explainer.activation_server.derived_scalar_computation import (
    get_derived_scalars_for_prompt,
)
from neuron_explainer.activations.derived_scalars.indexing import DerivedScalarIndex, NodeIndex
from neuron_explainer.activations.derived_scalars.postprocessing import (
    TokenReadConverter,
    TokenWriteConverter,
)
from neuron_explainer.activations.derived_scalars.scalar_deriver import DerivedScalarType, DstConfig
from neuron_explainer.models.autoencoder_context import AutoencoderContext
from neuron_explainer.models.model_component_registry import Dimension, NodeType, PassType
from neuron_explainer.models.model_context import StandardModelContext
AUTOENCODER_TEST_DST = DerivedScalarType.MLP_POST_ACT
def test_read_and_write(
    standard_model_context: StandardModelContext,
    standard_autoencoder_context: AutoencoderContext,
) -> None:
    # test token-space read and write postprocessing, for defined MLP and autoencoder latents
    prompt = "This is a test"
    dst_config = DstConfig(
        model_context=standard_model_context,
        autoencoder_context=standard_autoencoder_context,
    )
    starting_dst_and_config_list = [
        (DerivedScalarType.MLP_POST_ACT, dst_config),
        (DerivedScalarType.ONLINE_AUTOENCODER_LATENT, dst_config),
    ]
    postprocessor_by_kind = {
        "write": TokenWriteConverter(
            model_context=standard_model_context,
            multi_autoencoder_context=standard_autoencoder_context,
        ),
        "read": TokenReadConverter(
            model_context=standard_model_context,
            multi_autoencoder_context=standard_autoencoder_context,
        ),
    }
    post_dst_and_config_list_by_kind = {
        kind: postprocessor_by_kind[kind].get_input_dst_and_config_list(
            starting_dst_and_config_list
        )
        for kind in postprocessor_by_kind.keys()
    }
    total_post_dst_and_config_list = []
    for kind in post_dst_and_config_list_by_kind.keys():
        total_post_dst_and_config_list.extend(post_dst_and_config_list_by_kind[kind])
    unique_dst_and_config_list = []
    seen_dsts = set()
    for dst_and_config in total_post_dst_and_config_list:
        dst, _ = dst_and_config
        if dst not in seen_dsts:
            unique_dst_and_config_list.append(dst_and_config)
            seen_dsts.add(dst)
    ds_store, _, _ = get_derived_scalars_for_prompt(
        model_context=standard_model_context,
        autoencoder_context=standard_autoencoder_context,
        prompt=prompt,
        dst_and_config_list=unique_dst_and_config_list,  # type: ignore
    )
    layer_index = 5
    node_indices = [
        NodeIndex(
            node_type=NodeType.MLP_NEURON,
            layer_index=layer_index,
            tensor_indices=(0, 0),
            pass_type=PassType.FORWARD,
        ),
        NodeIndex(
            node_type=NodeType.AUTOENCODER_LATENT,
            layer_index=layer_index,
            tensor_indices=(0, 0),
            pass_type=PassType.FORWARD,
        ),
    ]
    for kind in ["write", "read"]:
        for node_index in node_indices:
            print(f"{kind} {node_index}")
            p = postprocessor_by_kind[kind]
            token_vector = p.postprocess(
                ds_store=ds_store,
                node_index=node_index,
            )
            topk = torch.topk(token_vector, 10)
            top_token_ints = topk.indices.tolist()
            top_token_strings = standard_model_context.decode_token_list(top_token_ints)
            top_token_scores = topk.values.tolist()
            activation = ds_store[
                DerivedScalarIndex.from_node_index(
                    node_index,
                    (
                        DerivedScalarType.MLP_POST_ACT
                        if node_index.node_type == NodeType.MLP_NEURON
                        else DerivedScalarType.ONLINE_AUTOENCODER_LATENT
                    ),
                )
            ]
            print(f"Activation: {activation}")
            print(f"Top tokens: {top_token_strings}")
            print(f"Top token scores: {top_token_scores}")
            assert token_vector.shape == (
                standard_model_context.get_dim_size(Dimension.VOCAB_SIZE),
            )
            if node_index.node_type == NodeType.MLP_NEURON:
                assert activation < 0.0  # for this prompt
                if kind == "write":
                    assert (
                        " infancy" in top_token_strings
                    )  # matches downvoted tokens in neuron-viewer page for neuron 5:0
                elif kind == "read":
                    assert (
                        " unison" in top_token_strings
                    )  # matches upvoting input tokens in neuron-viewer page for neuron 5:0
    ds_store, _, _ = get_derived_scalars_for_prompt(
        model_context=standard_model_context,
        autoencoder_context=standard_autoencoder_context,
        prompt=".\n\nA fake orca",
        dst_and_config_list=unique_dst_and_config_list,  # type: ignore
    )
    layer_index = 5
    node_indices = [
        NodeIndex(
            node_type=NodeType.MLP_NEURON,
            layer_index=layer_index,
            tensor_indices=(6, 0),
            pass_type=PassType.FORWARD,
        ),
        NodeIndex(
            node_type=NodeType.AUTOENCODER_LATENT,
            layer_index=layer_index,
            tensor_indices=(6, 0),
            pass_type=PassType.FORWARD,
        ),
    ]

================
File: neuron_explainer/tests/test_reconstituted_gradients.py
================
import torch
from neuron_explainer.activation_server.derived_scalar_computation import (
    get_derived_scalars_for_prompt,
)
from neuron_explainer.activations.derived_scalars import DerivedScalarType
from neuron_explainer.activations.derived_scalars.indexing import ActivationIndex, TraceConfig
from neuron_explainer.activations.derived_scalars.locations import (
    get_previous_residual_dst_for_node_type,
)
from neuron_explainer.activations.derived_scalars.make_scalar_derivers import make_scalar_deriver
from neuron_explainer.activations.derived_scalars.scalar_deriver import (
    ActivationsAndMetadata,
    DstConfig,
)
from neuron_explainer.models.model_component_registry import (
    ActivationLocationType,
    Dimension,
    PassType,
)
from neuron_explainer.models.model_context import StandardModelContext
def run_smoke_test(
    gpt2_small_model_context: StandardModelContext,
    n_tokens: int,
    activation_indices_for_grad: list[ActivationIndex],
    dst_and_shape: tuple[DerivedScalarType, tuple[int, ...]],
    test_name: str,
) -> None:
    print(f"Running smoke test for {test_name}")
    for activation_index_for_grad in activation_indices_for_grad:
        trace_config = TraceConfig.from_activation_index(activation_index_for_grad)
        dst_config = DstConfig(
            model_context=gpt2_small_model_context,
            trace_config=trace_config,
        )
        assert dst_config.trace_config is not None
        dst, shape = dst_and_shape
        # create scalar deriver
        scalar_deriver = make_scalar_deriver(dst, dst_config)
        residual_dst = get_previous_residual_dst_for_node_type(
            dst_config.trace_config.node_type, autoencoder_dst=None
        )
        print(f"{residual_dst=}")
        # create fake dataset of activations
        activations_and_metadata_tuple = create_fake_dataset_of_activations(
            dst,
            residual_dst,
            n_tokens,
            gpt2_small_model_context,
        )
        # calculate derived scalar
        new_activations_and_metadata = (
            scalar_deriver.activations_and_metadata_calculate_derived_scalar_fn(
                activations_and_metadata_tuple, PassType.FORWARD
            )
        )
        layer_indices = list(new_activations_and_metadata.activations_by_layer_index.keys())
        assert (
            new_activations_and_metadata.activations_by_layer_index[layer_indices[0]].shape == shape
        ), (
            new_activations_and_metadata.activations_by_layer_index[layer_indices[0]].shape,
            shape,
        )
        assert new_activations_and_metadata.dst == dst
def create_fake_dataset_of_activations(
    dst: DerivedScalarType,
    residual_dst: DerivedScalarType,
    n_tokens: int,
    model_context: StandardModelContext,
) -> tuple[ActivationsAndMetadata, ...]:
    layer_indices = list(range(model_context.n_layers))
    resid_activations_and_metadata = (
        ActivationsAndMetadata(
            activations_by_layer_index={
                layer_index: torch.randn(
                    (n_tokens, model_context.n_residual_stream_channels),
                    device=model_context.device,
                )
                for layer_index in layer_indices
            },
            pass_type=PassType.FORWARD,
            dst=residual_dst,
        ),
    )
    if dst == DerivedScalarType.UNFLATTENED_ATTN_WRITE_TO_FINAL_ACTIVATION_RESIDUAL_GRAD:
        activations_and_metadata_tuple: tuple[ActivationsAndMetadata, ...] = (
            ActivationsAndMetadata(
                activations_by_layer_index={
                    layer_index: torch.randn(
                        (
                            n_tokens,
                            n_tokens,
                            model_context.n_attention_heads,
                            model_context.get_dim_size(Dimension.VALUE_CHANNELS),
                        ),
                        device=model_context.device,
                    )
                    for layer_index in layer_indices
                },
                pass_type=PassType.FORWARD,
                dst=DerivedScalarType.ATTN_WEIGHTED_VALUE,
            ),
        ) + resid_activations_and_metadata
    elif dst == DerivedScalarType.MLP_WRITE_TO_FINAL_ACTIVATION_RESIDUAL_GRAD:
        activations_and_metadata_tuple = (
            ActivationsAndMetadata(
                activations_by_layer_index={
                    layer_index: torch.randn(
                        (n_tokens, model_context.n_neurons),
                        device=model_context.device,
                    )
                    for layer_index in layer_indices
                },
                pass_type=PassType.FORWARD,
                dst=DerivedScalarType.MLP_POST_ACT,
            ),
        ) + resid_activations_and_metadata
    else:
        raise ValueError("Invalid activation location type")
    return activations_and_metadata_tuple
def test_mlp_write_to_final_activation_residual_grad_smoke(
    standard_model_context: StandardModelContext,
) -> None:
    n_tokens = 10
    n_neurons = 3072
    run_smoke_test(
        gpt2_small_model_context=standard_model_context,
        n_tokens=n_tokens,
        activation_indices_for_grad=[
            ActivationIndex(
                activation_location_type=ActivationLocationType.ATTN_QK_PROBS,
                layer_index=5,
                tensor_indices=(0, 0, 0),
                pass_type=PassType.FORWARD,
            ),
            ActivationIndex(
                activation_location_type=ActivationLocationType.MLP_POST_ACT,
                layer_index=5,
                tensor_indices=(0, 0),
                pass_type=PassType.FORWARD,
            ),
        ],
        dst_and_shape=(
            DerivedScalarType.MLP_WRITE_TO_FINAL_ACTIVATION_RESIDUAL_GRAD,
            (n_tokens, n_neurons),
        ),
        test_name="MLP_WRITE_TO_FINAL_ACTIVATION_RESIDUAL_GRAD",
    )
def test_attn_write_to_final_activation_residual_grad_smoke(
    standard_model_context: StandardModelContext,
) -> None:
    n_tokens = 10
    n_attention_heads = 12
    run_smoke_test(
        gpt2_small_model_context=standard_model_context,
        n_tokens=n_tokens,
        activation_indices_for_grad=[
            ActivationIndex(
                activation_location_type=ActivationLocationType.ATTN_QK_PROBS,
                layer_index=5,
                tensor_indices=(0, 0, 0),
                pass_type=PassType.FORWARD,
            ),
            ActivationIndex(
                activation_location_type=ActivationLocationType.MLP_POST_ACT,
                layer_index=5,
                tensor_indices=(0, 0),
                pass_type=PassType.FORWARD,
            ),
        ],
        dst_and_shape=(
            DerivedScalarType.UNFLATTENED_ATTN_WRITE_TO_FINAL_ACTIVATION_RESIDUAL_GRAD,
            (n_tokens, n_tokens, n_attention_heads),
        ),
        test_name="ATTN_WRITE_TO_FINAL_ACTIVATION_RESIDUAL_GRAD",
    )
def test_write_to_final_activation_residual_grad_equality(
    standard_model_context: StandardModelContext,
) -> None:
    prompt = "This is a test"
    dst_list = [
        DerivedScalarType.UNFLATTENED_ATTN_WRITE_TO_FINAL_RESIDUAL_GRAD,
        DerivedScalarType.UNFLATTENED_ATTN_WRITE_TO_FINAL_ACTIVATION_RESIDUAL_GRAD,
        DerivedScalarType.MLP_WRITE_TO_FINAL_RESIDUAL_GRAD,
        DerivedScalarType.MLP_WRITE_TO_FINAL_ACTIVATION_RESIDUAL_GRAD,
    ]
    activation_index_for_grad_list = [
        ActivationIndex(
            activation_location_type=ActivationLocationType.ATTN_QK_PROBS,
            layer_index=5,
            tensor_indices=(
                1,
                1,
                0,
            ),  # (0, 0, 0) is constrained to be 1 always, so gradient is zero
            pass_type=PassType.FORWARD,
        ),
        ActivationIndex(
            activation_location_type=ActivationLocationType.MLP_POST_ACT,
            layer_index=5,
            tensor_indices=(1, 0),
            pass_type=PassType.FORWARD,
        ),
    ]
    for activation_index_for_grad in activation_index_for_grad_list:
        trace_config = TraceConfig.from_activation_index(activation_index_for_grad)
        dst_config = DstConfig(
            model_context=standard_model_context,
            trace_config=trace_config,
        )
        dst_and_config_list = [(dst, dst_config) for dst in dst_list]
        ds_store, _, raw_store = get_derived_scalars_for_prompt(
            model_context=standard_model_context,
            prompt=prompt,
            trace_config=trace_config,
            dst_and_config_list=dst_and_config_list,  # type: ignore
        )
        if (
            activation_index_for_grad.activation_location_type
            == ActivationLocationType.ATTN_QK_PROBS
        ):
            layer_indices = [3, 4]
            resid_activation_location_type = ActivationLocationType.RESID_POST_MLP
        else:
            assert (
                activation_index_for_grad.activation_location_type
                == ActivationLocationType.MLP_POST_ACT
            )
            layer_indices = [3, 4, 5]
            resid_activation_location_type = ActivationLocationType.RESID_POST_ATTN
        for layer_index in layer_indices:
            print(
                layer_index,
                raw_store.get_activations_and_metadata(
                    activation_location_type=resid_activation_location_type,
                    pass_type=PassType.BACKWARD,
                ).activations_by_layer_index[layer_index][:, 0],
            )
        activation_dst_to_vanilla_dst = {
            DerivedScalarType.UNFLATTENED_ATTN_WRITE_TO_FINAL_ACTIVATION_RESIDUAL_GRAD: DerivedScalarType.UNFLATTENED_ATTN_WRITE_TO_FINAL_RESIDUAL_GRAD,
            DerivedScalarType.MLP_WRITE_TO_FINAL_ACTIVATION_RESIDUAL_GRAD: DerivedScalarType.MLP_WRITE_TO_FINAL_RESIDUAL_GRAD,
        }
        activation_dst_to_slicer: dict[DerivedScalarType, tuple[slice | int, ...]] = {
            DerivedScalarType.UNFLATTENED_ATTN_WRITE_TO_FINAL_ACTIVATION_RESIDUAL_GRAD: (
                slice(None),
                slice(None),
                0,
            ),
            DerivedScalarType.MLP_WRITE_TO_FINAL_ACTIVATION_RESIDUAL_GRAD: (
                slice(None),
                slice(None, 5),
            ),
        }
        for activation_dst in activation_dst_to_vanilla_dst.keys():
            pass_type = PassType.FORWARD
            layer_index_to_print = 4
            if activation_dst in dst_list:
                vanilla_dst = activation_dst_to_vanilla_dst[activation_dst]
                slicer = activation_dst_to_slicer[activation_dst]
                print(
                    activation_dst,
                    activation_index_for_grad.activation_location_type,
                    ds_store.activations_and_metadata_by_dst_and_pass_type[
                        (
                            activation_dst,
                            pass_type,
                        )
                    ].activations_by_layer_index[layer_index_to_print][slicer],
                    ds_store.activations_and_metadata_by_dst_and_pass_type[
                        (
                            activation_dst,
                            pass_type,
                        )
                    ]
                    .activations_by_layer_index[layer_index_to_print]
                    .shape,
                    ds_store.activations_and_metadata_by_dst_and_pass_type[
                        (vanilla_dst, pass_type)
                    ].activations_by_layer_index[layer_index_to_print][slicer],
                    ds_store.activations_and_metadata_by_dst_and_pass_type[(vanilla_dst, pass_type)]
                    .activations_by_layer_index[layer_index_to_print]
                    .shape,
                )
                assert (
                    ds_store.activations_and_metadata_by_dst_and_pass_type[
                        (
                            activation_dst,
                            pass_type,
                        )
                    ]
                    == ds_store.activations_and_metadata_by_dst_and_pass_type[
                        (vanilla_dst, pass_type)
                    ]
                )

================
File: neuron_explainer/tests/test_serialization_of_model_config_from_model_context.py
================
import json
from neuron_explainer.models.model_context import StandardModelContext
def test_standard_model_context(standard_model_context: StandardModelContext) -> None:
    json.dumps(standard_model_context.get_model_config_as_dict())

================
File: neuron_explainer/tests/test_trace_through_v.py
================
from neuron_explainer.activation_server.derived_scalar_computation import (
    get_derived_scalars_for_prompt,
    maybe_construct_loss_fn_for_backward_pass,
)
from neuron_explainer.activation_server.requests_and_responses import LossFnConfig, LossFnName
from neuron_explainer.activations.derived_scalars import DerivedScalarType
from neuron_explainer.activations.derived_scalars.derived_scalar_store import AttentionTraceType
from neuron_explainer.activations.derived_scalars.indexing import (
    NodeIndex,
    PreOrPostAct,
    TraceConfig,
)
from neuron_explainer.activations.derived_scalars.scalar_deriver import DstConfig
from neuron_explainer.models.autoencoder_context import AutoencoderContext
from neuron_explainer.models.model_component_registry import NodeType, PassType
from neuron_explainer.models.model_context import StandardModelContext
DETACH_LAYER_NORM_SCALE_FOR_TEST = (
    False  # this sets whether to detach layer norm scale when computing these DSTs.
)
def test_trace_through_v(
    standard_model_context: StandardModelContext,
    standard_autoencoder_context: AutoencoderContext,
) -> None:
    prompt = "This is a test"
    loss_fn_for_backward_pass = maybe_construct_loss_fn_for_backward_pass(
        model_context=standard_model_context,
        config=LossFnConfig(
            name=LossFnName.LOGIT_DIFF,
            target_tokens=["."],
            distractor_tokens=["!"],
        ),
    )
    for downstream_trace_config in [
        None,
        TraceConfig(
            node_index=NodeIndex(
                node_type=NodeType.ATTENTION_HEAD,
                layer_index=5,
                pass_type=PassType.FORWARD,
                tensor_indices=(0, 0, 0),
            ),
            pre_or_post_act=PreOrPostAct.POST,
            detach_layer_norm_scale=DETACH_LAYER_NORM_SCALE_FOR_TEST,
            attention_trace_type=AttentionTraceType.K,
        ),
    ]:
        trace_config = TraceConfig(
            node_index=NodeIndex(
                node_type=NodeType.ATTENTION_HEAD,
                layer_index=3,
                pass_type=PassType.FORWARD,
                tensor_indices=(0, 0, 0),
            ),
            pre_or_post_act=PreOrPostAct.POST,
            detach_layer_norm_scale=DETACH_LAYER_NORM_SCALE_FOR_TEST,
            attention_trace_type=AttentionTraceType.V,
            downstream_trace_config=downstream_trace_config,
        )
        dst_config = DstConfig(
            model_context=standard_model_context,
            autoencoder_context=standard_autoencoder_context,
            trace_config=trace_config,
        )
        dst_list = [
            DerivedScalarType.UNFLATTENED_ATTN_WRITE_TO_FINAL_RESIDUAL_GRAD,
            DerivedScalarType.ONLINE_AUTOENCODER_WRITE_TO_FINAL_RESIDUAL_GRAD,
        ]
        dst_and_config_list = [(dst, dst_config) for dst in dst_list]
        current_ds_store, _, raw_store = get_derived_scalars_for_prompt(
            model_context=standard_model_context,
            prompt=prompt,
            trace_config=trace_config,
            dst_and_config_list=dst_and_config_list,  # type: ignore
            autoencoder_context=standard_autoencoder_context,
            loss_fn_for_backward_pass=loss_fn_for_backward_pass,
        )

================
File: neuron_explainer/tests/test_transformer.py
================
import functools
import numpy as np
import pytest
import torch
from neuron_explainer.models import Transformer
from neuron_explainer.models.transformer import (
    causal_attn_mask,
    prep_input_and_pad,
    prep_pos_from_pad_and_prev_lens,
)
# models for testing on
REFERENCE_MODELS = ["gpt2/small"]
# uncomment this for more extensive testing
# REFERENCE_MODELS = ["gpt2/small", "gpt2/medium", "gpt2/large", "gpt2/xl"]
# if we run on a device with a GPU, let's test determinism on it!
REFERENCE_DEVICES = ["cpu", "cuda"] if torch.cuda.is_available() else ["cpu"]
# convenience function for testing
assert_equal = functools.partial(torch.testing.assert_close, rtol=0, atol=0)
@functools.cache
def get_test_model(model, device):
    return Transformer.load(model, device=device)
# ======
# tests
# ======
def test_attention_masks():
    """Correctness tests for the attention mask functions."""
    # ======================
    # test causal_attn_mask
    # ======================
    M_ref = torch.BoolTensor(
        [[1, 0, 0, 0, 0], [1, 1, 0, 0, 0], [1, 1, 1, 0, 0], [1, 1, 1, 1, 0], [1, 1, 1, 1, 1]]
    )
    M = causal_attn_mask(size=5, device="cpu")
    assert_equal(M_ref, M, msg="causal_attn_mask produced incorrect result")
def test_sample_utilities():
    """Correctness tests for functions that enable batched sampling."""
    # ========================
    # test prep_input_and_pad
    # ========================
    # make a set of test prompts of unequal lengths
    test_inputs = [
        [1, 100, 50, 47],  # prompt 0
        [1298, 618, 952, 223, 4, 42],  # prompt 1
        [31],  # prompt 2
    ]
    X_ref = torch.LongTensor(
        [
            [0, 0, 1, 100, 50, 47],
            [1298, 618, 952, 223, 4, 42],
            [0, 0, 0, 0, 0, 31],
        ]
    )
    pad_ref = torch.BoolTensor(
        [
            [1, 1, 0, 0, 0, 0],
            [0, 0, 0, 0, 0, 0],
            [1, 1, 1, 1, 1, 0],
        ]
    )
    X, pad = prep_input_and_pad(test_inputs, pad_side="left", device="cpu")
    assert_equal(X_ref, X, msg="prep_input_and_pad produced incorrect X")
    assert_equal(pad_ref, pad, msg="prep_input_and_pad produced incorrect pad")
    # =====================================
    # test prep_pos_from_pad_and_prev_lens
    # =====================================
    # based on the same example as before, compute the associated pos
    pos_ref = torch.LongTensor([[0, 0, 0, 1, 2, 3], [0, 1, 2, 3, 4, 5], [0, 0, 0, 0, 0, 0]])
    prev_lens = torch.zeros(3, 1).long()
    pos = prep_pos_from_pad_and_prev_lens(pad_ref, prev_lens)
    assert_equal(pos_ref, pos, msg="prep_pos_from_pad_and_prev_lens produced incorrect pos")
    # simulate one step forward of sampling
    seq_lens = (pos[:, -1] + 1).unsqueeze(-1)
    new_pad_ref = torch.BoolTensor(
        [
            [0],
            [0],
            [0],
        ]
    )
    new_pos = prep_pos_from_pad_and_prev_lens(new_pad_ref, seq_lens)
    new_pos_ref = torch.LongTensor([[4], [6], [1]])
    assert_equal(new_pos_ref, new_pos, msg="prep_pos_from_pad_and_prev_lens produced incorrect pos")
@pytest.mark.parametrize("model", REFERENCE_MODELS)
@pytest.mark.parametrize("device", REFERENCE_DEVICES)
def test_sampling_determinism(model, device):
    """
    Some tests for sampling determinism.
    Note: DOES NOT test determinism for nucleus sampling, which is not currently
    compatible with "use_deterministic_algorithms" mode.
    """
    torch.use_deterministic_algorithms(True)
    def reset_seed():
        torch.manual_seed(0)
        np.random.seed(0)
    # ============================================================================
    # Argmax sampling (temperature=0) and categorical sampling with temperature=1
    # ============================================================================
    for temperature in [0, 1]:
        reset_seed()
        xf = get_test_model(model, device)
        # sample 1
        reset_seed()
        s1 = xf.sample("\n", num_tokens=10, temperature=temperature, top_p=None)
        # sample 2
        reset_seed()
        s2 = xf.sample("\n", num_tokens=10, temperature=temperature, top_p=None)
        assert (
            s1["strings"] == s2["strings"]
        ), f"Sampling for {model} was not deterministic with {temperature=}"
@pytest.mark.parametrize("model", REFERENCE_MODELS)
@pytest.mark.parametrize("device", REFERENCE_DEVICES)
def test_batched_sampling_correctness(model, device):
    torch.use_deterministic_algorithms(True)
    def reset_seed():
        torch.manual_seed(0)
        np.random.seed(0)
    reset_seed()
    xf = get_test_model(model, device)
    cfg = xf.cfg
    xf.eval()
    p1 = "if this test works, then these prompts should have the same"
    p2 = "completion regardless of their order"
    # sample 1
    s1 = xf.sample([p1, p2], num_tokens=10, temperature=0, top_p=None)
    # sample 2
    s2 = xf.sample([p2, p1], num_tokens=10, temperature=0, top_p=None)
    # completions for prompt p1
    c1_v1 = s1["strings"][0]
    c1_v2 = s2["strings"][1]
    assert c1_v1 == c1_v2, "Out-of-order prompts resulted in different completions"
    # completions for prompt p2
    c2_v1 = s1["strings"][0]
    c2_v2 = s2["strings"][1]
    assert c2_v1 == c2_v2, "Out-of-order prompts resulted in different completions"

================
File: neuron_viewer/public/robots.txt
================
# https://www.robotstxt.org/robotstxt.html
User-agent: *
Disallow:

================
File: neuron_viewer/README.md
================
# Neuron viewer

A React app that hosts TDB as well as pages with information about individual model components
(MLP neurons, attention heads and autoencoder latents for both).


## Running the server locally

First, install the app:

```sh
npm install
```

Then run the frontend:

```sh
npm start
```

- To open a Neuron Viewer page, navigate to `http://localhost:1234`.
- To open TDB, navigate to `http://localhost:1234/gpt2-small/tdb_alpha`.
- To open TDB with autoencoders, navigate to `http://localhost:1234/gpt2-small_ae-resid-delta-mlp-v4_ae-resid-delta-attn-v4/tdb_alpha`
(where `ae-resid-delta-mlp-v4` and `ae-resid-delta-attn-v4` must match the autoencoder names that are used in the [activation server](../neuron_explainer/activation_server/README.md)).

## Formatting code

To check whether the code is correctly formatted:

```sh
npm run check-code-format
```

To format the code:

```sh
npm run format-code
```

## Code organization

- [src/client](src/client/): Auto-generated code for interacting with the activation server (the neuron viewer's backend). Do not edit this code! Follow the instructions in [the activation server README](../neuron_explainer/activation_server/README.md) to regenerate this code if you make changes to the activation server. Use [src/requests](src/requests/) when calling the activation server.
- [src/panes](src/panes/): UI elements that can be used as panes on a page: tokens+activations, similar neurons, etc.
- [src/requests](src/requests/): Client libraries for making network requests to the activation server.
- [src/TransformerDebugger](src/TransformerDebugger/): Code related to the Transformer Debugger.
- [src](src/): Other code.

## Using a remote activation server

If you decide to run your activation server on a different host or port than the default, you can
point neuron viewer at it by setting the `NEURON_VIEWER_ACTIVATION_SERVER_URL` environment variable:
    
```sh
NEURON_VIEWER_ACTIVATION_SERVER_URL=https://some.url:port npm start
```

## Making changes

Be sure to run the following to validate any changes you make:

```sh
npm run check-type-warnings && npm run check-code-format && npm run build
```

================
File: README.md
================
# Transformer Debugger

Transformer Debugger (TDB) is a tool developed by OpenAI's [Superalignment
team](https://openai.com/blog/introducing-superalignment) with the goal of
supporting investigations into specific behaviors of small language models. The tool combines
[automated interpretability](https://openai.com/research/language-models-can-explain-neurons-in-language-models)
techniques with [sparse autoencoders](https://transformer-circuits.pub/2023/monosemantic-features).

TDB enables rapid exploration before needing to write code, with the ability to intervene in the
forward pass and see how it affects a particular behavior. It can be used to answer questions like,
"Why does the model output token A instead of token B for this prompt?" or "Why does attention head
H attend to token T for this prompt?" It does so by identifying specific components (neurons,
attention heads, autoencoder latents) that contribute to the behavior, showing automatically
generated explanations of what causes those components to activate most strongly, and tracing
connections between components to help discover circuits.

These videos give an overview of TDB and show how it can be used to investigate [indirect object
identification in GPT-2 small](https://arxiv.org/abs/2211.00593):

- [Introduction](https://www.loom.com/share/721244075f12439496db5d53439d2f84?sid=8445200e-c49e-4028-8b8e-3ea8d361dec0)
- [Neuron viewer pages](https://www.loom.com/share/21b601b8494b40c49b8dc7bfd1dc6829?sid=ee23c00a-9ede-4249-b9d7-c2ba15993556)
- [Example: Investigating name mover heads, part 1](https://www.loom.com/share/3478057cec484a1b85471585fef10811?sid=b9c3be4b-7117-405a-8d31-0f9e541dcfb6)
- [Example: Investigating name mover heads, part 2](https://www.loom.com/share/6bd8c6bde84b42a98f9a26a969d4a3ad?sid=4a09ac29-58a2-433e-b55d-762414d9a7fa)

## What's in the release?

- [Neuron viewer](neuron_viewer/README.md): A React app that hosts TDB as well as pages with information about individual model components (MLP neurons, attention heads and autoencoder latents for both).
- [Activation server](neuron_explainer/activation_server/README.md): A backend server that performs inference on a subject model to provide data for TDB. It also reads and serves data from public Azure buckets.
- [Models](neuron_explainer/models/README.md): A simple inference library for GPT-2 models and their autoencoders, with hooks to grab activations.
- [Collated activation datasets](datasets.md): top-activating dataset examples for MLP neurons, attention heads and autoencoder latents.

## Setup

Follow these steps to install the repo.  You'll first need python/pip, as well as node/npm.

Though optional, we recommend you use a virtual environment or equivalent:

```sh
# If you're already in a venv, deactivate it.
deactivate
# Create a new venv.
python -m venv ~/.virtualenvs/transformer-debugger
# Activate the new venv.
source ~/.virtualenvs/transformer-debugger/bin/activate
```

Once your environment is set up, follow the following steps:
```sh
git clone git@github.com:openai/transformer-debugger.git
cd transformer-debugger

# Install neuron_explainer
pip install -e .

# Set up the pre-commit hooks.
pre-commit install

# Install neuron_viewer.
cd neuron_viewer
npm install
cd ..
```

To run the TDB app, you'll then need to follow the instructions to set up the [activation server backend](neuron_explainer/activation_server/README.md) and [neuron viewer frontend](neuron_viewer/README.md).

## Making changes

To validate changes:

- Run `pytest`
- Run `mypy --config=mypy.ini .`
- Run activation server and neuron viewer and confirm that basic functionality like TDB and neuron
  viewer pages is still working


## Links

- [Terminology](terminology.md)

## How to cite

Please cite as:

```
Mossing, et al., Transformer Debugger, GitHub, 2024.
```

BibTex citation:

```
@misc{mossing2024tdb,
  title={Transformer Debugger},
  author={Mossing, Dan and Bills, Steven and Tillman, Henk and Dupr la Tour, Tom and Cammarata, Nick and Gao, Leo and Achiam, Joshua and Yeh, Catherine and Leike, Jan and Wu, Jeff and Saunders, William},
  year={2024},
  publisher={GitHub},
  howpublished={\url{https://github.com/openai/transformer-debugger}},
}
```

================
File: setup.py
================
from setuptools import find_packages, setup
setup(
    name="neuron_explainer",
    packages=find_packages(),
    version="0.0.1",
    author="OpenAI",
    install_requires=[
        "aiohttp",
        "click",
        "fastapi==0.97",
        "fire",
        "httpx>=0.22",
        "mypy==1.7.1",
        "numpy",
        "orjson",
        "pre-commit",
        "pydantic<2.0.0",
        "pytest",
        "pytest-asyncio",
        "scikit-learn",
        "starlette",
        "tiktoken",
        "torch>=1.13",
        "uvicorn",
    ],
    url="",
    description="",
    python_requires=">=3.11",
)

================
File: terminology.md
================
# TDB Terminology

**Component**

- An attention head or neuron, or autoencoder latent
- Has some set of weights that define what the component does
- Analogy:
    - Component is like the code for a function
    - Node is like the specific invocation of a function with specific input values and specific output values
- When invoked, each component produces nodes that read something from the unnormalized residual stream, then write some vector (the write vector) to the unnormalized residual stream
- Each component is independent from other components of the same type in the same layer

**Node**

- Specific invocation of a component which reads from the normalized residual stream at one token, maybe produces some intermediate values, and then writes to the normalized residual stream at one token
- Comes from talking about nodes in a computation graph/causal graph
- Neurons/latents produce one node per sequence token. they read from/write to the same token
    - Neuron pre/post activations are intermediate values
- Attention heads produce one node per pair of sequence tokens (reading from same/earlier token, writing to later token)
    - Attention QK products, value vectors are intermediate values
- Each node only exists in one forward/backward pass. If you modify the prompt and rerun, that would create different nodes

**Write vector**

- Vector written to the residual stream by a node

**Circuit**

- Set of nodes that work together to perform some behavior/reasoning

**Latent**

- Type of component corresponding to a direction in the activation space learned by a sparse autoencoder for a specific layer's MLP neurons or attention heads
- They correspond more often to semantically meaningful features than neurons or attention heads do

**Ablate**

- Turn off a node
- Right now we use zero-ablation, so the node wont write anything to the residual stream. In principle we could implement other versions
- Lets you observe the downstream effects of the node
- Answers the question What is the real effect of this node writing to the residual stream?

**Trace upstream**

- Look at what upstream nodes caused this node to write to the residual stream
- Answer the question Why did this node write to the residual stream in this way?

**Direction of interest**

- TDB looks at the importance of nodes in terms of their effect on a specific direction in the transformer representations
    - In principle this could be a direction in the unnormalized residual stream, normalized residual stream, or some other vector space in transformer representations
    - For now, the only option is the direction in the final normalized residual stream corresponding to the unembedding of a target token minus the unembedding of a distractor token
- The activation of the direction of interest is the projection of the transformers representations onto the direction of interest at a specific token on a specific prompt, which yields a scalar value
    - For token differences, the activation = difference in logits = log ratio of probabilities between these two tokens: log(p(target_token)/p(distractor_token))

**Target token**

- The token that corresponds to positive activation along direction of interest, usually the token the model assigns highest probability to

**Distractor token**

- The token that corresponds to negative activation along direction of interest, usually a plausible but incorrect token

**Estimated total effect**

- Estimate of the total effect of the node on the activation of the direction of interest
- Positive values mean that the node increases the activation of the direction of interest; negative values mean that it decreases the activation
- Accounts for the node directly affecting the direction of interest and the indirect effect through intermediate nodes
- Implementation details:
    - Computed by taking the dot product of the activation and the gradient of the direction of interest
    - ACT_TIMES_GRAD in [derived scalars terminology](neuron_explainer/activations/derived_scalars/README.md)

**Direct effect**

- Projection of the nodes output onto the direction of interest
- Positive values mean that the node increases the activation of the direction of interest; negative values mean that it decreases the activation
- Only accounts for the node directly affecting the final residual stream, not impact through intermediate nodes
- Implementation details:
    - Computed by taking the dot product of the activation and the gradient of interest from the final residual stream
    - WRITE_TO_FINAL_RESIDUAL_GRAD in [derived scalars terminology](neuron_explainer/activations/derived_scalars/README.md)

**Write magnitude**

- Magnitude of the write vector produced by the node, including information not relevant to direction of interest
- Higher means that the node is more important to the forward pass
- Implementation details:
    - WRITE_NORM in [derived scalars terminology](neuron_explainer/activations/derived_scalars/README.md)

**Layer**

- Transformers consist of layers which each contain a block of attention heads, followed by a block of MLP neurons

**Upstream**

- Upstream in the causal graph. Modifying an upstream node can impact a downstream node
- Node must be earlier in the forward pass, and must be at the same token or previous token, not a future token

**Downstream**

- Downstream in the causal graph. Modifying a downstream node cannot impact an upstream node (but can impact our estimates, because they use gradients which are impacted by all nodes in the graph)
- Node must be later in the forward pass, and must be at the same token or a subsequent token, not a previous token



================================================================
End of Codebase
================================================================
