This file is a merged representation of a subset of the codebase, containing specifically included files and files not matching ignore patterns, combined into a single document by Repomix.
The content has been processed where empty lines have been removed.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
- Pay special attention to the Repository Description. These contain important context and guidelines specific to this project.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: **/*.py, **/*.md, **/*.txt
- Files matching these patterns are excluded: **/.git/**, **/.github/**, CHANGELOG.md
- Empty lines have been removed from all files
- Files are sorted by Git change count (files with more changes are at the bottom)

Additional Info:
----------------
User Provided Header:
-----------------------
This file is a consolidated single-file compilation of all code in the repository generated by Repomix. Note that .ipynb files have been converted to .py files.

================================================================
Directory Structure
================================================================
README.md
sae_vis/__init__.py
sae_vis/data_config_classes.py
sae_vis/data_fetching_fns.py
sae_vis/data_storing_fns.py
sae_vis/demos/demo.py
sae_vis/model_fns.py
sae_vis/utils_fns.py

================================================================
Files
================================================================

================
File: README.md
================
Note - I'm still open to accepting PRs on this library, and am very happy for other people to build on it, but I won't be actively maintaining it going forwards since I'll be focusing on my job. The [SAELens](https://github.com/jbloomAus/SAELens) library will continue to have more development and iteration, and it uses a fork of this repo as well as containing a much larger suite of tools for working with SAEs, so depending on your use case you might find that library preferable!

---

This codebase was designed to replicate Anthropic's sparse autoencoder visualisations, which you can see [here](https://transformer-circuits.pub/2023/monosemantic-features/vis/a1.html). The codebase provides 2 different views: a **feature-centric view** (which is like the one in the link, i.e. we look at one particular feature and see things like which tokens fire strongest on that feature) and a **prompt-centric view** (where we look at once particular prompt and see which features fire strongest on that prompt according to a variety of different metrics).

Install with `pip install sae-vis`. Link to PyPI page [here](https://pypi.org/project/sae-vis/).

See [here](https://colab.research.google.com/drive/1SuoFIjLvzOAuSg1nkqNbtkwXr8EvQ7SY?usp=drive_link) for a demo Colab notebook (all the code to produce it is also in this repo, in the file `sae_vis/demos/demo.py`, as well as the files containing the created visualizations).

The library supports two types of visualizations:

1. **Feature-centric vis**, where you look at a single feature and see e.g. which sequences in a large dataset this feature fires strongest on.

<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/feature-vis-video.gif" width="800">

2. **Prompt-centric vis**, where you input a custom prompt and see which features score highest on that prompt, according to a variety of possible metrics.

<img src="https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/prompt-vis-video.gif" width="800">

# Citing this work

To cite this work, you can use this bibtex citation:

```
@misc{sae_vis,
    title  = {{SAE Visualizer}},
    author = {Callum McDougall},
    howpublished    = {\url{https://github.com/callummcdougall/sae_vis}},
    year   = {2024}
}
```

# Contributing

This project is uses [Poetry](https://python-poetry.org/) for dependency management. After cloning the repo, install dependencies with `poetry install`.

This project uses [Ruff](https://docs.astral.sh/ruff/) for formatting and linting, [Pyright](https://github.com/microsoft/pyright) for type-checking, and [Pytest](https://docs.pytest.org/) for tests. If you submit a PR, make sure that your code passes all checks. You can run all checks with `make check-all`.

# Version history (recording started at `0.2.9`)

- `0.2.9` - added table for pairwise feature correlations (not just encoder-B correlations)
- `0.2.10` - fix some anomalous characters
- `0.2.11` - update PyPI with longer description
- `0.2.12` - fix height parameter of config, add videos to PyPI description
- `0.2.13` - add to dependencies, and fix SAELens section
- `0.2.14` - fix mistake in dependencies
- `0.2.15` - refactor to support eventual scatterplot-based feature browser, fix `&rsquo;` HTML
- `0.2.16` - allow disabling buffer in feature generation, fix demo notebook, fix sae-lens compatibility & type checking
- `0.2.17` - use main branch of `sae-lens`
- `0.2.18` - remove circular dependency with `sae-lens`
- `0.2.19` - formatting, error-checking
- `0.2.20` - fix bugs, remove use of `batch_size` in config
- `0.2.21` - formatting
- `0.3.0` - major refactor which makes several improvements, removing complexity and adding new features:
    - OthelloGPT SAEs with linear probes (input / output space)
    - Attention output SAEs with max DFA visualized
    - Tokens labelled with their `(batch, seq)` indices as well as the change in correct-token probability on feature ablation, when hovered over
- `0.3.1` - fix transformerlens dependency
- `0.3.2` - adjust pyright type-checking
- `0.3.3` - remove pyright type-checking
- `0.3.6` - remove tests

================
File: sae_vis/__init__.py
================
__version__ = "0.2.19"
from .data_fetching_fns import *
from .data_storing_fns import *
from .model_fns import *
from .utils_fns import *

================
File: sae_vis/data_config_classes.py
================
from dataclasses import asdict, dataclass, field
from typing import Any, Iterable, Iterator
from frozendict import frozendict
from rich import print as rprint
from rich.table import Table
from rich.tree import Tree
SEQUENCES_CONFIG_HELP = dict(
    buffer="How many tokens to add as context to each sequence, on each side. The tokens chosen for the top acts / \
quantile groups can't be outside the buffer range. If None, we use the entire sequence as context.",
    compute_buffer="If False, then we don't compute the loss effect, activations, or any other data for tokens \
other than the bold tokens in our sequences (saving time).",
    n_quantiles="Number of quantile groups for the sequences. If zero, we only show top activations, no quantile \
groups.",
    top_acts_group_size="Number of sequences in the 'top activating sequences' group.",
    quantile_group_size="Number of sequences in each of the sequence quantile groups.",
    top_logits_hoverdata="Number of top/bottom logits to show in the hoverdata for each token.",
    hover_below="Whether the hover information about a token appears below or above the token.",
    othello="If True, we make Othello boards instead of sequences (requires OthelloGPT)",
    n_boards_per_row="Only relevant for Othello, sets number of boards per row in top examples",
    dfa_for_attn_saes="Only relevant for attention SAEs. If true, shows DFA for top attention tokens.",
)
ACTIVATIONS_HISTOGRAM_CONFIG_HELP = dict(
    n_bins="Number of bins for the histogram.",
)
LOGITS_HISTOGRAM_CONFIG_HELP = dict(
    n_bins="Number of bins for the histogram.",
)
LOGITS_TABLE_CONFIG_HELP = dict(
    n_rows="Number of top/bottom logits to show in the table.",
)
PROBE_LOGITS_TABLE_CONFIG_HELP = dict(
    n_rows="Number of top/bottom logits to show in the table, for each probe.",
)
FEATURE_TABLES_CONFIG_HELP = dict(
    n_rows="Number of rows to show for each feature table.",
    neuron_alignment_table="Whether to show the neuron alignment table.",
    correlated_neurons_table="Whether to show the correlated neurons table.",
    correlated_features_table="Whether to show the (pairwise) correlated features table.",
    correlated_b_features_table="Whether to show the correlated encoder-B features table.",
)
@dataclass
class BaseComponentConfig:
    def data_is_contained_in(self, other: "BaseComponentConfig") -> bool:
        """
        This returns False only when the data that was computed based on `other` wouldn't be enough to show the data
        that was computed based on `self`. For instance, if `self` was a config object with 10 rows, and `other` had
        just 5 rows, then this would return False. A less obvious example: if `self` was a histogram config with 50 bins
        then `other` would need to have exactly 50 bins (because we can't change the bins after generating them).
        """
        return True
    @property
    def help_dict(self) -> dict[str, str]:
        """
        This is a dictionary which maps the name of each argument to a description of what it does. This is used when
        printing out the help for a config object, to show what each argument does.
        """
        return {}
@dataclass
class PromptConfig(BaseComponentConfig):
    pass
@dataclass
class SeqMultiGroupConfig(BaseComponentConfig):
    buffer: tuple[int, int] | None = (5, 5)
    compute_buffer: bool = True
    n_quantiles: int = 10
    top_acts_group_size: int = 20
    quantile_group_size: int = 5
    top_logits_hoverdata: int = 5
    hover_below: bool = True
    # Everything for specific kinds of SAEs / base models
    othello: bool = False
    n_boards_per_row: int = 3
    dfa_for_attn_saes: bool = True
    dfa_buffer: tuple[int, int] | None = (5, 5)
    def data_is_contained_in(self, other: BaseComponentConfig) -> bool:
        assert isinstance(other, self.__class__)
        return all(
            [
                self.buffer is None
                or (
                    other.buffer is not None and self.buffer[0] <= other.buffer[0]
                ),  # the buffer needs to be <=
                self.buffer is None
                or (other.buffer is not None and self.buffer[1] <= other.buffer[1]),
                int(self.compute_buffer)
                <= int(
                    other.compute_buffer
                ),  # we can't compute the buffer if we didn't in `other`
                self.n_quantiles
                in [
                    0,
                    other.n_quantiles,
                ],  # we actually need the quantiles identical (or one to be zero)
                self.top_acts_group_size
                <= other.top_acts_group_size,  # group size needs to be <=
                self.quantile_group_size
                <= other.quantile_group_size,  # each quantile group needs to be <=
                self.top_logits_hoverdata
                <= other.top_logits_hoverdata,  # hoverdata rows need to be <=
            ]
        )
    def __post_init__(self):
        # Get list of group lengths, based on the config params
        self.group_sizes = [self.top_acts_group_size] + [
            self.quantile_group_size
        ] * self.n_quantiles
    @property
    def help_dict(self) -> dict[str, str]:
        return SEQUENCES_CONFIG_HELP
@dataclass
class ActsHistogramConfig(BaseComponentConfig):
    n_bins: int = 50
    def data_is_contained_in(self, other: BaseComponentConfig) -> bool:
        assert isinstance(other, self.__class__)
        return self.n_bins == other.n_bins
    @property
    def help_dict(self) -> dict[str, str]:
        return ACTIVATIONS_HISTOGRAM_CONFIG_HELP
@dataclass
class LogitsHistogramConfig(BaseComponentConfig):
    n_bins: int = 50
    def data_is_contained_in(self, other: BaseComponentConfig) -> bool:
        assert isinstance(other, self.__class__)
        return self.n_bins == other.n_bins
    @property
    def help_dict(self) -> dict[str, str]:
        return LOGITS_HISTOGRAM_CONFIG_HELP
@dataclass
class LogitsTableConfig(BaseComponentConfig):
    n_rows: int = 10
    def data_is_contained_in(self, other: BaseComponentConfig) -> bool:
        assert isinstance(other, self.__class__)
        return self.n_rows <= other.n_rows
    @property
    def help_dict(self) -> dict[str, str]:
        return LOGITS_TABLE_CONFIG_HELP
@dataclass
class ProbeLogitsTablesConfig(BaseComponentConfig):
    n_rows: int = 10
    othello: bool = False
    def data_is_contained_in(self, other: BaseComponentConfig) -> bool:
        assert isinstance(other, self.__class__)
        return self.n_rows <= other.n_rows
    @property
    def help_dict(self) -> dict[str, str]:
        return PROBE_LOGITS_TABLE_CONFIG_HELP
@dataclass
class FeatureTablesConfig(BaseComponentConfig):
    n_rows: int = 3
    neuron_alignment_table: bool = True
    correlated_neurons_table: bool = True
    correlated_features_table: bool = True
    correlated_b_features_table: bool = False
    def data_is_contained_in(self, other: BaseComponentConfig) -> bool:
        assert isinstance(other, self.__class__)
        return all(
            [
                self.n_rows <= other.n_rows,
                self.neuron_alignment_table <= other.neuron_alignment_table,
                self.correlated_neurons_table <= other.correlated_neurons_table,
                self.correlated_features_table <= other.correlated_features_table,
                self.correlated_b_features_table <= other.correlated_b_features_table,
            ]
        )
    @property
    def help_dict(self) -> dict[str, str]:
        return FEATURE_TABLES_CONFIG_HELP
GenericComponentConfig = (
    PromptConfig
    | SeqMultiGroupConfig
    | ActsHistogramConfig
    | LogitsHistogramConfig
    | LogitsTableConfig
    | ProbeLogitsTablesConfig
    | FeatureTablesConfig
)
class Column:
    def __init__(
        self,
        *args: GenericComponentConfig,
        width: int | None = None,
    ):
        self.components = list(args)
        self.width = width
    def __iter__(self) -> Iterator[Any]:
        return iter(self.components)
    def __getitem__(self, idx: int) -> Any:
        return self.components[idx]
    def __len__(self) -> int:
        return len(self.components)
@dataclass
class SaeVisLayoutConfig:
    """
    This object allows you to set all the ways the feature vis will be laid out.
    Args (specified by the user):
        columns:
            A list of `Column` objects, where each `Column` contains a list of component configs.
        height:
            The height of the vis (in pixels).
    Args (defined during __init__):
        seq_cfg: SeqMultiGroupConfig
            Contains all the parameters for the top activating sequences (and the
            quantile groups).
        act_hist_cfg: ActsHistogramConfig
            Contains all the parameters for the activations histogram.
        logits_hist_cfg: LogitsHistogramConfig
            Contains all the parameters for the logits histogram.
        logits_table_cfg: LogitsTableConfig
            Contains all the parameters for the logits table.
        probe_logits_table_cfg: ProbeLogitsTablesConfig
            Contains all the parameters for the probe logits table.
        feature_tables_cfg: FeatureTablesConfig
            Contains all the parameters for the feature tables.
        prompt_cfg: PromptConfig
            Contains all the parameters for the prompt-centric vis.
    """
    columns: dict[int, Column] = field(default_factory=dict)
    height: int = 750
    seq_cfg: SeqMultiGroupConfig | None = None
    act_hist_cfg: ActsHistogramConfig | None = None
    logits_hist_cfg: LogitsHistogramConfig | None = None
    logits_table_cfg: LogitsTableConfig | None = None
    probe_logits_table_cfg: ProbeLogitsTablesConfig | None = None
    feature_tables_cfg: FeatureTablesConfig | None = None
    prompt_cfg: PromptConfig | None = None
    COMPONENT_MAP: frozendict[str, str] = frozendict(
        {
            "Prompt": "prompt_cfg",
            "SeqMultiGroup": "seq_cfg",
            "ActsHistogram": "act_hist_cfg",
            "LogitsHistogram": "logits_hist_cfg",
            "LogitsTable": "logits_table_cfg",
            "ProbeLogitsTables": "probe_logits_table_cfg",
            "FeatureTables": "feature_tables_cfg",
        }
    )
    def get_cfg_from_name(self, comp_name: str) -> Any:
        if comp_name in self.COMPONENT_MAP:
            return getattr(self, self.COMPONENT_MAP[comp_name])
        raise ValueError(f"Unknown component name {comp_name}")
    @property
    def components(self):
        """Returns a dictionary mapping component names (lowercase) to their configs, filtering out Nones."""
        all_components = {
            k[0].lower() + k[1:]: self.get_cfg_from_name(k) for k in self.COMPONENT_MAP
        }
        return {k: v for k, v in all_components.items() if v is not None}
    def __init__(self, columns: list[Column], height: int = 750):
        """
        The __init__ method will allow you to extract things like `self.seq_cfg` from the object (even though they're
        initially stored in the `columns` attribute). It also verifies that there are no duplicate components (which is
        redundant, and could mess up the HTML).
        """
        # Define the columns (as dict) and the height
        self.columns = {idx: col for idx, col in enumerate(columns)}
        self.height = height
        # Get a list of all our components, and verify there's no duplicates
        all_components = [
            component for column in self.columns.values() for component in column
        ]
        all_component_names = [
            comp.__class__.__name__.rstrip("Config") for comp in all_components
        ]
        assert len(all_component_names) == len(
            set(all_component_names)
        ), "Duplicate components in layout config"
        # Once we've verified this, store each config component as an attribute
        for comp, comp_name in zip(all_components, all_component_names):
            if comp_name in self.COMPONENT_MAP:
                setattr(self, self.COMPONENT_MAP[comp_name], comp)
            else:
                raise ValueError(f"Unknown component name {comp_name}")
    @property
    def metadata(self) -> dict[str, Any]:
        """
        Returns string-ified METADATA, to be dumped into the JavaScript page. Fpr example, default
        Othello layout would return:
            {
                "layout": [["featureTables"], ["actsHistogram", "logitsTable"], ["seqMultiGroup"]],
                "othello": True,
            }
        """
        def config_name_to_component_name(config_name: str) -> str:
            component_name = config_name.rstrip("Config")
            component_name = component_name[0].lower() + component_name[1:]
            if component_name == "sequences":
                component_name = "seqMultiGroup"
            return component_name
        layout = [
            [config_name_to_component_name(comp.__class__.__name__) for comp in column]
            for column in self.columns.values()
        ]
        othello = self.seq_cfg.othello if (self.seq_cfg is not None) else False
        column_widths = [column.width for column in self.columns.values()]
        return {
            "layout": layout,
            "othello": othello,
            "columnWidths": column_widths,
            "height": self.height,
        }
    def data_is_contained_in(self, other: "SaeVisLayoutConfig") -> bool:
        """
        Returns True if `self` uses only data that would already exist in `other`. This is useful because our prompt-
        centric vis needs to only use data that was already computed as part of our initial data gathering. For example,
        if our SaeVisData object only contains the first 10 rows of the logits table, then we can't show the top 15 rows
        in the prompt centric view!
        """
        for comp_name, comp in self.components.items():
            # If the component in `self` is not present in `other`, return False
            if comp_name not in other.components:
                return False
            # If the component in `self` is present in `other`, but the `self` component is larger, then return False
            comp_other = other.components[comp_name]
            if not comp.data_is_contained_in(comp_other):
                return False
        return True
    def help(
        self,
        title: str = "SaeVisLayoutConfig",
        key: bool = True,
    ) -> Tree | None:
        """
        This prints out a tree showing the layout of the vis, by column (as well as the values of the arguments for each
        config object, plus their default values if they changed, and the descriptions of each arg).
        """
        # Create tree (with title and optionally the key explaining arguments)
        if key:
            title += "\n\n" + KEY_LAYOUT_VIS
        tree = Tree(title)
        n_columns = len(self.columns)
        # For each column, add a tree node
        for column_idx, vis_components in self.columns.items():
            n_components = len(vis_components)
            tree_column = tree.add(f"Column {column_idx}")
            # For each component in that column, add a tree node
            for component_idx, vis_component in enumerate(vis_components):
                n_params = len(asdict(vis_component))
                tree_component = tree_column.add(
                    f"{vis_component.__class__.__name__}".rstrip("Config")
                )
                # For each config parameter of that component
                for param_idx, (param, value) in enumerate(
                    asdict(vis_component).items()
                ):
                    # Get line break if we're at the final parameter of this component (unless it's the final component
                    # in the final column)
                    suffix = "\n" if (param_idx == n_params - 1) else ""
                    if (component_idx == n_components - 1) and (
                        column_idx == n_columns - 1
                    ):
                        suffix = ""
                    # Get argument description, and its default value
                    desc = vis_component.help_dict.get(param, "")
                    value_default = getattr(
                        vis_component.__class__, param, "no default"
                    )
                    # Add tree node (appearance is different if value is changed from default)
                    if value != value_default:
                        info = f"[b dark_orange]{param}: {value!r}[/] (default = {value_default!r}) \n[i #888888]{desc}[/]{suffix}"
                    else:
                        info = f"[b #00aa00]{param}: {value!r}[/] \n[i #888888]{desc}[/]{suffix}"
                    tree_component.add(info)
        rprint(tree)
    @classmethod
    def default_feature_centric_layout(cls) -> "SaeVisLayoutConfig":
        return cls(
            columns=[
                Column(FeatureTablesConfig()),
                Column(
                    ActsHistogramConfig(), LogitsTableConfig(), LogitsHistogramConfig()
                ),
                Column(SeqMultiGroupConfig()),
            ],
            height=750,
        )
    @classmethod
    def default_prompt_centric_layout(cls) -> "SaeVisLayoutConfig":
        return cls(
            columns=[
                Column(
                    PromptConfig(),
                    ActsHistogramConfig(),
                    LogitsTableConfig(n_rows=5),
                    SeqMultiGroupConfig(top_acts_group_size=10, n_quantiles=0),
                    LogitsHistogramConfig(),
                    width=420,
                ),
            ],
            height=1100,
        )
    @classmethod
    def default_othello_layout(cls, boards: bool = True) -> "SaeVisLayoutConfig":
        return cls(
            columns=[
                Column(FeatureTablesConfig()),
                Column(
                    ActsHistogramConfig(),
                    LogitsTableConfig(),
                    ProbeLogitsTablesConfig(),
                ),
                Column(
                    SeqMultiGroupConfig(
                        othello=boards,
                        buffer=None,
                        compute_buffer=not boards,
                        n_quantiles=5,
                        quantile_group_size=6,
                        top_acts_group_size=24,
                    ),
                ),
            ],
            height=1250,
        )
KEY_LAYOUT_VIS = """Key: 
  the tree shows which components will be displayed in each column (from left to right)
  arguments are [b #00aa00]green[/]
  arguments changed from their default are [b dark_orange]orange[/], with default in brackets
  argument descriptions are in [i]italics[/i]
"""
SAE_CONFIG_DICT = dict(
    hook_point="The hook point to use for the SAE",
    features="The set of features which we'll be gathering data for. If an integer, we only get data for 1 feature",
    minibatch_size_tokens="The minibatch size we'll use to split up the full batch during forward passes, to avoid \
OOMs.",
    minibatch_size_features="The feature minibatch size we'll use to split up our features, to avoid OOM errors",
    seed="Random seed, for reproducibility (e.g. sampling quantiles)",
    verbose="Whether to print out progress messages and other info during the data gathering process",
)
@dataclass
class SaeVisConfig:
    # Data
    features: int | Iterable[int] | None = None
    minibatch_size_features: int = 256
    minibatch_size_tokens: int = 64
    seqpos_slice: tuple[int | None, ...] = (None, None, None)
    # Vis
    feature_centric_layout: SaeVisLayoutConfig = field(
        default_factory=SaeVisLayoutConfig.default_feature_centric_layout
    )
    prompt_centric_layout: SaeVisLayoutConfig = field(
        default_factory=SaeVisLayoutConfig.default_prompt_centric_layout
    )
    # Misc
    seed: int | None = 0
    # Depreciated
    batch_size: None = None
    def __post_init__(self):
        assert (
            self.batch_size is None
        ), "The `batch_size` parameter has been depreciated. Please use `minibatch_size_tokens` instead."
        assert (
            len(self.prompt_centric_layout.columns) == 1
        ), "Only allowed a single column for prompt-centric layout."
    def help(self, title: str = "SaeVisConfig"):
        """
        Performs the `help` method for both of the layout objects, as well as for the non-layout-based configs.
        """
        # Create table for all the non-layout-based params
        table = Table(
            "Param", "Value (default)", "Description", title=title, show_lines=True
        )
        # Populate table (middle row is formatted based on whether value has changed from default)
        for param, desc in SAE_CONFIG_DICT.items():
            value = getattr(self, param)
            value_default = getattr(self.__class__, param, "no default")
            if value != value_default:
                value_default_repr = (
                    "no default"
                    if value_default == "no default"
                    else repr(value_default)
                )
                value_str = f"[b dark_orange]{value!r}[/]\n({value_default_repr})"
            else:
                value_str = f"[b #00aa00]{value!r}[/]"
            table.add_row(param, value_str, f"[i]{desc}[/]")
        # Print table, and print the help trees for the layout objects
        rprint(table)
        self.feature_centric_layout.help(
            title="SaeVisLayoutConfig: feature-centric vis", key=False
        )
        self.prompt_centric_layout.help(
            title="SaeVisLayoutConfig: prompt-centric vis", key=False
        )

================
File: sae_vis/data_fetching_fns.py
================
import itertools
import math
import time
from collections import defaultdict
from typing import Literal
import einops
import numpy as np
import torch
from eindex import eindex
from jaxtyping import Float, Int
from rich import print as rprint
from rich.table import Table
from sae_lens import SAE, HookedSAETransformer
from torch import Tensor
from tqdm.auto import tqdm
from transformer_lens import ActivationCache, HookedTransformer, utils
from sae_vis.data_config_classes import (
    SaeVisConfig,
    SaeVisLayoutConfig,
    SeqMultiGroupConfig,
)
from sae_vis.data_storing_fns import (
    ActsHistogramData,
    FeatureTablesData,
    LogitsHistogramData,
    LogitsTableData,
    ProbeLogitsTableData,
    SaeVisData,
    SeqGroupData,
    SeqMultiGroupData,
    SequenceData,
)
from sae_vis.model_fns import resid_final_pre_layernorm_to_logits, to_resid_dir
from sae_vis.utils_fns import (
    METRIC_TITLES,
    FeatureStatistics,
    RollingCorrCoef,
    TopK,
    VocabType,
    cross_entropy_loss,
    get_device,
    index_with_buffer,
    k_largest_indices,
    random_range_indices,
)
Arr = np.ndarray
device = get_device()
@torch.inference_mode()
def parse_feature_data(
    model: HookedSAETransformer,
    cfg: SaeVisConfig,
    sae: SAE,
    sae_B: SAE | None,
    tokens: Int[Tensor, "batch seq"],
    feature_indices: int | list[int],
    feature_resid_dir: Float[Tensor, "feats d_model"],
    feature_resid_dir_input: Float[Tensor, "feats d"],
    cache: ActivationCache,
    feature_out_dir: Float[Tensor, "feats d_out"] | None = None,
    corrcoef_neurons: RollingCorrCoef | None = None,
    corrcoef_sae: RollingCorrCoef | None = None,
    corrcoef_sae_B: RollingCorrCoef | None = None,
    linear_probes: list[
        tuple[Literal["input", "output"], str, Float[Tensor, "d_model d_vocab_out"]]
    ] = [],
    target_logits: Float[Tensor, "batch seq d_vocab_out"] | None = None,
    vocab_dict: dict[VocabType, dict[int, str]] | None = None,
    progress: list[tqdm] | None = None,
) -> tuple[SaeVisData, dict[str, float]]:
    """Convert generic activation data into a SaeVisData object, which can be used to create the feature-centric vis.
    Returns:
        sae_vis_data: SaeVisData
            Containing data for creating each feature visualization, as well as data for rank-ordering the feature
            visualizations when it comes time to make the prompt-centric view (the `feature_act_quantiles` attribute).
        time_logs: dict[str, float]
            A dictionary containing the time taken for each step of the computation. This is optionally printed at the
            end of the `get_feature_data` function, if `verbose` is set to True.
    # TODO - there's redundant docstrings here, each argument should only have a docstring in one function (the outer-most one)
    # TODO - this function was originally written so that there could be a fn that didn't use the saes and the models. But I don't know if that's really necessary any more, and maybe there's more funcs than we need?
    """
    acts_post_hook_name = f"{sae.cfg.hook_name}.hook_sae_acts_post"
    all_feat_acts = cache[acts_post_hook_name]
    time_logs = {
        "(2) Getting data for sequences": 0.0,
        "(3) Getting data for non-sequence components": 0.0,
    }
    t0 = time.monotonic()
    if target_logits is not None:
        assert (
            target_logits.shape[-1] < 1000
        ), "Not recommended to use target logits with a very large vocab size (this is intended for toy models e.g. OthelloGPT)"
        target_logits = target_logits.to(device)
    # Make feature_indices a list, for convenience
    if isinstance(feature_indices, int):
        feature_indices = [feature_indices]
    assert (
        feature_resid_dir.shape[0] == len(feature_indices)
    ), f"Num features in feature_resid_dir ({feature_resid_dir.shape[0]}) doesn't match {len(feature_indices)=}"
    if feature_out_dir is not None:
        assert (
            feature_out_dir.shape[0] == len(feature_indices)
        ), f"Num features in feature_out_dir ({feature_resid_dir.shape[0]}) doesn't match {len(feature_indices)=}"
    # ! Data setup code (defining the main objects we'll eventually return)
    feature_data_dict = {feat: {} for feat in feature_indices}
    # We're using `cfg.feature_centric_layout` to figure out what data we'll need to calculate during this function
    layout = cfg.feature_centric_layout
    assert isinstance(
        layout, SaeVisLayoutConfig
    ), f"Error: cfg.feature_centric_layout must be a SaeVisLayoutConfig object, got {type(layout)}"
    # ! Feature tables (i.e. left hand of vis)
    if layout.feature_tables_cfg is not None and feature_out_dir is not None:
        # Store kwargs (makes it easier to turn the tables on and off individually)
        feature_tables_data = {}
        # Table 1: neuron alignment, based on decoder weights
        if layout.feature_tables_cfg.neuron_alignment_table:
            top3_neurons_aligned = TopK(
                tensor=feature_out_dir, k=layout.feature_tables_cfg.n_rows, largest=True
            )
            feature_out_l1_norm = feature_out_dir.abs().sum(dim=-1, keepdim=True)
            pct_of_l1: Arr = np.absolute(top3_neurons_aligned.values) / utils.to_numpy(
                feature_out_l1_norm
            )
            feature_tables_data.update(
                neuron_alignment_indices=top3_neurons_aligned.indices.tolist(),
                neuron_alignment_values=top3_neurons_aligned.values.tolist(),
                neuron_alignment_l1=pct_of_l1.tolist(),
            )
        # Table 2: neurons correlated with this feature, based on their activations
        if isinstance(corrcoef_neurons, RollingCorrCoef):
            neuron_indices, neuron_pearson, neuron_cossim = (
                corrcoef_neurons.topk_pearson(
                    k=layout.feature_tables_cfg.n_rows,
                )
            )
            feature_tables_data.update(
                correlated_neurons_indices=neuron_indices,
                correlated_neurons_pearson=neuron_pearson,
                correlated_neurons_cossim=neuron_cossim,
            )
        # Table 3: primary sae features correlated with this feature, based on their activations
        if isinstance(corrcoef_sae, RollingCorrCoef):
            enc_indices, enc_pearson, enc_cossim = corrcoef_sae.topk_pearson(
                k=layout.feature_tables_cfg.n_rows,
            )
            feature_tables_data.update(
                correlated_features_indices=enc_indices,
                correlated_features_pearson=enc_pearson,
                correlated_features_cossim=enc_cossim,
            )
        # Table 4: sae-B features correlated with this feature, based on their activations
        if isinstance(corrcoef_sae_B, RollingCorrCoef):
            encB_indices, encB_pearson, encB_cossim = corrcoef_sae_B.topk_pearson(
                k=layout.feature_tables_cfg.n_rows,
            )
            feature_tables_data.update(
                correlated_b_features_indices=encB_indices,
                correlated_b_features_pearson=encB_pearson,
                correlated_b_features_cossim=encB_cossim,
            )
        # Add all this data to the list of FeatureTablesData objects
        for i, feat in enumerate(feature_indices):
            feature_data_dict[feat]["featureTables"] = FeatureTablesData(
                **{k: v[i] for k, v in feature_tables_data.items()}
            )
    # ! Histograms & logit tables & optional othello probes (i.e. middle column of vis)
    # Get the logits of all features (i.e. the directions this feature writes to the logit output)
    logits = einops.einsum(
        feature_resid_dir, model.W_U, "feats d_model, d_model d_vocab -> feats d_vocab"
    )
    probe_names_and_values = [
        (
            f"PROBE {name!r}, {mode.upper()} SPACE",
            einops.einsum(
                feature_resid_dir if mode == "output" else feature_resid_dir_input,
                probe,
                "feats d_model, d_model d_vocab_out -> feats d_vocab_out",
            ),
        )
        for mode, name, probe in linear_probes
    ]
    if any(
        x is not None
        for x in [layout.act_hist_cfg, layout.logits_hist_cfg, layout.logits_table_cfg]
    ):
        for i, feat in enumerate(feature_indices):
            # Get logits histogram data (no title)
            if layout.logits_hist_cfg is not None:
                # TODO - create more things like this, with `from_data` methods for the data-holding classes
                feature_data_dict[feat]["logitsHistogram"] = (
                    LogitsHistogramData.from_data(
                        data=logits[i],
                        n_bins=layout.logits_hist_cfg.n_bins,
                        tickmode="5 ticks",
                        title=None,
                    )
                )
            # Get data for feature activations histogram (including the title!)
            if layout.act_hist_cfg is not None:
                feat_acts = all_feat_acts[..., i]
                nonzero_feat_acts = feat_acts[feat_acts > 0]
                frac_nonzero = nonzero_feat_acts.numel() / feat_acts.numel()
                feature_data_dict[feat]["actsHistogram"] = ActsHistogramData.from_data(
                    data=nonzero_feat_acts,
                    n_bins=layout.act_hist_cfg.n_bins,
                    tickmode="5 ticks",
                    title=f"ACTIVATIONS<br><span style='color:#666;font-weight:normal'>DENSITY = {frac_nonzero:.3%}</span>",
                )
            # Get logits table data
            if layout.logits_table_cfg is not None:
                feature_data_dict[feat]["logitsTable"] = LogitsTableData.from_data(
                    logits[i], k=layout.logits_table_cfg.n_rows
                )
            # Optionally get probes data
            if layout.probe_logits_table_cfg is not None:
                feature_data_dict[feat]["probeLogitsTables"] = (
                    ProbeLogitsTableData.from_data(
                        probe_names_and_values=[
                            (name, logits[i]) for name, logits in probe_names_and_values
                        ],
                        k=layout.probe_logits_table_cfg.n_rows,
                    )
                )
    # ! Get stats (including quantiles, which will be useful for the prompt-centric visualisation)
    feature_stats = FeatureStatistics.create(
        data=einops.rearrange(all_feat_acts, "b s feats -> feats (b s)")
    )
    time_logs["(3) Getting data for non-sequence components"] = time.monotonic() - t0
    t0 = time.monotonic()
    # ! Sequences (i.e. right hand of vis)
    if layout.seq_cfg is not None:
        for i, feat in enumerate(feature_indices):
            # Add this feature's sequence data to the list
            feature_data_dict[feat]["seqMultiGroup"] = get_sequences_data(
                tokens=tokens,
                feat_idx=i,
                feat_logits=logits[i],
                cache=cache,
                feature_resid_dir=feature_resid_dir[i],
                model=model,
                sae=sae,
                target_logits=target_logits,
                seq_cfg=layout.seq_cfg,
            )
            # Update the 2nd progress bar (fwd passes & getting sequence data dominates the runtime of these computations)
            if progress is not None:
                progress[1].update(1)
    time_logs["(2) Getting data for sequences"] = time.monotonic() - t0
    t0 = time.monotonic()
    # ! Return the output, as a dict of FeatureData items
    sae_vis_data = SaeVisData(
        feature_data_dict=feature_data_dict,
        prompt_data_dict={},
        feature_stats=feature_stats,
        cfg=cfg,
        model=model,
        sae=sae,
        sae_B=sae_B,
        vocab_dict=vocab_dict,
    )
    return sae_vis_data, time_logs
@torch.inference_mode()
def _get_feature_data(
    sae: SAE,
    model: HookedSAETransformer,
    tokens: Int[Tensor, "batch seq"],
    feature_indices: int | list[int],
    cfg: SaeVisConfig,
    sae_B: SAE | None,
    linear_probes: list[
        tuple[Literal["input", "output"], str, Float[Tensor, "d_model d_vocab_out"]]
    ] = [],
    target_logits: Float[Tensor, "batch seq d_vocab_out"] | None = None,
    vocab_dict: dict[VocabType, dict[int, str]] | None = None,
    progress: list[tqdm] | None = None,
    clear_memory_between_batches: bool = False,
) -> tuple[SaeVisData, dict[str, float]]:
    """
    Gets data that will be used to create the sequences in the feature-centric HTML visualisation.
    Note - this function isn't called directly by the user, it actually gets called by the `get_feature_data` function
    which does exactly the same thing except it also batches this computation by features (in accordance with the
    arguments `features` and `minibatch_size_features` from the SaeVisConfig object).
    Returns:
        sae_vis_data: SaeVisData
            Containing data for creating each feature visualization, as well as data for
            rank-ordering the feature visualizations when it comes time to make the prompt-centric
            view (the `feature_act_quantiles` attribute).
        time_log: dict[str, float]
            A dictionary containing the time taken for each step of the computation. This is
            optionally printed at the end of the `get_feature_data` function, if `verbose=True`.
    """
    # ! Boring setup code
    time_logs = defaultdict(float)
    batch_size, seq_len = tokens.shape
    # Make feature_indices a list, for convenience
    if isinstance(feature_indices, int):
        feature_indices = [feature_indices]
    # Get tokens into minibatches, for the fwd pass. Same for target logits, if using them
    token_minibatches_list = (
        (tokens,)
        if cfg.minibatch_size_tokens is None
        else tokens.split(cfg.minibatch_size_tokens)
    )
    token_minibatches = [tok.to(device) for tok in token_minibatches_list]
    # ! Data setup code (defining the main objects we'll eventually return, for each of 5 possible vis components)
    # Create tensors to store the feature activations & final values of the residual stream
    seqpos_slice = slice(*cfg.seqpos_slice)
    resid_final_hook_name = utils.get_act_name("resid_post", model.cfg.n_layers - 1)
    acts_post_hook_name = f"{sae.cfg.hook_name}.hook_sae_acts_post"
    sae_input_hook_name = f"{sae.cfg.hook_name}.hook_sae_input"
    v_hook_name = utils.get_act_name("v", sae.cfg.hook_layer)
    pattern_hook_name = utils.get_act_name("pattern", sae.cfg.hook_layer)
    cache_dict = {
        resid_final_hook_name: torch.zeros(batch_size, seq_len, model.cfg.d_model),
        acts_post_hook_name: torch.zeros(batch_size, seq_len, len(feature_indices)),
        # sae_input_hook_name: torch.zeros(batch_size, seq_len, sae.cfg.d_in),
    }
    using_dfa = (
        (cfg.feature_centric_layout.seq_cfg is not None)
        and (cfg.feature_centric_layout.seq_cfg.dfa_for_attn_saes)
        and (sae.cfg.hook_name.endswith("_z"))
    )
    if using_dfa:
        cache_dict[v_hook_name] = torch.zeros(
            batch_size, seq_len, model.cfg.n_heads, model.cfg.d_head
        )
        cache_dict[pattern_hook_name] = torch.zeros(
            batch_size, model.cfg.n_heads, seq_len, seq_len
        )
    # Create objects to store the data for computing rolling stats
    sae_input_is_privileged = any(
        [sae.cfg.hook_name.endswith(x) for x in ["mlp.hook_pre", "mlp.hook_post"]]
    )
    corrcoef_neurons = RollingCorrCoef() if sae_input_is_privileged else None
    corrcoef_sae = RollingCorrCoef(indices=feature_indices, with_self=True)
    corrcoef_sae_B = RollingCorrCoef() if sae_B is not None else None
    # Get sae & decoder directions
    feature_out_dir = sae.W_dec[feature_indices]  # [feats d_sae]
    feature_resid_dir = to_resid_dir(feature_out_dir, sae, model)  # [feats d_model]
    feature_in_dir = sae.W_enc.T[feature_indices]  # [feats d_in]
    feature_resid_dir_input = to_resid_dir(
        feature_in_dir, sae, model, input=True
    )  # [feats d_model]
    # ! Compute & concatenate together all feature activations & post-activation function values
    start = 0
    for minibatch in token_minibatches:
        # Fwd pass, get model activations
        t0 = time.monotonic()
        sae.use_error_term = True
        _, cache = model.run_with_cache_with_saes(
            minibatch,
            saes=[sae],
            stop_at_layer=model.cfg.n_layers,
            names_filter=list(set(cache_dict.keys()) | {sae_input_hook_name}),
        )
        sae.use_error_term = False
        feat_acts_all = cache[acts_post_hook_name]  # [batch seq d_sae]
        feat_acts = cache[acts_post_hook_name][
            ..., feature_indices
        ]  # [batch seq d_sae]
        sae_input = cache[sae_input_hook_name]  # [batch seq d_in]
        time_logs["(1) Forward passes to gather model activations"] += (
            time.monotonic() - t0
        )
        if clear_memory_between_batches:
            t0 = time.monotonic()
            torch.cuda.empty_cache()
            time_logs["(1.5) Clearing memory"] = (
                time_logs.get("(1.5) Clearing memory", 0.0) + time.monotonic() - t0
            )
        # Update the CorrCoef object between feature activation & neurons
        if corrcoef_neurons is not None:
            corrcoef_neurons.update(feat_acts, sae_input)
        # Update the CorrCoef object between pairwise feature activations
        if corrcoef_sae is not None:
            corrcoef_sae.update(feat_acts, feat_acts_all)
        # Update the CorrCoef object between feature activation & sae-B features
        if corrcoef_sae_B is not None:
            assert sae_B is not None
            feat_acts_B = sae_B.encode(sae_input)  # [batch seq d_sae]
            corrcoef_sae_B.update(feat_acts, feat_acts_B)
        # Put these values into the tensors
        batch_slice = slice(start, start + len(minibatch))
        cache_dict[resid_final_hook_name][batch_slice, seqpos_slice] = cache[
            resid_final_hook_name
        ][:, seqpos_slice].cpu()
        cache_dict[acts_post_hook_name][batch_slice, seqpos_slice] = feat_acts[
            :, seqpos_slice
        ].cpu()
        if using_dfa:
            cache_dict[v_hook_name][batch_slice, seqpos_slice] = cache[v_hook_name][
                :, seqpos_slice
            ].cpu()
            cache_dict[pattern_hook_name][
                batch_slice, :, seqpos_slice, seqpos_slice
            ] = cache[pattern_hook_name][:, seqpos_slice, seqpos_slice].cpu()
        # Update the 1st progress bar (fwd passes & getting sequence data dominates the runtime of these computations)
        if progress is not None:
            progress[0].update(1)
        start += len(minibatch)
    torch.cuda.empty_cache()
    cache = ActivationCache(cache_dict, model=model)
    # ! Use the data we've collected to make a MultiFeatureData object
    sae_vis_data, _time_logs = parse_feature_data(
        model=model,
        cfg=cfg,
        sae=sae,
        sae_B=sae_B,
        tokens=tokens,
        feature_indices=feature_indices,
        feature_resid_dir=feature_resid_dir,
        feature_resid_dir_input=feature_resid_dir_input,
        cache=cache,
        feature_out_dir=feature_out_dir,
        corrcoef_neurons=corrcoef_neurons,
        corrcoef_sae=corrcoef_sae,
        corrcoef_sae_B=corrcoef_sae_B,
        linear_probes=linear_probes,
        target_logits=target_logits,
        vocab_dict=vocab_dict,
        progress=progress,
    )
    assert (
        set(time_logs.keys()) & set(_time_logs.keys()) == set()
    ), f"Invalid keys: {set(time_logs.keys()) & set(_time_logs.keys())} should have zero overlap"
    time_logs.update(_time_logs)
    return sae_vis_data, time_logs
@torch.inference_mode()
def get_feature_data(
    sae: SAE,
    model: HookedSAETransformer,
    tokens: Int[Tensor, "batch seq"],
    cfg: SaeVisConfig,
    sae_B: SAE | None = None,
    linear_probes: list[
        tuple[Literal["input", "output"], str, Float[Tensor, "d_model d_vocab_out"]]
    ] = [],
    target_logits: Float[Tensor, "batch seq d_vocab_out"] | None = None,
    vocab_dict: dict[VocabType, dict[int, str]] | None = None,
    verbose: bool = False,
    clear_memory_between_batches: bool = False,
) -> SaeVisData:
    """
    This is the main function which users will run to generate the feature visualization data. It
    batches this computation over features, in accordance with the arguments in the SaeVisConfig
    object (we don't want to compute all the features at once, since might give OOMs).
    See the `_get_feature_data` function for an explanation of the arguments, as well as a more
    detailed explanation of what this function is doing.
    The return object is the merged SaeVisData objects returned by the `_get_feature_data` function.
    """
    T0 = time.monotonic()
    # Apply random seed
    if cfg.seed is not None:
        torch.manual_seed(cfg.seed)
        np.random.seed(cfg.seed)
    # Create objects to store all the data we'll get from `_get_feature_data`
    sae_vis_data = SaeVisData(
        model=model,
        cfg=cfg,
        sae=sae,
        sae_B=sae_B,
        linear_probes=linear_probes,
        vocab_dict=vocab_dict,
    )
    time_logs = defaultdict(float)
    # Get a feature list (need to deal with the case where `cfg.features` is an int, or None)
    if cfg.features is None:
        assert isinstance(sae.cfg.d_sae, int)
        features_list = list(range(sae.cfg.d_sae))
    elif isinstance(cfg.features, int):
        features_list = [cfg.features]
    else:
        features_list = list(cfg.features)
    # Break up the features into batches
    feature_batches = [
        x.tolist()
        for x in torch.tensor(features_list).split(cfg.minibatch_size_features)
    ]
    # Calculate how many minibatches of tokens there will be (for the progress bar)
    n_token_batches = (
        1
        if (cfg.minibatch_size_tokens is None)
        else math.ceil(len(tokens) / cfg.minibatch_size_tokens)
    )
    # Get the denominator for each of the 2 progress bars
    totals = (n_token_batches * len(feature_batches), len(features_list))
    # Optionally add two progress bars (one for the forward passes, one for getting the sequence data)
    if verbose:
        progress = [
            tqdm(total=totals[0], desc="Forward passes to cache data for vis"),
            tqdm(total=totals[1], desc="Extracting vis data from cached data"),
        ]
    else:
        progress = None
    # For each batch of features: get new data and update global data storage objects
    for features in feature_batches:
        new_feature_data, new_time_logs = _get_feature_data(
            sae=sae,
            model=model,
            tokens=tokens,
            feature_indices=features,
            cfg=cfg,
            sae_B=sae_B,
            linear_probes=linear_probes,
            target_logits=target_logits,
            vocab_dict=vocab_dict,
            progress=progress,
            clear_memory_between_batches=clear_memory_between_batches,
        )
        sae_vis_data.update(new_feature_data)
        for key, value in new_time_logs.items():
            time_logs[key] += value
    # Now exited, make sure the progress bar is at 100%
    if progress is not None:
        for pbar in progress:
            pbar.n = pbar.total
    # If verbose, then print the output
    if verbose:
        time_logs["(?) Unaccounted time"] = (
            time.monotonic() - T0 - sum(time_logs.values())
        )
        total_time = sum(time_logs.values())
        table = Table("Task", "Time", "Pct %")
        for task, duration in time_logs.items():
            table.add_row(task, f"{duration:.2f}s", f"{duration/total_time:.1%}")
        rprint(table)
    return sae_vis_data
@torch.inference_mode()
def get_sequences_data(
    tokens: Int[Tensor, "batch seq"],
    feat_idx: int,  # the index in the batch, not the same as the feature id in the overall SAE
    feat_logits: Float[Tensor, "d_vocab"],
    cache: ActivationCache,
    feature_resid_dir: Float[Tensor, "d_model"],
    model: HookedSAETransformer,
    sae: SAE,
    target_logits: Float[Tensor, "batch seq d_vocab_out"] | None,
    seq_cfg: SeqMultiGroupConfig,
) -> SeqMultiGroupData:
    """
    This function returns the data which is used to create the top activating sequence visualizations. Steps are:
        (1) Get the indices of all example tokens we'll be taking (i.e. the bold tokens in the vis). This includes the
            top activations and all the quantiles.
        (2) Get the token IDs from all these indices.
        (3) Get other values at these positions & the surrounding buffer: feature acts, residual stream values
        (4) Compute the logit & logprob effect of this feature. Use this to also get the top affected tokens by this
            feat (i.e. the vis hoverdata).
        (5) If necessary, get the direct feature attribution info.
        (6) Return all this data as a SeqMultiGroupData object.
    Args:
        tokens:
            The tokens we'll be extracting sequence data from.
        feat_acts:
            The activations of the feature we're interested in, for each token in the batch.
        feat_logits:
            The logit vector for this feature (used to generate histogram, and is needed here for the line-on-hover).
        resid_final:
            The residual stream values before final layernorm, for each token in the batch.
        feature_resid_dir:
            The direction this feature writes to the logit output (i.e. the direction we'll be erasing from resid_final).
        feature_resid_dir_input:
            The input direction (i.e. we dot this with residual stream to get this feature's activation).
        W_U:
            The model's unembedding matrix, which we'll use to get the logits.
        cfg:
            Feature visualization parameters, containing some important params e.g. num sequences per group.
    Returns:
        SeqMultiGroupData
            This is a dataclass which contains a dict of SeqGroupData objects, where each SeqGroupData object
            contains the data for a particular group of sequences (i.e. the top-k, bottom-k, and the quantile groups).
    """
    resid_final_hook_name = utils.get_act_name(
        "resid_post", layer=model.cfg.n_layers - 1
    )
    acts_post_hook_name = f"{sae.cfg.hook_name}.hook_sae_acts_post"
    # sae_acts_pre_hook_name = f"{sae.cfg.hook_name}.hook_sae_acts_pre"
    v_hook_name = utils.get_act_name("v", layer=sae.cfg.hook_layer)
    pattern_hook_name = utils.get_act_name("pattern", layer=sae.cfg.hook_layer)
    resid_post = cache[resid_final_hook_name]
    feat_acts = cache[acts_post_hook_name][..., feat_idx]
    # ! (1) Find the tokens from each group
    # We define our full buffer going 1 token further back than all the visible tokens. This is because suppose we have
    # visible tokens [0, 1, 2, 3]: we need to compute feature activations from [0, 1, 2, 3] to highlight the tokens, but
    # we also need to compute residual stream & feature acts from [-1, 0, 1, 2] so we can ablate that feature from the
    # residual stream & see loss effect on all visible tokens.
    buffer_to_exclude_from_ex = (
        (0, -1)
        if seq_cfg.buffer is None
        else (seq_cfg.buffer[0] + 1, -seq_cfg.buffer[1])
    )
    feat_acts_max = feat_acts[
        :, buffer_to_exclude_from_ex[0] : buffer_to_exclude_from_ex[1]
    ].max()
    # Get the top-activating tokens
    indices = k_largest_indices(
        feat_acts,
        k=seq_cfg.top_acts_group_size,
        buffer=buffer_to_exclude_from_ex,
    )
    use_dfa = seq_cfg.dfa_for_attn_saes and sae.cfg.hook_name.endswith("hook_z")
    first_title = (
        "TOP ACTIVATIONS (right) & DFA (left)" if use_dfa else "TOP ACTIVATIONS"
    )
    indices_dict = {
        f"{first_title}<br><span style='color:#666;font-weight:normal'>MAX ACT = {feat_acts_max:.3f}</span>": indices
    }
    # Get all possible indices. Note, we need to be able to look 1 back (feature activation on prev token is needed for
    # computing loss effect on this token)
    if seq_cfg.n_quantiles > 0:
        quantiles = torch.linspace(0, feat_acts_max.item(), seq_cfg.n_quantiles + 1)
        quantiles[0] = 1e-6
        n_active = (feat_acts > 1e-6).float().sum()
        for i in range(seq_cfg.n_quantiles - 1, -1, -1):
            lower, upper = quantiles[i : i + 2].tolist()
            pct = ((feat_acts >= lower) & (feat_acts <= upper)).float().sum() / n_active
            indices = random_range_indices(
                feat_acts,
                k=seq_cfg.quantile_group_size,
                bounds=(lower, upper),
                buffer=buffer_to_exclude_from_ex,
            )
            indices_dict[
                f"INTERVAL {lower:.3f} - {upper:.3f}<br><span style='color:#666;font-weight:normal'>CONTAINS {pct:.3%}</span>"
            ] = indices
    # Concat all the indices together (in the next steps we do all groups at once). Shape of this object is [n_ex 2],
    # i.e. the [i, :]-th element are the batch and sequence dimensions for the i-th example.
    indices_ex = torch.concat(list(indices_dict.values())).cpu()
    n_ex = indices_ex.shape[0]
    # ! (2) Get our top tokens, and what we're displaying
    if seq_cfg.buffer is not None:
        # Index into the sequence with a buffer. We get all tokens in buffer, plus the token 1 further back (although we
        # won't be displaying this token in the final vis; it's just for getting effect on loss).
        buffer_full = (seq_cfg.buffer[0] + 1, -seq_cfg.buffer[1])
        token_ids = index_with_buffer(tokens, indices_ex, buffer=buffer_full)
        token_ids_to_display = token_ids[:, 1:]
    else:
        # If we don't specify a sequence, then do all of the seq positions in each seq we pick. In this case, our
        # display seqs are literally just the full sequences from bold tokens.
        buffer_full = None
        token_ids = index_with_buffer(tokens, indices_ex, buffer=None)
        token_ids_to_display = token_ids
    # ! (3) Extract feature activations & residual stream values for those positions
    # Note - the reason we split on cases here is that when computing the buffer, we need activations & loss effect for
    # all tokens (and loss effect requires us to compute activations & resid values 1 token back). But when we aren't
    # computing the buffer, we only need activations for bold token & loss effect on the token after that one.
    if seq_cfg.compute_buffer:
        # Get tokens we'll use to index correct logits (all the visible ones)
        token_ids_for_computing_loss = token_ids[:, 1:]
        # Get feature acts for all sequence positions (all visible used for coloring, 1 back used for loss)
        feat_acts_buf = index_with_buffer(feat_acts, indices_ex, buffer=buffer_full).to(
            device
        )
        feat_acts_pre_ablation = feat_acts_buf[:, :-1]
        feat_acts_coloring = feat_acts_buf[:, 1:]
        feat_acts_idx = [None for _ in range(n_ex)]
        # Get residual stream for all sequence positions that come immediately before a visible token (used for loss)
        resid_post = index_with_buffer(resid_post, indices_ex, buffer=buffer_full)[
            :, :-1
        ].to(device)
    else:
        # Get tokens we'll use to index correct logits (after the bold ones)
        token_ids_for_computing_loss = (
            index_with_buffer(tokens, indices_ex, buffer=0, offset=1)
            .unsqueeze(1)
            .to(device)
        )
        # Get feature acts for just the bold tokens (used for coloring on bold token & loss on token after bold token)
        feat_acts_pre_ablation = (
            index_with_buffer(feat_acts, indices_ex, buffer=0).unsqueeze(-1).to(device)
        )
        feat_acts_coloring = feat_acts_pre_ablation
        feat_acts_idx = indices_ex[
            :, 1
        ].tolist()  # need to remember which one is the bold one!
        # Get residual stream for just the bold tokens (used for loss on token after bold token)
        resid_post = (
            index_with_buffer(resid_post, indices_ex, buffer=0).unsqueeze(1).to(device)
        )
    # ! (4) Compute the logit effect if this feature is ablated
    # Get this feature's output vector, using an outer product over the feature activations for all tokens
    resid_post_feature_effect = einops.einsum(
        feat_acts_pre_ablation,
        feature_resid_dir.to(device),
        "n_ex buf, d_model -> n_ex buf d_model",
    )
    # Contribution to logits is computed without normalization
    logit_contribution = resid_post_feature_effect @ model.W_U
    # Do the ablations, and get difference in logprobs (direct effect)
    new_logits = resid_final_pre_layernorm_to_logits(
        resid_post - resid_post_feature_effect, model
    )
    orig_logits = resid_final_pre_layernorm_to_logits(resid_post, model)
    logprobs_contribution = orig_logits.log_softmax(-1) - new_logits.log_softmax(-1)
    orig_prob = orig_logits.softmax(-1)
    new_prob = new_logits.softmax(-1)
    # The TopK function can improve efficiency by masking the features which are zero
    acts_nonzero = feat_acts_pre_ablation.abs() > 1e-5  # shape [batch buf]
    top_logit_contribution = TopK(
        logprobs_contribution, k=seq_cfg.top_logits_hoverdata, tensor_mask=acts_nonzero
    )
    bottom_logit_contribution = TopK(
        logprobs_contribution,
        k=seq_cfg.top_logits_hoverdata,
        tensor_mask=acts_nonzero,
        largest=False,
    )
    if target_logits is None:
        # loss_cont[b, s] = -logprobs_cont[b, s, token_ids_for_computing_loss[b, s]]
        loss_contribution = -eindex(
            logprobs_contribution, token_ids_for_computing_loss, "batch seq [batch seq]"
        )
        logit_contribution = eindex(
            logit_contribution, token_ids_for_computing_loss, "batch seq [batch seq]"
        )
        orig_prob = eindex(
            orig_prob, token_ids_for_computing_loss, "batch seq [batch seq]"
        )
        new_prob = eindex(
            new_prob, token_ids_for_computing_loss, "batch seq [batch seq]"
        )
    else:
        assert not seq_cfg.compute_buffer, "Not expecting to compute buffer if using target logits (it's more indexing hassle)"
        target_logits_bold = eindex(
            target_logits, indices_ex, "[n_ex 0] [n_ex 1] d_vocab"
        ).unsqueeze(1)
        loss_orig = cross_entropy_loss(orig_logits, target_logits_bold)
        loss_new = cross_entropy_loss(new_logits, target_logits_bold)
        loss_contribution = loss_orig - loss_new
        logit_contribution = orig_prob = new_prob = None
    # ! (5) If this is an attention SAE, then do DFA
    if use_dfa:
        assert seq_cfg.dfa_buffer is not None
        indices_batch, indices_dest = indices_ex.unbind(dim=-1)
        v = cache[v_hook_name][indices_batch].to(device)  # [k src n_heads d_head]
        pattern = cache[pattern_hook_name][indices_batch, :, indices_dest].to(
            device
        )  # [k n_heads src]
        v_weighted = (
            v * einops.rearrange(pattern, "k n_heads src -> k src n_heads 1")
        ).flatten(-2)  # [k src d_in]
        dfa = v_weighted @ sae.W_enc[:, feat_idx]  # [k src]
        indices_src = dfa.argmax(dim=-1).to(indices_ex.device)  # [k,]
        indices_ex_src = torch.stack([indices_batch, indices_src], dim=-1)
        indices_ex_src[indices_ex_src[:, 1] < seq_cfg.dfa_buffer[0]] = (
            seq_cfg.dfa_buffer[0]
        )
        dfa_buffer = (seq_cfg.dfa_buffer[0], -seq_cfg.dfa_buffer[1])
        dfa_token_ids = index_with_buffer(tokens, indices_ex_src, buffer=dfa_buffer)
        indices_ex_src_for_values = torch.stack(
            [torch.arange(len(indices_src)), indices_src.cpu()], dim=-1
        )
        dfa_values = index_with_buffer(
            dfa, indices_ex_src_for_values, buffer=dfa_buffer
        )
    else:
        dfa_token_ids = dfa_values = indices_ex_src = None
    # ! (6) Store the results in a SeqMultiGroupData object
    sequence_groups_data = []
    group_sizes_cumsum = np.cumsum(
        [0] + [len(indices) for indices in indices_dict.values()]
    ).tolist()
    buffer_range = (
        range(-seq_cfg.buffer[0], seq_cfg.buffer[1] + 1)
        if seq_cfg.buffer is not None
        else range(tokens.shape[1])
    )
    dfa_buffer_range = (
        range(-seq_cfg.dfa_buffer[0], seq_cfg.dfa_buffer[1] + 1)
        if seq_cfg.dfa_buffer is not None
        else range(tokens.shape[1])
    )
    for group_idx, group_name in enumerate(indices_dict.keys()):
        seq_data = [
            SequenceData(
                token_ids=token_ids_to_display[i].tolist(),
                token_posns=[
                    f"({indices_ex[i, 0]}, {indices_ex[i, 1] + j})"
                    for j in buffer_range
                ],
                feat_acts=[round(f, 4) for f in feat_acts_coloring[i].tolist()],
                feat_acts_idx=feat_acts_idx[i],
                loss_contribution=loss_contribution[i].tolist(),
                logit_contribution=logit_contribution[i].tolist()
                if logit_contribution is not None
                else None,
                orig_prob=orig_prob[i].tolist() if orig_prob is not None else None,
                new_prob=new_prob[i].tolist() if new_prob is not None else None,
                token_logits=feat_logits[token_ids_to_display[i]].tolist(),
                top_token_ids=top_logit_contribution.indices[i].tolist(),
                top_logits=top_logit_contribution.values[i].tolist(),
                bottom_token_ids=bottom_logit_contribution.indices[i].tolist(),
                bottom_logits=bottom_logit_contribution.values[i].tolist(),
                dfa_token_ids=dfa_token_ids[i].tolist()
                if dfa_token_ids is not None
                else [],
                dfa_values=dfa_values[i].tolist() if dfa_values is not None else [],
                dfa_token_posns=[
                    f"({indices_ex_src[i, 0]}, {indices_ex_src[i, 1] + j})"
                    for j in dfa_buffer_range
                ]
                if indices_ex_src is not None
                else [],
            )
            for i in range(
                group_sizes_cumsum[group_idx], group_sizes_cumsum[group_idx + 1]
            )
        ]
        sequence_groups_data.append(SeqGroupData(seq_data, title=group_name))
    return SeqMultiGroupData(sequence_groups_data)
@torch.inference_mode()
def parse_prompt_data(
    tokens: Int[Tensor, "batch seq"],
    str_toks: list[str],
    sae_vis_data: SaeVisData,
    feat_acts: Float[Tensor, "seq feats"],
    feature_resid_dir: Float[Tensor, "feats d_model"],
    resid_post: Float[Tensor, "seq d_model"],
    model: HookedTransformer,
    feature_idx: list[int] | None = None,
    num_top_features: int = 10,
) -> tuple[
    dict[str, list[dict[Literal["feature", "title"], int | str]]],
    dict[int, SeqMultiGroupData],
]:
    """
    Gets data needed to create the sequences in the prompt-centric vis (displaying dashboards for the most relevant
    features on a prompt).
    Returns:
        scores_dict: dict[str, list[dict[Literal["feature", "title"], int | str]]]
            A dictionary mapping keys like "act_quantile|'django' (0)" to a list of tuples, each of
            the form (feature_idx, title).
        prompt_data_dict: dict[int, SeqMultiGroupData]
            A dictionary mapping feature index to the sequence multigroup data for that feature. Note
            that it will only contain data for a single prompt, but we wrap it in this for consistency.
    """
    # Populate our initial dictionaries
    seq_keys = [f"{t!r} ({i})" for i, t in enumerate(str_toks)]
    metrics = ["act_size", "act_quantile", "loss_effect"]
    scores_dict: dict[str, list[dict[Literal["feature", "title"], int | str]]] = {
        f"{metric}|{seq_key}": []
        for metric, seq_key in itertools.product(metrics, seq_keys)
    }
    prompt_data_dict: dict[int, SeqMultiGroupData] = {}
    if feature_idx is None:
        feature_idx = list(sae_vis_data.feature_data_dict.keys())
    n_feats = len(feature_idx)
    assert (
        feature_resid_dir.shape[0] == n_feats
    ), f"The number of features in feature_resid_dir ({feature_resid_dir.shape[0]}) does not match the number of feature indices ({n_feats})"
    assert (
        feat_acts.shape[1] == n_feats
    ), f"The number of features in feat_acts ({feat_acts.shape[1]}) does not match the number of feature indices ({n_feats})"
    feats_loss_contribution = torch.empty(
        size=(n_feats, tokens.shape[1] - 1), device=device
    )
    # Some logit computations which we only need to do once
    # correct_token_unembeddings = model_wrapped.W_U[:, tokens[0, 1:]] # [d_model seq]
    orig_logits = (
        resid_post / resid_post.std(dim=-1, keepdim=True)
    ) @ model.W_U  # [seq d_vocab]
    raw_logits = feature_resid_dir @ model.W_U  # [feats d_vocab]
    for i, feat in enumerate(feature_idx):
        # ! Calculate the sequence data for each feature, and store it as FeatureData.prompt_data
        # Get this feature's output vector, using an outer product over the feature activations for all tokens
        resid_post_feature_effect = einops.einsum(
            feat_acts[:, i], feature_resid_dir[i], "seq, d_model -> seq d_model"
        )
        # Ablate the output vector from the residual stream, and get logits post-ablation
        new_resid_post = resid_post - resid_post_feature_effect
        new_logits = (
            new_resid_post / new_resid_post.std(dim=-1, keepdim=True)
        ) @ model.W_U
        # Get the top5 & bottom5 changes in logits (don't bother with `efficient_topk` cause it's small)
        contribution_to_logprobs = orig_logits.log_softmax(
            dim=-1
        ) - new_logits.log_softmax(dim=-1)
        top_contribution_to_logits = TopK(contribution_to_logprobs[:-1], k=5)
        bottom_contribution_to_logits = TopK(
            contribution_to_logprobs[:-1], k=5, largest=False
        )
        # Get the change in logprobs (unnormalized) and loss (which is negative of change of logprobs for correct token)
        logit_contribution = einops.einsum(
            resid_post_feature_effect[:-1],
            model.W_U[:, tokens[0, 1:]],
            "seq d_model, d_model seq -> seq",
        )
        loss_contribution = eindex(
            -contribution_to_logprobs[:-1], tokens[0, 1:], "seq [seq]"
        )
        feats_loss_contribution[i, :] = loss_contribution
        # Store the sequence data, wrapped in a multi-group which only has one element
        prompt_data_dict[feat] = SeqMultiGroupData(
            [
                SeqGroupData(
                    [
                        SequenceData(
                            token_ids=tokens.squeeze(0).tolist(),
                            feat_acts=[round(f, 4) for f in feat_acts[:, i].tolist()],
                            loss_contribution=loss_contribution.tolist(),
                            logit_contribution=logit_contribution.tolist(),
                            token_logits=raw_logits[i, tokens.squeeze(0)].tolist(),
                            top_token_ids=top_contribution_to_logits.indices.tolist(),
                            top_logits=top_contribution_to_logits.values.tolist(),
                            bottom_token_ids=bottom_contribution_to_logits.indices.tolist(),
                            bottom_logits=bottom_contribution_to_logits.values.tolist(),
                        )
                    ]
                )
            ],
            is_prompt=True,
        )
    # ! Lastly, return a dictionary mapping each key like 'act_quantile|"django" (0)' to a list of feature indices & scores
    def title(metric: str, feat: int, score_str: str):
        return f"<h3>#{feat}<br>{METRIC_TITLES[metric]} = {score_str}</h3><hr>"
    for seq_pos, seq_key in enumerate(seq_keys):
        # Filter the feature activations, since we only need the ones that are non-zero
        feat_acts_nonzero_filter = utils.to_numpy(feat_acts[seq_pos] > 0)
        feat_acts_nonzero_locations = np.nonzero(feat_acts_nonzero_filter)[0].tolist()
        _feat_acts = feat_acts[seq_pos, feat_acts_nonzero_filter]  # [feats_filtered,]
        _feature_idx = np.array(feature_idx)[feat_acts_nonzero_filter]
        if feat_acts_nonzero_filter.sum() > 0:
            k = min(num_top_features, _feat_acts.numel())
            # Get the top features by activation size. This is just applying a TopK function to feat
            # acts (which were stored by the code before this)
            metric = "act_size"
            topk = TopK(_feat_acts, k=k, largest=True)
            scores_dict[f"{metric}|{seq_key}"] = [
                {"feature": feat, "title": title(metric, feat, score)}
                for feat, score in zip(
                    _feature_idx[topk.indices].tolist(),
                    [f"{v:.3f}" for v in topk.values.tolist()],
                )
            ]
            # Get the top features by activation quantile. We do this using the `feature_act_quantiles` object, which
            # was stored `sae_vis_data`. This quantiles object has a method to return quantiles for a given set of
            # data, as well as the precision (we make the precision higher for quantiles closer to 100%, because these
            # are usually the quantiles we're interested in, and it lets us to save space in `feature_act_quantiles`).
            metric = "act_quantile"
            act_quantile, act_precision = sae_vis_data.feature_stats.get_quantile(
                _feat_acts, feat_acts_nonzero_locations
            )
            topk = TopK(act_quantile, k=k, largest=True)
            act_formatting = [f".{act_precision[i]-2}%" for i in topk.indices]
            scores_dict[f"{metric}|{seq_key}"] = [
                {"feature": feat, "title": title(metric, feat, score)}
                for feat, score in zip(
                    _feature_idx[topk.indices].tolist(),
                    [f"{v:{f}}" for v, f in zip(topk.values.tolist(), act_formatting)],
                )
            ]
        # We don't measure loss effect on the first token
        if seq_pos == 0:
            continue
        # Filter the loss effects, since we only need the ones which have non-zero feature acts on the tokens before them
        prev_feat_acts_nonzero_filter = utils.to_numpy(feat_acts[seq_pos - 1] > 0)
        _loss_contribution = feats_loss_contribution[
            prev_feat_acts_nonzero_filter, seq_pos - 1
        ]  # [feats_filtered,]
        _feature_idx_prev = np.array(feature_idx)[prev_feat_acts_nonzero_filter]
        if prev_feat_acts_nonzero_filter.sum() > 0:
            k = min(num_top_features, _loss_contribution.numel())
            # Get the top features by loss effect. This is just applying a TopK function to the loss effects (which were
            # stored by the code before this). The loss effects are formatted to 3dp. We look for the most negative
            # values, i.e. the most loss-reducing features.
            metric = "loss_effect"
            topk = TopK(_loss_contribution, k=k, largest=False)
            scores_dict[f"{metric}|{seq_key}"] = [
                {"feature": feat, "title": title(metric, feat, score)}
                for feat, score in zip(
                    _feature_idx_prev[topk.indices].tolist(),
                    [f"{v:+.3f}" for v in topk.values.tolist()],
                )
            ]
    return scores_dict, prompt_data_dict
@torch.inference_mode()
def get_prompt_data(
    sae_vis_data: SaeVisData,
    prompt: str,
    num_top_features: int,
) -> dict[str, list[dict[Literal["feature", "title"], int | str]]]:
    """
    Does 2 things:
        (1) Adds "promptData" into the SaeVisData object's feature dict, so it can be used in the prompt-centric layout
        (2) Returns a dictionary mapping score keys (stringified) to a sorted list of feature IDs & their titles
    We do this simultaneously for every scoring metric & every seq pos in the prompt.
    """
    features = list(sae_vis_data.feature_data_dict.keys())
    sae = sae_vis_data.sae
    assert isinstance(sae, SAE)
    model = sae_vis_data.model
    assert isinstance(model, HookedSAETransformer)
    str_toks: list[str] = model.tokenizer.tokenize(prompt)  # type: ignore
    tokens = model.tokenizer.encode(prompt, return_tensors="pt").to(device)  # type: ignore
    assert isinstance(tokens, torch.Tensor)
    feature_act_dir = sae.W_enc[:, features]  # [d_in feats]
    feature_out_dir = sae.W_dec[features]  # [feats d_in]
    feature_resid_dir = to_resid_dir(feature_out_dir, sae, model)  # [feats d_model]
    assert (
        feature_act_dir.T.shape
        == feature_out_dir.shape
        == (len(features), sae.cfg.d_in)
    )
    resid_final_hook_name = utils.get_act_name("resid_post", model.cfg.n_layers - 1)
    acts_post_hook_name = f"{sae.cfg.hook_name}.hook_sae_acts_post"
    sae.use_error_term = True
    _, cache = model.run_with_cache_with_saes(
        tokens,
        saes=[sae],
        stop_at_layer=model.cfg.n_layers,
        names_filter=[acts_post_hook_name, resid_final_hook_name],
        remove_batch_dim=True,
    )
    sae.use_error_term = False
    scores_dict, prompt_data_dict = parse_prompt_data(
        tokens=tokens,
        str_toks=str_toks,
        sae_vis_data=sae_vis_data,
        feat_acts=cache[acts_post_hook_name][:, features],
        feature_resid_dir=feature_resid_dir,
        resid_post=cache[resid_final_hook_name],
        model=model,
        feature_idx=features,
        num_top_features=num_top_features,
    )
    # Set prompt data in feature_data_dict, and return scores dict
    for feature_idx, prompt_data in prompt_data_dict.items():
        sae_vis_data.feature_data_dict[feature_idx]["prompt"] = prompt_data
    return scores_dict

================
File: sae_vis/data_storing_fns.py
================
import json
import random
import re
from copy import deepcopy
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Callable, Literal
import numpy as np
from jaxtyping import Float, Int
from sae_lens import SAE, HookedSAETransformer
from torch import Tensor
from tqdm.auto import tqdm
from sae_vis.data_config_classes import (
    PromptConfig,
    SaeVisConfig,
    SaeVisLayoutConfig,
    SeqMultiGroupConfig,
)
from sae_vis.utils_fns import (
    FeatureStatistics,
    HistogramData,
    TopK,
    VocabType,
    compute_othello_board_state_and_valid_moves,
    get_decode_html_safe_fn,
    max_or_1,
    to_str_tokens,
    unprocess_str_tok,
)
PRECISION = 4
def round_1d_list(lst: list[float], precision: int = PRECISION) -> list[float]:
    return [round(f, precision) for f in lst]
def round_2d_list(
    lst: list[list[float]], precision: int = PRECISION
) -> list[list[float]]:
    return [[round(f, precision) for f in floats] for floats in lst]
@dataclass
class FeatureTablesData:
    """
    This contains all the data necessary to make the left-hand tables in prompt-centric visualization. See diagram
    in readme:
        https://github.com/callummcdougall/sae_vis#data_storing_fnspy
    Inputs:
        neuron_alignment...
            The data for the neuron alignment table (each of its 3 cols). In other words, the data
            containing which neurons in the transformer the sae feature is most aligned with.
        correlated_neurons...
            The data for the correlated neurons table (each of its 3 cols). In other words, the data
            containing which neurons in the transformer are most correlated with the sae feature.
        correlated_features...
            The data for the correlated features table (each of its 3 cols). In other words, the data
            containing which features in this sae are most correlated with each other.
        correlated_b_features...
            The data for the correlated features table (each of its 3 cols). In other words, the data
            containing which features in sae-B are most correlated with those in the original sae. Note,
            this one might be absent if we're not using a B-sae.
    """
    neuron_alignment_indices: list[int] = field(default_factory=list)
    neuron_alignment_values: list[float] = field(default_factory=list)
    neuron_alignment_l1: list[float] = field(default_factory=list)
    correlated_neurons_indices: list[int] = field(default_factory=list)
    correlated_neurons_pearson: list[float] = field(default_factory=list)
    correlated_neurons_cossim: list[float] = field(default_factory=list)
    correlated_features_indices: list[int] = field(default_factory=list)
    correlated_features_pearson: list[float] = field(default_factory=list)
    correlated_features_cossim: list[float] = field(default_factory=list)
    correlated_b_features_indices: list[int] = field(default_factory=list)
    correlated_b_features_pearson: list[float] = field(default_factory=list)
    correlated_b_features_cossim: list[float] = field(default_factory=list)
    def data(
        self,
        layout: SaeVisLayoutConfig,
        decode_fn: Callable[[int | list[int], VocabType], str | list[str]],
        component_specific_kwargs: dict[str, Any] = {},
    ) -> dict[str, Any]:
        """
        Returns the HTML for the left-hand tables, wrapped in a 'grid-column' div.
        Note, we only ever use this obj in the context of the left-hand column of the feature-centric vis, and it's
        always the same width & height, which is why there's no customization available for this function.
        """
        cfg = layout.feature_tables_cfg
        assert (
            cfg is not None
        ), "Calling `FeatureTablesData.data`, but with no vis config for that component."
        data = {}
        # Store the neuron alignment data, if it exists
        if len(self.neuron_alignment_indices) > 0:
            assert len(self.neuron_alignment_indices) >= cfg.n_rows, "Not enough rows!"
            data["neuronAlignment"] = [
                {"index": I, "value": f"{V:+.3f}", "percentageL1": f"{L:.1%}"}
                for I, V, L in zip(
                    self.neuron_alignment_indices,
                    self.neuron_alignment_values,
                    self.neuron_alignment_l1,
                )
            ]
        # Store the other 3, if they exist (they're all in the same format, so we can do it in a for loop)
        for name, js_name in zip(
            ["correlated_neurons", "correlated_features", "correlated_b_features"],
            ["correlatedNeurons", "correlatedFeatures", "correlatedFeaturesB"],
        ):
            if len(getattr(self, f"{name}_indices")) > 0:
                # assert len(getattr(self, f"{name}_indices")) >= cfg.n_rows, "Not enough rows!"
                data[js_name] = [
                    {"index": I, "value": f"{P:+.3f}", "percentageL1": f"{C:+.3f}"}
                    for I, P, C in zip(
                        getattr(self, f"{name}_indices")[: cfg.n_rows],
                        getattr(self, f"{name}_pearson")[: cfg.n_rows],
                        getattr(self, f"{name}_cossim")[: cfg.n_rows],
                    )
                ]
        return data
@dataclass
class ActsHistogramData(HistogramData):
    def data(
        self,
        layout: SaeVisLayoutConfig,
        decode_fn: Callable[[int | list[int], VocabType], str | list[str]],
        component_specific_kwargs: dict[str, Any] = {},
    ) -> dict[str, Any]:
        """
        Converts data -> HTML object, for the feature activations histogram (i.e. the histogram over all sampled tokens,
        showing the distribution of activations for this feature).
        """
        cfg = layout.act_hist_cfg
        assert (
            cfg is not None
        ), "Calling `ActsHistogramData.data`, but with no vis config for that component."
        # We can't post-hoc change the number of bins, so check this wasn't changed in the config
        # assert cfg.n_bins == len(self.bar_heights),\
        #     "Can't post-hoc change `n_bins` in histogram config - you need to regenerate data."
        return {
            "y": self.bar_heights,
            "x": self.bar_values,
            "ticks": self.tick_vals,
            "title": self.title if self.title is not None else False,
        }
@dataclass
class LogitsHistogramData(HistogramData):
    def data(
        self,
        layout: SaeVisLayoutConfig,
        decode_fn: Callable[[int | list[int], VocabType], str | list[str]],
        component_specific_kwargs: dict[str, Any] = {},
    ) -> dict[str, Any]:
        """
        Converts data -> HTML object, for the logits histogram (i.e. the histogram over all tokens in the vocab, showing
        the distribution of direct logit effect on that token).
        """
        cfg = layout.logits_hist_cfg
        assert (
            cfg is not None
        ), "Calling `LogitsHistogramData.data`, but with no vis config for that component."
        # We can't post-hoc change the number of bins, so check this wasn't changed in the config
        # assert cfg.n_bins == len(self.bar_heights),\
        #     "Can't post-hoc change `n_bins` in histogram config - you need to regenerate data."
        return {
            "y": self.bar_heights,
            "x": self.bar_values,
            "ticks": self.tick_vals,
            "title": self.title if self.title is not None else False,
        }
@dataclass
class LogitsTableData:
    bottom_token_ids: list[int] = field(default_factory=list)
    bottom_logits: list[float] = field(default_factory=list)
    top_token_ids: list[int] = field(default_factory=list)
    top_logits: list[float] = field(default_factory=list)
    vocab_type: VocabType = "unembed"
    max_logits: float | None = None
    @classmethod
    def from_data(
        cls, logits: Float[Tensor, "d_vocab"], k: int, max_logits: float | None = None
    ) -> "LogitsTableData":
        # Get logits table data
        top_logits = TopK(logits, k)
        bottom_logits = TopK(logits, k, largest=False)
        return LogitsTableData(
            bottom_logits=bottom_logits.values.tolist(),
            bottom_token_ids=bottom_logits.indices.tolist(),
            top_logits=top_logits.values.tolist(),
            top_token_ids=top_logits.indices.tolist(),
            vocab_type="unembed",
            max_logits=max_logits,
        )
    def data(
        self,
        layout: SaeVisLayoutConfig,
        decode_fn: Callable[[int | list[int], VocabType], str | list[str]],
        component_specific_kwargs: dict[str, Any] = {},
    ) -> dict[str, Any]:
        """
        Converts data -> HTML object, for the logits table (i.e. the top and bottom affected tokens by this feature).
        """
        cfg = layout.logits_table_cfg
        assert (
            cfg is not None
        ), "Calling `LogitsTableData.data`, but with no vis config for that component."
        # Crop the lists to `cfg.n_rows` (first checking the config doesn't ask for more rows than we have)
        assert cfg.n_rows <= len(self.bottom_logits)
        # Get the string tokens, using the decode function (unembed mode or probes mode!)
        neg_str = to_str_tokens(
            self.bottom_token_ids[: cfg.n_rows], decode_fn, self.vocab_type
        )
        pos_str = to_str_tokens(
            self.top_token_ids[: cfg.n_rows], decode_fn, self.vocab_type
        )
        # Get max loss (might be part of table data)
        max_logits = self.max_logits or max(
            0.0, self.top_logits[0], -self.bottom_logits[0]
        )
        # Get data for the tables of pos/neg logits
        return {
            "negLogits": [
                {
                    "symbol": unprocess_str_tok(neg_str[i]),
                    "value": round(self.bottom_logits[i], 2),
                }
                for i in range(len(neg_str))
            ],
            "posLogits": [
                {
                    "symbol": unprocess_str_tok(pos_str[i]),
                    "value": round(self.top_logits[i], 2),
                }
                for i in range(len(pos_str))
            ],
            "maxLogits": round(max_logits, PRECISION),
        }
@dataclass
class ProbeLogitsTableData:
    """
    Basically a wrapper for LogitsTableData, used when we have multiple probes (each with different
    names), but we treat those probes basically like we treat the unembedding when we're getting top
    logits.
    """
    probe_logits_data: dict[str, LogitsTableData] = field(default_factory=dict)
    @classmethod
    def from_data(
        cls,
        probe_names_and_values: list[tuple[str, Float[Tensor, "d_vocab"]]],
        k: int,
        max_logits: float | None = None,
    ) -> "ProbeLogitsTableData":
        """
        Each value in the logits dict is a single logit vector for the corresponding probe.
        # TODO - I like this method, I should make more methods work like this! Nice to have the complexity in classes, not functions. Don't really care about saving these classes, cause I save the JSON data now instead. Also, even if I wanted to save classes, it's still fine to have this logic in a classmethod!
        """
        probe_logits_data = {}
        for name, logits in probe_names_and_values:
            top_logits = TopK(logits, k)
            bottom_logits = TopK(logits, k, largest=False)
            probe_logits_data[name] = LogitsTableData(
                bottom_logits=bottom_logits.values.tolist(),
                bottom_token_ids=bottom_logits.indices.tolist(),
                top_logits=top_logits.values.tolist(),
                top_token_ids=top_logits.indices.tolist(),
                vocab_type="probes",
                max_logits=max_logits or logits.abs().max().item(),
                # max_logits=max_logits or max_or_1([L.abs().max().item() for L in probes.values()]),
            )
        return cls(probe_logits_data=probe_logits_data)
    def data(
        self,
        layout: SaeVisLayoutConfig,
        decode_fn: Callable[[int | list[int], VocabType], str | list[str]],
        component_specific_kwargs: dict[str, Any] = {},
    ) -> dict[str, Any]:
        return {
            name: probe_data.data(layout, decode_fn, component_specific_kwargs)
            for name, probe_data in self.probe_logits_data.items()
        }
@dataclass
class SequenceData:
    """
    This contains all the data necessary to make a sequence of tokens in the vis. See diagram in readme:
        https://github.com/callummcdougall/sae_vis#data_storing_fnspy
    Always-visible data:
        token_ids:          List of token IDs in the sequence
        token_posns:        Strings of "(batch_idx, seq_idx)" for our tokens (only for vis)
        feat_acts:          Sizes of activations on this sequence
        feat_acts_idx:      When feat_acts is a length-1 list, this is that index (we need it!)
        loss_contribution:  Effect on loss of this feature, for this particular token (neg = helpful)
        logit_contribution: Effect on correct logits (pos = helpful)
        orig_prob/new_prob: Original/new prediction for this feature, to help understand better
    Data which is visible on hover:
        token_logits:       The logits of the particular token in that sequence (used for line on logits histogram)
        top_token_ids:     List of the top 5 logit-boosted tokens by this feature
        top_logits:        List of the corresponding 5 changes in logits for those tokens
        bottom_token_ids:  List of the bottom 5 logit-boosted tokens by this feature
        bottom_logits:     List of the corresponding 5 changes in logits for those tokens
    Data which is specific to certain model or SAE classes:
        dfa_token_ids:     List of the buffer of top DFA source tokens for dest this token (centering on the top one)
        dfa_values:        List of the corresponding DFA values for those tokens
        dfa_token_posns:   Strings of "(batch_idx, seq_idx)" for our dfa tokens (only for vis)
    """
    token_ids: list[int] = field(default_factory=list)
    token_posns: list[str] = field(default_factory=list)
    feat_acts: list[float] = field(default_factory=list)
    feat_acts_idx: int | None = None
    loss_contribution: list[float] = field(default_factory=list)
    logit_contribution: list[float] | None = None
    orig_prob: list[float] | None = None
    new_prob: list[float] | None = None
    token_logits: list[float] = field(default_factory=list)
    top_token_ids: list[list[int]] = field(default_factory=list)
    top_logits: list[list[float]] = field(default_factory=list)
    bottom_token_ids: list[list[int]] = field(default_factory=list)
    bottom_logits: list[list[float]] = field(default_factory=list)
    dfa_token_ids: list[int] = field(default_factory=list)
    dfa_values: list[float] = field(default_factory=list)
    dfa_token_posns: list[str] = field(default_factory=list)
    def __post_init__(self) -> None:
        """
        Filters the logits & token IDs by removing any elements which are zero (this saves space in the eventual
        JavaScript).
        """
        self.seq_len = len(self.token_ids)
        self.top_logits, self.top_token_ids = self._filter(
            self.top_logits, self.top_token_ids
        )
        self.bottom_logits, self.bottom_token_ids = self._filter(
            self.bottom_logits, self.bottom_token_ids
        )
    def _filter(
        self, float_list: list[list[float]], int_list: list[list[int]]
    ) -> tuple[list[list[float]], list[list[int]]]:
        """
        Filters the list of floats and ints, by removing any elements which are zero. Note - the absolute values of the
        floats are monotonic non-increasing, so we can assume that all the elements we keep will be the first elements
        of their respective lists. Also reduces precisions of feature activations & logits.
        """
        # Next, filter out zero-elements and reduce precision
        float_list = [
            [round(f, PRECISION) for f in floats if abs(f) > 1e-6]
            for floats in float_list
        ]
        int_list = [ints[: len(floats)] for ints, floats in zip(int_list, float_list)]
        return float_list, int_list
    def _get_seq_data(
        self,
        cfg: PromptConfig | SeqMultiGroupConfig,
        decode_fn: Callable[[int | list[int], VocabType], str | list[str]],
        component_specific_kwargs: dict[str, Any] = {},
    ) -> dict[Literal["seqData", "dfaSeqData", "seqMetadata"], Any]:
        """
        Args:
        Returns:
            js_data: list[dict[str, Any]]
                The data for this sequence, in the form of a list of dicts for each token (where the dict stores things
                like token, feature activations, etc).
        """
        assert isinstance(
            cfg, (PromptConfig, SeqMultiGroupConfig)
        ), f"Invalid config type: {type(cfg)}"
        seq_group_id = component_specific_kwargs.get("seq_group_id", None)
        bold_idx = component_specific_kwargs.get("bold_idx", None)
        permanent_line = component_specific_kwargs.get("permanent_line", False)
        hover_above = component_specific_kwargs.get("hover_above", False)
        # If we didn't supply a sequence group ID, then we assume this sequence is on its own, and give it a unique ID
        if seq_group_id is None:
            seq_group_id = f"prompt-{random.randint(0, 999999):06d}"
        if isinstance(cfg, SeqMultiGroupConfig) and cfg.othello:
            # In this case, return a dict containing {board, valid, lastMove, etc}
            assert (
                bold_idx == "max"
                and len(self.feat_acts) == len(self.loss_contribution) == 1
            ), "Othello expects bold_idx='max' and only 1 feature act, because we see all 59 game states and only highlight one."
            assert self.feat_acts_idx is not None
            results = compute_othello_board_state_and_valid_moves(
                moves := self.token_ids[: self.feat_acts_idx + 1]
            )
            assert not isinstance(results, str), f"Error: {results!r} in {moves}"
            act = self.feat_acts[0]
            loss = self.loss_contribution[0]
            seq_data = {
                "board": results["board"],
                "valid": results["valid"],
                "move": results["move"],
                "captured": results["captured"],
                "act": act,
                "loss": loss,
            }
            dfa_seq_data = []
        else:
            # If we didn't specify bold_idx, then set it to be the midpoint
            if bold_idx is None:
                bold_idx = self.seq_len // 2
            def process_data_fn(x: Any):
                """
                Handles cases where fields have some entries missing (because we consciously didn't get data for them).
                """
                if (not isinstance(x, list)) or (len(x) == 0):
                    return x
                # If we only had data for the bold token, then pad the list out.
                x = deepcopy(x)
                default = [] if isinstance(x[0], list) else 0.0
                if isinstance(cfg, SeqMultiGroupConfig) and (not cfg.compute_buffer):
                    assert (
                        bold_idx != "max"
                    ), "Don't know how to deal with this case yet."
                    x = [
                        x[0] if (i == bold_idx) + 1 else default
                        for i in range(self.seq_len)
                    ]
                # If we're computing the whole sequence not a buffer, then data for the first token might need padding
                if (
                    isinstance(cfg, SeqMultiGroupConfig) and (cfg.buffer is None)
                ) or isinstance(cfg, PromptConfig):
                    if len(x) < self.seq_len:  # TODO - when is this true?
                        x = [default] + x
                # At this point, we expect the length of everything to match the length of `self.token_ids`
                assert len(x) == len(
                    self.token_ids
                ), f"Error: {len(x)=}, {len(self.token_ids)=}"
                return x
            feat_acts = process_data_fn(self.feat_acts)
            loss_contribution = process_data_fn(self.loss_contribution)
            logit_contribution = process_data_fn(self.logit_contribution)
            orig_prob = process_data_fn(self.orig_prob)
            new_prob = process_data_fn(self.new_prob)
            pos_ids = process_data_fn(self.top_token_ids)
            neg_ids = process_data_fn(self.bottom_token_ids)
            pos_val = process_data_fn(self.top_logits)
            neg_val = process_data_fn(self.bottom_logits)
            dfa_values = process_data_fn(self.dfa_values)
            # Process the tokens to get str toks
            toks = to_str_tokens(self.token_ids, decode_fn, "embed")
            pos_toks = [to_str_tokens(pos, decode_fn, "unembed") for pos in pos_ids]
            neg_toks = [to_str_tokens(neg, decode_fn, "unembed") for neg in neg_ids]
            # Get list of data dicts for each token
            seq_data = []
            dfa_seq_data = []
            # Get DFA data (there's way less of this than for the main sequences)
            if len(self.dfa_token_ids) > 0:
                dfa_toks = to_str_tokens(self.dfa_token_ids, decode_fn, "embed")
                for i in range(len(self.dfa_token_ids)):
                    kwargs_dfa = dict(
                        tok=unprocess_str_tok(dfa_toks[i]),
                        tokID=self.dfa_token_ids[i],
                        tokPosn=self.dfa_token_posns[i],
                        dfaValue=dfa_values[i],
                        # isBold=i == np.argmax(dfa_values).item(),
                    )
                    dfa_seq_data.append(kwargs_dfa)
            # Get data for primary sequences
            for i in range(len(self.token_ids)):
                # We might store a bunch of different case-specific data in the JavaScript object for each token. This is
                # done in the form of a disjoint union over different dictionaries (which can each be empty or not), this
                # minimizes the size of the overall JavaScript object. See function in `tokens_script.js` for more.
                kwargs_bold: dict[str, bool] = {}
                kwargs_hide: dict[str, bool] = {}
                kwargs_this_token_active: dict[str, Any] = {}
                kwargs_prev_token_active: dict[str, Any] = {}
                kwargs_hover_above: dict[str, bool] = {}
                # Get args if this is the bolded token (we make it bold, and maybe add permanent line to histograms)
                if bold_idx is not None:
                    kwargs_bold["isBold"] = (bold_idx == i) or (
                        bold_idx == "max" and i == np.argmax(feat_acts).item()
                    )
                    if kwargs_bold["isBold"] and permanent_line:
                        kwargs_bold["permanentLine"] = True
                # If we only have data for the bold token, we hide all other tokens' hoverdata (and skip other kwargs)
                if (
                    (isinstance(cfg, SeqMultiGroupConfig) and (not cfg.compute_buffer))
                    and isinstance(bold_idx, int)
                    and (i not in {bold_idx, bold_idx + 1})
                ):
                    kwargs_hide["hide"] = True
                else:
                    # Get args if we're making the tooltip hover above token (default is below)
                    if hover_above:
                        kwargs_hover_above["hoverAbove"] = True
                    # If feature active on this token, get background color and feature act (for hist line)
                    if abs(feat_acts[i]) > 1e-8:
                        kwargs_this_token_active = dict(
                            featAct=round(feat_acts[i], PRECISION)
                        )
                    # If prev token active, get the top/bottom logits table, underline color, and loss effect (for hist line)
                    if len(pos_toks[i]) + len(neg_toks[i]) > 0:
                        kwargs_prev_token_active = dict(
                            posToks=pos_toks[i],
                            negToks=neg_toks[i],
                            posVal=pos_val[i],
                            negVal=neg_val[i],
                            lossEffect=round(loss_contribution[i], PRECISION),
                            logitEffect=round(logit_contribution[i], PRECISION),
                        )
                        if (orig_prob is not None) and (new_prob is not None):
                            kwargs_prev_token_active |= dict(
                                origProb=round(orig_prob[i], PRECISION + 2),
                                newProb=round(new_prob[i], PRECISION + 2),
                            )
                # pyright 1.1.373 freaks out if rounding is done in-line below for some reason
                # this is likely a bug in 1.1.373, but rounding here is equivalent anyway
                token_logit = round(self.token_logits[i], PRECISION)
                seq_data.append(
                    dict(
                        tok=unprocess_str_tok(toks[i]),
                        tokID=self.token_ids[i],
                        tokPosn=self.token_posns[i] if self.token_posns else "",
                        tokenLogit=token_logit,
                        **kwargs_bold,
                        **kwargs_this_token_active,
                        **kwargs_prev_token_active,
                        **kwargs_hover_above,
                    )
                )
        return {
            "seqData": seq_data,
            "dfaSeqData": dfa_seq_data,
            "seqMetadata": {},
        }
@dataclass
class SeqGroupData:
    """
    This contains all the data necessary to make a single group of sequences (e.g. a quantile in prompt-centric
    visualization). See diagram in readme:
        https://github.com/callummcdougall/sae_vis#data_storing_fnspy
    Inputs:
        title:      The title that this sequence group will have, if any.
        seq_data:   The data for the sequences in this group.
    """
    seq_data: list[SequenceData] = field(default_factory=list)
    title: str = ""
    # dfa_title: str = ""
    def __len__(self) -> int:
        return len(self.seq_data)
    @property
    def max_feat_act(self) -> float:
        """Returns maximum value of feature activation over all sequences in this group."""
        return max_or_1([act for seq in self.seq_data for act in seq.feat_acts])
    @property
    def max_loss_contribution(self) -> float:
        """Returns maximum value of loss contribution over all sequences in this group."""
        return max_or_1(
            [loss for seq in self.seq_data for loss in seq.loss_contribution], abs=True
        )
    @property
    def max_dfa_value(self) -> float:
        """Returns maximum value of DFA values over all sequences in this group."""
        return max_or_1(
            [value for seq in self.seq_data for value in seq.dfa_values], abs=True
        )
    def _get_seq_group_data(
        self,
        cfg: SeqMultiGroupConfig | PromptConfig,
        decode_fn: Callable[[int | list[int], VocabType], str | list[str]],
        component_specific_kwargs: dict[str, Any] = {},
        # These default values should be correct when we only have one sequence group, because when we call this from
        # a SeqMultiGroupData we'll override them
    ) -> dict[Literal["seqGroupData", "seqGroupMetadata"], Any]:
        """
        This creates a single group of sequences, i.e. title plus some number of vertically stacked sequences.
        Args (from component-specific kwargs):
            seq_group_id:   The id of the sequence group div. This will usually be passed as e.g. "seq-group-001".
            group_size:     Max size of sequences in the group (i.e. we truncate after this many, if argument supplied).
            max_feat_act:   If supplied, then we use this as the most extreme value (for coloring by feature act).
        Returns:
            html_obj:       Object containing the HTML and JavaScript data for this seq group.
        """
        # Get metadata for this sequence group
        seq_group_id = (
            component_specific_kwargs.get("seq_group_id", None)
            or f"seq-group-{random.randint(0, 999999):06d}"
        )
        group_size = component_specific_kwargs.get("group_size", None)
        max_feat_act = component_specific_kwargs.get("max_feat_act", None) or round(
            max_or_1([act for seq in self.seq_data for act in seq.feat_acts]), PRECISION
        )
        max_loss_contribution = component_specific_kwargs.get(
            "max_loss_contribution", None
        ) or round(
            max_or_1([loss for seq in self.seq_data for loss in seq.loss_contribution]),
            PRECISION,  # abs=True?
        )
        max_dfa_value = component_specific_kwargs.get("max_dfa_value", None) or round(
            max_or_1([value for seq in self.seq_data for value in seq.dfa_values]),
            PRECISION,
        )
        seqGroupMetadata = {
            "title": self.title,
            "seqGroupId": seq_group_id,
            "maxAct": max_feat_act,
            "maxLoss": max_loss_contribution,
            "maxDFA": max_dfa_value,
        }
        # Deal with prompt cfg vs sequence cfg mode (we don't need to supply all arguments!)
        if isinstance(cfg, SeqMultiGroupConfig):
            seqGroupMetadata["nBoardsPerRow"] = cfg.n_boards_per_row
        # Get data for this sequence group
        seqGroupData = [
            seq._get_seq_data(
                cfg,
                decode_fn,
                component_specific_kwargs=dict(
                    bold_idx="max"
                    if isinstance(cfg, PromptConfig) or cfg.buffer is None
                    else cfg.buffer[0],
                    permanent_line=False,  # in a group, we're never showing a permanent line (only for single seqs)
                    seq_group_id=seq_group_id,
                ),
            )
            for seq in self.seq_data[:group_size]
        ]
        return {"seqGroupData": seqGroupData, "seqGroupMetadata": seqGroupMetadata}
@dataclass
class SeqMultiGroupData:
    """
    This contains all the data necessary to make multiple groups of sequences (e.g. the different quantiles in the
    prompt-centric visualization). See diagram in readme:
        https://github.com/callummcdougall/sae_vis#data_storing_fnspy
    """
    seq_group_data: list[SeqGroupData] = field(default_factory=list)
    is_prompt: bool = False
    def __getitem__(self, idx: int) -> SeqGroupData:
        return self.seq_group_data[idx]
    def data(
        self,
        layout: SaeVisLayoutConfig,
        decode_fn: Callable[[int | list[int], VocabType], str | list[str]],
        component_specific_kwargs: dict[str, Any] = {},
    ) -> list[dict[Literal["seqGroupData", "seqGroupMetadata"], Any]]:
        """
        Args:
            decode_fn:                  Mapping from token IDs to string tokens.
            component_specific_kwargs:  Contains any specific kwargs that could be used to customize this component.
        Returns:
            html_obj:  Object containing the HTML and JavaScript data for these multiple seq groups.
        """
        cfg = layout.prompt_cfg if self.is_prompt else layout.seq_cfg
        assert (
            cfg is not None
        ), "Calling `SeqMultiGroupData.data`, but with no vis config for that component."
        # Get max activation value & max loss contributions, over all sequences in all groups
        max_feat_act = component_specific_kwargs.get(
            "max_feat_act",
            max_or_1([seq_group.max_feat_act for seq_group in self.seq_group_data]),
        )
        max_loss_contribution = component_specific_kwargs.get(
            "max_loss_contribution", self.seq_group_data[0].max_loss_contribution
        )
        max_dfa_value = component_specific_kwargs.get(
            "max_dfa_value", self.seq_group_data[0].max_dfa_value
        )
        group_sizes = cfg.group_sizes if isinstance(cfg, SeqMultiGroupConfig) else [1]
        return [
            sequences_group._get_seq_group_data(
                cfg=cfg,
                decode_fn=decode_fn,
                component_specific_kwargs=dict(
                    group_size=group_size,
                    max_feat_act=max_feat_act,
                    max_loss_contribution=max_loss_contribution,
                    max_dfa_value=max_dfa_value,
                    seq_group_id=f"seq-group-{i}",
                ),
            )
            for i, (group_size, sequences_group) in enumerate(
                zip(group_sizes, self.seq_group_data)
            )
        ]
GenericData = (
    FeatureTablesData
    | ActsHistogramData
    | LogitsTableData
    | LogitsHistogramData
    | SeqMultiGroupData
)
@dataclass
class SaeVisData:
    """
    # TODO - check if this docstring is still correct
    This contains all the data necessary for constructing the feature-centric visualization, over multiple
    features (i.e. being able to navigate through them).
    Args:
        feature_data_dict:  Contains data for each individual feature-centric vis. For each feature, this looks like a
                            dict like {"seqMultiGroup": seqMultiGroupData, ...}.
        prompt_data_dict:   Contains data for each prompt-centric vis. For each feature, rather than keys being
                            component names, they're tuple-ified prompts, and the values are the corresponding
                            SeqMultiGroupData objects (only containing 1 prompt).
        feature_stats:      Contains stats over all features (e.g. activation quantiles for each feature, used for
                            rank-ordering features in the prompt-centric vis)
        cfg:                The vis config, used for the both the data gathering and the vis layout
        model:              Model that our sae was trained on
        sae:                Used to get the feature activations
        sae_B:              Used to get the feature activations for the second model (if applicable)
        linear_probes:      Used to get logits for probe directions in e.g. Othello models
    """
    feature_data_dict: dict[int, dict[str, GenericData]] = field(default_factory=dict)
    prompt_data_dict: dict[int, dict[tuple[str | int, ...], SeqMultiGroupData]] = field(
        default_factory=dict
    )
    feature_stats: FeatureStatistics = field(default_factory=FeatureStatistics)
    cfg: SaeVisConfig = field(default_factory=SaeVisConfig)
    model: HookedSAETransformer | None = None
    sae: SAE | None = None
    sae_B: SAE | None = None
    linear_probes: list[
        tuple[Literal["input", "output"], str, Float[Tensor, "d_model d_vocab_out"]]
    ] = field(default_factory=list)
    vocab_dict: dict[VocabType, dict[int, str]] | None = None
    def __post_init__(self):
        # TODO - actually use the unembedding mode
        if self.vocab_dict is None:
            assert self.model is not None
            self.vocab_dict = {v: k for k, v in self.model.tokenizer.vocab.items()}  # type: ignore
            self.vocab_dict = {
                k: self.vocab_dict for k in ["embed", "unembed", "probes"]
            }  # type: ignore
        self.decode_fn = get_decode_html_safe_fn(self.vocab_dict)  # type: ignore
    def update(self, other: "SaeVisData") -> None:
        """
        Updates SaeVisData with the data from another SaeVisData object. This is useful during the `get_feature_data`
        function, since this function is broken up into different groups of features then merged together.
        """
        if other is None:
            return
        self.feature_data_dict.update(other.feature_data_dict)
        self.feature_stats.update(other.feature_stats)
    @classmethod
    def create(
        cls,
        sae: SAE,
        model: HookedSAETransformer,
        tokens: Int[Tensor, "batch seq"],
        cfg: SaeVisConfig,
        # optional
        sae_B: SAE | None = None,
        linear_probes: list[
            tuple[Literal["input", "output"], str, Float[Tensor, "d_model d_vocab_out"]]
        ] = [],
        target_logits: Float[Tensor, "batch seq d_vocab_out"] | None = None,
        vocab_dict: dict[VocabType, dict[int, str]] | None = None,
        verbose: bool = False,
        clear_memory_between_batches: bool = False,
    ) -> "SaeVisData":
        """
        Optional args:
            sae_B: SAE
                Extra SAE for computing stuff like top feature correlations between the 2 SAEs.
            linear_probes: list[tuple[str, str, Tensor]]
                A list of linear probes, which will be used to create tables that go below the logit tables. Each linear
                probe consists of:
                    label:   either "input" or "output". If "input" then the table will tell us which probe dirs the sae
                             fires strongly on; if "output" then it tells us which dirs the sae writes to strongly.
                    name:    the name of the probe, which will be displayed in the table
                    weights: the weights of the linear probe, which will be used to compute the table values
                For example, an OthelloGPT input probe for the "theirs vs mine" direction would tell us if the latent
                is firing strongly when the model thinks the square is "theirs", and an output probe for that direction
                would tell us if the latent is writing strongly to this direction when it fires.
            target_logits: Tensor
                If supplied, then rather than comparing logits[:, 1:] to tokens[:, :-1], we'll compare logits[:, 1:] to
                target_logits, where target_logits is assumed to be a tensor of logprobs. This is useful when e.g. we
                have models like OthelloGPT whose target distribution isn't deterministic (they match a uniform distn
                over legal moves). Note, this isn't practical to use for language models because the vocab is massive;
                it's more of a special thing for a certain kind of toy models.
        """
        from sae_vis.data_fetching_fns import get_feature_data
        return get_feature_data(
            sae=sae,
            model=model,
            tokens=tokens,
            cfg=cfg,
            sae_B=sae_B,
            linear_probes=linear_probes,
            target_logits=target_logits,
            vocab_dict=vocab_dict,
            verbose=verbose,
            clear_memory_between_batches=clear_memory_between_batches,
        )
    def save_feature_centric_vis(
        self,
        filename: str,
        feature: int | None = None,
        verbose: bool = False,
    ) -> None:
        """
        Saves the HTML page for feature-centric vis. By default it will open on `feature`, or on the
        smallest feature if none is specified.
        """
        layout = self.cfg.feature_centric_layout
        # Set the default argument for the dropdown (i.e. when the page first loads)
        if feature not in self.feature_data_dict:
            first_feature = min(self.feature_data_dict)
            if feature is not None:
                print(f"Feat {feature} not found, defaulting to {first_feature}")
            feature = first_feature
        assert isinstance(feature, int)
        all_feature_data = list(self.feature_data_dict.items())
        if verbose:
            all_feature_data = tqdm(all_feature_data, desc="Saving feature-centric vis")
        DATA = {
            str(feat): {
                comp_name: components_dict[comp_name].data(
                    layout=layout, decode_fn=self.decode_fn
                )
                for comp_name in layout.components
            }
            for feat, components_dict in all_feature_data
        }
        html = self._create_html_file(layout, str(feature), DATA)
        with open(filename, "w") as f:
            f.write(html)
    def save_prompt_centric_vis(
        self,
        filename: str | Path,
        prompt: str,
        metric: str | None = None,
        seq_pos: int | None = None,
        num_top_features: int = 10,
        verbose: bool = False,
    ):
        """
        Saves the HTML page for prompt-centric vis. If supplied then it will open on the `seq_pos`-
        indexed sequence position, and the metric `metric`, unless these aren't supplied in which
        case it'll open on the first listed (seq_pos, metric) in the dictionary returned from
        `get_prompt_data`.
        """
        layout = self.cfg.prompt_centric_layout
        # Run forward passes on our prompt, and store the data within each FeatureData object as `self.prompt_data` as
        # well as returning the scores_dict (which maps from score hash to a list of feature indices & formatted scores)
        from sae_vis.data_fetching_fns import get_prompt_data
        # This function populates "self.feature_data" by adding the prompt data component, and also
        # returns dict mapping stringified metric keys to the top features for that metric
        PROMPT_DATA = get_prompt_data(
            sae_vis_data=self, prompt=prompt, num_top_features=num_top_features
        )
        assert len(PROMPT_DATA) > 0, "No active feats found for any prompt tokens"
        # Get all possible values for dropdowns
        str_toks = self.model.tokenizer.tokenize(prompt)  # type: ignore
        str_toks = [
            t.replace("|", "│") for t in str_toks
        ]  # vertical line -> pipe (hacky, so key splitting on | works)
        seq_keys = [f"{t!r} ({i})" for i, t in enumerate(str_toks)]
        # Get default values for dropdowns
        if PROMPT_DATA.get(first_key := f"{metric}|{seq_keys[seq_pos or 0]}", []) == []:
            valid_keys = [k for k, v in PROMPT_DATA.items() if len(v) > 0]
            assert len(valid_keys) > 0, "No active feats found for any prompt tokens"
            first_key = valid_keys[0]
            first_metric = first_key.split("|")[0]
            first_seq_pos_match = re.search(r"\((\d+)\)", first_key.split("|")[1])
            assert first_seq_pos_match is not None
            first_seq_pos = int(first_seq_pos_match.group(1))
            if metric is not None and seq_pos is not None:
                print(
                    f"Invalid choice of {metric=} and {seq_pos=}, defaulting to metric={first_metric!r} and seq_pos={first_seq_pos}"
                )
        all_feature_data = list(self.feature_data_dict.items())
        if verbose:
            all_feature_data = tqdm(all_feature_data, desc="Saving feature-centric vis")
        DATA = {
            str(feat): {
                comp_name: components_dict[comp_name].data(
                    layout=layout, decode_fn=self.decode_fn
                )
                for comp_name in layout.components
            }
            for feat, components_dict in all_feature_data
        }
        # TODO - do I need `self.feature_stats.aggdata`?
        html = self._create_html_file(layout, first_key, DATA, PROMPT_DATA)
        with open(filename, "w") as f:
            f.write(html)
    def _create_html_file(
        self,
        layout: SaeVisLayoutConfig,
        start_key: str,
        data: dict[str, Any],
        prompt_data: dict[str, Any] = {},
    ):
        init_js_str = (Path(__file__).parent / "init.js").read_text()
        style_css_str = (Path(__file__).parent / "style.css").read_text()
        return f"""
<div id='dropdown-container'></div>
<div class='grid-container'></div>
<script src="https://d3js.org/d3.v6.min.js"></script>
<script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
<style>
{style_css_str}
</style>
<script>
const START_KEY = {json.dumps(start_key)};
const METADATA = {json.dumps(layout.metadata)};
const DATA = defineData();
const PROMPT_DATA = definePromptData();
{init_js_str}
function defineData() {{
    return {json.dumps(data)};
}}
function definePromptData() {{
    return {json.dumps(prompt_data)};
}}
</script>
"""

================
File: sae_vis/demos/demo.py
================
# %%
import torch
from datasets import load_dataset
from huggingface_hub import hf_hub_download
from sae_lens import SAE, HookedSAETransformer
from sae_vis.data_config_classes import (
    ActsHistogramConfig,
    Column,
    FeatureTablesConfig,
    SaeVisConfig,
    SaeVisLayoutConfig,
    SeqMultiGroupConfig,
)
from sae_vis.data_storing_fns import SaeVisData
from sae_vis.model_fns import load_demo_model_saes_and_data, load_othello_vocab
torch.set_grad_enabled(False)
assert torch.cuda.is_available(), "This demo won't run well on CPU"
device = torch.device("cuda")
# %%
# * [1/5] Feature-centric, vanilla
# First we run setup code, for loading in the model & SAE as well as the dataset
SEQ_LEN = 128
DATASET_PATH = "NeelNanda/c4-code-20k"
MODEL_NAME = "gelu-1l"
HOOK_NAME = "blocks.0.mlp.hook_post"
sae, sae_B, model, all_tokens = load_demo_model_saes_and_data(SEQ_LEN, str(device))
sae_vis_data = SaeVisData.create(
    sae=sae,
    sae_B=sae_B,
    model=model,
    tokens=all_tokens[:4096],  # 8192
    cfg=SaeVisConfig(features=range(128)),  # 256
    verbose=True,
)
sae_vis_data.save_feature_centric_vis(filename="demo_feature_vis.html", feature=8)
# %%
# * [2/5] Feature-centric, custom
layout = SaeVisLayoutConfig(
    columns=[
        Column(
            SeqMultiGroupConfig(buffer=None, n_quantiles=0, top_acts_group_size=30),
            width=1000,
        ),
        Column(ActsHistogramConfig(), FeatureTablesConfig(n_rows=5), width=500),
    ],
    height=1000,
)
layout.help()  # this prints out what our vis will look like
sae_vis_data_custom = SaeVisData.create(
    sae=sae,
    sae_B=sae_B,
    model=model,
    tokens=all_tokens[:4096, :48],
    cfg=SaeVisConfig(
        features=range(256), feature_centric_layout=layout
    ),  # specify layout here
    verbose=True,
)
sae_vis_data_custom.save_feature_centric_vis(
    filename="demo_feature_vis_custom.html", feature=8, verbose=True
)
# %%
# * [3/5] Prompt-centric
# This is done on top of a pre-existing SaeVisData object (because most of the data-gathering is already done!)
prompt = "'first_name': ('django.db.models.fields"
sae_vis_data.save_prompt_centric_vis(
    filename="demo_prompt_vis.html",
    prompt=prompt,
    seq_pos=model.tokenizer.tokenize(prompt).index("Ġ('"),  # type: ignore
    metric="act_quantile",
)
# %%
# * [4/5] Othello
# First we run setup code, for loading in the model & SAE as well as the dataset
hf_repo_id = "callummcdougall/arena-demos-othellogpt"
othellogpt = HookedSAETransformer.from_pretrained("othello-gpt")
othellogpt_sae = SAE.from_pretrained(
    release=hf_repo_id, sae_id="blocks.5.mlp.hook_post-v1", device=str(device)
)[0]
batch_size = 5000
batch_size_for_computing_alive_feats = 1000
hf_othello_load = lambda x: torch.load(
    hf_hub_download(repo_id=hf_repo_id, filename=x), device, weights_only=True
)
othello_tokens = hf_othello_load("tokens.pt")[:batch_size]
othello_target_logits = hf_othello_load("target_logits.pt")[:batch_size]
othello_linear_probes = hf_othello_load("linear_probes.pt")
print(f"Tokens loaded from Othello dataset: {othello_tokens.shape=}")
_, cache = othellogpt.run_with_cache_with_saes(
    othello_tokens[:batch_size_for_computing_alive_feats],
    saes=[othellogpt_sae],
    names_filter=lambda x: "hook_sae" in x,
)
othello_acts = cache[f"{othellogpt_sae.cfg.hook_name}.hook_sae_acts_post"]
othello_alive_feats = (
    (othello_acts[:, 5:-5].flatten(0, 1) > 1e-8).any(dim=0).nonzero().squeeze().tolist()
)
print(f"Alive features: {len(othello_alive_feats)}/{othellogpt_sae.cfg.d_sae}\n")
sae_vis_data = SaeVisData.create(
    sae=othellogpt_sae,
    model=othellogpt,  # type: ignore
    linear_probes=[
        ("input", "theirs vs mine", othello_linear_probes["theirs vs mine"]),
        ("output", "theirs vs mine", othello_linear_probes["theirs vs mine"]),
        ("input", "empty", othello_linear_probes["empty"]),
        ("output", "empty", othello_linear_probes["empty"]),
    ],
    tokens=othello_tokens,
    target_logits=othello_target_logits,
    cfg=SaeVisConfig(
        features=othello_alive_feats[:16],
        seqpos_slice=(5, -5),
        feature_centric_layout=SaeVisLayoutConfig.default_othello_layout(),
    ),
    vocab_dict=load_othello_vocab(),
    verbose=True,
    clear_memory_between_batches=True,
)
sae_vis_data.save_feature_centric_vis(filename="demo_othello_vis.html", verbose=True)
# %%
# * [5/5] Attention models
# First we run setup code, for loading in the model & SAE as well as the dataset
attn_model = HookedSAETransformer.from_pretrained("attn-only-2l-demo")
attn_sae = SAE.from_pretrained(
    "callummcdougall/arena-demos-attn2l", "blocks.0.attn.hook_z-v2", str(device)
)[0]
batch_size = 4096
batch_size_for_computing_alive_feats = 512
seq_len = 256
original_dataset = load_dataset(
    attn_sae.cfg.dataset_path, split="train", streaming=True, trust_remote_code=True
)
attn_tokens_as_list = [
    x["input_ids"][: seq_len - 1] for (_, x) in zip(range(batch_size), original_dataset)
]
attn_tokens = torch.tensor(attn_tokens_as_list, device=device)
bos_token = torch.tensor(
    [attn_model.tokenizer.bos_token_id for _ in range(batch_size)], device=device
)  # type: ignore
attn_tokens = torch.cat([bos_token.unsqueeze(1), attn_tokens], dim=1)
print(f"Tokens loaded for attn-only model: {attn_tokens.shape=}")
_, cache = attn_model.run_with_cache_with_saes(
    attn_tokens[:batch_size_for_computing_alive_feats],
    saes=[attn_sae],
    names_filter=(post_acts_hook := f"{attn_sae.cfg.hook_name}.hook_sae_acts_post"),
    stop_at_layer=attn_sae.cfg.hook_layer + 1,
)
attn_acts = cache[post_acts_hook]
attn_alive_feats = (
    (attn_acts.flatten(0, 1) > 1e-8).any(dim=0).nonzero().squeeze().tolist()
)
print(f"Alive features: {len(attn_alive_feats)}/{attn_sae.cfg.d_sae}\n")
sae_vis_data = SaeVisData.create(
    sae=attn_sae,
    model=attn_model,
    tokens=attn_tokens,
    cfg=SaeVisConfig(features=attn_alive_feats[:32]),
    verbose=True,
    clear_memory_between_batches=True,
)
sae_vis_data.save_feature_centric_vis(filename="demo_feature_vis_attn2l-v3.html")
# %%

================
File: sae_vis/model_fns.py
================
import einops
import torch
from datasets import load_dataset
from datasets.arrow_dataset import Dataset
from jaxtyping import Float
from sae_lens import SAE, HookedSAETransformer, SAEConfig
from torch import Tensor
from transformer_lens import HookedTransformer, utils
from sae_vis.utils_fns import VocabType
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
def to_resid_dir(
    dir: Float[Tensor, "feats d"],
    sae: SAE,
    model: HookedSAETransformer,
    input: bool = False,
):
    """
    Converts a batch of feature directions to residual stream directions (i.e. for writing to the
    model's residual stream). For example, if the SAE was trained on the residual stream then this
    is just the identity function, but if the SAE was trained on the MLP activations or attn head
    z-values then we return dir @ W_out or dir @ W_O.flatten(-2, -1) respectively.
    We also have the `input` argument: if True then we'll be returning the reading direction rather
    than writing direction (i.e. @ W_in or @ W_V.flatten(-2, -1) respectively).
    """
    match sae.cfg.hook_name.split(".hook_")[-1]:
        case "resid_pre" | "resid_mid" | "resid_post" | "attn_out" | "mlp_out":
            return dir
        case "pre" | "post":
            return dir @ (
                model.W_in[sae.cfg.hook_layer].T
                if input
                else model.W_out[sae.cfg.hook_layer]
            )
        case "z":
            return dir @ (
                einops.rearrange(
                    model.W_V[sae.cfg.hook_layer],
                    "n_heads d_model d_head -> (n_heads d_head) d_model",
                )
                if input
                else einops.rearrange(
                    model.W_O[sae.cfg.hook_layer],
                    "n_heads d_head d_model -> (n_heads d_head) d_model",
                )
            )
        case _:
            raise ValueError(f"Unexpected hook name: {sae.cfg.hook_name}")
def resid_final_pre_layernorm_to_logits(x: Tensor, model: HookedTransformer):
    x = x - x.mean(-1, keepdim=True)  # [batch, pos, length]
    scale = x.pow(2).mean(-1, keepdim=True) + model.cfg.eps
    x_normalized = x / scale
    return x_normalized @ model.W_U + model.b_U
def load_othello_vocab() -> dict[VocabType, dict[int, str]]:
    """
    Returns vocab dicts (embedding and unembedding) for OthelloGPT, i.e. token_id -> token_str.
    This means ["pass"] + ["A0", "A1", ..., "H7"].
    If probes=True, then this is actually the board squares (including middle ones)
    """
    all_squares = [r + c for r in "ABCDEFGH" for c in "01234567"]
    legal_squares = [sq for sq in all_squares if sq not in ["D3", "D4", "E3", "E4"]]
    vocab_dict_probes = {
        token_id: str_token for token_id, str_token in enumerate(all_squares)
    }
    vocab_dict = {
        token_id: str_token
        for token_id, str_token in enumerate(["pass"] + legal_squares)
    }
    return {
        "embed": vocab_dict,
        "unembed": vocab_dict,
        "probes": vocab_dict_probes,
    }
# def load_othello_linear_probes(
#     device: str = str(device),
# ) -> dict[str, Float[Tensor, "d_model d_vocab_out"]]:
#     """
#     Loads linear probe from paper & rearranges it to the correct format.
#     Interpretation:
#         - Initial linear probe has shape (3, d_model, rows, cols, 3) where:
#             - 0th dim = different move parity probes (black to play / odd, white / even, both)
#             - Last dim = classification of empty / black / white squares
#         - We create 3 new probes in a different basis (the "empty / theirs / mine" basis rather
#           than "empty / black / white"), by averaging over the old probes.
#             - Each new probe has shape (d_model, rows*cols=d_vocab_out).
#     """
#     OTHELLO_ROOT = Path(__file__).parent.parent / "othello_world"
#     OTHELLO_MECHINT_ROOT = OTHELLO_ROOT / "mechanistic_interpretability"
#     assert OTHELLO_MECHINT_ROOT.exists()
#     linear_probe = t.load(OTHELLO_MECHINT_ROOT / "main_linear_probe.pth", map_location=device)
#     black_to_play, white_to_play, _ = 0, 1, 2
#     square_is_empty, square_is_white, square_is_black = 0, 1, 2
#     linear_probe = einops.rearrange(
#         linear_probe,
#         "probes d_model rows cols classes -> probes classes d_model (rows cols)",
#     )
#     # Change of basis 1: from "blank/black/white" to "blank/mine/theirs"
#     linear_probe = {
#         "theirs": linear_probe[[black_to_play, white_to_play], [square_is_white, square_is_black]].mean(0),
#         "mine": linear_probe[[black_to_play, white_to_play], [square_is_black, square_is_white]].mean(0),
#         "empty": linear_probe[[black_to_play, white_to_play], [square_is_empty, square_is_empty]].mean(0),
#     }
#     # Change of basis 2: get a "mine vs theirs" direction & "blank" direction
#     linear_probe = {
#         "mine vs theirs": linear_probe["mine"] - linear_probe["theirs"],
#         "empty": linear_probe["empty"] - 0.5 * (linear_probe["mine"] + linear_probe["theirs"]),
#     }
#     # Normalize
#     linear_probe = {k: v / v.norm(dim=0).mean() for k, v in linear_probe.items()}
#     # important thing: when these probes say "mine" they mean what just got moved, not who is to move!
#     linear_probe = {
#         "theirs vs mine": linear_probe["mine vs theirs"],
#         "empty": linear_probe["empty"],
#     }
#     # the middle 4 squares being empty is meaningless
#     linear_probe["empty"][:, [27, 28, 35, 36]] = 0.0
#     return linear_probe
def load_demo_model_saes_and_data(
    seq_len: int, device: str
) -> tuple[SAE, SAE, HookedSAETransformer, Tensor]:
    """
    This loads in the SAEs (and dataset) we'll be using for our demo examples.
    """
    SEQ_LEN = seq_len
    DATASET_PATH = "NeelNanda/c4-code-20k"
    MODEL_NAME = "gelu-1l"
    HOOK_NAME = "blocks.0.mlp.hook_post"
    saes: list[SAE] = []
    model = HookedSAETransformer.from_pretrained(MODEL_NAME)
    for version in [25, 47]:
        state_dict = utils.download_file_from_hf(
            "NeelNanda/sparse_autoencoder", f"{version}.pt", force_is_torch=True
        )
        assert isinstance(state_dict, dict)
        assert set(state_dict.keys()) == {"W_enc", "W_dec", "b_enc", "b_dec"}
        d_in, d_sae = state_dict["W_enc"].shape
        # Create autoencoder
        cfg = SAEConfig(
            architecture="standard",
            # forward pass details.
            d_in=d_in,
            d_sae=d_sae,
            activation_fn_str="relu",
            apply_b_dec_to_input=True,
            finetuning_scaling_factor=False,
            # dataset it was trained on details.
            context_size=SEQ_LEN,
            model_name=MODEL_NAME,
            hook_name=HOOK_NAME,
            hook_layer=0,
            hook_head_index=None,
            prepend_bos=True,
            dataset_path=DATASET_PATH,
            dataset_trust_remote_code=False,
            normalize_activations="None",
            # misc
            sae_lens_training_version=None,
            dtype="float32",
            device=str(device),
        )
        sae = SAE(cfg)
        sae.load_state_dict(state_dict)
        saes.append(sae)
    sae, sae_B = saes
    # Load in the data (it's a Dataset object)
    data = load_dataset(DATASET_PATH, split="train")
    assert isinstance(data, Dataset)
    dataset = load_dataset(path=DATASET_PATH, split="train", streaming=False)
    tokenized_data = utils.tokenize_and_concatenate(
        dataset=dataset,  # type: ignore
        tokenizer=model.tokenizer,  # type: ignore
        streaming=True,
        max_length=sae.cfg.context_size,
        add_bos_token=sae.cfg.prepend_bos,
    )
    tokenized_data = tokenized_data.shuffle(42)
    all_tokens = tokenized_data["tokens"]
    assert isinstance(all_tokens, torch.Tensor)
    print(all_tokens.shape)
    return sae, sae_B, model, all_tokens

================
File: sae_vis/utils_fns.py
================
import copy
import itertools
import re
from dataclasses import dataclass, field
from typing import (
    Any,
    Callable,
    Iterable,
    Literal,
    TypeAlias,
    TypeVar,
    overload,
)
import einops
import numpy as np
import torch
from dataclasses_json import dataclass_json
from eindex import eindex
from jaxtyping import Bool, Float, Int
from matplotlib import colors
from tabulate import tabulate
from torch import Tensor
from tqdm import tqdm
from transformer_lens import utils
VocabType: TypeAlias = Literal["embed", "unembed", "probes"]
Arr = np.ndarray
T = TypeVar("T")
BG_COLOR_MAP = colors.LinearSegmentedColormap.from_list(
    "bg_color_map", ["white", "darkorange"]
)
def get_device() -> torch.device:
    """
    Helper function to return the correct device (cuda, mps, or cpu).
    """
    return torch.device(
        "mps"
        if torch.backends.mps.is_available()
        else "cuda"
        if torch.cuda.is_available()
        else "cpu"
    )
MAIN = __name__ == "__main__"
METRIC_TITLES = {
    "act_size": "Activation Size",
    "act_quantile": "Activation Quantile",
    "loss_effect": "Loss Effect",
}
def create_iterator(
    iterator: Iterable[T], verbose: bool, desc: str | None = None
) -> Iterable[T]:
    """
    Returns an iterator, useful for reducing code repetition.
    """
    return tqdm(iterator, desc=desc, leave=False) if verbose else iterator
def k_largest_indices(
    x: Float[Tensor, "rows cols"],
    k: int,
    largest: bool = True,
    buffer: tuple[int, int] | None = None,
    no_overlap: bool = True,
) -> Int[Tensor, "k 2"]:
    """
    Args:
        x:
            2D array of floats, usually the activatons for our batch of tokens or smth
        k:
            Number of indices to return
        largest:
            Whether to return the indices for the largest or smallest values
        buffer:
            Positions to avoid at the start / end of the sequence. If None, then we use all sequence positions.
    Returns:
        The indices of the top or bottom `k` elements in `x`. In other words, output[i, :] is the (row, column) index of
        the i-th largest/smallest element in `x`.
    """
    # if buffer is None:
    #     buffer = (0, x.size(1))
    # x = x[:, buffer[0] : buffer[1]]
    # indices = x.flatten().topk(k=k, largest=largest).indices
    # rows = indices // x.size(1)
    # cols = indices % x.size(1) + buffer[0]
    # return torch.stack((rows, cols), dim=1)
    if buffer is None:
        buffer = (0, x.size(1))
    x = x[:, buffer[0] : buffer[1]]
    indices = x.flatten().argsort(-1, descending=largest)
    rows = indices // x.size(1)
    cols = indices % x.size(1) + buffer[0]
    if no_overlap:
        unique_indices = torch.empty((0, 2)).long()
        while len(unique_indices) < k:
            unique_indices = torch.cat(
                (unique_indices, torch.tensor([[rows[0], cols[0]]]))
            )
            is_overlapping_mask = (rows == rows[0]) & (
                (cols - cols[0]).abs() <= max(abs(buffer[0]), abs(buffer[1]))
            )
            rows = rows[~is_overlapping_mask]
            cols = cols[~is_overlapping_mask]
        return unique_indices
    return torch.stack((rows, cols), dim=1)[:k]
def index_with_buffer(
    x: Float[Tensor, "batch seq"],
    indices: Int[Tensor, "k 2"],
    buffer: int | tuple[int, int] | None = None,
    offset: int = 0,
) -> Float[Tensor, "k buffer_range"]:
    """
    Args:
        x:
            2D array of floats, usually the activatons for our batch of tokens or smth
        indices:
            Tensor returned from `k_largest_indices`
        buffer:
            Positions to avoid at the start / end of the sequence. If None, then we use all sequence positions (so this
            will mean the values indices[:, 1] don't get used)
        offset:
            We add this to all sequence positions. Note that we could just change the buffer instead, but this is easier
            to work with as an argument.
    Returns:
        x, indexed into by `indices` and with an optional buffer around it.
    """
    if buffer is None:
        return x[indices[:, 0]]
    rows, cols = indices.unbind(dim=-1)
    cols = cols + offset
    if buffer != 0:
        if isinstance(buffer, int):
            buffer = (buffer, -buffer)
        assert (
            buffer[1] < 0
        ), f"Should have negative second buffer entry, found {buffer=}."
        buffer_width = buffer[0] - buffer[1] + 1
        rows = einops.repeat(rows, "k -> k buffer", buffer=buffer_width)
        cols[cols < buffer[0]] = buffer[0]
        cols[cols > x.size(1) + buffer[1] - 1] = x.size(1) + buffer[1] - 1
        cols = einops.repeat(cols, "k -> k buffer", buffer=buffer_width) + torch.arange(
            -buffer[0], -buffer[1] + 1, device=cols.device
        )
    return x[rows, cols]
def sample_unique_indices(
    large_number: int, small_number: int
) -> Int[Tensor, "small_number"]:
    """
    Samples a small number of unique indices from a large number of indices.
    This is more efficient than using `torch.permutation`, because we don't need to shuffle everything.
    """
    weights = torch.ones(large_number)  # Equal weights for all indices
    sampled_indices = torch.multinomial(weights, small_number, replacement=False)
    return sampled_indices
def random_range_indices(
    x: Float[Tensor, "batch seq"],
    k: int,
    bounds: tuple[float, float],
    buffer: tuple[int, int] | None = None,
) -> Int[Tensor, "k 2"]:
    """
    Args:
        x:
            2D array of floats (these will be the values of feature activations or losses for each
            token in our batch)
        k:
            Number of indices to return
        bounds:
            The range of values to consider (so we can get quantiles)
        buffer:
            Positions to avoid at the start / end of the sequence, i.e. we can include the slice buffer[0]: buffer[1]
    Returns:
        Same thing as `k_largest_indices`, but the difference is that we're using quantiles rather than top/bottom k.
    """
    if buffer is None:
        buffer = (0, x.size(1))
    # Limit x, because our indices (bolded words) shouldn't be too close to the left/right of sequence
    x = x[:, buffer[0] : buffer[1]]
    # Creat a mask for where x is in range, and get the indices as a tensor of shape (k, 2)
    mask = (bounds[0] <= x) & (x <= bounds[1])
    indices = torch.stack(torch.where(mask), dim=-1)
    # If we have more indices than we need, randomly select k of them
    if len(indices) > k:
        indices = indices[sample_unique_indices(len(indices), k)]
    # Adjust indices to account for the buffer
    return indices.cpu() + torch.tensor([0, buffer[0]])
# TODO - solve the `get_decode_html_safe_fn` issue
# The verion using `tokenizer.decode` is much slower, but Stefan's raised issues about it not working correctly for e.g.
# Cyrillic characters. I think patching the `vocab_dict` in some way is the best solution.
def get_decode_html_safe_fn(
    vocab_dict: dict[VocabType, dict[int, str]],
    html: bool = False,
) -> Callable[[int | list[int], VocabType], str | list[str]]:
    """
    Returns a function which will take a token id (or list of ids) as well as an optional keyword
    argument to specify the vocab type, and returns the corresponding string token(s).
    """
    def decode_fn(token_id: int | list[int], vocab_type: VocabType) -> str | list[str]:
        assert (
            _vocab_dict := vocab_dict[vocab_type]
        ), f"Error: vocab_type {vocab_type} not found, only types are {vocab_dict.keys()}."
        if isinstance(token_id, int):
            str_tok = _vocab_dict.get(token_id, "UNK")
            return process_str_tok(str_tok, html=html)
        else:
            if isinstance(token_id, torch.Tensor):
                token_id = token_id.tolist()
            return [decode_fn(tok) for tok in token_id]  # type: ignore
    return decode_fn
# # Code to test this function:
# from transformer_lens import HookedTransformer
# model = HookedTransformer.from_pretrained("gelu-1l")
# unsafe_token = "<"
# unsafe_token_id = model.tokenizer.encode(unsafe_token, return_tensors="pt")[0].item() # type: ignore
# assert get_decode_html_safe_fn(model.tokenizer)(unsafe_token_id) == "<"
# assert get_decode_html_safe_fn(model.tokenizer, html=True)(unsafe_token_id) == "&lt;"
HTML_CHARS = {
    "\\": "&bsol;",
    "<": "&lt;",
    ">": "&gt;",
    ")": "&#41;",
    "(": "&#40;",
    "[": "&#91;",
    "]": "&#93;",
    "{": "&#123;",
    "}": "&#125;",
}
HTML_ANOMALIES = {
    "âĢĶ": "&mdash;",
    "âĢĵ": "&ndash;",
    "âĢĭ": "&#8203;",
    "âĢľ": "&ldquo;",
    "âĢĿ": "&rdquo;",
    "âĢĺ": "&lsquo;",
    "âĢĻ": "&rsquo;",
    "Ġ": "&nbsp;",
    "Ċ": "&bsol;n",
    "ĉ": "&bsol;t",
}
HTML_ANOMALIES_REVERSED = {
    "&mdash;": "—",
    "&ndash;": "–",
    # "&#8203;": "​", # TODO: this is actually zero width space character. what's the best way to represent it?
    "&ldquo;": "“",
    "&rdquo;": "”",
    "&lsquo;": "‘",
    "&rsquo;": "’",
    "&nbsp;": " ",
    "&bsol;": "\\",
}
HTML_QUOTES = {
    "'": "&apos;",
    '"': "&quot;",
}
HTML_ALL = {**HTML_CHARS, **HTML_QUOTES, " ": "&nbsp;"}
HTML_ALL_REVERSED = {
    **{v: k for k, v in HTML_CHARS.items()},
    **HTML_ANOMALIES_REVERSED,
}
def process_str_tok(str_tok: str, html: bool = True) -> str:
    """
    Takes a string token, and does the necessary formatting to produce the right HTML output. There are 2 things that
    might need to be changed:
        (1) Anomalous chars like Ġ should be replaced with their normal Python string representations
            e.g. "Ġ" -> " "
        (2) Special HTML characters like "<" should be replaced with their HTML representations
            e.g. "<" -> "&lt;", or " " -> "&nbsp;"
        (3) Do something special with line breaks: should be "↵"
    We always do (1) and (3), the argument `html` determines whether we do (2) as well.
    """
    for k, v in HTML_ANOMALIES.items():
        str_tok = str_tok.replace(k, v)
    str_tok = str_tok.replace("\n", "↵")
    if html:
        # Get rid of the quotes and apostrophes, and replace them with their HTML representations
        for k, v in HTML_QUOTES.items():
            str_tok = str_tok.replace(k, v)
        # # repr turns \n into \\n, while slicing removes the quotes from the repr
        # str_tok = repr(str_tok)[1:-1]
        # Apply the map from special characters to their HTML representations
        for k, v in HTML_CHARS.items():
            str_tok = str_tok.replace(k, v)
    return str_tok
def unprocess_str_tok(str_tok: str) -> str:
    """
    Performs the reverse of the `process_str_tok` function, i.e. maps from HTML representations back to their original
    characters. This is useful when e.g. our string is inside a <code>...</code> element, because then we have to use
    the literal characters.
    """
    for k, v in HTML_ALL_REVERSED.items():
        str_tok = str_tok.replace(k, v)
    return str_tok
@overload
def to_str_tokens(
    tokens: int,
    decode_fn: Callable[[int | list[int], VocabType], str | list[str]],
    vocab_type: VocabType = "embed",
) -> str: ...
@overload
def to_str_tokens(
    tokens: list[int],
    decode_fn: Callable[[int | list[int], VocabType], str | list[str]],
    vocab_type: VocabType = "embed",
) -> list[str]: ...
def to_str_tokens(
    tokens: int | list[int] | torch.Tensor,
    decode_fn: Callable[[int | list[int], VocabType], str | list[str]],
    vocab_type: VocabType = "embed",
) -> str | Any:
    """
    Helper function which converts tokens to their string representations, but (if tokens is a tensor) keeps
    them in the same shape as the original tensor (i.e. nested lists).
    """
    # Deal with the int case separately
    if isinstance(tokens, int):
        return decode_fn(tokens, vocab_type)
    # If the tokens are a (possibly nested) list, turn them into a tensor
    if isinstance(tokens, list):
        tokens = torch.tensor(tokens)
    # Get flattened list of tokens
    str_tokens = [decode_fn(t, vocab_type) for t in tokens.flatten().tolist()]
    # Reshape
    return np.reshape(str_tokens, tokens.shape).tolist()
class TopK:
    """
    This function implements a version of torch.topk over the last dimension. It offers the following:
        (1) Nicer type signatures (the default obj returned by torck.topk isn't well typed)
        (2) Helper functions for indexing & other standard tensor operations like .ndim, .shape, etc.
        (3) An efficient topk calculation, which doesn't bother applying it to the zero elements of a tensor.
    """
    values: Arr
    indices: Arr
    def __init__(
        self,
        tensor: Float[Tensor, "... d"],
        k: int,
        largest: bool = True,
        tensor_mask: Bool[Tensor, "..."] | None = None,
    ):
        self.k = k
        self.largest = largest
        self.values, self.indices = self.topk(tensor, tensor_mask)
    def __getitem__(self, item: int | slice) -> "TopK":
        new_topk = TopK.__new__(TopK)
        new_topk.k = self.k
        new_topk.largest = self.largest
        new_topk.values = self.values[item]
        new_topk.indices = self.indices[item]
        return new_topk
    def __len__(self) -> int:
        return len(self.values)
    @property
    def ndim(self) -> int:
        return self.values.ndim
    @property
    def shape(self) -> tuple[int]:
        return tuple(self.values.shape)  # type: ignore
    def numel(self) -> int:
        return self.values.size
    def topk(
        self,
        tensor: Float[Tensor, "... d"],
        tensor_mask: Bool[Tensor, "..."] | None = None,
    ) -> tuple[Arr, Arr]:
        """
        This is an efficient version of `torch.topk(..., dim=-1)`. It saves time by only doing the topk calculation over
        the bits of `tensor` where `tensor_mask=True`. This is useful when `tensor` is very sparse, e.g. it has shape
        (batch, seq, d_vocab) and its elements are zero if the corresponding token has feature activation zero. In this
        case, we don't want to waste time taking topk over a tensor of zeros.
        """
        # If no tensor mask is provided, then we just return the topk values and indices
        if tensor_mask is None or not tensor_mask.any():
            k = min(self.k, tensor.shape[-1])
            topk = tensor.topk(k=k, largest=self.largest)
            return utils.to_numpy(topk.values), utils.to_numpy(topk.indices)
        # Get the topk of the tensor, but only computed over the values of the tensor which are nontrivial
        assert (
            tensor_mask.shape == tensor.shape[:-1]
        ), "Error: unexpected shape for tensor mask."
        tensor_nontrivial_values = tensor[tensor_mask]  # shape [rows d]
        k = min(self.k, tensor_nontrivial_values.shape[-1])
        k = self.k
        topk = tensor_nontrivial_values.topk(
            k=k, largest=self.largest
        )  # shape [rows k]
        # Get an array of indices and values (with unimportant elements) which we'll index into using the topk object
        topk_shape = (*tensor_mask.shape, k)
        topk_indices = torch.zeros(topk_shape).to(tensor.device).long()  # shape [... k]
        topk_indices[tensor_mask] = topk.indices
        topk_values = torch.zeros(topk_shape).to(tensor.device)  # shape [... k]
        topk_values[tensor_mask] = topk.values
        return utils.to_numpy(topk_values), utils.to_numpy(topk_indices)
def merge_lists(*lists: Iterable[T]) -> list[T]:
    """
    Merges a bunch of lists into a single list.
    """
    return [item for sublist in lists for item in sublist]
def extract_and_remove_scripts(html_content: str) -> tuple[str, str]:
    """
    Extracts JavaScript from script tags in the HTML content, and returns it as a single string,
    along with the original content with the script tags removed.
    """
    # Pattern to find <script>...</script> tags and capture content inside
    pattern = r"<script[^>]*>(.*?)</script>"
    # Find all script tags and extract content
    scripts = re.findall(pattern, html_content, re.DOTALL)
    # Remove script tags from the original content
    html_without_scripts = re.sub(pattern, "", html_content, flags=re.DOTALL)
    # Join extracted JavaScript code
    javascript = "\n".join(scripts)
    return javascript, html_without_scripts
def pad_with_zeros(
    x: list[float],
    n: int,
    side: Literal["left", "right"] = "left",
) -> list[float]:
    """
    Pads a list with zeros to make it the correct length.
    """
    assert len(x) <= n, "Error: x must have fewer than n elements."
    if side == "right":
        return x + [0.0] * (n - len(x))
    else:
        return [0.0] * (n - len(x)) + x
# %%
# This defines the number of decimal places we'll use. It's assumed to refer to values in the range [0, 1] rather than
# pct, e.g. precision of 5 would be 99.497% = 0.99497. In other words, decimal_places = precision - 2.
SYMMETRIC_RANGES_AND_PRECISIONS: list[tuple[list[float], int]] = [
    ([0.0, 0.01], 5),
    ([0.01, 0.05], 4),
    ([0.05, 0.95], 3),
    ([0.95, 0.99], 4),
    ([0.99, 1.0], 5),
]
ASYMMETRIC_RANGES_AND_PRECISIONS: list[tuple[list[float], int]] = [
    ([0.0, 0.95], 3),
    ([0.95, 0.99], 4),
    ([0.99, 1.0], 5),
]
@dataclass_json
@dataclass
class FeatureStatistics:
    """
    This object (which used to be called QuantileCalculator) stores stats about a dataset.
    The quantiles are a bit complicated because we store a higher precision for values closer to 100%. Most of the
    other stats are pretty straightforward.
    We create these objects using the `create` method. We assume data supplid is 2D, where each row is a different
    dataset that we want to compute the stats for.
    """
    # Stats: max, frac_nonzero, skew, kurtosis
    max: list[float] = field(default_factory=list)
    frac_nonzero: list[float] = field(default_factory=list)
    skew: list[float] = field(default_factory=list)
    kurtosis: list[float] = field(default_factory=list)
    # Quantile data
    quantile_data: list[list[float]] = field(default_factory=list)
    quantiles: list[float] = field(default_factory=list)
    ranges_and_precisions: list[tuple[list[float], int]] = field(
        default_factory=lambda: ASYMMETRIC_RANGES_AND_PRECISIONS
    )
    @property
    def aggdata(
        self,
        precision: int = 5,
    ) -> dict[str, list[float]]:
        return {
            "max": [round(x, precision) for x in self.max],
            "frac_nonzero": [round(x, precision) for x in self.frac_nonzero],
            "skew": [round(x, precision) for x in self.skew],
            "kurtosis": [round(x, precision) for x in self.kurtosis],
        }
    @classmethod
    def create(
        cls,
        data: Float[Tensor, "batch data"] | None = None,
        ranges_and_precisions: list[
            tuple[list[float], int]
        ] = ASYMMETRIC_RANGES_AND_PRECISIONS,
    ) -> "FeatureStatistics":
        # Generate quantiles from the ranges_and_precisions list
        quantiles = []
        for r, p in ranges_and_precisions:
            start, end = r
            step = 10**-p
            quantiles.extend(np.arange(start, end - 0.5 * step, step))
        # If data is None, then set the quantiles and quantile_data to None, and return
        skew = []
        kurtosis = []
        if data is None:
            _max = []
            frac_nonzero = []
            quantiles = []
            quantile_data = []
        # Else, get the actual stats & quantile values (which we'll use to calculate the quantiles of any new data)
        else:
            _max = data.max(dim=-1).values.tolist()
            frac_nonzero = (data.abs() > 1e-6).float().mean(dim=-1).tolist()
            quantiles_tensor = torch.tensor(quantiles, dtype=data.dtype).to(data.device)
            quantile_data = torch.quantile(data, quantiles_tensor, dim=-1).T.tolist()
        quantiles = [round(q, 6) for q in quantiles + [1.0]]
        quantile_data = [[round(q, 6) for q in qd] for qd in quantile_data]
        # Now, strip out the quantile data prefixes which are all zeros
        for i, qd in enumerate(quantile_data):
            first_nonzero = next(
                (i for i, x in enumerate(qd) if abs(x) > 1e-6), len(qd)
            )
            quantile_data[i] = qd[first_nonzero:]
        return cls(
            max=_max,
            frac_nonzero=frac_nonzero,
            skew=skew,
            kurtosis=kurtosis,
            quantile_data=quantile_data,
            quantiles=quantiles,
            ranges_and_precisions=ranges_and_precisions,
        )
    def update(self, other: "FeatureStatistics"):
        """
        Merges two FeatureStatistics objects together (changing self inplace). This is useful when we're batching our
        calculations over different groups of features, and we want to merge them together at the end.
        Note, we also deal with the special case where self has no data.
        """
        assert (
            self.ranges_and_precisions == other.ranges_and_precisions
        ), "Error: can't merge two FeatureStatistics objects with different ranges."
        self.max.extend(other.max)
        self.frac_nonzero.extend(other.frac_nonzero)
        self.skew.extend(other.skew)
        self.kurtosis.extend(other.kurtosis)
        self.quantiles.extend(other.quantiles)
        self.quantile_data.extend(other.quantile_data)
    def get_quantile(
        self,
        values: Float[Tensor, "batch *data_dim"],
        batch_indices: list[int] | None = None,
    ) -> tuple[Float[Tensor, "batch *data_dim"], Int[Tensor, "batch *data_dim"]]:
        """
        Args:
            values:
                Tensor of values for which we want to compute the quantiles. If this is 1D then it is interpreted as a
                single value for each dataset (i.e. for each row of the reference data), if it's 2D then it's a row of
                values for each dataset.
            batch_indices:
                If not None, then this should be a list of batch indices we're actually using, in other words we should
                index `self.quantiles` down to only these indices. This is useful because often we're only doing this
                calculation on a small set of features (the ones which are non-zero).
        Returns:
            quantiles:
                The quantiles of `values` within the respective rows of the reference data.
            precisions:
                The precision of the quantiles (i.e. how many decimal places we're accurate to).
        """
        rp = self.ranges_and_precisions
        ranges = torch.tensor([r[0] for (r, _p) in rp] + [1.0]).to(values.device)
        precisions = torch.tensor([rp[0][1]] + [p for (_r, p) in rp] + [rp[-1][1]]).to(
            values.device
        )
        # For efficient storage, we remove the zeros from quantile_data (it may start with zeros). So when converting it
        # back to a tensor, we need to pad it with zeros again.
        n_buckets = len(self.quantiles) - 1
        quantiles = torch.tensor(self.quantiles).to(values.device)
        quantile_data = torch.tensor(
            [pad_with_zeros(x, n_buckets) for x in self.quantile_data]
        ).to(values.device)
        values_is_1d = values.ndim == 1
        if values_is_1d:
            values = values.unsqueeze(1)
        # Get an object to slice into the tensor (along batch dimension)
        my_slice = slice(None) if batch_indices is None else batch_indices
        # Find the quantiles of these values (i.e. the values between 0 and 1)
        quantile_indices = torch.searchsorted(
            quantile_data[my_slice], values
        )  # shape [batch data_dim]
        quantiles = quantiles[quantile_indices]
        # Also get the precisions (which we do using a separate searchsorted, only over the range dividers)
        precision_indices = torch.searchsorted(
            ranges, quantiles
        )  # shape [batch data_dim]
        precisions = precisions[precision_indices]
        # If values was 1D, we want to return the result as 1D also (for convenience)
        if values_is_1d:
            quantiles = quantiles.squeeze(1)
            precisions = precisions.squeeze(1)
        return quantiles, precisions
# Example usage
if MAIN:
    # 2D data: each row represents the activations data of a different feature. We set some of it to zero, so we can
    # test the "JSON doesn't store zeros" feature of the FeatureStatistics class.
    device = get_device()
    N = 100_000
    data = torch.stack(
        [torch.rand(N).masked_fill(torch.rand(N) < 0.5, 0.0), torch.rand(N)]
    ).to(device)
    qc = FeatureStatistics.create(data)
    print(f"Total datapoints stored = {sum(len(x) for x in qc.quantile_data):_}")
    print(f"Total datapoints used to compute quantiles = {data.numel():_}\n")
    # 2D values tensor: each row applies to a different dataset
    values = torch.tensor([[0.0, 0.005, 0.02, 0.25], [0.75, 0.98, 0.995, 1.0]]).to(
        device
    )
    quantiles, precisions = qc.get_quantile(values)
    print("When 50% of data is 0, and 50% is Unif[0, 1]")
    for v, q, p in zip(values[0], quantiles[0], precisions[0]):
        print(f"Value: {v:.3f}, Precision: {p}, Quantile: {q:.{p-2}%}")
    print("\nWhen 100% of data is Unif[0, 1]")
    for v, q, p in zip(values[1], quantiles[1], precisions[1]):
        print(f"Value: {v:.3f}, Precision: {p}, Quantile: {q:.{p-2}%}")
# %%
def split_string(
    input_string: str,
    str1: str,
    str2: str,
) -> tuple[str, str]:
    assert (
        str1 in input_string and str2 in input_string
    ), "Error: str1 and str2 must be in input_string"
    pattern = f"({re.escape(str1)}.*?){re.escape(str2)}"
    match = re.search(pattern, input_string, flags=re.DOTALL)
    if match:
        between_str1_str2 = match.group(1)
        remaining_string = input_string.replace(between_str1_str2, "")
        return between_str1_str2, remaining_string
    else:
        return "", input_string
# Example usage
if MAIN:
    input_string = "The quick brown fox jumps over the lazy dog"
    str1 = "quick"
    str2 = "jumps"
    print(split_string(input_string, str1, str2))
    input_string = (
        "Before table <!-- Logits table --> Table <!-- Logits histogram --> After table"
    )
    str1 = r"<!-- Logits table -->"
    str2 = r"<!-- Logits histogram -->"
    print(split_string(input_string, str1, str2))
# %%
def apply_indent(
    text: str,
    prefix: str,
    first_line_indented: bool = True,
) -> str:
    """
    Indents a string at every new line (e.g. by spaces or tabs). This is useful for formatting when we're dumping things
    into an HTML file.
    Args:
        text:
            The text to indent
        prefix:
            The string to add at the start of each line
        first_line_indented:
            Whether the first line should be indented. If False, then the first line will be left as it is.
    """
    text_indented = "\n".join(prefix + line for line in text.strip().split("\n"))
    if not first_line_indented:
        text_indented = text_indented[len(prefix) :]
    return text_indented
# %%
def deep_union(
    dict1: dict[Any, Any], dict2: dict[Any, Any], path: str = ""
) -> dict[Any, Any]:
    """
    Returns a deep union of dictionaries (recursive operation). In other words, if `dict1` and `dict2` have the same
    keys then the value of that key will be the deep union of the values.
    Also, base case where one of the values is a list: we concatenate the lists together
    Examples:
        # Normal union
        deep_union({1: 2}, {3: 4}) == {1: 2, 3: 4}
        # 1-deep union
        deep_union(
            {1: {2: [3, 4]}},
            {1: {3: [3, 4]}}
        ) == {1: {2: [3, 4], 3: [3, 4]}}
        # 2-deep union
        assert deep_union(
            {"x": {"y": {"z": 1}}},
            {"x": {"y": {"w": 2}}},
        ) == {"x": {"y": {"z": 1, "w": 2}}}
        # list concatenation
        assert deep_union(
            {"x": [1, 2]},
            {"x": [3, 4]},
        ) == {"x": [1, 2, 3, 4]}
    The `path` accumulates the key/value paths from the recursive calls, so that we can see the full dictionary path
    which caused problems (not just the end-nodes).
    """
    result = dict1.copy()
    # For each new key & value in dict2
    for key2, value2 in dict2.items():
        # If key not in result, then we have a simple case: just add it to the result
        if key2 not in result:
            result[key2] = value2
        # If key in result, both should values be either dicts (then we recursively merge) or lists (then we concat). If
        # not, then we throw an error unconditionally (even if values are the same).
        else:
            value1 = result[key2]
            # Both dicts
            if isinstance(value1, dict) and isinstance(value2, dict):
                result[key2] = deep_union(value1, value2, path=f"{path}[{key2!r}]")
            # Both lists
            elif isinstance(value1, list) and isinstance(value2, list):
                result[key2] = value1 + value2
            # Error
            else:
                path1 = f"dict{path}[{key2!r}] = {value1!r}"
                path2 = f"dict{path}[{key2!r}] = {value2!r}"
                raise ValueError(f"Merge failed. Conflicting paths:\n{path1}\n{path2}")
    return result
if MAIN:
    # Normal union
    assert deep_union({1: 2}, {3: 4}) == {1: 2, 3: 4}
    # 1-deep union
    assert deep_union({1: {2: [3, 4]}}, {1: {3: [3, 4]}}) == {1: {2: [3, 4], 3: [3, 4]}}
    # 2-deep union
    assert deep_union(
        {"x": {"y": {"z": 1}}},
        {"x": {"y": {"w": 2}}},
    ) == {"x": {"y": {"z": 1, "w": 2}}}
    # list concatenation
    assert deep_union(
        {"x": [1, 2]},
        {"x": [3, 4]},
    ) == {"x": [1, 2, 3, 4]}
# %%
# class RollingStats:
#     '''
#     This class helps us compute rolling stats of a dataset as we feed in activations, without ever having to store the
#     entire batch in data.
#     '''
#     def __init__(self):
#         self.n = 0
#         self.x_sum = 0.0
#         self.x2_sum = 0.0
#         self.x3_sum = 0.0
#         self.x4_sum = 0.0
#         self.frac_nonzero = 0.0
#         self.max = 0.0
#     def update(self, x: Tensor):
#         x_frac_nonzero = x.nonzero().size(0) / x.numel()
#         x_n = x.numel()
#         self.frac_nonzero = (self.n * self.frac_nonzero + x_n * x_frac_nonzero) / (self.n + x_n)
#         self.n += x.numel()
#         self.x_sum += x.sum().item()
#         self.x2_sum += x.pow(2).sum().item()
#         self.x3_sum += x.pow(3).sum().item()
#         self.x4_sum += x.pow(4).sum().item()
#         self.max = max(self.max, x.max().item())
#     @property
#     def skew(self) -> float:
#         raise NotImplementedError
#     @property
#     def kurtosis(self) -> float:
#         raise NotImplementedError
class RollingCorrCoef:
    """
    This class helps compute corrcoef (Pearson & cosine sim) between 2 batches of vectors, without having to store the
    entire batch in memory.
    How exactly does it work? We exploit the formula below (x, y assumed to be vectors here), which writes corrcoefs in
    terms of scalars which can be computed on a rolling basis.
        cos_sim(x, y) = xy_sum / ((x2_sum ** 0.5) * (y2_sum ** 0.5))
        pearson_corrcoef(x, y) = num / denom
            num = n * xy_sum - x_sum * y_sum
            denom = (n * x2_sum - x_sum ** 2) ** 0.5 * (n * y2_sum - y_sum ** 2) ** 0.5
    This class batches this computation, i.e. x.shape = (X, N), y.shape = (Y, N), where (for example) we have:
        N = batch_size * seq_len, i.e. it's the number of datapoints we have
        x = features of our original encoder
        y = features of our encoder-B, or neurons of our original model (the thing we're topk-ing over)
    So we can e.g. compute the correlation coefficients for every combination of feature in encoder and model neurons,
    then take topk to find the most correlated neurons for each feature.
    """
    def __init__(
        self,
        indices: list[int] | None = None,
        with_self: bool = False,
    ) -> None:
        """
        If we want to remove diagonals (because the corrcoef is with self), then we pass `with_self=True`.
        We also pass a list of indices, if they're not just range(X).
        """
        self.n = 0
        self.X = None
        self.Y = None
        self.indices = indices
        self.with_self = with_self
    def update(self, x: Float[Tensor, "... X"], y: Float[Tensor, "... Y"]) -> None:
        # Get values of x and y, and check for consistency with each other & with previous values
        x = x.flatten(end_dim=-2).T
        y = y.flatten(end_dim=-2).T
        X, Nx = x.shape
        Y, Ny = y.shape
        assert (
            Nx == Ny
        ), "Error: x and y should have the same size in the last dimension"
        if self.X is not None:
            assert (
                X == self.X
            ), "Error: updating a corrcoef object with different sized dataset."
        if self.Y is not None:
            assert (
                Y == self.Y
            ), "Error: updating a corrcoef object with different sized dataset."
        self.X = X
        self.Y = Y
        # If this is the first update step, then we need to initialise the sums
        if self.n == 0:
            self.x_sum = torch.zeros(X, device=x.device)
            self.xy_sum = torch.zeros(X, Y, device=x.device)
            self.x2_sum = torch.zeros(X, device=x.device)
            self.y_sum = torch.zeros(Y, device=y.device)
            self.y2_sum = torch.zeros(Y, device=y.device)
        # Next, update the sums
        self.n += x.shape[-1]
        self.x_sum += einops.reduce(x, "X N -> X", "sum")
        self.xy_sum += einops.einsum(x, y, "X N, Y N -> X Y")
        self.x2_sum += einops.reduce(x**2, "X N -> X", "sum")
        self.y_sum += einops.reduce(y, "Y N -> Y", "sum")
        self.y2_sum += einops.reduce(y**2, "Y N -> Y", "sum")
    def corrcoef(
        self,
    ) -> tuple[Float[Tensor, "X Y"], Float[Tensor, "X Y"]]:
        """
        Computes the correlation coefficients between x and y, using the formulae given in the class docstring.
        """
        # Compute cosine sim
        cossim_numer = self.xy_sum
        cossim_denom = torch.sqrt(torch.outer(self.x2_sum, self.y2_sum)) + 1e-6
        cossim = cossim_numer / cossim_denom
        # Compute pearson corrcoef
        pearson_numer = self.n * self.xy_sum - torch.outer(self.x_sum, self.y_sum)
        pearson_denom = (
            torch.sqrt(
                torch.outer(
                    self.n * self.x2_sum - self.x_sum**2,
                    self.n * self.y2_sum - self.y_sum**2,
                )
            )
            + 1e-6
        )
        pearson = pearson_numer / pearson_denom
        # If with_self, we exclude the diagonal
        if self.with_self:
            x_indices = range(cossim.shape[0])
            y_indices = self.indices or range(cossim.shape[0])
            cossim[x_indices, y_indices] = 0.0
            pearson[x_indices, y_indices] = 0.0
        return pearson, cossim
    def topk_pearson(
        self,
        k: int,
        largest: bool = True,
    ) -> tuple[list[list[int]], list[list[float]], list[list[float]]]:
        """
        Takes topk of the pearson corrcoefs over the y-dimension (e.g. giving us the most correlated neurons or most
        correlated encoder-B features for each encoder feature).
        Args:
            k: int
                Number of top indices to take (usually 3, for the left-hand tables)
            largest: bool
                If True, then we take the largest k indices. If False, then we take the smallest k indices.
        Returns:
            pearson_indices: list[list[int]]
                y-indices which are most correlated with each x-index (in terms of pearson corrcoef)
            pearson_values: list[list[float]]
                Values of pearson corrcoef for each of the topk indices
            cossim_values: list[list[float]]
                Values of cosine similarity for each of the topk indices
        """
        # Get correlation coefficient, using the formula from corrcoef method
        pearson, cossim = self.corrcoef()
        # Get the top pearson values
        pearson_topk = TopK(tensor=pearson, k=k, largest=largest)  # shape (X, k)
        # Get the cossim values for the top pearson values, i.e. cossim_values[X, k] = cossim[X, pearson_indices[X, k]]
        cossim_values = eindex(cossim, pearson_topk.indices, "X [X k]")
        return (
            pearson_topk.indices.tolist(),
            pearson_topk.values.tolist(),
            cossim_values.tolist(),
        )
@dataclass_json
@dataclass
class HistogramData:
    """
    This class contains all the data necessary to construct a single histogram (e.g. the logits or feat acts histogram).
    See diagram in readme:
        https://github.com/callummcdougall/sae_vis#data_storing_fnspy
    We don't need to store the entire `data` tensor, so we initialize instances of this class using the `from_data`
    method, which computes statistics from the input data tensor then discards it.
        bar_heights: The height of each bar in the histogram
        bar_values: The value of each bar in the histogram
        tick_vals: The tick values we want to use for the histogram
    """
    bar_heights: list[float] = field(default_factory=list)
    bar_values: list[float] = field(default_factory=list)
    tick_vals: list[float] = field(default_factory=list)
    title: str | None = None
    @classmethod
    def from_data(
        cls,
        data: Tensor,
        n_bins: int,
        tickmode: Literal["ints", "5 ticks"],
        title: str | None,
    ):
        """
        Args:
            data: 1D tensor of data which will be turned into histogram
            n_bins: Number of bins in the histogram
            line_posn: list of possible positions of vertical lines we want to put on the histogram
        Returns a HistogramData object, with data computed from the inputs. This is to support the goal of only storing
        the minimum necessary data (and making it serializable, for JSON saving).
        """
        # There might be no data, if the feature never activates
        if data.numel() == 0:
            return cls()
        # Get min and max of data
        max_value = data.max().item()
        min_value = data.min().item()
        # Divide range up into 40 bins
        bin_size = (max_value - min_value) / n_bins
        bin_edges = torch.linspace(min_value, max_value, n_bins + 1)
        # Calculate the heights of each bin
        bar_heights = torch.histc(data, bins=n_bins).int().tolist()
        bar_values = [round(x, 5) for x in (bin_edges[:-1] + bin_size / 2).tolist()]
        # Choose tickvalues
        # TODO - improve this, it's currently a bit hacky (currently I only use the 5 ticks mode)
        assert tickmode in ["ints", "5 ticks"]
        if tickmode == "ints":
            top_tickval = int(max_value)
            tick_vals = torch.arange(0, top_tickval + 1, 1).tolist()
        elif tickmode == "5 ticks":
            # Ticks chosen in multiples of 0.1, set to ensure the longer side of {positive, negative} is 3 ticks long
            if max_value > -min_value:
                tickrange = 0.1 * int(1e-4 + max_value / (3 * 0.1)) + 1e-6
                num_positive_ticks = 3
                num_negative_ticks = int(-min_value / tickrange)
            else:
                tickrange = 0.1 * int(1e-4 + -min_value / (3 * 0.1)) + 1e-6
                num_negative_ticks = 3
                num_positive_ticks = int(max_value / tickrange)
            # Tick values = merged list of negative ticks, zero, positive ticks
            tick_vals = merge_lists(
                reversed([-tickrange * i for i in range(1, 1 + num_negative_ticks)]),
                [0],  # zero (always is a tick)
                [tickrange * i for i in range(1, 1 + num_positive_ticks)],
            )
            tick_vals = [round(t, 1) for t in tick_vals]
        return cls(
            bar_heights=bar_heights,
            bar_values=bar_values,
            tick_vals=tick_vals,
            title=title,
        )
def max_or_1(mylist: Iterable[float | int], abs: bool = False) -> float | int:
    """
    Returns max of a list, or 1 if the list is empty.
    Args:
        mylist: list of numbers
        abs: If True, then we take the max of the absolute values of the list
    """
    mylist = list(mylist)
    if len(mylist) == 0:
        return 1
    if abs:
        return max(max(x, -x) for x in mylist)
    else:
        return max(mylist)
LEGAL_SQUARES = [i for i in range(64) if i not in [27, 28, 35, 36]]
SQUARE_NAMES = [r + c for r in "ABCDEFGH" for c in "01234567"]
LEGAL_SQUARE_NAMES = [SQUARE_NAMES[i] for i in LEGAL_SQUARES]
def get_row_col_from_move(move: int) -> tuple[int, int]:
    # move is in range [1, ..., 60] and we want to get zeroindexed rows and columns, steps are:
    # - subtract 1 to get [0, ..., 59]
    # - fill in empty squares to get [0, ..., 63] minus {27, 28, 35, 36}
    # - divmod to get rows and columns
    # Examples: 1 -> (0, 0), 60 -> (7, 7), 27 -> (3, 2), 28 -> (3, 5)
    move -= 1
    return divmod(LEGAL_SQUARES[move], 8)
def test_get_row_col_from_move():
    assert get_row_col_from_move(1) == (0, 0)
    assert get_row_col_from_move(60) == (7, 7)
    assert get_row_col_from_move(27) == (3, 2)
    assert get_row_col_from_move(28) == (3, 5)
@overload
def compute_othello_board_state_and_valid_moves(
    moves: list[int],
    move_indices: int | None = None,
    ditch_game_buffer: int | bool = False,
) -> dict[str, Any] | str: ...
@overload
def compute_othello_board_state_and_valid_moves(
    moves: list[int],
    move_indices: list[int] | Literal["all", "all-except-last"] = "all",
    ditch_game_buffer: int | bool = False,
) -> list[dict[str, Any]] | str: ...
def compute_othello_board_state_and_valid_moves(
    moves: list[int],
    move_indices: list[int] | Literal["all", "all-except-last"] | int | None = None,
    ditch_game_buffer: int | bool = False,
) -> list[dict[str, Any]] | dict[str, Any] | str:
    """
    Args:
        moves:          List of token IDs of moves (i.e. they're in range 1 - 60, where 1 = pass)
        move_indices:   List of move indices we'll be using (i.e. we plot the board state after this
                        many moves have been made). If None, we only plot the final board state. If
                        "all", we plot the board state after every move (including before any moves).
        ditch_game_buffer:
                        There's usually no need to ditch the board if e.g. the last move was invalid,
                        since we'll be looking for top activations in a [5, -5] buffer. This argument
                        helps us do this: if int then only ditch violations inside an [int, -int]
                        buffer, if False then we ditch nothing, if True then we ditch everything.
    Return type is an error string if the game is invalid, else it's a dict (or list of dicts, one
    for each move index we're plotting) with the following keys:
        board:          The current state of the Othello board
        valid_moves:    A list of bools, where valid_moves[r][c] is True if the current player can
                        place a piece at (r, c)
        last_move:      The row and column of the previous move (or arbitrary piece that was already
                        there if it's the first move)
        captured:       1s and 0s for whether a piece was captured.
    """
    if move_indices is None:
        move_indices = [len(moves)]
    elif move_indices == "all":
        move_indices = list(range(len(moves) + 1))
    elif move_indices == "all-except-last":
        move_indices = list(range(len(moves)))
    elif isinstance(move_indices, int):
        move_indices = [move_indices]
    def is_valid_move(board: list[list[int]], row: int, col: int, player: int):
        if board[row][col] != 0:
            return False
        for dr, dc in itertools.product([-1, 0, 1], repeat=2):
            if dr == 0 and dc == 0:
                continue
            r, c = row + dr, col + dc
            if 0 <= r < 8 and 0 <= c < 8 and board[r][c] == 3 - player:
                while 0 <= r < 8 and 0 <= c < 8 and board[r][c] != 0:
                    if board[r][c] == player:
                        return True
                    r, c = r + dr, c + dc
        return False
    def get_valid_moves(board: list[list[int]], player: int):
        valid_moves = [
            [int(is_valid_move(board, r, c, player)) for c in range(8)]
            for r in range(8)
        ]
        any_valid = any(any(row) for row in valid_moves)
        return (valid_moves, any_valid)
    def make_move(board: list[list[int]], row: int, col: int, player: int):
        """
        Edits board state, and returns an array of captured pieces, as well as a boolean for whether
        the move was legal (if it didn't capture any pieces, it's not legal).
        """
        captured = [[0 for _ in range(8)] for _ in range(8)]
        board[row][col] = player
        is_legal = False
        for dr, dc in itertools.product([-1, 0, 1], repeat=2):
            if dr == 0 and dc == 0:
                continue
            r, c = row + dr, col + dc
            to_flip = []
            while 0 <= r < 8 and 0 <= c < 8 and board[r][c] == 3 - player:
                to_flip.append((r, c))
                r, c = r + dr, c + dc
            if 0 <= r < 8 and 0 <= c < 8 and board[r][c] == player:
                is_legal = is_legal or (len(to_flip) > 0)
                for flip_r, flip_c in to_flip:
                    board[flip_r][flip_c] = player
                    captured[flip_r][flip_c] = 1
        return (captured, is_legal)
    board = [[0 for _ in range(8)] for _ in range(8)]
    board[3][3] = board[4][4] = 2  # White
    board[3][4] = board[4][3] = 1  # Black
    player = 1  # Black starts
    results = []
    row = col = 3  # start with arbitrary previous move
    captured = [[0 for _ in range(8)] for _ in range(8)]
    for i, move in enumerate(moves + [None]):
        can_ditch_game = (ditch_game_buffer is True) or (
            isinstance(ditch_game_buffer, int) and i <= (len(moves) - ditch_game_buffer)
        )
        valid_moves, any_valid = get_valid_moves(board, player)
        # If we're not at endgame, and there are no valid moves, then ditch the game
        if (move is not None) and (not any_valid) and can_ditch_game:
            return "no valid moves"
        if i in move_indices:
            results.append(
                {
                    "board": copy.deepcopy(board),
                    "valid": valid_moves,
                    "move": [row, col],
                    "captured": captured,
                }
            )
        if move is None:
            break
        row, col = get_row_col_from_move(move)  # type: ignore
        captured, is_legal = make_move(board, row, col, player)
        # If the move wasn't legal, then we ditch the game
        if (not is_legal) and can_ditch_game:
            return "no captures"
        player = 3 - player  # Switch player
    return results[0] if len(results) == 1 else results
def othello_valid_legal_moves_as_target_distribution(
    tokens: Int[Tensor, "batch seq"],
) -> tuple[
    Int[Tensor, "batchFiltered seq"], Float[Tensor, "batchFiltered seq d_vocab"]
]:
    """
    Idea: measuring cross entropy loss in Othello is misleading, since even though there is always
    one correct prediction, the correct target distribution is uniform over all correct moves. So we
    instead return a tensor of shape (batch, seq, d_vocab) representing the correct logprobs for
    next moves for Othello at that point.
    When writing this function, I was confused about how some games have no legal moves at certain
    points, and this messes up the progression of the game. So this function will filter out those
    games, and give me target distributions.
    """
    batch_size, seq_len = tokens.shape
    # target_distributions = torch.zeros(batch_size, seq_len, 61)
    valid_tokens = []
    target_distributions = []
    errors = {"no captures": 0, "no valid moves": 0, "valid": 0}
    ditch_game_buffer = True
    iterator = tqdm(tokens) if batch_size >= 500 else tokens
    for seq in iterator:
        results = compute_othello_board_state_and_valid_moves(
            moves=seq.tolist(),
            move_indices="all-except-last",
            ditch_game_buffer=ditch_game_buffer,  # TODO - can make this 5? Doesn't really matter
        )
        # First way it might be an invalid game: if one of the moves didn't capture anything
        if isinstance(results, str):
            errors[results] += 1
            continue
        valid_moves = [r["valid"] for r in results]
        target_distribution = torch.tensor(valid_moves).flatten(1)  # (seq, 8*8=64)
        target_distribution = target_distribution[:, LEGAL_SQUARES]  # (seq, 60)
        # We should have valid games past this point, i.e. there was always a legal move to play
        assert (target_distribution.sum(-1) > 0).all().item()
        valid_tokens.append(seq)
        target_distribution = torch.concat(
            [torch.zeros(seq_len, 1), target_distribution], -1
        )  # (seq, 61)
        target_distributions.append((15 * target_distribution).log_softmax(-1))
        errors["valid"] += 1
    print(tabulate(errors.items(), ["Class", "Count"], tablefmt="simple_outline"))
    return torch.stack(valid_tokens), torch.stack(target_distributions)[:, 1:]
def test_compute_othello_board_state():
    # Test invalid game: this move didn't capture anything
    game_prefix = [1]
    results = compute_othello_board_state_and_valid_moves(game_prefix, "all")
    assert results == "no captures"
    # ? Test invalid game: this move produced a board with no valid moves
    # Test valid game
    game_prefix = [20, 21]
    results = compute_othello_board_state_and_valid_moves(game_prefix, "all")
    assert not isinstance(results, str)
    assert len(results) == 3
    assert results[0]["board"] == [
        [0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 2, 1, 0, 0, 0],
        [0, 0, 0, 1, 2, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0],
    ]
    assert results[0]["valid"] == [
        [0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 1, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 1, 0, 0],
        [0, 0, 0, 0, 1, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0],
    ]
    assert results[1]["board"] == [
        [0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 1, 1, 0, 0, 0],
        [0, 0, 0, 1, 2, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0],
    ]
    assert results[1]["captured"] == [
        [0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 1, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0],
    ]
def visualize_compute_othello_board_state():
    game_prefix = [20, 21, 28, 23, 13, 5, 34, 19, 16, 43, 14, 30, 22, 40, 47, 48]
    results = compute_othello_board_state_and_valid_moves(
        game_prefix, move_indices="all"
    )
    assert not isinstance(results, str)
    for i, result in enumerate(results):
        print(f"Move {i}:")
        for row in result["board"]:
            print("  " + "".join(str(sq) if sq != 0 else "." for sq in row))
def cross_entropy_loss(
    logits: Float[Tensor, "... d"], target_logits: Float[Tensor, "... d"]
) -> Float[Tensor, "..."]:
    """
    Version of CE loss that works with a target logits tensor, rather than class indices.
    """
    assert (
        logits.shape == target_logits.shape
    ), f"Error: {logits.shape=}, {target_logits.shape=}"
    target_probs = target_logits.softmax(-1)
    logprobs = logits.log_softmax(-1)
    return -(logprobs * target_probs).sum(-1)



================================================================
End of Codebase
================================================================
