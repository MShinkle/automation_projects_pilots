This file is a merged representation of a subset of the codebase, containing specifically included files and files not matching ignore patterns, combined into a single document by Repomix.
The content has been processed where empty lines have been removed.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
- Pay special attention to the Repository Description. These contain important context and guidelines specific to this project.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: **/*.py, **/*.md, **/*.txt
- Files matching these patterns are excluded: **/.git/**, **/.github/**, CHANGELOG.md
- Empty lines have been removed from all files
- Files are sorted by Git change count (files with more changes are at the bottom)

Additional Info:
----------------
User Provided Header:
-----------------------
This file is a consolidated single-file compilation of all code in the repository generated by Repomix. Note that .ipynb files have been converted to .py files.

================================================================
Directory Structure
================================================================
README.md
requirements.txt
sae_training/activations_store.py
sae_training/config.py
sae_training/geom_median/__init__.py
sae_training/geom_median/README.md
sae_training/geom_median/setup.py
sae_training/geom_median/src/geom_median/__init__.py
sae_training/geom_median/src/geom_median/numpy/__init__.py
sae_training/geom_median/src/geom_median/numpy/main.py
sae_training/geom_median/src/geom_median/numpy/utils.py
sae_training/geom_median/src/geom_median/numpy/weiszfeld_array.py
sae_training/geom_median/src/geom_median/numpy/weiszfeld_list_of_array.py
sae_training/geom_median/src/geom_median/torch/__init__.py
sae_training/geom_median/src/geom_median/torch/main.py
sae_training/geom_median/src/geom_median/torch/utils.py
sae_training/geom_median/src/geom_median/torch/weiszfeld_array.py
sae_training/geom_median/src/geom_median/torch/weiszfeld_list_of_array.py
sae_training/optim.py
sae_training/requirements.txt
sae_training/sparse_autoencoder.py
sae_training/train_sae_on_language_model.py
sae_training/utils.py
train_transcoder.py
transcoder_circuits/circuit_analysis.py
transcoder_circuits/feature_dashboards.py
transcoder_circuits/replacement_ctx.py

================================================================
Files
================================================================

================
File: README.md
================
# Transcoder-circuits: reverse-engineering LLM circuits with transcoders

This repository contains tools for understanding what's going on inside large language models by using a tool called "transcoders". Transcoders decompose MLP sublayers in transformer models into a sparse linear combination of interpretable features. By using transcoders, we can reverse-engineer fine-grained circuits of features within the model.

To get started, first run `bash setup.sh` to install dependencies and [download transcoder weights](https://huggingface.co/pchlenski/gpt2-transcoders). Then, to learn how to use `transcoder_circuits`, we recommend working through the `walkthrough.ipynb` notebook. The full structure of the repository is as follows:

* Installation tools:
  * `setup.sh`: A shell script for installing dependencies and downloading transcoder weights.
  * `requirements.txt`: The standard Python dependencies list.
* Examples:
  * `walkthrough.ipynb`: A walkthrough notebook that demonstrates how to use the tools provided in this repository for reverse-engineering LLM circuits with transcoders.
  * `train_transcoder.py`: An example script for training a transcoder.
* Experimental results;
  * `sweep.ipynb`: An evaluation of transcoders and SAEs trained on Pythia-410M (weights not provided).
  * `interp-comparison.ipynb`: Code for performing a blind comparison of SAE and transcoder feature interpretability
* Case studies:
  * `case_study_citations.ipynb`: An example of a reverse-engineering case study that we carried out, in which we investigated a transcoder feature that activates on semicolons in parenthetical citations.
  * `case_study_caught.ipynb`: An example of a reverse-engineering case study that we carried out, in which we investigated a transcoder feature that activates on the verb "caught".
  * `case_study_local_context.ipynb`: An example of a reverse-engineering case study that we carried out, in which we attempted to reverse-engineer a circuit that computes a harder-to-interpret transcoder feature. (We were less successful in this case study, but are including it in the interest of transparency.)
  * `restricted blind case studies.ipynb`: A notebook containing a set of "restricted blind case studies" that reverse-engineer random GPT2-small transcoder features *without looking at MLP0 transcoder feature activations*.
* Libraries:
  * `sae_training/`: Code for training and using transcoders. The code is largely based on an older version of [Joseph Bloom's excellent SAE repository](https://github.com/jbloomAus/SAELens) -- **shoutouts to him!**. (The misnomer `sae_training` is a vestige of this origin of the code.)
  * `transcoder_circuits/`: Code for reverse-engineering and analyzing circuits with transcoders. These are the tools that we use in the walkthrough notebook and in the case studies.

================
File: requirements.txt
================
matplotlib
numpy
plotly
torch==2.2.0
tqdm
transformer_lens==1.11.0
datasets==2.12.0
einops==0.7.0
setuptools==67.7.2
wandb==0.16.0
huggingface-hub==0.17.3

================
File: sae_training/activations_store.py
================
import os
import torch
from datasets import load_dataset
from torch.utils.data import DataLoader
from tqdm import tqdm
from transformer_lens import HookedTransformer
import gc
class ActivationsStore:
    """
    Class for streaming tokens and generating and storing activations
    while training SAEs. 
    """
    def __init__(
        self, cfg, model: HookedTransformer, create_dataloader: bool = True,
    ):
        self.cfg = cfg
        self.model = model
        self.dataset = load_dataset(cfg.dataset_path, split="train", streaming=True)
        self.iterable_dataset = iter(self.dataset)
        # check if it's tokenized
        if "tokens" in next(self.iterable_dataset).keys():
            self.cfg.is_dataset_tokenized = True
            print("Dataset is tokenized! Updating config.")
        elif "text" in next(self.iterable_dataset).keys():
            self.cfg.is_dataset_tokenized = False
            print("Dataset is not tokenized! Updating config.")
        if self.cfg.use_cached_activations:
            # Sanity check: does the cache directory exist?
            assert os.path.exists(self.cfg.cached_activations_path), \
                f"Cache directory {self.cfg.cached_activations_path} does not exist. Consider double-checking your dataset, model, and hook names."
            self.next_cache_idx = 0 # which file to open next
            self.next_idx_within_buffer = 0 # where to start reading from in that file
            # Check that we have enough data on disk
            first_buffer = torch.load(f"{self.cfg.cached_activations_path}/0.pt")
            buffer_size_on_disk = first_buffer.shape[0]
            n_buffers_on_disk = len(os.listdir(self.cfg.cached_activations_path))
            # Note: we're assuming all files have the same number of tokens
            # (which seems reasonable imo since that's what our script does)
            n_activations_on_disk = buffer_size_on_disk * n_buffers_on_disk
            assert n_activations_on_disk > self.cfg.total_training_tokens, \
                f"Only {n_activations_on_disk/1e6:.1f}M activations on disk, but cfg.total_training_tokens is {self.cfg.total_training_tokens/1e6:.1f}M."
            # TODO add support for "mixed loading" (ie use cache until you run out, then switch over to streaming from HF)
        if create_dataloader:
            # fill buffer half a buffer, so we can mix it with a new buffer
            self.storage_buffer_out = None
            if self.cfg.is_transcoder:
                # if we're a transcoder, then we want to keep a buffer for our input activations and our output activations
                self.storage_buffer, self.storage_buffer_out = self.get_buffer(self.cfg.n_batches_in_buffer // 2)
            else:
                self.storage_buffer = self.get_buffer(self.cfg.n_batches_in_buffer // 2)
            self.dataloader = self.get_data_loader()
    def get_batch_tokens(self):
        """
        Streams a batch of tokens from a dataset.
        """
        batch_size = self.cfg.store_batch_size
        context_size = self.cfg.context_size
        device = self.cfg.device
        batch_tokens = torch.zeros(size=(0, context_size), device=device, dtype=torch.long, requires_grad=False)
        current_batch = []
        current_length = 0
        # pbar = tqdm(total=batch_size, desc="Filling batches")
        while batch_tokens.shape[0] < batch_size:
            if not self.cfg.is_dataset_tokenized:
                s = next(self.iterable_dataset)["text"]
                tokens = self.model.to_tokens(
                    s, 
                    truncate=True, 
                    move_to_device=True,
                    ).squeeze(0)
                assert len(tokens.shape) == 1, f"tokens.shape should be 1D but was {tokens.shape}"
            else:
                tokens = torch.tensor(
                    next(self.iterable_dataset)["tokens"],
                    dtype=torch.long,
                    device=device,
                    requires_grad=False,
                )
            token_len = tokens.shape[0]
            # TODO: Fix this so that we are limiting how many tokens we get from the same context.
            bos_token_id_tensor = torch.tensor([self.model.tokenizer.bos_token_id], device=tokens.device, dtype=torch.long)
            while token_len > 0 and batch_tokens.shape[0] < batch_size:
                # Space left in the current batch
                space_left = context_size - current_length
                # If the current tokens fit entirely into the remaining space
                if token_len <= space_left:
                    current_batch.append(tokens[:token_len])
                    current_length += token_len
                    break
                else:
                    # Take as much as will fit
                    current_batch.append(tokens[:space_left])
                    # Remove used part, add BOS
                    tokens = tokens[space_left:]
                    tokens = torch.cat(
                        (
                            bos_token_id_tensor,
                            tokens,
                        ),
                        dim=0,
                    )
                    token_len -= space_left
                    token_len += 1
                    current_length = context_size
                # If a batch is full, concatenate and move to next batch
                if current_length == context_size:
                    full_batch = torch.cat(current_batch, dim=0)
                    batch_tokens = torch.cat(
                        (batch_tokens, full_batch.unsqueeze(0)), dim=0
                    )
                    current_batch = []
                    current_length = 0
            # pbar.n = batch_tokens.shape[0]
            # pbar.refresh()
        return batch_tokens[:batch_size]
    def get_activations(self, batch_tokens, get_loss=False):
        # TODO: get transcoders working with head indices
        assert(not (self.cfg.is_transcoder and (self.cfg.hook_point_head_index is not None)))
        act_name = self.cfg.hook_point
        hook_point_layer = self.cfg.hook_point_layer
        if self.cfg.hook_point_head_index is not None:
            activations = self.model.run_with_cache(
                batch_tokens,
                names_filter=act_name,
                stop_at_layer=hook_point_layer+1
            )[
                1
            ][act_name][:,:,self.cfg.hook_point_head_index]
        else:
            if not self.cfg.is_transcoder:
                activations = self.model.run_with_cache(
                    batch_tokens,
                    names_filter=act_name,
                    stop_at_layer=hook_point_layer+1
                )[
                    1
                ][act_name]
            else:
                cache = self.model.run_with_cache(
                    batch_tokens,
                    names_filter=[act_name, self.cfg.out_hook_point],
                    stop_at_layer=self.cfg.out_hook_point_layer+1
                )[1]
                activations = (cache[act_name], cache[self.cfg.out_hook_point])
        return activations
    def get_buffer(self, n_batches_in_buffer):
        gc.collect()
        torch.cuda.empty_cache()
        context_size = self.cfg.context_size
        batch_size = self.cfg.store_batch_size
        d_in = self.cfg.d_in
        total_size = batch_size * n_batches_in_buffer
        # TODO: get transcoders working with cached activations
        assert(not (self.cfg.is_transcoder and self.cfg.use_cached_activations))
        if self.cfg.use_cached_activations:
            # Load the activations from disk
            buffer_size = total_size * context_size
            # Initialize an empty tensor (flattened along all dims except d_in)
            new_buffer = torch.zeros((buffer_size, d_in), dtype=self.cfg.dtype,
                                     device=self.cfg.device)
            n_tokens_filled = 0
            # The activations may be split across multiple files,
            # Or we might only want a subset of one file (depending on the sizes)
            while n_tokens_filled < buffer_size:
                # Load the next file
                # Make sure it exists
                if not os.path.exists(f"{self.cfg.cached_activations_path}/{self.next_cache_idx}.pt"):
                    print("\n\nWarning: Ran out of cached activation files earlier than expected.")
                    print(f"Expected to have {buffer_size} activations, but only found {n_tokens_filled}.")
                    if buffer_size % self.cfg.total_training_tokens != 0:
                        print("This might just be a rounding error — your batch_size * n_batches_in_buffer * context_size is not divisible by your total_training_tokens")
                    print(f"Returning a buffer of size {n_tokens_filled} instead.")
                    print("\n\n")
                    new_buffer = new_buffer[:n_tokens_filled]
                    break
                activations = torch.load(f"{self.cfg.cached_activations_path}/{self.next_cache_idx}.pt")
                # If we only want a subset of the file, take it
                taking_subset_of_file = False
                if n_tokens_filled + activations.shape[0] > buffer_size:
                    activations = activations[:buffer_size - n_tokens_filled]
                    taking_subset_of_file = True
                # Add it to the buffer
                new_buffer[n_tokens_filled : n_tokens_filled + activations.shape[0]] = activations
                # Update counters
                n_tokens_filled += activations.shape[0]
                if taking_subset_of_file:
                    self.next_idx_within_buffer = activations.shape[0]
                else:
                    self.next_cache_idx += 1
                    self.next_idx_within_buffer = 0
            return new_buffer
        refill_iterator = range(0, batch_size * n_batches_in_buffer, batch_size)
        # refill_iterator = tqdm(refill_iterator, desc="generate activations")
        # Initialize empty tensor buffer of the maximum required size
        new_buffer = torch.zeros(
            (total_size, context_size, d_in),
            dtype=self.cfg.dtype,
            device=self.cfg.device,
        )
        new_buffer_out = None
        if self.cfg.is_transcoder:
            new_buffer_out = torch.zeros(
                (total_size, context_size, self.cfg.d_out),
                dtype=self.cfg.dtype,
                device=self.cfg.device,
            )
        # Insert activations directly into pre-allocated buffer
        # pbar = tqdm(total=n_batches_in_buffer, desc="Filling buffer")
        for refill_batch_idx_start in refill_iterator:
            refill_batch_tokens = self.get_batch_tokens()
            if not self.cfg.is_transcoder:
                refill_activations = self.get_activations(refill_batch_tokens)
                new_buffer[
                    refill_batch_idx_start : refill_batch_idx_start + batch_size
                ] = refill_activations
            else:
                refill_activations_in, refill_activations_out = self.get_activations(refill_batch_tokens)
                new_buffer[
                    refill_batch_idx_start : refill_batch_idx_start + batch_size
                ] = refill_activations_in
                new_buffer_out[
                    refill_batch_idx_start : refill_batch_idx_start + batch_size
                ] = refill_activations_out
            # pbar.update(1)
        new_buffer = new_buffer.reshape(-1, d_in)
        randperm = torch.randperm(new_buffer.shape[0])
        new_buffer = new_buffer[randperm]
        if self.cfg.is_transcoder:
            new_buffer_out = new_buffer_out.reshape(-1, self.cfg.d_out)
            new_buffer_out = new_buffer_out[randperm]
        if self.cfg.is_transcoder:
            return new_buffer, new_buffer_out
        else:
            return new_buffer
    def get_data_loader(self,) -> DataLoader:
        '''
        Return a torch.utils.dataloader which you can get batches from.
        Should automatically refill the buffer when it gets to n % full. 
        (better mixing if you refill and shuffle regularly).
        '''
        batch_size = self.cfg.train_batch_size
        if self.cfg.is_transcoder:
            # ugly code duplication if we're a transcoder
            new_buffer, new_buffer_out = self.get_buffer(self.cfg.n_batches_in_buffer // 2)
            mixing_buffer = torch.cat(
                [new_buffer,
                 self.storage_buffer]
            )
            mixing_buffer_out = torch.cat(
                [new_buffer_out,
                 self.storage_buffer_out]
            )
            assert(mixing_buffer.shape[0] == mixing_buffer_out.shape[0])
            randperm = torch.randperm(mixing_buffer.shape[0])
            mixing_buffer = mixing_buffer[randperm]
            mixing_buffer_out = mixing_buffer_out[randperm]
            self.storage_buffer = mixing_buffer[:mixing_buffer.shape[0]//2]
            self.storage_buffer_out = mixing_buffer_out[:mixing_buffer_out.shape[0]//2]
            # have to properly stack both of our new buffers into the dataloader
            """stacked_buffers = torch.stack([
                mixing_buffer[mixing_buffer.shape[0]//2:],
                mixing_buffer_out[mixing_buffer.shape[0]//2:]
            ], dim=1)"""
            catted_buffers = torch.cat([
                mixing_buffer[mixing_buffer.shape[0]//2:],
                mixing_buffer_out[mixing_buffer.shape[0]//2:]
            ], dim=1)
            #dataloader = iter(DataLoader(stacked_buffers, batch_size=batch_size, shuffle=True))
            dataloader = iter(DataLoader(catted_buffers, batch_size=batch_size, shuffle=True))
        else:
            # 1. # create new buffer by mixing stored and new buffer
            mixing_buffer = torch.cat(
                [self.get_buffer(self.cfg.n_batches_in_buffer // 2),
                 self.storage_buffer]
            )
            mixing_buffer = mixing_buffer[torch.randperm(mixing_buffer.shape[0])]
            # 2.  put 50 % in storage
            self.storage_buffer = mixing_buffer[:mixing_buffer.shape[0]//2]
            # 3. put other 50 % in a dataloader
            dataloader = iter(DataLoader(mixing_buffer[mixing_buffer.shape[0]//2:], batch_size=batch_size, shuffle=True))
        return dataloader
    def next_batch(self):
        """
        Get the next batch from the current DataLoader. 
        If the DataLoader is exhausted, refill the buffer and create a new DataLoader.
        """
        try:
            # Try to get the next batch
            return next(self.dataloader)
        except StopIteration:
            # If the DataLoader is exhausted, create a new one
            self.dataloader = self.get_data_loader()
            return next(self.dataloader)

================
File: sae_training/config.py
================
from abc import ABC
from dataclasses import dataclass
from typing import Optional
import torch
import wandb
@dataclass
class RunnerConfig(ABC):
    """
    The config that's shared across all runners.
    """
    # Data Generating Function (Model + Training Distibuion)
    model_name: str = "gelu-2l"
    hook_point: str = "blocks.0.hook_mlp_out"
    hook_point_layer: int = 0
    hook_point_head_index: Optional[int] = None
    dataset_path: str = "NeelNanda/c4-tokenized-2b"
    is_dataset_tokenized: bool = True
    context_size: int = 128
    use_cached_activations: bool = False
    cached_activations_path: Optional[
        str
    ] = None  # Defaults to "activations/{dataset}/{model}/{full_hook_name}_{hook_point_head_index}"
    # SAE Parameters
    d_in: int = 512
    # Activation Store Parameters
    n_batches_in_buffer: int = 20
    total_training_tokens: int = 2_000_000
    store_batch_size: int = 1024
    # Misc
    device: str = "cpu"
    seed: int = 42
    dtype: torch.dtype = torch.float32
    # transcoder stuff
    is_transcoder: bool = False
    out_hook_point: Optional[str] = None
    out_hook_point_layer: Optional[int] = None
    d_out: Optional[int] = None
    # sparse-connection sparse transcoder stuff
    is_sparse_connection: bool = False
    sparse_connection_sae_path: Optional[str] = None
    sparse_connection_l1_coeff: Optional[float] = None
    sparse_connection_use_W_enc: bool = True
    def __post_init__(self):
        # Autofill cached_activations_path unless the user overrode it
        if self.cached_activations_path is None:
            self.cached_activations_path = f"activations/{self.dataset_path.replace('/', '_')}/{self.model_name.replace('/', '_')}/{self.hook_point}"
            if self.hook_point_head_index is not None:
                self.cached_activations_path += f"_{self.hook_point_head_index}"
@dataclass
class LanguageModelSAERunnerConfig(RunnerConfig):
    """
    Configuration for training a sparse autoencoder on a language model.
    """
    # SAE Parameters
    b_dec_init_method: str = "geometric_median"
    expansion_factor: int = 4
    from_pretrained_path: Optional[str] = None
    # Training Parameters
    l1_coefficient: float = 1e-3
    lr: float = 3e-4
    lr_scheduler_name: str = "constant"  # constant, constantwithwarmup, linearwarmupdecay, cosineannealing, cosineannealingwarmup
    lr_warm_up_steps: int = 500
    train_batch_size: int = 4096
    # transcoder stuff
    is_transcoder: bool = False
    out_hook_point: Optional[str] = None
    out_hook_point_layer: Optional[int] = None
    d_out: Optional[int] = None
    # sparse-connection sparse transcoder stuff
    is_sparse_connection: bool = False
    sparse_connection_sae_path: Optional[str] = None
    sparse_connection_l1_coeff: Optional[float] = None
    sparse_connection_use_W_enc: bool = True
    # Resampling protocol args
    use_ghost_grads: bool = False, # want to change this to true on some timeline.
    feature_sampling_window: int = 2000
    feature_sampling_method: str = "Anthropic"  # None or Anthropic
    resample_batches: int = 32
    feature_reinit_scale: float = 0.2
    dead_feature_window: int = 1000  # unless this window is larger feature sampling,
    dead_feature_estimation_method: str = "no_fire"
    dead_feature_threshold: float = 1e-8
    # WANDB
    log_to_wandb: bool = True
    wandb_project: str = "mats_sae_training_language_model"
    wandb_entity: str = None
    wandb_log_frequency: int = 10
    # Misc
    n_checkpoints: int = 0
    checkpoint_path: str = "checkpoints"
    use_tqdm: bool = True
    def __post_init__(self):
        super().__post_init__()
        self.d_sae = self.d_in * self.expansion_factor
        self.tokens_per_buffer = (
            self.train_batch_size * self.context_size * self.n_batches_in_buffer
        )
        self.run_name = f"{self.d_sae}-L1-{self.l1_coefficient}-LR-{self.lr}-Tokens-{self.total_training_tokens:3.3e}"
        if self.feature_sampling_method not in [None, "l2", "anthropic"]:
            raise ValueError(
                f"feature_sampling_method must be None, l2, or anthropic. Got {self.feature_sampling_method}"
            )
        if self.b_dec_init_method not in ["geometric_median", "mean", "zeros"]:
            raise ValueError(
                f"b_dec_init_method must be geometric_median, mean, or zeros. Got {self.b_dec_init_method}"
            )
        if self.b_dec_init_method == "zeros":
            print(
                "Warning: We are initializing b_dec to zeros. This is probably not what you want."
            )
        self.device = torch.device(self.device)
        unique_id = wandb.util.generate_id()
        self.checkpoint_path = f"{self.checkpoint_path}/{unique_id}"
        print(
            f"Run name: {self.d_sae}-L1-{self.l1_coefficient}-LR-{self.lr}-Tokens-{self.total_training_tokens:3.3e}"
        )
        # Print out some useful info:
        n_tokens_per_buffer = (
            self.store_batch_size * self.context_size * self.n_batches_in_buffer
        )
        print(f"n_tokens_per_buffer (millions): {n_tokens_per_buffer / 10 **6}")
        n_contexts_per_buffer = self.store_batch_size * self.n_batches_in_buffer
        print(
            f"Lower bound: n_contexts_per_buffer (millions): {n_contexts_per_buffer / 10 **6}"
        )
        total_training_steps = self.total_training_tokens // self.train_batch_size
        print(f"Total training steps: {total_training_steps}")
        total_wandb_updates = total_training_steps // self.wandb_log_frequency
        print(f"Total wandb updates: {total_wandb_updates}")
        # how many times will we sample dead neurons?
        # assert self.dead_feature_window <= self.feature_sampling_window, "dead_feature_window must be smaller than feature_sampling_window"
        n_dead_feature_samples = total_training_steps // self.dead_feature_window
        n_feature_window_samples = total_training_steps // self.feature_sampling_window
        print(
            f"n_tokens_per_feature_sampling_window (millions): {(self.feature_sampling_window * self.context_size * self.train_batch_size) / 10 **6}"
        )
        print(
            f"n_tokens_per_dead_feature_window (millions): {(self.dead_feature_window * self.context_size * self.train_batch_size) / 10 **6}"
        )
        if self.feature_sampling_method != None:
            print(f"We will reset neurons {n_dead_feature_samples} times.")
        if self.use_ghost_grads:
            print("Using Ghost Grads.")
        print(
            f"We will reset the sparsity calculation {n_feature_window_samples} times."
        )
        print(f"Number of tokens when resampling: {self.resample_batches * self.store_batch_size}")
        # print("Number tokens in dead feature calculation window: ", self.dead_feature_window * self.train_batch_size)
        print(
            f"Number tokens in sparsity calculation window: {self.feature_sampling_window * self.train_batch_size:.2e}"
        )
@dataclass
class CacheActivationsRunnerConfig(RunnerConfig):
    """
    Configuration for caching activations of an LLM.
    """
    # Activation caching stuff
    shuffle_every_n_buffers: int = 10
    n_shuffles_with_last_section: int = 10
    n_shuffles_in_entire_dir: int = 10
    n_shuffles_final: int = 100
    def __post_init__(self):
        super().__post_init__()
        if self.use_cached_activations:
            # this is a dummy property in this context; only here to avoid class compatibility headaches
            raise ValueError(
                "use_cached_activations should be False when running cache_activations_runner"
            )

================
File: sae_training/geom_median/__init__.py
================
from .src.geom_median import numpy, torch

================
File: sae_training/geom_median/README.md
================
# Differentiable and Fast Geometric Median in NumPy and PyTorch

This package implements a fast numerical algorithm to compute the geometric median of high dimensional vectors.
As a generalization of the median (of scalars), the [geometric median](https://en.wikipedia.org/wiki/Geometric_median) 
is a robust estimator of the mean in the presence of outliers and contaminations (adversarial or otherwise). 

![illustration](fig/illustration.png)

It is defined as the minimizer of the convex optimization problem as follows.
![definition](fig/gm.jpg)

The geometric median is also known as the Fermat point, Weber's L1 median, Fréchet median among others. 
It has a breakdown point of 0.5, meaning that it yields a robust aggregate even under arbitrary corruptions to points accounting for under half the total weight. We use the smoothed Weiszfeld algorithm to compute the geometric median. 

**Features**:
- Implementation in both NumPy and PyTorch.
- PyTorch implementation is fully differentiable (compatible with gradient backpropagation a.k.a. automatic differentiation) and can run on GPUs with CUDA tensors.
- Blazing fast algorithm that converges linearly in almost all practical settings. 

# Installation
This package can be installed via pip as `pip install geom_median`. Alternatively, for an editable install, 
run
```bash
git clone git@github.com:krishnap25/geom_median.git
cd geom_median
pip install -e .
```

You must have a working installation of PyTorch, version 1.7 or over in case you wish to use the PyTorch API. 
See details [here](https://pytorch.org/get-started/locally/).

# Usage Guide
We describe the PyTorch usage here. The NumPy API is entirely analogous. 

```python
import torch
from geom_median.torch import compute_geometric_median   # PyTorch API
# from geom_median.numpy import compute_geometric_median  # NumPy API
```

For the simplest use case, supply a list of tensors: 

```python
n = 10  # Number of vectors
d = 25  # dimensionality of each vector
points = [torch.rand(d) for _ in range(n)]   # list of n tensors of shape (d,)
# The shape of each tensor is the same and can be arbitrary (not necessarily 1-dimensional)
weights = torch.rand(n)  # non-negative weights of shape (n,)
out = compute_geometric_median(points, weights)
# Access the median via `out.median`, which has the same shape as the points, i.e., (d,)
```
The termination condition can be examined through `out.termination`, which gives a message such as 
`"function value converged within tolerance"` or `"maximum iterations reached"`.

We also support a use case where each point is given by list of tensors. 
For instance, each point is the list of parameters of a `torch.nn.Module` for instance as `point = list(module.parameters())`.
In this case, this is equivalent to flattening and concatenating all the tensors into a single vector via 
`flatted_point = torch.stack([v.view(-1) for v in point])`.
This functionality can be invoked as follows: 

```python
models = [torch.nn.Linear(20, 10) for _ in range(n)]  # a list of n models
points = [list(model.parameters()) for model in models]  # list of points, where each point is a list of tensors
out = compute_geometric_median(points, weights=None)  # equivalent to `weights = torch.ones(n)`. 
# Access the median via `out.median`, also given as a list of tensors
```

We also support computing the geometric median for each component separately in the list-of-tensors format:
```python
models = [torch.nn.Linear(20, 10) for _ in range(n)]  # a list of n models
points = [list(model.parameters()) for model in models]  # list of points, where each point is a list of tensors
out = compute_geometric_median(points, weights=None, per_component=True)  
# Access the median via `out.median`, also given as a list of tensors
```
This per-component geometric median is equivalent in functionality to 
```python
out.median[j] = compute_geometric_median([p[j] for p in points], weights)
```

## Backpropagation support
When using the PyTorch API, the result `out.median`, as a function of `points`, supports gradient backpropagation, also known as reverse-mode automatic differentiation. Here is a toy example illustrating this behavior.
```python
points = [torch.rand(d).requires_grad_(True) for _ in range(n)]   # list of tensors with `requires_grad=True`
out = compute_geometric_median(points, weights=None)
torch.linalg.norm(out.median).backward()  # call backward on any downstream function of `out.median`
gradients = [p.grad for p in points]  # gradients with respect of `points` and upstream nodes in the computation graph
```

## GPU support
Simply use as above where `points` and `weights` are CUDA tensors. 

# Authors and Contact
[Krishna Pillutla](https://krishnap25.github.io/)   
[Sham Kakade](https://sham.seas.harvard.edu/)   
[Zaid Harchaoui](https://faculty.washington.edu/zaid/)

In case of questions, please raise an issue on GitHub. 

# Citation
If you found this package useful, please consider citing this paper. 

```
@article{pillutla:etal:rfa,
  author={Pillutla, Krishna and Kakade, Sham M. and Harchaoui, Zaid},
  journal={IEEE Transactions on Signal Processing}, 
  title={{Robust Aggregation for Federated Learning}}, 
  year={2022},
  volume={70},
  number={},
  pages={1142-1154},
  doi={10.1109/TSP.2022.3153135}
}
```

================
File: sae_training/geom_median/setup.py
================
import setuptools
with open("README.md", "r", encoding="utf-8") as fh:
    long_description = fh.read()
setuptools.setup(
    name="geom_median",
    version="0.1.0",
    author="Krishna Pillutla",
    author_email="pillutla@cs.washington.edu",
    description="Implementation of the smoothed Weiszfeld algorithm to compute the geometric median",
    long_description=long_description,
    long_description_content_type="text/markdown",
    url="https://github.com/krishnap25/geom_median",
    project_urls={
        "Bug Tracker": "https://github.com/krishnap25/geom_median/issues",
    },
    classifiers=[
        "Programming Language :: Python :: 3",
        "License :: OSI Approved :: GNU General Public License v3 (GPLv3)",
        "Operating System :: OS Independent",
    ],
    package_dir={"": "src"},
    packages=setuptools.find_packages(where="src"),
    python_requires=">=3.6",
    install_requires=[
        'numpy>=1.18.1',
        ]
)

================
File: sae_training/geom_median/src/geom_median/__init__.py
================


================
File: sae_training/geom_median/src/geom_median/numpy/__init__.py
================
from .main import compute_geometric_median
__all__ = [compute_geometric_median]

================
File: sae_training/geom_median/src/geom_median/numpy/main.py
================
import numpy as np
from .weiszfeld_array import geometric_median_array, geometric_median_per_component
from .weiszfeld_list_of_array import geometric_median_list_of_array
from . import utils
def compute_geometric_median(
	points, weights=None, per_component=False, skip_typechecks=False,
	eps=1e-6, maxiter=100, ftol=1e-20
):
	""" Compute the geometric median of points `points` with weights given by `weights`. 
	"""
	if weights is None:
		n = len(points)
		weights = np.ones(n)
	if type(points) == np.ndarray:
		# `points` are given as an array of shape (n, d)
		points = [p for p in points]  # translate to list of arrays format
	if type(points) not in [list, tuple]:
		raise ValueError(
			f"We expect `points` as a list of arrays or a list of tuples of arrays. Got {type(points)}"
		)
	if type(points[0]) == np.ndarray: # `points` are given in list of arrays format
		if not skip_typechecks:
			utils.check_list_of_array_format(points)
		to_return = geometric_median_array(points, weights, eps, maxiter, ftol)
	elif type(points[0]) in [list, tuple]: # `points` are in list of list of arrays format
		if not skip_typechecks:
			utils.check_list_of_list_of_array_format(points)
		if per_component:
			to_return = geometric_median_per_component(points, weights, eps, maxiter, ftol)
		else:
			to_return = geometric_median_list_of_array(points, weights, eps, maxiter, ftol)
	else:
		raise ValueError(f"Unexpected format {type(points[0])} for list of list format.")
	return to_return

================
File: sae_training/geom_median/src/geom_median/numpy/utils.py
================
from itertools import zip_longest
import numpy as np
def check_list_of_array_format(points):
	check_shapes_compatibility(points, -1)
def check_list_of_list_of_array_format(points):
	# each element of `points` is a list of arrays of compatible shapes
	components = zip_longest(*points, fillvalue=np.array(0))
	for i, component in enumerate(components):
		check_shapes_compatibility(component, i)
def check_shapes_compatibility(list_of_arrays, i):
	arr0 = list_of_arrays[0]
	if not isinstance(arr0, np.ndarray):
		raise ValueError(
			"Expected points of format list of `numpy.ndarray`s.", 
			f"Got {type(arr0)} for component {i} of point 0."
		)
	shape = arr0.shape
	for j, arr in enumerate(list_of_arrays[1:]):
		if not isinstance(arr, np.ndarray):
			raise ValueError(
				f"Expected points of format list of `numpy.ndarray`s. Got {type(arr)}",
				f"for component {i} of point {j+1}."
			)
		if arr.shape != shape:
			raise ValueError(
				f"Expected shape {shape} for component {i} of point {j+1}.",
				f"Got shape {arr.shape} instead."
			)

================
File: sae_training/geom_median/src/geom_median/numpy/weiszfeld_array.py
================
import numpy as np
from types import SimpleNamespace
def geometric_median_array(points, weights, eps=1e-6, maxiter=100, ftol=1e-20):
    """
    :param points: list of length :math:`n`, whose elements are each a ``numpy.array`` of shape ``(d,)``
    :param weights: ``numpy.array`` of shape :math:``(n,)``.
    :param eps: Smallest allowed value of denominator, to avoid divide by zero. 
    	Equivalently, this is a smoothing parameter. Default 1e-6. 
    :param maxiter: Maximum number of Weiszfeld iterations. Default 100
    :param ftol: If objective value does not improve by at least this `ftol` fraction, terminate the algorithm. Default 1e-20.
    :return: SimpleNamespace object with fields
        - `median`: estimate of the geometric median, which is a ``numpy.array`` object of shape :math:``(d,)``
        - `termination`: string explaining how the algorithm terminated.
        - `logs`: function values encountered through the course of the algorithm in a list.
    """
    # initialize median estimate at mean
    median = weighted_average(points, weights)
    objective_value = geometric_median_objective(median, points, weights)
    logs = [objective_value]
    # Weiszfeld iterations
    early_termination = False
    for _ in range(maxiter):
        prev_obj_value = objective_value
        norms = [np.linalg.norm((p - median).reshape(-1)) for p in points]
        new_weights = weights / np.maximum(eps, norms)
        median = weighted_average(points, new_weights)
        objective_value = geometric_median_objective(median, points, weights)
        logs.append(objective_value)
        if abs(prev_obj_value - objective_value) <= ftol * objective_value:
            early_termination = True
            break
    return SimpleNamespace(
        median=median,
        termination="function value converged within tolerance" if early_termination else "maximum iterations reached",
        logs=logs,
    )
def geometric_median_per_component(points, weights, eps=1e-6, maxiter=100, ftol=1e-20):
    """
    :param points: list of length :math:``n``, where each element is itself a list of ``numpy.ndarray``.
        Each inner list has the same "shape".
    :param weights: ``numpy.ndarray`` of shape :math:``(n,)``.
    :param eps: Smallest allowed value of denominator, to avoid divide by zero. 
    	Equivalently, this is a smoothing parameter. Default 1e-6. 
    :param maxiter: Maximum number of Weiszfeld iterations. Default 100
    :param ftol: If objective value does not improve by at least this `ftol` fraction, terminate the algorithm. Default 1e-20.
    :return: SimpleNamespace object with fields
        - `median`: estimate of the geometric median, which is a list of ``numpy.ndarray`` of the same "shape" as the input.
        - `termination`: string explaining how the algorithm terminated, one for each component. 
        - `logs`: function values encountered through the course of the algorithm.
    """
    components = list(zip(*points))
    median = []
    termination = []
    logs = []
    for component in components:
        ret = geometric_median_array(component, weights, eps, maxiter, ftol)
        median.append(ret.median)
        termination.append(ret.termination)
        logs.append(ret.logs)
    return SimpleNamespace(median=median, termination=termination, logs=logs)
def weighted_average(points, weights):
    """
    Compute a weighted average of rows of `points`, with each row weighted by the corresponding entry in `weights`
    :param points: ``np.ndarray`` of shape (n, d, ...)
    :param weights: ``np.ndarray`` of shape (n,)
    :return: weighted average, np.ndarray of shape (d, ...)
    """
    return np.average(points, weights=weights, axis=0)
def geometric_median_objective(median, points, weights):
    return np.average([np.linalg.norm((p - median).reshape(-1)) for p in points], weights=weights)

================
File: sae_training/geom_median/src/geom_median/numpy/weiszfeld_list_of_array.py
================
import numpy as np
from types import SimpleNamespace
def geometric_median_list_of_array(points, weights, eps=1e-6, maxiter=100, ftol=1e-20):
    """
    :param points: list of length :math:``n``, where each element is itself a list of ``numpy.ndarray``.
        Each inner list has the same "shape".
    :param weights: ``numpy.ndarray`` of shape :math:``(n,)``.
    :param eps: Smallest allowed value of denominator, to avoid divide by zero. 
    	Equivalently, this is a smoothing parameter. Default 1e-6. 
    :param maxiter: Maximum number of Weiszfeld iterations. Default 100
    :param ftol: If objective value does not improve by at least this `ftol` fraction, terminate the algorithm. Default 1e-20.
    :return: SimpleNamespace object with fields
        - `median`: estimate of the geometric median, which is a list of ``numpy.ndarray`` of the same "shape" as the input.
        - `termination`: string explaining how the algorithm terminated.
        - `logs`: function values encountered through the course of the algorithm in a list.
    """
    # initialize median estimate at mean
    median = weighted_average(points, weights)
    objective_value = geometric_median_objective(median, points, weights)
    logs = [objective_value]
    # Weiszfeld iterations
    early_termination = False
    for _ in range(maxiter):
        prev_obj_value = objective_value
        new_weights = weights / np.maximum(eps, np.asarray([l2distance(p, median) for p in points]))
        median = weighted_average(points, new_weights)
        objective_value = geometric_median_objective(median, points, weights)
        logs.append(objective_value)
        if abs(prev_obj_value - objective_value) <= ftol * objective_value:
            early_termination = True
            break
    return SimpleNamespace(
        median=median,
        termination="function value converged within tolerance" if early_termination else "maximum iterations reached",
        logs=logs,
    )
def weighted_average(points, weights):
    return [np.average(component, weights=weights, axis=0) for component in zip(*points)]
def geometric_median_objective(median, points, weights):
    return np.average([l2distance(p, median) for p in points], weights=weights)
# Simple operators for list-of-array format
def l2distance(p1, p2):
    return np.linalg.norm([np.linalg.norm(x1 - x2) for (x1, x2) in zip(p1, p2)])
def subtract(p1, p2):
    return [x1 - x2 for (x1, x2) in zip(p1, p2)]

================
File: sae_training/geom_median/src/geom_median/torch/__init__.py
================
from .main import compute_geometric_median
__all__ = [compute_geometric_median]

================
File: sae_training/geom_median/src/geom_median/torch/main.py
================
import torch
from .weiszfeld_array import geometric_median_array, geometric_median_per_component
from .weiszfeld_list_of_array import geometric_median_list_of_array
from . import utils
def compute_geometric_median(
	points, weights=None, per_component=False, skip_typechecks=False,
	eps=1e-6, maxiter=100, ftol=1e-20
):
	""" Compute the geometric median of points `points` with weights given by `weights`. 
	"""
	if type(points) == torch.Tensor:
		# `points` are given as an array of shape (n, d)
		points = [p for p in points]  # translate to list of arrays format
	if type(points) not in [list, tuple]:
		raise ValueError(
			f"We expect `points` as a list of arrays or a list of tuples of arrays. Got {type(points)}"
		)
	if type(points[0]) == torch.Tensor: # `points` are given in list of arrays format
		if not skip_typechecks:
			utils.check_list_of_array_format(points)
		if weights is None:
			weights = torch.ones(len(points), device=points[0].device)
		to_return = geometric_median_array(points, weights, eps, maxiter, ftol)
	elif type(points[0]) in [list, tuple]: # `points` are in list of list of arrays format
		if not skip_typechecks:
			utils.check_list_of_list_of_array_format(points)
		if weights is None:
			weights = torch.ones(len(points), device=points[0][0].device)
		if per_component:
			to_return = geometric_median_per_component(points, weights, eps, maxiter, ftol)
		else:
			to_return = geometric_median_list_of_array(points, weights, eps, maxiter, ftol)
	else:
		raise ValueError(f"Unexpected format {type(points[0])} for list of list format.")
	return to_return

================
File: sae_training/geom_median/src/geom_median/torch/utils.py
================
from itertools import zip_longest
import torch
def check_list_of_array_format(points):
	check_shapes_compatibility(points, -1)
def check_list_of_list_of_array_format(points):
	# each element of `points` is a list of arrays of compatible shapes
	components = zip_longest(*points, fillvalue=torch.Tensor())
	for i, component in enumerate(components):
		check_shapes_compatibility(component, i)
def check_shapes_compatibility(list_of_arrays, i):
	arr0 = list_of_arrays[0]
	if not isinstance(arr0, torch.Tensor):
		raise ValueError(
			"Expected points of format list of `torch.Tensor`s.", 
			f"Got {type(arr0)} for component {i} of point 0."
		)
	shape = arr0.shape
	for j, arr in enumerate(list_of_arrays[1:]):
		if not isinstance(arr, torch.Tensor):
			raise ValueError(
				f"Expected points of format list of `torch.Tensor`s. Got {type(arr)}",
				f"for component {i} of point {j+1}."
			)
		if arr.shape != shape:
			raise ValueError(
				f"Expected shape {shape} for component {i} of point {j+1}.",
				f"Got shape {arr.shape} instead."
			)

================
File: sae_training/geom_median/src/geom_median/torch/weiszfeld_array.py
================
from types import SimpleNamespace
import numpy as np
import torch
import tqdm
def geometric_median_array(points, weights, eps=1e-6, maxiter=100, ftol=1e-20):
    """
    :param points: list of length :math:`n`, whose elements are each a ``torch.Tensor`` of shape ``(d,)``
    :param weights: ``torch.Tensor`` of shape :math:``(n,)``.
    :param eps: Smallest allowed value of denominator, to avoid divide by zero. 
    	Equivalently, this is a smoothing parameter. Default 1e-6. 
    :param maxiter: Maximum number of Weiszfeld iterations. Default 100
    :param ftol: If objective value does not improve by at least this `ftol` fraction, terminate the algorithm. Default 1e-20.
    :return: SimpleNamespace object with fields
        - `median`: estimate of the geometric median, which is a ``torch.Tensor`` object of shape :math:``(d,)``
        - `termination`: string explaining how the algorithm terminated.
        - `logs`: function values encountered through the course of the algorithm in a list.
    """
    with torch.no_grad():
        # initialize median estimate at mean
        new_weights = weights
        median = weighted_average(points, weights)
        objective_value = geometric_median_objective(median, points, weights)
        logs = [objective_value]
        # Weiszfeld iterations
        early_termination = False
        pbar = tqdm.tqdm(range(maxiter))
        for _ in pbar:
            prev_obj_value = objective_value
            norms = torch.stack([torch.linalg.norm((p - median).view(-1)) for p in points])
            new_weights = weights / torch.clamp(norms, min=eps)
            median = weighted_average(points, new_weights)
            objective_value = geometric_median_objective(median, points, weights)
            logs.append(objective_value)
            if abs(prev_obj_value - objective_value) <= ftol * objective_value:
                early_termination = True
                break
            pbar.set_description(f"Objective value: {objective_value:.4f}")
    median = weighted_average(points, new_weights)  # allow autodiff to track it
    return SimpleNamespace(
        median=median,
        new_weights=new_weights,
        termination="function value converged within tolerance" if early_termination else "maximum iterations reached",
        logs=logs,
    )
def geometric_median_per_component(points, weights, eps=1e-6, maxiter=100, ftol=1e-20):
    """
    :param points: list of length :math:``n``, where each element is itself a list of ``numpy.ndarray``.
        Each inner list has the same "shape".
    :param weights: ``numpy.ndarray`` of shape :math:``(n,)``.
    :param eps: Smallest allowed value of denominator, to avoid divide by zero. 
    	Equivalently, this is a smoothing parameter. Default 1e-6. 
    :param maxiter: Maximum number of Weiszfeld iterations. Default 100
    :param ftol: If objective value does not improve by at least this `ftol` fraction, terminate the algorithm. Default 1e-20.
    :return: SimpleNamespace object with fields
        - `median`: estimate of the geometric median, which is a list of ``numpy.ndarray`` of the same "shape" as the input.
        - `termination`: string explaining how the algorithm terminated, one for each component. 
        - `logs`: function values encountered through the course of the algorithm.
    """
    components = list(zip(*points))
    median = []
    termination = []
    logs = []
    new_weights = []
    pbar = tqdm.tqdm(components)
    for component in pbar:
        ret = geometric_median_array(component, weights, eps, maxiter, ftol)
        median.append(ret.median)
        new_weights.append(ret.new_weights)
        termination.append(ret.termination)
        logs.append(ret.logs)
    return SimpleNamespace(median=median, termination=termination, logs=logs)
def weighted_average(points, weights):
    weights = weights / weights.sum()
    ret = points[0] * weights[0]
    for i in range(1, len(points)):
        ret += points[i] * weights[i]
    return ret
@torch.no_grad()
def geometric_median_objective(median, points, weights):
    return np.average([torch.linalg.norm((p - median).reshape(-1)).item() for p in points], weights=weights.cpu())

================
File: sae_training/geom_median/src/geom_median/torch/weiszfeld_list_of_array.py
================
import numpy as np
import torch
from types import SimpleNamespace
def geometric_median_list_of_array(points, weights, eps=1e-6, maxiter=100, ftol=1e-20):
    """
    :param points: list of length :math:``n``, where each element is itself a list of ``torch.Tensor``.
        Each inner list has the same "shape".
    :param weights: ``torch.Tensor`` of shape :math:``(n,)``.
    :param eps: Smallest allowed value of denominator, to avoid divide by zero. 
    	Equivalently, this is a smoothing parameter. Default 1e-6. 
    :param maxiter: Maximum number of Weiszfeld iterations. Default 100
    :param ftol: If objective value does not improve by at least this `ftol` fraction, terminate the algorithm. Default 1e-20.
    :return: SimpleNamespace object with fields
        - `median`: estimate of the geometric median, which is a list of ``torch.Tensor`` of the same "shape" as the input.
        - `termination`: string explaining how the algorithm terminated.
        - `logs`: function values encountered through the course of the algorithm in a list.
    """
    with torch.no_grad():
        # initialize median estimate at mean
        median = weighted_average(points, weights)
        new_weights = weights
        objective_value = geometric_median_objective(median, points, weights)
        logs = [objective_value]
        # Weiszfeld iterations
        early_termination = False
        for _ in range(maxiter):
            prev_obj_value = objective_value
            denom = torch.stack([l2distance(p, median) for p in points])
            new_weights = weights / torch.clamp(denom, min=eps) 
            median = weighted_average(points, new_weights)
            objective_value = geometric_median_objective(median, points, weights)
            logs.append(objective_value)
            if abs(prev_obj_value - objective_value) <= ftol * objective_value:
                early_termination = True
                break
    median = weighted_average(points, new_weights)  # for autodiff
    return SimpleNamespace(
        median=median,
        new_weights=new_weights,
        termination="function value converged within tolerance" if early_termination else "maximum iterations reached",
        logs=logs,
    )
def weighted_average_component(points, weights):
    ret = points[0] * weights[0]
    for i in range(1, len(points)):
        ret += points[i] * weights[i]
    return ret
def weighted_average(points, weights):
    weights = weights / weights.sum()
    return [weighted_average_component(component, weights=weights) for component in zip(*points)]
@torch.no_grad()
def geometric_median_objective(median, points, weights):
    return np.average([l2distance(p, median).item() for p in points], weights=weights.cpu())
@torch.no_grad()
def l2distance(p1, p2):
    return torch.linalg.norm(torch.stack([torch.linalg.norm(x1 - x2) for (x1, x2) in zip(p1, p2)]))

================
File: sae_training/optim.py
================
'''
Took the LR scheduler from my previous work: https://github.com/jbloomAus/DecisionTransformerInterpretability/blob/ee55df35cdb92e81d689c72fb9dd5a7252893363/src/decision_transformer/utils.py#L425
'''
import math
from typing import Optional
import torch.optim as optim
import torch.optim.lr_scheduler as lr_scheduler
#  None
#  Linear Warmup and decay
#  Cosine Annealing with Warmup
#  Cosine Annealing with Warmup / Restarts
def get_scheduler(
    scheduler_name: Optional[str], optimizer: optim.Optimizer, **kwargs
):
    """
    Loosely based on this, seemed simpler write this than import
    transformers: https://huggingface.co/docs/transformers/main_classes/optimizer_schedules
    Args:
        scheduler_name (Optional[str]): Name of the scheduler to use. If None, returns a constant scheduler
        optimizer (optim.Optimizer): Optimizer to use
        **kwargs: Additional arguments to pass to the scheduler including warm_up_steps,
            training_steps, num_cycles, lr_end.
    """
    def get_warmup_lambda(warm_up_steps, training_steps):
        def lr_lambda(steps):
            if steps < warm_up_steps:
                return (steps + 1) / warm_up_steps
            else:
                return (training_steps - steps) / (
                    training_steps - warm_up_steps
                )
        return lr_lambda
    # heavily derived from hugging face although copilot helped.
    def get_warmup_cosine_lambda(warm_up_steps, training_steps, lr_end):
        def lr_lambda(steps):
            if steps < warm_up_steps:
                return (steps + 1) / warm_up_steps
            else:
                progress = (steps - warm_up_steps) / (
                    training_steps - warm_up_steps
                )
                return lr_end + 0.5 * (1 - lr_end) * (
                    1 + math.cos(math.pi * progress)
                )
        return lr_lambda
    if scheduler_name is None or scheduler_name.lower() == "constant":
        return lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda steps: 1.0)
    elif scheduler_name.lower() == "constantwithwarmup":
        warm_up_steps = kwargs.get("warm_up_steps", 0)
        return lr_scheduler.LambdaLR(
            optimizer,
            lr_lambda=lambda steps: min(1.0, (steps + 1) / warm_up_steps),
        )
    elif scheduler_name.lower() == "linearwarmupdecay":
        warm_up_steps = kwargs.get("warm_up_steps", 0)
        training_steps = kwargs.get("training_steps")
        lr_lambda = get_warmup_lambda(warm_up_steps, training_steps)
        return lr_scheduler.LambdaLR(optimizer, lr_lambda)
    elif scheduler_name.lower() == "cosineannealing":
        training_steps = kwargs.get("training_steps")
        eta_min = kwargs.get("lr_end", 0)
        return lr_scheduler.CosineAnnealingLR(
            optimizer, T_max=training_steps, eta_min=eta_min
        )
    elif scheduler_name.lower() == "cosineannealingwarmup":
        warm_up_steps = kwargs.get("warm_up_steps", 0)
        training_steps = kwargs.get("training_steps")
        eta_min = kwargs.get("lr_end", 0)
        lr_lambda = get_warmup_cosine_lambda(
            warm_up_steps, training_steps, eta_min
        )
        return lr_scheduler.LambdaLR(optimizer, lr_lambda)
    elif scheduler_name.lower() == "cosineannealingwarmrestarts":
        training_steps = kwargs.get("training_steps")
        eta_min = kwargs.get("lr_end", 0)
        num_cycles = kwargs.get("num_cycles", 1)
        T_0 = training_steps // num_cycles
        return lr_scheduler.CosineAnnealingWarmRestarts(
            optimizer, T_0=T_0, eta_min=eta_min
        )
    else:
        raise ValueError(f"Unsupported scheduler: {scheduler_name}")

================
File: sae_training/requirements.txt
================
datasets==2.12.0
einops==0.7.0
ipython==8.12.3
jaxtyping==0.2.28
matplotlib==3.7.1
plotly==5.18.0
setuptools==67.7.2
tqdm==4.65.0
transformer_lens==1.15.0
wandb==0.16.0

================
File: sae_training/sparse_autoencoder.py
================
"""Most of this is just copied over from Arthur's code and slightly simplified:
https://github.com/ArthurConmy/sae/blob/main/sae/model.py
"""
import gzip
import os
import pickle
from functools import partial
import einops
import torch
import torch.nn.functional as F
from jaxtyping import Float
from torch import Tensor, nn
from torch.distributions.categorical import Categorical
from tqdm import tqdm
from transformer_lens.hook_points import HookedRootModule, HookPoint
from .geom_median.src.geom_median.torch import compute_geometric_median
class SparseAutoencoder(HookedRootModule):
    """
    """
    def __init__(
        self,
        cfg,
    ):
        super().__init__()
        self.cfg = cfg
        self.d_in = cfg.d_in
        if not isinstance(self.d_in, int):
            raise ValueError(
                f"d_in must be an int but was {self.d_in=}; {type(self.d_in)=}"
            )
        self.d_sae = cfg.d_sae
        self.l1_coefficient = cfg.l1_coefficient
        self.dtype = cfg.dtype
        self.device = cfg.device
        # transcoder stuff
        self.d_out = self.d_in
        if cfg.is_transcoder and cfg.d_out is not None:
            self.d_out = cfg.d_out
        # sparse-connection transcoder stuff
        self.spacon_sae_W_dec = None
        if cfg.is_sparse_connection:
            # load in the sae decoder weights that we'll use to train sparse connections
            sparse_connection_sae_path = cfg.sparse_connection_sae_path
            if sparse_connection_sae_path.endswith(".pt"):
                state_dict = torch.load(sparse_connection_sae_path)
            elif sparse_connection_sae_path.endswith(".pkl.gz"):
                with gzip.open(sparse_connection_sae_path, 'rb') as f:
                    state_dict = pickle.load(f)
            elif sparse_connection_sae_path.endswith(".pkl"):
                with open(sparse_connection_sae_path, 'rb') as f:
                    state_dict = pickle.load(f)
            else:
                raise ValueError(f"Unexpected file extension: {sparse_connection_sae_path}, supported extensions are .pt, .pkl, and .pkl.gz")
            self.spacon_sae_W_dec = state_dict['state_dict']['W_dec'].cuda() if not cfg.sparse_connection_use_W_enc else state_dict['state_dict']['W_enc'].cuda().T
            del state_dict
            torch.cuda.empty_cache()
        # NOTE: if using resampling neurons method, you must ensure that we initialise the weights in the order W_enc, b_enc, W_dec, b_dec
        self.W_enc = nn.Parameter(
            torch.nn.init.kaiming_uniform_(
                torch.empty(self.d_in, self.d_sae, dtype=self.dtype, device=self.device)
            )   
        )
        self.b_enc = nn.Parameter(
            torch.zeros(self.d_sae, dtype=self.dtype, device=self.device)
        )
        self.W_dec = nn.Parameter(
            torch.nn.init.kaiming_uniform_(
                torch.empty(self.d_sae, self.d_out, dtype=self.dtype, device=self.device)
            )
        )
        with torch.no_grad():
            # Anthropic normalize this to have unit columns
            self.W_dec.data /= torch.norm(self.W_dec.data, dim=1, keepdim=True)
        self.b_dec = nn.Parameter(
            torch.zeros(self.d_in, dtype=self.dtype, device=self.device)
        )
        self.b_dec_out = None
        if cfg.is_transcoder:
            self.b_dec_out = nn.Parameter(
                torch.zeros(self.d_out, dtype=self.dtype, device=self.device)
            )
        self.hook_sae_in = HookPoint()
        self.hook_hidden_pre = HookPoint()
        self.hook_hidden_post = HookPoint()
        self.hook_sae_out = HookPoint()
        self.setup()  # Required for `HookedRootModule`s
    def forward(self, x, dead_neuron_mask = None, mse_target=None):
        # move x to correct dtype
        x = x.to(self.dtype)
        sae_in = self.hook_sae_in(
            x - self.b_dec
        )  # Remove encoder bias as per Anthropic
        hidden_pre = self.hook_hidden_pre(
            einops.einsum(
                sae_in,
                self.W_enc,
                "... d_in, d_in d_sae -> ... d_sae",
            )
            + self.b_enc
        )
        feature_acts = self.hook_hidden_post(torch.nn.functional.relu(hidden_pre))
        if self.cfg.is_transcoder:
            # dumb if statement to deal with transcoders
            # hopefully branch prediction takes care of this
            sae_out = self.hook_sae_out(
                einops.einsum(
                    feature_acts,
                    self.W_dec,
                    "... d_sae, d_sae d_out -> ... d_out",
                )
                + self.b_dec_out
            )
        else:
            sae_out = self.hook_sae_out(
                einops.einsum(
                    feature_acts,
                    self.W_dec,
                    "... d_sae, d_sae d_out -> ... d_out",
                )
                + self.b_dec
            )
        # add config for whether l2 is normalized:
        if mse_target is None:
            mse_loss = (torch.pow((sae_out-x.float()), 2) / (x**2).sum(dim=-1, keepdim=True).sqrt())
        else:
            mse_loss = (torch.pow((sae_out-mse_target.float()), 2) / (mse_target**2).sum(dim=-1, keepdim=True).sqrt())
        mse_loss_ghost_resid = torch.tensor(0.0, dtype=self.dtype, device=self.device)
        # gate on config and training so evals is not slowed down.
        if self.cfg.use_ghost_grads and self.training and dead_neuron_mask.sum() > 0:
            assert dead_neuron_mask is not None 
            # ghost protocol
            # 1.
            residual = x - sae_out
            l2_norm_residual = torch.norm(residual, dim=-1)
            # 2.
            feature_acts_dead_neurons_only = torch.exp(hidden_pre[:, dead_neuron_mask])
            ghost_out =  feature_acts_dead_neurons_only @ self.W_dec[dead_neuron_mask,:]
            l2_norm_ghost_out = torch.norm(ghost_out, dim = -1)
            norm_scaling_factor = l2_norm_residual / (1e-6+ l2_norm_ghost_out* 2)
            ghost_out = ghost_out*norm_scaling_factor[:, None].detach()
            # 3. 
            mse_loss_ghost_resid = (
                torch.pow((ghost_out - residual.detach().float()), 2) / (residual.detach()**2).sum(dim=-1, keepdim=True).sqrt()
            )
            mse_rescaling_factor = (mse_loss / (mse_loss_ghost_resid + 1e-6)).detach()
            mse_loss_ghost_resid = mse_rescaling_factor * mse_loss_ghost_resid
        mse_loss_ghost_resid = mse_loss_ghost_resid.mean()
        mse_loss = mse_loss.mean()
        sparsity = torch.abs(feature_acts).sum(dim=1).mean(dim=(0,)) 
        l1_loss = self.l1_coefficient * sparsity
        loss = mse_loss + l1_loss + mse_loss_ghost_resid
        return sae_out, feature_acts, loss, mse_loss, l1_loss, mse_loss_ghost_resid
    def get_sparse_connection_loss(self):
        dots = self.spacon_sae_W_dec @ self.W_dec.T
        # each row is an sae feature, each column is a transcoder feature
        loss = torch.sum(dots.abs(), dim=1).mean() # mean over each sae feature of L1 of transcoder features activated
        return self.cfg.sparse_connection_l1_coeff * loss
    @torch.no_grad()
    def initialize_b_dec(self, activation_store):
        if self.cfg.b_dec_init_method == "geometric_median":
            self.initialize_b_dec_with_geometric_median(activation_store)
        elif self.cfg.b_dec_init_method == "mean":
            self.initialize_b_dec_with_mean(activation_store)
        elif self.cfg.b_dec_init_method == "zeros":
            pass
        else:
            raise ValueError(f"Unexpected b_dec_init_method: {self.cfg.b_dec_init_method}")
    @torch.no_grad()
    def initialize_b_dec_with_geometric_median(self, activation_store):
        assert(self.cfg.is_transcoder == activation_store.cfg.is_transcoder)
        previous_b_dec = self.b_dec.clone().cpu()
        all_activations = activation_store.storage_buffer.detach().cpu()
        out = compute_geometric_median(
            all_activations,
            skip_typechecks=True, 
            maxiter=100, per_component=False
        ).median
        previous_distances = torch.norm(all_activations - previous_b_dec, dim=-1)
        distances = torch.norm(all_activations - out, dim=-1)
        print("Reinitializing b_dec with geometric median of activations")
        print(f"Previous distances: {previous_distances.median(0).values.mean().item()}")
        print(f"New distances: {distances.median(0).values.mean().item()}")
        out = torch.tensor(out, dtype=self.dtype, device=self.device)
        self.b_dec.data = out
        if self.b_dec_out is not None:
            # stupid code duplication
            previous_b_dec_out = self.b_dec_out.clone().cpu()
            all_activations_out = activation_store.storage_buffer_out.detach().cpu()
            out_out = compute_geometric_median(
                all_activations_out,
                skip_typechecks=True, 
                maxiter=100, per_component=False
            ).median
            previous_distances_out = torch.norm(all_activations_out - previous_b_dec_out, dim=-1)
            distances_out = torch.norm(all_activations_out - out_out, dim=-1)
            print("Reinitializing b_dec with geometric median of activations")
            print(f"Previous distances: {previous_distances_out.median(0).values.mean().item()}")
            print(f"New distances: {distances_out.median(0).values.mean().item()}")
            out_out = torch.tensor(out_out, dtype=self.dtype, device=self.device)
            self.b_dec_out.data = out_out
    @torch.no_grad()
    def initialize_b_dec_with_mean(self, activation_store):
        assert(self.cfg.is_transcoder == activation_store.cfg.is_transcoder)
        previous_b_dec = self.b_dec.clone().cpu()
        all_activations = activation_store.storage_buffer.detach().cpu()
        out = all_activations.mean(dim=0)
        previous_distances = torch.norm(all_activations - previous_b_dec, dim=-1)
        distances = torch.norm(all_activations - out, dim=-1)
        print("Reinitializing b_dec with mean of activations")
        print(f"Previous distances: {previous_distances.median(0).values.mean().item()}")
        print(f"New distances: {distances.median(0).values.mean().item()}")
        self.b_dec.data = out.to(self.dtype).to(self.device)
        if self.b_dec_out is not None:
            # stupid code duplication        
            previous_b_dec_out = self.b_dec_out.clone().cpu()
            all_activations_out = activation_store.storage_buffer_out.detach().cpu()
            out_out = all_activations_out.mean(dim=0)
            previous_distances_out = torch.norm(all_activations_out - previous_b_dec_out, dim=-1)
            distances_out = torch.norm(all_activations_out - out_out, dim=-1)
            print("Reinitializing b_dec with mean of activations")
            print(f"Previous distances: {previous_distances_out.median(0).values.mean().item()}")
            print(f"New distances: {distances_out.median(0).values.mean().item()}")
            self.b_dec_out.data = out_out.to(self.dtype).to(self.device)
    @torch.no_grad()
    def resample_neurons_l2(
        self,
        x: Float[Tensor, "batch_size n_hidden"],
        feature_sparsity: Float[Tensor, "n_hidden_ae"],
        optimizer: torch.optim.Optimizer,
    ) -> None:
        '''
        Resamples neurons that have been dead for `dead_neuron_window` steps, according to `frac_active`.
        I'll probably break this now and fix it later!
        '''
        feature_reinit_scale = self.cfg.feature_reinit_scale
        sae_out, _, _, _, _ = self.forward(x)
        per_token_l2_loss = (sae_out - x).pow(2).sum(dim=-1).squeeze()
        # Find the dead neurons in this instance. If all neurons are alive, continue
        is_dead = (feature_sparsity < self.cfg.dead_feature_threshold)
        dead_neurons = torch.nonzero(is_dead).squeeze(-1)
        alive_neurons = torch.nonzero(~is_dead).squeeze(-1)
        n_dead = dead_neurons.numel()
        if n_dead == 0:
            return 0 # If there are no dead neurons, we don't need to resample neurons
        # Compute L2 loss for each element in the batch
        # TODO: Check whether we need to go through more batches as features get sparse to find high l2 loss examples. 
        if per_token_l2_loss.max() < 1e-6:
            return 0 # If we have zero reconstruction loss, we don't need to resample neurons
        # Draw `n_hidden_ae` samples from [0, 1, ..., batch_size-1], with probabilities proportional to l2_loss squared
        per_token_l2_loss = per_token_l2_loss.to(torch.float32) # wont' work with bfloat16
        distn = Categorical(probs = per_token_l2_loss.pow(2) / (per_token_l2_loss.pow(2).sum()))
        replacement_indices = distn.sample((n_dead,)) # shape [n_dead]
        # Index into the batch of hidden activations to get our replacement values
        replacement_values = (x - self.b_dec)[replacement_indices] # shape [n_dead n_input_ae]
        # unit norm
        replacement_values = (replacement_values / (replacement_values.norm(dim=1, keepdim=True) + 1e-8))
        # St new decoder weights
        self.W_dec.data[is_dead, :] = replacement_values
        # Get the norm of alive neurons (or 1.0 if there are no alive neurons)
        W_enc_norm_alive_mean = 1.0 if len(alive_neurons) == 0 else self.W_enc[:, alive_neurons].norm(dim=0).mean().item()
        # Lastly, set the new weights & biases
        self.W_enc.data[:, is_dead] = (replacement_values * W_enc_norm_alive_mean * feature_reinit_scale).T
        self.b_enc.data[is_dead] = 0.0
        # reset the Adam Optimiser for every modified weight and bias term
        # Reset all the Adam parameters
        for dict_idx, (k, v) in enumerate(optimizer.state.items()):
            for v_key in ["exp_avg", "exp_avg_sq"]:
                if dict_idx == 0:
                    assert k.data.shape == (self.d_in, self.d_sae)
                    v[v_key][:, is_dead] = 0.0
                elif dict_idx == 1:
                    assert k.data.shape == (self.d_sae,)
                    v[v_key][is_dead] = 0.0
                elif dict_idx == 2:
                    assert k.data.shape == (self.d_sae, self.d_out)
                    v[v_key][is_dead, :] = 0.0
                elif dict_idx == 3:
                    assert k.data.shape == (self.d_out,)
                else:
                    if not self.cfg.is_transcoder:
                        raise ValueError(f"Unexpected dict_idx {dict_idx}")
                        # if we're a transcoder, then this is fine, because we also have b_dec_out
        # Check that the opt is really updated
        for dict_idx, (k, v) in enumerate(optimizer.state.items()):
            for v_key in ["exp_avg", "exp_avg_sq"]:
                if dict_idx == 0:
                    if k.data.shape != (self.d_in, self.d_sae):
                        print(
                            "Warning: it does not seem as if resetting the Adam parameters worked, there are shapes mismatches"
                        )
                    if v[v_key][:, is_dead].abs().max().item() > 1e-6:
                        print(
                            "Warning: it does not seem as if resetting the Adam parameters worked"
                        )
        return n_dead
    @torch.no_grad()
    def resample_neurons_anthropic(
        self, 
        dead_neuron_indices, 
        model,
        optimizer, 
        activation_store):
        """
        Arthur's version of Anthropic's feature resampling
        procedure.
        """
        # collect global loss increases, and input activations
        global_loss_increases, global_input_activations = self.collect_anthropic_resampling_losses(
            model, activation_store
        )
        # sample according to losses
        probs = global_loss_increases / global_loss_increases.sum()
        sample_indices = torch.multinomial(
            probs,
            min(len(dead_neuron_indices), probs.shape[0]),
            replacement=False,
        )
        # if we don't have enough samples for for all the dead neurons, take the first n
        if sample_indices.shape[0] < len(dead_neuron_indices):
            dead_neuron_indices = dead_neuron_indices[:sample_indices.shape[0]]
        # Replace W_dec with normalized differences in activations
        self.W_dec.data[dead_neuron_indices, :] = (
            (
                global_input_activations[sample_indices]
                / torch.norm(global_input_activations[sample_indices], dim=1, keepdim=True)
            )
            .to(self.dtype)
            .to(self.device)
        )
        # Lastly, set the new weights & biases
        self.W_enc.data[:, dead_neuron_indices] = self.W_dec.data[dead_neuron_indices, :].T
        self.b_enc.data[dead_neuron_indices] = 0.0
        # Reset the Encoder Weights
        if dead_neuron_indices.shape[0] < self.d_sae:
            sum_of_all_norms = torch.norm(self.W_enc.data, dim=0).sum()
            sum_of_all_norms -= len(dead_neuron_indices)
            average_norm = sum_of_all_norms / (self.d_sae - len(dead_neuron_indices))
            self.W_enc.data[:, dead_neuron_indices] *= self.cfg.feature_reinit_scale * average_norm
            # Set biases to resampled value
            relevant_biases = self.b_enc.data[dead_neuron_indices].mean()
            self.b_enc.data[dead_neuron_indices] = relevant_biases * 0 # bias resample factor (put in config?)
        else:
            self.W_enc.data[:, dead_neuron_indices] *= self.cfg.feature_reinit_scale
            self.b_enc.data[dead_neuron_indices] = -5.0
        # TODO: Refactor this resetting to be outside of resampling.
        # reset the Adam Optimiser for every modified weight and bias term
        # Reset all the Adam parameters
        for dict_idx, (k, v) in enumerate(optimizer.state.items()):
            for v_key in ["exp_avg", "exp_avg_sq"]:
                if dict_idx == 0:
                    assert k.data.shape == (self.d_in, self.d_sae)
                    v[v_key][:, dead_neuron_indices] = 0.0
                elif dict_idx == 1:
                    assert k.data.shape == (self.d_sae,)
                    v[v_key][dead_neuron_indices] = 0.0
                elif dict_idx == 2:
                    assert k.data.shape == (self.d_sae, self.d_out)
                    v[v_key][dead_neuron_indices, :] = 0.0
                elif dict_idx == 3:
                    assert k.data.shape == (self.d_out,)
                else:
                    if not self.cfg.is_transcoder:
                        raise ValueError(f"Unexpected dict_idx {dict_idx}")
                        # if we're a transcoder, then this is fine, because we also have b_dec_out
        # Check that the opt is really updated
        for dict_idx, (k, v) in enumerate(optimizer.state.items()):
            for v_key in ["exp_avg", "exp_avg_sq"]:
                if dict_idx == 0:
                    if k.data.shape != (self.d_in, self.d_sae):
                        print(
                            "Warning: it does not seem as if resetting the Adam parameters worked, there are shapes mismatches"
                        )
                    if v[v_key][:, dead_neuron_indices].abs().max().item() > 1e-6:
                        print(
                            "Warning: it does not seem as if resetting the Adam parameters worked"
                        )
        return 
    @torch.no_grad()
    def collect_anthropic_resampling_losses(self, model, activation_store):
        """
        Collects the losses for resampling neurons (anthropic)
        """
        batch_size = self.cfg.store_batch_size
        # we're going to collect this many forward passes
        number_final_activations = self.cfg.resample_batches * batch_size
        # but have seq len number of tokens in each
        number_activations_total = number_final_activations * self.cfg.context_size
        anthropic_iterator = range(0, number_final_activations, batch_size)
        anthropic_iterator = tqdm(anthropic_iterator, desc="Collecting losses for resampling...")
        global_loss_increases = torch.zeros((number_final_activations,), dtype=self.dtype, device=self.device)
        global_input_activations = torch.zeros((number_final_activations, self.d_in), dtype=self.dtype, device=self.device)
        for refill_idx in anthropic_iterator:
            # get a batch, calculate loss with/without using SAE reconstruction.
            batch_tokens = activation_store.get_batch_tokens()
            ce_loss_with_recons = self.get_test_loss(batch_tokens, model)
            ce_loss_without_recons, normal_activations_cache = model.run_with_cache(
                batch_tokens,
                names_filter=self.cfg.hook_point,
                return_type = "loss",
                loss_per_token = True,
            )
            # ce_loss_without_recons = model.loss_fn(normal_logits, batch_tokens, True)
            # del normal_logits
            normal_activations = normal_activations_cache[self.cfg.hook_point]
            if self.cfg.hook_point_head_index is not None:
                normal_activations = normal_activations[:,:,self.cfg.hook_point_head_index]
            # calculate the difference in loss
            changes_in_loss = ce_loss_with_recons - ce_loss_without_recons
            changes_in_loss = changes_in_loss.cpu()
            # sample from the loss differences
            probs = F.relu(changes_in_loss) / F.relu(changes_in_loss).sum(dim=1, keepdim=True)
            changes_in_loss_dist = Categorical(probs)
            samples = changes_in_loss_dist.sample()
            assert samples.shape == (batch_size,), f"{samples.shape=}; {self.cfg.store_batch_size=}"
            end_idx = refill_idx + batch_size
            global_loss_increases[refill_idx:end_idx] = changes_in_loss[torch.arange(batch_size), samples]
            global_input_activations[refill_idx:end_idx] = normal_activations[torch.arange(batch_size), samples]
        return global_loss_increases, global_input_activations
    @torch.no_grad()
    def get_test_loss(self, batch_tokens, model):
        """
        A method for running the model with the SAE activations in order to return the loss.
        returns per token loss when activations are substituted in.
        """
        if not self.cfg.is_transcoder:
            head_index = self.cfg.hook_point_head_index
            def standard_replacement_hook(activations, hook):
                activations = self.forward(activations)[0].to(activations.dtype)
                return activations
            def head_replacement_hook(activations, hook):
                new_actions = self.forward(activations[:,:,head_index])[0].to(activations.dtype)
                activations[:,:,head_index] = new_actions
                return activations
            replacement_hook = standard_replacement_hook if head_index is None else head_replacement_hook
            ce_loss_with_recons = model.run_with_hooks(
                batch_tokens,
                return_type="loss",
                fwd_hooks=[(self.cfg.hook_point, replacement_hook)],
            )
        else:
            # TODO: currently, this only works with MLP transcoders
            assert("mlp" in self.cfg.out_hook_point)
            old_mlp = model.blocks[self.cfg.hook_point_layer]
            class TranscoderWrapper(torch.nn.Module):
                def __init__(self, transcoder):
                    super().__init__()
                    self.transcoder = transcoder
                def forward(self, x):
                    return self.transcoder(x)[0]
            model.blocks[self.cfg.hook_point_layer].mlp = TranscoderWrapper(self)
            ce_loss_with_recons = model.run_with_hooks(
                batch_tokens,
                return_type="loss"
            )
            model.blocks[self.cfg.hook_point_layer] = old_mlp
        return ce_loss_with_recons
    @torch.no_grad()
    def set_decoder_norm_to_unit_norm(self):
        self.W_dec.data /= torch.norm(self.W_dec.data, dim=1, keepdim=True)
    @torch.no_grad()
    def remove_gradient_parallel_to_decoder_directions(self):
        '''
        Update grads so that they remove the parallel component
            (d_sae, d_in) shape
        '''
        parallel_component = einops.einsum(
            self.W_dec.grad,
            self.W_dec.data,
            "d_sae d_out, d_sae d_out -> d_sae",
        )
        self.W_dec.grad -= einops.einsum(
            parallel_component,
            self.W_dec.data,
            "d_sae, d_sae d_out -> d_sae d_out",
        )
    def save_model(self, path: str):
        '''
        Basic save function for the model. Saves the model's state_dict and the config used to train it.
        '''
        # check if path exists
        folder = os.path.dirname(path)
        os.makedirs(folder, exist_ok=True)
        state_dict = {
            "cfg": self.cfg,
            "state_dict": self.state_dict()
        }
        if path.endswith(".pt"):
            torch.save(state_dict, path)
        elif path.endswith("pkl.gz"):
            with gzip.open(path, "wb") as f:
                pickle.dump(state_dict, f)
        else:
            raise ValueError(f"Unexpected file extension: {path}, supported extensions are .pt and .pkl.gz")
        print(f"Saved model to {path}")
    @classmethod
    def load_from_pretrained(cls, path: str):
        '''
        Load function for the model. Loads the model's state_dict and the config used to train it.
        This method can be called directly on the class, without needing an instance.
        '''
        # Ensure the file exists
        if not os.path.isfile(path):
            raise FileNotFoundError(f"No file found at specified path: {path}")
        # Load the state dictionary
        if path.endswith(".pt"):
            try:
                if torch.backends.mps.is_available():
                    state_dict = torch.load(path, map_location="mps")
                    state_dict["cfg"].device = "mps"
                else:
                    state_dict = torch.load(path)
            except Exception as e:
                raise IOError(f"Error loading the state dictionary from .pt file: {e}")
        elif path.endswith(".pkl.gz"):
            try:
                with gzip.open(path, 'rb') as f:
                    state_dict = pickle.load(f)
            except Exception as e:
                raise IOError(f"Error loading the state dictionary from .pkl.gz file: {e}")
        elif path.endswith(".pkl"):
            try:
                with open(path, 'rb') as f:
                    state_dict = pickle.load(f)
            except Exception as e:
                raise IOError(f"Error loading the state dictionary from .pkl file: {e}")
        else:
            raise ValueError(f"Unexpected file extension: {path}, supported extensions are .pt, .pkl, and .pkl.gz")
        # Ensure the loaded state contains both 'cfg' and 'state_dict'
        if 'cfg' not in state_dict or 'state_dict' not in state_dict:
            raise ValueError("The loaded state dictionary must contain 'cfg' and 'state_dict' keys")
        # Create an instance of the class using the loaded configuration
        instance = cls(cfg=state_dict["cfg"])
        instance.load_state_dict(state_dict["state_dict"])
        return instance
    def get_name(self):
        sae_name = f"sparse_autoencoder_{self.cfg.model_name}_{self.cfg.hook_point}_{self.cfg.d_sae}"
        return sae_name

================
File: sae_training/train_sae_on_language_model.py
================
from functools import partial
import numpy as np
import torch
from torch.optim import Adam
from tqdm import tqdm
from transformer_lens import HookedTransformer
from transformer_lens.utils import get_act_name
import wandb
from sae_training.activations_store import ActivationsStore
from sae_training.optim import get_scheduler
from sae_training.sparse_autoencoder import SparseAutoencoder
def train_sae_on_language_model(
    model: HookedTransformer,
    sparse_autoencoder: SparseAutoencoder,
    activation_store: ActivationsStore,
    batch_size: int = 1024,
    n_checkpoints: int = 0,
    feature_sampling_method: str = "l2",  # None, l2, or anthropic
    feature_sampling_window: int = 1000,  # how many training steps between resampling the features / considiring neurons dead
    feature_reinit_scale: float = 0.2,  # how much to scale the resampled features by
    dead_feature_threshold: float = 1e-8,  # how infrequently a feature has to be active to be considered dead
    dead_feature_window: int = 2000,  # how many training steps before a feature is considered dead
    use_wandb: bool = False,
    wandb_log_frequency: int = 50,
):
    if feature_sampling_method is not None:
        feature_sampling_method = feature_sampling_method.lower()
    total_training_tokens = sparse_autoencoder.cfg.total_training_tokens
    total_training_steps = total_training_tokens // batch_size
    n_training_steps = 0
    n_training_tokens = 0
    n_resampled_neurons = 0
    steps_before_reset = 0
    if n_checkpoints > 0:
        checkpoint_thresholds = list(range(0, total_training_tokens, total_training_tokens // (n_checkpoints+1)))[1:]
    # track active features
    act_freq_scores = torch.zeros(sparse_autoencoder.cfg.d_sae, device=sparse_autoencoder.cfg.device)
    n_forward_passes_since_fired = torch.zeros(sparse_autoencoder.cfg.d_sae, device=sparse_autoencoder.cfg.device)
    n_frac_active_tokens = 0
    optimizer = Adam(sparse_autoencoder.parameters(),
                     lr = sparse_autoencoder.cfg.lr)
    scheduler = get_scheduler(
        sparse_autoencoder.cfg.lr_scheduler_name,
        optimizer=optimizer,
        warm_up_steps = sparse_autoencoder.cfg.lr_warm_up_steps, 
        training_steps=total_training_steps,
        lr_end=sparse_autoencoder.cfg.lr / 10, # heuristic for now. 
    )
    sparse_autoencoder.initialize_b_dec(activation_store)
    sparse_autoencoder.train()
    if sparse_autoencoder.cfg.use_tqdm:
        pbar = tqdm(total=total_training_tokens, desc="Training SAE")
    while n_training_tokens < total_training_tokens:
        # Do a training step.
        sparse_autoencoder.train()
        # Make sure the W_dec is still zero-norm
        sparse_autoencoder.set_decoder_norm_to_unit_norm()
        if (feature_sampling_method=="anthropic") and ((n_training_steps + 1) % dead_feature_window == 0):
            feature_sparsity = act_freq_scores / n_frac_active_tokens
            # if reset criterion is frequency in window, then then use that to generate indices.
            if sparse_autoencoder.cfg.dead_feature_estimation_method == "no_fire":
                dead_neuron_indices = (act_freq_scores == 0).nonzero(as_tuple=False)[:, 0]
            elif sparse_autoencoder.cfg.dead_feature_estimation_method == "frequency":
                dead_neuron_indices = (feature_sparsity < sparse_autoencoder.cfg.dead_feature_threshold).nonzero(as_tuple=False)[:, 0]
            if len(dead_neuron_indices) > 0:
                if len(dead_neuron_indices) > sparse_autoencoder.cfg.resample_batches * sparse_autoencoder.cfg.store_batch_size:
                    print("Warning: more dead neurons than number of tokens. Consider sampling more tokens when resampling.")
                sparse_autoencoder.resample_neurons_anthropic(
                    dead_neuron_indices, 
                    model,
                    optimizer, 
                    activation_store
                )
                if use_wandb:
                    n_resampled_neurons = min(len(dead_neuron_indices), sparse_autoencoder.cfg.store_batch_size * sparse_autoencoder.cfg.resample_batches)
                    wandb.log(
                        {
                            "metrics/n_resampled_neurons": n_resampled_neurons,
                        },
                        step=n_training_steps,
                    )
                # for now, we'll hardcode this.
                current_lr = scheduler.get_last_lr()[0]
                reduced_lr = current_lr / 10_000
                increment = (current_lr - reduced_lr) / 10_000
                optimizer.param_groups[0]['lr'] = reduced_lr
                steps_before_reset = 10_000
            else:
                print("No dead neurons, skipping resampling")
        # Resample dead neurons
        if (feature_sampling_method == "l2") and ((n_training_steps + 1) % dead_feature_window == 0):
            print("no l2 resampling currently. Please use anthropic resampling")
        # after resampling, reset the sparsity:
        if (n_training_steps + 1) % feature_sampling_window == 0:
            feature_sparsity = act_freq_scores / n_frac_active_tokens
            log_feature_sparsity = torch.log10(feature_sparsity + 1e-10).detach().cpu()
            if use_wandb:
                wandb_histogram = wandb.Histogram(log_feature_sparsity.numpy())
                wandb.log(
                    {   
                        "metrics/mean_log10_feature_sparsity": log_feature_sparsity.mean().item(),
                        "plots/feature_density_line_chart": wandb_histogram,
                    },
                    step=n_training_steps,
                )
            act_freq_scores = torch.zeros(sparse_autoencoder.cfg.d_sae, device=sparse_autoencoder.cfg.device)
            n_frac_active_tokens = 0
        if (steps_before_reset > 0) and n_training_steps > 0:
            steps_before_reset -= 1
            optimizer.param_groups[0]['lr'] += increment
            if steps_before_reset == 0:
                optimizer.param_groups[0]['lr'] = current_lr
        else:
            scheduler.step()
        optimizer.zero_grad()
        ghost_grad_neuron_mask = (n_forward_passes_since_fired > sparse_autoencoder.cfg.dead_feature_window).bool()
        next_batch = activation_store.next_batch()
        assert(sparse_autoencoder.cfg.is_transcoder == activation_store.cfg.is_transcoder)
        if not sparse_autoencoder.cfg.is_transcoder:
            sae_in = next_batch
            # Forward and Backward Passes
            sae_out, feature_acts, loss, mse_loss, l1_loss, ghost_grad_loss = sparse_autoencoder(
                sae_in,
                ghost_grad_neuron_mask,
                mse_target=sae_in
            )
        else:
            sae_in = next_batch[:, :sparse_autoencoder.cfg.d_in]
            mlp_out = next_batch[:, sparse_autoencoder.cfg.d_in:]
            sae_out, feature_acts, loss, mse_loss, l1_loss, ghost_grad_loss = sparse_autoencoder(
                sae_in,
                ghost_grad_neuron_mask,
                mse_target=mlp_out
            )
        spacon_loss = 0
        if sparse_autoencoder.cfg.is_sparse_connection:
            spacon_loss = sparse_autoencoder.get_sparse_connection_loss()
            loss = loss + spacon_loss
        did_fire = ((feature_acts > 0).float().sum(-2) > 0)
        n_forward_passes_since_fired += 1
        n_forward_passes_since_fired[did_fire] = 0
        n_training_tokens += batch_size
        with torch.no_grad():
            # Calculate the sparsities, and add it to a list, calculate sparsity metrics
            act_freq_scores += (feature_acts.abs() > 0).float().sum(0)
            n_frac_active_tokens += batch_size
            feature_sparsity = act_freq_scores / n_frac_active_tokens
            if use_wandb and ((n_training_steps + 1) % wandb_log_frequency == 0):
                # metrics for currents acts
                l0 = (feature_acts > 0).float().sum(-1).mean()
                current_learning_rate = optimizer.param_groups[0]["lr"]
                per_token_l2_loss = (sae_out - sae_in).pow(2).sum(dim=-1).squeeze()
                total_variance = sae_in.pow(2).sum(-1)
                explained_variance = 1 - per_token_l2_loss/total_variance
                wandb.log(
                    {
                        # losses
                        "losses/mse_loss": mse_loss.item(),
                        "losses/l1_loss": l1_loss.item() / sparse_autoencoder.l1_coefficient, # normalize by l1 coefficient
                        "losses/ghost_grad_loss": ghost_grad_loss.item(),
                        "losses/overall_loss": loss.item(),
                        # variance explained
                        "metrics/explained_variance": explained_variance.mean().item(),
                        "metrics/explained_variance_std": explained_variance.std().item(),
                        "metrics/l0": l0.item(),
                        # sparsity
                        "sparsity/mean_passes_since_fired": n_forward_passes_since_fired.mean().item(),
                        "sparsity/n_passes_since_fired_over_threshold": ghost_grad_neuron_mask.sum().item(),
                        "sparsity/below_1e-5": (feature_sparsity < 1e-5)
                        .float()
                        .mean()
                        .item(),
                        "sparsity/below_1e-6": (feature_sparsity < 1e-6)
                        .float()
                        .mean()
                        .item(),
                        "sparsity/dead_features": (
                            feature_sparsity < dead_feature_threshold
                        )
                        .float()
                        .mean()
                        .item(),
                        "details/n_training_tokens": n_training_tokens,
                        "details/current_learning_rate": current_learning_rate,
                    },
                    step=n_training_steps,
                )
            # record loss frequently, but not all the time.
            if use_wandb and ((n_training_steps + 1) % (wandb_log_frequency * 10) == 0):
                sparse_autoencoder.eval()
                run_evals(sparse_autoencoder, activation_store, model, n_training_steps)
                sparse_autoencoder.train()
            if sparse_autoencoder.cfg.use_tqdm:
                if sparse_autoencoder.cfg.is_sparse_connection:
                    pbar.set_description(
                        f"{n_training_steps}| MSE Loss {mse_loss.item():.3f} | L1 {l1_loss.item():.3f} | SCST {spacon_loss.item():.3f}"
                    )
                else:
                    pbar.set_description(
                        f"{n_training_steps}| MSE Loss {mse_loss.item():.3f} | L1 {l1_loss.item():.3f}"
                    )
                pbar.update(batch_size)
        loss.backward()
        sparse_autoencoder.remove_gradient_parallel_to_decoder_directions()
        optimizer.step()
        # checkpoint if at checkpoint frequency
        if n_checkpoints > 0 and n_training_tokens > checkpoint_thresholds[0]:
            cfg = sparse_autoencoder.cfg
            path = f"{sparse_autoencoder.cfg.checkpoint_path}/{n_training_tokens}_{sparse_autoencoder.get_name()}.pt"
            log_feature_sparsity_path = f"{sparse_autoencoder.cfg.checkpoint_path}/{n_training_tokens}_{sparse_autoencoder.get_name()}_log_feature_sparsity.pt"
            sparse_autoencoder.save_model(path)
            try: log_feature_sparsity
            except NameError:
                feature_sparsity = act_freq_scores / n_frac_active_tokens
                log_feature_sparsity = torch.log10(feature_sparsity + 1e-10).detach().cpu()
            torch.save(log_feature_sparsity, log_feature_sparsity_path)
            checkpoint_thresholds.pop(0)
            if len(checkpoint_thresholds) == 0:
                n_checkpoints = 0
            if cfg.log_to_wandb:
                model_artifact = wandb.Artifact(
                    f"{sparse_autoencoder.get_name()}", type="model", metadata=dict(cfg.__dict__)
                )
                model_artifact.add_file(path)
                wandb.log_artifact(model_artifact)
                sparsity_artifact = wandb.Artifact(
                    f"{sparse_autoencoder.get_name()}_log_feature_sparsity", type="log_feature_sparsity", metadata=dict(cfg.__dict__)
                )
                sparsity_artifact.add_file(log_feature_sparsity_path)
                wandb.log_artifact(sparsity_artifact)
        n_training_steps += 1
    path = f"{sparse_autoencoder.cfg.checkpoint_path}/final_{sparse_autoencoder.get_name()}.pt"
    log_feature_sparsity_path = f"{sparse_autoencoder.cfg.checkpoint_path}/final_{sparse_autoencoder.get_name()}_log_feature_sparsity.pt"
    sparse_autoencoder.save_model(path)
    torch.save(log_feature_sparsity, log_feature_sparsity_path)
    if cfg.log_to_wandb:
        sparsity_artifact = wandb.Artifact(
                f"{sparse_autoencoder.get_name()}_log_feature_sparsity", type="log_feature_sparsity", metadata=dict(cfg.__dict__)
            )
        sparsity_artifact.add_file(log_feature_sparsity_path)
        wandb.log_artifact(sparsity_artifact)
    return sparse_autoencoder
@torch.no_grad()
def run_evals(sparse_autoencoder: SparseAutoencoder, activation_store: ActivationsStore, model: HookedTransformer, n_training_steps: int):
    hook_point = sparse_autoencoder.cfg.hook_point
    hook_point_layer = sparse_autoencoder.cfg.hook_point_layer
    hook_point_head_index = sparse_autoencoder.cfg.hook_point_head_index
     ### Evals
    eval_tokens = activation_store.get_batch_tokens()
    # Get Reconstruction Score
    recons_score, ntp_loss, recons_loss, zero_abl_loss = get_recons_loss(sparse_autoencoder, model, activation_store, eval_tokens)
    # get cache
    _, cache = model.run_with_cache(eval_tokens, prepend_bos=False, names_filter=[get_act_name("pattern", hook_point_layer), hook_point])
    # get act
    if sparse_autoencoder.cfg.hook_point_head_index is not None:
        original_act = cache[sparse_autoencoder.cfg.hook_point][:,:,sparse_autoencoder.cfg.hook_point_head_index]
    else:
        original_act = cache[sparse_autoencoder.cfg.hook_point]
    sae_out, feature_acts, _, _, _, _ = sparse_autoencoder(
        original_act
    )
    patterns_original = cache[get_act_name("pattern", hook_point_layer)][:,hook_point_head_index].detach().cpu()
    del cache
    if "cuda" in str(model.cfg.device):
        torch.cuda.empty_cache()
    l2_norm_in = torch.norm(original_act, dim=-1)
    l2_norm_out = torch.norm(sae_out, dim=-1)
    l2_norm_ratio = l2_norm_out / l2_norm_in
    wandb.log(
        {
            # l2 norms
            "metrics/l2_norm": l2_norm_out.mean().item(),
            "metrics/l2_ratio": l2_norm_ratio.mean().item(),
            # CE Loss
            "metrics/CE_loss_score": recons_score,
            "metrics/ce_loss_without_sae": ntp_loss,
            "metrics/ce_loss_with_sae": recons_loss,
            "metrics/ce_loss_with_ablation": zero_abl_loss,
        },
        step=n_training_steps,
    )
    head_index = sparse_autoencoder.cfg.hook_point_head_index
    def standard_replacement_hook(activations, hook):
        activations = sparse_autoencoder.forward(activations)[0].to(activations.dtype)
        return activations
    def head_replacement_hook(activations, hook):
        new_actions = sparse_autoencoder.forward(activations[:,:,head_index])[0].to(activations.dtype)
        activations[:,:,head_index] = new_actions
        return activations
    head_index = sparse_autoencoder.cfg.hook_point_head_index
    replacement_hook = standard_replacement_hook if head_index is None else head_replacement_hook
    # get attn when using reconstructed activations
    with model.hooks(fwd_hooks=[(hook_point, partial(replacement_hook))]):
        _, new_cache = model.run_with_cache(eval_tokens, names_filter=[get_act_name("pattern", hook_point_layer)])
        patterns_reconstructed = new_cache[get_act_name("pattern", hook_point_layer)][:,hook_point_head_index].detach().cpu()
        del new_cache
    # get attn when using reconstructed activations
    with model.hooks(fwd_hooks=[(hook_point, partial(zero_ablate_hook))]):
        _, zero_ablation_cache = model.run_with_cache(eval_tokens, names_filter=[get_act_name("pattern", hook_point_layer)])
        patterns_ablation = zero_ablation_cache[get_act_name("pattern", hook_point_layer)][:,hook_point_head_index].detach().cpu()
        del zero_ablation_cache
    # if dealing with a head SAE, do the head metrics.
    if sparse_autoencoder.cfg.hook_point_head_index:
        # show patterns before/after
        # fig_patterns_original = px.imshow(patterns_original[0].numpy(), title="original attn scores",
        #     color_continuous_midpoint=0, color_continuous_scale="RdBu")
        # fig_patterns_original.update_layout(coloraxis_showscale=False)         # hide colorbar 
        # wandb.log({"attention/patterns_original": wandb.Plotly(fig_patterns_original)}, step = n_training_steps)
        # fig_patterns_reconstructed = px.imshow(patterns_reconstructed[0].numpy(), title="reconstructed attn scores",
        #         color_continuous_midpoint=0, color_continuous_scale="RdBu")
        # fig_patterns_reconstructed.update_layout(coloraxis_showscale=False)         # hide colorbar
        # wandb.log({"attention/patterns_reconstructed": wandb.Plotly(fig_patterns_reconstructed)}, step = n_training_steps)
        kl_result_reconstructed = kl_divergence_attention(patterns_original, patterns_reconstructed)
        kl_result_reconstructed = kl_result_reconstructed.sum(dim=-1).numpy()
        # print(kl_result.mean().item())
        # px.imshow(kl_result, title="KL Divergence", width=800, height=800,
        #       color_continuous_midpoint=0, color_continuous_scale="RdBu").show()
        # px.histogram(kl_result.flatten()).show()
        # px.line(kl_result.mean(0), title="KL Divergence by Position").show()
        kl_result_ablation = kl_divergence_attention(patterns_original, patterns_ablation)
        kl_result_ablation = kl_result_ablation.sum(dim=-1).numpy()
        # print(kl_result.mean().item())
        # # px.imshow(kl_result, title="KL Divergence", width=800, height=800,
        # #       color_continuous_midpoint=0, color_continuous_scale="RdBu").show()
        # px.histogram(kl_result.flatten()).show()
        # px.line(kl_result.mean(0), title="KL Divergence by Position").show()
        wandb.log(
            {
              "metrics/kldiv_reconstructed": kl_result_reconstructed.mean().item(),
              "metrics/kldiv_ablation": kl_result_ablation.mean().item(),
            },
            step=n_training_steps,
        )
@torch.no_grad()
def get_recons_loss(sparse_autoencoder, model, activation_store, batch_tokens):
    hook_point = activation_store.cfg.hook_point
    loss = model(batch_tokens, return_type="loss")
    head_index = sparse_autoencoder.cfg.hook_point_head_index
    def standard_replacement_hook(activations, hook):
        activations = sparse_autoencoder.forward(activations)[0].to(activations.dtype)
        return activations
    def head_replacement_hook(activations, hook):
        new_actions = sparse_autoencoder.forward(activations[:,:,head_index])[0].to(activations.dtype)
        activations[:,:,head_index] = new_actions
        return activations
    replacement_hook = standard_replacement_hook if head_index is None else head_replacement_hook
    recons_loss = model.run_with_hooks(
        batch_tokens,
        return_type="loss",
        fwd_hooks=[(hook_point, partial(replacement_hook))],
    )
    zero_abl_loss = model.run_with_hooks(
        batch_tokens, return_type="loss", fwd_hooks=[(hook_point, zero_ablate_hook)]
    )
    score = (zero_abl_loss - recons_loss) / (zero_abl_loss - loss)
    return score, loss, recons_loss, zero_abl_loss
def mean_ablate_hook(mlp_post, hook):
    mlp_post[:] = mlp_post.mean([0, 1]).to(mlp_post.dtype)
    return mlp_post
def zero_ablate_hook(mlp_post, hook):
    mlp_post[:] = 0.0
    return mlp_post
def kl_divergence_attention(y_true, y_pred):
    # Compute log probabilities for KL divergence
    log_y_true = torch.log2(y_true + 1e-10)
    log_y_pred = torch.log2(y_pred + 1e-10)
    return y_true * (log_y_true - log_y_pred)

================
File: sae_training/utils.py
================
from typing import Tuple
import torch
from transformer_lens import HookedTransformer
from sae_training.activations_store import ActivationsStore
from sae_training.config import LanguageModelSAERunnerConfig
from sae_training.sparse_autoencoder import SparseAutoencoder
class LMSparseAutoencoderSessionloader():
    """
    Responsible for loading all required
    artifacts and files for training
    a sparse autoencoder on a language model
    or analysing a pretraining autoencoder
    """
    def __init__(self, cfg: LanguageModelSAERunnerConfig):
        self.cfg = cfg
    def load_session(self) -> Tuple[HookedTransformer, SparseAutoencoder, ActivationsStore]:
        '''
        Loads a session for training a sparse autoencoder on a language model.
        '''
        model = self.get_model(self.cfg.model_name)
        model.to(self.cfg.device)
        activations_loader = self.get_activations_loader(self.cfg, model)
        sparse_autoencoder = self.initialize_sparse_autoencoder(self.cfg)
        return model, sparse_autoencoder, activations_loader
    @classmethod
    def load_session_from_pretrained(cls, path: str) -> Tuple[HookedTransformer, SparseAutoencoder, ActivationsStore]:
        '''
        Loads a session for analysing a pretrained sparse autoencoder.
        '''
        if torch.backends.mps.is_available():
            cfg = torch.load(path, map_location="mps")["cfg"]
            cfg.device = "mps"
        elif torch.cuda.is_available():
            cfg = torch.load(path, map_location="cuda")["cfg"]
        else:
            cfg = torch.load(path, map_location="cpu")["cfg"]
        model, _, activations_loader = cls(cfg).load_session()
        sparse_autoencoder = SparseAutoencoder.load_from_pretrained(path)
        return model, sparse_autoencoder, activations_loader
    def get_model(self, model_name: str):
        '''
        Loads a model from transformer lens
        '''
        # Todo: add check that model_name is valid
        model = HookedTransformer.from_pretrained(model_name)
        return model 
    def initialize_sparse_autoencoder(self, cfg: LanguageModelSAERunnerConfig):
        '''
        Initializes a sparse autoencoder
        '''
        sparse_autoencoder = SparseAutoencoder(cfg)
        return sparse_autoencoder
    def get_activations_loader(self, cfg: LanguageModelSAERunnerConfig, model: HookedTransformer):
        '''
        Loads a DataLoaderBuffer for the activations of a language model.
        '''
        activations_loader = ActivationsStore(
            cfg, model,
        )
        return activations_loader
def shuffle_activations_pairwise(datapath: str, buffer_idx_range: Tuple[int, int]):
    """
    Shuffles two buffers on disk.
    """
    assert buffer_idx_range[0] < buffer_idx_range[1], \
        "buffer_idx_range[0] must be smaller than buffer_idx_range[1]"
    buffer_idx1 = torch.randint(buffer_idx_range[0], buffer_idx_range[1], (1,)).item()
    buffer_idx2 = torch.randint(buffer_idx_range[0], buffer_idx_range[1], (1,)).item()
    while buffer_idx1 == buffer_idx2: # Make sure they're not the same
        buffer_idx2 = torch.randint(buffer_idx_range[0], buffer_idx_range[1], (1,)).item()
    buffer1 = torch.load(f"{datapath}/{buffer_idx1}.pt")
    buffer2 = torch.load(f"{datapath}/{buffer_idx2}.pt")
    joint_buffer = torch.cat([buffer1, buffer2])
    # Shuffle them
    joint_buffer = joint_buffer[torch.randperm(joint_buffer.shape[0])]
    shuffled_buffer1 = joint_buffer[:buffer1.shape[0]]
    shuffled_buffer2 = joint_buffer[buffer1.shape[0]:]
    # Save them back
    torch.save(shuffled_buffer1, f"{datapath}/{buffer_idx1}.pt")
    torch.save(shuffled_buffer2, f"{datapath}/{buffer_idx2}.pt")

================
File: train_transcoder.py
================
# Transcoder training sample code
"""
This sample script can be used to train a transcoder on a model of your choice.
This code, along with the transcoder training code more generally, was largely
    adapted from an older version of Joseph Bloom's SAE training repo, the latest
    version of which can be found at https://github.com/jbloomAus/SAELens.
Most of the parameters given here are the same as the SAE training parameters
    listed at https://jbloomaus.github.io/SAELens/training_saes/.
Transcoder-specific parameters are marked as such in comments.
"""
import torch
import os 
import sys
import numpy as np
os.environ["TOKENIZERS_PARALLELISM"] = "false"
from sae_training.config import LanguageModelSAERunnerConfig
from sae_training.utils import LMSparseAutoencoderSessionloader
from sae_training.train_sae_on_language_model import train_sae_on_language_model
lr = 0.0004 # learning rate
l1_coeff = 0.00014 # l1 sparsity regularization coefficient
cfg = LanguageModelSAERunnerConfig(
    # Data Generating Function (Model + Training Distibuion)
    # "hook_point" is the TransformerLens HookPoint representing
    #    the input activations to the transcoder that we want to train on.
    # Here, "ln2.hook_normalized" refers to the activations after the
    #    pre-MLP LayerNorm -- that is, the inputs to the MLP.
    # You might alternatively prefer to train on "blocks.8.hook_resid_mid",
    #    which corresponds to the input to the pre-MLP LayerNorm.
    hook_point = "blocks.8.ln2.hook_normalized",
    hook_point_layer = 8,
    d_in = 768,
    dataset_path = "Skylion007/openwebtext",
    is_dataset_tokenized=False,
    model_name='gpt2-small',
    # Transcoder-specific parameters.
    is_transcoder = True, # We're training a transcoder here.
    # "out_hook_point" is the TransformerLens HookPoint representing
    #    the output activations that the transcoder should reconstruct.
    # In our use case, we're using transcoders to interpret MLP sublayers.
    # This means that our transcoder will take in the input to an MLP and
    #    attempt to spit out the output of the MLP (but in the form of a
    #    sparse linear combination of feature vectors).
    # As such, we want to grab the "hook_mlp_out" activations from our
    #    transformer, which (as the name suggests), represent the
    #    output activations of the original MLP sublayer.
    out_hook_point = "blocks.8.hook_mlp_out",
    out_hook_point_layer = 8,
    d_out = 768,
    # SAE Parameters
    expansion_factor = 32,
    b_dec_init_method = "mean",
    # Training Parameters
    lr = lr,
    l1_coefficient = l1_coeff,
    lr_scheduler_name="constantwithwarmup",
    train_batch_size = 4096,
    context_size = 128,
    lr_warm_up_steps=5000,
    # Activation Store Parameters
    n_batches_in_buffer = 128,
    total_training_tokens = 1_000_000 * 60,
    store_batch_size = 32,
    # Dead Neurons and Sparsity
    use_ghost_grads=True,
    feature_sampling_method = None,
    feature_sampling_window = 1000,
    resample_batches=1028,
    dead_feature_window=5000,
    dead_feature_threshold = 1e-8,
    # WANDB
    log_to_wandb = False,
    # Misc
    use_tqdm = True,
    device = "cuda",
    seed = 42,
    n_checkpoints = 3,
    checkpoint_path = "gpt2-small-transcoders", # change as you please
    dtype = torch.float32,
)
print(f"About to start training with lr {lr} and l1 {l1_coeff}")
print(f"Checkpoint path: {cfg.checkpoint_path}")
print(cfg)
loader = LMSparseAutoencoderSessionloader(cfg)
model, sparse_autoencoder, activations_loader = loader.load_session()
# train SAE
sparse_autoencoder = train_sae_on_language_model(
    model, sparse_autoencoder, activations_loader,
    n_checkpoints=cfg.n_checkpoints,
    batch_size = cfg.train_batch_size,
    feature_sampling_method = cfg.feature_sampling_method,
    feature_sampling_window = cfg.feature_sampling_window,
    feature_reinit_scale = cfg.feature_reinit_scale,
    dead_feature_threshold = cfg.dead_feature_threshold,
    dead_feature_window=cfg.dead_feature_window,
    use_wandb = cfg.log_to_wandb,
    wandb_log_frequency = cfg.wandb_log_frequency
)
# save sae to checkpoints folder
path = f"{cfg.checkpoint_path}/final_{sparse_autoencoder.get_name()}.pt"
sparse_autoencoder.save_model(path)

================
File: transcoder_circuits/circuit_analysis.py
================
# --- code for circuit analysis --- #
import torch
import numpy as np
import tqdm
from transformer_lens.utils import get_act_name, to_numpy
# first, some preliminary functions, used to calculate
#   attribs between pairs of components
@torch.no_grad()
def get_attn_head_contribs(model, cache, layer, range_normal):
	split_vals = cache[get_act_name('v', layer)]
	attn_pattern = cache[get_act_name('pattern', layer)]
	#'batch head dst src, batch src head d_head -> batch head dst src d_head'
	weighted_vals = torch.einsum(
		'b h d s, b s h f -> b h d s f',
		attn_pattern, split_vals
	)
  # 'batch head dst src d_head, head d_head d_model -> batch head dst src d_model'
	weighted_outs = torch.einsum(
		'b h d s f, h f m -> b h d s m',
		weighted_vals, model.W_O[layer]
	)
  # 'batch head dst src d_model, d_model -> batch head dst src'
	contribs = torch.einsum(
		'b h d s m, m -> b h d s',
		weighted_outs, range_normal
	)
	return contribs
@torch.no_grad()
def get_transcoder_ixg(transcoder, cache, range_normal, input_layer, input_token_idx, return_numpy=True, is_transcoder_post_ln=True, return_feature_activs=True):
    pulledback_feature = transcoder.W_dec @ range_normal
    if is_transcoder_post_ln:
        act_name = get_act_name('normalized', input_layer, 'ln2')
    else:
        act_name = get_act_name('resid_mid', input_layer)
    feature_activs = transcoder(cache[act_name])[1][0,input_token_idx]
    pulledback_feature = pulledback_feature * feature_activs
    if return_numpy:
        pulledback_feature = to_numpy(pulledback_feature)
        feature_activs = to_numpy(feature_activs)
    if not return_feature_activs:
        return pulledback_feature
    else:
        return pulledback_feature, feature_activs
# get the mean input-times-gradient vector over a dataset of tokens
@torch.no_grad()
def get_mean_ixg(model, tokens_arr, range_transcoder, range_feature_idx, transcoder, token_idxs=None, batch_size=64, do_sum_count=False):
    act_name = transcoder.cfg.hook_point
    layer = transcoder.cfg.hook_point_layer
    range_normal = range_transcoder.W_enc[:, range_feature_idx]
    pulledback_feature = transcoder.W_dec @ range_normal
    if token_idxs is None:
        tokens_gen = tqdm.tqdm(range(0, tokens_arr.shape[0], batch_size))
    else:
        tokens_gen = tqdm.tqdm(token_idxs)
    if not do_sum_count:
        mean_ixgs = []
    else:
        ixgs_sum = np.zeros(transcoder.W_enc.shape[1])
        ixgs_count = np.zeros(transcoder.W_enc.shape[1])
    for t in tokens_gen:
        if token_idxs is not None:
            example_idx, token_idx = t
            with torch.no_grad():
                _, cache = model.run_with_cache(tokens_arr[example_idx, :token_idx+1], stop_at_layer=layer+1, names_filter=[
                    act_name
                ])
                acts = cache[act_name]
                feature_activs = transcoder(acts)[1][0, token_idx]
                cur_ixg = (pulledback_feature * feature_activs)[None]
        else:
            i = t
            with torch.no_grad():
                _, cache = model.run_with_cache(tokens_arr[i:i+batch_size], stop_at_layer=layer+1, names_filter=[
                    act_name
                ])
                acts = cache[act_name]
                feature_activs = transcoder(acts)[1].reshape(-1, transcoder.W_enc.shape[1])
                cur_ixg = torch.einsum('i, ji -> ji', pulledback_feature, feature_activs)
        if not do_sum_count:
            mean_ixgs.append(np.mean(to_numpy(cur_ixg), axis=0))
        else:
            ixgs_sum += to_numpy(cur_ixg).sum(axis=0)
            ixgs_count += np.abs(to_numpy(cur_ixg)>0).sum(axis=0)
    if do_sum_count:
        ixgs_count[ixgs_count == 0] = 1
        return ixgs_sum/ixgs_count, ixgs_count/len(token_idxs)
    else:
        return np.mean(mean_ixgs, axis=0)
# approximate layernorms as constants when propagating feature vectors backward
# for theoretical motivation, see the LayerNorm section of
#	https://www.neelnanda.io/mechanistic-interpretability/attribution-patching
@torch.no_grad()
def get_ln_constant(model, cache, vector, layer, token, is_ln2=False, recip=False):
    x_act_name = get_act_name('resid_mid', layer) if is_ln2 else get_act_name('resid_pre', layer)
    x = cache[x_act_name][0, token]
    y_act_name = get_act_name('normalized', layer, 'ln2' if is_ln2 else 'ln1')
    y = cache[y_act_name][0, token]
    if torch.dot(vector, x) == 0:
        return torch.tensor(0.)
    return torch.dot(vector, y)/torch.dot(vector, x) if not recip else torch.dot(vector, x)/torch.dot(vector, y)
# now, we'll actually write the circuit analysis code that finds
#  computational paths and calculates their importances
import enum
from dataclasses import dataclass, field
from typing import Optional, List
import copy
# define some classes
class ComponentType(enum.Enum):
    MLP = 'mlp'
    ATTN = 'attn'
    EMBED = 'embed'
    # error terms
    TC_ERROR = 'tc_error' # error due to inaccurate transcoders
    PRUNE_ERROR = 'prune_error' # error due to only looking at top paths in graph
    BIAS_ERROR = 'bias_error' # account for bias terms in transcoders
class FeatureType(enum.Enum):
    NONE = 'none'
    SAE = 'sae'
    TRANSCODER = 'tc'
class ContribType(enum.Enum):
    RAW = 'raw'
    ZERO_ABLATION = 'zero_ablation'
# Component: an individual component (e.g. an attn head or a transcoder feature)
@dataclass
class Component:
    layer: int
    component_type: ComponentType
    token: Optional[int] = None
    attn_head: Optional[int] = None
    feature_type: Optional[FeatureType] = None
    feature_idx: Optional[int] = None
    def __str__(self, show_token=True):
        retstr = ''
        feature_type_str = ''
        base_str = f'{self.component_type.value}{self.layer}'
        attn_str = '' if self.component_type != ComponentType.ATTN else f'[{self.attn_head}]'
        feature_str = ''
        if self.feature_type is not None and self.feature_idx is not None:
            feature_str = f"{self.feature_type.value}[{self.feature_idx}]"
        token_str = ''
        if self.token is not None and show_token:
            token_str = f'@{self.token}'
        retstr = ''.join([base_str, attn_str, feature_str, token_str])
        return retstr
    def __repr__(self):
        return f'<Component object {str(self)}>'
# FeatureVector: a unique feature vector potentially associated with a path of components
#  along with a contrib
@dataclass
class FeatureVector:
    # a list of components that can be used to uniquely specify the direction of the feature vector
    component_path: List[Component]
    # TODO: potentially add "gradients" and "activations" lists
    vector: torch.Tensor
    # sublayer can be 'mlp_out', 'resid_post', 'resid_mid', 'resid_pre'
    # denotes where the feature vector lives
    layer: int
    sublayer: str
    token: Optional[int] = None
    contrib: Optional[float] = None
    contrib_type: Optional[ContribType] = None
    error: float = 0.0
    def __post_init__(self):
        if self.token is None and len(self.component_path)>0: self.token = self.component_path[-1].token
        if self.layer is None and len(self.component_path)>0: self.layer = self.component_path[-1].layer
    # note: str(FeatureVector) should return a string that uniquely identifies a feature direction (e.g. for use in a causal graph)
    # (this is distinct from a unique feature *vector*, by the way)
    def __str__(self, show_full=True, show_contrib=True, show_last_token=True):
        retstr = ''
        token_str = '' if self.token is None or not show_last_token else f'@{self.token}'
        if len(self.component_path) > 0:
            if show_full:
                retstr = ''.join(x.__str__(show_token=False) for x in self.component_path[:-1])
            retstr = ''.join([retstr, self.component_path[-1].__str__(show_token=False), token_str])
        else:
            retstr = f'*{self.sublayer}{self.layer}{token_str}'
        if show_contrib and self.contrib is not None:
            retstr = ''.join([retstr, f': {self.contrib:.2}'])
        return retstr
    def __repr__(self):
        contrib_type_str = '' if self.contrib_type is None else f' contrib_type={self.contrib_type.value}'
        return f'<FeatureVector object {str(self)}, sublayer={self.sublayer}{contrib_type_str}>'
@torch.no_grad()
def make_sae_feature_vector(sae, feature_idx, use_encoder=True, token=-1):
    hook_point = sae.cfg.hook_point if (use_encoder or not sae.cfg.is_transcoder) else sae.cfg.out_hook_point
    layer = sae.cfg.hook_point_layer if (use_encoder or not sae.cfg.is_transcoder) else sae.cfg.out_hook_point_layer
    feature_type = FeatureType.SAE if not sae.cfg.is_transcoder else FeatureType.TRANSCODER
    vector = sae.W_enc[:,feature_idx] if use_encoder else sae.W_dec[feature_idx]
    vector = torch.clone(vector.detach())
    vector.requires_grad = False
    vector.requires_grad_(False)
    if 'resid_mid' in hook_point or ('normalized' in hook_point and 'ln2' in hook_point):
        # currently, we treat ln2normalized as resid_mid
        # this is kinda ugly, but because we account for layernorm constants in later
        #  functions, this does work now
        sublayer = 'resid_mid'
        component_type = ComponentType.MLP
    elif 'resid_pre' in hook_point:
        sublayer = 'resid_pre'
        component_type = ComponentType.ATTN
    elif 'mlp_out' in hook_point:
        sublayer = 'mlp_out'
        component_type = ComponentType.MLP
    elif 'resid_post' in hook_point:
        sublayer = 'resid_post'
        component_type = ComponentType.ATTN
    my_feature = FeatureVector(
        component_path=[Component(
            layer=layer,
            component_type=component_type,
            token=token,
            feature_type=feature_type,
            feature_idx=feature_idx
        )],
        layer = layer,
        sublayer = sublayer,
        vector = vector
    )
    return my_feature
@torch.no_grad()
def get_top_transcoder_features(model, transcoder, cache, feature_vector, layer, k=5):
    my_token = feature_vector.token if feature_vector.token >= 0 else cache[get_act_name('resid_pre', 0)].shape[1]+feature_vector.token
    is_transcoder_post_ln = 'ln2' in transcoder.cfg.hook_point and 'normalized' in transcoder.cfg.hook_point
    # compute error
    if is_transcoder_post_ln:
        act_name = get_act_name('normalized', layer, 'ln2')
    else:
        act_name = get_act_name('resid_mid', layer)
    transcoder_out = transcoder(cache[act_name])[0][0,my_token]
    mlp_out = model.blocks[layer].mlp(cache[act_name])[0, my_token]
    error = torch.dot(feature_vector.vector, mlp_out - transcoder_out)/torch.dot(feature_vector.vector, mlp_out)
    # compute pulledback feature
    pulledback_feature, feature_activs = get_transcoder_ixg(transcoder, cache, feature_vector.vector, layer, feature_vector.token, return_numpy=False, is_transcoder_post_ln=is_transcoder_post_ln)
    top_contribs, top_indices = torch.topk(pulledback_feature, k=k)
    top_contribs_list = []
    for contrib, index in zip(top_contribs, top_indices):
        vector = transcoder.W_enc[:, index]
        vector = vector * (transcoder.W_dec @ feature_vector.vector)[index]
        if is_transcoder_post_ln:
            vector = vector * get_ln_constant(model, cache, vector, layer, feature_vector.token, is_ln2=True)
        new_component = Component(
            layer=layer,
            component_type=ComponentType.MLP,
            token=my_token,
            feature_type=FeatureType.TRANSCODER,
            feature_idx=index.item(),
        )
        top_contribs_list.append(FeatureVector(
            component_path=[new_component],
            vector = vector,
            layer=layer,
            sublayer="resid_mid",
            contrib=contrib.item(),
            contrib_type=ContribType.RAW,
            error=error,
        ))
    return top_contribs_list
@torch.no_grad()
def get_top_contribs(model, transcoders, cache, feature_vector, k=5, ignore_bos=True, only_return_all_scores=False, cap=None, filter=None):
    if feature_vector.sublayer == "mlp_out":
        return get_top_transcoder_features(model, transcoders[feature_vector.layer], cache, feature_vector, feature_vector.layer, k=k)
    my_layer = feature_vector.layer
    # get MLP contribs
    all_mlp_contribs = []
    mlp_max_layer = my_layer + (1 if feature_vector.sublayer == 'resid_post' else 0)
    for cur_layer in range(mlp_max_layer):
        cur_top_features = get_top_transcoder_features(model, transcoders[cur_layer], cache, feature_vector, cur_layer, k=k)
        all_mlp_contribs = all_mlp_contribs + cur_top_features
    # get attn contribs
    all_attn_contribs = []
    attn_max_layer = my_layer + (1 if feature_vector.sublayer == 'resid_post' or feature_vector.sublayer == 'resid_mid' else 0)
    for cur_layer in range(attn_max_layer):
        attn_contribs = get_attn_head_contribs(model, cache, cur_layer, feature_vector.vector)[0, :, feature_vector.token, :]
        if ignore_bos:
            attn_contribs = attn_contribs[:, 1:]
        top_attn_contribs_flattened, top_attn_contrib_indices_flattened = torch.topk(attn_contribs.flatten(), k=np.min([k, len(attn_contribs)]))
        top_attn_contrib_indices = np.array(np.unravel_index(to_numpy(top_attn_contrib_indices_flattened), attn_contribs.shape)).T
        for contrib, (head, src_token) in zip(top_attn_contribs_flattened, top_attn_contrib_indices):
            if ignore_bos:
                src_token = src_token + 1
            vector = model.OV[cur_layer, head] @ feature_vector.vector
            attn_pattern = cache[get_act_name('pattern', cur_layer)]
            vector = vector * attn_pattern[0, head, feature_vector.token, src_token]
            ln_constant = get_ln_constant(model, cache, vector, cur_layer, src_token, is_ln2=False)
            vector = vector * ln_constant
            if ln_constant.isnan(): print("Nan!")
            new_component = Component(
                layer=cur_layer,
                component_type=ComponentType.ATTN,
                token=src_token,
                attn_head=head
            )
            new_feature_vector = FeatureVector(
                component_path=feature_vector.component_path + [new_component],
                vector=vector,
                layer=cur_layer,
                sublayer='resid_pre',
                contrib=contrib.item(),
                contrib_type=ContribType.RAW
            )
            all_attn_contribs.append(new_feature_vector)
    # get embedding contribs
    my_token = feature_vector.token if feature_vector.token >= 0 else cache[get_act_name('resid_pre', 0)].shape[1]+feature_vector.token
    embedding_contrib = FeatureVector(
        component_path = feature_vector.component_path + [Component(
            layer=0,
            component_type=ComponentType.EMBED,
            token=my_token,
        )],
        vector=feature_vector.vector,
        layer=0,
        sublayer='resid_pre',
        contrib=torch.dot(cache[get_act_name('resid_pre', 0)][0, feature_vector.token], feature_vector.vector).item(),
        contrib_type=ContribType.RAW
    )
    # get top contribs from all categories
    all_contribs = all_mlp_contribs + all_attn_contribs + [embedding_contrib]
    if filter is not None:
        all_contribs = [ x for x in all_contribs if filter.match(x) ]
    if cap is not None:
        for i, contrib in enumerate(all_contribs):
            if contrib.contrib > cap:
                all_contribs[i].contrib = cap
                all_contribs[i].contrib_type = ContribType.ZERO_ABLATION
    all_contrib_scores = torch.tensor([x.contrib for x in all_contribs])
    if only_return_all_scores: return all_contrib_scores
    _, top_contrib_indices = torch.topk(all_contrib_scores, k=np.min([k, len(all_contrib_scores)]))
    return [all_contribs[i.item()] for i in top_contrib_indices]
@torch.no_grad()
def greedy_get_top_paths(model, transcoders, cache, feature_vector, num_iters=2, num_branches=5, ignore_bos=True, do_raw_attribution=False, filter=None):
    do_cap = not do_raw_attribution # historical name change; TODO: refactor
    all_paths = []
    new_root = copy.deepcopy(feature_vector)
    # deal with LN constant
    # TODO: this is hacky and makes the assumption that if feature_vector is a transcoder feature, then it comes from the passed list of transcoders
    if new_root.component_path[-1].feature_type == FeatureType.TRANSCODER:
        tc = transcoders[new_root.layer]
        if 'ln2.hook_normalized' in tc.cfg.hook_point:
            ln_constant = get_ln_constant(model, cache, new_root.vector, new_root.layer, new_root.token, is_ln2=True)
            new_root.vector *= ln_constant
        new_root.contrib = tc(cache[tc.cfg.hook_point])[1][0, new_root.token, new_root.component_path[-1].feature_idx].item()
    cur_paths = [[new_root]]
    for iter in range(num_iters):
        new_paths = []
        for path in cur_paths:
            cur_feature = path[-1]
            if cur_feature.layer == 0 and cur_feature.sublayer == 'resid_pre': continue
            cap = None
            if do_cap:
                # Cap feature contribs at smallest transcoder feature activation
                # This corresponds to calculating feature attribs by
                #   zero-ablating the output of the feature
                for cap_feature in path:
                    if len(cap_feature.component_path) > 0 and (cap_feature.component_path[-1].feature_type == FeatureType.TRANSCODER or cap_feature.component_path[-1].feature_type == FeatureType.SAE and (cap is None or cap_feature.contrib < cap)):
                        cap = cap_feature.contrib
            cur_top_contribs = get_top_contribs(model, transcoders, cache, cur_feature, k=num_branches, ignore_bos=ignore_bos, cap=cap, filter=filter)
            new_paths = new_paths + [ path + [cur_top_contrib] for cur_top_contrib in cur_top_contribs ]
        _, top_new_path_indices = torch.topk(torch.tensor([new_path[-1].contrib for new_path in new_paths]), k=np.min([num_branches, len(new_paths)]))
        cur_paths = [ new_paths[i] for i in top_new_path_indices ]
        all_paths.append(cur_paths)
    return all_paths
def print_all_paths(paths):
    if len(paths) == 0: return
    if type(paths[0][0]) is list:
        for i, cur_paths in enumerate(paths):
            try:
                print(f"--- Paths of size {len(cur_paths[0])} ---")
            except:
                continue
            for j, cur_path in enumerate(cur_paths):
                print(f"Path [{i}][{j}]: ", end="")
                print(" <- ".join(map(lambda x: x.__str__(show_full=False, show_last_token=True), cur_path)))
    else:
        for j, cur_path in enumerate(paths):
            print(f"Path [{j}]: ", end="")
            print(" <- ".join(map(lambda x: x.__str__(show_full=False, show_last_token=True), cur_path)))
@torch.no_grad()
def get_raw_top_features_among_paths(all_paths, use_tokens=True, top_k=5, filter_layers=None, filter_sublayers=None):
    retdict = {}
    str_to_feature = {}
    for i, cur_length_paths in enumerate(all_paths):
        for j, cur_path in enumerate(cur_length_paths):
            cur_feature = cur_path[-1]
            if filter_layers is not None and cur_feature.layer not in filter_layers: continue
            if filter_sublayers is not None and cur_feature.sublayer not in filter_sublayers: continue
            cur_feature_str = cur_feature.__str__(show_contrib=False, show_last_token=use_tokens)
            contrib = 0
            if cur_feature.contrib is not None:
                assert(cur_feature.contrib_type == ContribType.RAW)
                contrib = cur_feature.contrib
            if cur_feature_str not in retdict:
                try:
                    retdict[cur_feature_str] = copy.deepcopy(cur_feature)
                except Exception as e:
                    print(cur_feature)
                    print(cur_feature.component_path)
                    print(cur_feature.vector)
                    raise e
                retdict[cur_feature_str].contrib = contrib
            else:
                retdict[cur_feature_str].contrib = retdict[cur_feature_str].contrib + contrib
    if top_k is None or top_k > len(retdict): top_k = len(retdict)
    top_scores, top_indices = torch.topk(torch.tensor([x.contrib for x in retdict.values()], dtype=torch.float), k=top_k)
    retlist = []
    keys_list = list(retdict.keys())
    for score, index in zip(top_scores, top_indices):
        cur_feature = retdict[keys_list[index.item()]]
        if not use_tokens:
            for component in cur_feature.component_path: component.token=None
            cur_feature.token=None
        retlist.append(cur_feature)
    return retlist
# now, code for filtering computational paths
import dataclasses
class FilterType(enum.Enum):
    EQ = enum.auto() # equals
    NE = enum.auto() # not equal to
    GT = enum.auto() # greater than
    GE = enum.auto() # greater than or equal to
    LT = enum.auto() # less than 
    LE = enum.auto() # less than or equal to
@dataclass
class FeatureFilter:
    # feature-level filters
    layer: Optional[int] = field(default=None, metadata={'filter_level': 'feature'})
    layer_filter_type: FilterType = FilterType.EQ
    sublayer: Optional[int] = field(default=None, metadata={'filter_level': 'feature'})
    sublayer_filter_type: FilterType = FilterType.EQ
    token: Optional[int] = field(default=None, metadata={'filter_level': 'feature'})
    token_filter_type: FilterType = FilterType.EQ
    # filters on last component in component_path
    component_type: Optional[ComponentType] = field(default=None, metadata={'filter_level': 'component'})
    component_type_filter_type: FilterType = FilterType.EQ
    attn_head: Optional[int] = field(default=None, metadata={'filter_level': 'component'})
    attn_head_filter_type: FilterType = FilterType.EQ
    feature_type: Optional[FeatureType] = field(default=None, metadata={'filter_level': 'component'})
    feature_type_filter_type: FilterType = FilterType.EQ
    feature_idx: Optional[int] = field(default=None, metadata={'filter_level': 'component'}) 
    feature_idx_filter_type: FilterType = FilterType.EQ       
    def match(self, feature):
        component = None
        for field in dataclasses.fields(self):
            name = field.name
            val = self.__dict__[name]
            if val is None: continue
            try:
                filter_level = field.metadata['filter_level']
            except KeyError:
                continue # not a filter
            if filter_level == 'feature':
                if val is not None:
                    filter_type = self.__dict__[f'{name}_filter_type']
                    if filter_type == FilterType.EQ and val != feature.__dict__[name]: return False
                    if filter_type == FilterType.NE and val == feature.__dict__[name]: return False
                    if filter_type == FilterType.GT and feature.__dict__[name] <= val: return False
                    if filter_type == FilterType.GE and feature.__dict__[name] < val: return False
                    if filter_type == FilterType.LT and feature.__dict__[name] >= val: return False
                    if filter_type == FilterType.LE and feature.__dict__[name] > val: return False
            elif filter_level == 'component':
                if component is None:
                    if len(feature.component_path) <= 0: return False
                    component = feature.component_path[-1]
                if val is not None:
                    filter_type = self.__dict__[f'{name}_filter_type']
                    if filter_type == FilterType.EQ and val != component.__dict__[name]: return False
                    if filter_type == FilterType.NE and val == component.__dict__[name]: return False
        return True
import functools
def flatten_nested_list(x):
    return list(functools.reduce(lambda a,b: a+b, x))
def get_paths_via_filter(all_paths, infix_path=None, not_infix_path=None, suffix_path=None):
    retpaths = []
    if type(all_paths[0][0]) is list:
        path_list = flatten_nested_list(all_paths)
    else:
        path_list = all_paths
    for path in path_list:
        if not_infix_path is not None:
            if len(path) < len(not_infix_path): continue
            match_started = False
            path_good = True
            i = 0
            for j, cur_feature in enumerate(path):
                cur_infix_filter = not_infix_path[i]
                if cur_infix_filter.match(cur_feature):
                    if not match_started:
                        if len(path[j:]) < len(not_infix_path): break
                        match_started = True
                elif match_started:
                    path_good = False
                    break
                if match_started:
                    i = i + 1
                    if i >= len(not_infix_path): break
            if not (match_started and path_good): retpaths.append(path)
        if infix_path is not None:
            if len(path) < len(infix_path): continue
            match_started = False
            path_good = True
            i = 0
            for j, cur_feature in enumerate(path):
                cur_infix_filter = infix_path[i]
                if cur_infix_filter.match(cur_feature):
                    if not match_started:
                        if len(path[j:]) < len(infix_path): break
                        match_started = True
                elif match_started:
                    path_good = False
                    break
                if match_started:
                    i = i + 1
                    if i >= len(infix_path): break
            if match_started and path_good: retpaths.append(path)
        if suffix_path is not None:
            if len(path) < len(suffix_path): continue
            path_good = True
            for i in range(1, len(suffix_path)+1):
                cur_feature = path[-i]
                cur_suffix_filter = suffix_path[-i]
                if not cur_suffix_filter.match(cur_feature):
                    path_good = False
                    break
            if path_good: retpaths.append(path)
    return retpaths
# code for combining computational paths into graphs
def path_to_str(path, show_contrib=False, show_last_token=False):
    return " <- ".join(list(x.__str__(show_contrib=show_contrib, show_last_token=show_last_token) for x in path))
import collections
@torch.no_grad()
def paths_to_graph(all_paths):
    if type(all_paths[0][0]) is list:
        path_list = flatten_nested_list(all_paths)
    else:
        path_list = all_paths
    retdict = collections.defaultdict(int)
    nodes = {}
    seen_prefixes = set()
    for i, cur_path in enumerate(path_list):
        for j in range(0, len(cur_path)):
            prefix = cur_path[:j+1]
            prefix_str = path_to_str(prefix, show_last_token=True)
            if prefix_str in seen_prefixes: continue
            seen_prefixes.add(prefix_str)
            if j == 0:
                # prefix is of size 1
                try:
                    my_contrib = prefix[0].contrib if prefix[0].contrib is not None else 0
                    nodes[prefix_str].contrib += my_contrib
                    if prefix[0].contrib is not None and my_contrib != 0:
                        nodes[prefix_str].vector = nodes[prefix_str].vector + prefix[0].vector
                except KeyError:
                    nodes[prefix_str] = copy.copy(prefix[0])
                continue
            parent, child = cur_path[j-1], cur_path[j]
            parent_str = parent.__str__(show_contrib=False, show_last_token=True, show_full=False)
            child_str = child.__str__(show_contrib=False, show_last_token=True, show_full=False)
            assert child.contrib_type == ContribType.RAW, f"Contrib type is not ContribType.RAW: {child_str}->{parent_str}"
            retdict[(child_str, parent_str)] += child.contrib
            try:
                nodes[child_str].contrib += child.contrib
                nodes[child_str].vector = nodes[child_str].vector + child.vector
            except KeyError:
                nodes[child_str] = copy.deepcopy(child)
    # last step: go through all the attention nodes and trim their component_paths
    # (this is because they now correspond to multiple later-layer features)
    new_nodes = { node_str: nodes[node_str] for node_str in nodes }
    for node_str in nodes:
        node = nodes[node_str]
        if node.component_path[-1].component_type != ComponentType.ATTN: continue
        node.component_path = [node.component_path[-1]]
        try:
            del new_nodes[node_str]
        except:
            pass
        new_nodes[node.__str__(show_contrib=False, show_last_token=True, show_full=False)] = node
    return retdict, new_nodes
@torch.no_grad()
def add_error_nodes_to_graph(model, cache, transcoders, edges, nodes, do_bias=True):
    # add error nodes representing error in transcoders and error due to computational paths being pruned
    # first: deal with transcoder error
    new_edges = { edge: edges[edge] for edge in edges }
    new_nodes = { node_str: nodes[node_str] for node_str in nodes }
    # only want to add error nodes to non-leaf nodes
    # also, fill up a dict of nodes' children for later
    children_dict = {}
    for child_str, parent_str in edges:
        if parent_str in children_dict:
            children_dict[parent_str].append(child_str)
            continue
        else:
            children_dict[parent_str] = [child_str]
        parent = nodes[parent_str]
        if parent.sublayer not in ['resid_mid', 'resid_pre', 'resid_post']: continue
        max_layer = parent.layer
        if parent.sublayer == 'resid_post': max_layer = max_layer + 1
        error = 0.
        #print(max_layer, parent.sublayer)
        for layer in range(max_layer):
            mlp_out = cache[get_act_name('mlp_out', layer)][0, parent.token]
            tc = transcoders[layer]
            tc_out = tc(cache[tc.cfg.hook_point])[0][0, parent.token]
            error += torch.dot(parent.vector, mlp_out - tc_out).item()
        error_feature = FeatureVector(
            component_path=[parent.component_path[-1], Component(
                layer=parent.layer,
                component_type=ComponentType.TC_ERROR,
                token=parent.token,
            )],
            layer = parent.layer,
            sublayer = parent.sublayer,
            vector = None, # TODO: deal with conflicting type annotation
            contrib = error,
        )
        error_str = error_feature.__str__(show_contrib=False, show_last_token=True, show_full=True)
        new_edges[(error_str, parent_str)] = error
        new_nodes[error_str] = error_feature
    edges = new_edges
    nodes = new_nodes
    # next, deal with bias term error
    if do_bias:
        new_edges = { edge: edges[edge] for edge in edges }
        new_nodes = { node_str: nodes[node_str] for node_str in nodes }
        for parent_str in children_dict:
            #print(parent_str)
            parent = nodes[parent_str]
            if parent.component_path[-1].feature_type not in [FeatureType.TRANSCODER]: continue #[FeatureType.TRANSCODER, FeatureType.SAE]
            bias = (-transcoders[parent.layer].W_enc[:, parent.component_path[-1].feature_idx]\
                @ transcoders[parent.layer].b_dec\
                + transcoders[parent.layer].b_enc[ parent.component_path[-1].feature_idx]).item()
            bias_feature = FeatureVector(
                component_path=[parent.component_path[-1], Component(
                    layer=parent.layer,
                    component_type=ComponentType.BIAS_ERROR,
                    token=parent.token,
                )],
                layer = parent.layer,
                sublayer = parent.sublayer,
                vector = None, # TODO: deal with conflicting type annotation
                contrib = bias,
            )
            bias_str = bias_feature.__str__(show_contrib=False, show_last_token=True, show_full=True)
            new_edges[(bias_str, parent_str)] = bias
            new_nodes[bias_str] = bias_feature
        edges = new_edges
        nodes = new_nodes
    # next, deal with pruning error
    new_edges = { edge: edges[edge] for edge in edges }
    new_nodes = { node_str: nodes[node_str] for node_str in nodes }
    for parent_str in children_dict:
        edge_contribs = 0.
        for child_str in children_dict[parent_str]:
            edge_contribs += edges[(child_str, parent_str)]
        parent = nodes[parent_str]
        error = parent.contrib - edge_contribs
        error_feature = FeatureVector(
            component_path=[parent.component_path[-1], Component(
                layer=parent.layer,
                component_type=ComponentType.PRUNE_ERROR,
                token=parent.token,
            )],
            layer = parent.layer,
            sublayer = parent.sublayer,
            vector = None, # TODO: deal with conflicting type annotation
            contrib = error,
        )
        error_str = error_feature.__str__(show_contrib=False, show_last_token=True, show_full=True)
        new_edges[(error_str, parent_str)] = error
        new_nodes[error_str] = error_feature
    return new_edges, new_nodes    
def sum_over_tokens(edges, nodes):
    new_edges = collections.defaultdict(int)
    for (child_str, parent_str), score in edges.items():
        new_child_str = nodes[child_str].__str__(show_contrib=False, show_last_token=False, show_full=False)
        new_parent_str = nodes[parent_str].__str__(show_contrib=False, show_last_token=False, show_full=False)
        new_edges[new_child_str, new_parent_str] += score
    new_nodes = {}
    for node in nodes.values():
        node_str = node.__str__(show_contrib=False, show_last_token=False, show_full=False)
        try:
            new_nodes[node_str].contrib += node.contrib
        except KeyError:
            new_nodes[node_str] = copy.copy(node)
    return new_edges, new_nodes
# graph plotting code
# NOTE: this code doesn't produce the nicest looking graphs
import plotly.graph_objs as go
import collections
def layer_to_float(feature):
    layer = feature.layer
    if feature.sublayer == 'resid_mid': layer = layer + 0.5
    if feature.component_path[-1].component_type in [ComponentType.PRUNE_ERROR, ComponentType.BIAS_ERROR, ComponentType.TC_ERROR]:
        layer = layer - 0.25
    return layer
def nodes_to_coords(nodes, y_jitter=0.3, y_mult=1.0):
    retdict = {}
    num_nodes_in_xval = collections.defaultdict(int)
    for node_name, feature in nodes.items():
        xval = layer_to_float(feature)
        retdict[node_name] = [xval, num_nodes_in_xval[xval]]
        num_nodes_in_xval[xval] += 1
    for node_name in retdict:
        num_nodes = num_nodes_in_xval[retdict[node_name][0]]
        retdict[node_name][1] = retdict[node_name][1]/(num_nodes-1) if num_nodes != 1 else 0.5
        if num_nodes != 1: 
            cur_y_noise = np.random.uniform(0, y_jitter)
            if retdict[node_name][1] > 0.5: cur_y_noise *= -1
            cur_y_noise /= (num_nodes-1)
        else:
            cur_y_noise = np.random.uniform(-y_jitter, y_jitter)
        retdict[node_name][1] += cur_y_noise
        retdict[node_name][1] *= y_mult
    return retdict
def get_contribs_in_graph(edges, nodes):
    new_nodes = {}
    for (child_str, parent_str), contrib in edges.items():
        try:
            new_nodes[parent_str].contrib += contrib
        except KeyError:
            new_nodes[parent_str] = copy.copy(nodes[parent_str])
            new_nodes[parent_str].contrib = contrib
    for node in nodes:
        if node not in new_nodes:
            new_nodes[node] = copy.copy(nodes[node])
    return new_nodes
def plot_graph(edges, nodes, y_mult=1.0, width=800, height=600, arrow_width_multiplier=3.0, only_get_contribs_in_graph=False):
    # TODO: ugly code reuse with feature_dashboard
    def batch_color_interpolate(scores, max_color, zero_color, scores_min=None, scores_max=None):
        if scores_min is None: scores_min = scores.min()
        if scores_max is None: scores_max = scores.max()
        scores_normalized = (scores - scores_min) / (scores_max - scores_min)
        max_color_vec = np.array([int(max_color[1:3], 16), int(max_color[3:5], 16), int(max_color[5:7], 16)])
        zero_color_vec = np.array([int(zero_color[1:3], 16), int(zero_color[3:5], 16), int(zero_color[5:7], 16)])
        color_vecs = np.einsum('i, j -> ij', scores_normalized, max_color_vec) + np.einsum('i, j -> ij', 1-scores_normalized, zero_color_vec)
        color_strs = [f"#{int(x[0]):02x}{int(x[1]):02x}{int(x[2]):02x}" for x in color_vecs]
        return color_strs
    arrow_widths = np.array(list(edges.values()))
    arrow_widths = 1 + arrow_width_multiplier*(arrow_widths-arrow_widths.min())/(arrow_widths.max()-arrow_widths.min())
    layout = nodes_to_coords(nodes, y_mult=y_mult)
    colors = batch_color_interpolate(np.array([x.contrib if x.contrib is not None else 0 for x in nodes.values()]), '#22ff22', '#ffffff')
    trace = go.Scatter( 
        x=[val[0] for val in layout.values()], 
        y=[val[1] for val in layout.values()],
        hoverinfo='text',
        hovertext=[f"{key}<br>Contrib: {val.contrib:.2}" if val.contrib is not None else f"{key}" for key, val in nodes.items()],
        mode='markers',
        marker=dict(size=20, line=dict(width=1, color='Black'), color=colors)
    )
    trace2 = go.Scatter( 
        x=[(layout[edge[0]][0]+layout[edge[1]][0])/2 for edge in edges], 
        y=[(layout[edge[0]][1]+layout[edge[1]][1])/2 for edge in edges],
        hoverinfo='text',
        hovertext=[f"{edge[0]} -> {edge[1]}<br>Contrib: {contrib:.2}" if contrib is not None else f"{edge[0]} -> {edge[1]}" for edge, contrib in edges.items()],
        mode='markers', marker_symbol='square',
        marker=dict(size=5, line=dict(width=1, color='Black'), color='black')
    )
    # Plot edges
    # Thank you to https://stackoverflow.com/a/51430912 for the general idea
    x0, y0, x1, y1 = [], [], [], []
    for edge in edges:
        x0.append(layout[edge[0]][0])
        y0.append(layout[edge[0]][1])
        x1.append(layout[edge[1]][0])
        y1.append(layout[edge[1]][1])
    fig = go.Figure(
        data=[trace, trace2],
        layout=go.Layout(
            autosize=False,
            width=width,
            height=height,
            showlegend=False,
            xaxis_title='Layer',
            annotations = [
                dict(ax=x0[i], ay=y0[i], axref='x', ayref='y',
                    x=x1[i], y=y1[i], xref='x', yref='y',
                    showarrow=True, arrowhead=1, arrowwidth=arrow_widths[i]) for i in range(0, len(x0))
            ],
            yaxis = dict(
                tickmode = 'array',
                tickvals = [],
                ticktext = []
            )
        )
    ) 
    fig.show()

================
File: transcoder_circuits/feature_dashboards.py
================
# --- feature scores / feature dashboard functions --- #
import numpy as np
import tqdm
import torch
import matplotlib.pyplot as plt
from IPython.display import HTML
from transformer_lens.utils import get_act_name, to_numpy
def get_feature_scores(model, encoder, tokens_arr, feature_idx, batch_size=64, act_name='resid_pre', layer=0, use_raw_scores=False, use_decoder=False, feature_post=None, ignore_endoftext=False):
	act_name = encoder.cfg.hook_point
	layer = encoder.cfg.hook_point_layer
	scores = []
	endoftext_token = model.tokenizer.eos_token 
	for i in tqdm.tqdm(range(0, tokens_arr.shape[0], batch_size)):
		with torch.no_grad():
			_, cache = model.run_with_cache(tokens_arr[i:i+batch_size], stop_at_layer=layer+1, names_filter=[
				act_name
			])
			mlp_acts = cache[act_name]
			mlp_acts_flattened = mlp_acts.reshape(-1, encoder.W_enc.shape[0])
			if feature_post is None:
				feature_post = encoder.W_enc[:, feature_idx] if not use_decoder else encoder.W_dec[feature_idx]
			bias = -(encoder.b_dec @ feature_post) if use_decoder else encoder.b_enc[feature_idx] - (encoder.b_dec @ feature_post)
			if use_raw_scores:
				cur_scores = (mlp_acts_flattened @ feature_post) + bias
			else:
				_, hidden_acts, _, _, _, _ = encoder(mlp_acts_flattened)
				cur_scores = hidden_acts[:, feature_idx]
			if ignore_endoftext:
					cur_scores[tokens_arr[i:i+batch_size].reshape(-1) == endoftext_token] = -torch.inf
		scores.append(to_numpy(cur_scores.reshape(-1, tokens_arr.shape[1])).astype(np.float16))
	return np.concatenate(scores)
# get indices and values at uniform percentiles of arr
def sample_percentiles(arr, num_samples):
	sample_idxs = []
	sample_vals = []
	num_samples = num_samples - 1
	p_step = 100./num_samples
	for p in np.arange(0,100,p_step):
		value_at_p = np.percentile(arr.reshape(-1), p, interpolation='nearest')
		p_idx = np.abs(arr-value_at_p).argmin()
		sample_vals.append(value_at_p)
		sample_idxs.append(np.unravel_index(p_idx, arr.shape))
	# get maximum
	value_at_p = np.max(arr)
	p_idx = np.abs(arr-value_at_p).argmin()
	sample_vals.append(value_at_p)
	sample_idxs.append(np.unravel_index(p_idx, arr.shape))
	return np.array(sample_vals), np.array(sample_idxs)
# get indices and values uniformly spaced throughout arr
def sample_uniform(arr, num_samples, unique=True, use_tqdm=False, only_max_range=False):
    sample_idxs = []
    sample_vals = []
    max_val = np.max(arr)
    if not only_max_range:
        min_val = np.min(arr)
    else:
        min_val = -max_val
    num_samples = num_samples - 1
    p_step = 1./num_samples
    func = (lambda x: x) if not use_tqdm else (lambda x: tqdm.tqdm(x))
    for p in func(np.arange(0,1,p_step)):
        value_at_p = min_val + p * (max_val - min_val)
        p_idx = np.abs(arr-value_at_p).argmin()
        real_val = arr[np.unravel_index(p_idx, arr.shape)]
        sample_vals.append(real_val)
        sample_idxs.append(np.unravel_index(p_idx, arr.shape))
    # get maximum
    value_at_p = np.max(arr)
    p_idx = np.abs(arr-value_at_p).argmin()
    sample_vals.append(value_at_p)
    sample_idxs.append(np.unravel_index(p_idx, arr.shape))
    sample_vals = np.array(sample_vals)
    sample_idxs = np.array(sample_idxs)
    if unique:
        sample_vals, sample_pos = np.unique(sample_vals, return_index=True)
        sample_idxs = sample_idxs[sample_pos]
    return sample_vals, sample_idxs
import html
def make_sequence_html(token_strs, scores,
    scores_min=None,
    scores_max=None,
    max_color='#ff8c00',
    zero_color='#ffffff',
    return_head=False,
    cur_token_idx=None,
    window_size=None,
):
    if scores_min is None: scores_min = scores.min()
    if scores_max is None: scores_max = scores.max()
    scores_normalized = (scores-scores_min)/(scores_max-scores_min)
    if window_size is not None:
        left_idx = np.max([0, cur_token_idx-window_size])
        right_idx = np.min([len(scores), cur_token_idx+window_size])
        scores = scores[left_idx:right_idx]
        scores_normalized = scores_normalized[left_idx:right_idx]
        token_strs = token_strs[left_idx:right_idx]
        cur_token_idx = cur_token_idx - left_idx
    max_color_vec = np.array([int(max_color[1:3], 16), int(max_color[3:5], 16), int(max_color[5:7], 16)])
    zero_color_vec = np.array([int(zero_color[1:3], 16), int(zero_color[3:5], 16), int(zero_color[5:7], 16)])
    color_vecs = np.einsum('i, j -> ij', scores_normalized, max_color_vec) + np.einsum('i, j -> ij', 1-scores_normalized, zero_color_vec)
    color_strs = [f"#{int(x[0]):02x}{int(x[1]):02x}{int(x[2]):02x}" for x in color_vecs]
    tokens_html = "".join([
        f"""<span class='token'
            style='background-color: {color_strs[i]}'
            onMouseOver='showTooltip(this)'
            onMouseOut='hideTooltip(this)'>{"<b>" if cur_token_idx is not None and i == cur_token_idx else ""}{html.escape(token_str)}{"</b>" if cur_token_idx is not None and i == cur_token_idx else ""}<span class='feature_val'> ({scores[i]:.2f})</span></span>"""
         for i, token_str in enumerate(token_strs)
    ])
    if return_head:
        head = """
<script>
    function showTooltip(element) {
        feature_val = element.querySelector('.feature_val')
        feature_val.style.display='inline'
    }
    function hideTooltip(element) {
        feature_val = element.querySelector('.feature_val')
        feature_val.style.display='none'
    }
</script>
<style>
    span.token {
        font-family: monospace;
        border-style: solid;
        border-width: 1px;
        border-color: #dddddd;
    }
    .feature_val {
        display: none;
        font-family: serif;
    }
    #tooltip {
        display: none;
    }
</style>
"""
        return head + tokens_html
    else:
        return tokens_html
def get_uniform_band_examples(scores, uniform_vals, uniform_idxs, num_bands, band_size, return_percentages=False):
    retlist = []
    total_num_exs = num_bands*band_size
    scores_min = scores.min() 
    scores_max = scores.max()
    bandwidth = (scores_max-scores_min)/total_num_exs
    denom = 1 if not return_percentages else np.prod(scores.shape)
    for band in range(0, total_num_exs+band_size, band_size):
        #print(band)
        low_score = scores_min + band*bandwidth
        high_score = scores_min + (band+band_size)*bandwidth
        num_examples_in_band = np.sum(np.logical_and(
            scores > low_score, scores <= high_score
        ))/denom
        retlist.append((low_score, high_score, num_examples_in_band, uniform_idxs[np.logical_and(uniform_vals>=low_score, uniform_vals<=high_score)]))
    return retlist
def display_activating_examples_dash(model, all_tokens, scores,
     num_examples=50,
     num_bands=5,
     bandwidth=10,
     return_percentages=True,
     window_size=5,
     header_level=3
    ):
    if type(header_level) is int:
        header_tag = f'h{header_level}'
    else:
        header_tag = 'p'
    display(HTML(f"<{header_tag} style='font-family: serif'>Firing frequency: {100*np.sum(scores > 0)/np.prod(scores.shape):.4f}%</{header_tag}>"))
    uniform_vals, uniform_idxs = sample_uniform(scores, num_examples, unique=True)
    unif_bands = get_uniform_band_examples(scores, uniform_vals, uniform_idxs, num_bands, bandwidth, return_percentages=return_percentages)
    for band in reversed(unif_bands):
        cur_html_list = [f"<details><summary><{header_tag} style='display: inline; font-family: serif'>Between {band[0]:.2f} and {band[1]:.2f}: {100*band[2]:.4f}%</{header_tag}></summary>"]
        for example_idx, token_idx in reversed(band[3]):
            cur_html_list.append(
                make_sequence_html(
                    model.to_str_tokens(all_tokens[example_idx]), scores[example_idx],
                    scores_min=scores.min(), scores_max=scores.max(), return_head=True, cur_token_idx=token_idx, window_size=window_size
                ) + f"<span> Example {example_idx}, token {token_idx}</span>" + "<br/>"
            )
        cur_html_list.append("</details>")
        display(HTML("".join(cur_html_list)))
def get_logits_for_feature(model, sae, feature_idx, k=7):
    feature = sae.W_dec[feature_idx]
    with torch.no_grad():
        most_pos = torch.topk(feature @ model.W_U, k=k)
        most_neg = torch.topk(-feature @ model.W_U, k=k)
    top_vals = to_numpy(most_pos.values)
    top_idxs = to_numpy(most_pos.indices)
    top_tokens = model.to_str_tokens(top_idxs)
    bot_vals = to_numpy(-most_neg.values)
    bot_idxs = to_numpy(most_neg.indices)
    bot_tokens = model.to_str_tokens(bot_idxs)
    return zip(top_vals, top_tokens, bot_vals, bot_tokens)
def batch_color_interpolate(scores, max_color, zero_color, scores_min=None, scores_max=None):
    if scores_min is None: scores_min = scores.min()
    if scores_max is None: scores_max = scores.max()
    scores_normalized = (scores - scores_min) / (scores_max - scores_min)
    max_color_vec = np.array([int(max_color[1:3], 16), int(max_color[3:5], 16), int(max_color[5:7], 16)])
    zero_color_vec = np.array([int(zero_color[1:3], 16), int(zero_color[3:5], 16), int(zero_color[5:7], 16)])
    color_vecs = np.einsum('i, j -> ij', scores_normalized, max_color_vec) + np.einsum('i, j -> ij', 1-scores_normalized, zero_color_vec)
    color_strs = [f"#{int(x[0]):02x}{int(x[1]):02x}{int(x[2]):02x}" for x in color_vecs]
    return color_strs
def display_logits_for_feature(model, sae, feature_idx, k=7):
    logits = list(get_logits_for_feature(model, sae, feature_idx, k=k))
    table_html = """
<style>
    span.token {
        font-family: monospace;
        border-style: solid;
        border-width: 1px;
        border-color: #dddddd;
    }
</style>
<table>
    <thead>
        <tr>
            <th colspan=2 style='text-align:center'>Bottom logits</th>
            <th colspan=2 style='text-align:center'>Top logits</th>
        </tr>
    </thead>
    <tbody>
"""
    top_scores = np.array([x[0] for x in logits])
    bot_scores = np.array([x[2] for x in logits])
    scores_max = np.max(top_scores)
    scores_min = np.min(bot_scores)
    top_color_strs = batch_color_interpolate(top_scores, '#7f7fff', '#ffffff', scores_min=scores_min, scores_max=scores_max)
    bot_color_strs = batch_color_interpolate(-bot_scores, '#ff7f7f', '#ffffff', scores_min=scores_min, scores_max=scores_max)
    for i, (top_val, top_token, bot_val, bot_token) in enumerate(logits):
        row_html =\
f"""<tr>
    <td style='text-align:left'><span class='token' style='background-color: {bot_color_strs[i]}'>{html.escape(bot_token).replace(' ', '&nbsp;')}</span></td>
    <td style='text-align:right'>{bot_val:.3f}</td>
    <td style='text-align:left'><span class='token' style='background-color: {top_color_strs[i]}'>{html.escape(top_token).replace(' ', '&nbsp;')}</span></td>
    <td style='text-align:right'>+{top_val:.3f}</td>
</tr>"""
        table_html = table_html + row_html
    table_html = table_html + "</tbody></table>"
    display(HTML(table_html))
    all_logits = to_numpy(sae.W_dec[feature_idx] @ model.W_U)
    fig, ax = plt.subplots()
    ax.hist(all_logits[all_logits < 0], color='#ff7f7f')
    ax.hist(all_logits[all_logits > 0], color='#7f7fff')
    fig.set_size_inches(5,2)
    plt.show()
def plot_pulledback_feature(model, feature_vector, transcoder, size=None, do_plot=True,
                            input_tokens=None, input_example=None, input_token_idx=None):
    if size is None: size=(5,3)
    with torch.no_grad():
        pulledback_feature = transcoder.W_dec @ feature_vector.vector
        if input_example is not None:
            input_layer = transcoder.cfg.hook_point_layer
            if type(input_example) is int and input_tokens is not None:
                prompt = input_tokens[input_example]
            elif type(input_example) is str:
                prompt = input_example
            # TODO: add list support
            _, cache = model.run_with_cache(prompt, stop_at_layer=input_layer+1,
                names_filter=get_act_name(f'normalized{input_layer}ln2', input_layer)
            )
            feature_activs = transcoder(cache[get_act_name(f'normalized{input_layer}ln2', input_layer)])[1][0,input_token_idx]
            pulledback_feature = pulledback_feature * feature_activs
    pulledback_feature = to_numpy(pulledback_feature)
    if do_plot:
        score_max = np.max(np.abs([pulledback_feature.max(), pulledback_feature.min()]))
        score_min = -np.min(np.abs([pulledback_feature.max(), pulledback_feature.min()]))
        colors = batch_color_interpolate(pulledback_feature, '#7f7fff', '#ff7f7f')#, scores_min=score_min, scores_max=score_max)
        fig, ax = plt.subplots()
        ax.plot(pulledback_feature, alpha=0.5)
        ax.scatter(range(len(pulledback_feature)), pulledback_feature, color=colors)
        fig.set_size_inches(size[0], size[1])
        plt.xlabel("Transcoder feature index")
        plt.ylabel("Connection strength")
        if type(input_example) is int:
            plt.title(f"Example {input_example} token {input_token_idx}:\n {str(feature_vector)} from transcoder features")
        elif type(input_example) is str:
            plt.title(f"Connections on prompt:\n {str(feature_vector)} from transcoder features")
        else:
            plt.title(f"Input-independent connections:\n {str(feature_vector)} from transcoder features")
        plt.show()
    return pulledback_feature
def get_transcoder_pullback_features(model, feature_vector, transcoder, k=7, do_plot=True,
    input_tokens=None, input_example=None, input_token_idx=None
):
    pulledback_feature = plot_pulledback_feature(model, feature_vector, transcoder, do_plot=do_plot,
        input_tokens=input_tokens, input_example=input_example, input_token_idx=input_token_idx)
    pulledback_feature = torch.from_numpy(pulledback_feature)
    with torch.no_grad():
        most_pos = torch.topk(pulledback_feature, k=k)
        most_neg = torch.topk(-pulledback_feature, k=k)
    top_vals = to_numpy(most_pos.values)
    top_idxs = to_numpy(most_pos.indices)
    bot_vals = to_numpy(-most_neg.values)
    bot_idxs = to_numpy(most_neg.indices)
    return zip(top_vals, top_idxs, bot_vals, bot_idxs)
def display_transcoder_pullback_features(model, feature_vector, transcoder, k=7,
    input_tokens=None, input_example=None, input_token_idx=None
):
    logits = list(get_transcoder_pullback_features(model, feature_vector, transcoder, k=k,
        input_tokens=input_tokens, input_example=input_example, input_token_idx=input_token_idx)
    )
    table_html = """
<style>
    span.token {
        font-family: monospace;
        border-style: solid;
        border-width: 1px;
        border-color: #dddddd;
    }
</style>
<table>
    <thead>
        <tr>
            <th colspan=2 style='text-align:center'>Most-negative transcoder features</th>
            <th colspan=2 style='text-align:center'>Most-positive transcoder features</th>
        </tr>
    </thead>
    <tbody>
"""
    top_scores = np.array([x[0] for x in logits])
    bot_scores = np.array([x[2] for x in logits])
    scores_max = np.max(top_scores)
    scores_min = np.min(bot_scores)
    top_color_strs = batch_color_interpolate(top_scores, '#7f7fff', '#ffffff', scores_min=scores_min, scores_max=scores_max)
    bot_color_strs = batch_color_interpolate(-bot_scores, '#ff7f7f', '#ffffff', scores_min=scores_min, scores_max=scores_max)
    for i, (top_val, top_idx, bot_val, bot_idx) in enumerate(logits):
        row_html =\
f"""<tr>
    <td style='text-align:left'><span class='token' style='background-color: {bot_color_strs[i]}'>{bot_idx}</span></td>
    <td style='text-align:right'>{bot_val:.3f}</td>
    <td style='text-align:left'><span class='token' style='background-color: {top_color_strs[i]}'>{top_idx}</span></td>
    <td style='text-align:right'>+{top_val:.3f}</td>
</tr>"""
        table_html = table_html + row_html
    table_html = table_html + "</tbody></table>"
    display(HTML(table_html))
def get_ov_norms_for_transcoder_feature(model, transcoder, feature_idx, layer=None):
    with torch.no_grad():
        propagated_vecs = torch.einsum('lhio,o->lhi', model.OV.AB, transcoder.W_enc[:, feature_idx])
        if layer is not None:
            propagated_vecs = propagated_vecs[:layer+1]
        ov_norms = propagated_vecs.norm(dim=-1)
        fig, ax = plt.subplots()
        mat = ax.matshow(to_numpy(ov_norms), cmap='Reds', vmin=0)
        fig.colorbar(mat, location="bottom")
        ax.set_yticks(range(ov_norms.shape[0]))
        ax.set_xlabel("Attention head")
        ax.set_ylabel("Layer")
        fig.set_size_inches(5,3)
        ax.set_title(f"OV de-embedding norms for transcoder feature {feature_idx}", fontsize=10)
        plt.show()
    return ov_norms
def get_deembeddings_for_transcoder_feature(model, transcoder, feature_idx, attn_head=None, attn_layer=0, k=7):
    with torch.no_grad():
        if attn_head is not None:
            pulledback_feature = model.W_E @ model.OV.AB[attn_layer, attn_head] @ transcoder.W_enc[:, feature_idx]
        else:
            pulledback_feature = model.W_E @ transcoder.W_enc[:, feature_idx]
        if k == 0:
            return to_numpy(pulledback_feature)
        else:
            most_pos = torch.topk(pulledback_feature, k=k)
            most_neg = torch.topk(-pulledback_feature, k=k)
            top_vals = to_numpy(most_pos.values)
            top_idxs = to_numpy(most_pos.indices)
            top_tokens = model.to_str_tokens(top_idxs)
            bot_vals = to_numpy(-most_neg.values)
            bot_idxs = to_numpy(most_neg.indices)
            bot_tokens = model.to_str_tokens(bot_idxs)
            return to_numpy(pulledback_feature), zip(top_vals, top_tokens, bot_vals, bot_tokens)
def get_deembeddings_for_feature_vector(model, feature_vector, k=7):
    with torch.no_grad():
        pulledback_feature = model.W_E @ feature_vector.vector
        if k == 0:
            return to_numpy(pulledback_feature)
        else:
            most_pos = torch.topk(pulledback_feature, k=k)
            most_neg = torch.topk(-pulledback_feature, k=k)
            top_vals = to_numpy(most_pos.values)
            top_idxs = to_numpy(most_pos.indices)
            top_tokens = model.to_str_tokens(top_idxs)
            bot_vals = to_numpy(-most_neg.values)
            bot_idxs = to_numpy(most_neg.indices)
            bot_tokens = model.to_str_tokens(bot_idxs)
            return to_numpy(pulledback_feature), zip(top_vals, top_tokens, bot_vals, bot_tokens)
def plot_deembedding_for_transcoder_feature(model, transcoder, feature_idx, attn_head=None, attn_layer=0):
    pulledback_feature = get_deembeddings_for_transcoder_feature(model, transcoder, feature_idx, attn_head=attn_head, attn_layer=attn_layer, k=0)
    score_max = np.max(np.abs([pulledback_feature.max(), pulledback_feature.min()]))
    score_min = -np.min(np.abs([pulledback_feature.max(), pulledback_feature.min()]))
    colors = batch_color_interpolate(pulledback_feature, '#7f7fff', '#ff7f7f')#, scores_min=score_min, scores_max=score_max)
    fig, ax = plt.subplots()
    ax.plot(pulledback_feature, alpha=0.5)
    ax.scatter(range(len(pulledback_feature)), pulledback_feature, color=colors)
    fig.set_size_inches(4,2)
    plt.xlabel("Token index")
    plt.ylabel("Connection strength")
    plt.show()
def display_deembeddings_for_transcoder_feature(model, transcoder, feature_idx, attn_head=None, attn_layer=0, k=7):
    pulledback_feature, deembeddings = get_deembeddings_for_transcoder_feature(model, transcoder, feature_idx, attn_head=attn_head, attn_layer=attn_layer, k=k)
    deembeddings = list(deembeddings)
    table_html = """
<style>
    span.token {
        font-family: monospace;
        border-style: solid;
        border-width: 1px;
        border-color: #dddddd;
    }
</style>"""f"""
<b>{"Direct path" if attn_head is None else f"Attention head {attn_head}"}</b>
<table>
    <thead>
        <tr>
            <th colspan=2 style='text-align:center'>Most-negative de-embedding tokens</th>
            <th colspan=2 style='text-align:center'>Most-positive de-embedding tokens</th>
        </tr>
    </thead>
    <tbody>
"""
    top_scores = np.array([x[0] for x in deembeddings])
    bot_scores = np.array([x[2] for x in deembeddings])
    scores_max = np.max(top_scores)
    scores_min = np.min(bot_scores)
    top_color_strs = batch_color_interpolate(top_scores, '#7f7fff', '#ffffff', scores_min=scores_min, scores_max=scores_max)
    bot_color_strs = batch_color_interpolate(-bot_scores, '#ff7f7f', '#ffffff', scores_min=scores_min, scores_max=scores_max)
    for i, (top_val, top_token, bot_val, bot_token) in enumerate(deembeddings):
        row_html =\
f"""<tr>
    <td style='text-align:left'><span class='token' style='background-color: {bot_color_strs[i]}'>{html.escape(bot_token).replace(" ", "&nbsp;")}</span></td>
    <td style='text-align:right'>{bot_val:.3f}</td>
    <td style='text-align:left'><span class='token' style='background-color: {top_color_strs[i]}'>{html.escape(top_token).replace(" ", "&nbsp;")}</span></td>
    <td style='text-align:right'>+{top_val:.3f}</td>
</tr>"""
        table_html = table_html + row_html
    table_html = table_html + "</tbody></table>"
    display(HTML(table_html))
def display_deembeddings_for_feature_vector(model, feature_vector, k=7):
    pulledback_feature, deembeddings = get_deembeddings_for_feature_vector(model, feature_vector, k=k)
    deembeddings = list(deembeddings)
    table_html = """
<style>
    span.token {
        font-family: monospace;
        border-style: solid;
        border-width: 1px;
        border-color: #dddddd;
    }
</style>"""f"""
<b>{str(feature_vector)}</b>
<table>
    <thead>
        <tr>
            <th colspan=2 style='text-align:center'>Most-negative de-embedding tokens</th>
            <th colspan=2 style='text-align:center'>Most-positive de-embedding tokens</th>
        </tr>
    </thead>
    <tbody>
"""
    top_scores = np.array([x[0] for x in deembeddings])
    bot_scores = np.array([x[2] for x in deembeddings])
    scores_max = np.max(top_scores)
    scores_min = np.min(bot_scores)
    top_color_strs = batch_color_interpolate(top_scores, '#7f7fff', '#ffffff', scores_min=scores_min, scores_max=scores_max)
    bot_color_strs = batch_color_interpolate(-bot_scores, '#ff7f7f', '#ffffff', scores_min=scores_min, scores_max=scores_max)
    for i, (top_val, top_token, bot_val, bot_token) in enumerate(deembeddings):
        row_html =\
f"""<tr>
    <td style='text-align:left'><span class='token' style='background-color: {bot_color_strs[i]}'>{html.escape(bot_token).replace(" ", "&nbsp;")}</span></td>
    <td style='text-align:right'>{bot_val:.3f}</td>
    <td style='text-align:left'><span class='token' style='background-color: {top_color_strs[i]}'>{html.escape(top_token).replace(" ", "&nbsp;")}</span></td>
    <td style='text-align:right'>+{top_val:.3f}</td>
</tr>"""
        table_html = table_html + row_html
    table_html = table_html + "</tbody></table>"
    display(HTML(table_html))
def display_analysis_for_transcoder_feature(model, transcoder, feature_idx, attn_k=2, k=8, layer=None):
    display(HTML(f"<h3>Transcoder feature {feature_idx}</h3>"))
    display_deembeddings_for_transcoder_feature(model, transcoder, feature_idx, k=k)
    plot_deembedding_for_transcoder_feature(model, transcoder, feature_idx)
    display(HTML(f"<h4>OV circuits</h4>"))
    ov_norms = get_ov_norms_for_transcoder_feature(model, transcoder, feature_idx, layer=layer)
    top_ov_norms, top_ov_heads_flattened = torch.topk(ov_norms.flatten(), k=attn_k)
    top_ov_indices = np.array(np.unravel_index(to_numpy(top_ov_heads_flattened), ov_norms.shape)).T        
    colors = batch_color_interpolate(to_numpy(top_ov_norms), '#ff7f7f', '#ffffff', scores_min=0, scores_max=top_ov_norms.max().item())
    table_html = f"""<h4>Top {top_ov_indices.shape[0]} attention heads by OV circuit norm</h4>
    <table>
        <thead>
            <tr>
                <th style='text-align:center'>Layer</th>
                <th style='text-align:center'>Head</th>
                <th style='text-align:center'>Norm</th>
            </tr>
        </thead>
        <tbody>
            { "".join(f"<tr style='background-color: {color}'><td>{layer}</td><td>{head}</td><td>{norm:.2f}</td>" for (layer, head), norm, color in zip(top_ov_indices, top_ov_norms, colors))}
        </tbody>
    """
    display(HTML(table_html))
    for cur_layer, head in top_ov_indices:
        display_deembeddings_for_transcoder_feature(model, transcoder, feature_idx, attn_head=head, attn_layer=cur_layer)
        plot_deembedding_for_transcoder_feature(model, transcoder, feature_idx, attn_head=head, attn_layer=cur_layer)

================
File: transcoder_circuits/replacement_ctx.py
================
# --- context manager for replacing MLP sublayers with transcoders ---
import torch
class TranscoderWrapper(torch.nn.Module):
    def __init__(self, transcoder):
        super().__init__()
        self.transcoder = transcoder
    def forward(self, x):
        return self.transcoder(x)[0]
class TranscoderReplacementContext:
    def __init__(self, model, transcoders):
        self.layers = [t.cfg.hook_point_layer for t in transcoders]
        self.original_mlps = [ model.blocks[i].mlp for i in self.layers ]
        self.transcoders = transcoders
        #self.layers = layers
        self.model = model
    def __enter__(self):
        for transcoder in self.transcoders:
           self.model.blocks[transcoder.cfg.hook_point_layer].mlp = TranscoderWrapper(transcoder)
    def __exit__(self, exc_type, exc_value, exc_tb):
        for layer, mlp in zip(self.layers, self.original_mlps):
            self.model.blocks[layer].mlp = mlp
class ZeroAblationWrapper(torch.nn.Module):
    def __init__(self):
        super().__init__()
    def forward(self, x):
        return x*0.0
class ZeroAblationContext:
    def __init__(self, model, layers):
        self.original_mlps = [ model.blocks[i].mlp for i in layers ]
        self.layers = layers
        self.model = model
    def __enter__(self):
        for layer in self.layers:
           self.model.blocks[layer].mlp = ZeroAblationWrapper()
    def __exit__(self, exc_type, exc_value, exc_tb):
        for layer, mlp in zip(self.layers, self.original_mlps):
            self.model.blocks[layer].mlp = mlp



================================================================
End of Codebase
================================================================
