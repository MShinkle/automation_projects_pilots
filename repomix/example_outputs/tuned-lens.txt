This file is a merged representation of a subset of the codebase, containing specifically included files and files not matching ignore patterns, combined into a single document by Repomix.
The content has been processed where empty lines have been removed.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
- Pay special attention to the Repository Description. These contain important context and guidelines specific to this project.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: **/*.py, **/*.md, **/*.txt
- Files matching these patterns are excluded: **/.git/**, **/.github/**, CHANGELOG.md
- Empty lines have been removed from all files
- Files are sorted by Git change count (files with more changes are at the bottom)

Additional Info:
----------------
User Provided Header:
-----------------------
This file is a consolidated single-file compilation of all code in the repository generated by Repomix. Note that .ipynb files have been converted to .py files.

================================================================
Directory Structure
================================================================
docs/source/conf.py
README.md
tests/conftest.py
tests/plotting/test_prediction_trajectory.py
tests/plotting/test_token_formatter.py
tests/plotting/test_trajectory_plotting.py
tests/scripts/test_integration.py
tests/test_data.py
tests/test_distance.py
tests/test_lenses.py
tests/test_load_artifact.py
tests/test_model_surgery.py
tests/test_stats.py
tests/test_subspaces.py
tests/test_unembed.py
tests/test_utils.py
tuned_lens/__init__.py
tuned_lens/__main__.py
tuned_lens/causal/__init__.py
tuned_lens/causal/ablation.py
tuned_lens/causal/subspaces.py
tuned_lens/causal/utils.py
tuned_lens/data.py
tuned_lens/load_artifacts.py
tuned_lens/model_surgery.py
tuned_lens/nn/__init__.py
tuned_lens/nn/lenses.py
tuned_lens/nn/unembed.py
tuned_lens/plotting/__init__.py
tuned_lens/plotting/prediction_trajectory.py
tuned_lens/plotting/token_formatter.py
tuned_lens/plotting/trajectory_plotting.py
tuned_lens/scripts/__init__.py
tuned_lens/scripts/eval_loop.py
tuned_lens/scripts/ingredients.py
tuned_lens/scripts/train_loop.py
tuned_lens/stats/__init__.py
tuned_lens/stats/distance.py
tuned_lens/stats/logit_stats.py
tuned_lens/utils.py

================================================================
Files
================================================================

================
File: docs/source/conf.py
================
# Configuration file for the Sphinx documentation builder.
#
# For the full list of built-in configuration values, see the documentation:
# https://www.sphinx-doc.org/en/master/usage/configuration.html
# -- Project information -----------------------------------------------------
# https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information
from importlib import metadata
project = "tuned-lens"
copyright = "2023, FAR AI"
html_title = "Tuned Lens"
html_favicon = (
    "data:image/svg+xml,"
    "<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22>"
    "<text y=%22.9em%22 font-size=%2290%22>ðŸ”Ž</text>"
    "</svg>"
)
author = (
    "Nora Belrose"
    " Zach Furman,"
    " Logan Smith,"
    " Danny Halawi,"
    " Lev McKinney,"
    " Igor Ostrovsky,"
    " Stella Biderman,"
    " Jacob Steinhardt"
)
release = metadata.version("tuned_lens")
extensions = [
    "sphinx.ext.napoleon",
    "sphinx.ext.autodoc",
    "sphinx.ext.autosummary",
    "sphinx.ext.autosectionlabel",
    "sphinx_autodoc_typehints",
    "sphinx.ext.doctest",
    "myst_parser",
    "nbsphinx",
]
napoleon_google_docstring = True
napoleon_use_param = False
napoleon_use_ivar = True
templates_path = ["_templates"]
exclude_patterns = ["build", "Thumbs.db", ".DS_Store", "**.ipynb_checkpoints"]
html_theme = "furo"
html_static_path = ["_static"]
html_theme_options = {
    "source_repository": "https://github.com/AlignmentResearch/tuned-lens",
    "source_branch": "main",
    "source_directory": "docs/source",
    "light_css_variables": {
        "sidebar-item-font-size": "85%",
    },
}

================
File: README.md
================
# Tuned Lens ðŸ”Ž
<a target="_blank" href="https://colab.research.google.com/github/AlignmentResearch/tuned-lens/blob/main/notebooks/interactive.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a>
<a target="_blank" href="https://huggingface.co/spaces/AlignmentResearch/tuned-lens">
<img src="https://huggingface.co/datasets/huggingface/badges/resolve/main/open-in-hf-spaces-sm-dark.svg", alt="Open in Spaces">
</a>


Tools for understanding how transformer predictions are built layer-by-layer.

<img src=https://user-images.githubusercontent.com/12176390/224879115-8bc95f26-68e4-4f43-9b4c-06ca5934a29d.png>

This package provides a simple interface for training and evaluating __tuned lenses__. A tuned lens allows us to peek at the iterative computations a transformer uses to compute the next token.


## What is a Lens?
<img alt="A diagram showing how a translator within the lens allows you to skip intermediate layers." src="https://user-images.githubusercontent.com/12176390/227057947-1ef56811-f91f-48ff-8d2d-ff04cc599125.png"  width=400/>

A lens into a transformer with _n_ layers allows you to replace the last _m_ layers of the model with an [affine transformation](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) (we call these affine translators). Each affine translator is trained to minimize the KL divergence between its prediction and the final output distribution of the original model. This means that after training, the tuned lens allows you to skip over these last few layers and see the best prediction that can be made from the model's intermediate representations, i.e., the residual stream, at layer _n - m_.

The reason we need to train an affine translator is that the representations may be rotated, shifted, or stretched from layer to layer. This training differentiates this method from simpler approaches that unembed the residual stream of the network directly using the unembedding matrix, i.e., the [logit lens](https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens). We explain this process and its applications in the paper [Eliciting Latent Predictions from Transformers with the Tuned Lens](https://arxiv.org/abs/2303.08112).

### Acknowledgments
Originally conceived by [Igor Ostrovsky](https://twitter.com/igoro) and [Stella Biderman](https://www.stellabiderman.com/) at [EleutherAI](https://www.eleuther.ai/), this library was built as a collaboration between FAR and EleutherAI researchers.

## Install Instructions
### Installing from PyPI
First, you will need to install the basic prerequisites into a virtual environment:
* Python 3.9+
* PyTorch 1.13.0+

Then, you can simply install the package using pip.
```
pip install tuned-lens
```

### Installing the container
If you prefer to run the training scripts from within a container, you can use the provided Docker container.

```
docker pull ghcr.io/alignmentresearch/tuned-lens:latest
docker run --rm tuned-lens:latest tuned-lens --help
```

## Contributing
Make sure to install the dev dependencies and install the pre-commit hooks.
```
$ git clone https://github.com/AlignmentResearch/tuned-lens.git
$ pip install -e ".[dev]"
$ pre-commit install
```

## Citation

If you find this library useful, please cite it as:

```bibtex
@article{belrose2023eliciting,
  title={Eliciting Latent Predictions from Transformers with the Tuned Lens},
  authors={Belrose, Nora and Furman, Zach and Smith, Logan and Halawi, Danny and McKinney, Lev and Ostrovsky, Igor and Biderman, Stella and Steinhardt, Jacob},
  journal={to appear},
  year={2023}
}
```

> **Warning**
> This package has not reached 1.0. Expect the public interface to change regularly and without a major version bumps.

================
File: tests/conftest.py
================
from pathlib import Path
import pytest
import torch as th
import transformers as tr
from datasets import Dataset
@pytest.fixture(scope="module")
def text_dataset_path() -> Path:
    dir_path = Path(__file__).parent.absolute()
    return Path(dir_path, "test_data", "pile_text.jsonl")
@pytest.fixture(scope="module")
def text_dataset(text_dataset_path: Path) -> Dataset:
    dataset = Dataset.from_json(str(text_dataset_path))
    assert isinstance(dataset, Dataset)
    return dataset
@pytest.fixture(
    scope="module",
    params=[
        "EleutherAI/pythia-70m-deduped",
        "bigscience/bloom-560m",
        "EleutherAI/gpt-neo-125M",
        "facebook/opt-125m",
        "mockmodel/llama-tiny",
        "mockmodel/gemma-tiny",
        "gpt2",
    ],
)
def random_small_model(request: str) -> tr.PreTrainedModel:
    small_model_name = request.param
    th.manual_seed(42)
    # We use a random model with the correct config instead of downloading the
    # whole pretrained checkpoint.
    if small_model_name == "mockmodel/llama-tiny":
        config = tr.LlamaConfig(
            vocab_size=32_000,
            hidden_size=128,
            num_hidden_layers=4,
            num_attention_heads=4,
        )
    elif small_model_name == "mockmodel/gemma-tiny":
        config = tr.GemmaConfig(
            vocab_size=32_000,
            hidden_size=128,
            num_hidden_layers=4,
            num_attention_heads=4,
            num_key_value_heads=4,
            head_dim=32,
        )
    else:
        config = tr.AutoConfig.from_pretrained(small_model_name)
    model = tr.AutoModelForCausalLM.from_config(config)
    model.eval()
    return model
@pytest.fixture(
    scope="module",
    params=[
        "EleutherAI/pythia-70m-deduped",
        "bigscience/bloom-560m",
        "EleutherAI/gpt-neo-125M",
        "facebook/opt-125m",
        "gpt2",
    ],
)
def small_model_tokenizer(request: str) -> tr.PreTrainedTokenizerBase:
    return tr.AutoTokenizer.from_pretrained(request.param, use_fast=True)
@pytest.fixture(scope="module")
def gpt2_tokenizer():
    return tr.AutoTokenizer.from_pretrained("gpt2", use_fast=True)
@pytest.fixture(scope="module")
def opt_random_model() -> tr.PreTrainedModel:
    config = tr.AutoConfig.from_pretrained("facebook/opt-125m")
    model = tr.AutoModelForCausalLM.from_config(config)
    model.eval()
    return model
@pytest.fixture(scope="module")
def gpt2_tiny_random_model_local_path(
    tmpdir_factory, gpt2_tokenizer: tr.PreTrainedTokenizerBase
):
    config = tr.AutoConfig.from_pretrained("gpt2")
    config.n_heads = 2
    config.n_embed = 8
    config.n_layers = 2
    model = tr.AutoModelForCausalLM.from_config(config)
    assert isinstance(model, tr.PreTrainedModel)
    tmp_path = tmpdir_factory.mktemp("gpt2_random_model_local")
    model.save_pretrained(tmp_path)
    gpt2_tokenizer.save_pretrained(tmp_path)
    return tmp_path

================
File: tests/plotting/test_prediction_trajectory.py
================
import numpy as np
import pytest
import torch as th
import transformer_lens as tl
from transformers import AutoModelForCausalLM, AutoTokenizer
from tuned_lens.nn.lenses import LogitLens, TunedLens, Unembed
from tuned_lens.plotting import PredictionTrajectory
from tuned_lens.plotting.prediction_trajectory import _select_values_along_seq_axis
@pytest.fixture(params=[(), (2, 2), (1,)])
def prediction_trajectory(request):
    batch_shape = request.param
    layers = 3
    num_tokens = 10
    vocab_size = 12
    return PredictionTrajectory(
        log_probs=np.log(
            np.ones(batch_shape + (layers, num_tokens, vocab_size), dtype=np.float32)
            / vocab_size
        ),
        input_ids=np.ones(batch_shape + (num_tokens,), dtype=np.int64),
        targets=np.ones(batch_shape + (num_tokens,), dtype=np.int64),
    )
@pytest.fixture
def prediction_trajectory_with_tok(prediction_trajectory: PredictionTrajectory):
    tokenizer = AutoTokenizer.from_pretrained("EleutherAI/pythia-70m-deduped")
    prediction_trajectory.tokenizer = tokenizer
    return prediction_trajectory
@pytest.fixture
def hooked_transformer():
    return tl.HookedTransformer.from_pretrained(
        "EleutherAI/pythia-70m-deduped", device="cpu"
    )
@pytest.fixture
def model_and_tokenizer():
    model_name = "EleutherAI/pythia-70m-deduped"
    model = AutoModelForCausalLM.from_pretrained(model_name)
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    return model, tokenizer
@pytest.fixture
def lens(model_and_tokenizer):
    model, _ = model_and_tokenizer
    return LogitLens.from_model(model)
def test_select_values():
    log_probs = np.array(
        [[[[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]], [[0.7, 0.8, 0.9], [1.0, 1.1, 1.2]]]]
    )
    targets = np.array([[1, 2]])
    result = _select_values_along_seq_axis(log_probs, targets)
    expected_result = np.array([[[0.2, 0.6], [0.8, 1.2]]])
    np.testing.assert_almost_equal(result, expected_result)
    assert result.shape == (1, 2, 2)
def test_prediction_trajectory_from_lens_and_model_smoke(model_and_tokenizer, lens):
    model, tokenizer = model_and_tokenizer
    input_ids = tokenizer.encode("Hello world!")
    traj = PredictionTrajectory.from_lens_and_model(
        lens, model, input_ids, tokenizer=tokenizer
    )
    assert traj.num_layers == model.config.num_hidden_layers
    assert traj.num_tokens == len(input_ids)
    assert traj.vocab_size == model.config.vocab_size
def test_prediction_trajectory_from_cache_no_batch(hooked_transformer):
    lens = TunedLens.from_unembed_and_pretrained(
        unembed=Unembed(hooked_transformer),
        lens_resource_id="EleutherAI/pythia-70m-deduped",
        map_location="cpu",
    )
    input_ids = th.tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])
    targets = input_ids.clone()
    with th.inference_mode():
        logits, cache = hooked_transformer.run_with_cache(
            input=input_ids, return_type="logits"
        )
        assert isinstance(logits, th.Tensor)
        PredictionTrajectory.from_lens_and_cache(
            lens=lens,
            cache=cache,
            model_logits=logits,
            input_ids=input_ids,
            targets=targets,
        )
def test_get_sequence_labels_smoke(
    prediction_trajectory_with_tok: PredictionTrajectory,
):
    labels = prediction_trajectory_with_tok._get_sequence_labels()
    assert labels.shape == (10,)
def test_largest_prob_labels_smoke(
    prediction_trajectory_with_tok: PredictionTrajectory,
):
    labels = prediction_trajectory_with_tok._largest_prob_labels(min_prob=0.1, topk=5)
    assert labels is not None
    assert labels.label_strings.shape == (3, 10)
    assert labels.hover_over_entries is not None
    assert labels.hover_over_entries.shape[:-1] == (3, 10, 5)
def test_largest_delta_in_prob_labels_smoke(
    prediction_trajectory_with_tok: PredictionTrajectory,
):
    other = prediction_trajectory_with_tok
    labels = prediction_trajectory_with_tok._largest_delta_in_prob_labels(
        other, min_prob_delta=0.1, topk=5
    )
    assert labels is not None
    assert labels.label_strings.shape == (3, 10)
    assert labels.hover_over_entries is not None
    assert labels.hover_over_entries.shape[:-1] == (3, 10, 5)
def test_cross_entropy_smoke(prediction_trajectory: PredictionTrajectory):
    traj = prediction_trajectory
    ce_stat = traj.cross_entropy()
    assert ce_stat.name == "Cross Entropy"
    assert ce_stat.units == "nats"
    assert ce_stat.trajectory_labels is None
    assert ce_stat.sequence_labels is None
    assert ce_stat.stats.shape == (3, 10)
def test_rank_smoke(prediction_trajectory: PredictionTrajectory):
    traj = prediction_trajectory
    rank_stat = traj.rank(show_ranks=True)
    assert rank_stat.name == "Rank"
    assert rank_stat.units == ""
    assert rank_stat.trajectory_labels is None
    assert rank_stat.sequence_labels is None
    assert rank_stat.stats.shape == (3, 10)
def test_rank_correctness():
    # Test that the rank is correct.
    log_probs = np.array(
        [[[[0.1, 0.2, 0.3], [0.6, 0.5, 0.4]], [[0.85, 0.8, 0.9], [1.0, 1.1, 1.2]]]]
    )
    assert log_probs.shape == (1, 2, 2, 3)  # (batch, layer, seq, vocab)
    traj = PredictionTrajectory(
        log_probs=log_probs,
        input_ids=np.ones((1, 2), dtype=np.int64),
        targets=np.array([[0, 1]], dtype=np.int64),
    )
    rank_stat = traj.rank(show_ranks=True)
    assert rank_stat.stats.shape == (2, 2)
    assert rank_stat.stats[0, 0] == 2 + 1
    assert rank_stat.stats[0, 1] == 1 + 1
    assert rank_stat.stats[1, 0] == 1 + 1
    assert rank_stat.stats[1, 1] == 1 + 1
def test_entropy_smoke(prediction_trajectory: PredictionTrajectory):
    traj = prediction_trajectory
    entropy_stat = traj.entropy()
    assert entropy_stat.name == "Entropy"
    assert entropy_stat.units == "nats"
    assert entropy_stat.trajectory_labels is None
    assert entropy_stat.sequence_labels is None
    assert entropy_stat.stats.shape == (3, 10)
def test_forward_kl_smoke(prediction_trajectory: PredictionTrajectory):
    traj = prediction_trajectory
    forward_kl_stat = traj.forward_kl()
    assert forward_kl_stat.name == "Forward KL"
    assert forward_kl_stat.units == "nats"
    assert forward_kl_stat.sequence_labels is None
    assert forward_kl_stat.trajectory_labels is None
    assert forward_kl_stat.stats.shape == (3, 10)
def test_max_probability_smoke(prediction_trajectory: PredictionTrajectory):
    traj = prediction_trajectory
    max_probability_stat = traj.max_probability()
    assert max_probability_stat.name == "Max Probability"
    assert max_probability_stat.units == "probs"
    assert max_probability_stat.sequence_labels is None
    assert max_probability_stat.trajectory_labels is None
    assert max_probability_stat.stats.shape == (3, 10)
def test_kl_divergence_smoke(
    prediction_trajectory: PredictionTrajectory,
):
    traj = prediction_trajectory
    other = prediction_trajectory
    kl_stat = traj.kl_divergence(other)
    assert kl_stat.name == "KL(Self | Other)"
    assert kl_stat.units == "nats"
    assert kl_stat.sequence_labels is None
    assert kl_stat.trajectory_labels is None
    assert kl_stat.stats.shape == (3, 10)
    assert np.isclose(kl_stat.stats, 0.0).all()
def test_js_divergence_smoke(
    prediction_trajectory: PredictionTrajectory,
):
    traj = prediction_trajectory
    other = prediction_trajectory
    js_stat = traj.js_divergence(other)
    assert js_stat.name == "JS(Self | Other)"
    assert js_stat.units == "nats"
    assert js_stat.sequence_labels is None
    assert js_stat.trajectory_labels is None
    assert js_stat.stats.shape == (3, 10)
    assert np.isclose(js_stat.stats, 0.0).all()
def test_total_variation_smoke(
    prediction_trajectory: PredictionTrajectory,
):
    traj = prediction_trajectory
    other = prediction_trajectory
    js_stat = traj.total_variation(other)
    assert js_stat.name == "TV(Self | Other)"
    assert js_stat.units == "probs"
    assert js_stat.sequence_labels is None
    assert js_stat.trajectory_labels is None
    assert js_stat.stats.shape == (3, 10)
    assert np.isclose(js_stat.stats, 0.0).all()

================
File: tests/plotting/test_token_formatter.py
================
import pytest
from tuned_lens.plotting import TokenFormatter
@pytest.fixture
def formatter():
    return TokenFormatter()
def test_format_non_string(formatter):
    assert formatter.format(None) == "<unk>"
    assert formatter.format(123) == "<unk>"
def test_format_ellipsis(formatter):
    token = "ThisIsALongToken"
    expected = "ThisIsâ€¦"
    assert formatter.format(token) == expected
def test_format_no_ellipsis(formatter):
    formatter.max_string_len = None
    token = "ThisIsALongToken"
    expected = "ThisIsALongToken"
    assert formatter.format(token) == expected
def test_format_newline_token_replacement(formatter):
    formatter.max_string_len = None
    token = "HelloÄŠWorld"
    expected = "Hello\\nWorld"
    assert formatter.format(token) == expected
def test_format_whitespace_token_replacement(formatter):
    formatter.max_string_len = None
    token = "HelloÄ World"
    expected = "Hello_World"
    assert formatter.format(token) == expected
def test_format_multiple_replacements(formatter):
    formatter.max_string_len = None
    token = "Line1ÄŠLine2Ä Line3"
    expected = "Line1\\nLine2_Line3"
    assert formatter.format(token) == expected

================
File: tests/plotting/test_trajectory_plotting.py
================
import numpy as np
import pytest
from plotly import graph_objects as go
from tuned_lens.plotting.trajectory_plotting import (
    TrajectoryLabels,
    TrajectoryStatistic,
    _stride_keep_last,
)
def test_stride_keep_last():
    x = np.array([1, 2, 3, 4, 5])
    assert np.array_equal(_stride_keep_last(x, 1), x)
    assert np.array_equal(_stride_keep_last(x, 2), np.array([1, 3, 5]))
    assert np.array_equal(_stride_keep_last(x, 3), np.array([1, 4, 5]))
    assert np.array_equal(_stride_keep_last(x, 4), np.array([1, 5]))
    assert np.array_equal(_stride_keep_last(x, 5), np.array([1, 5]))
def test_trajectory_statistic_post_init():
    stats = np.zeros((2, 2), dtype=float)
    trajectory_labels = TrajectoryLabels(
        label_strings=np.zeros((2, 2), dtype=np.str_),
    )
    with pytest.raises(AssertionError):
        TrajectoryStatistic(
            name="test",
            stats=np.zeros((2, 3), dtype=float),
            trajectory_labels=trajectory_labels,
        )
    stats = np.zeros((3, 3), dtype=float)
    trajectory_labels = TrajectoryLabels(
        label_strings=np.zeros((3, 3), dtype=np.str_),
    )
    ts = TrajectoryStatistic(
        "test",
        stats,
        trajectory_labels=trajectory_labels,
    )
    assert ts is not None
    assert ts._layer_labels is not None
    assert np.array_equal(ts._layer_labels, np.array(["0", "1", "output"]))
    ts = TrajectoryStatistic(
        "test",
        stats,
        trajectory_labels=trajectory_labels,
        includes_output=False,
    )
    assert ts is not None
    assert ts._layer_labels is not None
    assert np.array_equal(ts._layer_labels, np.array(["0", "1", "2"]))
def test_template_and_customdata():
    n_layers = 2
    seq_len = 1
    trajectory_labels = TrajectoryLabels(
        label_strings=np.zeros((n_layers, seq_len), dtype=np.str_),
        hover_over_entries=np.array(
            [
                [
                    [["ab", "c"], ["de", "f"]],
                ],
                [
                    [["idk", "l"], ["mno", "p"]],
                ],
            ],
            dtype=np.str_,
        ),
    )
    template, customdata = trajectory_labels.template_and_customdata(col_width_limit=10)
    assert template == (
        "%{customdata[0]}%{customdata[1]}"
        "<br>%{customdata[2]}%{customdata[3]}<br>"
        "<extra></extra>"
    )
    assert (
        customdata
        == np.array([[["  ab", " c", "  de", " f"]], [[" idk", " l", " mno", " p"]]])
    ).all()
def test_trajectory_statistic_heatmap():
    stats = np.zeros((2, 2), dtype=float)
    ts = TrajectoryStatistic("test", stats)
    heatmap = ts.heatmap()
    assert isinstance(heatmap, go.Heatmap)
def test_trajectory_statistic_figure():
    stats = np.zeros((2, 2), dtype=float)
    ts = TrajectoryStatistic("test", stats)
    figure = ts.figure()
    assert isinstance(figure, go.Figure)
def test_stride_method():
    stats = np.zeros((3, 2), dtype=float)
    ts = TrajectoryStatistic("test", stats)
    stride = 2
    stride_ts = ts.stride(stride)
    assert stride_ts is not None
    assert stride_ts.name == ts.name
    assert stride_ts.units == ts.units
    assert stride_ts.max == ts.max
    assert stride_ts.min == ts.min
    assert stride_ts.includes_output == ts.includes_output
    assert stride_ts.stats is not None
    assert stride_ts.stats.shape == (2, 2)
    assert stride_ts._layer_labels is not None
    assert np.array_equal(stride_ts._layer_labels, np.array(["0", "output"]))

================
File: tests/scripts/test_integration.py
================
import warnings
from pathlib import Path
import pytest
from tuned_lens.__main__ import main
def test_eval_subcommand(
    text_dataset_path: Path, gpt2_tiny_random_model_local_path: Path, tmp_path: Path
):
    # Note we do not specify a lens here, so we are using the logit lens
    args = (
        # Using a very small test dataset here to speed up the test
        f"--log_level DEBUG eval --data.name {text_dataset_path}"
        f" --model.name {gpt2_tiny_random_model_local_path}"
        " --record_logit_stats"
        # Since the test dataset is so small we will need to use a small number of
        # tokens per sequence and request a small number of tokens to train on.
        " --max_seq_len 128"
        " --tokens 4000"
        " --logit"
        f" --output {tmp_path}"
    )
    args = args.split()
    with warnings.catch_warnings():
        warnings.simplefilter("error")
        main(args)
def test_eval_subcommand_fails_when_not_enough_data_given(
    text_dataset_path: Path, gpt2_tiny_random_model_local_path: Path, tmp_path: Path
):
    args = (
        f"--log_level DEBUG eval --data.name {text_dataset_path}"
        f" --model.name {gpt2_tiny_random_model_local_path}"
        " --record_logit_stats"
        " --max_seq_len 128"
        " --tokens 100000"
        " --logit"
        f" --output {tmp_path}"
    )
    args = args.split()
    with pytest.raises(ValueError, match="Requested to evaluate on"):
        main(args)
def test_train_subcommand(
    text_dataset_path: Path, gpt2_tiny_random_model_local_path: Path, tmp_path: Path
):
    args = (
        f"--log_level DEBUG train --data.name {text_dataset_path}"
        f" --model.name {gpt2_tiny_random_model_local_path}"
        # Again, since the test dataset is so small we will need to use a small number
        # of tokens per sequence and request a small number of tokens to train on.
        " --max_seq_len 128"
        " --tokens_per_step 256"
        " --num_steps 4"
        " --checkpoint_freq 2"
        f" --output {tmp_path}"
    )
    args = args.split()
    with warnings.catch_warnings():
        warnings.simplefilter("error")
        main(args)
    assert Path(tmp_path, "checkpoints/snapshot_2.pth").exists()
    assert Path(tmp_path, "config.json").exists()
    assert Path(tmp_path, "params.pt").exists()
def test_train_subcommand_fails_when_not_enough_data_given(
    text_dataset_path: Path, gpt2_tiny_random_model_local_path: Path, tmp_path: Path
):
    args = (
        f"--log_level DEBUG train --data.name {text_dataset_path}"
        f" --model.name {gpt2_tiny_random_model_local_path}"
        " --max_seq_len 128"
        " --tokens_per_step 256"
        " --num_steps 100000"  # This number of steps should not be feasible with
        # the dataset we are using
        " --checkpoint_freq 2"
        f" --output {tmp_path}"
    )
    args = args.split()
    with pytest.raises(ValueError, match="Can only take"):
        main(args)

================
File: tests/test_data.py
================
import math
import transformers as tr
from datasets import Dataset
from tuned_lens import data
def test_chunk_and_tokenize(
    text_dataset: Dataset, small_model_tokenizer: tr.PreTrainedTokenizerBase
):
    max_seq_len = 128
    chunked, _ = data.chunk_and_tokenize(
        text_dataset,
        small_model_tokenizer,
        load_from_cache_file=False,
        max_seq_len=max_seq_len,
    )
    length = min(small_model_tokenizer.model_max_length, max_seq_len)
    for i in range(len(chunked)):
        assert len(chunked[i]["input_ids"]) == length
def test_chunk_and_tokenize_slow(text_dataset: Dataset):
    tokenizer = tr.AutoTokenizer.from_pretrained("gpt2", use_fast=False)
    max_seq_len = 128
    chunked, _ = data.chunk_and_tokenize(
        text_dataset,
        tokenizer,
        load_from_cache_file=False,
        max_seq_len=max_seq_len,
    )
    length = min(tokenizer.model_max_length, max_seq_len)
    for i in range(len(chunked)):
        assert len(chunked[i]["input_ids"]) == length
def test_compute_nats_to_bpb_ratio(
    text_dataset: Dataset, gpt2_tokenizer: tr.PreTrainedTokenizerBase
):
    max_seq_len = 128
    _, ratio = data.chunk_and_tokenize(
        text_dataset, gpt2_tokenizer, load_from_cache_file=True, max_seq_len=max_seq_len
    )
    # We expect the ratio to be around 0.29, see https://arxiv.org/pdf/2101.00027.pdf,
    # section 3.1
    assert 0.2 / math.log(2) < ratio < 0.4 / math.log(2)

================
File: tests/test_distance.py
================
import torch as th
from torch.distributions import Categorical, kl_divergence
from tuned_lens.stats import js_distance, js_divergence
def test_js_divergence():
    p = Categorical(logits=th.randn(10))
    q = Categorical(logits=th.randn(10))
    m = Categorical(probs=0.5 * (p.probs + q.probs))  # type: ignore
    kl_fwd = kl_divergence(p, m)
    kl_bwd = kl_divergence(q, m)
    gt_js = 0.5 * (kl_fwd + kl_bwd)
    our_js_fwd = js_divergence(p.logits, q.logits)  # type: ignore
    our_js_bwd = js_divergence(q.logits, p.logits)  # type: ignore
    th.testing.assert_close(gt_js, our_js_fwd)
    th.testing.assert_close(our_js_fwd, our_js_bwd)  # Symmetry
def test_js_distance():
    a = th.randn(1000, 3)
    b = th.randn(1000, 3)
    c = th.randn(1000, 3)
    dist_ab = js_distance(a, b)
    dist_bc = js_distance(b, c)
    dist_ac = js_distance(a, c)
    # Triangle inequality
    assert th.all(dist_ab + dist_bc >= dist_ac)

================
File: tests/test_lenses.py
================
from pathlib import Path
import mock
import pytest
import torch as th
import transformers as trf
from tuned_lens.load_artifacts import load_lens_artifacts
from tuned_lens.nn.lenses import LogitLens, TunedLens, TunedLensConfig
from tuned_lens.nn.unembed import Unembed
@pytest.fixture
def model_config():
    config = mock.MagicMock(trf.PretrainedConfig)
    config.hidden_size = 128
    config.vocab_size = 100
    config.num_hidden_layers = 3
    return config
@pytest.fixture
def model(model_config):
    model = mock.MagicMock(trf.PreTrainedModel)
    model.config = model_config
    model.get_output_embeddings = mock.MagicMock(return_value=th.nn.Linear(128, 100))
    return model
@pytest.fixture
def unembed():
    mock_unembed = mock.MagicMock(Unembed)
    W = th.randn(100, 128)
    mock_unembed.forward = lambda x: th.matmul(x, W.T)
    mock_unembed.unembedding = th.nn.Linear(128, 100)
    mock_unembed.unembedding_hash.return_value = 42
    return mock_unembed
@pytest.fixture
def logit_lens(unembed):
    logit_lens = LogitLens(unembed)
    return logit_lens
@pytest.fixture
def tuned_lens_config():
    return TunedLensConfig(
        base_model_name_or_path="test-model",
        d_model=128,
        num_hidden_layers=3,
        bias=True,
    )
@pytest.fixture
def random_tuned_lens(tuned_lens_config, unembed):
    tuned_lens = TunedLens(
        unembed,
        tuned_lens_config,
    )
    return tuned_lens
def test_logit_lens_smoke(logit_lens):
    randn = th.randn(1, 10, 128)
    logit_lens(randn, 0)
def test_tuned_lens_from_model(random_small_model: trf.PreTrainedModel):
    tuned_lens = TunedLens.from_model(random_small_model)
    assert tuned_lens.config.d_model == random_small_model.config.hidden_size
def test_tuned_lens_forward(random_tuned_lens: TunedLens):
    randn = th.randn(1, 10, 128)
    logits_forward = random_tuned_lens.forward(randn, 0)
    logits = random_tuned_lens.unembed.forward(randn + random_tuned_lens[0](randn))
    assert th.allclose(logits_forward, logits)
def test_tuned_lens_save_and_load(
    unembed: Unembed, random_tuned_lens: TunedLens, tmp_path: Path
):
    randn = th.randn(1, 10, 128)
    logits_before = random_tuned_lens(randn, 1)
    random_tuned_lens.save(tmp_path)
    reloaded_tuned_lens = TunedLens.from_unembed_and_pretrained(
        lens_resource_id=tmp_path, unembed=unembed
    )
    logits_after = reloaded_tuned_lens(randn, 1)
    assert th.allclose(logits_before, logits_after)
def test_from_model_and_pretrained_propogates_kwargs(
    random_tuned_lens: TunedLens, unembed: Unembed, tmp_path: Path
):
    random_tuned_lens.save(tmp_path)
    with mock.patch(
        "tuned_lens.load_artifacts.load_lens_artifacts",
        mock.MagicMock(
            load_lens_artifacts,
            return_value=(tmp_path / "config.json", tmp_path / "params.pt"),
        ),
    ) as mock_load_lens_artifacts:
        mock_load_lens_artifacts.__code__.co_varnames = (
            "resource_id",
            "unembed",
            "revision",
        )
        TunedLens.from_unembed_and_pretrained(
            lens_resource_id="does not use", unembed=unembed, revision="foo"
        )
        assert mock_load_lens_artifacts.call_args.kwargs["revision"] == "foo"
        with pytest.raises(TypeError):
            # Should not just be able to pass any kwarg
            TunedLens.from_unembed_and_pretrained(
                lens_resource_id="does not use",
                unembed=unembed,
                revision="foo",
                bad_kwarg="bar",
            )
        with pytest.raises(TypeError):
            # Should not be able to specify both resource_id and and lens_resource_id
            TunedLens.from_unembed_and_pretrained(
                lens_resource_id="does not use", unembed=unembed, resource_id="bar"
            )
def test_tuned_lens_generate_smoke(random_small_model: trf.PreTrainedModel):
    tuned_lens = TunedLens.from_model(random_small_model)
    bos_token_id = random_small_model.config.bos_token_id
    input_ids = th.tensor([bos_token_id])
    tokens = tuned_lens.generate(
        model=random_small_model,
        layer=2,
        do_sample=True,
        input_ids=input_ids,
        max_new_tokens=10,
    )
    assert tokens.shape[-1] <= 11
    assert tokens.shape[-1] > 1
    assert input_ids == tokens[:, :1]
    assert input_ids == th.tensor([bos_token_id]), "Don't mutate input_ids!"
    tokens = tuned_lens.generate(
        model=random_small_model,
        layer=2,
        input_ids=input_ids,
        do_sample=False,
        max_new_tokens=10,
    )
    assert tokens.shape[-1] <= 11
    assert tokens.shape[-1] > 1

================
File: tests/test_load_artifact.py
================
import pytest
from tuned_lens.load_artifacts import available_lens_artifacts, load_lens_artifacts
def test_load_lens_artifact_smoke():
    load_lens_artifacts("gpt2", "AlignmentResearch/tuned-lens")
def test_load_lens_artifact_raises_smoke():
    with pytest.raises(ValueError, match="Could not find lens at the specified"):
        load_lens_artifacts("sia23s3asdr", "AlignmentResearch/tuned-lens")
def test_list_available_lens_artifacts_smoke():
    artifacts = available_lens_artifacts("AlignmentResearch/tuned-lens", "space")
    assert len(artifacts) > 0
    with pytest.raises(FileNotFoundError):
        artifacts = available_lens_artifacts(
            "AlignmentResearch/tuned-lens",
            "space",
            "revision_does_not_exist",
        )

================
File: tests/test_model_surgery.py
================
import pytest
import torch as th
from transformers import PreTrainedModel
from tuned_lens import model_surgery
def test_get_final_layer_norm_raises(opt_random_model: PreTrainedModel):
    opt_random_model.base_model.decoder.final_layer_norm = None
    with pytest.raises(ValueError):
        assert model_surgery.get_final_norm(opt_random_model)
def test_get_final_layer_norm(random_small_model: PreTrainedModel):
    ln = model_surgery.get_final_norm(random_small_model)
    assert any(isinstance(ln, Norm) for Norm in model_surgery.Norm.__args__)
def test_get_layers_from_model(random_small_model: PreTrainedModel):
    path, layers = model_surgery.get_transformer_layers(random_small_model)
    assert isinstance(layers, th.nn.ModuleList)
    assert isinstance(path, str)
    assert len(layers) == random_small_model.config.num_hidden_layers

================
File: tests/test_stats.py
================
import random
import torch as th
from torch.distributions import Dirichlet, kl_divergence
from tuned_lens.stats import LogitStats
def test_logit_stats_correctness():
    """Test that `LogitStats` recovers the true Dirichlet within a small error."""
    th.manual_seed(42)
    x = Dirichlet(th.tensor([1.0, 1.0, 1.0]))
    logits1 = x.sample(th.Size([10000])).log() + random.uniform(-0.1, 0.1)
    logits2 = x.sample(th.Size([10000])).log() + random.uniform(-0.1, 0.1)
    stats = LogitStats()
    stats.update(logits1)
    stats.update(logits2)
    x2 = stats.mle()
    assert kl_divergence(x, x2) < 1e-3

================
File: tests/test_subspaces.py
================
import pytest
import torch as th
from tuned_lens.causal import remove_subspace
@pytest.mark.parametrize("d", list(range(1, 1000, 100)))
def test_remove_subspace(d: int):
    a = th.randn(10, d, dtype=th.float64)
    for k in range(1, d, 10):
        b = th.randn(d, k, dtype=th.float64)
        inner = a @ b
        a_ = remove_subspace(a, b, mode="zero")
        inner_ = a_ @ b
        th.testing.assert_close(inner_, th.zeros_like(inner_))
        a_ = remove_subspace(a, b, mode="mean")
        inner_ = a_ @ b
        th.testing.assert_close(inner_, inner.mean(0, keepdim=True).expand_as(inner_))

================
File: tests/test_unembed.py
================
import torch as th
import transformers as tr
from tuned_lens.model_surgery import get_final_norm
from tuned_lens.nn import Unembed
def back_translate(unembed: Unembed, h: th.Tensor, tol: float = 1e-4) -> th.Tensor:
    """Project hidden states into logits and then back into hidden states."""
    scale = h.norm(dim=-1, keepdim=True) / h.shape[-1] ** 0.5
    logits = unembed(h)
    return unembed.invert(logits, h0=th.randn_like(h), tol=tol).preimage * scale
def test_correctness(random_small_model: tr.PreTrainedModel):
    # One problem: we want to check that we handle GPT-J's unembedding bias
    # correctly, but it's zero-initialized. Give it a random Gaussian bias.
    U = random_small_model.get_output_embeddings()
    if U.bias is not None:
        U.bias.data.normal_()
    unembed = Unembed(random_small_model)
    ln_f = get_final_norm(random_small_model)
    x = th.randn(1, 1, random_small_model.config.hidden_size)
    y = U(ln_f(x)).log_softmax(-1)  # type: ignore[attr-defined]
    th.testing.assert_close(y, unembed(x).log_softmax(-1))
    x_hat = back_translate(unembed, x, tol=1e-5)
    th.testing.assert_close(y.exp(), unembed(x_hat).softmax(-1), atol=5e-4, rtol=0.01)

================
File: tests/test_utils.py
================
import numpy as np
from tuned_lens.utils import tensor_hash
def test_tensor_hash():
    random = np.random.default_rng(42)
    a = random.normal(size=(10, 1000)).astype(np.float32)
    b = random.normal(size=(10, 1000)).astype(np.float32)
    assert tensor_hash(a) != tensor_hash(b)
    assert tensor_hash(a) == tensor_hash(a)
    assert tensor_hash(a) == tensor_hash(a.astype(np.float16))

================
File: tuned_lens/__init__.py
================
"""The tuned lens package."""
from .nn import TunedLens

================
File: tuned_lens/__main__.py
================
"""Script to train or evaluate a set of tuned lenses for a language model."""
import logging
import os
from dataclasses import dataclass
from typing import Literal, Optional, Union
from simple_parsing import ArgumentParser, ConflictResolution
from torch.distributed.elastic.multiprocessing.errors import record
from .scripts.eval_loop import Eval
from .scripts.train_loop import Train
@dataclass
class Main:
    """Routes to the subcommands."""
    command: Union[Train, Eval]
    log_level: Literal["DEBUG", "INFO", "WARNING", "ERROR"] = "INFO"
    """The log level to use."""
    def execute(self):
        """Run the script."""
        local_rank = os.environ.get("LOCAL_RANK")
        if local_rank is not None:
            FORMAT = f"[%(levelname)s] rank={local_rank} %(message)s"
        else:
            FORMAT = "[%(levelname)s] %(message)s"
        logging.basicConfig(level=self.log_level, format=FORMAT)
        self.command.execute()
@record
def main(args: Optional[list[str]] = None):
    """Entry point for the CLI."""
    parser = ArgumentParser(conflict_resolution=ConflictResolution.EXPLICIT)
    parser.add_arguments(Main, dest="prog")
    args = parser.parse_args(args=args)
    prog: Main = args.prog
    prog.execute()
if __name__ == "__main__":
    main()

================
File: tuned_lens/causal/__init__.py
================
"""Tools for finding and intervening on important subspaces of the residual stream."""
from .subspaces import (
    CausalBasis,
    ablate_subspace,
    extract_causal_bases,
    remove_subspace,
)
from .utils import derange, sample_derangement

================
File: tuned_lens/causal/ablation.py
================
"""Provides tools for ablating layers of a transformer model."""
from contextlib import contextmanager
from typing import Literal
import torch as th
from ..model_surgery import get_transformer_layers
from .utils import derange
@contextmanager
def ablate_layer(
    model: th.nn.Module,
    layer_index: int,
    method: Literal["resample", "mean", "zero"],
    *,
    mode: Literal["batch", "token"] = "batch",
):
    """Replace residual outputs of the specified layer with dummy values.
    If the method is "resample", the residuals are replaced with corresponding
    residuals from a randomly sampled sequence in the batch. If the method is "mean",
    the residuals are replaced with their minibatch means. If the method is "zero",
    all residuals are replaced with the zero vector.
    Args:
        model: The model to modify.
        layer_index: The index of the layer to modify.
        method: How to ablate the layer see above.
        mode: Whether to compute the mean only over the batch dimension or over the
            batch and token dimensions.
    """
    assert layer_index >= 0
    def ablate_hook(_, inputs, outputs):
        x, *_ = inputs
        y, *extras = outputs
        if method == "zero":
            return x, *extras
        residuals = y - x
        original_shape = x.shape
        if mode == "token":
            x = x.flatten(0, 1)
            residuals = residuals.flatten(0, 1)
        batch_size = x.shape[0]
        if batch_size < 2:
            raise ValueError("Mean ablation requires a batch size >= 2")
        if method == "resample":
            ablated = x + derange(residuals)
        elif method == "mean":
            ablated = x + residuals.mean(0, keepdim=True)
        else:
            raise ValueError(f"Unknown ablation method: {method}")
        return ablated.reshape(original_shape), *extras
    _, layers = get_transformer_layers(model)
    handle = layers[layer_index].register_forward_hook(ablate_hook)  # type: ignore
    try:
        yield model
    finally:
        handle.remove()

================
File: tuned_lens/causal/subspaces.py
================
"""Provides tools for extracting causal bases from models and ablating subspaces."""
import logging
from contextlib import contextmanager
from typing import Iterable, Literal, NamedTuple, Optional, Sequence
import torch as th
import torch.distributed as dist
import torch.nn.functional as F
from tqdm.auto import trange
from ..model_surgery import get_transformer_layers
from ..nn import Lens
from ..utils import maybe_all_reduce
from .utils import derange
logger = logging.getLogger(__name__)
@contextmanager
def ablate_subspace(
    model: th.nn.Module,
    A: th.Tensor,
    layer_index: int,
    mode: Literal["mean", "resample", "zero"] = "zero",
    orthonormal: bool = False,
):
    """Context manager that ablates a subspace of activations.
    Args:
        model: A hugging face transformer model.
        A: Either a 2D matrix whose column space is to be removed, or a 1D vector whose
            span is to be removed.
        layer_index: The index of the layer to ablate.
        mode: Which method to use for removing information along the subspace.
            Defaults to `"zero"`.
        orthonormal: if True, `A` is assumed to be orthonormal.
    """
    _, layers = get_transformer_layers(model)
    def wrapper(_, __, outputs):
        h, *extras = outputs
        h_ = remove_subspace(h, A, mode, orthonormal)
        return h_, *extras
    handle = layers[layer_index].register_forward_hook(wrapper)  # type: ignore
    try:
        yield model
    finally:
        handle.remove()
class CausalBasis(NamedTuple):
    """An ordered orthonormal basis for a subspace of activations.
    Attributes:
        energies: A vector of shape (k,) containing the energies of the
            basis vectors. Each energy is the expected KL divergence of
            the post-intervention logits wrt the control logits when the
            corresponding basis vector is ablated.
        vectors: A matrix of shape (d, k) where d is the ambient dimension
            and k is the dimension of the subspace. The columns of this
            matrix are basis vectors, ordered by decreasing energy.
    """
    energies: th.Tensor
    vectors: th.Tensor
def extract_causal_bases(
    lens: Lens,
    hiddens: Sequence[th.Tensor],
    k: int,
    *,
    labels: Optional[th.Tensor] = None,
    max_iter: int = 100,
    mode: Literal["mean", "resample", "zero"] = "mean",
) -> Iterable[CausalBasis]:
    """Extract causal bases for probes at each layer of a model.
    Args:
        lens: A lens to compute causal bases for.
        hiddens: A sequence of hidden states from the model.
        k: The number of basis vectors to compute for each layer.
        max_iter: The maximum number of iterations to run L-BFGS for each vector.
        mode: Which method to use for removing information along the subspace.
            Defaults to `"zero"`.
    """
    lens.requires_grad_(False)
    device = hiddens[0].device
    dtype = hiddens[0].dtype
    d = hiddens[0].shape[-1]
    hiddens = [h.detach() for h in hiddens]
    num_layers = len(hiddens) - 1
    assert k <= d
    if k < 1:
        k = d
    eye = th.eye(d, device=device, dtype=dtype)
    show_pbar = not dist.is_initialized() or dist.get_rank() == 0
    pbar = trange(num_layers * k) if show_pbar else None
    # Outer loop iterates over layers
    for i in range(num_layers):
        U = lens.unembed.unembedding.weight.data.T
        logits = lens(hiddens[i], i)
        log_p = logits.log_softmax(-1)
        U = lens.transform_hidden(U, i)  # TODO not sure if we need transposes here
        # Compute the baseline loss up front so that we can subtract it
        # from the post-ablation losses to get the loss increment
        if labels is not None:
            base_loss = F.cross_entropy(
                log_p[:, :-1].flatten(0, -2), labels[:, 1:].flatten()
            )
        else:
            base_loss = 0.0
        # Initialize basis vectors with left singular vectors of U
        u, *_ = th.linalg.svd(U, full_matrices=False)
        basis = CausalBasis(th.zeros(k, device=device), u[:, :k].float())
        # Inner loop iterates over directions
        p = log_p.exp()
        for j in range(k):
            if pbar:
                pbar.set_description(f"Layer {i + 1}/{num_layers}, vector {j + 1}/{k}")
            # Construct the operator for projecting away from the previously
            # identified basis vectors
            if j:
                A = basis.vectors[:, :j]
                proj = eye - A @ A.T
            else:
                proj = eye
            def project(x: th.Tensor) -> th.Tensor:
                # Project away from previously identified basis vectors
                x = proj @ x
                # Project to the unit sphere
                return x / (x.norm() + th.finfo(x.dtype).eps)
            basis.vectors[:, j] = project(basis.vectors[:, j])
            v = th.nn.Parameter(basis.vectors[:, j])
            nfev = 0
            energy_delta = th.tensor(0.0, device=device)
            last_energy = th.tensor(0.0, device=device)
            opt = th.optim.LBFGS(
                [v],
                line_search_fn="strong_wolfe",
                max_iter=max_iter,
            )
            def closure():
                nonlocal energy_delta, nfev, last_energy
                nfev += 1
                opt.zero_grad(set_to_none=False)
                v_ = project(v)
                h_ = remove_subspace(hiddens[i], v_, mode=mode, orthonormal=True)
                logits = lens(h_, i)
                if labels is not None:
                    loss = -F.cross_entropy(
                        logits[:, :-1].flatten(0, 1), labels[:, 1:].flatten()
                    )
                else:
                    log_q = logits.log_softmax(-1)
                    loss = -th.sum(p * (log_p - log_q), dim=-1).mean()
                loss.backward()
                maybe_all_reduce(loss)
                maybe_all_reduce(v.grad)  # type: ignore[arg-type]
                assert v.grad is not None
                new_energy = -loss.detach() - base_loss
                energy_delta = new_energy - last_energy
                last_energy = new_energy
                if pbar:
                    pbar.set_postfix(energy=last_energy.item())
                if not loss.isfinite():
                    logger.warning("Loss is not finite")
                    loss = th.tensor(0.0, device=device)
                    opt.zero_grad(set_to_none=False)
                return loss
            while nfev < max_iter:
                opt.step(closure)  # type: ignore
                v.data = project(v.data)
                if abs(energy_delta / last_energy) < 1e-4:
                    break
            basis.vectors[:, j] = project(v.data)
            basis.energies[j] = last_energy
            if pbar:
                pbar.update()
        indices = basis.energies.argsort(descending=True)
        yield CausalBasis(basis.energies[indices], basis.vectors[:, indices])
def remove_subspace(
    u: th.Tensor,
    A: th.Tensor,
    mode: Literal["mean", "resample", "zero"] = "zero",
    orthonormal: bool = False,
) -> th.Tensor:
    """Remove all information in `u` along the column space of `A`.
    This can be done by zero, mean, or resample ablation. With zero ablation,
    `u` is projected onto the orthogonal complement of col(`A`), so the resulting
    vectors are orthogonal to every column in `A`. With mean ablation, `u` is projected
    onto the subspace s.t. the angles between the resulting vectors and the columns of
    `A` are equal to their mean values. With resample ablation, the variation in `u`
    is shuffled across vectors.
    Args:
        u: The vectors to be projected.
        A: Either a 2D matrix whose column space is to be removed, or a 1D vector whose
            span is to be removed.
        mode: Which method to use for removing information along the subspace.
            Defaults to `"zero"`.
        orthonormal: Whether to assume `A` is orthonormal. Defaults to `False`.
    Returns:
        th.Tensor: The transformed vectors.
    """
    if A.ndim == 1:
        A = A[..., None]
    d, _ = A.shape
    if u.shape[-1] != d:
        raise ValueError(f"Last dimension of u must be {d}, but is {u.shape[-1]}")
    # https://en.wikipedia.org/wiki/Projection_(linear_algebra)#Properties_and_special_cases
    if orthonormal:
        proj = A @ A.mT
    else:
        proj = A @ th.linalg.solve(A.mT @ A, A.mT)
    if mode == "zero":
        dummy = -u
    else:
        samples = u.flatten(0, -2)
        N = samples.shape[0]
        if N < 2:
            raise ValueError("Need at least 2 vectors for mean and resample ablation")
        if mode == "mean":
            dummy = samples.mean(0) - u
        elif mode == "resample":
            # Shuffle the rows of `samples` without fixed points.
            dummy = derange(samples).view_as(u) - u
        else:
            raise ValueError(f"Unknown mode {mode}")
    return u + th.einsum("ij,...j->...i", proj, dummy)

================
File: tuned_lens/causal/utils.py
================
from typing import Optional
import torch as th
def derange(batch: th.Tensor, generator: Optional[th.Generator] = None) -> th.Tensor:
    """Shuffle a tensor along axis 0, making sure there are no fixed points."""
    # Things get more complicated if there are multiple ranks. We perform the
    # derangement *hierarchically*, first generating a shared permutation of the ranks
    indices = sample_derangement(
        batch.shape[0], device=batch.device, generator=generator
    )
    return batch[indices]
def sample_derangement(
    n: int,
    device: th.device = th.device("cpu"),
    generator: Optional[th.Generator] = None,
) -> th.Tensor:
    """Uniformly sample a random permutation with no fixed points."""
    if n < 2:
        raise ValueError("Derangements only exist for n > 1")
    indices = th.arange(n, device=device)
    permutation = th.randperm(n, device=device, generator=generator)
    # Reject any permutations with fixed points. This seems inefficient,
    # but the expected number of th.randperm calls is actually O(1); it
    # asymptotically approaches e â‰ˆ 2.7.
    # See https://www.cs.upc.edu/~conrado/research/talks/analco08.pdf.
    while th.any(permutation == indices):
        permutation = th.randperm(n, device=device, generator=generator)
    return permutation

================
File: tuned_lens/data.py
================
"""Tools for tokenizing and manipulating text datasets."""
import math
from multiprocessing import cpu_count
from typing import TypeVar, Union
from datasets import Dataset, DatasetDict
from transformers import PreTrainedTokenizerBase
T = TypeVar("T", bound=Union[Dataset, DatasetDict])
def chunk_and_tokenize(
    data: T,
    tokenizer: PreTrainedTokenizerBase,
    *,
    format: str = "torch",
    num_proc: int = min(cpu_count() // 2, 8),
    text_key: str = "text",
    max_seq_len: int = 2048,
    return_final_batch: bool = False,
    load_from_cache_file: bool = True,
) -> tuple[T, float]:
    """Perform GPT-style chunking and tokenization on a dataset.
    The resulting dataset will consist entirely of chunks exactly `max_seq_len` tokens
    long. Long sequences will be split into multiple chunks, and short sequences will
    be merged with their neighbors, using `eos_token` as a separator. The fist token
    will also always be an `eos_token`.
    Args:
        data: The dataset to chunk and tokenize.
        tokenizer: The tokenizer to use.
        format: The format to return the dataset in, passed to `Dataset.with_format`.
        num_proc: The number of processes to use for tokenization.
        text_key: The key in the dataset to use as the text to tokenize.
        max_seq_len: The maximum length of a batch of input ids.
        return_final_batch: Whether to return the final batch, which may be smaller
            than the others.
        load_from_cache_file: Whether to load from the cache file.
    Returns:
        * The chunked and tokenized dataset.
        * The ratio of nats to bits per byte see https://arxiv.org/pdf/2101.00027.pdf,
            section 3.1.
    """
    def _tokenize_fn(x: dict[str, list]):
        chunk_size = min(tokenizer.model_max_length, max_seq_len)
        sep = tokenizer.eos_token or "<|endoftext|>"
        joined_text = sep.join([""] + x[text_key])
        output = tokenizer(
            # Concatenate all the samples together, separated by the EOS token.
            joined_text,  # start with an eos token
            max_length=chunk_size,
            return_attention_mask=False,
            return_overflowing_tokens=True,
            truncation=True,
        )
        if overflow := output.pop("overflowing_tokens", None):
            # Slow Tokenizers return unnested lists of ints
            assert isinstance(output["input_ids"][0], int)
            # Chunk the overflow into batches of size `chunk_size`
            chunks = [output["input_ids"]] + [
                overflow[i * chunk_size : (i + 1) * chunk_size]
                for i in range(math.ceil(len(overflow) / chunk_size))
            ]
            output = {"input_ids": chunks}
        total_tokens = sum(len(ids) for ids in output["input_ids"])
        total_bytes = len(joined_text.encode("utf-8"))
        if not return_final_batch:
            # We know that the last sample will almost always be less than the max
            # number of tokens, and we don't want to pad, so we just drop it.
            output = {k: v[:-1] for k, v in output.items()}
        output_batch_size = len(output["input_ids"])
        if output_batch_size == 0:
            raise ValueError(
                "Not enough data to create a single batch complete batch."
                " Either allow the final batch to be returned,"
                " or supply more data."
            )
        # We need to output this in order to compute the number of bits per byte
        div, rem = divmod(total_tokens, output_batch_size)
        output["length"] = [div] * output_batch_size
        output["length"][-1] += rem
        div, rem = divmod(total_bytes, output_batch_size)
        output["bytes"] = [div] * output_batch_size
        output["bytes"][-1] += rem
        return output
    data = data.map(
        _tokenize_fn,
        # Batching is important for ensuring that we don't waste tokens
        # since we always throw away the last element of the batch we
        # want to keep the batch size as large as possible
        batched=True,
        batch_size=2048,
        num_proc=num_proc,
        remove_columns=get_columns_all_equal(data),
        load_from_cache_file=load_from_cache_file,
    )
    total_bytes: float = sum(data["bytes"])
    total_tokens: float = sum(data["length"])
    return data.with_format(format, columns=["input_ids"]), (
        total_tokens / total_bytes
    ) / math.log(2)
def get_columns_all_equal(dataset: Union[Dataset, DatasetDict]) -> list[str]:
    """Get a single list of columns in a `Dataset` or `DatasetDict`.
    We assert the columms are the same across splits if it's a `DatasetDict`.
    Args:
        dataset: The dataset to get the columns from.
    Returns:
        A list of columns.
    """
    if isinstance(dataset, DatasetDict):
        cols_by_split = dataset.column_names.values()
        columns = next(iter(cols_by_split))
        if not all(cols == columns for cols in cols_by_split):
            raise ValueError("All splits must have the same columns")
        return columns
    return dataset.column_names

================
File: tuned_lens/load_artifacts.py
================
"""Load lens artifacts from the hub or locally storage."""
import os
from pathlib import Path
from typing import Optional
from huggingface_hub import HfFileSystem, hf_hub_download
from huggingface_hub.utils import EntryNotFoundError
def available_lens_artifacts(
    repo_id: str,
    repo_type: str,
    revision: str = "main",
    config_file: str = "config.json",
    ckpt_file: str = "params.pt",
    subfolder: str = "lens",
) -> set[str]:
    """Get the available lens artifacts from the hub."""
    fs = HfFileSystem()
    repo_type = repo_type + "s" if not repo_type.endswith("s") else repo_type
    root = Path(repo_type, repo_id, subfolder)
    with_config = map(
        Path,
        fs.glob(
            (root / "**" / config_file).as_posix(), revision=revision  # type: ignore
        ),
    )
    with_pt = map(
        Path,
        fs.glob(
            (root / "**" / ckpt_file).as_posix(), revision=revision  # type: ignore
        ),
    )
    paths = {p.parent for p in with_pt}.intersection({p.parent for p in with_config})
    return {p.relative_to(root).as_posix() for p in paths}
def load_lens_artifacts(
    resource_id: str,
    repo_id: Optional[str] = None,
    repo_type: Optional[str] = None,
    revision: str = "main",
    config_file: str = "config.json",
    ckpt_file: str = "params.pt",
    subfolder: str = "lens",
    cache_dir: Optional[str] = None,
) -> tuple[Path, Path]:
    """First checks for lens resource locally then tries to download it from the hub.
    Args:
        resource_id: The id of the lens resource.
        repo_id: The repository to download the lens from. Defaults to
            'AlignmentResearch/tuned-lens'. However, this default can be overridden by
            setting the TUNED_LENS_REPO_ID environment variable.
        repo_type: The type of repository to download the lens from. Defaults to
            'space'. However, this default can be overridden by setting the
            TUNED_LENS_REPO_TYPE environment variable.
        config_file: The name of the config file in the folder contain the lens.
        ckpt_file: The name of the checkpoint file in the folder contain the lens.
        revision: The revision of the lens to download.
        subfolder: The subfolder of the repository to download the lens from.
        cache_dir: The directory to cache the lens in.
    Returns:
        * The path to the config.json file
        * The path to the params.pt file
    Raises:
        ValueError: if the lens resource could not be found.
    """
    if repo_id is None:
        if os.environ.get("TUNED_LENS_REPO_ID"):
            repo_id = os.environ["TUNED_LENS_REPO_ID"]
        else:
            repo_id = "AlignmentResearch/tuned-lens"
    if repo_type is None:
        if os.environ.get("TUNED_LENS_REPO_TYPE"):
            repo_type = os.environ["TUNED_LENS_REPO_TYPE"]
        else:
            repo_type = "space"
    # Fist check if the resource id is a path to a folder that exists
    local_path = Path(resource_id)
    if (local_path / config_file).exists() and (local_path / ckpt_file).exists():
        return local_path / config_file, local_path / ckpt_file
    resource_folder = "/".join((subfolder, resource_id))
    try:
        params_path = hf_hub_download(
            filename=ckpt_file,
            repo_id=repo_id,
            repo_type=repo_type,
            revision=revision,
            subfolder=resource_folder,
            cache_dir=cache_dir,
        )
        config_path = hf_hub_download(
            filename=config_file,
            repo_id=repo_id,
            repo_type=repo_type,
            revision=revision,
            subfolder=resource_folder,
            cache_dir=cache_dir,
        )
    except EntryNotFoundError:
        available_lenses = available_lens_artifacts(
            repo_id=repo_id,
            repo_type=repo_type,
            revision=revision,
            config_file=config_file,
            ckpt_file=ckpt_file,
            subfolder=subfolder,
        )
        message = (
            f"Could not find lens at the specified resource id. Available lens"
            f"resources are: {', '.join(available_lenses)}"
        )
        raise ValueError(message)
    if config_path is not None and params_path is not None:
        return Path(config_path), Path(params_path)
    raise ValueError("Could not find lens resource locally or on the hf hub.")

================
File: tuned_lens/model_surgery.py
================
"""Tools for finding and modifying components in a transformer model."""
from contextlib import contextmanager
from typing import Any, Generator, TypeVar, Union
try:
    import transformer_lens as tl
    _transformer_lens_available = True
except ImportError:
    _transformer_lens_available = False
import torch as th
import transformers as tr
from torch import nn
from transformers import models
def get_value_for_key(obj: Any, key: str) -> Any:
    """Get a value using `__getitem__` if `key` is numeric and `getattr` otherwise."""
    return obj[int(key)] if key.isdigit() else getattr(obj, key)
def set_value_for_key_(obj: Any, key: str, value: Any) -> None:
    """Set value in-place if `key` is numeric and `getattr` otherwise."""
    if key.isdigit():
        obj[int(key)] = value
    else:
        setattr(obj, key, value)
def get_key_path(model: th.nn.Module, key_path: str) -> Any:
    """Get a value by key path, e.g. `layers.0.attention.query.weight`."""
    for key in key_path.split("."):
        model = get_value_for_key(model, key)
    return model
def set_key_path_(
    model: th.nn.Module, key_path: str, value: Union[th.nn.Module, th.Tensor]
) -> None:
    """Set a value by key path in-place, e.g. `layers.0.attention.query.weight`."""
    keys = key_path.split(".")
    for key in keys[:-1]:
        model = get_value_for_key(model, key)
    setattr(model, keys[-1], value)
T = TypeVar("T", bound=th.nn.Module)
@contextmanager
def assign_key_path(model: T, key_path: str, value: Any) -> Generator[T, None, None]:
    """Temporarily set a value by key path while in the context."""
    old_value = get_key_path(model, key_path)
    set_key_path_(model, key_path, value)
    try:
        yield model
    finally:
        set_key_path_(model, key_path, old_value)
Model = Union[tr.PreTrainedModel, "tl.HookedTransformer"]
Norm = Union[
    th.nn.LayerNorm,
    models.llama.modeling_llama.LlamaRMSNorm,
    models.gemma.modeling_gemma.GemmaRMSNorm,
    nn.Module,
]
def get_unembedding_matrix(model: Model) -> nn.Linear:
    """The final linear tranformation from the model hidden state to the output."""
    if isinstance(model, tr.PreTrainedModel):
        unembed = model.get_output_embeddings()
        if not isinstance(unembed, nn.Linear):
            raise ValueError("We currently only support linear unemebdings")
        return unembed
    elif _transformer_lens_available and isinstance(model, tl.HookedTransformer):
        linear = nn.Linear(
            in_features=model.cfg.d_model,
            out_features=model.cfg.d_vocab_out,
        )
        linear.bias.data = model.unembed.b_U
        linear.weight.data = model.unembed.W_U.transpose(0, 1)
        return linear
    else:
        raise ValueError(f"Model class {type(model)} not recognized!")
def get_final_norm(model: Model) -> Norm:
    """Get the final norm from a model.
    This isn't standardized across models, so this will need to be updated as
    we add new models.
    """
    if _transformer_lens_available and isinstance(model, tl.HookedTransformer):
        return model.ln_final
    if not hasattr(model, "base_model"):
        raise ValueError("Model does not have a `base_model` attribute.")
    base_model = model.base_model
    if isinstance(base_model, models.opt.modeling_opt.OPTModel):
        final_layer_norm = base_model.decoder.final_layer_norm
    elif isinstance(base_model, models.gpt_neox.modeling_gpt_neox.GPTNeoXModel):
        final_layer_norm = base_model.final_layer_norm
    elif isinstance(
        base_model,
        (
            models.bloom.modeling_bloom.BloomModel,
            models.gpt2.modeling_gpt2.GPT2Model,
            models.gpt_neo.modeling_gpt_neo.GPTNeoModel,
            models.gptj.modeling_gptj.GPTJModel,
        ),
    ):
        final_layer_norm = base_model.ln_f
    elif isinstance(base_model, models.llama.modeling_llama.LlamaModel):
        final_layer_norm = base_model.norm
    elif isinstance(base_model, models.gemma.modeling_gemma.GemmaModel):
        final_layer_norm = base_model.norm
    else:
        raise NotImplementedError(f"Unknown model type {type(base_model)}")
    if final_layer_norm is None:
        raise ValueError("Model does not have a final layer norm.")
    assert isinstance(final_layer_norm, Norm.__args__)  # type: ignore
    return final_layer_norm
def get_transformer_layers(model: Model) -> tuple[str, th.nn.ModuleList]:
    """Get the decoder layers from a model.
    Args:
        model: The model to search.
    Returns:
        A tuple containing the key path to the layer list and the list itself.
    Raises:
        ValueError: If no such list exists.
    """
    # TODO implement this so that we can do hooked transformer training.
    if not hasattr(model, "base_model"):
        raise ValueError("Model does not have a `base_model` attribute.")
    path_to_layers = ["base_model"]
    base_model = model.base_model
    if isinstance(base_model, models.opt.modeling_opt.OPTModel):
        path_to_layers += ["decoder", "layers"]
    elif isinstance(base_model, models.gpt_neox.modeling_gpt_neox.GPTNeoXModel):
        path_to_layers += ["layers"]
    elif isinstance(
        base_model,
        (
            models.bloom.modeling_bloom.BloomModel,
            models.gpt2.modeling_gpt2.GPT2Model,
            models.gpt_neo.modeling_gpt_neo.GPTNeoModel,
            models.gptj.modeling_gptj.GPTJModel,
        ),
    ):
        path_to_layers += ["h"]
    elif isinstance(base_model, models.llama.modeling_llama.LlamaModel):
        path_to_layers += ["layers"]
    elif isinstance(base_model, models.gemma.modeling_gemma.GemmaModel):
        path_to_layers += ["layers"]
    else:
        raise NotImplementedError(f"Unknown model type {type(base_model)}")
    path_to_layers = ".".join(path_to_layers)
    return path_to_layers, get_key_path(model, path_to_layers)
@contextmanager
def delete_layers(model: T, indices: list[int]) -> Generator[T, None, None]:
    """Temporarily delete the layers at `indices` from `model` while in the context."""
    list_path, layer_list = get_transformer_layers(model)
    modified_list = th.nn.ModuleList(layer_list)
    for i in sorted(indices, reverse=True):
        del modified_list[i]
    set_key_path_(model, list_path, modified_list)
    try:
        yield model
    finally:
        set_key_path_(model, list_path, layer_list)
@contextmanager
def permute_layers(model: T, indices: list[int]) -> Generator[T, None, None]:
    """Temporarily permute the layers of `model` by `indices` while in the context.
    The number of indices provided may be not be equal to the number of
    layers in the model. Layers will be dropped or duplicated accordingly.
    """
    list_path, layer_list = get_transformer_layers(model)
    permuted_list = th.nn.ModuleList([layer_list[i] for i in indices])
    set_key_path_(model, list_path, permuted_list)
    try:
        yield model
    finally:
        set_key_path_(model, list_path, layer_list)
def permute_layers_(model: th.nn.Module, indices: list[int]):
    """Permute the layers of `model` by `indices` in-place.
    The number of indices provided may be not be equal to the number of
    layers in the model. Layers will be dropped or duplicated accordingly.
    """
    list_path, layer_list = get_transformer_layers(model)
    permuted_list = th.nn.ModuleList([layer_list[i] for i in indices])
    set_key_path_(model, list_path, permuted_list)
@contextmanager
def replace_layers(
    model: T, indices: list[int], replacements: list[th.nn.Module]
) -> Generator[T, None, None]:
    """Replace the layers at `indices` with `replacements` while in the context."""
    list_path, layer_list = get_transformer_layers(model)
    modified_list = th.nn.ModuleList(layer_list)
    for i, replacement in zip(indices, replacements):
        modified_list[i] = replacement
    set_key_path_(model, list_path, modified_list)
    try:
        yield model
    finally:
        set_key_path_(model, list_path, layer_list)

================
File: tuned_lens/nn/__init__.py
================
"""A set of PyTorch modules for transforming the residual streams of models."""
from .lenses import Lens, LogitLens, TunedLens, TunedLensConfig
from .unembed import (
    InversionOutput,
    Unembed,
)

================
File: tuned_lens/nn/lenses.py
================
"""Provides lenses for decoding hidden states into logits."""
import abc
import inspect
import json
import logging
from copy import deepcopy
from dataclasses import asdict, dataclass
from pathlib import Path
from typing import Dict, Generator, Optional, Union
import torch as th
from transformers import PreTrainedModel
from tuned_lens import load_artifacts
from tuned_lens.nn.unembed import Unembed
logger = logging.getLogger(__name__)
class Lens(abc.ABC, th.nn.Module):
    """Abstract base class for all Lens."""
    unembed: Unembed
    def __init__(self, unembed: Unembed):
        """Create a Lens.
        Args:
            unembed: The unembed operation to use.
        """
        super().__init__()
        self.unembed = unembed
    @abc.abstractmethod
    def transform_hidden(self, h: th.Tensor, idx: int) -> th.Tensor:
        """Convert a hidden state to the final hidden just before the unembedding.
        Args:
            h: The hidden state to convert.
            idx: The layer of the transformer these hidden states come from.
        """
        ...
    @abc.abstractmethod
    def forward(self, h: th.Tensor, idx: int) -> th.Tensor:
        """Decode hidden states into logits."""
        ...
class LogitLens(Lens):
    """Unembeds the residual stream into logits."""
    unembed: Unembed
    def __init__(
        self,
        unembed: Unembed,
    ):
        """Create a Logit Lens.
        Args:
            unembed: The unembed operation to use.
        """
        super().__init__(unembed)
    @classmethod
    def from_model(
        cls,
        model: PreTrainedModel,
    ) -> "LogitLens":
        """Create a LogitLens from a pretrained model.
        Args:
            model: A pretrained model from the transformers library you wish to inspect.
        """
        unembed = Unembed(model)
        return cls(unembed)
    def transform_hidden(self, h: th.Tensor, idx: int) -> th.Tensor:
        """For the LogitLens, this is the identity function."""
        del idx
        return h
    def forward(self, h: th.Tensor, idx: int) -> th.Tensor:
        """Decode a hidden state into logits.
        Args:
            h: The hidden state to decode.
            idx: the layer of the transformer these hidden states come from.
        """
        del idx
        return self.unembed.forward(h)
@dataclass
class TunedLensConfig:
    """A configuration for a TunedLens."""
    # The name of the base model this lens was tuned for.
    base_model_name_or_path: str
    # The hidden size of the base model.
    d_model: int
    # The number of layers in the base model.
    num_hidden_layers: int
    # whether to use a bias in the linear translators.
    bias: bool = True
    # The revision of the base model this lens was tuned for.
    base_model_revision: Optional[str] = None
    # The hash of the base's unembed model this lens was tuned for.
    unembed_hash: Optional[str] = None
    # The name of the lens type.
    lens_type: str = "linear_tuned_lens"
    def to_dict(self):
        """Convert this config to a dictionary."""
        return asdict(self)
    @classmethod
    def from_dict(cls, config_dict: Dict):
        """Create a config from a dictionary."""
        config_dict = deepcopy(config_dict)
        # Drop unrecognized config keys
        unrecognized = set(config_dict) - set(inspect.getfullargspec(cls).args)
        for key in unrecognized:
            logger.warning(f"Ignoring config key '{key}'")
            del config_dict[key]
        return cls(**config_dict)
class TunedLens(Lens):
    """A tuned lens for decoding hidden states into logits."""
    config: TunedLensConfig
    unembed: Unembed
    layer_translators: th.nn.ModuleList
    def __init__(
        self,
        unembed: Unembed,
        config: TunedLensConfig,
    ):
        """Create a TunedLens.
        Args:
            unembed: The unembed operation to use.
            config: The configuration for this lens.
        """
        super().__init__(unembed)
        self.config = config
        unembed_hash = unembed.unembedding_hash()
        config.unembed_hash = unembed_hash
        # The unembedding might be int8 if we're using bitsandbytes
        w = unembed.unembedding.weight
        dtype = w.dtype if th.is_floating_point(w) else th.float16
        translator = th.nn.Linear(
            config.d_model, config.d_model, bias=config.bias, dtype=dtype
        )
        translator.weight.data.zero_()
        translator.bias.data.zero_()
        # Don't include the final layer since it does not need a translator
        self.layer_translators = th.nn.ModuleList(
            [deepcopy(translator) for _ in range(self.config.num_hidden_layers)]
        )
    def __getitem__(self, item: int) -> th.nn.Module:
        """Get the probe module at the given index."""
        return self.layer_translators[item]
    def __iter__(self) -> Generator[th.nn.Module, None, None]:
        """Get iterator over the translators within the lens."""
        yield from self.layer_translators
    @classmethod
    def from_model(
        cls,
        model: PreTrainedModel,
        model_revision: Optional[str] = None,
        bias: bool = True,
    ) -> "TunedLens":
        """Create a lens from a pretrained model.
        Args:
            model: The model to create the lens from.
            model_revision: The git revision of the model to used.
            bias: Whether to use a bias in the linear translators.
        Returns:
            A TunedLens instance.
        """
        unembed = Unembed(model)
        config = TunedLensConfig(
            base_model_name_or_path=model.config.name_or_path,
            base_model_revision=model_revision,
            d_model=model.config.hidden_size,
            num_hidden_layers=model.config.num_hidden_layers,
            bias=bias,
        )
        return cls(unembed, config)
    @classmethod
    def from_model_and_pretrained(
        cls,
        model: PreTrainedModel,
        lens_resource_id: Optional[str] = None,
        **kwargs,
    ) -> "TunedLens":
        """Load a tuned lens from a folder or hugging face hub.
        Args:
            model: The model to create the lens from.
            lens_resource_id: The resource id of the lens to load. Defaults to the
                model's name_or_path.
            **kwargs: Additional arguments to pass to
                :func:`tuned_lens.load_artifacts.load_lens_artifacts` and
                `th.load <https://pytorch.org/docs/stable/generated/torch.load.html>`_.
        Returns:
            A TunedLens instance whose unembedding is derived from the given model
            and whose layer translators are loaded from the given resource id.
        """
        if lens_resource_id is None:
            lens_resource_id = model.config.name_or_path
        return cls.from_unembed_and_pretrained(
            Unembed(model), lens_resource_id, **kwargs
        )
    @classmethod
    def from_unembed_and_pretrained(
        cls,
        unembed: Unembed,
        lens_resource_id: str,
        **kwargs,
    ) -> "TunedLens":
        """Load a tuned lens from a folder or hugging face hub.
        Args:
            unembed: The unembed operation to use for the lens.
            lens_resource_id: The resource id of the lens to load.
            **kwargs: Additional arguments to pass to
                :func:`tuned_lens.load_artifacts.load_lens_artifacts` and
                `th.load <https://pytorch.org/docs/stable/generated/torch.load.html>`_.
        Returns:
            A TunedLens instance.
        """
        # Validate kwargs
        load_artifact_varnames = load_artifacts.load_lens_artifacts.__code__.co_varnames
        config_path, ckpt_path = load_artifacts.load_lens_artifacts(
            resource_id=lens_resource_id,
            **{k: v for k, v in kwargs.items() if k in load_artifact_varnames},
        )
        with open(config_path, "r") as f:
            config = TunedLensConfig.from_dict(json.load(f))
        # validate the unembed is the same as the one used to train the lens
        if config.unembed_hash and unembed.unembedding_hash() != config.unembed_hash:
            logger.warning(
                "The unembedding matrix hash does not match the lens' hash."
                "This lens may have been trained with a different unembedding."
            )
        # Create the lens
        lens = cls(unembed, config)
        th_load_kwargs = {
            **{k: v for k, v in kwargs.items() if k not in load_artifact_varnames}
        }
        # Load parameters
        state = th.load(ckpt_path, **th_load_kwargs)
        lens.layer_translators.load_state_dict(state)
        return lens
    def save(
        self,
        path: Union[Path, str],
        ckpt: str = "params.pt",
        config: str = "config.json",
    ) -> None:
        """Save the lens to a directory.
        Args:
            path : The path to the directory to save the lens to.
            ckpt : The name of the checkpoint file to save the parameters to.
            config : The name of the config file to save the config to.
        """
        path = Path(path)
        path.mkdir(exist_ok=True, parents=True)
        state_dict = self.layer_translators.state_dict()
        th.save(state_dict, path / ckpt)
        with open(path / config, "w") as f:
            json.dump(self.config.to_dict(), f)
    def transform_hidden(self, h: th.Tensor, idx: int) -> th.Tensor:
        """Transform hidden state from layer `idx`."""
        # Note that we add the translator output residually, in contrast to the formula
        # in the paper. By parametrizing it this way we ensure that weight decay
        # regularizes the transform toward the identity, not the zero transformation.
        return h + self[idx](h)
    def forward(self, h: th.Tensor, idx: int) -> th.Tensor:
        """Transform and then decode the hidden states into logits."""
        h = self.transform_hidden(h, idx)
        return self.unembed.forward(h)
    def __len__(self) -> int:
        """Return the number of layer translators in the lens."""
        return len(self.layer_translators)
    @th.inference_mode()
    def generate(
        self,
        model: PreTrainedModel,
        layer: int,
        input_ids: th.Tensor,
        do_sample: bool = True,
        temp: float = 1.0,
        max_new_tokens: int = 100,
    ) -> th.Tensor:
        """Generate from the tuned lens at the given layer.
        Args:
            model: The base model the generate from. Usually the model this lens trained
                on.
            layer: The layer to generate from.
            input_ids: (batch x prompt_len) The input ids to generate from.
            do_sample: Whether to use sampling or greedy decoding.
            temp: The temperature to use for sampling.
            max_new_tokens: The maximum number of tokens to generate.
        Returns:
            The prompt concatenated with the newly generated tokens.
        """
        eos_token = model.generation_config.eos_token_id
        tokens = input_ids
        if tokens.ndim == 1:
            tokens = tokens.unsqueeze(0)
        batch, prompt_len = tokens.shape
        del prompt_len
        past_key_values = None
        done = th.zeros(batch, dtype=th.bool)
        for _ in range(max_new_tokens):
            output = model(
                input_ids=tokens,
                output_hidden_states=True,
                use_cache=True,
                past_key_values=past_key_values,
            )
            past_key_values = output.past_key_values
            hidden = output.hidden_states[layer]
            new_hidden = hidden[:, -1, :]
            new_logits = self.forward(new_hidden, layer)
            if do_sample:
                new_logits = new_logits / temp
                probs = new_logits.softmax(dim=-1)
                new_tokens = th.multinomial(probs, num_samples=1)
            else:
                new_tokens = new_logits.argmax(dim=-1, keepdim=True)
            # Once a sequence has generated an EOS token, it should not generate any
            # other tokens.
            done = done | (new_tokens == eos_token)
            new_tokens = new_tokens.masked_fill(done, eos_token)
            tokens = th.cat([tokens, new_tokens], dim=-1)
            # Halt generation if all sequences have generated an EOS token.
            if done.all():
                break
        return tokens

================
File: tuned_lens/nn/unembed.py
================
"""Provides a class for mapping transformer hidden states to logits (and vice versa)."""
import copy
from dataclasses import dataclass
from typing import Literal, Optional, cast
try:
    # Needed for the docs to build without complaining
    import transformer_lens as tl  # noqa: F401
    _transformer_lens_available = True
except ImportError:
    _transformer_lens_available = False
import torch as th
from torch.distributions import Distribution
from tuned_lens import model_surgery
from tuned_lens.utils import tensor_hash
@dataclass
class InversionOutput:
    """Output of `Unemebd.invert`."""
    preimage: th.Tensor
    grad_norm: th.Tensor
    kl: th.Tensor
    loss: th.Tensor
    nfev: int
class Unembed(th.nn.Module):
    """Module that maps transformer hidden states to logits (and vice versa)."""
    final_norm: model_surgery.Norm
    unembedding: th.nn.Linear
    def __init__(
        self,
        model: model_surgery.Model,
    ):
        """Initialize unmebed.
        Args:
            model: A HuggingFace model from which to extract the unembedding matrix.
        """
        super().__init__()
        final_norm = model_surgery.get_final_norm(model)
        unembedding_matrix = model_surgery.get_unembedding_matrix(model)
        self.final_norm = copy.deepcopy(final_norm)
        self.unembedding = copy.deepcopy(unembedding_matrix)
        # In general we don't want to finetune the unembed operation.
        self.requires_grad_(False)
    def unembedding_hash(self) -> str:
        """Hash the unmbedding matrix to identify the model."""
        parameter = self.unembedding.weight.data.detach().cpu().float().numpy()
        return tensor_hash(parameter)
    def forward(self, h: th.Tensor) -> th.Tensor:
        """Convert hidden states into logits."""
        return self.unembedding(self.final_norm(h))
    def invert(
        self,
        logits: th.Tensor,
        *,
        h0: Optional[th.Tensor] = None,
        max_iter: int = 1000,
        optimizer: Literal["lbfgs", "sgd"] = "lbfgs",
        prior_weight: float = 0.0,
        prior: Optional[Distribution] = None,
        step_size: float = 1.0,
        tol: float = 1e-3,
        weight: Optional[th.Tensor] = None,
    ) -> InversionOutput:
        """Project logits onto the image of the unemebed operation.
        When the hidden state dimension is smaller than the vocabulary size, the
        unembed operation cannot perfectly represent arbitrary logits, since its image
        is restricted to a subspace; this phenomenon is known as the softmax bottleneck
        (cf. https://arxiv.org/abs/1711.03953). Because of this, the inverse can only
        be approximate in general. Here, we use gradient-based optimization to find a
        hidden state that minimizes the KL divergence from the target distribution p to
        unembeded logits q(h): h* = argmin_h KL(p || q(h)).
        Args:
            logits: Tensor of shape `[..., vocab_size]` containing logits to invert.
            h0: Initial guess for the hidden state. If `None`, the least-squares
                solution of the linear equation xU = logits is used, where U is the
                unembedding matrix.
            max_iter: Maximum number of iterations for the optimizer to take.
            optimizer: Optimization algorithm to use. Currently, only "lbfgs" and "sgd"
                are supported.
            prior_weight: The weight of the prior distribution is given in the loss.
            prior: Prior distribution over hidden states used to regularize
                the inversion.
            step_size: The step size for the optimizer.
            tol: Tolerance for the inversion objective.
            weight: Optional tensor of shape `[..., vocab_size]` containing weights
                for each vocabulary item. If `None`, all classes are weighted equally.
        """
        d_model = cast(int, self.unembedding.in_features)
        leading_dims = logits.shape[:-1]
        if h0 is None:
            # Initialize with the Moore-Penrose pseudoinverse
            h0 = th.zeros((*leading_dims, d_model), device=logits.device)
        # Sanity check the shape of the initial hidden state. Can silently lead to
        # incorrect results due to broadcasting if we don't check this.
        elif h0.shape != (*leading_dims, d_model):
            raise ValueError(
                f"Initial hidden state has shape {h0.shape} but should have shape "
                f"{(*leading_dims, d_model)} given logits shape {logits.shape}."
            )
        h_star = th.nn.Parameter(h0)
        if optimizer == "lbfgs":
            opt = th.optim.LBFGS(
                [h_star],
                line_search_fn="strong_wolfe",
                lr=step_size,
                max_iter=max_iter,
                tolerance_change=tol,
            )
        elif optimizer == "sgd":
            opt = th.optim.SGD([h_star], lr=step_size)
        else:
            raise ValueError(f"Unknown optimizer '{optimizer}'")
        log_p = logits.log_softmax(dim=-1)
        p = log_p.exp()
        if weight is not None:
            p *= weight
        def compute_loss(h: th.Tensor) -> tuple[th.Tensor, th.Tensor]:
            log_q = self(h).log_softmax(-1)
            kl = th.sum(p * (log_p - log_q), dim=-1).nanmean()
            loss = kl.clone()
            if prior_weight and prior is not None:
                # We evaluate the prior density on the post-norm hidden state,
                # to prevent the pre-norm hidden from collapsing towards zero.
                h_ = self.final_norm(h)
                loss += prior_weight * -prior.log_prob(h_).mean()
            return loss, kl
        nfev = 0  # Number of function evals, like in scipy.optimize.minimize
        loss, kl = log_p.new_tensor(th.inf), log_p.new_tensor(th.inf)
        def closure():
            nonlocal nfev, loss, kl
            nfev += 1
            opt.zero_grad(set_to_none=False)
            loss, kl = compute_loss(h_star)
            if not loss.isfinite():
                raise RuntimeError("Inversion objective is not finite.")
            loss.backward()
            return loss
        grad_norm = log_p.new_tensor(th.inf)
        while nfev < max_iter:
            opt.step(closure)  # type: ignore
            final_grad = h_star.grad
            assert final_grad is not None
            grad_norm = final_grad.norm()
            if grad_norm < tol or loss < tol:
                break
        with th.no_grad():
            output = InversionOutput(
                preimage=self.final_norm(h_star.data),
                grad_norm=grad_norm,
                kl=kl.detach(),
                loss=loss.detach(),
                nfev=nfev,
            )
        return output

================
File: tuned_lens/plotting/__init__.py
================
"""Provides tools for plotting."""
from .prediction_trajectory import PredictionTrajectory
from .token_formatter import TokenFormatter
from .trajectory_plotting import TrajectoryLabels, TrajectoryStatistic

================
File: tuned_lens/plotting/prediction_trajectory.py
================
"""Plot a lens table for some given text and model."""
from dataclasses import dataclass
from typing import Literal, Optional, Sequence, Union
try:
    import transformer_lens as tl
    _transformer_lens_available = True
except ImportError:
    _transformer_lens_available = False
import numpy as np
import torch as th
from numpy.typing import NDArray
from transformers import (
    PreTrainedModel,
    PreTrainedTokenizer,
    PreTrainedTokenizerFast,
)
from ..nn.lenses import Lens
from .token_formatter import TokenFormatter
from .trajectory_plotting import TrajectoryLabels, TrajectoryStatistic
Tokenizer = Union[PreTrainedTokenizer, PreTrainedTokenizerFast]
ResidualComponent = Literal[
    "resid_pre", "resid_mid", "resid_post", "attn_out", "mlp_out"
]
def _select_values_along_seq_axis(values: NDArray, targets: NDArray[np.int64]):
    """Select targe values along the the vocab dimension.
    Args:
        values: (..., n_layers, seq_len, vocab_size) the values to select from.
        targets: (..., seq_len) the indices to select.
    Returns:
        (..., n_layers, seq_len) the selected values.
    """
    return np.take_along_axis(
        values,
        targets[..., None, :, None],
        axis=-1,
    ).squeeze(-1)
def _ids_to_tokens(
    ids: NDArray[np.int64],
    tokenizer: Tokenizer,
) -> NDArray[np.str_]:
    """Convert a batch of ids to tokens.
    Args:
        ids: the input ids.
        tokenizer: The tokenizer to use for decoding the input ids.
    Returns:
        A batch of tokens.
    """
    tokens = tokenizer.convert_ids_to_tokens(ids.flatten().tolist())
    tokens = np.array(tokens).reshape(ids.shape)
    return tokens
def _consolidate_labels_from_batch(
    tokens: NDArray[np.str_],
    n_batch_axes: int,
    het_token_repr: str = "*",
) -> NDArray[np.str_]:
    """Get the input labels from a batch of input ids.
    Args:
        tokens: (*batch_axes, *axes_to_keep) the input ids.
        tokenizer: The tokenizer to use for decoding the input ids.
        n_batch_axes: The batch axes for in the input ids.
        het_token_repr: The string to use when the tokens are not the same across the
            batch i.e. they are heterogeneous.
    Returns:
        (*axes_to_keep) the input labels where all items in the batch are the same
        the token is used otherwise the token is replaced with `repeated_token_repr`.
    """
    first = tokens.reshape(-1, *tokens.shape[n_batch_axes:])[0, :]
    mask = np.all(
        tokens == first.reshape((1,) * n_batch_axes + tokens.shape[n_batch_axes:]),
        axis=tuple(range(n_batch_axes)),
    )
    return np.where(mask, first, het_token_repr)
@dataclass
class PredictionTrajectory:
    """Contains the trajectory predictions for a sequence of tokens.
    A prediction trajectory is the set of next token predictions produced by the
    conjunction of a lens and a model when evaluated on a specific sequence of tokens.
    This class include multiple methods for visualizing different
    aspects of the trajectory.
    """
    log_probs: NDArray[np.float32]
    """(..., n_layers, seq_len, vocab_size) The log probabilities of the predictions
    for each hidden layer + the models logits"""
    input_ids: NDArray[np.int64]
    """(..., seq_len)"""
    targets: Optional[NDArray[np.int64]] = None
    """(..., seq_len)"""
    anti_targets: Optional[NDArray[np.int64]] = None
    """(..., seq_len)"""
    tokenizer: Optional[Tokenizer] = None
    def __post_init__(self) -> None:
        """Validate class invariants."""
        assert (
            self.log_probs.shape[:-3] == self.input_ids.shape[:-1]
        ), "Batch shapes do not match log_probs.shape: {}, input_ids.shape: {}".format(
            self.log_probs.shape, self.input_ids.shape
        )
        assert (
            self.log_probs.shape[-2] == self.input_ids.shape[-1]
        ), "seq_len doesn't match log_probs.shape: {}, input_ids.shape: {}".format(
            self.log_probs.shape, self.input_ids.shape
        )
        assert (
            self.targets is None or self.targets.shape == self.input_ids.shape
        ), "Shapes don't match targets.shape: {}, input_ids.shape: {}".format(
            self.targets.shape, self.input_ids.shape
        )
        assert (
            self.anti_targets is None or self.anti_targets.shape == self.input_ids.shape
        ), "Shapes don't match anti_targets.shape: {}, input_ids.shape: {}".format(
            self.anti_targets.shape, self.input_ids.shape
        )
    @property
    def n_batch_axis(self) -> int:
        """Returns the number of batch dimensions."""
        return len(self.batch_axes)
    @property
    def batch_axes(self) -> Sequence[int]:
        """Returns the batch axes for the trajectory."""
        return tuple(range(len(self.log_probs.shape) - 3))
    @property
    def batch_shape(self) -> Sequence[int]:
        """Returns the batch shape of the trajectory."""
        return self.log_probs.shape[:-3]
    @property
    def num_layers(self) -> int:
        """Returns the number of layers in the stream not including the model output."""
        return self.log_probs.shape[-3] - 1
    @property
    def num_tokens(self) -> int:
        """Returns the number of tokens in this slice of the sequence."""
        return self.log_probs.shape[-2]
    @property
    def vocab_size(self) -> int:
        """Returns the size of the vocabulary."""
        return self.log_probs.shape[-1]
    @property
    def model_log_probs(self) -> NDArray[np.float32]:
        """Returns the log probs of the model (..., seq_len, vocab_size)."""
        return self.log_probs[..., -1, :, :]
    @property
    def probs(self) -> NDArray[np.float32]:
        """Returns the probabilities of the predictions."""
        return np.exp(self.log_probs)
    @classmethod
    def from_lens_and_cache(
        cls,
        lens: Lens,
        input_ids: th.Tensor,
        cache: "tl.ActivationCache",
        model_logits: th.Tensor,
        targets: Optional[th.Tensor] = None,
        anti_targets: Optional[th.Tensor] = None,
        residual_component: ResidualComponent = "resid_pre",
        mask_input: bool = False,
    ) -> "PredictionTrajectory":
        """Construct a prediction trajectory from a set of residual stream vectors.
        Args:
            lens: A lens to use to produce the predictions.
            cache: the activation cache produced by running the model.
            input_ids: (..., seq_len) Ids that where input into the model.
            model_logits: (..., seq_len x d_vocab) the models final output logits.
            targets: (..., seq_len) the targets the model is should predict. Used
                for :meth:`cross_entropy` and :meth:`log_prob_diff` visualization.
            anti_targets: (..., seq_len) the incorrect label the model should not
                predict. Used for :meth:`log_prob_diff` visualization.
            residual_component: Name of the stream vector being visualized.
            mask_input: Whether to mask the input ids when computing the log probs.
        Returns:
            PredictionTrajectory constructed from the residual stream vectors.
        """
        tokenizer = cache.model.tokenizer
        traj_log_probs = []
        for layer in range(cache.model.cfg.n_layers):
            hidden = cache[residual_component, layer]
            if input_ids.shape[-1] != hidden.shape[-2]:
                raise ValueError(
                    f"Length of input ids {input_ids.shape[-1]} does "
                    f"not match cache sequence length {hidden.shape[-2]}."
                )
            logits = lens.forward(hidden, layer)
            if mask_input:
                logits[..., input_ids] = -th.finfo(hidden.dtype).max
            traj_log_probs.append(
                logits.log_softmax(dim=-1).detach().cpu().float().numpy()
            )
        model_log_probs = model_logits.log_softmax(-1).detach().cpu().float().numpy()
        traj_log_probs.append(model_log_probs)
        return cls(
            tokenizer=tokenizer,
            log_probs=np.stack(traj_log_probs, axis=-3),
            input_ids=input_ids.cpu().numpy(),
            targets=None if targets is None else targets.cpu().numpy(),
            anti_targets=None if anti_targets is None else anti_targets.cpu().numpy(),
        )
    @classmethod
    def from_lens_and_model(
        cls,
        lens: Lens,
        model: PreTrainedModel,
        input_ids: Sequence[int],
        tokenizer: Optional[Tokenizer] = None,
        targets: Optional[Sequence[int]] = None,
        anti_targets: Optional[Sequence[int]] = None,
        mask_input: bool = False,
    ) -> "PredictionTrajectory":
        """Construct a prediction trajectory from a set of residual stream vectors.
        Args:
            lens: A lens to use to produce the predictions. Note this should be
                compatible with the model.
            model: A Hugging Face causal language model to use to produce
                the predictions.
            tokenizer: The tokenizer to use for decoding the input ids.
            input_ids: (seq_len) Ids that where input into the model.
            targets: (seq_len) the targets the model is should predict. Used
                for :meth:`cross_entropy` and :meth:`log_prob_diff` visualization.
            anti_targets: (seq_len) the incorrect label the model should not
                predict. Used for :meth:`log_prob_diff` visualization.
            residual_component: Name of the stream vector being visualized.
            mask_input: Whether to mask the input ids when computing the log probs.
        Returns:
            PredictionTrajectory constructed from the residual stream vectors.
        """
        with th.no_grad():
            input_ids_th = th.tensor(input_ids, dtype=th.int64, device=model.device)
            outputs = model(input_ids_th.unsqueeze(0), output_hidden_states=True)
        # Slice arrays the specified range
        model_log_probs = (
            outputs.logits[..., :]
            .log_softmax(-1)
            .squeeze()
            .detach()
            .cpu()
            .float()
            .numpy()
        )
        stream = list(outputs.hidden_states)
        input_ids_np = np.array(input_ids)
        targets_np = np.array(targets) if targets is not None else None
        anti_targets_np = np.array(anti_targets) if anti_targets is not None else None
        # Create the stream of log probabilities from the lens
        traj_log_probs = []
        for i, h in enumerate(stream[:-1]):
            logits = lens.forward(h, i)
            if mask_input:
                logits[..., input_ids_np] = -th.finfo(h.dtype).max
            traj_log_probs.append(
                logits.log_softmax(dim=-1).squeeze().detach().cpu().float().numpy()
            )
        # Add model predictions
        traj_log_probs.append(model_log_probs)
        return cls(
            tokenizer=tokenizer,
            log_probs=np.array(traj_log_probs),
            targets=targets_np,
            input_ids=input_ids_np,
            anti_targets=anti_targets_np,
        )
    def _get_sequence_labels(
        self, token_formatter: Optional[TokenFormatter] = None
    ) -> Optional[NDArray[np.str_]]:
        """Get the input labels from a batch of input ids."""
        if self.tokenizer is None:
            return None
        if token_formatter is None:
            token_formatter = TokenFormatter()
        return _consolidate_labels_from_batch(
            tokens=token_formatter.vectorized_format(
                _ids_to_tokens(self.input_ids, self.tokenizer)
            ),
            n_batch_axes=self.n_batch_axis,
        )
    def _get_topk_tokens_and_values(
        self,
        k: int,
        sort_by: NDArray[np.float32],
        values: NDArray[np.float32],
    ) -> tuple[NDArray[np.str_], NDArray[np.float32]]:
        """Get the top-k tokens according to sort_by for each layer and position.
        Args:
            k: The number of top tokens to get.
            sort_by: (..., n_layers, seq_len) the values to sort by to get the top-k
            values: (..., n_layers, seq_len) the values to get the top-k tokens for.
        Returns:
            * (..., n_layers, seq_len, k) the top-k tokens for each layer and position.
            * (..., n_layers, seq_len, k) the top-k values for each layer and position.
        """
        assert self.tokenizer is not None
        # Get the top-k tokens & probabilities for each
        topk_inds = np.argpartition(sort_by, -k, axis=-1)[..., -k:]
        topk_sort_by = np.take_along_axis(sort_by, topk_inds, axis=-1)
        topk_values = np.take_along_axis(values, topk_inds, axis=-1)
        # Ensure that the top-k tokens are sorted by probability
        sorted_top_k_inds = np.argsort(-topk_sort_by, axis=-1)
        topk_inds = np.take_along_axis(topk_inds, sorted_top_k_inds, axis=-1)
        topk_values = np.take_along_axis(topk_values, sorted_top_k_inds, axis=-1)
        topk_tokens = _ids_to_tokens(topk_inds, self.tokenizer)
        return topk_tokens, topk_values
    def _hover_over_entries(
        self,
        topk_tokens: NDArray[np.str_],
        topk_values: NDArray[np.str_],
        max_entries_to_show: int = 3,
    ) -> NDArray[np.str_]:
        """Get the hover over entries for the stream.
        Args:
            topk_tokens: (..., n_layers, seq_len, k) the top-k tokens for each layer and
                position.
            topk_values: (..., n_layers, seq_len, k) the top-k values associated with
                each token.
            max_entries_to_show: The maximum number of entries in the batch to show in
                the hover over menu.
        Returns:
            (n_layers, seq_len, batch, 2*k) the table of entries to show when hovering
            over the stream. Here `batch` is the minimum of the batch size and the
            `max_entries_to_show`.
        """
        k = topk_tokens.shape[-1]
        topk_tokens = topk_tokens.reshape(-1, self.num_layers + 1, self.num_tokens, k)
        topk_values = topk_values.reshape(-1, self.num_layers + 1, self.num_tokens, k)
        topk_tokens = np.moveaxis(topk_tokens, 0, -1)
        topk_values = np.moveaxis(topk_values, 0, -1)
        hover_over_entries = np.empty(
            topk_tokens.shape[:-1] + (2 * topk_tokens.shape[-1],),
            dtype=topk_tokens.dtype,
        )
        hover_over_entries[..., 0::2] = topk_tokens
        hover_over_entries[..., 1::2] = topk_values
        return hover_over_entries[..., : 2 * max_entries_to_show]
    def _largest_prob_labels(
        self,
        formatter: Optional[TokenFormatter] = None,
        min_prob: float = 0,
        topk: int = 10,
        max_entries_to_show: int = 3,
    ) -> Optional[TrajectoryLabels]:
        """Labels for the prediction trajectory based on the most probable tokens.
        Args:
            formatter : The formatter to use for formatting the tokens.
            min_prob : The minimum probability for a token to used as a label.
            topk : The number of top tokens to include in the hover over menu.
            max_entries_to_show : The number of items in the batch to show in the
                hover over menu.
            show_values : Whether to show the probability values in the hover over
        Returns:
            A set of stream labels that can be applied to a trajectory statistic or
            None if the tokenizer is not set.
        """
        if self.tokenizer is None:
            return None
        if formatter is None:
            formatter = TokenFormatter()
        topk_tokens, topk_probs = self._get_topk_tokens_and_values(
            k=topk, sort_by=self.log_probs, values=self.probs
        )
        # Create the labels for the stream
        top_tokens = topk_tokens[..., 0]
        top_probs = topk_probs[..., 0]
        label_strings = _consolidate_labels_from_batch(
            tokens=formatter.vectorized_format(top_tokens),
            n_batch_axes=self.n_batch_axis,
        )
        label_strings = np.where((top_probs > min_prob).all(), label_strings, "")
        topk_probs_formatted = np.char.add(np.char.mod("%.2f", topk_probs * 100), "%")
        topk_tokens_formatted = formatter.vectorized_format(topk_tokens)
        topk_probs_formatted = np.char.add(np.char.mod("%.2f", topk_probs * 100), "%")
        topk_tokens_formatted = formatter.vectorized_format(topk_tokens)
        return TrajectoryLabels(
            label_strings=label_strings,
            hover_over_entries=self._hover_over_entries(
                topk_tokens=topk_tokens_formatted,
                topk_values=topk_probs_formatted,
                max_entries_to_show=max_entries_to_show,
            ),
        )
    def _largest_delta_in_prob_labels(
        self,
        other: "PredictionTrajectory",
        formatter: Optional[TokenFormatter] = None,
        min_prob_delta: float = 0,
        max_entries_to_show: int = 3,
        topk: int = 10,
    ) -> Optional[TrajectoryLabels]:
        """Labels for a trajectory statistic based on the largest change in probability.
        Args:
            other : The other prediction trajectory to compare to.
            formatter : A TokenFormatter to use for formatting the labels.
            min_prob_delta : The minimum change in probability to include a label.
            topk : The number of top tokens to include in the hover over menu.
            max_entries_to_show: The maximum number of entries in the batch to show in
                the hover over menu.
        Returns:
            A set of stream labels that can be added to a trajectory statistic.
        """
        if self.tokenizer is None:
            return None
        if formatter is None:
            formatter = TokenFormatter()
        deltas = other.probs - self.probs
        topk_tokens, topk_deltas = self._get_topk_tokens_and_values(
            k=topk, sort_by=np.abs(deltas), values=deltas
        )
        top_deltas = topk_deltas[..., 0]
        topk_tokens_formatted = formatter.vectorized_format(topk_tokens)
        top_tokens_formatted = topk_tokens_formatted[..., 0]
        topk_deltas_formatted = np.char.add(
            np.char.add("Î”", np.char.mod("%.2f", topk_deltas * 100)), "%"
        )
        label_strings = np.where(
            np.abs(top_deltas) > min_prob_delta,
            top_tokens_formatted,
            "",
        )
        label_strings = _consolidate_labels_from_batch(
            tokens=top_tokens_formatted,
            n_batch_axes=self.n_batch_axis,
        )
        return TrajectoryLabels(
            label_strings=label_strings,
            hover_over_entries=self._hover_over_entries(
                topk_tokens=topk_tokens_formatted,
                topk_values=topk_deltas_formatted,
                max_entries_to_show=max_entries_to_show,
            ),
        )
    def slice_sequence(self, slice: slice) -> "PredictionTrajectory":
        """Create a slice of the prediction trajectory along the sequence dimension."""
        return PredictionTrajectory(
            log_probs=self.log_probs[..., slice, :],
            input_ids=self.input_ids[..., slice],
            targets=self.targets[..., slice] if self.targets is not None else None,
            anti_targets=self.anti_targets[..., slice]
            if self.anti_targets is not None
            else None,
            tokenizer=self.tokenizer,
        )
    def cross_entropy(self, **kwargs) -> TrajectoryStatistic:
        """The cross entropy of the predictions to the targets.
        Args:
            **kwargs: are passed to largest_prob_labels.
        Returns:
            A TrajectoryStatistic with the cross entropy of the predictions to the
            targets.
        """
        if self.targets is None:
            raise ValueError("Cannot compute cross entropy without targets.")
        stats = -_select_values_along_seq_axis(self.log_probs, self.targets)
        if self.n_batch_axis:
            stats = stats.mean(axis=self.batch_axes)
        return TrajectoryStatistic(
            name="Cross Entropy",
            units="nats",
            trajectory_labels=self._largest_prob_labels(**kwargs),
            sequence_labels=self._get_sequence_labels(),
            stats=stats,
        )
    def rank(self, show_ranks=False, **kwargs) -> TrajectoryStatistic:
        """The rank of the targets among the predictions.
        That is, if the target is the most likely prediction, its rank is 1;
        the second most likely has rank 2, etc.
        Args:
            show_ranks: Whether to show the the rank of the target or the top token.
            **kwargs: are passed to largest_prob_labels.
        Returns:
            A TrajectoryStatistic with the rank of the targets among the predictions.
        """
        if self.targets is None:
            raise ValueError("Cannot compute rank without targets.")
        # Yes I know this is not the most efficient way to do this
        idx_of_kth_likeliest_token = np.argsort(-self.log_probs, axis=-1)
        ranks = np.argsort(idx_of_kth_likeliest_token, axis=-1) + 1
        targets_rank = _select_values_along_seq_axis(ranks, self.targets)
        if self.n_batch_axis:
            targets_rank = targets_rank.mean(axis=self.batch_axes)
        trajectory_labels = self._largest_prob_labels(**kwargs)
        if show_ranks and trajectory_labels is not None:
            trajectory_labels.label_strings = np.char.mod("%d", targets_rank)
        return TrajectoryStatistic(
            name="Rank",
            units="",
            trajectory_labels=trajectory_labels,
            sequence_labels=self._get_sequence_labels(),
            stats=targets_rank,
            min=1,
            max=None if self.tokenizer is None else self.tokenizer.vocab_size,
        )
    def entropy(self, **kwargs) -> TrajectoryStatistic:
        """The entropy of the predictions.
        Args:
            **kwargs: are passed to largest_prob_labels.
        Returns:
            A TrajectoryStatistic with the entropy of the predictions.
        """
        stats = -np.sum(self.probs * self.log_probs, axis=-1)
        if self.n_batch_axis:
            stats = stats.mean(axis=self.batch_axes)
        return TrajectoryStatistic(
            name="Entropy",
            units="nats",
            trajectory_labels=self._largest_prob_labels(**kwargs),
            sequence_labels=self._get_sequence_labels(),
            stats=stats,
        )
    def forward_kl(self, **kwargs) -> TrajectoryStatistic:
        """KL divergence of the lens predictions to the model predictions.
        Args:
            **kwargs: are passed to largest_prob_labels.
        Returns:
            A TrajectoryStatistic with the KL divergence of the lens predictions to the
            final output of the model.
        """
        model_log_probs = self.model_log_probs[..., np.newaxis, :, :]
        stats = np.sum(
            np.exp(model_log_probs) * (model_log_probs - self.log_probs), axis=-1
        )
        if self.n_batch_axis:
            stats = stats.mean(axis=self.batch_axes)
        return TrajectoryStatistic(
            name="Forward KL",
            units="nats",
            trajectory_labels=self._largest_prob_labels(**kwargs),
            sequence_labels=self._get_sequence_labels(),
            stats=stats,
        )
    def log_prob_diff(self, delta: bool = False) -> TrajectoryStatistic:
        """The difference in logits between two tokens.
        Returns:
            The difference between the log probabilities of the two tokens.
        """
        # TODO implement this as a way to compare two distributions
        if self.targets is None or self.anti_targets is None:
            raise ValueError(
                "Cannot compute log prob diff without targets" " and anti_targets."
            )
        targets_log_probs = _select_values_along_seq_axis(self.log_probs, self.targets)
        anti_targets_log_probs = _select_values_along_seq_axis(
            self.log_probs, self.anti_targets
        )
        stats = targets_log_probs - anti_targets_log_probs
        if delta:
            stats = stats[..., 1:, :] - stats[..., :-1, :]
        if self.n_batch_axis:
            stats = stats.mean(axis=self.batch_axes)
        return TrajectoryStatistic(
            name="Î” Log Prob Difference" if delta else "Log Prob Difference",
            units="nats",
            includes_output=not delta,
            sequence_labels=self._get_sequence_labels(),
            stats=stats,
        )
    def max_probability(self, **kwargs) -> TrajectoryStatistic:
        """Max probability of the among the predictions.
        Args:
            **kwargs: are passed to largest_prob_labels.
        Returns:
            A TrajectoryStatistic with the max probability of the among the predictions.
        """
        stats = np.exp(self.log_probs.max(-1))
        if self.n_batch_axis:
            stats = stats.mean(axis=self.batch_axes)
        return TrajectoryStatistic(
            name="Max Probability",
            units="probs",
            trajectory_labels=self._largest_prob_labels(**kwargs),
            sequence_labels=self._get_sequence_labels(),
            stats=stats,
        )
    def kl_divergence(
        self, other: "PredictionTrajectory", **kwargs
    ) -> TrajectoryStatistic:
        """Compute the KL divergence between self and other prediction trajectory.
        Args:
            other : The other prediction trajectory to compare to.
            **kwargs: are passed to largest_delta_in_prob_labels.
        Returns:
            A TrajectoryStatistic with the KL divergence between self and other.
        """
        kl_div = np.sum(self.probs * (self.log_probs - other.log_probs), axis=-1)
        if self.n_batch_axis:
            kl_div = kl_div.mean(axis=self.batch_axes)
        return TrajectoryStatistic(
            name="KL(Self | Other)",
            units="nats",
            stats=kl_div,
            trajectory_labels=self._largest_delta_in_prob_labels(other, **kwargs),
            sequence_labels=self._get_sequence_labels(),
            min=0,
            max=None,
        )
    def js_divergence(
        self, other: "PredictionTrajectory", **kwargs
    ) -> TrajectoryStatistic:
        """Compute the JS divergence between self and other prediction trajectory.
        Args:
            other : The other prediction trajectory to compare to.
            **kwargs: are passed to largest_delta_in_prob_labels.
        Returns:
            A TrajectoryStatistic with the JS divergence between self and other.
        """
        js_div = 0.5 * np.sum(
            self.probs * (self.log_probs - other.log_probs), axis=-1
        ) + 0.5 * np.sum(other.probs * (other.log_probs - self.log_probs), axis=-1)
        if self.n_batch_axis:
            js_div = js_div.mean(axis=self.batch_axes)
        return TrajectoryStatistic(
            name="JS(Self | Other)",
            units="nats",
            stats=js_div,
            trajectory_labels=self._largest_delta_in_prob_labels(other, **kwargs),
            sequence_labels=self._get_sequence_labels(),
            min=0,
            max=None,
        )
    def total_variation(
        self, other: "PredictionTrajectory", **kwargs
    ) -> TrajectoryStatistic:
        """Total variation distance between self and other prediction trajectory.
        Args:
            other : The other prediction trajectory to compare to.
            **kwargs: are passed to largest_delta_in_prob_labels.
        Returns:
            A TrajectoryStatistic with the total variational distance between
            self and other.
        """
        t_var = np.abs(self.probs - other.probs).max(axis=-1)
        if self.n_batch_axis:
            t_var = t_var.mean(axis=self.batch_axes)
        return TrajectoryStatistic(
            name="TV(Self | Other)",
            units="probs",
            stats=t_var,
            trajectory_labels=self._largest_delta_in_prob_labels(other, **kwargs),
            sequence_labels=self._get_sequence_labels(),
            min=0,
            max=1,
        )

================
File: tuned_lens/plotting/token_formatter.py
================
"""Contains a class for formatting tokens for display in plots."""
from dataclasses import dataclass
from typing import Optional
import numpy as np
@dataclass
class TokenFormatter:
    """Format tokens for display in a plots."""
    ellipsis: str = "â€¦"
    newline_replacement: str = "\\n"
    newline_token: str = "ÄŠ"
    whitespace_token: str = "Ä "
    whitespace_replacement: str = "_"
    max_string_len: Optional[int] = 7
    def __post_init__(self) -> None:
        """Post init hook to vectorize the format function."""
        self.vectorized_format = np.vectorize(self.format)
    def format(self, token: str) -> str:
        """Format a token for display in a plot."""
        if not isinstance(token, str):
            return "<unk>"
        if self.max_string_len is not None and len(token) > self.max_string_len:
            token = token[: self.max_string_len - len(self.ellipsis)] + self.ellipsis
        token = token.replace(self.newline_token, self.newline_replacement)
        token = token.replace(self.whitespace_token, self.whitespace_replacement)
        return token
    def pad_token_repr_to_max_len(self, token_repr: str) -> str:
        """Pad a token representation to the max string length."""
        if self.max_string_len is None:
            return token_repr
        return token_repr[: self.max_string_len] + " " * (
            self.max_string_len - len(token_repr)
        )

================
File: tuned_lens/plotting/trajectory_plotting.py
================
"""Contains utility classes for creating heatmap visualizations."""
from dataclasses import dataclass, replace
from typing import Any, Dict, Optional, Tuple
import numpy as np
from numpy.typing import NDArray
from plotly import graph_objects as go
def trunc_string_left(string: str, new_len: int) -> str:
    """Truncate a string to the left."""
    return " " * (new_len - len(string)) + string[-new_len:]
@dataclass
class TrajectoryLabels:
    """Contains sets of labels for each layer and position in the residual stream."""
    label_strings: NDArray[np.str_]
    """(n_layers x sequence_length) label for each layer and position in the stream."""
    hover_over_entries: Optional[NDArray[np.str_]] = None
    """(n_layers x sequence_length x rows x cols) table of strings to display when
        hovering over a cell. For example, the top k prediction from the lens."""
    def stride(self, stride: int) -> "TrajectoryLabels":
        """Return a new TrajectoryLabels with the given stride.
        Args:
            stride : The number of layers between each layer we keep.
        Returns:
            A new TrajectoryLabels with the given stride.
        """
        assert stride > 0, f"stride must be positive, got {stride}"
        return replace(
            self,
            label_strings=_stride_keep_last(self.label_strings, stride),
            hover_over_entries=None
            if self.hover_over_entries is None
            else _stride_keep_last(self.hover_over_entries, stride),
        )
    def template_and_customdata(
        self, col_width_limit: int = 10
    ) -> Tuple[str, NDArray[np.str_]]:
        """Construct a template for use with Plotly's hovertemplate."""
        assert self.hover_over_entries is not None
        n_rows, n_cols = self.hover_over_entries.shape[-2:]
        vec_str_len = np.vectorize(len)
        lengths = vec_str_len(self.hover_over_entries)
        max_col_lens = np.max(lengths, axis=(0, 1, 2), keepdims=True)
        max_col_lens = np.minimum(max_col_lens + 1, col_width_limit)
        vec_truncate = np.vectorize(trunc_string_left)
        truncated_entries = vec_truncate(self.hover_over_entries, max_col_lens)
        html_table = ""
        for row in range(n_rows):
            for col in range(n_cols):
                html_table += f"%{{customdata[{row*n_cols + col}]}}"
            html_table += "<br>"
        html_table += "<extra></extra>"
        customdata = truncated_entries.reshape(
            self.hover_over_entries.shape[:2] + (-1,)
        )
        return html_table, customdata
@dataclass
class TrajectoryStatistic:
    """This class represents a trajectory statistic that can be visualized.
    For example, the entropy of the lens predictions at each layer.
    """
    name: str
    """The name of the statistic. For example, "entropy"."""
    stats: NDArray[np.float32]
    """(n_layers x sequence_length) value of the statistic across layer and position."""
    sequence_labels: Optional[NDArray[np.str_]] = None
    """(sequence_length) labels for the sequence dimension e.g. input tokens."""
    trajectory_labels: Optional[TrajectoryLabels] = None
    """Labels for each layer and position in the stream. For example, the top 1
    prediction from the lens at each layer."""
    units: Optional[str] = None
    """The units of the statistic."""
    max: Optional[float] = None
    """The maximum value of the statistic."""
    min: Optional[float] = None
    """The minimum value of the statistic."""
    includes_output: bool = True
    """Whether the statistic includes the final output layer."""
    _layer_labels: Optional[NDArray[np.str_]] = None
    def __post_init__(self) -> None:
        """Validate class invariants."""
        assert len(self.stats.shape) == 2, f"{self.stats.shape} != (n_layers, seq_len)"
        assert self.trajectory_labels is None or (
            self.trajectory_labels.label_strings.shape == self.stats.shape
        ), f"{self.trajectory_labels.label_strings.shape} != {self.stats.shape}"
        assert self.sequence_labels is None or (
            self.sequence_labels.shape[-1] == self.stats.shape[-1]
        ), f"{self.sequence_labels.shape[-1]} != {self.stats.shape[-1]}"
        if self._layer_labels is None:
            if self.includes_output:
                self._layer_labels = np.array(
                    [*map(str, range(self.stats.shape[0] - 1)), "output"]
                )
            else:
                self._layer_labels = np.array([*map(str, range(self.stats.shape[0]))])
    def clip(self, min: float, max: float) -> "TrajectoryStatistic":
        """Return a new TrajectoryStatistic with the given min and max.
        Args:
            min : The minimum value to clip to.
            max : The maximum value to clip to.
        Returns:
            A new TrajectoryStatistic with the given min and max.
        """
        assert min < max, f"min must be less than max, got {min} >= {max}"
        return replace(
            self,
            stats=np.clip(self.stats, min, max),
            max=max,
            min=min,
        )
    def stride(self, stride: int) -> "TrajectoryStatistic":
        """Return a new TrajectoryStatistic with the given stride.
        Args:
            stride : The number of layers between each layer we keep.
        Returns:
            A new TrajectoryStatistic with the given stride.
        """
        assert stride > 0, f"stride must be positive, got {stride}"
        assert self._layer_labels is not None
        return replace(
            self,
            stats=_stride_keep_last(self.stats, stride),
            trajectory_labels=None
            if self.trajectory_labels is None
            else self.trajectory_labels.stride(stride),
            _layer_labels=None
            if self._layer_labels is None
            else _stride_keep_last(self._layer_labels, stride),
        )
    def heatmap(
        self,
        colorscale: str = "rdbu_r",
        log_scale: bool = False,
        **kwargs,
    ) -> go.Heatmap:
        """Returns a Plotly Heatmap object for this statistic.
        Args:
            colorscale : The colorscale to use for the heatmap.
            log_scale : Whether to use a log scale for the colorbar.
            **kwargs : Additional keyword arguments to pass to the Heatmap constructor.
        Returns:
            A plotly Heatmap where the x-axis is the sequence dimension, the y-axis is
            the layer dimension, and the color of each cell is the value of
            the statistic.
        """
        max = self.max if self.max is not None else np.max(self.stats)
        min = self.min if self.min is not None else np.min(self.stats)
        heatmap_kwargs: Dict[str, Any] = dict(
            y=self._layer_labels,
            z=self.stats if not log_scale else np.log10(self.stats),
            colorbar=dict(
                title=f"{self.name} ({self.units})",
                titleside="right",
            ),
            colorscale=colorscale,
            zmax=max if not log_scale else np.log10(max),
            zmin=min if not log_scale else np.log10(min),
        )
        if log_scale:
            smallest_tick = np.ceil(np.log10(min))
            biggest_tick = np.floor(np.log10(max))
            tickvals = np.arange(smallest_tick, biggest_tick + 1)
            heatmap_kwargs["colorbar"] = dict(
                tickmode="array",
                tickvals=tickvals,
                ticktext=["10^{}".format(i) for i in tickvals],
            )
        if self.sequence_labels is not None:
            # Hack to ensure that Plotly doesn't de-duplicate the x-axis labels
            x_labels = [x + "\u200c" * i for i, x in enumerate(self.sequence_labels)]
            heatmap_kwargs.update(x=x_labels)
        if self.trajectory_labels is not None:
            heatmap_kwargs.update(
                text=self.trajectory_labels.label_strings,
                texttemplate="<b>%{text}</b>",
            )
            if self.trajectory_labels.hover_over_entries is not None:
                (
                    hovertemplate,
                    custom_data,
                ) = self.trajectory_labels.template_and_customdata()
                heatmap_kwargs.update(
                    hoverlabel=dict(bgcolor="rgb(42, 42, 50)", font_family="Monospace"),
                    customdata=custom_data,
                    hovertemplate=hovertemplate,
                )
        heatmap_kwargs.update(kwargs)
        return go.Heatmap(**heatmap_kwargs)
    def figure(
        self,
        title: str = "",
        colorscale: str = "rdbu_r",
        token_width: int = 80,
    ) -> go.Figure:
        """Produce a heatmap plot of the statistic.
        Args:
            title : The title of the plot.
            colorscale : The colorscale to use for the heatmap.
            token_width : The width of each token in the plot.
        Returns:
            The plotly heatmap figure.
        """
        heatmap = self.heatmap(colorscale)
        figure_width = 200 + token_width * self.stats.shape[1]
        fig = go.Figure(heatmap).update_layout(
            title_text=title,
            title_x=0.5,
            width=figure_width,
            xaxis_title="Input",
            yaxis_title="Layer",
        )
        return fig
def _stride_keep_last(x: NDArray, stride: int) -> NDArray:
    return np.concatenate([x[:-1:stride], [x[-1]]])

================
File: tuned_lens/scripts/__init__.py
================
"""Implementations of subcommands."""

================
File: tuned_lens/scripts/eval_loop.py
================
"""Evaluation loop for the tuned lens model."""
import json
import logging
from collections import defaultdict
from dataclasses import dataclass
from itertools import islice
from pathlib import Path
from typing import Literal, Optional
import torch as th
from simple_parsing import field
from tqdm.auto import tqdm
from transformers import PreTrainedModel
from tuned_lens.nn.lenses import Lens, LogitLens, TunedLens
from tuned_lens.scripts.ingredients import (
    Data,
    Distributed,
    Model,
)
from tuned_lens.stats import LogitStats
from tuned_lens.utils import (
    maybe_all_reduce,
    pytree_map,
    pytree_stack,
    shift_labels,
    shift_preds,
)
LensType = Literal["logit", "tuned"]
logger = logging.getLogger(__name__)
def _nested_dict():
    return defaultdict(_nested_dict)
@dataclass
class Eval:
    """Type hinting for CLI args."""
    data: Data
    model: Model
    dist: Distributed
    output: Path = field(alias=["-o"])
    """Folder to save the eval results to."""
    lens_name: Optional[str] = field(alias=["-l"], default=None)
    """Path to the tuned lens model to evaluate. Defaults to None."""
    logit: bool = True
    """Whether to evaluate the logit lens"""
    seed: int = 42
    """Random seed used for data shuffling."""
    tokens: Optional[int] = None
    """Number of tokens to evaluate on. If None, will use the entire dataset."""
    token_shift: int = field(default=1)
    """How to shift the labels wrt the input tokens (1 = next token, 0 = current token,
    -1 = previous token, etc.)"""
    per_gpu_batch_size: int = 1
    """Number of samples to try to fit on a GPU at once."""
    layer_transfer: bool = field(action="store_true")
    """Evaluate the transfer of the lens to different layers of the transformer."""
    record_logit_stats: bool = field(action="store_true")
    """Record the statistics of the marginal token distribution at each layer."""
    def load_lens(self, model: PreTrainedModel) -> dict[str, Lens]:
        """Load the tuned lens model."""
        lenses = {}
        if self.logit:
            lenses["logit"] = LogitLens.from_model(model)
        if self.lens_name is not None:
            lenses["tuned"] = TunedLens.from_model_and_pretrained(model, self.lens_name)
        return lenses
    def calculate_batch_limit(self, tokens_per_sample: int):
        """Calculate the total number of batches to evaluate on."""
        assert self.tokens is not None
        global_batch_size = self.dist.world_size * self.per_gpu_batch_size
        tokens_per_batch = global_batch_size * tokens_per_sample
        return self.tokens // tokens_per_batch
    def _initialize_logit_stats_recorders(
        self, lenses: dict[str, Lens], total_layers: int
    ):
        if self.record_logit_stats:
            self.logit_stats_recorders = {
                lens_type: {f"layer_{i}": LogitStats() for i in range(total_layers)}
                for lens_type in lenses.keys()
            }
            self.logit_stats_recorder_final = LogitStats()
        else:
            self.logit_stats_recorders = None
            self.logit_stats_recorder_final = None
    def _record_logit_stats(self, logp: th.Tensor, layer: int, lens_type: str):
        if self.logit_stats_recorders is not None:
            self.logit_stats_recorders[lens_type][f"layer_{layer}"].update(
                logp, assume_normalized=True
            )
    def _record_logit_stats_final(self, logp: th.Tensor):
        if self.logit_stats_recorder_final is not None:
            self.logit_stats_recorder_final.update(logp, assume_normalized=True)
    def _save_logit_stats(self) -> defaultdict:
        logit_stats = _nested_dict()
        if self.logit_stats_recorders is not None:
            for lens_type, recorders in self.logit_stats_recorders.items():
                for layer, recorder in recorders.items():
                    recorder.all_reduce_()
                    logit_stats[lens_type]["logit_stats"][layer] = (
                        recorder.marginal_probs.cpu().numpy().tolist()
                    )
        if self.logit_stats_recorder_final is not None:
            self.logit_stats_recorder_final.all_reduce_()
            logit_stats["baseline"]["logit_stats"]["final"] = (
                self.logit_stats_recorder_final.marginal_probs.cpu().numpy().tolist()
            )
        return logit_stats
    def _evaluate_lenses_on_hidden(
        self,
        lenses: dict[str, Lens],
        hidden: th.Tensor,
        layer: int,
        final_probs: th.Tensor,
        final_lps: th.Tensor,
        labels: th.Tensor,
        batch_output: defaultdict,
        total_layers: int,
    ):
        """Evaluate a lens at a given layer. Batch output is modified in place.
        Args:
            lenses: The dictionary of lenses to evaluate on this hidden state.
            hidden: (batch x seq x d_model) The hidden states of the transformer.
            layer: The layer this hidden state is from.
            final_probs: (batch x seq x vocab) The final probabilities of
                the transformer.
            final_lps: (batch x seq x vocab) The final log probabilities
                of the transformer.
            labels: (batch x seq) The labels for the transformer.
            batch_output: Where to store the logging results.
            total_layers: The total number of layers in the transformer.
            logp_stats: where to record the logging results.
        """
        for lens_type, lens in lenses.items():
            layer_name = f"layer_{layer}"
            lens_lps = lens(hidden, idx=layer).log_softmax(dim=-1)
            lens_probs = lens_lps.exp()
            self._record_logit_stats(lens_lps, layer, lens_type)
            batch_output[lens_type]["ce"][layer_name] = th.nn.functional.cross_entropy(
                shift_preds(lens_lps, self.token_shift).flatten(0, 1),
                labels.flatten(),
                reduction="none",
            )
            batch_output[lens_type]["entropy"][layer_name] = th.sum(
                -lens_probs * lens_lps, dim=-1
            )
            batch_output[lens_type]["kl"][layer_name] = th.sum(
                final_probs * (final_lps - lens_lps), dim=-1
            )
            if self.layer_transfer:
                for i in range(total_layers):
                    trans_name = f"layer_{i}"
                    transfer_lps = lens(hidden, idx=i).log_softmax(dim=-1)
                    batch_output[lens_type]["layer_transfer"]["ce"][trans_name][
                        layer_name
                    ] = th.nn.functional.cross_entropy(
                        shift_preds(transfer_lps, self.token_shift).flatten(0, 1),
                        labels.flatten(),
                    )
                    batch_output[lens_type]["layer_transfer"]["kl"][trans_name][
                        layer_name
                    ] = th.sum(lens_probs * (lens_lps - transfer_lps), dim=-1).mean()
    @th.autocast("cuda", enabled=th.cuda.is_available())
    @th.no_grad()
    def execute(self):
        """Evaluates a TunedLens model against a transformer on a dataset."""
        # Load model, tokenizer, data, and lens
        self.dist.init()
        model = tokenizer = data = lenses = nats_to_bpb = None
        # See comment in train_loop.py for why we do this
        load_device = self.dist.device if not self.dist.fsdp else None
        if self.dist.primary:
            # Let the primary processes populate the cache
            model, tokenizer = self.model.load(load_device)
            data, nats_to_bpb = self.data.load(tokenizer)
            lenses = self.load_lens(model)
        self.dist.barrier()  # Wait for primary to finish filling the cache
        if not self.dist.primary:
            # Let the non-primary processes load from the cache
            model, tokenizer = self.model.load(load_device, must_use_cache=True)
            data, nats_to_bpb = self.data.load(tokenizer)
            lenses = self.load_lens(model)
        assert model and tokenizer and data and lenses and nats_to_bpb
        model = self.dist.shard_model(model)
        # Note since we are not training we can just move the lens to the device.
        # No need to use DDP
        lenses = {name: lens.to(self.dist.device) for name, lens in lenses.items()}
        dl = self.dist.dataloader(data)
        dl.seed(self.seed)
        for lens in lenses.values():
            lens.eval()
        if self.tokens is not None:
            tokens_per_sample = len(data[0]["input_ids"])
            if self.tokens > len(data) * tokens_per_sample:
                raise ValueError(
                    f"Requested to evaluate on {self.tokens} tokens, "
                    f"but dataset only contains {len(data)*tokens_per_sample} tokens."
                )
            batch_limit = self.calculate_batch_limit(tokens_per_sample)
            assert batch_limit > 0, "Batch limit must be positive."
            dl = islice(dl, batch_limit)
            total = batch_limit
        else:
            total = len(data) // self.dist.world_size
        L = model.config.num_hidden_layers
        self._initialize_logit_stats_recorders(lenses, L)
        root_dir = self.output
        root_dir.mkdir(exist_ok=True, parents=True)
        batches = []
        self.dist.barrier()
        logger.info(
            f"All processes initialized. Running evaluation on {total} batches."
        )
        pbar = tqdm(dl, desc="Evaluating", position=self.dist.rank, total=total)
        for batch in pbar:
            batch = self.dist.send_to_device(batch)
            output = model(**batch, output_hidden_states=True)
            hidden_states = output.hidden_states[:-1]
            final_lps = output.logits.log_softmax(dim=-1)
            final_probs = final_lps.exp()
            assert not th.isnan(output.logits).any(), "Logits are NaN"
            labels = shift_labels(batch["input_ids"], self.token_shift)
            batch_output = _nested_dict()
            # Compute tuned lens eval and statistics if applicable
            for j, h in zip(range(L), hidden_states):
                self._evaluate_lenses_on_hidden(
                    lenses=lenses,
                    hidden=h,
                    layer=j,
                    final_probs=final_probs,
                    final_lps=final_lps,
                    labels=labels,
                    batch_output=batch_output,
                    total_layers=L,
                )
            batch_output["baseline"]["ce"]["final"] = th.nn.functional.cross_entropy(
                shift_preds(final_lps, self.token_shift).flatten(0, 1),
                labels.flatten(),
                reduction="none",
            )
            batch_output["baseline"]["entropy"]["final"] = th.sum(
                -final_probs * final_lps, dim=-1
            )
            batches.append(pytree_map(th.mean, batch_output))  # type: ignore[arg-type]
            self._record_logit_stats_final(final_lps)
        pbar.close()
        agg = pytree_map(lambda x: nats_to_bpb * x.mean(), pytree_stack(batches))
        agg = pytree_map(lambda x: maybe_all_reduce(x), agg)
        agg = pytree_map(lambda x: x.cpu().numpy().item(), agg)
        assert isinstance(agg, dict)
        batches = pytree_map(lambda x: nats_to_bpb * x, batches)
        batches = pytree_map(lambda x: maybe_all_reduce(x), batches)
        batches = pytree_map(lambda x: x.cpu().item(), batches)
        assert isinstance(batches, list)
        logit_stats = self._save_logit_stats()
        if self.dist.primary:
            with (root_dir / "batches.jsonl").open("w") as f:
                json.dump(batches, f)
            with (root_dir / "aggregate_metrics.json").open("w") as f:
                json.dump(agg, f)
            if self.record_logit_stats:
                with (root_dir / "logit_stats.json").open("w") as f:
                    json.dump(logit_stats, f)

================
File: tuned_lens/scripts/ingredients.py
================
"""Shared configuration for the scripts."""
import enum
import logging
import os
from dataclasses import dataclass
from datetime import timedelta
from functools import partial
from typing import Optional, Union
import torch as th
import torch.distributed as dist
from datasets import Dataset, DatasetDict, load_dataset
from simple_parsing import field
from torch.distributed.fsdp import (
    CPUOffload,
    FullyShardedDataParallel,
    MixedPrecision,
)
from torch.distributed.fsdp.wrap import transformer_auto_wrap_policy
from torch.distributed.optim import ZeroRedundancyOptimizer
from torch.nn.parallel import DistributedDataParallel as DDP
from torchdata import dataloader2, datapipes
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    PreTrainedModel,
    PreTrainedTokenizerBase,
    get_linear_schedule_with_warmup,
)
from typing_extensions import Literal
from tuned_lens.data import (
    chunk_and_tokenize,
)
from tuned_lens.model_surgery import get_transformer_layers
from tuned_lens.nn.lenses import Lens
from tuned_lens.utils import (
    TreeType,
    handle_name_conflicts,
    send_to_device,
)
logger = logging.getLogger(__name__)
@dataclass
class Data:
    """Configuration for the dataset."""
    name: list[str] = field(default_factory=lambda: ["the_pile", "all"], nargs="*")
    """Name of dataset to use. Can either be a local .jsonl file or a name
    suitable to be passed to the HuggingFace load_dataset function."""
    split: str = "validation"
    """Split of the dataset to use."""
    text_column: str = "text"
    """Column of the dataset containing text to run the model on."""
    revision: Optional[str] = None
    """The revision of the dataset to use"""
    max_seq_len: int = 2048
    """The maximum length of the input sequences."""
    dataset_shuffle: bool = False
    """Whether to shuffle the dataset prior to tokenization."""
    dataset_shuffle_seed: int = 42
    """Seed to use for shuffling the dataset"""
    def load(self, tokenizer: PreTrainedTokenizerBase) -> tuple[Dataset, float]:
        """Load the dataset, tokenize it and compute nats_to_bpb."""
        logger.info(f"Loading dataset '{' '.join(self.name)}'")
        logger.debug(f"Using split '{self.split}', revision '{self.revision}'")
        if len(self.name) == 1 and self.name[0].endswith(".jsonl"):
            dataset = Dataset.from_json(self.name[0])
            assert isinstance(dataset, Dataset)
        else:
            dataset = load_dataset(*self.name, split=self.split, revision=self.revision)
            if not isinstance(dataset, (Dataset, DatasetDict)):
                raise ValueError(
                    "Only Dataset and DatasetDict instances are supported."
                )
        logger.debug(f"Dataset has {len(dataset)} samples.")
        logger.debug(f"Dataset columns: {dataset.column_names}")
        if self.dataset_shuffle:
            logger.debug(f"Shuffling dataset with seed: {self.dataset_shuffle_seed}")
            dataset = dataset.shuffle(self.dataset_shuffle_seed)
        logger.debug("Beginning tokenization...")
        processed, nats_to_bpb = chunk_and_tokenize(
            dataset,
            tokenizer,
            text_key=self.text_column,
            max_seq_len=self.max_seq_len,
        )
        logger.info(f"Using nats per token to bits per byte ratio: {nats_to_bpb}")
        assert isinstance(processed, Dataset)
        return processed, nats_to_bpb
@dataclass
class Model:
    """Configuration for the model and tokenizer."""
    name: str
    """Name of model to use in the Huggingface Hub."""
    precision: Literal["auto", "bfloat16", "float16", "float32", "int8"] = "auto"
    """Precision in which to load the model weights."""
    revision: str = "main"
    """Git revision to use for pretrained models."""
    slow_tokenizer: bool = field(action="store_true")
    """Use a slow tokenizer."""
    tokenizer: Optional[str] = None
    """Name of pretrained tokenizer to use from the Huggingface Hub. If None, will use
    AutoTokenizer.from_pretrained('<model name>')."""
    tokenizer_type: Optional[str] = None
    """Name of tokenizer class to use. If None, will use AutoTokenizer."""
    def load_tokenizer(self, must_use_cache: bool = False) -> PreTrainedTokenizerBase:
        """Load the tokenizer from huggingface hub."""
        with handle_name_conflicts():
            tokenizer = AutoTokenizer.from_pretrained(
                self.tokenizer or self.name,
                revision=self.revision,
                use_fast=not self.slow_tokenizer,
                tokenizer_type=self.tokenizer_type,
                local_files_only=must_use_cache,
            )
        assert isinstance(tokenizer, PreTrainedTokenizerBase)
        return tokenizer
    def load(
        self, device: Optional[th.device], must_use_cache: bool = False
    ) -> tuple[PreTrainedModel, PreTrainedTokenizerBase]:
        """Load the model and tokenizer.
        Args:
            device: The device to load the model on. Implemented with the `device_map`
                argument of `AutoModelForCausalLM.from_pretrained`.
            must_use_cache: If True, will raise an error if the model is not cached.
        """
        logger.info(f"Loading pretrained weights for '{self.name}'...")
        logger.debug(
            "Using revision {revision} dtype {dtype}, and device {device}".format(
                revision=self.revision, dtype=self.precision, device=device
            )
        )
        try:
            dtype = {
                "auto": "auto",
                "bfloat16": th.bfloat16,
                "float16": th.float16,
                "float32": th.float32,
                # `bitsandbytes` requires weights to initially be in fp16
                "int8": th.float16,
            }[self.precision]
        except KeyError as e:
            raise ValueError(f"Unknown precision: {self.precision}") from e
        with handle_name_conflicts():
            model = AutoModelForCausalLM.from_pretrained(  # type: ignore
                self.name,
                device_map={"": device} if device is not None else None,
                load_in_8bit=self.precision == "int8",
                low_cpu_mem_usage=True,
                revision=self.revision,
                torch_dtype=dtype,
                local_files_only=must_use_cache,
            )
        assert isinstance(model, PreTrainedModel)
        model.eval()
        model.requires_grad_(False)
        return model, self.load_tokenizer(must_use_cache=must_use_cache)
class OptimizerOption(enum.Enum):
    """Options for the optimizer to use when training the model."""
    ADAM = "adam"
    SGD = "sgd"
@dataclass
class Optimizer:
    """Configuration for the optimizer."""
    weight_decay: float = 1e-3
    """Weight decay coefficient."""
    lr_scale: float = 1.0
    """The default LR (1e-3 for Adam, 1.0 for SGD) is scaled by this factor."""
    momentum: float = 0.9
    """Momentum coefficient for SGD, or beta1 for Adam."""
    zero: Optional[bool] = field(action="store_true")
    """Use ZeroRedundancyOptimizer."""
    optimizer: OptimizerOption = OptimizerOption.SGD
    """The type of optimizer to use."""
    warmup_steps: Optional[int] = None
    """Number of warmup steps. Defaults to min(0.2 * num_steps, 1000) for Adam and 0
    for SGD."""
    def create_scheduler(
        self, opt: th.optim.Optimizer, num_steps: int
    ) -> th.optim.lr_scheduler.LambdaLR:
        """Create the LR scheduler."""
        if self.warmup_steps is None:
            # Adam generally performs poorly without an LR warmup
            if self.optimizer == "adam":
                self.warmup_steps = min(1000, num_steps // 5)
                logger.info(f"Using {self.warmup_steps} LR warmup steps for Adam")
            else:
                self.warmup_steps = 0
        scheduler = get_linear_schedule_with_warmup(
            opt, self.warmup_steps, num_steps - self.warmup_steps
        )
        return scheduler
    def create_optim(self, params: list[th.nn.Parameter]) -> th.optim.Optimizer:
        """Create the optimizer."""
        # Don't train things that don't need gradients
        Î² = self.momentum
        if self.optimizer == OptimizerOption.SGD:
            config = dict(
                # PyTorch's implementation effectively scales the LR by 1 / (1 - Î²),
                # so we undo that here. See https://www.youtube.com/watch?v=k8fTYJPd3_I
                # for discussion. Once we do this, the optimal LR seems to be unity.
                lr=self.lr_scale * (1 - Î²),
                momentum=Î²,
                # Empirically Nesterov momentum seems to improve convergence speed.
                nesterov=True,
                weight_decay=self.weight_decay,
            )
            opt_class = th.optim.SGD
        elif self.optimizer == OptimizerOption.ADAM:
            config = dict(
                # Helps convergence slightly by ensuring that the LR actually decays
                amsgrad=True,
                betas=(Î², 0.999),
                lr=self.lr_scale * 1e-3,
                weight_decay=self.weight_decay,
            )
            opt_class = th.optim.Adam
        else:
            raise ValueError(f"Unknown optimizer '{self.optimizer}'")
        if self.zero:
            opt = ZeroRedundancyOptimizer(params, optimizer_class=opt_class, **config)
        else:
            opt = opt_class(params, **config)  # type: ignore[call-arg]
        return opt
    def per_parameter_optim_state_size(self) -> int:
        """The number of elements in the optimizer state per parameter."""
        return 2 if self.optimizer == OptimizerOption.ADAM else 1
@dataclass
class Distributed:
    """Configuration and utilities for distributing the model."""
    fsdp: bool = field(action="store_true")
    """Run the model with Fully Sharded Data Parallelism."""
    cpu_offload: bool = field(action="store_true")
    """Use CPU offloading. Must be combined with fsdp"""
    nccl_timeout: int = 1200  # 20 minutes
    """Timeout for NCCL operations in seconds."""
    per_gpu_batch_size: int = 1
    """The batch size per GPU."""
    dataloader_shuffle: bool = True
    """Whether to shuffle the batches of tokenized data as they are loaded."""
    @property
    def rank(self) -> int:
        """The rank of this process.
        Note that in general this is not the same as the local rank.
        However, for single-node training, the local rank is the same as the
        global rank.
        """
        return int(os.environ["RANK"]) if dist.is_initialized() else 0
    @property
    def local_rank(self) -> int:
        """The local rank of this process."""
        return int(os.environ["LOCAL_RANK"]) if dist.is_initialized() else 0
    @property
    def world_size(self) -> int:
        """Get the world size from torch.distributed."""
        return int(os.environ["WORLD_SIZE"]) if dist.is_initialized() else 1
    @property
    def primary(self) -> bool:
        """Whether this is the rank 0 process."""
        return self.rank == 0
    @property
    def device(self) -> th.device:
        """The device associated with this process."""
        return (
            th.device("cuda", self.local_rank)
            if th.cuda.is_available()
            else th.device("cpu")
        )
    def shard_model(
        self, model: PreTrainedModel
    ) -> Union[FullyShardedDataParallel, PreTrainedModel]:
        """Shard the model using Fully Sharded Data Parallelism if needed."""
        if self.fsdp:
            _, layers = get_transformer_layers(model)
            layer_cls = type(layers[0])
            logger.info(
                f"Using '{layer_cls.__name__}' for transformer_auto_wrap_policy."
            )
            return FullyShardedDataParallel(
                model,
                auto_wrap_policy=partial(
                    transformer_auto_wrap_policy, transformer_layer_cls={layer_cls}
                ),
                cpu_offload=CPUOffload(offload_params=self.cpu_offload),
                device_id=self.rank,
                # This turns out to be important for training speed
                forward_prefetch=True,
                mixed_precision=MixedPrecision(
                    param_dtype=th.float16,
                    reduce_dtype=th.float16,
                    buffer_dtype=th.float16,
                ),
            )
        elif self.cpu_offload:
            raise ValueError("CPU offload requires FSDP.")
        else:
            return model
    def distribute_lens(self, lens: Lens) -> Union[DDP, Lens]:
        """Distribute the lens using DistributedDataParallel and send lens to device."""
        logger.debug(f"Sending Lens to device {self.device}")
        if self.world_size > 1:
            lens.to(self.device)
            logger.debug("Distributing the lens across the GPUS using DDP ...")
            return DDP(lens, device_ids=[self.local_rank], find_unused_parameters=True)
        else:
            return lens.to(self.device)
    def dataloader(
        self,
        dataset: Dataset,
    ) -> dataloader2.DataLoader2:
        """Shard the dataset based on local rank."""
        dp = datapipes.iter.IterableWrapper(dataset)
        if self.world_size > 1:
            rs = dataloader2.DistributedReadingService()
        else:
            rs = None
        if self.dataloader_shuffle:
            dp = dp.shuffle()
        dp = dp.sharding_filter()
        dp = dp.batch(self.per_gpu_batch_size)
        dp = dp.collate()
        return dataloader2.DataLoader2(dp, reading_service=rs)
    def init(self):
        """Initialize distributed process group if started with elastic launch."""
        # Support both distributed and non-distributed training
        local_rank = os.environ.get("LOCAL_RANK")
        if local_rank is not None:
            dist.init_process_group(
                "nccl", timeout=timedelta(seconds=self.nccl_timeout)
            )
            assert (
                th.cuda.is_available()
            ), "CUDA must be available for distributed training"
            th.cuda.set_device(self.local_rank)
    def barrier(self) -> None:
        """Barrier for all processes."""
        if dist.is_initialized():
            dist.barrier()
    def send_to_device(self, pytree: TreeType) -> TreeType:
        """Move pytree to the current device."""
        return send_to_device(pytree, self.device)

================
File: tuned_lens/scripts/train_loop.py
================
"""Training loop for training a TunedLens model against a transformer on a dataset."""
import dataclasses
import enum
import logging
import re
from collections import defaultdict
from dataclasses import dataclass
from pathlib import Path
from typing import Optional, Union
import torch as th
from simple_parsing import field
from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
from torch.distributed.optim import ZeroRedundancyOptimizer
from torch.optim import Optimizer
from torch.optim.lr_scheduler import LambdaLR
from torchdata.dataloader2 import DataLoader2
from tqdm.auto import trange
from transformers import PreTrainedModel
import tuned_lens.scripts.ingredients as ing
from tuned_lens import TunedLens
from tuned_lens.utils import maybe_all_reduce, shift_labels, shift_preds
logger = logging.getLogger(__name__)
class LossChoice(enum.Enum):
    """Options of what loss to select when training the model."""
    CE = "ce"
    KL = "kl"
@dataclass
class State:
    """All of the stateful information in the training loop."""
    dataloader: DataLoader2
    lens: TunedLens
    opt: Optimizer
    scheduler: LambdaLR
    wandb_id: Optional[str]
    nats_to_bpb: float
    step: int = 0
    def load(self, snapshot_file: Path, device: th.device) -> None:
        """Load a snapshot file."""
        logger.info(f"Loading snapshot from {snapshot_file}...")
        snapshot = th.load(snapshot_file, map_location=device)
        self.step = snapshot["step"]
        self.wandb_id = snapshot["wandb_id"]
        self.lens.load_state_dict(snapshot["lens"])
        self.opt.load_state_dict(snapshot["optim"])
        self.scheduler.load_state_dict(snapshot["scheduler"])
        self.dataloader.load_state_dict(snapshot["dataloader"])
    def save(self, snapshot_file: Path) -> None:
        """Save a snapshot file."""
        logger.info(f"Saving snapshot to {snapshot_file}...")
        if isinstance(self.opt, ZeroRedundancyOptimizer):
            self.opt.consolidate_state_dict()
        th.save(
            {
                "lens": self.lens.state_dict(),
                "optim": self.opt.state_dict(),
                "scheduler": self.scheduler.state_dict(),
                "dataloader": self.dataloader.state_dict(),
                "step": self.step,
                "wandb_id": self.wandb_id,
            },
            snapshot_file,
        )
@dataclass
class Train:
    """Training loop for the tuned lens."""
    model: ing.Model
    """Model configuration."""
    data: ing.Data
    """Data configuration."""
    opt: ing.Optimizer
    """Optimizer configuration."""
    dist: ing.Distributed
    """Configuration for how to distribute the training."""
    output: Path = field(alias=["-o"])
    """Directory to save the lenses to."""
    seed: int = 42
    """Random seed for data shuffling."""
    lens_name_or_path: Optional[str] = field(alias=["-l"], default=None)
    """Name of a pretrained lens to load for fine-tuning."""
    bias_only: Optional[bool] = field(action="store_true")
    """Train only the bias term."""
    num_steps: int = 250
    """Number of training steps."""
    tokens_per_step: int = 2**18
    """Number of tokens per step."""
    wandb: Optional[str] = None
    """Name of run in Weights & Biases."""
    token_shift: Optional[int] = None
    """How to shift the labels wrt the input tokens (1 = next token, 0 = current token,
    -1 = previous token, etc.)"""
    checkpoint_freq: Optional[int] = None
    """Steps between saving a checkpoint. If None, no checkpoints are saved."""
    checkpoint_dir: Optional[Path] = None
    """Directory to save checkpoints to. If None, will use <output>/checkpoints."""
    loss: LossChoice = LossChoice.KL
    """Loss function to use."""
    def __post_init__(self):
        """Set defaults for some fields."""
        if self.checkpoint_dir is None:
            self.checkpoint_dir = self.output / "checkpoints"
    def get_lens(self, model: PreTrainedModel) -> TunedLens:
        """Load or create a TunedLens model."""
        if self.lens_name_or_path is None:
            logger.info("Randomly initializing lens...")
            lens = TunedLens.from_model(model)
        else:
            logger.info("Loading pretrained lens...")
            lens = TunedLens.from_model_and_pretrained(model, self.lens_name_or_path)
        dtypes = {p.dtype for p in lens.parameters()}
        assert (
            len(dtypes) == 1
        ), f"Expected all parameters to have the same dtype, got {dtypes}"
        lens_dtype = next(iter(dtypes))
        lens_size = sum(p.numel() * p.element_size() for p in lens.parameters())
        # Include the optimizer state in the memory usage
        num_bytes = lens_size * (self.opt.per_parameter_optim_state_size() + 1)
        logger.info(
            f"Tuned lens memory usage: {num_bytes / 2 ** 20:.2f} MB in {lens_dtype}"
        )
        if self.bias_only:
            logger.info("Freezing the matrix weights to train only the bias terms.")
            for probe in lens:
                probe.weight.requires_grad_(False)
        return lens
    def _get_wandb_id(self) -> Optional[str]:
        if not self.dist.primary or not self.wandb:
            return None
        from wandb.sdk.lib import runid
        return runid.generate_id()
    def _init_logging(self, model_name: str, lens: TunedLens, wandb_id: Optional[str]):
        """Initialize logging to weights and biases."""
        if not self.dist.primary or not self.wandb:
            return
        logger.debug("Initializing Weights & Biases ...")
        import wandb
        wandb.init(
            config=dataclasses.asdict(self),
            group=model_name,
            name=self.wandb,
            id=wandb_id,
            resume="allow",
        )
        wandb.watch(lens)
    def _log(
        self,
        opt: th.optim.Optimizer,
        step: int,
        losses: dict[str, list[float]],
        tuned_lens: TunedLens,
        nats_to_bpb: float,
    ):
        """Log statistics about the training process to weights and biases."""
        if not self.dist.primary or not self.wandb:
            return
        import wandb
        log_dict = {}
        log_dict.update(
            {f"loss/{k}": th.tensor(v).mean() * nats_to_bpb for k, v in losses.items()}
        )
        # Log statistics about optimizer & probes
        for i, probe in enumerate(tuned_lens):
            name = "input" if i == 0 else f"{i - 1}.ffn"
            states = [opt.state[p] for p in probe.parameters()]
            # Approximate the true grad norm using the optimizer's moving
            # avg
            corr = 1 - self.opt.momentum**step
            if self.opt.optimizer == "sgd" and not self.opt.zero:
                log_dict["grad_norm/" + name] = th.cat(
                    [
                        # Undo PyTorch's scaling of the gradient by
                        # 1 / (1 - Î²)
                        (1 - self.opt.momentum) * s["momentum_buffer"].flatten() / corr
                        for s in states
                    ]
                ).norm()
            elif self.opt.optimizer == "adam" and not self.opt.zero:
                log_dict["grad_norm/" + name] = th.cat(
                    [s["exp_avg"].flatten() / corr for s in states]
                ).norm()
            if isinstance(probe, th.nn.Linear):
                log_dict["bias_norm/" + name] = probe.bias.data.norm()
                log_dict["weight_norm/" + name] = probe.weight.data.norm()
        wandb.log(log_dict)
    def snapshot(self, state: State):
        """Save a snapshot of the training process to disk."""
        if self.dist.primary:
            assert self.checkpoint_dir is not None
            self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
            state.save(self.checkpoint_dir / f"snapshot_{state.step}.pth")
    def load_recent_snapshot(self, state: State) -> None:
        """Load the most recent snapshot of the training process from disk."""
        assert self.checkpoint_dir is not None
        if not self.checkpoint_dir.exists():
            logger.warning("No checkpoint directory found. Snapshotting is disabled.")
            return None
        # Find the folder containing the most recent snapshot
        def sort_key_from_path(p: Path):
            if match := re.match(r".*snapshot_(\d+)\.pth", str(p)):
                return int(match.group(1))
            else:
                return -1
        snapshot_location = max(
            self.checkpoint_dir.glob("snapshot_*.pth"),
            key=sort_key_from_path,
            default=None,
        )
        if snapshot_location is None:
            return None
        state.load(snapshot_location, self.dist.device)
    def calculate_gradient_accumulation_steps(
        self, tokens_per_sample: int, total_samples: int
    ) -> int:
        """Calculate the number of batches of data to process before taking a step."""
        # chunk_and_tokenize ensures the samples are all the same length
        samples_per_step, rem = divmod(self.tokens_per_step, tokens_per_sample)
        if rem:
            raise ValueError(
                f"Number of tokens per step ({self.tokens_per_step:_}) must be "
                f"divisible by the number of tokens per sample ({tokens_per_sample})."
            )
        if total_samples / samples_per_step < self.num_steps:
            raise ValueError(
                f"Can only take {total_samples / samples_per_step:.2f} steps on "
                f"dataset with --tokens_per_step={self.tokens_per_step}."
                f"Requested {self.num_steps} steps."
            )
        global_batch_size = self.dist.per_gpu_batch_size * self.dist.world_size
        grad_acc_steps, rem = divmod(samples_per_step, global_batch_size)
        if rem:
            # If the number of samples per step isn't divisible by the global batch
            # size, use ceil division and let the user know about it.
            grad_acc_steps += 1
            adjusted_count = grad_acc_steps * global_batch_size * tokens_per_sample
            logger.warning(
                f"Note: Increasing grad acc steps from {grad_acc_steps - 1} to "
                f"{grad_acc_steps} to maintain load balance across "
                f"{self.dist.world_size} GPUs."
            )
            logger.warning(
                f"Using {adjusted_count:_} tokens per training step "
                f"({self.tokens_per_step:_} requested)."
            )
        else:
            logger.info(f"Gradient accumulation steps: {grad_acc_steps}")
            logger.info(f"Using {self.tokens_per_step:_} tokens per training step.")
        return grad_acc_steps
    def setup(self) -> tuple[State, Union[PreTrainedModel, FSDP], int]:
        """Initialize the training process."""
        self.dist.init()
        model = tokenizer = data = lens = nats_to_bpb = None
        # Annoyingly, FSDP is incompatible with the `device_map` parameter on
        # `from_pretrained`, because it adds forward hooks to the submodules that move
        # things around to different devices. But `bitsandbytes` requires `device_map`
        # to work at all. So we use `device_map` iff we're using FSDP.
        load_device = self.dist.device if not self.dist.fsdp else None
        if self.dist.primary:
            logger.debug("Primary rank populating cache...")
            model, tokenizer = self.model.load(load_device)
            data, nats_to_bpb = self.data.load(tokenizer)
            lens = self.get_lens(model)
        self.dist.barrier()  # Wait for primary to finish filling the cache
        if not self.dist.primary:
            # Let the non-primary processes load from the cache
            logger.debug("Non-primary rank loading from cache...")
            model, tokenizer = self.model.load(load_device, must_use_cache=True)
            data, nats_to_bpb = self.data.load(tokenizer)
            lens = self.get_lens(model)
        assert model and tokenizer and data and lens and nats_to_bpb
        logger.debug(f"Creating data loader and setting seed to {self.seed} ...")
        dl = self.dist.dataloader(data)
        dl.seed(self.seed)
        logger.debug("Creating optimizer and scheduler ...")
        params = [p for p in lens.parameters() if p.requires_grad]
        opt = self.opt.create_optim(params)
        scheduler = self.opt.create_scheduler(opt, self.num_steps)
        ddp_lens = self.dist.distribute_lens(lens)
        state = State(
            step=0,
            wandb_id=self._get_wandb_id(),
            lens=ddp_lens,  # type: ignore
            opt=opt,
            scheduler=scheduler,
            dataloader=dl,
            nats_to_bpb=nats_to_bpb,
        )
        self.load_recent_snapshot(state)
        # Shard the model using fully shared data parallel
        model = self.dist.shard_model(model)
        self._init_logging(
            model_name=self.model.name, lens=state.lens, wandb_id=state.wandb_id
        )
        tokens_per_sample = len(data[0]["input_ids"])
        grad_acc_steps = self.calculate_gradient_accumulation_steps(
            tokens_per_sample, len(data)
        )
        self.dist.barrier()  # Wait for all processes to finish setup
        logger.info("All processes have completed setup.")
        return state, model, grad_acc_steps
    def execute(self):
        """Trains a TunedLens model against a transformer on a dataset."""
        # Load model, tokenizer, data, and lens
        state, model, grad_acc_steps = self.setup()
        losses = defaultdict(list)
        init_batches = state.step * grad_acc_steps
        total_batches = self.num_steps * grad_acc_steps
        # Wait for all processes to finish setup
        self.dist.barrier()
        logger.info("All processes have completed setup. Starting training.")
        # Main training loop
        t = trange(
            init_batches,
            total_batches,
            desc="Training",
            initial=init_batches,
            total=total_batches,
        )
        # TODO this currently silently fails if the dataloader is exhausted
        for batch_idx, batch in zip(t, state.dataloader):
            assert isinstance(batch, dict), f"Expected dict, got {type(batch)}"
            with th.no_grad():
                batch = self.dist.send_to_device(batch)
                output = model(**batch, output_hidden_states=True)
            final_logits = output.logits
            hidden_states = output.hidden_states[:-1]
            shift = self.token_shift
            if self.loss == LossChoice.CE:
                labels = batch["input_ids"]
                # Predict the *next* token by default w/ cross entropy
                if shift is None:
                    shift = 1
            elif self.loss == LossChoice.KL:
                labels = final_logits.float().log_softmax(dim=-1)
                # Match the *current* token distribution by default
                if shift is None:
                    shift = 0
            else:
                raise NotImplementedError(f"Unknown loss {self.loss}")
            labels = shift_labels(labels, shift)
            # We do this sequentially to save VRAM
            for i, h in enumerate(hidden_states):
                # We use bfloat16 because it has a larger dynamic range than float16
                # and it seems to remove the need for doing grad scaling, which is very
                # annoying to set up in the context of multiple backward passes.
                with th.autocast(self.dist.device.type, dtype=th.bfloat16):
                    logits = shift_preds(state.lens(h, idx=i), shift)
                    if self.loss == LossChoice.CE:
                        loss = th.nn.functional.cross_entropy(
                            logits.flatten(0, -2), labels.flatten()
                        )
                    elif self.loss == LossChoice.KL:
                        loss = th.sum(
                            labels.exp() * (labels - logits.log_softmax(-1)), dim=-1
                        ).mean()
                    else:
                        raise NotImplementedError
                    logging_loss = loss.detach()
                    logging_loss = maybe_all_reduce(logging_loss).item()
                    if self.dist.primary:
                        losses[f"translator_{i}"].append(logging_loss)
                    scaled_loss = loss / grad_acc_steps
                scaled_loss.backward()
            step, rem = divmod(batch_idx, grad_acc_steps)
            if rem == grad_acc_steps - 1:
                th.nn.utils.clip_grad_norm_(state.lens.parameters(), 1.0)
                state.opt.step()
                state.opt.zero_grad(set_to_none=False)
                state.scheduler.step()
                # Unwrap the lens from DDP if needed
                lens = getattr(state.lens, "module", state.lens)
                self._log(state.opt, step, losses, lens, state.nats_to_bpb)
                losses.clear()
                state.step = step + 1
                if (
                    self.checkpoint_freq
                    and step % self.checkpoint_freq == self.checkpoint_freq - 1
                ):
                    self.snapshot(state)
        if self.dist.primary:
            logger.info(f"Saving lens to {self.output}")
            # Unwrap the lens from DDP if needed
            lens = getattr(state.lens, "module", state.lens)
            lens.save(self.output)

================
File: tuned_lens/stats/__init__.py
================
"""Statistics for evaluating lens performance."""
from .distance import (
    js_distance,
    js_divergence,
    kl_divergence,
)
from .logit_stats import LogitStats

================
File: tuned_lens/stats/distance.py
================
"""Various distance metrics for probability distributions."""
import math
import torch as th
def js_divergence(logit_p: th.Tensor, logit_q: th.Tensor, dim: int = -1) -> th.Tensor:
    """Compute the Jensen-Shannon divergence between two sets of logits.
    Conceptually, the JSD is the info value of learning which of two distributions,
    P or Q, that a random variable is drawn from, starting from a uniform prior over
    P and Q. Since the entropy of a Bernoulli variable is at most ln(2), the JSD is
    guaranteed to be in the range [0, ln(2)]. It is also symmetric and finite even
    for distributions with disjoint supports.
    Mathematically, the JSD is simply [KL(P || M) + KL(Q || M)] / 2, where M
    is the mean of P and Q.
    """
    log_p = logit_p.log_softmax(dim)
    log_q = logit_q.log_softmax(dim)
    # Mean of P and Q
    log_m = th.stack([log_p, log_q]).sub(math.log(2)).logsumexp(0)
    kl_p = th.sum(log_p.exp() * (log_p - log_m), dim)
    kl_q = th.sum(log_q.exp() * (log_q - log_m), dim)
    return 0.5 * (kl_p + kl_q)
def js_distance(logit_p: th.Tensor, logit_q: th.Tensor, dim: int = -1) -> th.Tensor:
    """Compute the square root of the Jensen-Shannon divergence of two logit vectors."""
    return js_divergence(logit_p, logit_q, dim).sqrt()
def kl_divergence(logit_p: th.Tensor, logit_q: th.Tensor, dim: int = -1) -> th.Tensor:
    """Compute the KL divergence between two sets of logits."""
    log_p = logit_p.log_softmax(dim)
    log_q = logit_q.log_softmax(dim)
    return th.sum(log_p.exp() * (log_p - log_q), dim)
def sqrtmh(x: th.Tensor) -> th.Tensor:
    """Unique PSD square root of a Hermitian positive semi-definite matrix."""
    dtype = x.dtype
    # This is actually precision-sensitive
    L, Q = th.linalg.eigh(x.double())
    res = Q * L.clamp(0.0).sqrt() @ Q.mH
    return res.to(dtype)

================
File: tuned_lens/stats/logit_stats.py
================
"""Online MLE for the Dirichlet distribution from which logits are sampled."""
from typing import Optional
import torch as th
from torch.distributions import Dirichlet
from ..utils import maybe_all_reduce
class LogitStats:
    """Online MLE for the Dirichlet distribution from which logits are sampled.
    Shape and device are lazily inferred from the first stream that is passed to
    `update()`. Only a running mean of the log-likelihoods for each class is stored,
    so memory use is negligible and constant in the number of samples. The maximum
    likelihood distribution is computed on request using L-BFGS.
    """
    n: Optional[th.Tensor]
    marginal_probs: Optional[th.Tensor]
    sufficient_stats: Optional[th.Tensor]
    def __init__(
        self,
        n: Optional[th.Tensor] = None,
        marginal_probs: Optional[th.Tensor] = None,
        sufficient_stats: Optional[th.Tensor] = None,
    ):
        """Create a LogitStats object."""
        self.n = None
        self.marginal_probs = marginal_probs
        self.sufficient_stats = sufficient_stats
    def all_reduce_(self):
        """All-reduce the stats across all processes."""
        if (
            self.sufficient_stats is not None
            and self.marginal_probs is not None
            and self.n is not None
        ):
            n_x_sufficient_stats = self.n * self.sufficient_stats
            n_x_marginal_probs = self.n * self.marginal_probs
            maybe_all_reduce(n_x_sufficient_stats, op="sum")
            maybe_all_reduce(n_x_marginal_probs, op="sum")
            maybe_all_reduce(self.n, op="sum")
            self.sufficient_stats = n_x_sufficient_stats / self.n
            self.marginal_probs = n_x_marginal_probs / self.n
        else:
            raise ValueError("Attempting to reduce an uninitialized LogitStats object")
    @th.no_grad()
    def update(self, logits: th.Tensor, assume_normalized: bool = False):
        """Update the sufficient statistics with a new batch of logits."""
        K = logits.shape[-1]
        logits = logits.reshape(-1, K).float()
        if not assume_normalized:
            logits = logits.log_softmax(dim=-1)
        N = logits.shape[0]
        if self.n is None:
            self.n = th.tensor(0, dtype=th.int64, device=logits.device)
        elif len(self.n.shape) > 0:
            raise ValueError(f"Expected n to be a scalar but got {self.n.shape=}")
        if self.marginal_probs is None:
            self.marginal_probs = logits.new_zeros(K)
        elif self.marginal_probs.shape[-1] != K:
            raise ValueError(f"Expected {self.marginal_probs.shape[-1]} but got {K}")
        if self.sufficient_stats is None:
            self.sufficient_stats = logits.new_zeros(K)
        # Online mean update for the marginal probabilities
        delta = logits.exp().mean(0) - self.marginal_probs
        self.n += N
        self.marginal_probs += delta * N / self.n
        # Online mean update for the sufficient statistics
        delta = logits.mean(0) - self.sufficient_stats
        self.sufficient_stats += delta * N / self.n
    def mle(self, max_iter: int = 100, tol: float = 1e-4) -> Dirichlet:
        """Compute the MLE for the Dirichlet generating the logits seen so far."""
        if self.sufficient_stats is None:
            raise ValueError("No sufficient statistics available")
        log_alpha = th.nn.Parameter(th.zeros_like(self.sufficient_stats))
        opt = th.optim.LBFGS(
            [log_alpha],
            line_search_fn="strong_wolfe",
            max_iter=max_iter,
            tolerance_change=tol,
        )
        def closure():
            opt.zero_grad(set_to_none=False)
            # See http://jonathan-huang.org/research/dirichlet/dirichlet.pdf,
            # page 5 for the formula
            alpha = log_alpha.exp()
            normalizer = alpha.sum().lgamma() - alpha.lgamma().sum()
            loss = -(normalizer + (alpha - 1) @ self.sufficient_stats)
            loss.backward()
            return loss
        opt.step(closure)  # type: ignore
        return Dirichlet(log_alpha.data.exp())

================
File: tuned_lens/utils.py
================
"""Utilities for distributed training and handling nested collections of tensors."""
import hashlib
from contextlib import contextmanager
from itertools import islice
from typing import Any, Callable, Iterable, Sequence, Type, TypeVar, Union, cast
import numpy as np
import torch as th
import torch.distributed as dist
from numpy.typing import NDArray
T = TypeVar("T")
def assert_type(typ: Type[T], obj: Any) -> T:
    """Assert that an object is of a given type at runtime and return it."""
    if not isinstance(obj, typ):
        raise TypeError(f"Expected {typ.__name__}, got {type(obj).__name__}")
    return cast(typ, obj)
def maybe_all_cat(x: th.Tensor) -> th.Tensor:
    """Concatenate a tensor across all processes."""
    if not dist.is_initialized():
        return x
    buffer = x.new_empty([dist.get_world_size() * x.shape[0], *x.shape[1:]])
    dist.all_gather_into_tensor(buffer, x)
    return buffer
def maybe_all_gather_lists(lst: list) -> list:
    """Gather a list of objects from all processes."""
    if not dist.is_initialized():
        return lst
    lists = [[] for _ in range(dist.get_world_size())]
    dist.all_gather_object(lists, lst)
    return sum(lists, [])
def maybe_all_reduce(x: th.Tensor, op: str = "mean") -> th.Tensor:
    """Reduce a tensor across all processes."""
    if not dist.is_initialized():
        return x
    if op == "sum":
        dist.all_reduce(x, op=dist.ReduceOp.SUM)
    elif op == "mean":
        dist.all_reduce(x, op=dist.ReduceOp.SUM)
        x /= dist.get_world_size()
    else:
        raise ValueError(f"Unknown reduction op '{op}'")
    return x
def maybe_unpack(x):
    """Unpack a tuple if it's a tuple, otherwise return the value."""
    if isinstance(x, tuple):
        x, *_ = x
    return x
def shift_labels(x: th.Tensor, shift: int):
    """Shift labels by a given amount.
    Args:
        x: (batch x seq_len) labels to shift.
        shift: Amount to shift by. Positive values take from the start, negative values
            negative values take from the end.
    Returns:
        (batch x (seq_len - shift)) labels shifted by the given amount.
    """
    if shift > 0:
        return x[:, shift:]
    if shift < 0:
        return x[:, :shift]
    return x
def shift_preds(x: th.Tensor, shift: int):
    """Shift predictions by a given amount.
    Args:
        x: (batch x seq_len) predictions to shift.
        shift: Amount to shift by. Positive values take from the end, negative values
            from the start.
    Returns:
        (batch x (seq_len - shift)) predictions shifted by the given amount.
    """
    if shift > 0:
        return x[:, :-shift]
    if shift < 0:
        return x[:, -shift:]
    return x
T = TypeVar("T")
# Backported from Python 3.10
def pairwise(it: Iterable[T]) -> Iterable[tuple[T, T]]:
    """Iterate over pairs of elements in an iterable."""
    yield from zip(it, islice(it, 1, None))
@contextmanager
def handle_name_conflicts():
    """Provide better error messages."""
    try:
        yield
    except OSError as e:
        raise RuntimeError(
            "HuggingFace is throwing an error during a `from_pretrained` call. Check "
            "your CWD to ensure there are no folders with names that may conflict "
            "with the model name you provided."
        ) from e
# Define pytree type recursively- this works for Pylance but unfortunately not MyPy
AnyTree = Union[th.Tensor, dict[Any, "AnyTree"], list["AnyTree"], tuple["AnyTree", ...]]
TreeType = TypeVar("TreeType", bound=AnyTree)
def pytree_flatten(tree: AnyTree) -> Iterable[th.Tensor]:
    """Recursively iterate over all tensors in a pytree, in topological order."""
    # Stopping condition
    if isinstance(tree, th.Tensor):
        yield tree
    # Recursive case
    elif isinstance(tree, dict):
        for elem in tree.values():
            yield from pytree_flatten(elem)
    elif isinstance(tree, Sequence):
        for elem in tree:
            yield from pytree_flatten(elem)
def pytree_map(
    func: Callable[[th.Tensor], Any], tree: TreeType, strict: bool = True
) -> TreeType:
    """Recursively apply a function to all tensors in a pytree.
    Args:
        func: Function to apply to each tensor.
        tree: Pytree to apply the function to.
        strict: If True, raise an error if a non-tensor leaf is encountered.
    Returns:
        A new pytree with the same structure. Non-tensor leaves are copied.
    """
    # Stopping condition
    if isinstance(tree, th.Tensor):
        return func(tree)
    # Recursive case
    if isinstance(tree, dict):
        return {k: pytree_map(func, v) for k, v in tree.items()}
    if isinstance(tree, list):
        return [pytree_map(func, v) for v in tree]
    if isinstance(tree, tuple):
        return tuple(pytree_map(func, v) for v in tree)
    if strict:
        raise TypeError(
            f"Found leaf '{tree}' of unsupported type '{type(tree).__name__}'- use "
            f"`strict=False` to ignore"
        )
    else:
        return tree
def pytree_cat(trees: Sequence[AnyTree], dim: int = 0) -> AnyTree:
    """Concatenate pytrees along a given dimension.
    All pytrees are expected to use the same collection; undefined behavior
    will occur if this is not the case.
    Args:
        trees: Sequence of pytrees containing tensors to concatenate.
        dim: Dimension to concatenate along.
    Returns:
        - A new pytree with the same structure.
    """
    transposed_iter = zip(*(pytree_flatten(tree) for tree in trees))
    leaf_iter = (th.cat(seq, dim) for seq in transposed_iter)
    try:
        return pytree_map(lambda _: next(leaf_iter), trees[0])  # type: ignore
    except (RuntimeError, StopIteration) as e:
        # Calling next() on an exhausted generator raises a RuntimeError, annoyingly
        if isinstance(e, StopIteration) or "StopIteration" in str(e):
            raise TypeError("All pytrees must have the same structure") from e
        else:
            raise
def pytree_stack(trees: Sequence, dim: int = 0) -> AnyTree:
    """Stack pytrees along a given dimension.
    All pytrees are expected to use the same collection; undefined behavior
    will occur if this is not the case.
    Args:
        trees: Sequence of pytrees containing tensors to stack.
        dim: Dimension to concatenate along.
    Returns:
        A new pytree with the same structure.
    """
    if not len(trees):
        raise ValueError("Cannot stack empty sequence of pytrees")
    transposed_iter = zip(*(pytree_flatten(tree) for tree in trees))
    leaf_iter = (th.stack(seq, dim) for seq in transposed_iter)
    try:
        return pytree_map(lambda _: next(leaf_iter), trees[0])  # type: ignore
    except (RuntimeError, StopIteration) as e:
        # Calling next() on an exhausted generator raises a RuntimeError, annoyingly
        if isinstance(e, StopIteration) or "StopIteration" in str(e):
            raise TypeError("All pytrees must have the same structure") from e
        else:
            raise
def revcumsum(x: Sequence[th.Tensor]) -> list[th.Tensor]:
    """Reverse cumulative sum of a sequence of tensors."""
    if not len(x):
        return []
    running_total = th.zeros_like(x[0])
    sums = [running_total.add_(r).clone() for r in reversed(x)]
    sums.reverse()
    return sums
def send_to_device(tree: TreeType, device: th.device) -> TreeType:
    """Recursively send all tensors in a pytree to a device."""
    return pytree_map(lambda t: t.to(device), tree)
def tensor_hash(tensor: NDArray) -> str:
    """Fast hash of a matrix that is robust to dtype and small perturbations.
    Note this relies on the ordering of the elements in the matrix, so it is
    if the matrix is in any way sorted this will not work well. In addition,
    this hash is intended for large tensors 64 + elements.
    """
    return hashlib.sha256(str.encode(np.array_str(tensor, precision=1))).hexdigest()



================================================================
End of Codebase
================================================================
