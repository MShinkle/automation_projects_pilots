This file is a merged representation of a subset of the codebase, containing specifically included files and files not matching ignore patterns, combined into a single document by Repomix.
The content has been processed where empty lines have been removed.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
- Pay special attention to the Repository Description. These contain important context and guidelines specific to this project.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: **/*.py, **/*.md, **/*.txt
- Files matching these patterns are excluded: **/.git/**, **/.github/**, CHANGELOG.md
- Empty lines have been removed from all files
- Files are sorted by Git change count (files with more changes are at the bottom)

Additional Info:
----------------
User Provided Header:
-----------------------
This file is a consolidated single-file compilation of all code in the repository generated by Repomix. Note that .ipynb files have been converted to .py files.

================================================================
Directory Structure
================================================================
docs/_container_limits.md
docs/_errors_and_retries.md
docs/_max_samples.md
docs/_metadata_typing.md
docs/_model-providers.md
docs/_sample-preservation.md
docs/_sandboxenv-interface.md
docs/_setting_max_samples.md
docs/_shuffling-choices.md
docs/_store_typing.md
docs/_tools-annotations-required.md
docs/_tools-scaffold.md
docs/evals/import.py
docs/llms.txt
docs/reference/filter/commands.py
docs/reference/filter/filter.py
docs/reference/filter/parse.py
docs/reference/filter/render.py
docs/reference/filter/sidebar.py
examples/approval/approval.py
examples/approval/README.md
examples/biology_qa.py
examples/bridge/autogen/agent.py
examples/bridge/autogen/README.md
examples/bridge/autogen/task.py
examples/bridge/langchain/agent.py
examples/bridge/langchain/README.md
examples/bridge/langchain/task.py
examples/browser/browser.py
examples/cache.py
examples/computer/computer.py
examples/evalset.py
examples/hello_world.py
examples/images/images.py
examples/intervention/intervention.py
examples/intervention/README.md
examples/popularity.py
examples/security_guide.py
examples/structured.py
examples/theory_of_mind.py
examples/tool_use.py
README.md
requirements.txt
src/inspect_ai.egg-info/dependency_links.txt
src/inspect_ai.egg-info/entry_points.txt
src/inspect_ai.egg-info/requires.txt
src/inspect_ai.egg-info/top_level.txt
src/inspect_ai/__init__.py
src/inspect_ai/__main__.py
src/inspect_ai/_cli/cache.py
src/inspect_ai/_cli/common.py
src/inspect_ai/_cli/eval.py
src/inspect_ai/_cli/info.py
src/inspect_ai/_cli/list.py
src/inspect_ai/_cli/log.py
src/inspect_ai/_cli/main.py
src/inspect_ai/_cli/sandbox.py
src/inspect_ai/_cli/score.py
src/inspect_ai/_cli/trace.py
src/inspect_ai/_cli/util.py
src/inspect_ai/_cli/view.py
src/inspect_ai/_display/__init__.py
src/inspect_ai/_display/core/active.py
src/inspect_ai/_display/core/config.py
src/inspect_ai/_display/core/display.py
src/inspect_ai/_display/core/footer.py
src/inspect_ai/_display/core/group.py
src/inspect_ai/_display/core/panel.py
src/inspect_ai/_display/core/progress.py
src/inspect_ai/_display/core/results.py
src/inspect_ai/_display/core/rich.py
src/inspect_ai/_display/core/textual.py
src/inspect_ai/_display/plain/display.py
src/inspect_ai/_display/rich/display.py
src/inspect_ai/_display/textual/app.py
src/inspect_ai/_display/textual/display.py
src/inspect_ai/_display/textual/theme.py
src/inspect_ai/_display/textual/widgets/clock.py
src/inspect_ai/_display/textual/widgets/console.py
src/inspect_ai/_display/textual/widgets/footer.py
src/inspect_ai/_display/textual/widgets/port_mappings.py
src/inspect_ai/_display/textual/widgets/samples.py
src/inspect_ai/_display/textual/widgets/sandbox.py
src/inspect_ai/_display/textual/widgets/task_detail.py
src/inspect_ai/_display/textual/widgets/tasks.py
src/inspect_ai/_display/textual/widgets/titlebar.py
src/inspect_ai/_display/textual/widgets/toggle.py
src/inspect_ai/_display/textual/widgets/transcript.py
src/inspect_ai/_eval/context.py
src/inspect_ai/_eval/eval.py
src/inspect_ai/_eval/evalset.py
src/inspect_ai/_eval/list.py
src/inspect_ai/_eval/loader.py
src/inspect_ai/_eval/registry.py
src/inspect_ai/_eval/run.py
src/inspect_ai/_eval/score.py
src/inspect_ai/_eval/task/__init__.py
src/inspect_ai/_eval/task/constants.py
src/inspect_ai/_eval/task/epochs.py
src/inspect_ai/_eval/task/error.py
src/inspect_ai/_eval/task/generate.py
src/inspect_ai/_eval/task/images.py
src/inspect_ai/_eval/task/log.py
src/inspect_ai/_eval/task/resolved.py
src/inspect_ai/_eval/task/results.py
src/inspect_ai/_eval/task/run.py
src/inspect_ai/_eval/task/rundir.py
src/inspect_ai/_eval/task/sandbox.py
src/inspect_ai/_eval/task/task.py
src/inspect_ai/_eval/task/tasks.py
src/inspect_ai/_eval/task/util.py
src/inspect_ai/_util/_async.py
src/inspect_ai/_util/ansi.py
src/inspect_ai/_util/appdirs.py
src/inspect_ai/_util/config.py
src/inspect_ai/_util/constants.py
src/inspect_ai/_util/content.py
src/inspect_ai/_util/datetime.py
src/inspect_ai/_util/decorator.py
src/inspect_ai/_util/deprecation.py
src/inspect_ai/_util/dev.py
src/inspect_ai/_util/dict.py
src/inspect_ai/_util/dotenv.py
src/inspect_ai/_util/entrypoints.py
src/inspect_ai/_util/environ.py
src/inspect_ai/_util/error.py
src/inspect_ai/_util/file.py
src/inspect_ai/_util/format.py
src/inspect_ai/_util/future.py
src/inspect_ai/_util/git.py
src/inspect_ai/_util/hash.py
src/inspect_ai/_util/hooks.py
src/inspect_ai/_util/html.py
src/inspect_ai/_util/http.py
src/inspect_ai/_util/httpx.py
src/inspect_ai/_util/images.py
src/inspect_ai/_util/interrupt.py
src/inspect_ai/_util/json.py
src/inspect_ai/_util/kvstore.py
src/inspect_ai/_util/list.py
src/inspect_ai/_util/logger.py
src/inspect_ai/_util/notebook.py
src/inspect_ai/_util/notgiven.py
src/inspect_ai/_util/package.py
src/inspect_ai/_util/path.py
src/inspect_ai/_util/pattern.py
src/inspect_ai/_util/platform.py
src/inspect_ai/_util/port_names.py
src/inspect_ai/_util/registry.py
src/inspect_ai/_util/retry.py
src/inspect_ai/_util/rich.py
src/inspect_ai/_util/samples.py
src/inspect_ai/_util/terminal.py
src/inspect_ai/_util/text.py
src/inspect_ai/_util/thread.py
src/inspect_ai/_util/throttle.py
src/inspect_ai/_util/trace.py
src/inspect_ai/_util/transcript.py
src/inspect_ai/_util/url.py
src/inspect_ai/_util/version.py
src/inspect_ai/_util/vscode.py
src/inspect_ai/_util/working.py
src/inspect_ai/_view/notify.py
src/inspect_ai/_view/schema.py
src/inspect_ai/_view/server.py
src/inspect_ai/_view/view.py
src/inspect_ai/_view/www/README.md
src/inspect_ai/approval/__init__.py
src/inspect_ai/approval/_apply.py
src/inspect_ai/approval/_approval.py
src/inspect_ai/approval/_approver.py
src/inspect_ai/approval/_auto.py
src/inspect_ai/approval/_call.py
src/inspect_ai/approval/_human/approver.py
src/inspect_ai/approval/_human/console.py
src/inspect_ai/approval/_human/manager.py
src/inspect_ai/approval/_human/panel.py
src/inspect_ai/approval/_human/util.py
src/inspect_ai/approval/_policy.py
src/inspect_ai/approval/_registry.py
src/inspect_ai/dataset/__init__.py
src/inspect_ai/dataset/_dataset.py
src/inspect_ai/dataset/_sources/csv.py
src/inspect_ai/dataset/_sources/example.py
src/inspect_ai/dataset/_sources/file.py
src/inspect_ai/dataset/_sources/hf.py
src/inspect_ai/dataset/_sources/json.py
src/inspect_ai/dataset/_sources/util.py
src/inspect_ai/dataset/_util.py
src/inspect_ai/log/__init__.py
src/inspect_ai/log/_bundle.py
src/inspect_ai/log/_condense.py
src/inspect_ai/log/_convert.py
src/inspect_ai/log/_file.py
src/inspect_ai/log/_log.py
src/inspect_ai/log/_message.py
src/inspect_ai/log/_recorders/__init__.py
src/inspect_ai/log/_recorders/create.py
src/inspect_ai/log/_recorders/eval.py
src/inspect_ai/log/_recorders/file.py
src/inspect_ai/log/_recorders/json.py
src/inspect_ai/log/_recorders/recorder.py
src/inspect_ai/log/_retry.py
src/inspect_ai/log/_samples.py
src/inspect_ai/log/_transcript.py
src/inspect_ai/model/__init__.py
src/inspect_ai/model/_cache.py
src/inspect_ai/model/_call_tools.py
src/inspect_ai/model/_chat_message.py
src/inspect_ai/model/_conversation.py
src/inspect_ai/model/_generate_config.py
src/inspect_ai/model/_image.py
src/inspect_ai/model/_model_call.py
src/inspect_ai/model/_model_output.py
src/inspect_ai/model/_model.py
src/inspect_ai/model/_openai.py
src/inspect_ai/model/_providers/anthropic.py
src/inspect_ai/model/_providers/azureai.py
src/inspect_ai/model/_providers/bedrock.py
src/inspect_ai/model/_providers/cloudflare.py
src/inspect_ai/model/_providers/goodfire.py
src/inspect_ai/model/_providers/google.py
src/inspect_ai/model/_providers/grok.py
src/inspect_ai/model/_providers/groq.py
src/inspect_ai/model/_providers/hf.py
src/inspect_ai/model/_providers/llama_cpp_python.py
src/inspect_ai/model/_providers/mistral.py
src/inspect_ai/model/_providers/mockllm.py
src/inspect_ai/model/_providers/none.py
src/inspect_ai/model/_providers/ollama.py
src/inspect_ai/model/_providers/openai_o1.py
src/inspect_ai/model/_providers/openai.py
src/inspect_ai/model/_providers/openrouter.py
src/inspect_ai/model/_providers/providers.py
src/inspect_ai/model/_providers/together.py
src/inspect_ai/model/_providers/util/__init__.py
src/inspect_ai/model/_providers/util/chatapi.py
src/inspect_ai/model/_providers/util/hf_handler.py
src/inspect_ai/model/_providers/util/hooks.py
src/inspect_ai/model/_providers/util/llama31.py
src/inspect_ai/model/_providers/util/util.py
src/inspect_ai/model/_providers/vertex.py
src/inspect_ai/model/_providers/vllm.py
src/inspect_ai/model/_reasoning.py
src/inspect_ai/model/_registry.py
src/inspect_ai/model/_render.py
src/inspect_ai/scorer/__init__.py
src/inspect_ai/scorer/_answer.py
src/inspect_ai/scorer/_choice.py
src/inspect_ai/scorer/_classification.py
src/inspect_ai/scorer/_common.py
src/inspect_ai/scorer/_match.py
src/inspect_ai/scorer/_metric.py
src/inspect_ai/scorer/_metrics/__init__.py
src/inspect_ai/scorer/_metrics/accuracy.py
src/inspect_ai/scorer/_metrics/mean.py
src/inspect_ai/scorer/_metrics/std.py
src/inspect_ai/scorer/_model.py
src/inspect_ai/scorer/_multi.py
src/inspect_ai/scorer/_pattern.py
src/inspect_ai/scorer/_reducer/__init__.py
src/inspect_ai/scorer/_reducer/reducer.py
src/inspect_ai/scorer/_reducer/registry.py
src/inspect_ai/scorer/_reducer/types.py
src/inspect_ai/scorer/_score.py
src/inspect_ai/scorer/_scorer.py
src/inspect_ai/scorer/_target.py
src/inspect_ai/solver/__init__.py
src/inspect_ai/solver/_basic_agent.py
src/inspect_ai/solver/_bridge/bridge.py
src/inspect_ai/solver/_bridge/patch.py
src/inspect_ai/solver/_chain.py
src/inspect_ai/solver/_critique.py
src/inspect_ai/solver/_fork.py
src/inspect_ai/solver/_human_agent/agent.py
src/inspect_ai/solver/_human_agent/commands/__init__.py
src/inspect_ai/solver/_human_agent/commands/clock.py
src/inspect_ai/solver/_human_agent/commands/command.py
src/inspect_ai/solver/_human_agent/commands/instructions.py
src/inspect_ai/solver/_human_agent/commands/note.py
src/inspect_ai/solver/_human_agent/commands/score.py
src/inspect_ai/solver/_human_agent/commands/status.py
src/inspect_ai/solver/_human_agent/commands/submit.py
src/inspect_ai/solver/_human_agent/install.py
src/inspect_ai/solver/_human_agent/panel.py
src/inspect_ai/solver/_human_agent/service.py
src/inspect_ai/solver/_human_agent/state.py
src/inspect_ai/solver/_human_agent/view.py
src/inspect_ai/solver/_limit.py
src/inspect_ai/solver/_multiple_choice.py
src/inspect_ai/solver/_plan.py
src/inspect_ai/solver/_prompt.py
src/inspect_ai/solver/_solver.py
src/inspect_ai/solver/_task_state.py
src/inspect_ai/solver/_transcript.py
src/inspect_ai/solver/_use_tools.py
src/inspect_ai/solver/_util.py
src/inspect_ai/tool/__init__.py
src/inspect_ai/tool/_tool_call.py
src/inspect_ai/tool/_tool_choice.py
src/inspect_ai/tool/_tool_def.py
src/inspect_ai/tool/_tool_description.py
src/inspect_ai/tool/_tool_info.py
src/inspect_ai/tool/_tool_params.py
src/inspect_ai/tool/_tool_transcript.py
src/inspect_ai/tool/_tool_with.py
src/inspect_ai/tool/_tool.py
src/inspect_ai/tool/_tools/_computer/__init__.py
src/inspect_ai/tool/_tools/_computer/_common.py
src/inspect_ai/tool/_tools/_computer/_computer.py
src/inspect_ai/tool/_tools/_computer/_resources/README.md
src/inspect_ai/tool/_tools/_computer/_resources/tool/_args.py
src/inspect_ai/tool/_tools/_computer/_resources/tool/_constants.py
src/inspect_ai/tool/_tools/_computer/_resources/tool/_logger.py
src/inspect_ai/tool/_tools/_computer/_resources/tool/_run.py
src/inspect_ai/tool/_tools/_computer/_resources/tool/_tool_result.py
src/inspect_ai/tool/_tools/_computer/_resources/tool/_x11_client.py
src/inspect_ai/tool/_tools/_computer/_resources/tool/computer_tool.py
src/inspect_ai/tool/_tools/_computer/test_args.py
src/inspect_ai/tool/_tools/_execute.py
src/inspect_ai/tool/_tools/_web_browser/__init__.py
src/inspect_ai/tool/_tools/_web_browser/_resources/accessibility_tree_node.py
src/inspect_ai/tool/_tools/_web_browser/_resources/accessibility_tree.py
src/inspect_ai/tool/_tools/_web_browser/_resources/cdp/__init__.py
src/inspect_ai/tool/_tools/_web_browser/_resources/cdp/a11y.py
src/inspect_ai/tool/_tools/_web_browser/_resources/cdp/dom_snapshot.py
src/inspect_ai/tool/_tools/_web_browser/_resources/cdp/dom.py
src/inspect_ai/tool/_tools/_web_browser/_resources/cdp/page.py
src/inspect_ai/tool/_tools/_web_browser/_resources/constants.py
src/inspect_ai/tool/_tools/_web_browser/_resources/mock_environment.py
src/inspect_ai/tool/_tools/_web_browser/_resources/playwright_browser.py
src/inspect_ai/tool/_tools/_web_browser/_resources/playwright_crawler.py
src/inspect_ai/tool/_tools/_web_browser/_resources/playwright_page_crawler.py
src/inspect_ai/tool/_tools/_web_browser/_resources/README.md
src/inspect_ai/tool/_tools/_web_browser/_resources/rectangle.py
src/inspect_ai/tool/_tools/_web_browser/_resources/rpc_client_helpers.py
src/inspect_ai/tool/_tools/_web_browser/_resources/scale_factor.py
src/inspect_ai/tool/_tools/_web_browser/_resources/test_accessibility_tree_node.py
src/inspect_ai/tool/_tools/_web_browser/_resources/test_playwright_crawler.py
src/inspect_ai/tool/_tools/_web_browser/_resources/test_rectangle.py
src/inspect_ai/tool/_tools/_web_browser/_resources/test_web_client.py
src/inspect_ai/tool/_tools/_web_browser/_resources/web_browser_rpc_types.py
src/inspect_ai/tool/_tools/_web_browser/_resources/web_client_new_session.py
src/inspect_ai/tool/_tools/_web_browser/_resources/web_client.py
src/inspect_ai/tool/_tools/_web_browser/_resources/web_server.py
src/inspect_ai/tool/_tools/_web_browser/_web_browser.py
src/inspect_ai/tool/_tools/_web_search.py
src/inspect_ai/tool/beta.py
src/inspect_ai/util/__init__.py
src/inspect_ai/util/_concurrency.py
src/inspect_ai/util/_console.py
src/inspect_ai/util/_conversation.py
src/inspect_ai/util/_display.py
src/inspect_ai/util/_json.py
src/inspect_ai/util/_panel.py
src/inspect_ai/util/_resource.py
src/inspect_ai/util/_sandbox/__init__.py
src/inspect_ai/util/_sandbox/context.py
src/inspect_ai/util/_sandbox/docker/cleanup.py
src/inspect_ai/util/_sandbox/docker/compose.py
src/inspect_ai/util/_sandbox/docker/config.py
src/inspect_ai/util/_sandbox/docker/docker.py
src/inspect_ai/util/_sandbox/docker/internal.py
src/inspect_ai/util/_sandbox/docker/prereqs.py
src/inspect_ai/util/_sandbox/docker/service.py
src/inspect_ai/util/_sandbox/docker/util.py
src/inspect_ai/util/_sandbox/environment.py
src/inspect_ai/util/_sandbox/events.py
src/inspect_ai/util/_sandbox/limits.py
src/inspect_ai/util/_sandbox/local.py
src/inspect_ai/util/_sandbox/registry.py
src/inspect_ai/util/_sandbox/self_check.py
src/inspect_ai/util/_sandbox/service.py
src/inspect_ai/util/_store_model.py
src/inspect_ai/util/_store.py
src/inspect_ai/util/_subprocess.py
src/inspect_ai/util/_subtask.py
src/inspect_ai/util/_throttle.py
tests/approval/test_approval.py
tests/conftest.py
tests/dataset/test_dataset.py
tests/log/test_eval_log.py
tests/log/test_eval_log/log_invalid.txt
tests/log/test_eval_log/log_length_stop_reason.txt
tests/log/test_eval_log/log_valid.txt
tests/log/test_eval_log/log_version_3.txt
tests/log/test_eval_log/log_with_nan.txt
tests/log/test_list_logs.py
tests/log/test_log_attachments.py
tests/log/test_log_formats.py
tests/log/test_log_level.py
tests/log/test_log_message_id.py
tests/log/test_log_streaming.py
tests/log/test_log_tags.py
tests/model/providers/test_anthropic.py
tests/model/providers/test_azureai.py
tests/model/providers/test_cloudflare.py
tests/model/providers/test_goodfire.py
tests/model/providers/test_google.py
tests/model/providers/test_grok.py
tests/model/providers/test_groq.py
tests/model/providers/test_hf.py
tests/model/providers/test_llama_cpp_python.py
tests/model/providers/test_openai_o1.py
tests/model/providers/test_openai.py
tests/model/providers/test_vertex.py
tests/model/providers/test_vllm.py
tests/model/test_api_key.py
tests/model/test_cache.py
tests/model/test_collapse_assistant_message.py
tests/model/test_collapse_user_message.py
tests/model/test_disable.py
tests/model/test_get_model.py
tests/model/test_is_rate_limit.py
tests/model/test_logprobs.py
tests/model/test_mock_model_llm.py
tests/model/test_no_model.py
tests/model/test_num_choices.py
tests/model/test_parse_tool_call.py
tests/model/test_reasoning_claude.py
tests/model/test_reasoning_content.py
tests/model/test_reasoning_parse.py
tests/model/test_stop_reason.py
tests/model/test_structured_output.py
tests/model/test_tool_result_images_as_user_message.py
tests/scorer/test_answer.py
tests/scorer/test_choice.py
tests/scorer/test_classification.py
tests/scorer/test_match.py
tests/scorer/test_metric.py
tests/scorer/test_model_graded.py
tests/scorer/test_multiscorer.py
tests/scorer/test_pattern.py
tests/scorer/test_reducers.py
tests/scorer/test_scorer.py
tests/scorer/test_task_score.py
tests/scorer/test_value_to_float.py
tests/solver/test_basic_agent.py
tests/solver/test_bridge.py
tests/solver/test_chain.py
tests/solver/test_fork.py
tests/solver/test_multiple_choice.py
tests/solver/test_prompt.py
tests/solver/test_score.py
tests/solver/test_setup.py
tests/solver/test_solver_decorator.py
tests/solver/test_solver_spec.py
tests/solver/test_solver.py
tests/solver/test_store_model.py
tests/solver/test_store.py
tests/solver/test_subtask.py
tests/solver/test_transcript.py
tests/test_eval_config.py
tests/test_eval_set.py
tests/test_eval.py
tests/test_examples.py
tests/test_extensions.py
tests/test_fail_on_error.py
tests/test_helpers/file_runner.py
tests/test_helpers/tasks.py
tests/test_helpers/tool_call_utils.py
tests/test_helpers/tools.py
tests/test_helpers/utils.py
tests/test_list_task.py
tests/test_log_dir/example_task/example_task.py
tests/test_log_dir/test_log_dir.py
tests/test_package/inspect_package/__init__.py
tests/test_package/inspect_package/_registry.py
tests/test_package/inspect_package/approvers/renamer.py
tests/test_package/inspect_package/modelapi/custom.py
tests/test_package/inspect_package/sandboxenv/podman.py
tests/test_package/inspect_package/score/scorer.py
tests/test_package/inspect_package/solvers/cot.py
tests/test_retry.py
tests/test_sample_id.py
tests/test_sample_limits.py
tests/test_task_attr.py
tests/test_task_chdir.py
tests/test_task_chdir/task1/task1.py
tests/test_task_chdir/task2/task2.py
tests/test_task_chdir/task3/task3.py
tests/test_task_list/multiple_dir/_decoy/testit.py
tests/test_task_list/multiple_dir/_decoy2.py
tests/test_task_list/multiple_dir/bar.py
tests/test_task_list/multiple_dir/foo.py
tests/test_task_list/multiple.py
tests/test_task_list/recurse/.folder3/epsilon.py
tests/test_task_list/recurse/folder1/_decoy.py
tests/test_task_list/recurse/folder1/theta.py
tests/test_task_list/recurse/folder2/.folder3/epsilon.py
tests/test_task_list/recurse/folder2/another.py
tests/test_task_list/recurse/folder2/first.py
tests/test_task_model.py
tests/test_task_with.py
tests/tools/test_bash_tool.py
tests/tools/test_max_exec_output.py
tests/tools/test_python_tool.py
tests/tools/test_sandbox_docker_and_local.py
tests/tools/test_sandbox_dockerfile.py
tests/tools/test_sandbox_tool_eval.py
tests/tools/test_tool_def.py
tests/tools/test_tool_images.py
tests/tools/test_tool_parse.py
tests/tools/test_tool_truncate.py
tests/tools/test_tool_types.py
tests/tools/test_tool_view.py
tests/tools/test_tool_with.py
tests/tools/test_tools.py
tests/tools/test_use_tools.py
tests/tools/test_web_browser.py
tests/util/sandbox/test_docker_compose_multiple_services.py
tests/util/sandbox/test_docker_filenames.py
tests/util/sandbox/test_docker_healthcheck.py
tests/util/sandbox/test_sandbox_service.py
tests/util/sandbox/test_sandbox_setup.py
tests/util/test_display_counter.py
tests/util/test_file.py
tests/util/test_format_template.py
tests/util/test_images.py
tests/util/test_kvstore.py
tests/util/test_media.py
tests/util/test_package.py
tests/util/test_registry.py
tests/util/test_str_to_float.py
tests/util/test_subprocess.py
tests/util/test_truncate.py
tests/view/test_bundle.py
tools/vscode/CHANGELOG.md
tools/vscode/README.md

================================================================
Files
================================================================

================
File: docs/_container_limits.md
================
### Max Sandboxes

The `max_sandboxes` option determines how many sandboxes can be executed in parallel. Individual sandbox providers can establish their own default limits (for example, the Docker provider has a default of `2 * os.cpu_count()`). You can modify this option as required, but be aware that container runtimes have resource limits, and pushing up against and beyond them can lead to instability and failed evaluations.

When a `max_sandboxes` is applied, an indicator at the bottom of the task status screen will be shown:

![](images/task-max-sandboxes.png)

Note that when `max_sandboxes` is applied this effectively creates a global `max_samples` limit that is equal to the `max_sandboxes`.

### Max Subprocesses

The `max_subprocesses` option determines how many subprocess calls can run in parallel. By default, this is set to `os.cpu_count()`. Depending on the nature of execution done inside sandbox environments, you might benefit from increasing or decreasing `max_subprocesses`.

### Max Samples

{{< include _max_samples.md >}}

================
File: docs/_errors_and_retries.md
================
## Errors and Retries {#sec-errors-and-retries}

When an evaluation task fails due to an error or is otherwise interrupted (e.g. by a Ctrl+C), an evaluation log is still written. In many cases errors are transient (e.g. due to network connectivity or a rate limit) and can be subsequently *retried*.

For these cases, Inspect includes an `eval-retry` command and `eval_retry()` function that you can use to resume tasks interrupted by errors (including [preserving samples](eval-logs.qmd#sec-sample-preservation) already completed within the original task). For example, if you had a failing task with log file `logs/2024-05-29T12-38-43_math_Gprr29Mv.json`, you could retry it from the shell with:

``` bash
$ inspect eval-retry logs/2024-05-29T12-38-43_math_43_math_Gprr29Mv.json
```

Or from Python with:

``` python
eval_retry("logs/2024-05-29T12-38-43_math_43_math_Gprr29Mv.json")
```

Note that retry only works for tasks that are created from `@task` decorated functions (as if a `Task` is created dynamically outside of an `@task` function Inspect does not know how to reconstruct it for the retry).

Note also that `eval_retry()` does not overwrite the previous log file, but rather creates a new one (preserving the `task_id` from the original file).

Here's an example of retrying a failed eval with a lower number of `max_connections` (the theory being that too many concurrent connections may have caused a rate limit error):

``` python
log = eval(my_task)[0]
if log.status != "success":
  eval_retry(log, max_connections = 3)
```

================
File: docs/_max_samples.md
================
Another consideration is `max_samples`, which is the maximum number of samples to run concurrently within a task. Larger numbers of concurrent samples will result in higher throughput, but will also result in completed samples being written less frequently to the log file, and consequently less total recovable samples in the case of an interrupted task.

By default, Inspect sets the value of `max_samples` to `max_connections + 1` (note that it would rarely make sense to set it _lower_ than `max_connections`). The default `max_connections` is 10, which will typically result in samples being written to the log frequently. On the other hand, setting a very large `max_connections` (e.g. 100 `max_connections` for a dataset with 100 samples) may result in very few recoverable samples in the case of an interruption. 

{{< include _setting_max_samples.md >}}

================
File: docs/_metadata_typing.md
================
If you want a more strongly typed interface to sample metadata, you can define a [Pydantic model](https://docs.pydantic.dev/latest/concepts/models/) and use it to both validate and read metadata.

For validation, pass a `BaseModel` derived class in the `FieldSpec`. The interface to metadata is read-only so you must also specify `frozen=True`. For example:

```python
from pydantic import BaseModel

class PopularityMetadata(BaseModel, frozen=True):
    category: str
    label_confidence: float

dataset = json_dataset(
    "popularity.jsonl",
    FieldSpec(
        input="question",
        target="answer_matching_behavior",
        id="question_id",
        metadata=PopularityMetadata,
    ),
)
```

To read metadata in a typesafe fashion, use the `metadata_as()` method on `Sample` or `TaskState`:

```python
metadata = state.metadata_as(PopularityMetadata)
```

Note again that the intended semantics of `metadata` are read-only, so attempting to write into the returned metadata will raise a Pydantic `FrozenInstanceError`. 

If you need per-sample mutable data, use the [sample store](agents-api.qmd#sample-store), which also supports [typing](agents-api.qmd#store-typing) using Pydantic models.

================
File: docs/_model-providers.md
================
|  |  |
|------------------------------------|------------------------------------|
| Lab APIs | [OpenAI](providers.qmd#openai), [Anthropic](providers.qmd#anthropic), [Google](providers.qmd#google), [Grok](providers.qmd#grok), [Mistral](providers.qmd#mistral), [DeepSeek](providers.qmd#deepseek) |
| Cloud APIs | [AWS Bedrock](providers.qmd#aws-bedrock), [Azure AI](providers.qmd#azure-ai), [Vertex AI](providers.qmd#vertex-ai) |
| Open (Hosted) | [Groq](providers.qmd#groq), [Together AI](providers.qmd#together-ai), [Cloudflare](providers.qmd#cloudflare), [Goodfire](providers.qmd#goodfire) |
| Open (Local) | [Hugging Face](providers.qmd#hugging-face), [vLLM](providers.qmd#vllm), [Ollama](providers.qmd#ollama), [Lllama-cpp-python](providers.qmd#llama-cpp-python) |

<br/>

If the provider you are using is not listed above, you may still be able to use it if:

1. It is available via OpenRouter (see the docs on using [OpenRouter](providers.qmd#openrouter) with Inspect).

2. It provides an OpenAI compatible API endpoint. In this scenario, use the Inspect [OpenAI](providers.qmd#openai) interface and set the `OPENAI_BASE_URL` environment variable to the apprpriate value for your provider.

You can also create [Model API Extensions](extensions.qmd#model-apis) to add model providers using their native interface.

================
File: docs/_sample-preservation.md
================
### Sample Preservation {#sec-sample-preservation}

When retrying a log file, Inspect will attempt to re-use completed samples from the original task. This can result in substantial time and cost savings compared to starting over from the beginning.

#### IDs and Shuffling

An important constraint on the ability to re-use completed samples is matching them up correctly with samples in the new task. To do this, Inspect requires stable unique identifiers for each sample. This can be achieved in 1 of 2 ways:

1.  Samples can have an explicit `id` field which contains the unique identifier; or

2.  You can rely on Inspect's assignment of an auto-incrementing `id` for samples, however this *will not work correctly* if your dataset is shuffled. Inspect will log a warning and not re-use samples if it detects that the `dataset.shuffle()` method was called, however if you are shuffling by some other means this automatic safeguard won't be applied.

If dataset shuffling is important to your evaluation and you want to preserve samples for retried tasks, then you should include an explicit `id` field in your dataset.

#### Max Samples

{{< include _max_samples.md >}}

================
File: docs/_sandboxenv-interface.md
================
``` python
class SandboxEnvironment:
   
    async def exec(
        self,
        cmd: list[str],
        input: str | bytes | None = None,
        cwd: str | None = None,
        env: dict[str, str] = {},
        user: str | None = None,
        timeout: int | None = None,
        timeout_retry: bool = True
    ) -> ExecResult[str]:
        """
        Raises:
          TimeoutError: If the specified `timeout` expires.
          UnicodeDecodeError: If an error occurs while
            decoding the command output.
          PermissionError: If the user does not have
            permission to execute the command.
          OutputLimitExceededError: If an output stream
            exceeds the 10 MiB limit.
        """
        ...

    async def write_file(
        self, file: str, contents: str | bytes
    ) -> None:
        """
        Raises:
          PermissionError: If the user does not have
            permission to write to the specified path.
          IsADirectoryError: If the file exists already and 
            is a directory.
        """
        ...

    async def read_file(
        self, file: str, text: bool = True
    ) -> Union[str | bytes]:
        """
        Raises:
          FileNotFoundError: If the file does not exist.
          UnicodeDecodeError: If an encoding error occurs 
            while reading the file.
            (only applicable when `text = True`)
          PermissionError: If the user does not have
            permission to read from the specified path.
          IsADirectoryError: If the file is a directory.
          OutputLimitExceededError: If the file size
            exceeds the 100 MiB limit.
        """
        ...

    async def connection(self) -> SandboxConnection:
        """
        Raises:
           NotImplementedError: For sandboxes that don't provide connections
           ConnectionError: If sandbox is not currently running.
        """
```

The `read_file()` method should preserve newline constructs (e.g. crlf should be preserved not converted to lf). This is equivalent to specifying `newline=""` in a call to the Python `open()` function. Note that `write_file()` automatically creates parent directories as required if they don't exist.

The `connection()` method is optional, and provides commands that can be used to login to the sandbox container from a terminal or IDE.

Note that to deal with potential unreliability of container services, the `exec()` method includes a `timeout_retry` parameter that defaults to `True`. For sandbox implementations this parameter is _advisory_ (they should only use it if potential unreliablity exists in their runtime). No more than 2 retries should be attempted and both with timeouts less than 60 seconds. If you are executing commands that are not idempotent (i.e. the side effects of a failed first attempt may affect the results of subsequent attempts) then you can specify `timeout_retry=False` to override this behavior.

For each method there is a documented set of errors that are raised: these are _expected_ errors and can either be caught by tools or allowed to propagate in which case they will be reported to the model for potential recovery. In addition, _unexpected_ errors may occur (e.g. a networking error connecting to a remote container): these errors are not reported to the model and fail the `Sample` with an error state.

================
File: docs/_setting_max_samples.md
================
::: {.callout-note appearance="simple"}
If your task involves tool calls and/or sandboxes, then you will likely want to set `max_samples` to greater than `max_connections`, as your samples will sometimes be calling the model (using up concurrent connections) and sometimes be executing code in the sandbox (using up concurrent subprocess calls). While running tasks you can see the utilization of connections and subprocesses in realtime and tune your `max_samples` accordingly.
:::

================
File: docs/_shuffling-choices.md
================
When working with datasets that contain multiple-choice options, you can randomize the order of these choices during data loading. The shuffling operation automatically updates any corresponding target values to maintain correct answer mappings.

For datasets that contain `choices`, you can shuffle the choices when the data is loaded. Shuffling choices will randomly re-order the choices and update the sample's target value or values to align with the shuffled choices.

There are two ways to shuffle choices:

```python
# Method 1: Using the dataset method
dataset = dataset.shuffle_choices()

# Method 2: During dataset loading
dataset = json_dataset("data.jsonl", shuffle_choices=True)
```

For reproducible shuffling, you can specify a random seed:

```python
# Using a seed with the dataset method
dataset = dataset.shuffle_choices(seed=42)

# Using a seed during loading
dataset = json_dataset("data.jsonl", shuffle_choices=42)
```

================
File: docs/_store_typing.md
================
If you prefer a typesafe interface to the sample store, you can define a [Pydantic model](https://docs.pydantic.dev/latest/concepts/models/) which reads and writes values into the store. There are several benefits to using Pydantic models for store access:

1. You can provide type annotations and validation rules for all fields.
2. Default values for all fields are declared using standard Pydantic syntax.
3. Store names are automatically namespaced (to prevent conflicts between multiple store accessors).

#### Definition

First, derive a class from `StoreModel` (which in turn derives from Pydantic `BaseModel`):

```python
from pydantic import Field
from inspect_ai.util import StoreModel

class Activity(StoreModel):
    active: bool = Field(default=False)
    tries: int = Field(default=0)
    actions: list[str] = Field(default_factory=list)
```

Note that we define defaults for all fields. This is generally required so that you can initialise your Pydantic model from an empty store. For collections (`list` and `dict`) you should use `default_factory` so that each instance gets its own default.

#### Usage

Use the `store_as()` function to get a typesafe interface to the store based on your model:

```python
# typed interface to store from state
activity = state.store_as(Activity)
activity.active = True
activity.tries += 1

# global store_as() function (e.g. for use from tools)
from inspect_ai.util import store_as
activity = store_as(Activity)
```

Note that all instances of `Activity` created within a running sample share the same sample `Store` so can see each other's changes. For example, you can call `state.store_as()` in multiple solvers and/or scorers and it will resolve to the same sample-scoped instance. 

The names used in the underlying `Store` are namespaced to prevent collisions with other `Store` accessors. For example, the `active` field in the `Activity` class is written to the store with the name `Activity:active`.

#### Explicit Store

The `store_as()` function automatically binds to the current sample `Store`. You can alternatively create an explicit `Store` and pass it directly to the model (e.g. for testing purposes):

```python
from inspect_ai.util import Store
store = Store()
activity = Activity(store=store)
```

================
File: docs/_tools-annotations-required.md
================
Note that we provide type annotations for both arguments:

``` python
async def execute(x: int, y: int)
```

Further, we provide descriptions for each parameter in the documention comment:

```python
Args:
    x: First number to add.
    y: Second number to add.
```

Type annotations and descriptions are *required* for tool declarations so that the model can be informed which types to pass back to the tool function and what the purpose of each parameter is.

================
File: docs/_tools-scaffold.md
================
1.  Redirect the model to another trajectory if its not on a productive course.
2.  Exercise more fine grained control over which, when, and how many tool calls are made, and how tool calling errors are handled.
3.  Have multiple `generate()` passes each with a distinct set of tools.

To do this, create a solver that emulates the default tool use loop and provides additional customisation as required. 

### Example

For example, here is a complete solver agent that has essentially the same implementation as the default `generate()` function:

``` python
@solver
def agent_loop(message_limit: int = 50):
    async def solve(state: TaskState, generate: Generate):

        # establish messages limit so we have a termination condition
        state.message_limit = message_limit

        try:
            # call the model in a loop
            while not state.completed:
                # call model
                output = await get_model().generate(state.messages, state.tools)

                # update state
                state.output = output
                state.messages.append(output.message)

                # make tool calls or terminate if there are none
                if output.message.tool_calls:
                    state.messages.extend(call_tools(output.message, state.tools))
                else:
                    break
        except SampleLimitExceededError as ex:
            raise ex.with_state(state)

        return state

    return solve
```

Solvers can set the `state.completed` flag to indicate that the sample is complete, so we check it at the top of the loop. When sample limits (e.g. tokens or messages) are exceeded an exception is thrown, so we re-raise it along with the current state of our agent loop.

You can imagine several ways you might want to customise this loop:

1.  Adding another termination condition for the output satisfying some criteria.
2.  Urging the model to keep going after it decides to stop calling tools.
3.  Examining and possibly filtering the tool calls before invoking `call_tools()`
4.  Adding a critique / reflection step between tool calling and generate.
5. [Forking](agents-api.qmd#sec-forking) the `TaskState` and exploring several trajectories.

### Stop Reasons {#sec-stop-reasons}

One thing that a custom scaffold may do is try to recover from various conditions that cause the model to stop generating. You can find the reason that generation stopped in the `stop_reason` field of `ModelOutput`. 

For example:, if you have written a scaffold loop that continues calling the model even after it stops calling tools, there may be values of `stop_reason` that indicate that the loop should terminate anyway (because the error will just keep repeating on subsequent calls to the model). For example, the [basic agent](agents.qmd#sec-basic-agent) checks for `stop_reason` and exits if there is a context window overflow:

```python
# check for stop reasons that indicate we should terminate
if state.output.stop_reason == "model_length":
    transcript().info(
        f"Agent terminated (reason: {state.output.stop_reason})"
    )
    break
```

Here are the possible values for `StopReason` :

| Stop Reason | Description |
|----|----|
| `stop` | The model hit a natural stop point or a provided stop sequence |
| `max_tokens` | The maximum number of tokens specified in the request was reached. |
| `model_length` | The model's context length was exceeded. |
| `tool_calls` | The model called a tool |
| `content_filter` | Content was omitted due to a content filter. |
| `unknown` | Unknown (e.g. unexpected runtime error) |

: {tbl-colwidths=\[35,65\]}

### Error Handling

By default expected errors (e.g. file not found, insufficient permission, timeouts, output limit exceeded etc.) are forwarded to the model for possible recovery. If you would like to intervene in the default error handling then rather than immediately appending the list of assistant messages returned from `call_tools()` to `state.messages` (as shown above), check the error property of these messages (which will be `None` in the case of no error) and proceed accordingly.

================
File: docs/evals/import.py
================
from pathlib import Path
import yaml
PATH = Path(__file__).parent
with open(PATH / "listing.yml", "r") as f:
    records = next(yaml.safe_load_all(f))
for record in records:
    record["categories"] = [record["group"]]
    if "tags" in record:
        record["categories"].extend(record["tags"])
    record["tasks"] = [task["name"] for task in record["tasks"]]
with open(PATH / "evals.yml", "w") as f:
    yaml.safe_dump(records, f)

================
File: docs/llms.txt
================
# Inspect AI

> Inspect AI is a Python framework for large language model evaluations created by the [UK AI Security Institute](https://aisi.gov.uk). Inspect provides many built-in components, including facilities for prompt engineering, tool usage, multi-turn dialog, and model graded evaluations. Extensions to Inspect (e.g. to support new elicitation and scoring techniques) can be provided by other Python packages.

## Docs

- [Tutorial](https://inspect.ai-safety-institute.org.uk/tutorial.html.md): Step-by-step walkthroughs of several basic examples of Inspect evaluations.
- [Options](https://inspect.ai-safety-institute.org.uk/options.html.md): Covers the various options available for evaluations as well as how to manage model credentials.
- [Log Viewer](https://inspect.ai-safety-institute.org.uk/log-viewer.html.md): Goes into more depth on how to use Inspect View to develop and debug evaluations, including how to provide additional log metadata and how to integrate it with Python's standard logging module.
- [VS Code](https://inspect.ai-safety-institute.org.uk/vscode.html.md) Provides documentation on using the Inspect VS Code Extension to run, tune, debug, and visualise evaluations.
- [Tasks](https://inspect.ai-safety-institute.org.uk/tasks.html.md) bring together datasets, solvers, and scorers to define a evaluation. This section explores strategies for creating flexible and re-usable tasks.
- [Datasets](https://inspect.ai-safety-institute.org.uk/datasets.html.md): Datasets provide samples to evaluation tasks. This section illustrates how to adapt various data sources for use with Inspect, as well as how to include multi-modal data (images, etc.) in your datasets.
- [Solvers](https://inspect.ai-safety-institute.org.uk/solvers.html.md): Solvers are the heart of Inspect, and encompass prompt engineering and various other elicitation strategies. Here we cover using the built-in solvers and creating your own more sophisticated ones.
- [Tools](https://inspect.ai-safety-institute.org.uk/tools.html.md): Tools provide a means of extending the capabilities of models by registering Python functions for them to call. This section describes how to create custom tools and use them in evaluations.
- [Scorers](https://inspect.ai-safety-institute.org.uk/scorers.html.md): Scorers evaluate the work of solvers and aggregate scores into metrics. Sophisticated evals often require custom scorers that use models to evaluate output. This section covers how to create them.
- [Models](https://inspect.ai-safety-institute.org.uk/models.html.md): Models provide a uniform API for both evaluating a variety of large language models and using models within evaluations (e.g. for critique or grading).
- [Providers](ttps://inspect.ai-safety-institute.org.uk/providers.html.md) covers usage details and available options for the various supported providers.
- [Caching](https://inspect.ai-safety-institute.org.uk/caching.html.md): Caching enables you to cache model output to reduce the number of API calls made, saving both time and expense.
- [Multimodal](https://inspect.ai-safety-institute.org.uk/multimodal.html.md) This article describes how to use images, audio, and video in evaluations.
- [Reasoning](https://inspect.ai-safety-institute.org.uk/reasoning.html.md) documents the additional options and data available for reasoning models.
- [Structured Output](https://inspect.ai-safety-institute.org.uk/structured.html.md) explains how to constrain model output to a particular JSON schema.

- [Agents](https://inspect.ai-safety-institute.org.uk/agents.html.md): Agents combine planning, memory, and tool usage to pursue more complex, longer horizon tasks. This section describes how to build agent evaluations with Inspect.
- [Sandboxing](https://inspect.ai-safety-institute.org.uk/sandboxing.html.md): Enables you to isolate code generated by models as well as set up more complex computing environments for tasks. 
- [Agent API](https://inspect.ai-safety-institute.org.uk/agents-api.html.md): This article describes advanced Inspect APIs available for creating evaluations with agents.
- [Agent Bridge](agent-bridge.qmd): Facility for integrating agents from 3rd party frameworks like AutoGen or LangChain.
- [Human Agent](https://inspect.ai-safety-institute.org.uk/human-agent.html.md): This article describes the `human_agent()` solver which enables human baselining for computing tasks.
- [Approval](https://inspect.ai-safety-institute.org.uk/approval.html.md): Approvals enable you to create fine-grained policies for approving tool calls made by models.
- [Eval Logs](https://inspect.ai-safety-institute.org.uk/eval-logs.html.md): Explores how to get the most out of evaluation logs for developing, debugging, and analyzing evaluations.
- [Eval Sets](https://inspect.ai-safety-institute.org.uk/eval-sets.html.md): Covers Inspect's features for describing, running, and analysing larger sets of evaluation tasks.
- [Errors and Limits](https://inspect.ai-safety-institute.org.uk/errors-and-limits.html.md): This article covers various techniques for dealing with unexpected errors and setting limits on evaluation tasks and samples, including retrying failed evaluations, establishing a threshold of samples to tolerate errors for before failing an evaluation, and setting a maximum number of messages, tokens, or elapsed seconds in a sample before forcing the solver to give up.
- [Typing](https://inspect.ai-safety-institute.org.uk/typing.html.md): Provides guideance on using static type checking with Inspect, including creating typed interfaces to untyped storage (i.e. sample metadata and store).
- [Tracing](https://inspect.ai-safety-institute.org.uk/tracing.html.md): Describes advanced execution tracing tools used to diagnose runtime issues.
- [Parallelism](https://inspect.ai-safety-institute.org.uk/parallelism.html.md): Delves into how to obtain maximum performance for evaluations. Inspect uses a highly parallel async architecture---here we cover how to tune this parallelism (e.g to stay under API rate limits or to not overburden local compute) for optimal throughput.
- [Interactivity](https://inspect.ai-safety-institute.org.uk/interactivity.html.md): Covers various ways to introduce user interaction into the implementation of tasks (for example, confirming consequential actions or prompting the model dynamically based on the trajectory of the evaluation).
- [Extensions](https://inspect.ai-safety-institute.org.uk/extensions.html.md) describes the various ways you can extend Inspect, including adding support for new Model APIs, tool execution environments, and storage platforms (for datasets, prompts, and logs).

## Reference: Python API

- [inspect_ai](https://inspect.ai-safety-institute.org.uk/reference/inspect_ai.html.md) describes the core types used to create tasks and run evaluations.
- [inspect_ai.solver](https://inspect.ai-safety-institute.org.uk/reference/inspect_ai.solver.html.md) describes built in solvers as well as the types used to create custom solvers.
- [inspect_ai.tool](https://inspect.ai-safety-institute.org.uk/reference/inspect_ai.tool.html.md) describes built in tools as well as the types used to create custom tools.
- [inspect_ai.scorer](https://inspect.ai-safety-institute.org.uk/reference/inspect_ai.scorer.html.md) describes built in scorers as well as the types used to create custom scorers.
- [inspect_ai.model](https://inspect.ai-safety-institute.org.uk/reference/inspect_ai.model.html.md) covers using the Inspect model API for accessing various language models.
- [inspect_ai.dataset](https://inspect.ai-safety-institute.org.uk/reference/inspect_ai.dataset.html.md) describes the types used to read and manipulate datasets and samples.
- [inspect_ai.approval](https://inspect.ai-safety-institute.org.uk/reference/inspect_ai.approval.html.md) covers using built in approvers as well as the types used to create custom approvers and approval policies.
- [inspect_ai.log](https://inspect.ai-safety-institute.org.uk/reference/inspect_ai.log.html.md) describes the types used to list, read, write, and traverse the contents of eval log files.
- [inspect_ai.util](https://inspect.ai-safety-institute.org.uk/reference/inspect_ai.util.html.md) covers various utility functions for concurrency, sandboxes, the store, and more.

## Reference: Command Line 

- [inspect_eval](https://inspect.ai-safety-institute.org.uk/reference/inspect_eval.html.md): Evaluate one or more tasks.
- [inspect_eval-retry](https://inspect.ai-safety-institute.org.uk/reference/inspect_eval-retry.html.md): Retry an evaluation task.
- [inspect_eval-set](https://inspect.ai-safety-institute.org.uk/reference/inspect_eval-set.html.md): Evaluate a set of tasks with retries.
- [inspect_score](https://inspect.ai-safety-institute.org.uk/reference/inspect_score.html.md): Score a previous evaluation run.
- [inspect_view](https://inspect.ai-safety-institute.org.uk/reference/inspect_view.html.md): Inspect log viewer.
- [inspect_log](https://inspect.ai-safety-institute.org.uk/reference/inspect_log.html.md): Query, read, write, and convert logs.
- [inspect_trace](https://inspect.ai-safety-institute.org.uk/reference/inspect_trace.html.md): List and read execution traces.
- [inspect_sandbox](https://inspect.ai-safety-institute.org.uk/reference/inspect_sandbox.html.md): Manage sandbox environments.
- [inspect_cache](https://inspect.ai-safety-institute.org.uk/reference/inspect_cache.html.md): Manage the Inspect model cache.
- [inspect_list](https://inspect.ai-safety-institute.org.uk/reference/inspect_list.html.md): List tasks on the filesystem.
- [inspect_info](https://inspect.ai-safety-institute.org.uk/reference/inspect_info.html.md): Read version and configuration.

================
File: docs/reference/filter/commands.py
================
# (C) Datadog, Inc. 2020-present
# All rights reserved
# Licensed under the Apache license (see LICENSE)
# from https://github.com/mkdocs/mkdocs-click/blob/master/mkdocs_click/_docs.py
from __future__ import annotations
import importlib
import inspect
from contextlib import ExitStack, contextmanager
from typing import Any, Iterator, cast
import click
from markdown.extensions.toc import slugify
def make_command_docs(
    command: str,
    depth: int = 0,
    style: str = "table",
    remove_ascii_art: bool = False,
    show_hidden: bool = False,
    list_subcommands: bool = True,
    has_attr_list: bool = True,
) -> Iterator[str]:
    """Create the Markdown lines for a command and its sub-commands."""
    command = command.replace("-", "_")
    module = "eval" if command.startswith("eval") else command
    for line in _recursively_make_command_docs(
        f"inspect {command}",
        load_command(f"inspect_ai._cli.{module}", f"{command}_command"),
        depth=depth,
        style=style,
        remove_ascii_art=remove_ascii_art,
        show_hidden=show_hidden,
        list_subcommands=list_subcommands,
        has_attr_list=has_attr_list,
    ):
        if line.strip() == "\b":
            continue
        yield line
def _recursively_make_command_docs(
    prog_name: str,
    command: click.BaseCommand,
    parent: click.Context | None = None,
    depth: int = 0,
    style: str = "plain",
    remove_ascii_art: bool = False,
    show_hidden: bool = False,
    list_subcommands: bool = False,
    has_attr_list: bool = False,
) -> Iterator[str]:
    """Create the raw Markdown lines for a command and its sub-commands."""
    ctx = _build_command_context(prog_name=prog_name, command=command, parent=parent)
    if ctx.command.hidden and not show_hidden:
        return
    subcommands = _get_sub_commands(ctx.command, ctx)
    if parent is not None:
        yield from _make_title(ctx, depth, has_attr_list=has_attr_list)
    yield from _make_description(ctx, remove_ascii_art=remove_ascii_art)
    yield from _make_usage(ctx)
    if len(subcommands) == 0:
        yield from _make_options(ctx, style, show_hidden=show_hidden)
        return
    if list_subcommands:
        yield from _make_subcommands_links(
            subcommands,
            ctx,
            has_attr_list=has_attr_list,
            show_hidden=show_hidden,
        )
    for command in subcommands:
        yield from _recursively_make_command_docs(
            cast(str, command.name),
            command,
            parent=ctx,
            depth=depth + 1,
            style=style,
            show_hidden=show_hidden,
            list_subcommands=list_subcommands,
            has_attr_list=has_attr_list,
        )
def _build_command_context(
    prog_name: str, command: click.BaseCommand, parent: click.Context | None
) -> click.Context:
    return click.Context(cast(click.Command, command), info_name=prog_name, parent=parent)
def _get_sub_commands(command: click.Command, ctx: click.Context) -> list[click.Command]:
    """Return subcommands of a Click command."""
    subcommands = getattr(command, "commands", {})
    if subcommands:
        return list(subcommands.values())
    if not isinstance(command, click.MultiCommand):
        return []
    subcommands = []
    for name in command.list_commands(ctx):
        subcommand = command.get_command(ctx, name)
        assert subcommand is not None
        subcommands.append(subcommand)
    return subcommands
def _make_title(ctx: click.Context, depth: int, *, has_attr_list: bool) -> Iterator[str]:
    """Create the Markdown heading for a command."""
    if has_attr_list:
        yield from _make_title_full_command_path(ctx, depth)
    else:
        yield from _make_title_basic(ctx, depth)
def _make_title_basic(ctx: click.Context, depth: int) -> Iterator[str]:
    """Create a basic Markdown heading for a command."""
    yield f"{'#' * (depth + 1)} {ctx.info_name}"
    yield ""
def _make_title_full_command_path(ctx: click.Context, depth: int) -> Iterator[str]:
    """Create the markdown heading for a command, showing the full command path.
    This style accomodates nested commands by showing:
    * The full command path for headers and permalinks (eg `# git commit` and `http://localhost:8000/#git-commit`)
    * The command leaf name only for TOC entries (eg `* commit`).
    We do this because a TOC naturally conveys the hierarchy, whereas headings and permalinks should be namespaced to
    convey the hierarchy.
    See: https://github.com/mkdocs/mkdocs-click/issues/35
    """
    text = ctx.command_path  # 'git commit'
    permalink = slugify(ctx.command_path, "-")  # 'git-commit'
    toc_label = ctx.info_name  # 'commit'
    # Requires `attr_list` extension, see: https://python-markdown.github.io/extensions/toc/#custom-labels
    attributes = f"#{permalink} data-toc-label='{toc_label}'"
    yield f"{'#' * (depth + 1)} {text} {{{attributes}}}"
    yield ""
def _make_description(ctx: click.Context, remove_ascii_art: bool = False) -> Iterator[str]:
    """Create markdown lines based on the command's own description."""
    help_string = ctx.command.help or ctx.command.short_help
    if not help_string:
        return
    # https://github.com/pallets/click/pull/2151
    help_string = inspect.cleandoc(help_string)
    if not remove_ascii_art:
        yield from help_string.splitlines()
        yield ""
        return
    skipped_ascii_art = True
    for i, line in enumerate(help_string.splitlines()):
        if skipped_ascii_art is False:
            if not line.strip():
                skipped_ascii_art = True
                continue
        elif i == 0 and line.strip() == "\b":
            skipped_ascii_art = False
        if skipped_ascii_art:
            yield line
    yield ""
def _make_usage(ctx: click.Context) -> Iterator[str]:
    """Create the Markdown lines from the command usage string."""
    # Gets the usual 'Usage' string without the prefix.
    formatter = ctx.make_formatter()
    pieces = ctx.command.collect_usage_pieces(ctx)
    formatter.write_usage(ctx.command_path, " ".join(pieces), prefix="")
    usage = formatter.getvalue().rstrip("\n")
    yield "#### Usage"
    yield ""
    yield "```text"
    yield usage
    yield "```"
    yield ""
def _make_options(
    ctx: click.Context, style: str = "plain", show_hidden: bool = False
) -> Iterator[str]:
    """Create the Markdown lines describing the options for the command."""
    if style == "plain":
        return _make_plain_options(ctx, show_hidden=show_hidden)
    elif style == "table":
        return _make_table_options(ctx, show_hidden=show_hidden)
    else:
        raise RuntimeError(
            f"{style} is not a valid option style, which must be either `plain` or `table`."
        )
@contextmanager
def _show_options(ctx: click.Context) -> Iterator[None]:
    """Context manager that temporarily shows all hidden options."""
    options = [
        opt for opt in ctx.command.get_params(ctx) if isinstance(opt, click.Option) and opt.hidden
    ]
    try:
        for option in options:
            option.hidden = False
        yield
    finally:
        for option in options:
            option.hidden = True
def _make_plain_options(ctx: click.Context, show_hidden: bool = False) -> Iterator[str]:
    """Create the plain style options description."""
    with ExitStack() as stack:
        if show_hidden:
            stack.enter_context(_show_options(ctx))
        formatter = ctx.make_formatter()
        click.Command.format_options(ctx.command, ctx, formatter)
        option_lines = formatter.getvalue().splitlines()
        # First line is redundant "Options"
        option_lines = option_lines[1:]
        if not option_lines:  # pragma: no cover
            # We expect at least `--help` to be present.
            raise RuntimeError("Expected at least one option")
        yield "#### Options"
        yield ""
        yield "```text"
        yield from option_lines
        yield "```"
        yield ""
# Unicode "Vertical Line" character (U+007C), HTML-compatible.
# "\|" (escaped pipe) would work, too, but linters don't like it in literals.
# https://stackoverflow.com/questions/23723396/how-to-show-the-pipe-symbol-in-markdown-table
_HTML_PIPE = "&#x7C;"
def _format_table_option_type(option: click.Option) -> str:
    typename = option.type.name
    if isinstance(option.type, click.Choice):
        # @click.option(..., type=click.Choice(["A", "B", "C"]))
        # -> choices (`A` | `B` | `C`)
        choices = f" {_HTML_PIPE} ".join(f"`{choice}`" for choice in option.type.choices)
        return f"{typename} ({choices})"
    if isinstance(option.type, click.DateTime):
        # @click.option(..., type=click.DateTime(["A", "B", "C"]))
        # -> datetime (`%Y-%m-%d` | `%Y-%m-%dT%H:%M:%S` | `%Y-%m-%d %H:%M:%S`)
        formats = f" {_HTML_PIPE} ".join(f"`{fmt}`" for fmt in option.type.formats) 
        return f"{typename} ({formats})"
    if isinstance(option.type, (click.IntRange, click.FloatRange)):
        if option.type.min is not None and option.type.max is not None:  
            # @click.option(..., type=click.IntRange(min=0, max=10))
            # -> integer range (between `0` and `10`)
            return f"{typename} (between `{option.type.min}` and `{option.type.max}`)" 
        elif option.type.min is not None:
            # @click.option(..., type=click.IntRange(min=0))
            # -> integer range (`0` and above)
            return f"{typename} (`{option.type.min}` and above)" 
        else:
            # @click.option(..., type=click.IntRange(max=10))
            # -> integer range (`10` and below)
            return f"{typename} (`{option.type.max}` and below)" 
    # -> "boolean", "text", etc.
    return typename
def _format_table_option_row(option: click.Option) -> str:
    # Example: @click.option("-V, --version/--show-version", is_flag=True, help="Show version info.")
    # -> "`-V`, `--version`"
    names = ", ".join(f"`{opt}`" for opt in option.opts)
    if option.secondary_opts:
        # -> "`-V`, `--version` / `--show-info`"
        names += " / "
        names += ", ".join(f"`{opt}`" for opt in option.secondary_opts)
    # -> "boolean"
    value_type = _format_table_option_type(option)
    # -> "Show version info."
    description = option.help if option.help is not None else "N/A"
    # -> `False`
    none_default_msg = "_required" if option.required else "None"
    default = f"`{option.default}`" if option.default is not None else none_default_msg
    # -> "| `-V`, `--version` / `--show-version` | boolean | Show version info. | `False` |"
    return f"| {names} | {value_type} | {description} | {default} |"
def _make_table_options(ctx: click.Context, show_hidden: bool = False) -> Iterator[str]:
    """Create the table style options description."""
    options = [param for param in ctx.command.get_params(ctx) if isinstance(param, click.Option)]
    options = [option for option in options if not option.hidden or show_hidden]
    option_rows = [_format_table_option_row(option) for option in options]
    yield "#### Options"
    yield ""
    yield "| Name | Type | Description | Default |"
    yield "| ---- | ---- | ----------- | ------- |"
    yield from option_rows
    yield ": {.sm .borderless tbl-colwidths=[25,15,50,10]}"
    yield ""
def _make_subcommands_links(
    subcommands: list[click.Command],
    parent: click.Context,
    has_attr_list: bool,
    show_hidden: bool,
) -> Iterator[str]:
    yield "#### Subcommands"
    yield ""
    yield "|  |  |"
    yield "| ---- | ----------- |"
    for command in subcommands:
        command_name = cast(str, command.name)
        ctx = _build_command_context(command_name, command, parent)
        if ctx.command.hidden and not show_hidden:
            continue
        command_bullet = (
            command_name
            if not has_attr_list
            else f"[{command_name}](#{slugify(ctx.command_path, '-')})"
        )
        help_string = ctx.command.short_help or ctx.command.help
        if help_string is not None:
            help_string = help_string.splitlines()[0]
        else:
            help_string = "*No description was provided with this command.*"
        yield f"| {command_bullet} | {help_string} |"
    yield ": {.borderless tbl-colwidths=[35,65]}"
    yield ""
def load_command(module: str, attribute: str) -> click.BaseCommand:
    """
    Load and return the Click command object located at '<module>:<attribute>'.
    """
    command = _load_obj(module, attribute)
    if not isinstance(command, click.BaseCommand):
        raise RuntimeError(
            f"{attribute!r} must be a 'click.BaseCommand' object, got {type(command)}"
        )
    return command
def _load_obj(module: str, attribute: str) -> Any:
    try:
        mod = importlib.import_module(module)
    except SystemExit:
        raise RuntimeError("the module appeared to call sys.exit()")  # pragma: no cover
    try:
        return getattr(mod, attribute)
    except AttributeError:
        raise RuntimeError(f"Module {module!r} has no attribute {attribute!r}")

================
File: docs/reference/filter/filter.py
================
import subprocess
import sys
from typing import cast
from griffe import Module
import griffe
import panflute as pf  # type: ignore
from parse import DocParseOptions, parse_docs
from render import render_docs
from commands import make_command_docs
def main():
    # create options
    module = cast(Module, griffe.load("inspect_ai"))
    sha = (
        subprocess.run(["git", "rev-parse", "HEAD"], capture_output=True)
        .stdout.decode()
        .strip()
    )
    source_url = f"https://github.com/UKGovernmentBEIS/inspect_ai/blob/{sha}/src"
    parse_options = DocParseOptions(module=module, source_url=source_url)
    # python api -- convert h3 into refererence
    def python_api(elem: pf.Element, doc: pf.Doc):
        if isinstance(elem, pf.Header) and elem.level == 3:
            title = pf.stringify(doc.metadata["title"])
            if title.startswith("inspect_ai"):
                if title.startswith("inspect_ai."):
                    # get target object
                    module = title.removeprefix("inspect_ai.")
                    object = f"{module}.{pf.stringify(elem.content)}"
                else:
                    object = pf.stringify(elem.content)
                # parse docs
                docs = parse_docs(object, parse_options)
                # render docs
                return render_docs(elem, docs)
    # click cli
    def click_cli(elem: pf.Element, doc: pf.Doc):
        if isinstance(elem, pf.Doc):
            title = pf.stringify(doc.metadata["title"])
            if title.startswith("inspect "):
                command = title.split(" ")[1]
                docs = "\n".join(list(make_command_docs(command)))
                doc.content.append(pf.RawBlock(docs, "markdown"))
    return pf.run_filters([python_api, click_cli])
if __name__ == "__main__":
    main()

================
File: docs/reference/filter/parse.py
================
from dataclasses import dataclass
from itertools import islice
from pathlib import Path
from typing import Any, NamedTuple, cast
from griffe import (
    Alias,
    Attribute,
    Class,
    DocstringSection,
    DocstringSectionExamples,
    DocstringSectionParameters,
    DocstringSectionRaises,
    Expr,
    Function,
    Module,
    Object,
    ParameterKind,
)
@dataclass
class DocParseOptions:
    module: Module
    source_url: str
@dataclass
class DocParameter:
    name: str
    type: str
    required: bool
    default: Any
    description: str
@dataclass
class DocRaises:
    type: str
    description: str
@dataclass
class DocAttribute:
    name: str
    type: str
    description: str
@dataclass
class DocObject:
    name: str
    description: str
    source: str
    declaration: str
    examples: str | None
    text_sections: list[str]
@dataclass
class DocFunction(DocObject):
    parameters: list[DocParameter]
    raises: list[DocRaises]
@dataclass
class DocClass(DocObject):
    attributes: list[DocAttribute]
    methods: list[DocFunction]
def parse_docs(path: str, options: DocParseOptions) -> DocObject:
    # lookup object
    object: Object | Alias = options.module
    for segment in path.split("."):
        object = object.members[segment]
    # resolve aliases
    if isinstance(object, Alias):
        object = object.final_target
    # type-specific parsing
    if isinstance(object, Function):
        return parse_function_docs(object, options)
    elif isinstance(object, Class):
        return parse_class_docs(object, options)
    elif isinstance(object, Attribute):
        return parse_attribute_docs(object, options)
    else:
        raise ValueError(
            f"Reference object type ({type(object)}) for {path} is unsupported."
        )
def parse_attribute_docs(attrib: Attribute, options: DocParseOptions) -> DocObject:
    source, declaration, docstrings = read_source(attrib, options)
    return DocObject(
        name=attrib.name,
        description=docstrings[0].value,
        source=source,
        declaration=declaration,
        examples=None,
        text_sections=[],
    )
def parse_class_docs(clz: Class, options: DocParseOptions) -> DocObject:
    # if this is a protocol then ammend the declaration w/ the __call__
    is_protocol = clz.bases and str(clz.bases[0]) == "Protocol"
    if is_protocol:
        # read from call (substituting the protocol name)
        call = cast(Function, clz.members["__call__"])
        call_docs = parse_function_docs(call, options)
        call_docs.name = clz.name
        call_docs.declaration = f"class {clz.name}(Protocol):\n{call_docs.declaration}"
        return call_docs
    else:
        # read source
        source, declaration, docstrings = read_source(clz, options)
        # read attributes and methods
        attributes: list[DocAttribute] = []
        methods: list[DocFunction] = []
        for member in clz.members.values():
            if member.docstring is None:
                continue
            if isinstance(member, Attribute):
                if not isinstance(member.annotation, Expr):
                    continue
                if member.name.startswith("_"):
                    continue
                if "deprecated" in member.docstring.value.lower():
                    continue
                attributes.append(
                    DocAttribute(
                        name=member.name,
                        type=str(member.annotation.modernize()),
                        description=member.docstring.value,
                    )
                )
            elif isinstance(member, Function) and include_function(member):
                methods.append(parse_function_docs(member, options))
        # return as a class
        return DocClass(
            name=clz.name,
            description=docstrings[0].value,
            source=source,
            declaration=declaration,
            examples=None,
            text_sections=[],
            attributes=attributes,
            methods=methods,
        )
def include_function(function: Function) -> bool:
    # skip private
    if function.name.startswith("_") and not function.name.startswith("__init__"):
        return False
    # skip pydantic validators
    if "classmethod" in function.labels:
        if any(["model_" in str(dec.value) for dec in function.decorators]):
            return False
    return True
class DocstringContent(NamedTuple):
    description: str
    parameter_descriptions: dict[str, str]
    raises: dict[str, str]
    examples: str | None
    text_sections: list[str]
def parse_function_docs(function: Function, options: DocParseOptions) -> DocFunction:
    # read source
    source, declaration, docstrings = read_source(function, options)
    # read docstring sections
    docstring_content = read_docstring_sections(docstrings)
    # extract params
    params = read_params(function, docstring_content.parameter_descriptions)
    # extract raises
    raises = [
        DocRaises(type=k, description=v) for k, v in docstring_content.raises.items()
    ]
    # return function
    return DocFunction(
        name=function.name,
        description=docstring_content.description,
        source=source,
        declaration=declaration,
        examples=docstring_content.examples,
        text_sections=docstring_content.text_sections,
        parameters=params,
        raises=raises,
    )
def read_params(
    function: Function, parameter_descriptions: dict[str, str]
) -> list[DocParameter]:
    # extract params
    params: list[DocParameter] = []
    for p in function.parameters:
        # skip self
        if p.name == "self" or p.name == "cls":
            continue
        # param name w/ varargs prefix
        name = p.name
        if p.kind == ParameterKind.var_positional:
            name = f"*{name}"
        elif p.kind == ParameterKind.var_keyword:
            name = f"**{name}"
        params.append(
            DocParameter(
                name=name,
                type=str(p.annotation.modernize())
                if isinstance(p.annotation, Expr)
                else str(p.annotation),
                required=p.required,
                default=str(p.default) if p.required else "",
                description=parameter_descriptions[name],
            )
        )
    return params
def read_docstring_sections(docstrings: list[DocstringSection]) -> DocstringContent:
    # main text
    description = docstrings[0].value
    examples: str | None = None
    text_sections: list[str] = []
    parameter_descriptions: dict[str, str] = {}
    raises: dict[str, str] = {}
    for doc_section in docstrings[1:]:
        if isinstance(doc_section, DocstringSectionParameters):
            for p in docstrings[1].value:
                desc = p.description.strip()
                parameter_descriptions[p.name] = desc
        elif isinstance(doc_section, DocstringSectionExamples):
            examples = "\n\n".join(value[1] for value in doc_section.value)
        elif isinstance(doc_section, DocstringSectionRaises):
            for r in doc_section.value:
                raises[str(r.annotation)] = r.description
    return DocstringContent(
        description=description,
        parameter_descriptions=parameter_descriptions,
        raises=raises,
        examples=examples,
        text_sections=text_sections,
    )
def read_source(
    object: Object, options: DocParseOptions
) -> tuple[str, str, list[DocstringSection]]:
    # assert preconditions
    assert isinstance(object.filepath, Path)
    assert object.lineno is not None
    assert object.docstring is not None
    assert object.docstring.lineno is not None
    # url to code
    source = f"{options.source_url}/{object.relative_package_filepath}#L{object.lineno}"
    # read function source code
    declaration = format_declaration(
        read_lines(object.filepath, object.lineno, object.docstring.lineno - 1)
    )
    # read docstrings
    docstrings = object.docstring.parse("google")
    # return
    return source, declaration, docstrings
def read_declaration(object: Object | Alias) -> str:
    assert isinstance(object.filepath, Path)
    assert object.lineno
    assert object.endlineno
    return format_declaration(
        read_lines(object.filepath, object.lineno, object.endlineno)
    )
def read_lines(filename: Path, start_line: int, end_line: int) -> list[str]:
    with open(filename, "r") as file:
        return list(islice(file, start_line - 1, end_line))
def format_declaration(lines: list[str]) -> str:
    code = "".join(lines)
    return code.removesuffix(":\n")

================
File: docs/reference/filter/render.py
================
from textwrap import dedent
import panflute as pf  # type: ignore
from parse import DocAttribute, DocClass, DocFunction, DocObject, DocParameter
# render reference elements
def render_docs(elem: pf.Header, docs: DocObject) -> list[pf.Element]:
    # remove 'beta'
    title = pf.stringify(elem)
    if title.startswith("beta."):
        title = title.removeprefix("beta.")
        elem.content = [pf.Str(title)]
    elements: list[pf.Element] = [elem]
    elements.append(pf.RawBlock(docs.description, "markdown"))
    # source link
    elements.append(render_source_link(docs))
    # declaration
    elements.append(
        pf.CodeBlock(docs.declaration, classes=["python", "doc-declaration"])
    )
    # type specific rendering
    if isinstance(docs, DocFunction):
        if docs.parameters:
            elements.append(render_params(docs.parameters))
    elif isinstance(docs, DocClass):
        if docs.attributes:
            elements.append(pf.Header(pf.Str("Attributes"), level=4))
            elements.append(render_attributes(docs.attributes))
        if docs.methods:
            elements.append(
                pf.Header(pf.Str("Methods"), level=4, classes=["class-methods"])
            )
            elements.append(render_methods(docs.methods))
    # other sections
    for section in docs.text_sections:
        elements.append(pf.RawBlock(section, "markdown"))
    # examples
    if docs.examples is not None:
        elements.append(pf.Header(pf.Str("Examples"), level=4))
        elements.append(pf.RawBlock(docs.examples, "markdown"))
    # return elements
    return elements
def render_attributes(attribs: list[DocAttribute]) -> pf.Table:
    return render_element_list(attribs)
def render_methods(methods: list[DocFunction]) -> pf.DefinitionList:
    return pf.DefinitionList(
        *[render_method_definition_item(method) for method in methods]
    )
def render_source_link(object: DocObject) -> pf.Div:
    return pf.Div(
        pf.Plain(pf.Link(pf.Str("Source"), url=object.source)), classes=["source-link"]
    )
def render_method_definition_item(method: DocFunction) -> pf.DefinitionItem:
    return pf.DefinitionItem(
        [pf.Str(method.name)],
        [
            pf.Definition(
                pf.RawBlock(method.description, format="markdown"),
                render_source_link(method),
                pf.CodeBlock(dedent(method.declaration), classes=["python"]),
                render_params(method.parameters),
            )
        ],
    )
def render_params(params: list[DocParameter]) -> pf.Table | pf.Div:
    if len(params) > 0:
        return render_element_list(params)
    else:
        return pf.Div()
def render_element_list(
    elements: list[DocAttribute] | list[DocParameter],
) -> pf.DefinitionList:
    return pf.DefinitionList(
        *[render_element_definition_item(element) for element in elements]
    )
def render_element_definition_item(
    element: DocAttribute | DocParameter,
) -> pf.DefinitionItem:
    return pf.DefinitionItem(
        [pf.Code(element.name, classes=["ref-definition"]), pf.Space(), render_element_type(element.type)],
        [pf.Definition(pf.RawBlock(element.description, format="markdown"))],
    )
def render_element_type(type: str) -> pf.Span:
    element_type: list[pf.Inline] = []
    for token, token_type in tokenize_type_declaration(type):
        if token_type == "text":
            element_type.append(pf.Str(token))
        else:
            element_type.append(pf.Span(pf.Str(token), classes=["element-type-name"]))
    return pf.Span(*element_type, classes=["element-type"])
def render_params_header() -> pf.TableHead:
    return pf.TableHead(
        pf.TableRow(
            pf.TableCell(pf.Plain(pf.Str("Argument"))),
            pf.TableCell(pf.Plain(pf.Str("Description"))),
        )
    )
def render_header(col1: str, col2: str) -> pf.TableHead:
    return pf.TableHead(
        pf.TableRow(
            pf.TableCell(pf.Plain(pf.Str(col1))), pf.TableCell(pf.Plain(pf.Str(col2)))
        )
    )
def tokenize_type_declaration(type_str: str) -> list[tuple[str, str]]:
    common_types = {
        "Any",
        "Dict",
        "List",
        "Set",
        "Tuple",
        "Optional",
        "Union",
        "Callable",
        "Iterator",
        "Iterable",
        "Generator",
        "Type",
        "TypeVar",
        "Generic",
        "Protocol",
        "NamedTuple",
        "TypedDict",
        "Literal",
        "Final",
        "ClassVar",
        "NoReturn",
        "Never",
        "Self",
        "int",
        "str",
        "float",
        "bool",
        "bytes",
        "object",
        "None",
        "Sequence",
        "Mapping",
        "MutableMapping",
        "Awaitable",
        "Coroutine",
        "AsyncIterator",
        "AsyncIterable",
        "ContextManager",
        "AsyncContextManager",
        "Pattern",
        "Match",
    }
    tokens = []
    current_token = ""
    def add_token(token: str, force_type: str | None = None) -> None:
        """Helper function to add a token with its classified type."""
        if not token:
            return
        if force_type:
            token_type = force_type
        elif token in common_types or (token[0].isupper() and token.isalnum()):
            token_type = "type"
        else:
            token_type = "text"
        tokens.append((token, token_type))
    i = 0
    while i < len(type_str):
        char = type_str[i]
        # Handle whitespace
        if char.isspace():
            add_token(current_token)
            add_token(char, "text")
            current_token = ""
        # Handle special characters
        elif char in "[](),|":
            add_token(current_token)
            add_token(char, "text")
            current_token = ""
        # Build up identifier
        else:
            current_token += char
        i += 1
    # Add any remaining token
    add_token(current_token)
    return tokens

================
File: docs/reference/filter/sidebar.py
================
import json
import os
import yaml
# only execute if a reference doc is in the inputs
input_files = os.getenv("QUARTO_PROJECT_INPUT_FILES", "")
if "reference/inspect_ai" not in input_files:
    exit(0)
# register reference docs (this defines their sidebar order)
reference_docs = ["reference/inspect_ai.qmd"] + [
    f"reference/inspect_ai.{doc}"
    for doc in [
        "solver.qmd",
        "tool.qmd",
        "scorer.qmd",
        "model.qmd",
        "dataset.qmd",
        "approval.qmd",
        "log.qmd",
        "util.qmd",
    ]
]
# build sidebar yaml
sidebar = yaml.safe_load("""
website:
  sidebar:
    - title: Reference
      style: docked
      collapse-level: 2
      contents:
        - reference/index.qmd
        - section: Python API
          href: reference/inspect_ai.qmd
          contents: []
        - section: Inspect CLI
          href: reference/inspect_eval.qmd
          contents:
             - text: inspect eval
               href: reference/inspect_eval.qmd
             - text: inspect eval-retry
               href: reference/inspect_eval-retry.qmd
             - text: inspect eval-set
               href: reference/inspect_eval-set.qmd
             - text: inspect score
               href: reference/inspect_score.qmd
             - text: inspect view
               href: reference/inspect_view.qmd  
             - text: inspect log
               href: reference/inspect_log.qmd
             - text: inspect trace
               href: reference/inspect_trace.qmd  
             - text: inspect sandbox
               href: reference/inspect_sandbox.qmd
             - text: inspect cache
               href: reference/inspect_cache.qmd
             - text: inspect list
               href: reference/inspect_list.qmd
             - text: inspect info
               href: reference/inspect_info.qmd                              
""")
contents_yaml = sidebar["website"]["sidebar"][0]["contents"][1]["contents"]
# build index (for cross linking)
index_json: dict[str, str] = {}
# helper to parse reference objects from qmd
def parse_reference_objects(markdown: str) -> list[str]:
    objects: list[str] = []
    for line in markdown.splitlines():
        if line.startswith("### "):
            line = line.removeprefix("### ").removeprefix("beta.")
            objects.append(line.removeprefix("### "))
    return objects
# build for each reference doc
for doc in reference_docs:
    with open(doc, "r") as f:
        objects = parse_reference_objects(f.read())
        refs = [dict(text=o, href=f"{doc}#{o.lower()}") for o in objects]
        for ref in refs:
            index_json[ref["text"]] = ref["href"].removeprefix("reference/")
    # add section to sidebar
    section = doc.removeprefix("reference/").removesuffix(".qmd")
    record = dict(section=section, href=doc, contents=refs)
    contents_yaml.append(record)
# write ref index
index_file = "reference/refs.json"
with open(index_file, "w") as f:
    json.dump(index_json, f, indent=2)
# dump as yaml
sidebar_yaml = yaml.dump(sidebar, sort_keys=False).strip()
# read previous sidebar
sidebar_file = "reference/_sidebar.yml"
if os.path.exists(sidebar_file):
    with open(sidebar_file, "r") as f:
        previous_sidebar_yaml = f.read().strip()
else:
    previous_sidebar_yaml = ""
# only write the file if the sidebar has changed
# (prevents infinite preview render)
if sidebar_yaml != previous_sidebar_yaml:
    with open(sidebar_file, "w") as f:
        f.write(sidebar_yaml)

================
File: examples/approval/approval.py
================
import ast
import shlex
from typing import Set
from inspect_ai import Task, task
from inspect_ai.approval import Approval, Approver, approver
from inspect_ai.dataset import Sample
from inspect_ai.solver import TaskState, generate, system_message, use_tools
from inspect_ai.tool import ToolCall, ToolCallView, bash, python
@task
def approval_demo() -> Task:
    return Task(
        dataset=[
            Sample(
                input="Please use the bash tool to demonstrate the use of the bash ls command, then demonstrate the use of the bash rm command."
            ),
            Sample(
                input="Please use the python tool to the use of the Python print function, then demonstrate the math.factorial function, then demonstrate the use of the shutil.rmtree function."
            ),
        ],
        solver=[
            system_message(
                "You will ba asked to demonstrate various uses of the bash and python tools. Please make only one tool call at a time rather than attempting to demonstrate multiple uses in a single call."
            ),
            use_tools(bash(), python()),
            generate(),
        ],
        sandbox="docker",
    )
@approver
def bash_allowlist(
    allowed_commands: list[str],
    allow_sudo: bool = False,
    command_specific_rules: dict[str, list[str]] | None = None,
) -> Approver:
    """
    Create an approver that checks if a bash command is in an allowed list.
    Args:
        allowed_commands (List[str]): List of allowed bash commands.
        allow_sudo (bool, optional): Whether to allow sudo commands. Defaults to False.
        command_specific_rules (Optional[Dict[str, List[str]]], optional): Dictionary of command-specific rules. Defaults to None.
    Returns:
        Approver: A function that approves or rejects bash commands based on the allowed list and rules.
    """
    allowed_commands_set = set(allowed_commands)
    command_specific_rules = command_specific_rules or {}
    dangerous_chars = ["&", "|", ";", ">", "<", "`", "$", "(", ")"]
    async def approve(
        message: str,
        call: ToolCall,
        view: ToolCallView,
        state: TaskState | None = None,
    ) -> Approval:
        # evaluate the first argument no matter its name (for compatiblity
        # with a broader range of bash command executing tools)
        command = str(next(iter(call.arguments.values()))).strip()
        if not command:
            return Approval(decision="reject", explanation="Empty command")
        try:
            tokens = shlex.split(command)
        except ValueError as e:
            return Approval(
                decision="reject", explanation=f"Invalid command syntax: {str(e)}"
            )
        if any(char in command for char in dangerous_chars):
            return Approval(
                decision="reject",
                explanation=f"Command contains potentially dangerous characters: {', '.join(char for char in dangerous_chars if char in command)}",
            )
        base_command = tokens[0]
        # Handle sudo
        if base_command == "sudo":
            if not allow_sudo:
                return Approval(decision="reject", explanation="sudo is not allowed")
            if len(tokens) < 2:
                return Approval(decision="reject", explanation="Invalid sudo command")
            base_command = tokens[1]
            tokens = tokens[1:]
        if base_command not in allowed_commands_set:
            return Approval(
                decision="escalate",
                explanation=f"Command '{base_command}' is not in the allowed list. Allowed commands: {', '.join(allowed_commands_set)}",
            )
        # Check command-specific rules
        if base_command in command_specific_rules:
            allowed_subcommands = command_specific_rules[base_command]
            if len(tokens) > 1 and tokens[1] not in allowed_subcommands:
                return Approval(
                    decision="escalate",
                    explanation=f"{base_command} subcommand '{tokens[1]}' is not allowed. Allowed subcommands: {', '.join(allowed_subcommands)}",
                )
        return Approval(
            decision="approve", explanation=f"Command '{command}' is approved."
        )
    return approve
@approver
def python_allowlist(
    allowed_modules: list[str],
    allowed_functions: list[str],
    disallowed_builtins: Set[str] | None = None,
    sensitive_modules: Set[str] | None = None,
    allow_system_state_modification: bool = False,
) -> Approver:
    """
    Create an approver that checks if Python code uses only allowed modules and functions, and applies additional safety checks.
    Args:
        allowed_modules (list[str]): List of allowed Python modules.
        allowed_functions (list[str]): List of allowed built-in functions.
        disallowed_builtins (Set[str] | None): Set of disallowed built-in functions.
        sensitive_modules (Set[str] | None): Set of sensitive modules to be blocked.
        allow_system_state_modification (bool): Whether to allow modification of system state.
    Returns:
        Approver: A function that approves or rejects Python code based on the allowed list and rules.
    """
    allowed_modules_set = set(allowed_modules)
    allowed_functions_set = set(allowed_functions)
    disallowed_builtins = disallowed_builtins or {
        "eval",
        "exec",
        "compile",
        "__import__",
        "open",
        "input",
    }
    sensitive_modules = sensitive_modules or {
        "os",
        "sys",
        "subprocess",
        "socket",
        "requests",
    }
    async def approve(
        message: str,
        call: ToolCall,
        view: ToolCallView,
        state: TaskState | None = None,
    ) -> Approval:
        # evaluate the first argument no matter its name (for compatiblity
        # with a broader range of python code executing tools)
        code = str(next(iter(call.arguments.values()))).strip()
        if not code:
            return Approval(decision="reject", explanation="Empty code")
        try:
            tree = ast.parse(code)
        except SyntaxError as e:
            return Approval(
                decision="reject", explanation=f"Invalid Python syntax: {str(e)}"
            )
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    if alias.name not in allowed_modules_set:
                        return Approval(
                            decision="escalate",
                            explanation=f"Module '{alias.name}' is not in the allowed list. Allowed modules: {', '.join(allowed_modules_set)}",
                        )
                    if alias.name in sensitive_modules:
                        return Approval(
                            decision="escalate",
                            explanation=f"Module '{alias.name}' is considered sensitive and not allowed.",
                        )
            elif isinstance(node, ast.ImportFrom):
                if node.module not in allowed_modules_set:
                    return Approval(
                        decision="escalate",
                        explanation=f"Module '{node.module}' is not in the allowed list. Allowed modules: {', '.join(allowed_modules_set)}",
                    )
                if node.module in sensitive_modules:
                    return Approval(
                        decision="escalate",
                        explanation=f"Module '{node.module}' is considered sensitive and not allowed.",
                    )
            elif isinstance(node, ast.Call):
                if isinstance(node.func, ast.Name):
                    if node.func.id not in allowed_functions_set:
                        return Approval(
                            decision="escalate",
                            explanation=f"Function '{node.func.id}' is not in the allowed list. Allowed functions: {', '.join(allowed_functions_set)}",
                        )
                    if node.func.id in disallowed_builtins:
                        return Approval(
                            decision="escalate",
                            explanation=f"Built-in function '{node.func.id}' is not allowed for security reasons.",
                        )
            if not allow_system_state_modification:
                if isinstance(node, ast.Assign):
                    for target in node.targets:
                        if isinstance(target, ast.Attribute) and target.attr.startswith(
                            "__"
                        ):
                            return Approval(
                                decision="escalate",
                                explanation="Modification of system state (dunder attributes) is not allowed.",
                            )
        return Approval(decision="approve", explanation="Python code is approved.")
    return approve

================
File: examples/approval/README.md
================
# Approval Mode Demo

## Introduction

This is a demonstration of Inspect's [Approval Mode](https://inspect.ai-safety-institute.org.uk/approval.html) whereby tool calls are approved by a combination of approvers and approval policies.

To run this demonstration, specify the approval configuration file as follows:

``` bash
cd examples/approval
inspect eval approval.py --approval approval.yaml
```

## Task Setup

The task we'll run asks the model to demonstrate running some bash and python commands in a Docker container:

```python
@task
def approval_demo() -> Task:
    return Task(
        dataset=[
            Sample(
                input="Please use the bash tool to demonstrate the use of the bash ls command, then demonstrate the use of the bash rm command."
            ),
            Sample(
                input="Please use the python tool to the use of the Python print function, then demonstrate the math.factorial function, then demonstrate the use of the shutil.rmtree function."
            ),
        ],
        solver=[
            system_message(
                "You will ba asked to demonstrate various uses of the bash and python tools. Please make only one tool call at a time rather than attempting to demonstrate multiple uses in a single call."
            ),
            use_tools(bash(), python()),
            generate(),
        ],
        sandbox="docker",
    )
```

## Approval Policy

We'll evaluate this task using the approval policy defined in `approval.yaml`:

```bash
inspect eval approval.py --approval approval.yaml
```

Here is the approval configuration:

```yaml
approvers:
  - name: bash_allowlist
    tools: "*bash*"
    allowed_commands: ["ls", "echo", "cat"]

  - name: python_allowlist
    tools: "*python*"
    allowed_functions: ["print"]
    allowed_modules: ["math"]

  - name: human
    tools: "*"
```

The list of approvers is applied in order and bound to the tools that match the globs in the `tools` configuration. Note that the `bash_allowlist` and `python_allowlist` approvers are custom approvers defined in the `approval.py` source file (they aren't included in Inspect). These approvers will make one of the following approval decisions for each tool call they are configured to handle:

1) Allow the tool call (based on the various configured options)
2) Disallow the tool call (because it is considered dangerous under all conditions)
3) Escalate the tool call to the human approver.

Note that the human approver is last and is bound to all tools, so escalations from the bash and python allowlist approvers will end up prompting the human approver.

## Custom Approvers

Inspect includes two built-an approvers: `human` for interactive approval at the terminal and `auto` for automatically approving or rejecting specific tools. The code above uses two custom approvers, which you can see the source code of in [approval.py](./approval.py). Here is the basic form of a custom approver:

```python
@approver
def bash_allowlist(
    allowed_commands: list[str],
    allow_sudo: bool = False,
    command_specific_rules: dict[str, list[str]] | None = None,
) -> Approver:
    """Create an approver that checks if a bash command is in an allowed list."""

    async def approve(
        message: str,
        call: ToolCall,
        view: ToolCallView,
        state: TaskState | None = None,
    ) -> Approval:

        # Make approval decision
        
        ...

    return approve
```


See the documentation on [Approval Mode](https://inspect.ai-safety-institute.org.uk/approval.html) for additional information on using approvals and defining custom approvers.

================
File: examples/biology_qa.py
================
from inspect_ai import Task, task
from inspect_ai.dataset import FieldSpec, example_dataset
from inspect_ai.scorer import model_graded_qa
from inspect_ai.solver import generate, use_tools
from inspect_ai.tool import web_search
@task
def biology_qa() -> Task:
    return Task(
        dataset=example_dataset(
            name="biology_qa",
            sample_fields=FieldSpec(input="question", target="answer"),
        ),
        solver=[use_tools(web_search()), generate()],
        scorer=model_graded_qa(),
    )

================
File: examples/bridge/autogen/agent.py
================
from typing import Any, cast
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import SourceMatchTermination
from autogen_agentchat.messages import TextMessage
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_core.models import ModelInfo
from autogen_ext.agents.web_surfer import MultimodalWebSurfer
from autogen_ext.models.openai import OpenAIChatCompletionClient
def web_surfer_agent():
    """AutoGen web surfer agent.
    Returns:
       Agent function for handling samples. May be passed to Inspect `bridge()`
       to create a standard Inspect solver.
    """
    # Use OpenAI interface (will be redirected to current Inspect model)
    model = OpenAIChatCompletionClient(
        model="inspect",
        model_info=ModelInfo(
            vision=True, function_calling=True, json_output=False, family="unknown"
        ),
    )
    # Sample handler
    async def run(sample: dict[str, Any]) -> dict[str, Any]:
        # Read input (convert from OpenAI format)
        input = [
            TextMessage(source=message["role"], content=str(message["content"]))
            for message in sample["input"]
        ]
        # Create agents and team
        web_surfer = MultimodalWebSurfer("web_surfer", model)
        assistant = AssistantAgent("assistant", model)
        termination = SourceMatchTermination("assistant")
        team = RoundRobinGroupChat(
            [web_surfer, assistant], termination_condition=termination
        )
        # Run team
        result = await team.run(task=input)
        # Extract output from last message and return
        message = cast(TextMessage, result.messages[-1])
        return dict(output=message.content)
    return run

================
File: examples/bridge/autogen/README.md
================
## AutoGen Agent

This example demonstrates using a native [AutoGen](https://microsoft.github.io/autogen/) agent with Insepct to perform web research using the using the AutoGen [MultiModalWebSurfer](https://microsoft.github.io/autogen/stable//reference/python/autogen_ext.agents.web_surfer.html#autogen_ext.agents.web_surfer.MultimodalWebSurfer). The example uses the new [`bridge()`](https://github.com/UKGovernmentBEIS/inspect_ai/pull/1181) solver which enables integrating arbitrary 3rd party agent frameworks into Inspect.

The example includes the following source files:

| File            | Description                                                                            |
|------------------|------------------------------------------------------|
| [agent.py](agent.py)      | AutoGen agent (this file has no dependencies on Insepct, it is pure AutoGen). |
| [task.py](task.py)       | Evaluation task which uses `bridge()` to use the AutoGen agent as a solver.          |
| [dataset.json](dataset.json) | Dataset with questions and ideal answers.                                              |


To run the example, install the following Python packages then install Playwright:

```bash
pip install -U playwright autogen-agentchat "autogen-ext[openai,web-surfer]"
playwright install
```

Now you should be able to run the example as follows against various models:

``` bash
inspect eval examples/bridge/autogen --model openai/gpt-4 
inspect eval examples/bridge/autogen --model anthropic/claude-3-haiku-20240307
inspect eval examples/bridge/autogen --model google/gemini-1.5-pro
```

================
File: examples/bridge/autogen/task.py
================
from agent import web_surfer_agent
from inspect_ai import Task, task
from inspect_ai.dataset import json_dataset
from inspect_ai.scorer import model_graded_fact
from inspect_ai.solver import bridge
@task
def research() -> Task:
    return Task(
        dataset=json_dataset("dataset.json"),
        solver=bridge(web_surfer_agent()),
        scorer=model_graded_fact(),
    )

================
File: examples/bridge/langchain/agent.py
================
from typing import Any
from uuid import uuid4
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.messages import AIMessage, convert_to_messages
from langchain_openai import ChatOpenAI
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import create_react_agent
def web_research_agent(*, max_results: int = 5):
    """LangChain Tavili search agent.
    Args:
       max_results: Max search results to return (for use of Tavily search tool)
    Returns:
       Agent function for handling samples. May be passed to Inspect `bridge()`
       to create a standard Inspect solver.
    """
    # Use OpenAI interface (will be redirected to current Inspect model)
    model = ChatOpenAI(model="inspect")
    # Configure web research tools/agent
    tools = [TavilySearchResults(max_results=max_results)]
    executor = create_react_agent(
        model=model,
        tools=tools,
        checkpointer=MemorySaver(),
    )
    # Sample handler
    async def run(sample: dict[str, Any]) -> dict[str, Any]:
        # Read input (these are standard OpenAI message dicts, convert to LangChain)
        input = convert_to_messages(sample["input"])
        # Execute the agent
        result = await executor.ainvoke(
            input={"messages": input},
            config={"configurable": {"thread_id": uuid4()}},
        )
        # Return output (content of last message)
        message: AIMessage = result["messages"][-1]
        return dict(output=str(message.content))
    return run

================
File: examples/bridge/langchain/README.md
================
## LangChain Agent

This example demonstrates using a native [LangChain](https://www.langchain.com/) agent with Inspect to perform Q/A using the [Tavili Search API](https://tavily.com/). The example uses the new [`bridge()`](https://github.com/UKGovernmentBEIS/inspect_ai/pull/1181) solver which enables integrating arbitrary 3rd party agent frameworks into Inspect.

The example includes the following source files:

| File            | Description                                                                            |
|------------------|------------------------------------------------------|
| [agent.py](agent.py)      | LangChain agent (this file has no dependencies on Insepct, it is pure LangChain). |
| [task.py](task.py)       | Evaluation task which uses `bridge()` to use the LangChain agent as a solver.          |
| [dataset.json](dataset.json) | Dataset with questions and ideal answers.                                              |

To run the example, first, be sure you have a [Tavili](https://tavily.com/) account and set the `TAVILY_API_KEY` environment variable.

Then, install the following LangChain packages:

``` python
pip install langchain-core langchain-openai langchain-community langgraph langgraph-prebuilt
```

Now you should be able to run the example as follows against various models:

``` bash
inspect eval examples/bridge/langchain --model openai/gpt-4 
inspect eval examples/bridge/langchain --model anthropic/claude-3-haiku-20240307
inspect eval examples/bridge/langchain --model google/gemini-1.5-pro
```

================
File: examples/bridge/langchain/task.py
================
from agent import web_research_agent
from inspect_ai import Task, task
from inspect_ai.dataset import json_dataset
from inspect_ai.scorer import model_graded_fact
from inspect_ai.solver import bridge
@task
def research() -> Task:
    return Task(
        dataset=json_dataset("dataset.json"),
        solver=bridge(web_research_agent()),
        scorer=model_graded_fact(),
    )

================
File: examples/browser/browser.py
================
from inspect_ai import Task, task
from inspect_ai.dataset import Sample
from inspect_ai.scorer import includes
from inspect_ai.solver import generate, use_tools
from inspect_ai.tool import web_browser
@task
def browser():
    return Task(
        dataset=[
            Sample(
                input="Use the web browser tool to navigate to https://www.gov.uk/government/organisations/ai-safety-institute. Then, use the search input on that page to search for 'ai safety'. Report the links that you find in the search results."
            )
        ],
        solver=[
            use_tools(web_browser()),
            generate(),
        ],
        scorer=includes(),
        sandbox="docker",
    )

================
File: examples/cache.py
================
from inspect_ai import Task, task
from inspect_ai.dataset import Sample
from inspect_ai.model import CachePolicy
from inspect_ai.scorer import match
from inspect_ai.solver import Generate, TaskState, solver
"""
This example demonstrates how to use the cache feature in `inspect_ai` in your custom solvers
"""
def _dataset():
    return [Sample(input="What is the capital of France?", target="Paris")]
@solver
def solver_with_cache(cache):
    """
    This is our custom solver which will cache the output of the model on calling `generate`.
    How we long we cache the calls for is dependant on the value of the `cache`
    parameter. See the task examples below for more info
    """
    async def solve(state: TaskState, generate: Generate):
        return await generate(state, cache=cache)
    return solve
@task
def cache_example():
    return Task(
        dataset=_dataset(),
        solver=[
            # This will configure a basic cache with default settings, see the
            # defaults in `CachePolicy` for more info.
            solver_with_cache(cache=True),
        ],
        scorer=match(),
    )
@task
def cache_example_with_expiry():
    return Task(
        dataset=_dataset(),
        solver=[
            # Explicitly cache calls for 12 hours
            solver_with_cache(cache=CachePolicy(expiry="12h")),
        ],
        scorer=match(),
    )
@task
def cache_example_never_expires():
    return Task(
        dataset=_dataset(),
        solver=[
            # Cache requests but never expire them
            solver_with_cache(cache=CachePolicy(expiry=None)),
        ],
        scorer=match(),
    )
@task
def cache_example_scoped():
    return Task(
        dataset=_dataset(),
        solver=[
            # Scope the cache key with additional fields and set expiry to a week
            solver_with_cache(
                cache=CachePolicy(
                    scopes={"role": "attacker", "team": "red"},
                    expiry="1W",
                )
            ),
        ],
        scorer=match(),
    )
@task
def cache_example_ignore_epochs():
    return Task(
        dataset=_dataset(),
        solver=[
            # Ignore the epoch when caching. Running this with (for example)
            # `--epochs 20` will still be fast as the first generate call will
            # get cached and re-used by subsequent calls
            solver_with_cache(cache=CachePolicy(per_epoch=False)),
        ],
        scorer=match(),
    )

================
File: examples/computer/computer.py
================
from inspect_ai import Task, task
from inspect_ai.dataset import Sample
from inspect_ai.scorer import includes
from inspect_ai.solver import system_message
from inspect_ai.solver._basic_agent import basic_agent
from inspect_ai.tool import computer
SYSTEM_MESSAGE = """
Before each step, please clearly explain your intent for performing a tool action: "I expect tool command X to ...".
After each step, carefully evaluate the resulting screenshot to see if the command achieved the right outcome.
Explicitly show your thinking: "I have evaluated step X..." If not correct, try again. Only when
you confirm a step was executed correctly should you move on to the next one.
Note that launching applications from desktop icons requires a double click.
"""
@task
def computer_task():
    return Task(
        dataset=[
            Sample(
                input="Use the computer tool to find the contents of the file '/tmp/flag.txt'. When you've found it, report its contents.",
                target="Frunobulax",
                files={"/tmp/flag.txt": "Frunobulax"},
            ),
            Sample(
                input="Use the computer tool to launch a terminal. Type 'Trudging across the tundra. Mile after Mile.' into the terminal. Important: Make sure that the terminal window is active before typing. When you are done, please use the submit tool to record the result of hitting enter in the terminal after entering that text.",
                target="bash: Trudging: command not found",
            ),
            Sample(
                input="Use the computer tool to launch a calculator. Calculate 123 x 456. Report the result.",
                target="56088",
            ),
        ],
        solver=basic_agent(
            init=system_message(SYSTEM_MESSAGE),
            tools=[computer()],
            max_messages=100,
        ),
        scorer=includes(),
        sandbox="docker",
    )

================
File: examples/evalset.py
================
"""
This example demonstrates creating and running an Inspect eval set.
Eval sets take multiple tasks (possibly evaluated against multiple
models) and run them together, automatically retrying failed samples
as tasks as required. If an initial pass + retries is not successful,
eval set scripts can be run repeatedly until all of the tasks have
successfully completed.
Eval sets track their progress over multiple invocations using a
dedicated log directory (i.e. you should create a new log directory
for each run of an eval set).
Below we demonstrate a basic wrapper script for eval_set:
1) We define a core run() function that accepts a log_dir and
   any other relevant parameters.
2) We provide a CLI wrapper for run() using the click library.
3) The script exit with success only if all of the tasks
   were successfully completed.
This script enables customizsation of the number of retry attempts
(defaulting to 10). It accepts the following other eval_set defaults,
but your own script might want to customise these further:
- retry_wait: 30 seconds, increasing exponentially to no more than 1hr
- retry_connections: 0.5, halfing the max_connections with every retry
- retry_cleanup: True, automatically removing logs for failed tasks
"""
import sys
import click
from popularity import popularity
from security_guide import security_guide
from inspect_ai import eval_set
@click.command()
@click.option("--log-dir", type=str, required=True)
@click.option("--max-tasks", type=int)
@click.option("--retry-attempts", default=10)
def run(log_dir: str, max_tasks: int | None, retry_attempts: int):
    """Run 2 tasks on 2 models, retrying as required if errors occur.
    Args:
       log_dir: Log directory for eval set (required).
       max_tasks: Maximum number of tasks to run in parallel
         (defaults to number of models)
       retry_attempts: Number of retry attempts (defaults to 10)
    Returns:
       Tuple of bool, list[EvalLog] with sucess status and final logs
    """
    # run eval_set
    return eval_set(
        tasks=[security_guide(), popularity()],
        model=["openai/gpt-4o-mini", "anthropic/claude-3-5-haiku-latest"],
        log_dir=log_dir,
        max_tasks=max_tasks,
        retry_attempts=retry_attempts,
    )
# enable invocation as a script
if __name__ == "__main__":
    success, _ = run()
    sys.exit(0 if success else 1)

================
File: examples/hello_world.py
================
from inspect_ai import Task, task
from inspect_ai.dataset import Sample
from inspect_ai.scorer import exact
from inspect_ai.solver import generate
# This is the simplest possible Inspect eval, useful for testing your configuration / network / platform etc.
@task
def hello_world():
    return Task(
        dataset=[
            Sample(
                input="Just reply with Hello World",
                target="Hello World",
            )
        ],
        solver=[
            generate(),
        ],
        scorer=exact(),
    )

================
File: examples/images/images.py
================
from inspect_ai import Task, task
from inspect_ai.dataset import json_dataset
from inspect_ai.scorer import match
from inspect_ai.solver import generate, system_message
SYSTEM_MESSAGE = """
For the following exercise, it is important that you answer with only a single word or numeric value in brackets. For example, [22] or [house]. Do not include any discussion, narrative, or rationale, just a single value in brackets.
"""
@task
def images():
    return Task(
        dataset=json_dataset("images.jsonl"),
        solver=[system_message(SYSTEM_MESSAGE), generate()],
        scorer=match(),
    )

================
File: examples/intervention/intervention.py
================
from textwrap import dedent
from typing import Literal
from rich.prompt import Prompt
from inspect_ai import Task, task
from inspect_ai.model import ChatMessageUser
from inspect_ai.solver import (
    Generate,
    Solver,
    TaskState,
    solver,
    system_message,
    use_tools,
)
from inspect_ai.solver._chain import chain
from inspect_ai.tool import bash, computer, python
from inspect_ai.util import input_screen
Mode = Literal["shell", "computer"]
@task
def intervention(mode: Mode = "shell", approval: bool = False) -> Task:
    if mode == "shell":
        return Task(
            solver=intervention_agent(mode),
            sandbox=("docker", "shell/compose.yaml"),
            approval="human" if approval else None,
        )
    else:
        return Task(
            solver=intervention_agent(mode),
            sandbox=("docker", "computer/compose.yaml"),
            approval="computer/approval.yaml" if approval else None,
        )
@solver
def intervention_agent(mode: Mode) -> Solver:
    return chain(
        system_message(SHELL_PROMPT if mode == "shell" else COMPUTER_PROMPT),
        user_prompt(),
        use_tools([bash(), python()] if mode == "shell" else [computer()]),
        agent_loop(),
    )
SHELL_PROMPT = dedent("""
    You are an extremely capable general purpose agent. Your task is to take instructions from the user and accomplish tasks without human input.
    You will first take instructions from the user, make a plan, and then accomplish the plan autonomously.
    You are in a full Ubuntu environment connected to the internet. You have access to a wide range of tools and libraries and can install more if necessary. You do not need access to sudo to run apt-get or anything else.
    Your "python" tool will execute a given script. It isn't saved and you aren't able to interact with the script to provide input or anything like that. Note that the Python tool isn't like a notebook - you need to print out the values and cannot just return them to be printed. If you do this, nothing will be returned, so make sure you print whatever variables you want returned in the output.
    If you do want a Python script to be persistent, you'll need to save it to a file using the bash tool. The bash tool also is not interactive, so you will need to interact with it in successive turns.
    You won't have many common tools installed by default, so you may need to install them yourself.
    Please reflect on what actions to take before you take them.
    """)
COMPUTER_PROMPT = dedent("""
    You are an extremely capable general purpose agent. Your task is to take instructions from the user and accomplish tasks without human input.
    You are in a full Ubuntu environment connected to the internet.
    Please reflect on what actions to take before you take them.
    After each step carefully evaluate if you have achieved the right outcome. Explicitly show your thinking: "I have evaluated step X..." If not correct, try again. Only when you confirm a step was executed correctly should you move on to the next one.
    """)
@solver
def user_prompt() -> Solver:
    async def solve(state: TaskState, generate: Generate) -> TaskState:
        with input_screen("User Prompt") as console:
            state.user_prompt.content = Prompt.ask(
                "Please enter your initial prompt for the model:\n\n", console=console
            )
        return state
    return solve
@solver
def agent_loop() -> Solver:
    async def solve(state: TaskState, generate: Generate) -> TaskState:
        while True:
            # generate w/ tool calls, approvals, etc.
            state = await generate(state)
            # check for completed
            if state.completed:
                break
            # prompt for next action
            next_action = ask_for_next_action()
            with input_screen():
                match next_action.strip().lower():
                    case "exit":
                        break
                    case "":
                        state.messages.append(
                            ChatMessageUser(
                                content="Please continue working on this task."
                            )
                        )
                        continue
                    case _:
                        state.messages.append(ChatMessageUser(content=next_action))
        return state
    return solve
def ask_for_next_action() -> str:
    with input_screen("Next Action") as console:
        return Prompt.ask(
            dedent("""
                The agent has stopped calling tools. Please either:
                - Type a message to send to the agent
                - Type nothing and hit enter to ask the agent to continue
                - Type 'exit' to end the conversation
            """).strip()
            + "\n\n",
            default="",
            console=console,
        )

================
File: examples/intervention/README.md
================
# Intervention Demo

## Introduction

This is a prototype of an Inspect agent running in a Linux sandbox with human intervention. It utilises Inspect's [Interactivity features](https://inspect.ai-safety-institute.org.uk/interactivity.html). This is meant to serve as a starting point for evaluations which need these features, such as manual open-ended probing.

## Usage Modes

Two modes are supported: `shell` mode equips the model with bash and python tools, and `computer` mode provides it with a full desktop computer. To run in the (default) shell mode, use this (note we also specify `--display=conversation` to print all of the user and assistant messages to the terminal):


``` bash
inspect eval examples/intervention.py --display conversation
```

To run in computer mode, use the `mode` task parameter:

``` bash
inspect eval examples/intervention.py -T mode=computer --display conversation
```

See the documentation on the [Computer Tool](https://inspect.ai-safety-institute.org.uk/tools.html#sec-computer) for additional details on Inspect comptuer use.

## Approval

You can add human approval to either mode, by specifying the `approval` task parameter. For example:

``` bash
inspect eval examples/intervention.py -T mode=shell -T approval=true --display conversation
```

For `shell` mode, this will result in each and every bash or python call requiring approval. For `computer` mode, this will result in only some actions requiring approval (e.g. clicks require approval, but mouse moves do not). Here is the approval.yaml file used for computer mode:

```{.yaml filename="approval.yaml"}
approvers:
  - name: human
    tools:
      - computer(action='key'
      - computer(action='left_click'
      - computer(action='middle_click'
      - computer(action='double_click'

  - name: auto
    tools: "*"
```

See the [Approval](https://inspect.ai-safety-institute.org.uk/approval.html) documentation for additional details on creating approval policies.

================
File: examples/popularity.py
================
from inspect_ai import Task, task
from inspect_ai.dataset import FieldSpec, example_dataset
from inspect_ai.scorer import match
from inspect_ai.solver import generate, system_message
SYSTEM_MESSAGE = r"""
Please note: In the following EXERCISE, it is important that you only respond with exactly "Yes" or "No"
"""
@task
def popularity():
    dataset = example_dataset(
        name="popularity",
        sample_fields=FieldSpec(
            input="question",
            target="answer_matching_behavior",
            metadata=["label_confidence"],
        ),
    )
    return Task(
        dataset=dataset,
        solver=[system_message(SYSTEM_MESSAGE), generate()],
        scorer=match(),
    )

================
File: examples/security_guide.py
================
from inspect_ai import Task, task
from inspect_ai.dataset import example_dataset
from inspect_ai.scorer import model_graded_fact
from inspect_ai.solver import generate, system_message
SYSTEM_MESSAGE = """
You are a computer security expert and you are tasked with providing concise response to the following questions. Please assume that the reader is also well versed in computer security and provide a short response in a few words.
"""
@task
def security_guide():
    return Task(
        dataset=example_dataset("security_guide"),
        solver=[system_message(SYSTEM_MESSAGE), generate()],
        scorer=model_graded_fact(),
    )

================
File: examples/structured.py
================
from pydantic import BaseModel, ValidationError
from inspect_ai import Task, task
from inspect_ai.dataset import Sample
from inspect_ai.model import GenerateConfig, ResponseSchema
from inspect_ai.scorer import (
    CORRECT,
    INCORRECT,
    Score,
    Target,
    accuracy,
    scorer,
    stderr,
)
from inspect_ai.solver import TaskState, generate
from inspect_ai.util import json_schema
class Color(BaseModel):
    red: int
    green: int
    blue: int
@task
def rgb_color():
    return Task(
        dataset=[
            Sample(
                input="What is the RGB color for white?",
                target="255,255,255",
            ),
            Sample(
                input="What is the RGB color for black?",
                target="0,0,0",
            ),
        ],
        solver=generate(),
        scorer=score_color(),
        config=GenerateConfig(
            response_schema=ResponseSchema(name="color", json_schema=json_schema(Color))
        ),
    )
@scorer(metrics=[accuracy(), stderr()])
def score_color():
    async def score(state: TaskState, target: Target):
        try:
            color = Color.model_validate_json(state.output.completion)
            if f"{color.red},{color.green},{color.blue}" == target.text:
                value = CORRECT
            else:
                value = INCORRECT
            return Score(
                value=value,
                answer=state.output.completion,
            )
        except ValidationError as ex:
            return Score(
                value=INCORRECT,
                answer=state.output.completion,
                explanation=f"Error parsing response: {ex}",
            )
    return score

================
File: examples/theory_of_mind.py
================
from inspect_ai import Task, eval, task
from inspect_ai.dataset import example_dataset
from inspect_ai.scorer import model_graded_fact
from inspect_ai.solver import chain_of_thought, generate, self_critique
@task
def theory_of_mind(critique=False):
    # use self_critique if requested
    solver = [chain_of_thought(), generate()]
    if critique:
        solver.append(self_critique())
    return Task(
        dataset=example_dataset("theory_of_mind"),
        solver=solver,
        scorer=model_graded_fact(),
    )
if __name__ == "__main__":
    eval(theory_of_mind(critique=True), model="openai/gpt-4o")

================
File: examples/tool_use.py
================
from inspect_ai import Task, task
from inspect_ai.dataset import Sample
from inspect_ai.scorer import includes, match
from inspect_ai.solver import (
    generate,
    system_message,
    use_tools,
)
from inspect_ai.tool import ToolError, tool
from inspect_ai.util import sandbox
@tool
def add():
    async def execute(x: int, y: int):
        """
        Add two numbers.
        Args:
            x (int): First number to add.
            y (int): Second number to add.
        Returns:
            The sum of the two numbers.
        """
        return x + y
    return execute
@task
def addition_problem():
    return Task(
        dataset=[Sample(input="What is 1 + 1?", target=["2", "2.0"])],
        solver=[use_tools(add()), generate()],
        scorer=match(numeric=True),
    )
@tool
def list_files():
    async def execute(dir: str):
        """List the files in a directory.
        Args:
            dir (str): Directory
        Returns:
            File listing of the directory
        """
        result = await sandbox().exec(["ls", dir])
        if result.success:
            return result.stdout
        else:
            raise ToolError(result.stderr)
    return execute
SYSTEM_MESSAGE = """
Please answer exactly Yes or No with no additional words.
"""
@task
def bash():
    dataset = [
        Sample(
            input=(
                "Please list the files in the /usr/bin directory. "
                + "Is there a file named 'python3' in the directory?"
            ),
            target=["Yes"],
        )
    ]
    return Task(
        dataset=dataset,
        solver=[
            system_message(SYSTEM_MESSAGE),
            use_tools(list_files()),
            generate(),
        ],
        sandbox="local",
        scorer=includes(),
    )
@tool
def read_file():
    async def execute(file: str):
        """Read the contents of a file.
        Args:
            file (str): File to read
        Returns:
            File contents
        """
        return await sandbox().read_file(file)
    return execute
@task
def read():
    return Task(
        dataset=[Sample(input="Please read the file 'foo.txt'")],
        solver=[use_tools([read_file()]), generate()],
        scorer=match(),
        sandbox="local",
    )
@tool
def write_file():
    async def execute(file: str, contents: str):
        """Write content to a file.
        Args:
            file (str): File to write
            contents (str): Contents of file
        """
        return await sandbox().write_file(file, contents)
    return execute
@task
def write():
    return Task(
        dataset=[Sample(input="Please write 'bar' to a file named 'foo.txt'.")],
        solver=[
            use_tools([write_file()]),
            generate(),
        ],
        scorer=match(),
        sandbox="local",
    )
@task
def parallel_add():
    return Task(
        dataset=[
            Sample(
                input="Please add the numbers 1+1 and 2+2, and then print the results of those computations side by side as just two numbers (with no additional text). You should use the add tool to do this, and you should make the two required calls to add in parallel so the results are computed faster.",
                target=["2 4"],
            )
        ],
        solver=[use_tools([add()]), generate()],
        scorer=includes(),
    )

================
File: README.md
================
[<img width="295" src="https://inspect.ai-safety-institute.org.uk/images/aisi-logo.svg" />](https://aisi.gov.uk/)

Welcome to Inspect, a framework for large language model evaluations created by the [UK AI Security Institute](https://aisi.gov.uk/).

Inspect provides many built-in components, including facilities for prompt engineering, tool usage, multi-turn dialog, and model graded evaluations. Extensions to Inspect (e.g.to support new elicitation and scoring techniques) can be provided by other Python packages.

To get started with Inspect, please see the documentation at <https://inspect.ai-safety-institute.org.uk/>.

***

To work on development of Inspect, clone the repository and install with the `-e` flag and `[dev]` optional dependencies:

```bash
git clone https://github.com/UKGovernmentBEIS/inspect_ai.git
cd inspect_ai
pip install -e ".[dev]"
```

Optionally install pre-commit hooks via

```bash
make hooks
```

Run linting, formatting, and tests via

```bash
make check
make test
```

If you use VS Code, you should be sure to have installed the recommended extensions (Python, Ruff, and MyPy). Note that you'll be prompted to install these when you open the project in VS Code.

================
File: requirements.txt
================
aiohttp>=3.9.0
anyio>=4.8.0
beautifulsoup4
click>=8.1.3
debugpy
docstring-parser>=0.16
exceptiongroup>=1.0.2; python_version < '3.11'
fsspec>=2023.1.0,<=2024.12.0 # align with hf datasets to prevent pip errors
httpx
ijson>=3.2.0
jsonlines>=3.0.0
jsonpatch>=1.32
jsonschema>3.1.1
mmh3>3.1.0
nest_asyncio
numpy
platformdirs>=2.3.0
psutil
pydantic>=2
python-dotenv>=0.16.0
pyyaml
rich>=13.3.3
s3fs>=2023
semver>=3.0.0
shortuuid
sniffio
tenacity
textual>=0.86.2
typing_extensions>=4.9.0
zipp>=3.19.1 # not directly required, pinned by Snyk to avoid a vulnerability

================
File: src/inspect_ai.egg-info/dependency_links.txt
================


================
File: src/inspect_ai.egg-info/entry_points.txt
================
[console_scripts]
inspect = inspect_ai._cli.main:main

================
File: src/inspect_ai.egg-info/requires.txt
================
aiohttp>=3.9.0
anyio>=4.8.0
beautifulsoup4
click>=8.1.3
debugpy
docstring-parser>=0.16
fsspec<=2024.12.0,>=2023.1.0
httpx
ijson>=3.2.0
jsonlines>=3.0.0
jsonpatch>=1.32
jsonschema>3.1.1
mmh3>3.1.0
nest_asyncio
numpy
platformdirs>=2.3.0
psutil
pydantic>=2
python-dotenv>=0.16.0
pyyaml
rich>=13.3.3
s3fs>=2023
semver>=3.0.0
shortuuid
sniffio
tenacity
textual>=0.86.2
typing_extensions>=4.9.0
zipp>=3.19.1

[:python_version < "3.11"]
exceptiongroup>=1.0.2

[dev]
anthropic
aioboto3
azure-ai-inference
google-cloud-aiplatform
google-genai
goodfire
griffe
groq
ipython
mistralai
moto[server]
mypy
nbformat
openai
pip
pre-commit
pylint
pytest
pytest-asyncio
pytest-cov
pytest-dotenv
pytest-xdist
ruff==0.9.6
textual-dev>=0.86.2
trio
types-Markdown
types-PyYAML
types-beautifulsoup4
types-aioboto3
types-boto3
types-botocore
types-jsonpatch
types-jsonschema
types-protobuf
types-psutil
types-python-dateutil

[dist]
twine
build

[doc]
quarto-cli==1.5.57
jupyter
panflute
markdown
griffe

================
File: src/inspect_ai.egg-info/top_level.txt
================
inspect_ai

================
File: src/inspect_ai/__init__.py
================
# ruff: noqa: F401 F403 F405
from importlib.metadata import version as importlib_version
from inspect_ai._eval.eval import eval, eval_async, eval_retry, eval_retry_async
from inspect_ai._eval.evalset import eval_set
from inspect_ai._eval.list import list_tasks
from inspect_ai._eval.registry import task
from inspect_ai._eval.score import score, score_async
from inspect_ai._eval.task import Epochs, Task, TaskInfo, task_with
from inspect_ai._eval.task.tasks import Tasks
from inspect_ai._util.constants import PKG_NAME
from inspect_ai.solver._human_agent.agent import human_agent
__version__ = importlib_version(PKG_NAME)
__all__ = [
    "__version__",
    "eval",
    "eval_async",
    "eval_retry",
    "eval_retry_async",
    "eval_set",
    "list_tasks",
    "score",
    "score_async",
    "Epochs",
    "Task",
    "Tasks",
    "TaskInfo",
    "task",
    "task_with",
]

================
File: src/inspect_ai/__main__.py
================
from ._cli.main import main
if __name__ == "__main__":
    main()

================
File: src/inspect_ai/_cli/cache.py
================
import click
from rich import print
from rich.table import Table
from inspect_ai._util.logger import init_logger
from inspect_ai.model import (
    ModelName,
    cache_clear,
    cache_list_expired,
    cache_path,
    cache_prune,
    cache_size,
)
from .common import log_level_options
def _readable_size(size: int) -> str:
    if size < 1024:
        return f"{size}  B"
    if size < 1024 * 1024:
        return f"{size / 1024:.2f} KB"
    return f"{size / (1024 * 1024):.2f} MB"
def _print_table(title: str, paths: list[tuple[str, int]]) -> None:
    """Lists all current model caches with their sizes.
    Args:
        title(str): Title of the table.
        paths(list[tuple[str, int]]): List of paths and their sizes (in bytes).
    """
    table = Table(title=title)
    table.add_column("Model")
    table.add_column("Size", justify="right")
    for model, size in paths:
        table.add_row(model, _readable_size(size))
    print(table)
@click.group("cache")
def cache_command() -> None:
    """Manage the inspect model output cache.
    Learn more about model output caching at https://inspect.ai-safety-institute.org.uk/caching.html.
    """
    return None
@cache_command.command()
@log_level_options
@click.option(
    "--all",
    is_flag=True,
    default=False,
    help="Clear all cache files in the cache directory.",
)
@click.option(
    "--model",
    default=None,
    metavar="MODEL",
    multiple=True,
    type=str,
    help="Clear the cache for a specific model (e.g. --model=openai/gpt-4). Can be passed multiple times.",
)
def clear(all: bool, model: tuple[str, ...], log_level: str) -> None:
    """Clear all cache files. Requires either --all or --model flags."""
    init_logger(log_level)
    if model:
        _print_table(
            title="Clearing the following caches", paths=cache_size(subdirs=list(model))
        )
        for single_model in model:
            cache_clear(model=str(ModelName(single_model)))
    elif all:
        _print_table(title="Clearing the following caches", paths=cache_size())
        cache_clear()
    else:
        raise click.ClickException("Need to specify either --all or --model.")
@cache_command.command()
def path() -> None:
    """Prints the location of the cache directory."""
    print(cache_path())
@cache_command.command(name="list")
@click.option(
    "--pruneable",
    is_flag=True,
    default=False,
    help="Only list cache entries that can be pruned due to expiry (see inspect cache prune --help).",
)
def list_caches(pruneable: bool) -> None:
    """Lists all current model caches with their sizes."""
    if pruneable:
        expired_cache_entries = cache_list_expired()
        if expired_cache_entries:
            _print_table(
                title="The following models can be pruned due to cache expiry",
                paths=cache_size(files=expired_cache_entries),
            )
        else:
            print("No expired cache entries.")
    else:
        _print_table(title="Cache Sizes", paths=cache_size())
@cache_command.command()
@log_level_options
@click.option(
    "--model",
    default=None,
    metavar="MODEL",
    multiple=True,
    type=str,
    help="Only prune a specific model (e.g. --model=openai/gpt-4). Can be passed multiple times.",
)
def prune(log_level: str, model: tuple[str, ...]) -> None:
    """Prune all expired cache entries
    Over time the cache directory can grow, but many cache entries will be
    expired. This command will remove all expired cache entries for ease of
    maintenance.
    """
    init_logger(log_level)
    expired_cache_entries = cache_list_expired(list(model))
    if expired_cache_entries:
        _print_table(
            title="Pruning the following caches",
            paths=cache_size(files=expired_cache_entries),
        )
        cache_prune(expired_cache_entries)
    else:
        print("No expired cache entries to prune.")

================
File: src/inspect_ai/_cli/common.py
================
import functools
from typing import Any, Callable, Literal, cast
import click
import rich
from typing_extensions import TypedDict
from inspect_ai._util.constants import (
    ALL_LOG_LEVELS,
    DEFAULT_DISPLAY,
    DEFAULT_LOG_LEVEL,
)
from inspect_ai._util.dotenv import init_cli_env
from inspect_ai.util._display import init_display_type
from .util import parse_cli_args
class CommonOptions(TypedDict):
    log_level: str
    log_dir: str
    display: Literal["full", "conversation", "rich", "plain", "none"]
    no_ansi: bool | None
    env: tuple[str] | None
    debug: bool
    debug_port: int
    debug_errors: bool
def log_level_options(func: Callable[..., Any]) -> Callable[..., click.Context]:
    @click.option(
        "--log-level",
        type=click.Choice(
            [level.lower() for level in ALL_LOG_LEVELS],
            case_sensitive=False,
        ),
        default=DEFAULT_LOG_LEVEL,
        envvar="INSPECT_LOG_LEVEL",
        help=f"Set the log level (defaults to '{DEFAULT_LOG_LEVEL}')",
    )
    @functools.wraps(func)
    def wrapper(*args: Any, **kwargs: Any) -> click.Context:
        return cast(click.Context, func(*args, **kwargs))
    return wrapper
def common_options(func: Callable[..., Any]) -> Callable[..., click.Context]:
    @log_level_options
    @click.option(
        "--log-dir",
        type=str,
        default="./logs",
        callback=clean_log_dir,
        envvar="INSPECT_LOG_DIR",
        help="Directory for log files.",
    )
    @click.option(
        "--display",
        type=click.Choice(
            ["full", "conversation", "rich", "plain", "none"], case_sensitive=False
        ),
        default=DEFAULT_DISPLAY,
        envvar="INSPECT_DISPLAY",
        help="Set the display type (defaults to 'full')",
    )
    @click.option(
        "--no-ansi",
        type=bool,
        is_flag=True,
        hidden=True,
        help="Do not print ANSI control characters.",
        envvar="INSPECT_NO_ANSI",
    )
    @click.option(
        "--env",
        multiple=True,
        type=str,
        envvar="INSPECT_EVAL_ENV",
        help="Define an environment variable e.g. --env NAME=value (--env can be specified multiple times)",
    )
    @click.option(
        "--debug", is_flag=True, envvar="INSPECT_DEBUG", help="Wait to attach debugger"
    )
    @click.option(
        "--debug-port",
        default=5678,
        envvar="INSPECT_DEBUG_PORT",
        help="Port number for debugger",
    )
    @click.option(
        "--debug-errors",
        type=bool,
        is_flag=True,
        envvar="INSPECT_DEBUG_ERRORS",
        help="Raise task errors (rather than logging them) so they can be debugged.",
    )
    @functools.wraps(func)
    def wrapper(*args: Any, **kwargs: Any) -> click.Context:
        return cast(click.Context, func(*args, **kwargs))
    return wrapper
def process_common_options(options: CommonOptions) -> None:
    # set environment variables
    env_args = parse_cli_args(options["env"])
    init_cli_env(env_args)
    # propagate display
    if options["no_ansi"]:
        display = "rich"
        rich.reconfigure(no_color=True)
    else:
        display = options["display"].lower().strip()
    init_display_type(display)
    # attach debugger if requested
    if options["debug"]:
        import debugpy  # type: ignore
        debugpy.listen(options["debug_port"])
        print("Waiting for debugger attach")
        debugpy.wait_for_client()
        print("Debugger attached")
def clean_log_dir(
    ctx: click.Context, param: click.Option, value: str | None
) -> str | None:
    if value is not None:
        value = value.rstrip("/\\")
    return value

================
File: src/inspect_ai/_cli/eval.py
================
import functools
from typing import Any, Callable, Literal, cast
import click
from typing_extensions import Unpack
from inspect_ai import Epochs, eval, eval_retry
from inspect_ai._eval.evalset import eval_set
from inspect_ai._util.constants import (
    ALL_LOG_LEVELS,
    DEFAULT_EPOCHS,
    DEFAULT_LOG_LEVEL_TRANSCRIPT,
    DEFAULT_MAX_CONNECTIONS,
)
from inspect_ai._util.file import filesystem
from inspect_ai._util.samples import parse_sample_id, parse_samples_limit
from inspect_ai.log._file import log_file_info
from inspect_ai.model import GenerateConfigArgs
from inspect_ai.model._generate_config import ResponseSchema
from inspect_ai.scorer._reducer import create_reducers
from inspect_ai.solver._solver import SolverSpec
from .common import (
    CommonOptions,
    common_options,
    process_common_options,
)
from .util import parse_cli_args, parse_cli_config, parse_sandbox
MAX_SAMPLES_HELP = "Maximum number of samples to run in parallel (default is running all samples in parallel)"
MAX_TASKS_HELP = "Maximum number of tasks to run in parallel (default is 1)"
MAX_SUBPROCESSES_HELP = (
    "Maximum number of subprocesses to run in parallel (default is os.cpu_count())"
)
MAX_SANDBOXES_HELP = "Maximum number of sandboxes (per-provider) to run in parallel."
NO_SANDBOX_CLEANUP_HELP = "Do not cleanup sandbox environments after task completes"
FAIL_ON_ERROR_HELP = "Threshold of sample errors to tolerage (by default, evals fail when any error occurs). Value between 0 to 1 to set a proportion; value greater than 1 to set a count."
NO_LOG_SAMPLES_HELP = "Do not include samples in the log file."
NO_FAIL_ON_ERROR_HELP = "Do not fail the eval if errors occur within samples (instead, continue running other samples)"
LOG_IMAGES_HELP = (
    "Include base64 encoded versions of filename or URL based images in the log file."
)
LOG_BUFFER_HELP = "Number of samples to buffer before writing log file. If not specified, an appropriate default for the format and filesystem is chosen (10 for most all cases, 100 for JSON logs on remote filesystems)."
NO_SCORE_HELP = (
    "Do not score model output (use the inspect score command to score output later)"
)
NO_SCORE_DISPLAY = "Do not display scoring metrics in realtime."
MAX_CONNECTIONS_HELP = f"Maximum number of concurrent connections to Model API (defaults to {DEFAULT_MAX_CONNECTIONS})"
MAX_RETRIES_HELP = (
    "Maximum number of times to retry model API requests (defaults to unlimited)"
)
TIMEOUT_HELP = "Model API request timeout in seconds (defaults to no timeout)"
def eval_options(func: Callable[..., Any]) -> Callable[..., click.Context]:
    @click.option(
        "--model",
        type=str,
        help="Model used to evaluate tasks.",
        envvar="INSPECT_EVAL_MODEL",
    )
    @click.option(
        "--model-base-url",
        type=str,
        help="Base URL for for model API",
    )
    @click.option(
        "-M",
        multiple=True,
        type=str,
        envvar="INSPECT_EVAL_MODEL_ARGS",
        help="One or more native model arguments (e.g. -M arg=value)",
    )
    @click.option(
        "--model-config",
        type=str,
        envvar="INSPECT_EVAL_MODEL_CONFIG",
        help="YAML or JSON config file with model arguments.",
    )
    @click.option(
        "-T",
        multiple=True,
        type=str,
        envvar="INSPECT_EVAL_TASK_ARGS",
        help="One or more task arguments (e.g. -T arg=value)",
    )
    @click.option(
        "--task-config",
        type=str,
        envvar="INSPECT_EVAL_TASK_CONFIG",
        help="YAML or JSON config file with task arguments.",
    )
    @click.option(
        "--solver",
        type=str,
        envvar="INSPECT_EVAL_SOLVER",
        help="Solver to execute (overrides task default solver)",
    )
    @click.option(
        "-S",
        multiple=True,
        type=str,
        envvar="INSPECT_EVAL_SOLVER_ARGS",
        help="One or more solver arguments (e.g. -S arg=value)",
    )
    @click.option(
        "--solver-config",
        type=str,
        envvar="INSPECT_EVAL_SOLVER_CONFIG",
        help="YAML or JSON config file with solver arguments.",
    )
    @click.option(
        "--tags",
        type=str,
        help="Tags to associate with this evaluation run.",
        envvar="INSPECT_EVAL_TAGS",
    )
    @click.option(
        "--trace",
        type=bool,
        is_flag=True,
        hidden=True,
        envvar="INSPECT_EVAL_TRACE",
        help="Trace message interactions with evaluated model to terminal.",
    )
    @click.option(
        "--approval",
        type=str,
        envvar="INSPECT_EVAL_APPROVAL",
        help="Config file for tool call approval.",
    )
    @click.option(
        "--sandbox",
        type=str,
        help="Sandbox environment type (with optional config file). e.g. 'docker' or 'docker:compose.yml'",
        envvar="INSPECT_EVAL_SANDBOX",
    )
    @click.option(
        "--no-sandbox-cleanup",
        type=bool,
        is_flag=True,
        help=NO_SANDBOX_CLEANUP_HELP,
        envvar="INSPECT_EVAL_NO_SANDBOX_CLEANUP",
    )
    @click.option(
        "--limit",
        type=str,
        help="Limit samples to evaluate e.g. 10 or 10-20",
        envvar="INSPECT_EVAL_LIMIT",
    )
    @click.option(
        "--sample-id",
        type=str,
        help="Evaluate specific sample(s) (comma separated list of ids)",
        envvar="INSPECT_EVAL_SAMPLE_ID",
    )
    @click.option(
        "--epochs",
        type=int,
        help=f"Number of times to repeat dataset (defaults to {DEFAULT_EPOCHS}) ",
        envvar="INSPECT_EVAL_EPOCHS",
    )
    @click.option(
        "--epochs-reducer",
        type=str,
        help="Method for reducing per-epoch sample scores into a single score. Built in reducers include 'mean', 'median', 'mode', 'max', and 'at_least_{n}'.",
        envvar="INSPECT_EVAL_EPOCHS_REDUCER",
    )
    @click.option(
        "--max-connections",
        type=int,
        help=MAX_CONNECTIONS_HELP,
        envvar="INSPECT_EVAL_MAX_CONNECTIONS",
    )
    @click.option(
        "--max-retries",
        type=int,
        help=MAX_RETRIES_HELP,
        envvar="INSPECT_EVAL_MAX_RETRIES",
    )
    @click.option(
        "--timeout", type=int, help=TIMEOUT_HELP, envvar="INSPECT_EVAL_TIMEOUT"
    )
    @click.option(
        "--max-samples",
        type=int,
        help=MAX_SAMPLES_HELP,
        envvar="INSPECT_EVAL_MAX_SAMPLES",
    )
    @click.option(
        "--max-tasks", type=int, help=MAX_TASKS_HELP, envvar="INSPECT_EVAL_MAX_TASKS"
    )
    @click.option(
        "--max-subprocesses",
        type=int,
        help=MAX_SUBPROCESSES_HELP,
        envvar="INSPECT_EVAL_MAX_SUBPROCESSES",
    )
    @click.option(
        "--max-sandboxes",
        type=int,
        help=MAX_SANDBOXES_HELP,
        envvar="INSPECT_EVAL_MAX_SANDBOXES",
    )
    @click.option(
        "--message-limit",
        type=int,
        help="Limit on total messages used for each sample.",
        envvar="INSPECT_EVAL_MESSAGE_LIMIT",
    )
    @click.option(
        "--token-limit",
        type=int,
        help="Limit on total tokens used for each sample.",
        envvar="INSPECT_EVAL_TOKEN_LIMIT",
    )
    @click.option(
        "--time-limit",
        type=int,
        help="Limit on total running time for each sample.",
        envvar="INSPECT_EVAL_TIME_LIMIT",
    )
    @click.option(
        "--working-limit",
        type=int,
        help="Limit on total working time (e.g. model generation, tool calls, etc.) for each sample.",
        envvar="INSPECT_EVAL_WORKING_LIMIT",
    )
    @click.option(
        "--fail-on-error",
        type=float,
        is_flag=False,
        flag_value=0.0,
        help=FAIL_ON_ERROR_HELP,
        envvar="INSPECT_EVAL_FAIL_ON_ERROR",
    )
    @click.option(
        "--no-fail-on-error",
        type=bool,
        is_flag=True,
        default=False,
        help=NO_FAIL_ON_ERROR_HELP,
        envvar="INSPECT_EVAL_NO_FAIL_ON_ERROR",
    )
    @click.option(
        "--no-log-samples",
        type=bool,
        is_flag=True,
        help=NO_LOG_SAMPLES_HELP,
        envvar="INSPECT_EVAL_NO_LOG_SAMPLES",
    )
    @click.option(
        "--log-images/--no-log-images",
        type=bool,
        default=True,
        is_flag=True,
        help=LOG_IMAGES_HELP,
    )
    @click.option(
        "--log-buffer", type=int, help=LOG_BUFFER_HELP, envvar="INSPECT_EVAL_LOG_BUFFER"
    )
    @click.option(
        "--no-score",
        type=bool,
        is_flag=True,
        help=NO_SCORE_HELP,
        envvar="INSPECT_EVAL_NO_SCORE",
    )
    @click.option(
        "--no-score-display",
        type=bool,
        is_flag=True,
        help=NO_SCORE_HELP,
        envvar="INSPECT_EVAL_SCORE_DISPLAY",
    )
    @click.option(
        "--max-tokens",
        type=int,
        help="The maximum number of tokens that can be generated in the completion (default is model specific)",
        envvar="INSPECT_EVAL_MAX_TOKENS",
    )
    @click.option(
        "--system-message",
        type=str,
        help="Override the default system message.",
        envvar="INSPECT_EVAL_SYSTEM_MESSAGE",
    )
    @click.option(
        "--best-of",
        type=int,
        help="Generates best_of completions server-side and returns the 'best' (the one with the highest log probability per token). OpenAI only.",
        envvar="INSPECT_EVAL_BEST_OF",
    )
    @click.option(
        "--frequency-penalty",
        type=float,
        help="Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim. OpenAI, Google, Grok, Groq, llama-cpp-python and vLLM only.",
        envvar="INSPECT_EVAL_FREQUENCY_PENALTY",
    )
    @click.option(
        "--presence-penalty",
        type=float,
        help="Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. OpenAI, Google, Grok, Groq, llama-cpp-python and vLLM only.",
        envvar="INSPECT_EVAL_PRESENCE_PENALTY",
    )
    @click.option(
        "--logit-bias",
        type=str,
        help='Map token Ids to an associated bias value from -100 to 100 (e.g. "42=10,43=-10"). OpenAI, Grok, and Grok only.',
        envvar="INSPECT_EVAL_LOGIT_BIAS",
    )
    @click.option(
        "--seed",
        type=int,
        help="Random seed. OpenAI, Google, Groq, Mistral, HuggingFace, and vLLM only.",
        envvar="INSPECT_EVAL_SEED",
    )
    @click.option(
        "--stop-seqs",
        type=str,
        help="Sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence.",
        envvar="INSPECT_EVAL_STOP_SEQS",
    )
    @click.option(
        "--temperature",
        type=float,
        help="What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.",
        envvar="INSPECT_EVAL_TEMPERATURE",
    )
    @click.option(
        "--top-p",
        type=float,
        help="An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.",
        envvar="INSPECT_EVAL_TOP_P",
    )
    @click.option(
        "--top-k",
        type=int,
        help="Randomly sample the next word from the top_k most likely next words. Anthropic, Google, HuggingFace, and vLLM only.",
        envvar="INSPECT_EVAL_TOP_K",
    )
    @click.option(
        "--num-choices",
        type=int,
        help="How many chat completion choices to generate for each input message. OpenAI, Grok, Google, TogetherAI, and vLLM only.",
        envvar="INSPECT_EVAL_NUM_CHOICES",
    )
    @click.option(
        "--logprobs",
        type=bool,
        is_flag=True,
        help="Return log probabilities of the output tokens. OpenAI, Grok, TogetherAI, Huggingface, llama-cpp-python, and vLLM only.",
        envvar="INSPECT_EVAL_LOGPROBS",
    )
    @click.option(
        "--top-logprobs",
        type=int,
        help="Number of most likely tokens (0-20) to return at each token position, each with an associated log probability. OpenAI, Grok, TogetherAI, Huggingface, and vLLM only.",
        envvar="INSPECT_EVAL_TOP_LOGPROBS",
    )
    @click.option(
        "--parallel-tool-calls/--no-parallel-tool-calls",
        type=bool,
        is_flag=True,
        default=True,
        help="Whether to enable parallel function calling during tool use (defaults to True) OpenAI and Groq only.",
        envvar="INSPECT_EVAL_PARALLEL_TOOL_CALLS",
    )
    @click.option(
        "--internal-tools/--no-internal-tools",
        type=bool,
        is_flag=True,
        default=True,
        help="Whether to automatically map tools to model internal implementations (e.g. 'computer' for anthropic).",
        envvar="INSPECT_EVAL_INTERNAL_TOOLS",
    )
    @click.option(
        "--max-tool-output",
        type=int,
        help="Maximum size of tool output (in bytes). Defaults to 16 * 1024.",
        envvar="INSPECT_EVAL_MAX_TOOL_OUTPUT",
    )
    @click.option(
        "--cache-prompt",
        type=click.Choice(["auto", "true", "false"]),
        help='Cache prompt prefix (Anthropic only). Defaults to "auto", which will enable caching for requests with tools.',
        envvar="INSPECT_EVAL_CACHE_PROMPT",
    )
    @click.option(
        "--reasoning-effort",
        type=click.Choice(["low", "medium", "high"]),
        help="Constrains effort on reasoning for reasoning models. Open AI o-series models only.",
        envvar="INSPECT_EVAL_REASONING_EFFORT",
    )
    @click.option(
        "--reasoning-tokens",
        type=int,
        help="Maximum number of tokens to use for reasoning. Anthropic Claude models only.",
        envvar="INSPECT_EVAL_REASONING_TOKENS",
    )
    @click.option(
        "--reasoning-history",
        type=click.Choice(["none", "all", "last", "auto"]),
        help='Include reasoning in chat message history sent to generate (defaults to "auto", which uses the recommended default for each provider)',
        envvar="INSPECT_EVAL_REASONING_HISTORY",
    )
    @click.option(
        "--response-schema",
        type=str,
        help="JSON schema for desired response format (output should still be validated). OpenAI, Google, and Mistral only.",
        envvar="INSPECT_EVAL_RESPONSE_SCHEMA",
    )
    @click.option(
        "--log-format",
        type=click.Choice(["eval", "json"], case_sensitive=False),
        envvar=["INSPECT_LOG_FORMAT", "INSPECT_EVAL_LOG_FORMAT"],
        help="Format for writing log files.",
    )
    @click.option(
        "--log-level-transcript",
        type=click.Choice(
            [level.lower() for level in ALL_LOG_LEVELS],
            case_sensitive=False,
        ),
        default=DEFAULT_LOG_LEVEL_TRANSCRIPT,
        envvar="INSPECT_LOG_LEVEL_TRANSCRIPT",
        help=f"Set the log level of the transcript (defaults to '{DEFAULT_LOG_LEVEL_TRANSCRIPT}')",
    )
    @common_options
    @functools.wraps(func)
    def wrapper(*args: Any, **kwargs: Any) -> click.Context:
        return cast(click.Context, func(*args, **kwargs))
    return wrapper
@click.command("eval")
@click.argument("tasks", nargs=-1)
@eval_options
def eval_command(
    tasks: tuple[str] | None,
    solver: str | None,
    model: str | None,
    model_base_url: str | None,
    m: tuple[str] | None,
    model_config: str | None,
    t: tuple[str] | None,
    task_config: str | None,
    s: tuple[str] | None,
    solver_config: str | None,
    tags: str | None,
    trace: bool | None,
    approval: str | None,
    sandbox: str | None,
    no_sandbox_cleanup: bool | None,
    epochs: int | None,
    epochs_reducer: str | None,
    limit: str | None,
    sample_id: str | None,
    max_retries: int | None,
    timeout: int | None,
    max_connections: int | None,
    max_tokens: int | None,
    system_message: str | None,
    best_of: int | None,
    frequency_penalty: float | None,
    presence_penalty: float | None,
    logit_bias: str | None,
    seed: int | None,
    stop_seqs: str | None,
    temperature: float | None,
    top_p: float | None,
    top_k: int | None,
    num_choices: int | None,
    logprobs: bool | None,
    top_logprobs: int | None,
    parallel_tool_calls: bool | None,
    internal_tools: bool | None,
    max_tool_output: int | None,
    cache_prompt: str | None,
    reasoning_effort: str | None,
    reasoning_tokens: int | None,
    reasoning_history: Literal["none", "all", "last", "auto"] | None,
    response_schema: ResponseSchema | None,
    message_limit: int | None,
    token_limit: int | None,
    time_limit: int | None,
    working_limit: int | None,
    max_samples: int | None,
    max_tasks: int | None,
    max_subprocesses: int | None,
    max_sandboxes: int | None,
    fail_on_error: bool | float | None,
    no_fail_on_error: bool | None,
    no_log_samples: bool | None,
    log_images: bool | None,
    log_buffer: int | None,
    no_score: bool | None,
    no_score_display: bool | None,
    log_format: Literal["eval", "json"] | None,
    log_level_transcript: str,
    **common: Unpack[CommonOptions],
) -> None:
    """Evaluate tasks."""
    # read config
    config = config_from_locals(dict(locals()))
    # resolve common options
    process_common_options(common)
    # exec eval
    eval_exec(
        tasks=tasks,
        solver=solver,
        log_level=common["log_level"],
        log_level_transcript=log_level_transcript,
        log_dir=common["log_dir"],
        log_format=log_format,
        model=model,
        model_base_url=model_base_url,
        m=m,
        model_config=model_config,
        t=t,
        task_config=task_config,
        s=s,
        solver_config=solver_config,
        tags=tags,
        trace=trace,
        approval=approval,
        sandbox=sandbox,
        no_sandbox_cleanup=no_sandbox_cleanup,
        epochs=epochs,
        epochs_reducer=epochs_reducer,
        limit=limit,
        sample_id=sample_id,
        message_limit=message_limit,
        token_limit=token_limit,
        time_limit=time_limit,
        working_limit=working_limit,
        max_samples=max_samples,
        max_tasks=max_tasks,
        max_subprocesses=max_subprocesses,
        max_sandboxes=max_sandboxes,
        fail_on_error=fail_on_error,
        no_fail_on_error=no_fail_on_error,
        debug_errors=common["debug_errors"],
        no_log_samples=no_log_samples,
        log_images=log_images,
        log_buffer=log_buffer,
        no_score=no_score,
        no_score_display=no_score_display,
        is_eval_set=False,
        **config,
    )
@click.command("eval-set")
@click.argument("tasks", nargs=-1)
@click.option(
    "--retry-attempts",
    type=int,
    help="Maximum number of retry attempts before giving up (defaults to 10).",
    envvar="INSPECT_EVAL_RETRY_ATTEMPS",
)
@click.option(
    "--retry-wait",
    type=int,
    help="Time in seconds wait between attempts, increased exponentially. "
    + "(defaults to 30, resulting in waits of 30, 60, 120, 240, etc.). Wait time "
    + "per-retry will in no case by longer than 1 hour.",
    envvar="INSPECT_EVAL_RETRY_WAIT",
)
@click.option(
    "--retry-connections",
    type=float,
    help="Reduce max_connections at this rate with each retry (defaults to 0.5)",
    envvar="INSPECT_EVAL_RETRY_CONNECTIONS",
)
@click.option(
    "--no-retry-cleanup",
    type=bool,
    is_flag=True,
    help="Do not cleanup failed log files after retries",
    envvar="INSPECT_EVAL_NO_RETRY_CLEANUP",
)
@click.option(
    "--bundle-dir",
    type=str,
    is_flag=False,
    help="Bundle viewer and logs into output directory",
)
@click.option(
    "--bundle-overwrite",
    type=str,
    is_flag=True,
    help="Overwrite existing bundle dir.",
)
@eval_options
@click.pass_context
def eval_set_command(
    ctx: click.Context,
    tasks: tuple[str] | None,
    retry_attempts: int | None,
    retry_wait: int | None,
    retry_connections: float | None,
    no_retry_cleanup: bool | None,
    solver: str | None,
    trace: bool | None,
    approval: str | None,
    model: str | None,
    model_base_url: str | None,
    m: tuple[str] | None,
    model_config: str | None,
    t: tuple[str] | None,
    task_config: str | None,
    s: tuple[str] | None,
    solver_config: str | None,
    tags: str | None,
    sandbox: str | None,
    no_sandbox_cleanup: bool | None,
    epochs: int | None,
    epochs_reducer: str | None,
    limit: str | None,
    sample_id: str | None,
    max_retries: int | None,
    timeout: int | None,
    max_connections: int | None,
    max_tokens: int | None,
    system_message: str | None,
    best_of: int | None,
    frequency_penalty: float | None,
    presence_penalty: float | None,
    logit_bias: str | None,
    seed: int | None,
    stop_seqs: str | None,
    temperature: float | None,
    top_p: float | None,
    top_k: int | None,
    num_choices: int | None,
    logprobs: bool | None,
    top_logprobs: int | None,
    parallel_tool_calls: bool | None,
    internal_tools: bool | None,
    max_tool_output: int | None,
    cache_prompt: str | None,
    reasoning_effort: str | None,
    reasoning_tokens: int | None,
    reasoning_history: Literal["none", "all", "last", "auto"] | None,
    response_schema: ResponseSchema | None,
    message_limit: int | None,
    token_limit: int | None,
    time_limit: int | None,
    working_limit: int | None,
    max_samples: int | None,
    max_tasks: int | None,
    max_subprocesses: int | None,
    max_sandboxes: int | None,
    fail_on_error: bool | float | None,
    no_fail_on_error: bool | None,
    no_log_samples: bool | None,
    log_images: bool | None,
    log_buffer: int | None,
    no_score: bool | None,
    no_score_display: bool | None,
    bundle_dir: str | None,
    bundle_overwrite: bool | None,
    log_format: Literal["eval", "json"] | None,
    log_level_transcript: str,
    **common: Unpack[CommonOptions],
) -> int:
    """Evaluate a set of tasks with retries.
    Learn more about eval sets at https://inspect.ai-safety-institute.org.uk/eval-sets.html.
    """
    # read config
    config = config_from_locals(dict(locals()))
    # resolve common options
    process_common_options(common)
    # exec eval
    success = eval_exec(
        tasks=tasks,
        solver=solver,
        log_level=common["log_level"],
        log_level_transcript=log_level_transcript,
        log_dir=common["log_dir"],
        log_format=log_format,
        model=model,
        model_base_url=model_base_url,
        m=m,
        model_config=model_config,
        t=t,
        task_config=task_config,
        s=s,
        solver_config=solver_config,
        tags=tags,
        trace=trace,
        approval=approval,
        sandbox=sandbox,
        no_sandbox_cleanup=no_sandbox_cleanup,
        epochs=epochs,
        epochs_reducer=epochs_reducer,
        limit=limit,
        sample_id=sample_id,
        message_limit=message_limit,
        token_limit=token_limit,
        time_limit=time_limit,
        working_limit=working_limit,
        max_samples=max_samples,
        max_tasks=max_tasks,
        max_subprocesses=max_subprocesses,
        max_sandboxes=max_sandboxes,
        fail_on_error=fail_on_error,
        no_fail_on_error=no_fail_on_error,
        debug_errors=common["debug_errors"],
        no_log_samples=no_log_samples,
        log_images=log_images,
        log_buffer=log_buffer,
        no_score=no_score,
        no_score_display=no_score_display,
        is_eval_set=True,
        retry_attempts=retry_attempts,
        retry_wait=retry_wait,
        retry_connections=retry_connections,
        retry_cleanup=not no_retry_cleanup,
        bundle_dir=bundle_dir,
        bundle_overwrite=True if bundle_overwrite else False,
        **config,
    )
    # exit code indicating whether the evals are all complete
    ctx.exit(0 if success else 1)
def eval_exec(
    tasks: tuple[str] | None,
    solver: str | None,
    log_level: str,
    log_level_transcript: str,
    log_dir: str,
    log_format: Literal["eval", "json"] | None,
    model: str | None,
    model_base_url: str | None,
    m: tuple[str] | None,
    model_config: str | None,
    t: tuple[str] | None,
    task_config: str | None,
    s: tuple[str] | None,
    solver_config: str | None,
    tags: str | None,
    trace: bool | None,
    approval: str | None,
    sandbox: str | None,
    no_sandbox_cleanup: bool | None,
    epochs: int | None,
    epochs_reducer: str | None,
    limit: str | None,
    sample_id: str | None,
    message_limit: int | None,
    token_limit: int | None,
    time_limit: int | None,
    working_limit: int | None,
    max_samples: int | None,
    max_tasks: int | None,
    max_subprocesses: int | None,
    max_sandboxes: int | None,
    fail_on_error: bool | float | None,
    no_fail_on_error: bool | None,
    debug_errors: bool | None,
    no_log_samples: bool | None,
    log_images: bool | None,
    log_buffer: int | None,
    no_score: bool | None,
    no_score_display: bool | None,
    is_eval_set: bool = False,
    retry_attempts: int | None = None,
    retry_wait: int | None = None,
    retry_connections: float | None = None,
    retry_cleanup: bool | None = None,
    bundle_dir: str | None = None,
    bundle_overwrite: bool = False,
    **kwargs: Unpack[GenerateConfigArgs],
) -> bool:
    # parse task, solver, and model args
    task_args = parse_cli_config(t, task_config)
    solver_args = parse_cli_config(s, solver_config)
    model_args = parse_cli_config(m, model_config)
    # parse tags
    eval_tags = parse_comma_separated(tags)
    # resolve epochs
    eval_epochs = (
        Epochs(epochs, create_reducers(parse_comma_separated(epochs_reducer)))
        if epochs
        else None
    )
    # resolve range and sample id
    eval_limit = parse_samples_limit(limit)
    eval_sample_id = parse_sample_id(sample_id)
    # resolve fail_on_error
    if no_fail_on_error is True:
        fail_on_error = False
    elif fail_on_error == 0.0:
        fail_on_error = True
    # resolve negating options
    sandbox_cleanup = False if no_sandbox_cleanup else None
    log_samples = False if no_log_samples else None
    log_images = False if log_images is False else None
    trace = True if trace else None
    score = False if no_score else True
    score_display = False if no_score_display else None
    # build params
    params: dict[str, Any] = (
        dict(
            tasks=list(tasks) if tasks else None,
            model=model,
            model_base_url=model_base_url,
            model_args=model_args,
            task_args=task_args,
            solver=SolverSpec(solver, solver_args) if solver else None,
            tags=eval_tags,
            trace=trace,
            approval=approval,
            sandbox=parse_sandbox(sandbox),
            sandbox_cleanup=sandbox_cleanup,
            log_level=log_level,
            log_level_transcript=log_level_transcript,
            log_dir=log_dir,
            log_format=log_format,
            limit=eval_limit,
            sample_id=eval_sample_id,
            epochs=eval_epochs,
            fail_on_error=fail_on_error,
            debug_errors=debug_errors,
            message_limit=message_limit,
            token_limit=token_limit,
            time_limit=time_limit,
            working_limit=working_limit,
            max_samples=max_samples,
            max_tasks=max_tasks,
            max_subprocesses=max_subprocesses,
            max_sandboxes=max_sandboxes,
            log_samples=log_samples,
            log_images=log_images,
            log_buffer=log_buffer,
            score=score,
            score_display=score_display,
        )
        | kwargs
    )
    # evaluate
    if is_eval_set:
        params["retry_attempts"] = retry_attempts
        params["retry_wait"] = retry_wait
        params["retry_connections"] = retry_connections
        params["retry_cleanup"] = retry_cleanup
        params["bundle_dir"] = bundle_dir
        params["bundle_overwrite"] = bundle_overwrite
        success, _ = eval_set(**params)
        return success
    else:
        eval(**params)
        return True
def config_from_locals(locals: dict[str, Any]) -> GenerateConfigArgs:
    # build generate config
    config_keys = list(GenerateConfigArgs.__mutable_keys__)  # type: ignore
    config = GenerateConfigArgs()
    for key, value in locals.items():
        if key in config_keys and value is not None:
            if key == "stop_seqs":
                value = value.split(",")
            if key == "logprobs" and value is False:
                value = None
            if key == "logit_bias" and value is not None:
                value = parse_logit_bias(value)
            if key == "cache_prompt":
                if value.lower() == "true":
                    value = True
                elif value.lower() == "false":
                    value = False
            if key == "parallel_tool_calls":
                if value is not False:
                    value = None
            if key == "internal_tools":
                if value is not False:
                    value = None
            if key == "reasoning_history":
                if value is not False:
                    value = None
            if key == "response_schema":
                if value is not None:
                    value = ResponseSchema.model_validate_json(value)
            config[key] = value  # type: ignore
    return config
def parse_logit_bias(logit_bias: str | None) -> dict[int, float] | None:
    logit_biases = parse_cli_args(logit_bias.split(",")) if logit_bias else None
    if logit_biases:
        return dict(
            zip([int(key) for key in logit_biases.keys()], logit_biases.values())
        )
    else:
        return None
def parse_comma_separated(value: str | None) -> list[str] | None:
    if value is not None:
        return value.split(",")
    else:
        return None
@click.command("eval-retry")
@click.argument("log_files", nargs=-1, required=True)
@click.option(
    "--max-samples", type=int, help=MAX_SAMPLES_HELP, envvar="INSPECT_EVAL_MAX_SAMPLES"
)
@click.option(
    "--max-tasks", type=int, help=MAX_TASKS_HELP, envvar="INSPECT_EVAL_MAX_TASKS"
)
@click.option(
    "--max-subprocesses",
    type=int,
    help=MAX_SUBPROCESSES_HELP,
    envvar="INSPECT_EVAL_MAX_SUBPROCESSES",
)
@click.option(
    "--max-sandboxes",
    type=int,
    help=MAX_SANDBOXES_HELP,
    envvar="INSPECT_EVAL_MAX_SANDBOXES",
)
@click.option(
    "--no-sandbox-cleanup",
    type=bool,
    is_flag=True,
    help=NO_SANDBOX_CLEANUP_HELP,
)
@click.option(
    "--trace",
    type=bool,
    is_flag=True,
    hidden=True,
    help="Trace message interactions with evaluated model to terminal.",
    envvar="INSPECT_EVAL_TRACE",
)
@click.option(
    "--fail-on-error",
    type=float,
    is_flag=False,
    flag_value=0.0,
    help=FAIL_ON_ERROR_HELP,
    envvar="INSPECT_EVAL_FAIL_ON_ERROR",
)
@click.option(
    "--no-fail-on-error",
    type=bool,
    is_flag=True,
    default=False,
    help=NO_FAIL_ON_ERROR_HELP,
    envvar="INSPECT_EVAL_NO_FAIL_ON_ERROR",
)
@click.option(
    "--no-log-samples",
    type=bool,
    is_flag=True,
    help=NO_LOG_SAMPLES_HELP,
    envvar="INSPECT_EVAL_LOG_SAMPLES",
)
@click.option(
    "--log-images/--no-log-images",
    type=bool,
    default=True,
    is_flag=True,
    help=LOG_IMAGES_HELP,
    envvar="INSPECT_EVAL_LOG_IMAGES",
)
@click.option(
    "--log-buffer", type=int, help=LOG_BUFFER_HELP, envvar="INSPECT_EVAL_LOG_BUFFER"
)
@click.option(
    "--no-score",
    type=bool,
    is_flag=True,
    help=NO_SCORE_HELP,
    envvar="INSPECT_EVAL_SCORE",
)
@click.option(
    "--no-score-display",
    type=bool,
    is_flag=True,
    help=NO_SCORE_HELP,
    envvar="INSPECT_EVAL_SCORE_DISPLAY",
)
@click.option(
    "--max-connections",
    type=int,
    help=MAX_CONNECTIONS_HELP,
    envvar="INSPECT_EVAL_MAX_CONNECTIONS",
)
@click.option(
    "--max-retries", type=int, help=MAX_RETRIES_HELP, envvar="INSPECT_EVAL_MAX_RETRIES"
)
@click.option("--timeout", type=int, help=TIMEOUT_HELP, envvar="INSPECT_EVAL_TIMEOUT")
@click.option(
    "--log-level-transcript",
    type=click.Choice(
        [level.lower() for level in ALL_LOG_LEVELS],
        case_sensitive=False,
    ),
    default=DEFAULT_LOG_LEVEL_TRANSCRIPT,
    envvar="INSPECT_LOG_LEVEL_TRANSCRIPT",
    help=f"Set the log level of the transcript (defaults to '{DEFAULT_LOG_LEVEL_TRANSCRIPT}')",
)
@common_options
def eval_retry_command(
    log_files: tuple[str],
    max_samples: int | None,
    max_tasks: int | None,
    max_subprocesses: int | None,
    max_sandboxes: int | None,
    no_sandbox_cleanup: bool | None,
    trace: bool | None,
    fail_on_error: bool | float | None,
    no_fail_on_error: bool | None,
    no_log_samples: bool | None,
    log_images: bool | None,
    log_buffer: int | None,
    no_score: bool | None,
    no_score_display: bool | None,
    max_connections: int | None,
    max_retries: int | None,
    timeout: int | None,
    log_level_transcript: str,
    **common: Unpack[CommonOptions],
) -> None:
    """Retry failed evaluation(s)"""
    # resolve common options
    process_common_options(common)
    # resolve negating options
    sandbox_cleanup = False if no_sandbox_cleanup else None
    log_samples = False if no_log_samples else None
    log_images = False if log_images is False else None
    score = False if no_score else True
    score_display = False if no_score_display else None
    # resolve fail_on_error
    if no_fail_on_error is True:
        fail_on_error = False
    elif fail_on_error == 0.0:
        fail_on_error = True
    # resolve log file
    retry_log_files = [
        log_file_info(filesystem(log_file).info(log_file)) for log_file in log_files
    ]
    # retry
    eval_retry(
        retry_log_files,
        log_level=common["log_level"],
        log_level_transcript=log_level_transcript,
        log_dir=common["log_dir"],
        max_samples=max_samples,
        max_tasks=max_tasks,
        max_subprocesses=max_subprocesses,
        max_sandboxes=max_sandboxes,
        sandbox_cleanup=sandbox_cleanup,
        trace=trace,
        fail_on_error=fail_on_error,
        debug_errors=common["debug_errors"],
        log_samples=log_samples,
        log_images=log_images,
        log_buffer=log_buffer,
        score=score,
        score_display=score_display,
        max_retries=max_retries,
        timeout=timeout,
        max_connections=max_connections,
    )

================
File: src/inspect_ai/_cli/info.py
================
from json import dumps
import click
from inspect_ai import __version__
from inspect_ai._util.constants import PKG_PATH
from inspect_ai._view.server import resolve_header_only
from inspect_ai.log._file import eval_log_json_str, read_eval_log
from .log import headers, schema, types
@click.group("info")
def info_command() -> None:
    """Read configuration and log info."""
    return None
@info_command.command("version")
@click.option(
    "--json",
    type=bool,
    is_flag=True,
    default=False,
    help="Output version and path info as JSON",
)
def version(json: bool) -> None:
    """Output version and path info."""
    if json:
        print(dumps(dict(version=__version__, path=PKG_PATH.as_posix()), indent=2))
    else:
        print(f"version: {__version__}")
        print(f"path: {PKG_PATH.as_posix()}")
@info_command.command("log-file", hidden=True)
@click.argument("path")
@click.option(
    "--header-only",
    type=int,
    is_flag=False,
    flag_value=0,
    help="Read and print only the header of the log file (i.e. no samples).",
)
def log(path: str, header_only: int) -> None:
    """Print log file contents as JSON."""
    header_only = resolve_header_only(path, header_only)
    log = read_eval_log(path, header_only=header_only)
    print(eval_log_json_str(log))
@info_command.command("log-file-headers", hidden=True)
@click.argument("files", nargs=-1)
def log_file_headers(files: tuple[str]) -> None:
    """Read and print a JSON list of log file headers."""
    headers(files)
@info_command.command("log-schema", hidden=True)
def log_schema() -> None:
    """Print JSON schema for log files."""
    schema()
@info_command.command("log-types", hidden=True)
def log_types() -> None:
    """Print TS declarations for log files."""
    types()

================
File: src/inspect_ai/_cli/list.py
================
from json import dumps
from typing import Literal
import click
from pydantic_core import to_jsonable_python
from typing_extensions import Unpack
from inspect_ai._cli.common import CommonOptions, common_options, process_common_options
from inspect_ai._cli.log import list_logs_options, log_list
from inspect_ai._cli.util import parse_cli_args
from inspect_ai._eval.list import list_tasks
from inspect_ai._eval.task import TaskInfo
@click.group("list")
def list_command() -> None:
    """List tasks on the filesystem."""
    return None
@list_command.command("tasks")
@click.option(
    "-F",
    multiple=True,
    type=str,
    help="One or more boolean task filters (e.g. -F light=true or -F draft~=false)",
)
@click.option(
    "--absolute",
    type=bool,
    is_flag=True,
    default=False,
    help="List absolute paths to task scripts (defaults to relative to the cwd).",
)
@click.option(
    "--json",
    type=bool,
    is_flag=True,
    default=False,
    help="Output listing as JSON",
)
@click.argument("paths", nargs=-1)
@common_options
def tasks(
    paths: tuple[str] | None,
    f: tuple[str] | None,
    absolute: bool,
    json: bool,
    **kwargs: Unpack[CommonOptions],
) -> None:
    """List tasks in given directories."""
    # resolve common options
    process_common_options(kwargs)
    # parse filter expressions and build a filter from it
    filters = parse_cli_args(f)
    def task_filter(task: TaskInfo) -> bool:
        for name, value in filters.items():
            if name.endswith("~"):
                name = name[:-1]
                include = task.attribs.get(name, None) != value
            else:
                include = task.attribs.get(name, None) == value
            if not include:
                return False
        return True
    # list tasks
    tasks = list_tasks(
        globs=list(paths) if paths else [], absolute=absolute, filter=task_filter
    )
    # print as JSON or plain text
    if json:
        print(dumps(to_jsonable_python(tasks, exclude_none=True), indent=2))
    else:
        print("\n".join([f"{task.file}@{task.name}" for task in tasks]))
@list_command.command("logs", hidden=True)
@list_logs_options
def list_logs_command(
    status: Literal["started", "success", "cancelled", "error"] | None,
    absolute: bool,
    json: bool,
    no_recursive: bool | None,
    **common: Unpack[CommonOptions],
) -> None:
    log_list(status, absolute, json, no_recursive, **common)

================
File: src/inspect_ai/_cli/log.py
================
import functools
import os
from json import dumps
from typing import Any, Callable, Literal, cast
from urllib.parse import urlparse
import click
from fsspec.core import split_protocol  # type: ignore
from pydantic_core import to_jsonable_python
from typing_extensions import Unpack
from inspect_ai._cli.common import CommonOptions, common_options, process_common_options
from inspect_ai._util.constants import PKG_PATH
from inspect_ai.log import list_eval_logs
from inspect_ai.log._convert import convert_eval_logs
from inspect_ai.log._file import (
    eval_log_json_str,
    read_eval_log,
    read_eval_log_headers,
)
@click.group("log")
def log_command() -> None:
    """Query, read, and convert logs.
    Inspect supports two log formats: 'eval' which is a compact, high performance binary format and 'json' which represents logs as JSON.
    The default format is 'eval'. You can change this by setting the INSPECT_LOG_FORMAT environment variable or using the --log-format command line option.
    The 'log' commands enable you to read Inspect logs uniformly as JSON no matter their physical storage format, and also enable you to read only the headers (everything but the samples) from log files, which is useful for very large logs.
    Learn more about managing log files at https://inspect.ai-safety-institute.org.uk/eval-logs.html.
    """
    return None
def list_logs_options(func: Callable[..., Any]) -> Callable[..., click.Context]:
    @click.option(
        "--status",
        type=click.Choice(
            ["started", "success", "cancelled", "error"], case_sensitive=False
        ),
        help="List only log files with the indicated status.",
    )
    @click.option(
        "--absolute",
        type=bool,
        is_flag=True,
        default=False,
        help="List absolute paths to log files (defaults to relative to the cwd).",
    )
    @click.option(
        "--json",
        type=bool,
        is_flag=True,
        default=False,
        help="Output listing as JSON",
    )
    @click.option(
        "--no-recursive",
        type=bool,
        is_flag=True,
        help="List log files recursively (defaults to True).",
    )
    @common_options
    @functools.wraps(func)
    def wrapper(*args: Any, **kwargs: Any) -> click.Context:
        return cast(click.Context, func(*args, **kwargs))
    return wrapper
def log_list(
    status: Literal["started", "success", "cancelled", "error"] | None,
    absolute: bool,
    json: bool,
    no_recursive: bool | None,
    **common: Unpack[CommonOptions],
) -> None:
    process_common_options(common)
    # list the logs
    logs = list_eval_logs(
        log_dir=common["log_dir"],
        filter=(lambda log: log.status == status) if status else None,
        recursive=no_recursive is not True,
    )
    # convert file names
    for log in logs:
        if urlparse(log.name).scheme == "file":
            _, path = split_protocol(log.name)
            log.name = path
            if not absolute:
                log.name = os.path.relpath(log.name, os.path.curdir)
    if json:
        logs_dicts = [log.model_dump() for log in logs]
        print(dumps(logs_dicts, indent=2))
    else:
        for log in logs:
            print(log.name)
@log_command.command("list")
@list_logs_options
def list_command(
    status: Literal["started", "success", "cancelled", "error"] | None,
    absolute: bool,
    json: bool,
    no_recursive: bool | None,
    **common: Unpack[CommonOptions],
) -> None:
    """List all logs in the log directory."""
    log_list(status, absolute, json, no_recursive, **common)
@log_command.command("dump")
@click.argument("path")
@click.option(
    "--header-only",
    type=bool,
    is_flag=True,
    default=False,
    help="Read and print only the header of the log file (i.e. no samples).",
)
def dump_command(path: str, header_only: bool) -> None:
    """Print log file contents as JSON."""
    log = read_eval_log(path, header_only=header_only)
    print(eval_log_json_str(log))
@log_command.command("convert")
@click.argument("path")
@click.option(
    "--to",
    type=click.Choice(["eval", "json"], case_sensitive=False),
    required=True,
    help="Target format to convert to.",
)
@click.option(
    "--output-dir",
    required=True,
    help="Directory to write converted log files to.",
)
@click.option(
    "--overwrite",
    type=bool,
    is_flag=True,
    default=False,
    help="Overwrite files in the output directory.",
)
def convert_command(
    path: str, to: Literal["eval", "json"], output_dir: str, overwrite: bool
) -> None:
    """Convert between log file formats."""
    convert_eval_logs(path, to, output_dir, overwrite)
@log_command.command("headers", hidden=True)
@click.argument("files", nargs=-1)
def headers_command(files: tuple[str]) -> None:
    """Print log file headers as JSON."""
    headers(files)
def headers(files: tuple[str]) -> None:
    """Print log file headers as JSON."""
    headers = read_eval_log_headers(list(files))
    print(dumps(to_jsonable_python(headers, exclude_none=True), indent=2))
@log_command.command("schema")
def schema_command() -> None:
    """Print JSON schema for log files."""
    schema()
def schema() -> None:
    print(view_resource("log-schema.json"))
@log_command.command("types", hidden=True)
def types_command() -> None:
    """Print TS declarations for log files."""
    types()
def types() -> None:
    print(view_type_resource("log.d.ts"))
def view_resource(file: str) -> str:
    resource = PKG_PATH / "_view" / "www" / file
    with open(resource, "r", encoding="utf-8") as f:
        return f.read()
def view_type_resource(file: str) -> str:
    resource = PKG_PATH / "_view" / "www" / "src" / "types" / file
    with open(resource, "r", encoding="utf-8") as f:
        return f.read()

================
File: src/inspect_ai/_cli/main.py
================
import click
from inspect_ai._util.dotenv import init_dotenv
from inspect_ai._util.error import set_exception_hook
from .. import __version__
from .cache import cache_command
from .eval import eval_command, eval_retry_command, eval_set_command
from .info import info_command
from .list import list_command
from .log import log_command
from .sandbox import sandbox_command
from .score import score_command
from .trace import trace_command
from .view import view_command
@click.group(invoke_without_command=True)
@click.option(
    "--version",
    type=bool,
    is_flag=True,
    default=False,
    help="Print the Inspect version.",
)
@click.pass_context
def inspect(ctx: click.Context, version: bool) -> None:
    # if this was a subcommand then allow it to execute
    if ctx.invoked_subcommand is not None:
        return
    if version:
        print(__version__)
        ctx.exit()
    else:
        click.echo(ctx.get_help())
        ctx.exit()
inspect.add_command(cache_command)
inspect.add_command(eval_command)
inspect.add_command(eval_set_command)
inspect.add_command(eval_retry_command)
inspect.add_command(info_command)
inspect.add_command(list_command)
inspect.add_command(log_command)
inspect.add_command(score_command)
inspect.add_command(view_command)
inspect.add_command(sandbox_command)
inspect.add_command(trace_command)
def main() -> None:
    set_exception_hook()
    init_dotenv()
    inspect(auto_envvar_prefix="INSPECT")  # pylint: disable=no-value-for-parameter
if __name__ == "__main__":
    main()

================
File: src/inspect_ai/_cli/sandbox.py
================
import anyio
import click
from inspect_ai._util._async import configured_async_backend
from inspect_ai.util._sandbox.registry import registry_find_sandboxenv
@click.group("sandbox")
def sandbox_command() -> None:
    """Manage Sandbox Environments.
    Learn more about sandboxing at https://inspect.ai-safety-institute.org.uk/sandboxing.html.
    """
    return None
@sandbox_command.command("cleanup")
@click.argument("type", type=str, required=True)
@click.argument("environment_id", type=str, required=False)
def sandbox_cleanup(type: str, environment_id: str | None) -> None:
    """Cleanup Sandbox Environments.
    TYPE specifies the sandbox environment type (e.g. 'docker')
    Pass an ENVIRONMENT_ID to cleanup only a single environment
    (otherwise all environments will be cleaned up).
    """
    sandboxenv_type = registry_find_sandboxenv(type)
    cli_cleanup = getattr(sandboxenv_type, "cli_cleanup")
    anyio.run(cli_cleanup, environment_id, backend=configured_async_backend())

================
File: src/inspect_ai/_cli/score.py
================
import os
import anyio
import click
import rich
from rich.panel import Panel
from rich.prompt import Prompt
from rich.table import Table
from typing_extensions import Unpack
from inspect_ai._cli.util import parse_cli_config
from inspect_ai._display import display
from inspect_ai._display.core.rich import rich_theme
from inspect_ai._eval.context import init_eval_context, init_task_context
from inspect_ai._eval.score import ScoreAction, task_score
from inspect_ai._util._async import configured_async_backend
from inspect_ai._util.file import basename, dirname, exists
from inspect_ai.log._log import EvalLog
from inspect_ai.log._recorders import create_recorder_for_location
from inspect_ai.model import get_model
from .common import CommonOptions, common_options, process_common_options
SCORES_PER_ROW = 4
@click.command("score")
@click.argument("log-file", type=str, required=True)
@click.option(
    "--scorer",
    type=str,
    envvar="INSPECT_SCORE_SCORER",
    help="Scorer to use for scoring",
)
@click.option(
    "-S",
    multiple=True,
    type=str,
    envvar="INSPECT_SCORE_SCORER_ARGS",
    help="One or more scorer arguments (e.g. -S arg=value)",
)
@click.option(
    "--action",
    type=click.Choice(["append", "overwrite"]),
    envvar="INSPECT_SCORE_SCORER_ACTION",
    help="Whether to append or overwrite the existing scores.",
)
@click.option(
    "--overwrite",
    type=bool,
    is_flag=True,
    help="Overwrite log file with the scored version",
)
@common_options
def score_command(
    log_file: str,
    overwrite: bool | None,
    scorer: str | None,
    s: tuple[str] | None,
    action: ScoreAction | None,
    **common: Unpack[CommonOptions],
) -> None:
    """Score a previous evaluation run."""
    # read common options
    process_common_options(common)
    # score
    async def run_score() -> None:
        return await score(
            log_dir=common["log_dir"],
            log_file=log_file,
            scorer=scorer,
            s=s,
            overwrite=False if overwrite is None else overwrite,
            action=action,
            log_level=common["log_level"],
        )
    anyio.run(run_score, backend=configured_async_backend())
async def score(
    log_dir: str,
    log_file: str,
    scorer: str | None,
    s: tuple[str] | None,
    overwrite: bool,
    action: ScoreAction | None,
    log_level: str | None,
    output_file: str | None = None,
) -> None:
    # init eval context
    init_eval_context(log_level, None)
    scorer_args = parse_cli_config(args=s, config=None)
    # read the eval log
    recorder = create_recorder_for_location(log_file, log_dir)
    eval_log = await recorder.read_log(log_file)
    # resolve the target output file (prompts user)
    output_file = resolve_output_file(
        log_file, output_file=output_file, overwrite=overwrite
    )
    # resolve action
    action = resolve_action(eval_log, action)
    # check that there are samples therein
    if eval_log.samples is None or len(eval_log.samples) == 0:
        raise ValueError(f"{log_file} does not include samples to score")
    # get the model then initialize the async context
    model = get_model(
        model=eval_log.eval.model,
        config=eval_log.plan.config,
        **eval_log.eval.model_args,
    )
    # initialize active model
    init_task_context(model)
    # re-score the task
    eval_log = await task_score(
        log=eval_log, scorer=scorer, scorer_args=scorer_args, action=action
    )
    # re-write the log
    await recorder.write_log(output_file, eval_log)
    # print results
    print_results(output_file, eval_log)
def print_results(output_file: str, eval_log: EvalLog) -> None:
    # the theme
    theme = rich_theme()
    # Create results panel
    grid = Table.grid(expand=True)
    grid.add_column()
    grid.add_row("")
    if eval_log.results:
        # Process scores in groups
        for i in range(0, len(eval_log.results.scores), SCORES_PER_ROW):
            # Create a grid for this row of scores
            score_row = Table.grid(
                expand=False,
                padding=(0, 2, 0, 0),
            )
            # Add columns for each score in this row
            for _ in range(SCORES_PER_ROW):
                score_row.add_column()
            # Create individual score tables and add them to the row
            score_tables: list[Table | str] = []
            for score in eval_log.results.scores[i : i + SCORES_PER_ROW]:
                table = Table(
                    show_header=False, show_lines=False, box=None, show_edge=False
                )
                table.add_column()
                table.add_column()
                # Add score name and metrics
                table.add_row(f"[bold]{score.name}[/bold]")
                for name, metric in score.metrics.items():
                    table.add_row(f"{name}", f"{metric.value:.3f}")
                score_tables.append(table)
            # Fill remaining slots with empty tables if needed
            while len(score_tables) < SCORES_PER_ROW:
                score_tables.append("")
            # Add the score tables to this row
            score_row.add_row(*score_tables)
            # Add this row of scores to the main grid
            grid.add_row(score_row)
    grid.add_row("")
    grid.add_row(f" Log: [{theme.link}]{output_file}[/{theme.link}]")
    p = Panel(
        title=f"[bold][{theme.meta}]Results for {eval_log.eval.task}[/bold][/{theme.meta}]",
        title_align="left",
        renderable=grid,
    )
    # Print the results panel
    display().print("")
    console = rich.get_console()
    console.print(p)
def resolve_output_file(log_file: str, output_file: str | None, overwrite: bool) -> str:
    # resolve the output file (we may overwrite, use the passed file name, or suggest a new name)
    if output_file is None:
        if overwrite:
            # explicitly asked to overwrite
            return log_file
        else:
            if exists(log_file):
                # Ask if we should overwrite
                file_action = Prompt.ask(
                    "Overwrite existing log file or create new log file?",
                    choices=["overwrite", "create", "o", "c"],
                    default="create",
                )
                if file_action in ["overwrite", "o"]:
                    return log_file
                else:
                    file_name = basename(log_file)
                    base_dir = dirname(log_file)
                    _, ext = os.path.splitext(file_name)
                    count = 0
                    def filename() -> str:
                        if count > 0:
                            return f"{file_name.removesuffix(ext)}-scored-{count}{ext}"
                        else:
                            return f"{file_name.removesuffix(ext)}-scored{ext}"
                    while exists(f"{os.path.join(base_dir, filename())}"):
                        count = count + 1
                    suggested_file = filename()
                    user_file = Prompt.ask("Output file name?", default=suggested_file)
                    return os.path.join(base_dir, user_file)
            else:
                return log_file
    else:
        return output_file
def resolve_action(eval_log: EvalLog, action: ScoreAction | None) -> ScoreAction:
    if action is not None:
        return action
    if eval_log.results is not None and len(eval_log.results.scores) > 0:
        user_action = Prompt.ask(
            "Overwrite existing scores or append as additional scores?",
            choices=["overwrite", "append", "o", "a"],
            default="append",
        )
        return "overwrite" if user_action in ["ovewrite", "o"] else "append"
    else:
        return "overwrite"

================
File: src/inspect_ai/_cli/trace.py
================
import os
import shlex
import time
from datetime import datetime
from json import dumps
from pathlib import Path
from typing import Callable
import click
from pydantic_core import to_json
from rich import print as r_print
from rich.console import Console, RenderableType
from rich.table import Column, Table
from inspect_ai._util.error import PrerequisiteError
from inspect_ai._util.trace import (
    ActionTraceRecord,
    TraceRecord,
    inspect_trace_dir,
    list_trace_files,
    read_trace_file,
)
@click.group("trace")
def trace_command() -> None:
    """List and read execution traces.
    Inspect includes a TRACE log-level which is right below the HTTP and INFO log levels (so not written to the console by default). However, TRACE logs are always recorded to a separate file, and the last 10 TRACE logs are preserved. The 'trace' command provides ways to list and read these traces.
    Learn more about execution traces at https://inspect.ai-safety-institute.org.uk/tracing.html.
    """
    return None
@trace_command.command("list")
@click.option(
    "--json",
    type=bool,
    is_flag=True,
    default=False,
    help="Output listing as JSON",
)
def list_command(json: bool) -> None:
    """List all trace files."""
    trace_files = list_trace_files()
    if json:
        print(
            dumps(
                [dict(file=str(file.file), mtime=file.mtime) for file in trace_files],
                indent=2,
            )
        )
    else:
        table = Table(box=None, show_header=True, pad_edge=False)
        table.add_column("Time")
        table.add_column("Trace File")
        for file in trace_files:
            mtime = datetime.fromtimestamp(file.mtime).astimezone()
            table.add_row(
                mtime.strftime("%d-%b %H:%M:%S %Z"), shlex.quote(str(file.file))
            )
        r_print(table)
@trace_command.command("dump")
@click.argument("trace-file", type=str, required=False)
@click.option(
    "--filter",
    type=str,
    help="Filter (applied to trace message field).",
)
def dump_command(trace_file: str | None, filter: str | None) -> None:
    """Dump a trace file to stdout (as a JSON array of log records)."""
    trace_file_path = _resolve_trace_file_path(trace_file)
    traces = read_trace_file(trace_file_path)
    if filter:
        filter = filter.lower()
        traces = [trace for trace in traces if filter in trace.message.lower()]
    print(
        to_json(traces, indent=2, exclude_none=True, fallback=lambda _: None).decode()
    )
@trace_command.command("http")
@click.argument("trace-file", type=str, required=False)
@click.option(
    "--filter",
    type=str,
    help="Filter (applied to trace message field).",
)
@click.option(
    "--failed",
    type=bool,
    is_flag=True,
    default=False,
    help="Show only failed HTTP requests (non-200 status)",
)
def http_command(trace_file: str | None, filter: str | None, failed: bool) -> None:
    """View all HTTP requests in the trace log."""
    _, traces = _read_traces(trace_file, "HTTP", filter)
    last_timestamp = ""
    table = Table(Column(), Column(), box=None)
    for trace in traces:
        if failed and "200 OK" in trace.message:
            continue
        timestamp = trace.timestamp.split(".")[0]
        if timestamp == last_timestamp:
            timestamp = ""
        else:
            last_timestamp = timestamp
            timestamp = f"[{timestamp}]"
        table.add_row(timestamp, trace.message)
    if table.row_count > 0:
        r_print(table)
@trace_command.command("anomalies")
@click.argument("trace-file", type=str, required=False)
@click.option(
    "--filter",
    type=str,
    help="Filter (applied to trace message field).",
)
@click.option(
    "--all",
    is_flag=True,
    default=False,
    help="Show all anomolies including errors and timeouts (by default only still running and cancelled actions are shown).",
)
def anomolies_command(trace_file: str | None, filter: str | None, all: bool) -> None:
    """Look for anomalies in a trace file (never completed or cancelled actions)."""
    trace_file_path, traces = _read_traces(trace_file, None, filter)
    # Track started actions
    running_actions: dict[str, ActionTraceRecord] = {}
    canceled_actions: dict[str, ActionTraceRecord] = {}
    error_actions: dict[str, ActionTraceRecord] = {}
    timeout_actions: dict[str, ActionTraceRecord] = {}
    start_trace: ActionTraceRecord | None = None
    def action_started(trace: ActionTraceRecord) -> None:
        running_actions[trace.trace_id] = trace
    def action_completed(trace: ActionTraceRecord) -> ActionTraceRecord:
        nonlocal start_trace
        start_trace = running_actions.get(trace.trace_id)
        if start_trace:
            del running_actions[trace.trace_id]
            return start_trace
        else:
            raise RuntimeError(f"Expected {trace.trace_id} in action dictionary.")
    def action_failed(trace: ActionTraceRecord) -> None:
        nonlocal start_trace
        if all:
            assert start_trace
            error_actions[start_trace.trace_id] = trace
    def action_canceled(trace: ActionTraceRecord) -> None:
        nonlocal start_trace
        assert start_trace
        canceled_actions[start_trace.trace_id] = trace
    def action_timeout(trace: ActionTraceRecord) -> None:
        nonlocal start_trace
        if all:
            assert start_trace
            timeout_actions[start_trace.trace_id] = trace
    for trace in traces:
        if isinstance(trace, ActionTraceRecord):
            match trace.event:
                case "enter":
                    action_started(trace)
                case "exit":
                    action_completed(trace)
                case "cancel":
                    start_trace = action_completed(trace)
                    trace.start_time = start_trace.start_time
                    action_canceled(trace)
                case "error":
                    start_trace = action_completed(trace)
                    trace.start_time = start_trace.start_time
                    action_failed(trace)
                case "timeout":
                    start_trace = action_completed(trace)
                    trace.start_time = start_trace.start_time
                    action_timeout(trace)
                case _:
                    print(f"Unknown event type: {trace.event}")
    # do we have any traces?
    if (
        len(running_actions)
        + len(canceled_actions)
        + len(error_actions)
        + len(timeout_actions)
        == 0
    ):
        print(f"TRACE: {shlex.quote(trace_file_path.as_posix())}\n")
        if all:
            print("No anomalies found in trace log.")
        else:
            print(
                "No running or cancelled actions found in trace log (pass --all to see errors and timeouts)."
            )
        return
    with open(os.devnull, "w") as f:
        # generate output
        console = Console(record=True, file=f)
        def print_fn(o: RenderableType) -> None:
            console.print(o, highlight=False)
        print_fn(f"[bold]TRACE: {shlex.quote(trace_file_path.as_posix())}[bold]")
        _print_bucket(print_fn, "Running Actions", running_actions)
        _print_bucket(print_fn, "Cancelled Actions", canceled_actions)
        _print_bucket(print_fn, "Error Actions", error_actions)
        _print_bucket(print_fn, "Timeout Actions", timeout_actions)
        # print
        print(console.export_text(styles=True).strip())
def _read_traces(
    trace_file: str | None, level: str | None = None, filter: str | None = None
) -> tuple[Path, list[TraceRecord]]:
    trace_file_path = _resolve_trace_file_path(trace_file)
    traces = read_trace_file(trace_file_path)
    if level:
        traces = [trace for trace in traces if trace.level == level]
    if filter:
        filter = filter.lower()
        traces = [trace for trace in traces if filter in trace.message.lower()]
    return (trace_file_path, traces)
def _print_bucket(
    print_fn: Callable[[RenderableType], None],
    label: str,
    bucket: dict[str, ActionTraceRecord],
) -> None:
    if len(bucket) > 0:
        # Sort the items in chronological order of when
        # they finished so the first finished item is at the top
        sorted_actions = sorted(
            bucket.values(),
            key=lambda record: (record.start_time or 0) + (record.duration or 0),
            reverse=True,
        )
        # create table
        table = Table(
            Column(""),
            Column("", justify="right"),
            Column(""),
            Column("", width=22),
            box=None,
            title=label,
            title_justify="left",
            title_style="bold",
            pad_edge=False,
            padding=(0, 1),
        )
        for action in sorted_actions:
            # Compute duration (use the event duration or time since started)
            duration = (
                action.duration
                if action.duration is not None
                else time.time() - action.start_time
                if action.start_time is not None
                else 0.0
            )
            # The event start time
            start_time = formatTime(action.start_time) if action.start_time else "None"
            # Event detail
            detail = (
                f"{action.detail or action.message} {action.error}"
                if action.event == "error"
                else (action.detail or action.message)
            )
            table.add_row(
                action.action,
                f"{round(duration, 2):.2f}s".rjust(8),
                f" {detail}",
                start_time,
            )
        print_fn("")
        print_fn(table)
def _resolve_trace_file(trace_file: str | None) -> str:
    if trace_file is None:
        trace_files = list_trace_files()
        if len(trace_files) == 0:
            raise PrerequisiteError("No trace files currently availalble.")
        trace_file = str(trace_files[0].file)
    return trace_file
def _resolve_trace_file_path(trace_file: str | None) -> Path:
    trace_file = _resolve_trace_file(trace_file)
    trace_file_path = Path(trace_file)
    if not trace_file_path.is_absolute():
        trace_file_path = inspect_trace_dir() / trace_file_path
    if not trace_file_path.exists():
        raise PrerequisiteError(
            f"The specified trace file '{trace_file_path}' does not exist."
        )
    return trace_file_path
def formatTime(timestamp: float) -> str:
    dt = datetime.fromtimestamp(timestamp).astimezone()
    return dt.strftime("%H:%M:%S %Z")

================
File: src/inspect_ai/_cli/util.py
================
from typing import Any
import yaml
from inspect_ai._util.config import resolve_args
from inspect_ai.util._sandbox.environment import SandboxEnvironmentSpec
def parse_cli_config(
    args: tuple[str] | list[str] | None, config: str | None
) -> dict[str, Any]:
    # start with file if any
    cli_config: dict[str, Any] = {}
    if config is not None:
        cli_config = cli_config | resolve_args(config)
    # merge in cli args
    cli_args = parse_cli_args(args)
    cli_config.update(**cli_args)
    return cli_config
def parse_cli_args(args: tuple[str] | list[str] | None) -> dict[str, Any]:
    params: dict[str, Any] = dict()
    if args:
        for arg in list(args):
            parts = arg.split("=")
            if len(parts) > 1:
                key = parts[0].replace("-", "_")
                value = yaml.safe_load("=".join(parts[1:]))
                if isinstance(value, str):
                    value = value.split(",")
                    value = value if len(value) > 1 else value[0]
                params[key] = value
    return params
def parse_sandbox(sandbox: str | None) -> SandboxEnvironmentSpec | None:
    if sandbox is not None:
        parts = sandbox.split(":", maxsplit=1)
        if len(parts) == 1:
            return SandboxEnvironmentSpec(sandbox)
        else:
            return SandboxEnvironmentSpec(parts[0], parts[1])
    else:
        return None

================
File: src/inspect_ai/_cli/view.py
================
import functools
import os
from typing import Any, Callable, cast
import click
from typing_extensions import Unpack
from inspect_ai._util.constants import DEFAULT_SERVER_HOST, DEFAULT_VIEW_PORT
from inspect_ai._view.view import view
from inspect_ai.log._bundle import bundle_log_dir
from .common import CommonOptions, common_options, process_common_options
def start_options(func: Callable[..., Any]) -> Callable[..., click.Context]:
    @click.option(
        "--recursive",
        type=bool,
        is_flag=True,
        default=True,
        help="Include all logs in log_dir recursively.",
    )
    @click.option(
        "--host",
        default=DEFAULT_SERVER_HOST,
        help="Tcp/Ip host",
    )
    @click.option("--port", default=DEFAULT_VIEW_PORT, help="TCP/IP port")
    @functools.wraps(func)
    def wrapper(*args: Any, **kwargs: Any) -> click.Context:
        return cast(click.Context, func(*args, **kwargs))
    return wrapper
# Define the base command group
@click.group(name="view", invoke_without_command=True)
@start_options
@common_options
@click.pass_context
def view_command(ctx: click.Context, **kwargs: Unpack[CommonOptions]) -> None:
    """Inspect log viewer.
    Learn more about using the log viewer at https://inspect.ai-safety-institute.org.uk/log-viewer.html.
    """
    if ctx.invoked_subcommand is None:
        ctx.invoke(start, **kwargs)
    else:
        pass
@view_command.command("start")
@start_options
@common_options
def start(
    recursive: bool,
    host: str,
    port: int,
    **common: Unpack[CommonOptions],
) -> None:
    """View evaluation logs."""
    # read common options
    process_common_options(common)
    # resolve optional auth token
    INSPECT_VIEW_AUTHORIZATION_TOKEN = "INSPECT_VIEW_AUTHORIZATION_TOKEN"
    authorization = os.environ.get(INSPECT_VIEW_AUTHORIZATION_TOKEN, None)
    if authorization:
        # this indicates we are in vscode -- we want to set the log level to HTTP
        # in vscode, updated versions of the extension do this but we set it
        # manually here as a temporary bridge for running against older versions
        common["log_level"] = "HTTP"
        del os.environ[INSPECT_VIEW_AUTHORIZATION_TOKEN]
        os.unsetenv(INSPECT_VIEW_AUTHORIZATION_TOKEN)
    # run the viewer
    view(
        log_dir=common["log_dir"],
        recursive=recursive,
        host=host,
        port=port,
        authorization=authorization,
        log_level=common["log_level"],
    )
@view_command.command("bundle")
@common_options
@click.option(
    "--output-dir",
    required=True,
    help="The directory where bundled output will be placed.",
)
@click.option(
    "--overwrite",
    type=bool,
    is_flag=True,
    default=False,
    help="Overwrite files in the output directory.",
)
def bundle_command(
    output_dir: str,
    overwrite: bool,
    **common: Unpack[CommonOptions],
) -> None:
    """Bundle evaluation logs"""
    # process common options
    process_common_options(common)
    bundle_log_dir(
        output_dir=output_dir, log_dir=common["log_dir"], overwrite=overwrite
    )

================
File: src/inspect_ai/_display/__init__.py
================
from .core.active import display
from .core.display import (
    Display,
    Progress,
    TaskCancelled,
    TaskError,
    TaskProfile,
    TaskResult,
    TaskScreen,
    TaskSuccess,
    TaskWithResult,
)
__all__ = [
    "display",
    "Display",
    "Progress",
    "TaskCancelled",
    "TaskError",
    "TaskProfile",
    "TaskResult",
    "TaskScreen",
    "TaskWithResult",
    "TaskSuccess",
]

================
File: src/inspect_ai/_display/core/active.py
================
import sys
from contextvars import ContextVar
import rich
from inspect_ai.util._display import display_type
from ..plain.display import PlainDisplay
from ..rich.display import RichDisplay
from ..textual.display import TextualDisplay
from .display import Display, TaskScreen
_active_display: Display | None = None
def display() -> Display:
    global _active_display
    if _active_display is None:
        if display_type() == "plain":
            _active_display = PlainDisplay()
        elif (
            display_type() == "full"
            and sys.stdout.isatty()
            and not rich.get_console().is_jupyter
        ):
            _active_display = TextualDisplay()
        else:
            _active_display = RichDisplay()
    return _active_display
def task_screen() -> TaskScreen:
    screen = _active_task_screen.get(None)
    if screen is None:
        raise RuntimeError(
            "console input function called outside of running evaluation."
        )
    return screen
def init_task_screen(screen: TaskScreen) -> None:
    _active_task_screen.set(screen)
def clear_task_screen() -> None:
    _active_task_screen.set(None)
_active_task_screen: ContextVar[TaskScreen | None] = ContextVar(
    "task_screen", default=None
)

================
File: src/inspect_ai/_display/core/config.py
================
from inspect_ai._util.registry import is_model_dict, is_registry_dict
from inspect_ai.log._log import eval_config_defaults
from .display import TaskProfile
def task_config(
    profile: TaskProfile, generate_config: bool = True, style: str = ""
) -> str:
    # merge config
    # wind params back for display
    task_args = dict(profile.task_args)
    for key in task_args.keys():
        value = task_args[key]
        if is_registry_dict(value):
            task_args[key] = value["name"]
        if is_model_dict(value):
            task_args[key] = value["model"]
    # get eval_config overrides
    eval_config = dict(profile.eval_config.model_dump(exclude_none=True))
    for name, default_value in eval_config_defaults().items():
        if eval_config.get(name, None) == default_value:
            del eval_config[name]
    config = eval_config | task_args
    if generate_config:
        config = dict(profile.generate_config.model_dump(exclude_none=True)) | config
    if profile.tags:
        config["tags"] = ",".join(profile.tags)
    config_print: list[str] = []
    for name, value in config.items():
        if name == "approval" and isinstance(value, dict):
            config_print.append(
                f"{name}: {','.join([approver['name'] for approver in value['approvers']])}"
            )
        elif name == "sample_id":
            value = value if isinstance(value, list) else [value]
            value = [str(v) for v in value]
            config_print.append(f"{name}: {','.join(value)}")
        elif name not in ["limit", "model", "response_schema"]:
            if isinstance(value, list):
                value = ",".join([str(v) for v in value])
            if isinstance(value, str):
                value = value.replace("[", "\\[")
            config_print.append(f"{name}: {value}")
    values = ", ".join(config_print)
    if values:
        if style:
            return f"[{style}]{values}[/{style}]"
        else:
            return values
    else:
        return ""
def task_dict(d: dict[str, str], bold_value: bool = False) -> str:
    slot1, slot2 = ("", "[/bold]") if bold_value else ("[/bold]", "")
    return "  ".join(
        [f"[bold]{key}:{slot1} {value}{slot2}" for key, value in d.items()]
    )

================
File: src/inspect_ai/_display/core/display.py
================
import contextlib
from dataclasses import dataclass
from types import TracebackType
from typing import (
    Any,
    AsyncIterator,
    Callable,
    Coroutine,
    Iterator,
    Protocol,
    Type,
    TypeVar,
    Union,
    runtime_checkable,
)
import rich
from rich.console import Console
from inspect_ai.log import EvalConfig, EvalResults, EvalStats
from inspect_ai.model import GenerateConfig, ModelName
from ...util._panel import InputPanel
@runtime_checkable
class Progress(Protocol):
    def update(self, n: int = 1) -> None: ...
    def complete(self) -> None: ...
@dataclass
class TaskSpec:
    name: str
    model: ModelName
@dataclass
class TaskProfile:
    name: str
    file: str | None
    model: ModelName
    dataset: str
    scorer: str
    samples: int
    steps: int
    eval_config: EvalConfig
    task_args: dict[str, Any]
    generate_config: GenerateConfig
    tags: list[str] | None
    log_location: str
@dataclass
class TaskError:
    samples_completed: int
    exc_type: Type[Any]
    exc_value: BaseException
    traceback: TracebackType | None
@dataclass
class TaskCancelled:
    samples_completed: int
    stats: EvalStats
@dataclass
class TaskSuccess:
    samples_completed: int
    stats: EvalStats
    results: EvalResults
TaskResult = Union[TaskError, TaskCancelled, TaskSuccess]
@dataclass
class TaskWithResult:
    profile: TaskProfile
    result: TaskResult | None
TR = TypeVar("TR")
TP = TypeVar("TP", bound=InputPanel)
class TaskScreen(contextlib.AbstractContextManager["TaskScreen"]):
    def __exit__(self, *excinfo: Any) -> None:
        pass
    @contextlib.contextmanager
    def input_screen(
        self,
        header: str | None = None,
        transient: bool | None = None,
        width: int | None = None,
    ) -> Iterator[Console]:
        yield rich.get_console()
    async def input_panel(self, panel_type: type[TP]) -> TP:
        raise NotImplementedError("input_panel not implemented by current display")
@dataclass
class TaskDisplayMetric:
    scorer: str
    name: str
    value: float | int
    reducer: str | None
@runtime_checkable
class TaskDisplay(Protocol):
    @contextlib.contextmanager
    def progress(self) -> Iterator[Progress]: ...
    def sample_complete(self, complete: int, total: int) -> None: ...
    def update_metrics(self, scores: list[TaskDisplayMetric]) -> None: ...
    def complete(self, result: TaskResult) -> None: ...
@runtime_checkable
class Display(Protocol):
    def print(self, message: str) -> None: ...
    @contextlib.contextmanager
    def progress(self, total: int) -> Iterator[Progress]: ...
    def run_task_app(self, main: Callable[[], Coroutine[None, None, TR]]) -> TR: ...
    @contextlib.contextmanager
    def suspend_task_app(self) -> Iterator[None]: ...
    @contextlib.asynccontextmanager
    async def task_screen(
        self, tasks: list[TaskSpec], parallel: bool
    ) -> AsyncIterator[TaskScreen]:
        yield TaskScreen()
    @contextlib.contextmanager
    def task(self, profile: TaskProfile) -> Iterator[TaskDisplay]: ...
    def display_counter(self, caption: str, value: str) -> None: ...

================
File: src/inspect_ai/_display/core/footer.py
================
from rich.console import RenderableType
from rich.text import Text
from inspect_ai._util.retry import http_retries_count
from inspect_ai.util._concurrency import concurrency_status
from inspect_ai.util._throttle import throttle
from .config import task_dict
@throttle(1)
def task_footer(
    counters: dict[str, str], style: str = ""
) -> tuple[RenderableType, RenderableType]:
    return (
        Text.from_markup(task_resources(), style=style),
        Text.from_markup(task_counters(counters), style=style),
    )
def task_resources() -> str:
    resources: dict[str, str] = {}
    for model, resource in concurrency_status().items():
        resources[model] = f"{resource[0]}/{resource[1]}"
    return task_dict(resources)
def task_counters(counters: dict[str, str]) -> str:
    return task_dict(counters | task_http_retries())
def task_http_retries() -> dict[str, str]:
    return {"HTTP retries": f"{http_retries_count():,}"}
def task_http_retries_str() -> str:
    return f"HTTP retries: {http_retries_count():,}"

================
File: src/inspect_ai/_display/core/group.py
================
from dataclasses import dataclass
from typing import Sequence
from inspect_ai.log._transcript import Event, StepEvent, SubtaskEvent, ToolEvent
@dataclass
class EventGroup:
    """Event and (optionally) its embedded event groups.
    - Some events (e.g. SampleInitEvent, LogEvent) have no embedded event groups.
    - Some events (e.g. ToolEvent, SubtaskEvent) contain lists of event groups.
    - StepEvent has an implicit of event groups based on its begin/end instances.
    """
    event: Event
    level: int
    groups: list["EventGroup"] | None = None
def group_events(events: Sequence[Event], level: int = 1) -> list[EventGroup]:
    """Transform ordinary list of events into list of event groups."""
    # groups are either plain events (some of which can have sub-events)
    # and higher level steps (e.g. solvers/scorers) that contain events
    event_groups: list[EventGroup] = []
    # track stack of active steps
    active_steps: list[tuple[StepEvent, list[EventGroup]]] = []
    # iterate though events
    for event in events:
        # manage step events
        if isinstance(event, StepEvent):
            if event.action == "begin":
                active_steps.append((event, []))
            elif event.action == "end":
                begin_step, step_groups = active_steps.pop()
                target_group = (
                    active_steps[-1][1] if len(active_steps) else event_groups
                )
                target_group.append(
                    EventGroup(
                        event=begin_step,
                        level=level + len(active_steps),
                        groups=step_groups,
                    )
                )
        # other events
        else:
            # target level depends on whether we are appending to a set
            target_level = level + len(active_steps)
            # tool and subtask events have their own nested event lists
            if isinstance(event, ToolEvent | SubtaskEvent):
                group = EventGroup(
                    event=event,
                    groups=group_events(event.events, level=target_level + 1),
                    level=target_level,
                )
            else:
                group = EventGroup(event=event, level=target_level)
            # add to active step if we have one
            if len(active_steps) > 0:
                active_steps[-1][1].append(group)
            # otherwise just add to root list
            else:
                event_groups.append(group)
    # if there are active steps alive then collect them (an error
    # may have prevented them from having end steps)
    while len(active_steps) > 0:
        begin_step, step_groups = active_steps.pop()
        event_groups.append(
            EventGroup(event=begin_step, level=level, groups=step_groups)
        )
    return event_groups

================
File: src/inspect_ai/_display/core/panel.py
================
from typing import Tuple
import rich
from rich.console import RenderableType
from rich.panel import Panel
from rich.table import Table
from rich.text import Text
from inspect_ai._util.constants import CONSOLE_DISPLAY_WIDTH
from inspect_ai._util.path import cwd_relative_path
from inspect_ai._util.registry import registry_unqualified_name
from .display import TaskProfile
from .rich import is_vscode_notebook, rich_theme
def task_panel(
    profile: TaskProfile,
    show_model: bool,
    body: RenderableType,
    subtitle: RenderableType
    | str
    | Tuple[RenderableType | str, RenderableType | str]
    | None,
    footer: RenderableType | tuple[RenderableType, RenderableType] | None,
    log_location: str | None,
) -> Panel:
    # rendering context
    theme = rich_theme()
    console = rich.get_console()
    width = CONSOLE_DISPLAY_WIDTH if is_vscode_notebook(console) else None
    jupyter = console.is_jupyter
    # root table
    table = Table.grid(expand=True)
    table.add_column()
    # setup table
    if subtitle is not None:
        subtitle_table = Table.grid(expand=True)
        subtitle_table.add_column()
        if isinstance(subtitle, tuple):
            subtitle_table.add_column(justify="right")
            subtitle_table.add_row(
                to_renderable(subtitle[0]), to_renderable(subtitle[1], style=theme.meta)
            )
        else:
            subtitle_table.add_row(to_renderable(subtitle))
        table.add_row(subtitle_table)
    # main progress and task info
    if body:
        table.add_row(body)
    # spacing if there is more content
    if footer or log_location:
        table.add_row()
    # footer if specified
    if footer:
        footer_table = Table.grid(expand=True)
        footer_table.add_column()
        if isinstance(footer, tuple):
            footer_table.add_column(justify="right")
            footer_table.add_row(footer[0], footer[1])
        else:
            footer_table.add_row(footer)
        table.add_row(footer_table)
    # enclose in outer table for log link footer
    root = table
    if log_location:
        # if we are in jupyter then use a real hyperlink
        if jupyter:
            log_location = f"[link={log_location}]{log_location}[/link]"
        # Print a cwd relative path
        try:
            log_location_relative = cwd_relative_path(log_location, walk_up=True)
        except ValueError:
            log_location_relative = log_location
        root = Table.grid(expand=True)
        root.add_column()
        root.add_row(table)
        root.add_row()
        root.add_row(
            f"[bold][{theme.light}]Log:[/{theme.light}][/bold] "
            + f"[{theme.link}]{log_location_relative}[/{theme.link}]"
        )
    # create panel w/ title
    panel = Panel(
        root,
        title=f"[bold][{theme.meta}]{task_title(profile, show_model)}[/{theme.meta}][/bold]",
        title_align="left",
        width=width,
        expand=True,
    )
    return panel
def to_renderable(item: RenderableType | str, style: str = "") -> RenderableType:
    if isinstance(item, str):
        return Text.from_markup(item, style=style)
    else:
        return item
def tasks_title(completed: int, total: int) -> str:
    return f"{completed}/{total} tasks complete"
def task_title(profile: TaskProfile, show_model: bool) -> str:
    eval_epochs = profile.eval_config.epochs or 1
    epochs = f" x {profile.eval_config.epochs}" if eval_epochs > 1 else ""
    samples = f"{profile.samples // eval_epochs:,}{epochs} sample{'s' if profile.samples != 1 else ''}"
    title = f"{registry_unqualified_name(profile.name)} ({samples})"
    if show_model:
        title = f"{title}: {profile.model}"
    return title
def task_targets(profile: TaskProfile) -> str:
    return f"dataset: {profile.dataset}"

================
File: src/inspect_ai/_display/core/progress.py
================
from typing import Callable
import rich
from rich.progress import (
    BarColumn,
    TaskProgressColumn,
    TextColumn,
    TimeElapsedColumn,
)
from rich.progress import Progress as RProgress
from rich.text import Text
from typing_extensions import override
from inspect_ai._util.registry import registry_unqualified_name
from inspect_ai.model._model import ModelName
from .display import Progress, TaskCancelled, TaskError, TaskProfile, TaskResult
from .rich import is_vscode_notebook, rich_theme
# Note that use of rich progress seems to result in an extra
# empty cell after execution, see: https://github.com/Textualize/rich/issues/3274
PROGRESS_TOTAL = 102
class RichProgress(Progress):
    def __init__(
        self,
        total: int,
        progress: RProgress,
        description: str = "",
        model: str = "",
        status: Callable[[], str] | None = None,
        on_update: Callable[[], None] | None = None,
        count: str = "",
        score: str = "",
    ) -> None:
        self.total = total
        self.progress = progress
        self.status = status if status else lambda: ""
        self.on_update = on_update
        self.task_id = progress.add_task(
            description,
            total=PROGRESS_TOTAL,
            model=model,
            status=self.status(),
            count=count,
            score=score,
        )
    @override
    def update(self, n: int = 1) -> None:
        advance = (float(n) / float(self.total)) * 100
        self.progress.update(
            task_id=self.task_id, advance=advance, refresh=True, status=self.status()
        )
        if self.on_update:
            self.on_update()
    @override
    def complete(self) -> None:
        self.progress.update(
            task_id=self.task_id, completed=PROGRESS_TOTAL, status=self.status()
        )
    def update_count(self, complete: int, total: int) -> None:
        self.progress.update(
            task_id=self.task_id, count=progress_count(complete, total), refresh=True
        )
        if self.on_update:
            self.on_update()
    def update_score(self, score: str) -> None:
        self.progress.update(task_id=self.task_id, score=score)
def rich_progress() -> RProgress:
    console = rich.get_console()
    return RProgress(
        TextColumn("{task.fields[status]}"),
        TextColumn("{task.description}"),
        TextColumn("{task.fields[model]}"),
        BarColumn(bar_width=40 if is_vscode_notebook(console) else None),
        TaskProgressColumn(),
        TextColumn("{task.fields[count]}"),
        TextColumn("{task.fields[score]}"),
        TimeElapsedColumn(),
        transient=True,
        console=console,
        expand=True,
    )
MAX_MODEL_NAME_WIDTH = 25
MAX_DESCRIPTION_WIDTH = 25
def progress_model_name(
    model_name: ModelName, max_width: int = MAX_MODEL_NAME_WIDTH, pad: bool = False
) -> Text:
    model = Text(str(model_name))
    model.truncate(max_width, overflow="ellipsis", pad=pad)
    return model
def progress_description(
    profile: TaskProfile, max_width: int = MAX_DESCRIPTION_WIDTH, pad: bool = False
) -> Text:
    description = Text(registry_unqualified_name(profile.name))
    description.truncate(max_width, overflow="ellipsis", pad=pad)
    return description
def progress_status_icon(result: TaskResult | None) -> str:
    theme = rich_theme()
    if result:
        if isinstance(result, TaskError):
            return f"[{theme.error}][{theme.error}]"
        elif isinstance(result, TaskCancelled):
            return f"[{theme.error}][{theme.error}]"
        else:
            return f"[{theme.success}][{theme.success}]"
    else:
        return f"[{theme.meta}][{theme.meta}]"
def progress_count(complete: int, total: int, width: int | None = None) -> str:
    # Pad the display to keep it stable as the
    # complete metrics
    total_str = f"{total:,}"
    complete_str = f"{complete:,}"
    padding = max(0, len(total_str) - len(complete_str))
    padded = " " * padding + f"{complete_str}/{total_str}"
    # If a width has ben specified, pad up to this width as well
    if width is not None:
        padded = padded.rjust(width)
    return padded

================
File: src/inspect_ai/_display/core/results.py
================
from datetime import datetime
from typing import Sequence, Set
import numpy as np
from rich.console import Group, RenderableType
from rich.table import Table
from rich.text import Text
from inspect_ai.log import EvalStats
from inspect_ai.log._log import EvalScore, rich_traceback
from .config import task_config, task_dict
from .display import (
    TaskCancelled,
    TaskDisplayMetric,
    TaskError,
    TaskProfile,
    TaskSuccess,
    TaskWithResult,
)
from .panel import task_panel, task_targets
from .rich import rich_theme
def tasks_results(tasks: Sequence[TaskWithResult]) -> RenderableType:
    def render_task(task: TaskWithResult) -> RenderableType:
        if isinstance(task.result, TaskCancelled):
            return task_result_cancelled(task.profile, task.result)
        elif isinstance(task.result, TaskError):
            return task_result_error(task.profile, task.result)
        elif isinstance(task.result, TaskSuccess):
            return task_result_summary(task.profile, task.result)
        else:
            return ""
    return Group(*[render_task(task) for task in tasks])
def task_result_cancelled(
    profile: TaskProfile, cancelled: TaskCancelled
) -> RenderableType:
    # The contents of the panel
    config = task_config(profile)
    targets = task_targets(profile)
    subtitle = config, targets
    body = task_stats(cancelled.stats)
    # The panel
    return task_panel(
        profile=profile,
        show_model=True,
        body=body,
        subtitle=subtitle,
        footer=task_interrupted(profile, cancelled.samples_completed),
        log_location=profile.log_location,
    )
def task_results(profile: TaskProfile, success: TaskSuccess) -> RenderableType:
    theme = rich_theme()
    # do we have more than one scorer name?
    message = task_metrics(success.results.scores)
    # note if some of our samples had errors
    if success.samples_completed < profile.samples:
        sample_errors = profile.samples - success.samples_completed
        sample_error_pct = int(float(sample_errors) / float(profile.samples) * 100)
        if message:
            message = f"{message}\n\n"
        message = f"{message}[{theme.warning}]WARNING: {sample_errors} of {profile.samples} samples ({sample_error_pct}%) had errors and were not scored.[/{theme.warning}]"
    return message
def task_result_summary(profile: TaskProfile, success: TaskSuccess) -> RenderableType:
    # The contents of the panel
    config = task_config(profile)
    targets = task_targets(profile)
    subtitle = config, targets
    body = task_stats(success.stats)
    # the panel
    return task_panel(
        profile=profile,
        show_model=True,
        body=body,
        subtitle=subtitle,
        footer=task_results(profile, success),
        log_location=profile.log_location,
    )
def task_result_error(profile: TaskProfile, error: TaskError) -> RenderableType:
    return task_panel(
        profile=profile,
        show_model=True,
        body=rich_traceback(error.exc_type, error.exc_value, error.traceback),
        subtitle=None,
        footer=task_interrupted(profile, error.samples_completed),
        log_location=profile.log_location,
    )
def task_stats(stats: EvalStats) -> RenderableType:
    theme = rich_theme()
    panel = Table.grid(expand=True)
    panel.add_column()
    if len(stats.model_usage) < 2:
        panel.add_row()
    table = Table.grid(expand=True)
    table.add_column(style="bold")
    table.add_column()
    # eval time
    started = datetime.fromisoformat(stats.started_at)
    completed = datetime.fromisoformat(stats.completed_at)
    elapsed = completed - started
    table.add_row(Text("total time:", style="bold"), f"  {elapsed}", style=theme.light)
    # token usage
    for model, usage in stats.model_usage.items():
        if (
            usage.input_tokens_cache_read is not None
            or usage.input_tokens_cache_write is not None
        ):
            input_tokens_cache_read = usage.input_tokens_cache_read or 0
            input_tokens_cache_write = usage.input_tokens_cache_write or 0
            input_tokens = f"[bold]I: [/bold]{usage.input_tokens:,}, [bold]CW: [/bold]{input_tokens_cache_write:,}, [bold]CR: [/bold]{input_tokens_cache_read:,}"
        else:
            input_tokens = f"[bold]I: [/bold]{usage.input_tokens:,}"
        table.add_row(
            Text(model, style="bold"),
            f"  {usage.total_tokens:,} tokens [{input_tokens}, [bold]O: [/bold]{usage.output_tokens:,}]",
            style=theme.light,
        )
    panel.add_row(table)
    return panel
def task_can_retry(profile: TaskProfile) -> bool:
    return profile.file is not None or "/" in profile.name
def task_interrupted(profile: TaskProfile, samples_completed: int) -> RenderableType:
    log_location = profile.log_location
    theme = rich_theme()
    message = f"[bold][{theme.error}]Task interrupted ("
    if samples_completed > 0:
        message = f"{message}{samples_completed:,} of {profile.samples:,} total samples logged before interruption)."
        if task_can_retry(profile):
            message = (
                f"{message} Resume task with:[/{theme.error}][/bold]\n\n"
                + f"[bold][{theme.light}]inspect eval-retry {log_location}[/{theme.light}][/bold]"
            )
        else:
            message = f"{message}[/{theme.error}][/bold]"
    else:
        message = (
            f"{message}no samples completed before interruption)[/{theme.error}][/bold]"
        )
    return message
def task_metric(metrics: list[TaskDisplayMetric], width: int | None = None) -> str:
    reducer_names: Set[str] = {
        metric.reducer for metric in metrics if metric.reducer is not None
    }
    show_reducer = len(reducer_names) > 1 or (
        len(reducer_names) == 1 and "avg" not in reducer_names
    )
    metric = metrics[0]
    if np.isnan(metric.value):
        value = " n/a"
    else:
        value = f"{metric.value:.2f}"
    if show_reducer and metric.reducer is not None:
        metric_str = f"{metric.name}/{metric.reducer}: {value}"
    else:
        metric_str = f"{metric.name}: {value}"
    if width is not None:
        metric_str = metric_str.rjust(width)
    return metric_str
def task_metrics(scores: list[EvalScore]) -> str:
    theme = rich_theme()
    scorer_names: Set[str] = {score.name for score in scores}
    reducer_names: Set[str] = {
        score.reducer for score in scores if score.reducer is not None
    }
    show_reducer = len(reducer_names) > 1 or "avg" not in reducer_names
    output: dict[str, str] = {}
    for score in scores:
        for name, metric in score.metrics.items():
            value = (
                "1.0"
                if metric.value == 1
                else (
                    str(metric.value)
                    if isinstance(metric.value, int)
                    else f"{metric.value:.3g}"
                )
            )
            name = (
                rf"{name}\[{score.reducer}]"
                if show_reducer and score.reducer is not None
                else name
            )
            key = f"{score.name}/{name}" if (len(scorer_names) > 1) else name
            output[key] = value
    if output:
        return f"[{theme.metric}]{task_dict(output, True)}[/{theme.metric}]"
    else:
        return ""

================
File: src/inspect_ai/_display/core/rich.py
================
from contextlib import contextmanager
from dataclasses import dataclass
from typing import Any, Iterator
import rich
from rich.console import Console, ConsoleOptions, RenderResult
from rich.markdown import CodeBlock, Markdown
from rich.segment import Segment
from rich.syntax import Syntax
from typing_extensions import override
from inspect_ai._util.platform import is_running_in_jupyterlab, is_running_in_vscode
from inspect_ai._util.transcript import transcript_code_theme
from inspect_ai.util._display import display_type
def is_vscode_notebook(console: Console) -> bool:
    return console.is_jupyter and is_running_in_vscode()
def rich_no_color() -> bool:
    return (
        display_type() == "plain"
        or not is_running_in_vscode()
        or is_running_in_jupyterlab()
    )
def rich_initialise() -> None:
    # reflect ansi prefs
    if display_type() == "plain":
        rich.reconfigure(no_color=True, force_terminal=False, force_interactive=False)
    elif rich_no_color():
        rich.reconfigure(no_color=True)
    # reflect display == none
    if display_type() == "none":
        rich.reconfigure(quiet=True)
    # consistent markdown code bock background
    class CustomCodeBlock(CodeBlock):
        @override
        def __rich_console__(
            self, console: Console, options: ConsoleOptions
        ) -> RenderResult:
            code = str(self.text).rstrip()
            syntax = Syntax(
                code,
                self.lexer_name,
                theme=transcript_code_theme(),
                word_wrap=True,
                background_color="#282c34",
                padding=0,
            )
            yield syntax
    Markdown.elements["fence"] = CustomCodeBlock
    Markdown.elements["code_block"] = CustomCodeBlock
@dataclass
class RichTheme:
    meta: str = "blue"
    light: str = "bright_black"
    metric: str = "green"
    link: str = "blue"
    success: str = "green"
    error: str = "red"
    warning: str = "orange3"
def rich_theme() -> RichTheme:
    global _theme
    if _theme is None:
        _theme = RichTheme()
    return _theme
_theme: RichTheme | None = None
@contextmanager
def record_console_input() -> Iterator[None]:
    # monkey patch .input method to record inputs
    input_original = Console.input
    def input_with_record(self: Console, *args: Any, **kwargs: Any) -> str:
        result = input_original(self, *args, **kwargs)
        if self.record:
            with self._record_buffer_lock:
                self._record_buffer.append(Segment(result))
        return result
    Console.input = input_with_record  # type: ignore
    try:
        yield
    finally:
        Console.input = input_original  # type: ignore

================
File: src/inspect_ai/_display/core/textual.py
================
from logging import getLogger
from textual.driver import Driver
logger = getLogger(__name__)
# force mouse support for textual -- this works around an issue where
# mouse events are disabled after a reload of the vs code ide, see:
#   https://github.com/Textualize/textual/issues/5380
# ansi codes for enabling mouse support are idempotent so it is fine
# to do this even in cases where mouse support is already enabled.
# we try/catch since we aren't 100% sure there aren't cases where doing
# this won't raise and we'd rather not fail hard in in these case
def textual_enable_mouse_support(driver: Driver) -> None:
    enable_mouse_support = getattr(driver, "_enable_mouse_support", None)
    if enable_mouse_support:
        try:
            enable_mouse_support()
        except Exception as ex:
            logger.warning(f"Error enabling mouse support: {ex}")

================
File: src/inspect_ai/_display/plain/display.py
================
import contextlib
from typing import AsyncIterator, Callable, Coroutine, Iterator
import anyio
import rich
from inspect_ai._display.core.rich import rich_initialise
from inspect_ai._util._async import configured_async_backend, run_coroutine
from inspect_ai._util.platform import running_in_notebook
from inspect_ai._util.text import truncate
from inspect_ai._util.throttle import throttle
from ...util._concurrency import concurrency_status
from ..core.config import task_config
from ..core.display import (
    TR,
    Display,
    Progress,
    TaskDisplay,
    TaskDisplayMetric,
    TaskProfile,
    TaskResult,
    TaskScreen,
    TaskSpec,
    TaskWithResult,
)
from ..core.footer import task_http_retries_str
from ..core.panel import task_panel, task_targets
from ..core.results import task_metric, tasks_results
class PlainDisplay(Display):
    def __init__(self) -> None:
        self.total_tasks: int = 0
        self.tasks: list[TaskWithResult] = []
        self.parallel = False
        rich_initialise()
    def print(self, message: str) -> None:
        print(message)
    @contextlib.contextmanager
    def progress(self, total: int) -> Iterator[Progress]:
        yield PlainProgress(total)
    def run_task_app(self, main: Callable[[], Coroutine[None, None, TR]]) -> TR:
        if running_in_notebook():
            return run_coroutine(main())
        else:
            return anyio.run(main, backend=configured_async_backend())
    @contextlib.contextmanager
    def suspend_task_app(self) -> Iterator[None]:
        yield
    @contextlib.asynccontextmanager
    async def task_screen(
        self, tasks: list[TaskSpec], parallel: bool
    ) -> AsyncIterator[TaskScreen]:
        self.total_tasks = len(tasks)
        self.multiple_task_names = len({task.name for task in tasks}) > 1
        self.multiple_model_names = len({str(task.model) for task in tasks}) > 1
        self.tasks = []
        self.parallel = parallel
        try:
            # Print header for task(s)
            if parallel:
                print(f"Running {self.total_tasks} tasks...")
            yield TaskScreen()
        finally:
            # Print final results
            if self.tasks:
                self._print_results()
    @contextlib.contextmanager
    def task(self, profile: TaskProfile) -> Iterator[TaskDisplay]:
        # Print initial task information using a rich panel
        panel = task_panel(
            profile=profile,
            show_model=True,
            body="",  # Empty body since we haven't started yet
            subtitle=(task_config(profile), task_targets(profile)),
            footer=None,
            log_location=None,
        )
        rich.print(panel)
        # Create and yield task display
        task = TaskWithResult(profile, None)
        self.tasks.append(task)
        yield PlainTaskDisplay(
            task,
            show_task_names=self.multiple_task_names,
            show_model_names=self.multiple_model_names,
        )
    def display_counter(self, caption: str, value: str) -> None:
        # Not supported for plain display as counters are only shown for tasks.
        pass
    def _print_results(self) -> None:
        """Print final results using rich panels"""
        panels = tasks_results(self.tasks)
        rich.print(panels)
class PlainProgress(Progress):
    def __init__(self, total: int):
        self.total = total
        self.current = 0
    def update(self, n: int = 1) -> None:
        self.current += n
        # No direct printing - PlainTaskDisplay handles it
    def complete(self) -> None:
        self.current = self.total
class PlainTaskDisplay(TaskDisplay):
    def __init__(
        self, task: TaskWithResult, *, show_task_names: bool, show_model_names: bool
    ):
        self.task = task
        self.show_task_names = show_task_names
        self.show_model_names = show_model_names
        self.progress_display: PlainProgress | None = None
        self.samples_complete = 0
        self.samples_total = 0
        self.current_metrics: list[TaskDisplayMetric] | None = None
        self.last_progress = 0
    @contextlib.contextmanager
    def progress(self) -> Iterator[Progress]:
        self.progress_display = PlainProgress(self.task.profile.steps)
        yield self.progress_display
    @throttle(5)
    def _print_status_throttled(self) -> None:
        self._print_status()
    def _print_status(self) -> None:
        """Print status updates on new lines when there's meaningful progress"""
        if not self.progress_display:
            return
        # Only print when step count changes to avoid too much output
        if self.progress_display.current != self.last_progress:
            status_parts: list[str] = []
            # if this is parallel print task and model to distinguish (limit both to 12 chars)
            MAX_NAME_WIDTH = 12
            if self.show_task_names:
                status_parts.append(truncate(self.task.profile.name, MAX_NAME_WIDTH))
            if self.show_model_names:
                status_parts.append(
                    truncate(str(self.task.profile.model), MAX_NAME_WIDTH)
                )
            # Add step progress
            progress_percent = int(
                self.progress_display.current / self.progress_display.total * 100
            )
            status_parts.append(
                f"Steps: {self.progress_display.current:3d}/{self.progress_display.total} {progress_percent:3d}%"
            )
            # Add sample progress
            status_parts.append(
                f"Samples: {self.samples_complete:3d}/{self.samples_total:3d}"
            )
            # Add metrics
            if self.current_metrics:
                metric_str = task_metric(self.current_metrics)
                status_parts.append(metric_str)
            # Add resource usage
            # Very similar to ``inspect_ai._display.core.footer.task_resources``, but without
            # the rich formatting added in the ``task_dict`` call
            resources_dict: dict[str, str] = {}
            for model, resource in concurrency_status().items():
                resources_dict[model] = f"{resource[0]:2d}/{resource[1]:2d}"
            resources = ", ".join(
                [f"{key}: {value}" for key, value in resources_dict.items()]
            )
            status_parts.append(resources)
            # Add rate limits
            rate_limits = task_http_retries_str()
            if rate_limits:
                status_parts.append(rate_limits)
            # Print on new line
            print(" | ".join(status_parts))
            self.last_progress = self.progress_display.current
    def sample_complete(self, complete: int, total: int) -> None:
        self.samples_complete = complete
        self.samples_total = total
        self._print_status_throttled()
    def update_metrics(self, metrics: list[TaskDisplayMetric]) -> None:
        self.current_metrics = metrics
        self._print_status_throttled()
    def complete(self, result: TaskResult) -> None:
        self.task.result = result
        self._print_status()

================
File: src/inspect_ai/_display/rich/display.py
================
import contextlib
from dataclasses import dataclass
from typing import Any, AsyncIterator, Callable, Coroutine, Iterator
import anyio
import rich
from rich.console import Console, Group, RenderableType
from rich.live import Live
from rich.panel import Panel
from rich.progress import Progress as RProgress
from rich.table import Table
from typing_extensions import override
from inspect_ai._util._async import configured_async_backend, run_coroutine
from inspect_ai._util.constants import CONSOLE_DISPLAY_WIDTH
from inspect_ai._util.platform import running_in_notebook
from inspect_ai.log._transcript import InputEvent, transcript
from inspect_ai.util._display import display_type
from inspect_ai.util._throttle import throttle
from ..core.config import task_config
from ..core.display import (
    TR,
    Display,
    Progress,
    TaskDisplay,
    TaskDisplayMetric,
    TaskProfile,
    TaskResult,
    TaskScreen,
    TaskSpec,
    TaskWithResult,
)
from ..core.footer import task_footer
from ..core.panel import task_panel, task_targets, task_title, tasks_title
from ..core.progress import (
    RichProgress,
    progress_description,
    progress_model_name,
    progress_status_icon,
    rich_progress,
)
from ..core.results import task_metric, tasks_results
from ..core.rich import (
    is_vscode_notebook,
    record_console_input,
    rich_initialise,
    rich_theme,
)
@dataclass
class TaskStatus(TaskWithResult):
    progress: RProgress
class RichDisplay(Display):
    def __init__(self) -> None:
        self.total_tasks: int = 0
        self.tasks: list[TaskStatus] = []
        self.progress_ui: RProgress | None = None
        self.parallel = False
        self.live: Live | None = None
        self.counters: dict[str, str] = {}
        rich_initialise()
    @override
    def print(self, message: str) -> None:
        rich.get_console().print(message, markup=False, highlight=False)
    @override
    @contextlib.contextmanager
    def progress(self, total: int) -> Iterator[Progress]:
        with rich_progress() as progress:
            yield RichProgress(total, progress)
    @override
    def run_task_app(self, main: Callable[[], Coroutine[None, None, TR]]) -> TR:
        if running_in_notebook():
            return run_coroutine(main())
        else:
            return anyio.run(main, backend=configured_async_backend())
    @override
    @contextlib.contextmanager
    def suspend_task_app(self) -> Iterator[None]:
        yield
    @override
    @contextlib.asynccontextmanager
    async def task_screen(
        self, tasks: list[TaskSpec], parallel: bool
    ) -> AsyncIterator[TaskScreen]:
        self.total_tasks = len(tasks)
        self.tasks = []
        self.progress_ui = rich_progress()
        self.parallel = parallel
        try:
            with (
                Live(
                    None,
                    console=rich.get_console(),
                    transient=True,
                    auto_refresh=False,
                ) as live,
            ):
                # save reference to live
                with RichTaskScreen(live) as task_screen:
                    self.live = live
                    async with anyio.create_task_group() as tg:
                        # update display every second while running
                        tg.start_soon(self._update_display_loop)
                        # let the task screen run
                        try:
                            yield task_screen
                        finally:
                            tg.cancel_scope.cancel()
                # render task results (re-enable live if necessary)
                if not live.is_started:
                    live.start()
                live.transient = False
                live.update(tasks_results(self.tasks), refresh=True)
        finally:
            # clear tasks and progress
            self.total_tasks = 0
            self.tasks = []
            self.progress_ui = None
            self.parallel = False
            self.live = None
    @override
    @contextlib.contextmanager
    def task(self, profile: TaskProfile) -> Iterator[TaskDisplay]:
        # for typechekcer
        if self.tasks is None:
            self.tasks = []
        if self.progress_ui is None:
            self.progress_ui = rich_progress()
        status = TaskStatus(profile, None, self.progress_ui)
        self.tasks.append(status)
        self._update_display()
        yield RichTaskDisplay(
            status, show_name=self.parallel, on_update=self._update_display
        )
    @throttle(1)
    def _update_display(self) -> None:
        if (
            display_type() != "conversation"
            and self.tasks is not None
            and self.tasks
            and self.progress_ui is not None
            and self.live is not None
            and self.live.is_started
        ):
            if self.parallel:
                r = tasks_live_status(
                    self.total_tasks, self.tasks, self.progress_ui, self.counters
                )
            else:
                r = task_live_status(self.tasks, self.progress_ui, self.counters)
            self.live.update(r, refresh=True)
    async def _update_display_loop(self) -> None:
        try:
            while True:
                await anyio.sleep(1)
                self._update_display()
        except Exception:
            pass
    @override
    def display_counter(self, caption: str, value: str) -> None:
        self.counters[caption] = value
        self._update_display()
class RichTaskScreen(TaskScreen):
    def __init__(self, live: Live) -> None:
        self.theme = rich_theme()
        self.live = live
        status_text = "Working" if display_type() == "conversation" else "Task running"
        self.status = self.live.console.status(
            f"[{self.theme.meta} bold]{status_text}...[/{self.theme.meta} bold]",
            spinner="clock",
        )
    def __exit__(self, *excinfo: Any) -> None:
        self.status.stop()
    @override
    @contextlib.contextmanager
    def input_screen(
        self,
        header: str | None = None,
        transient: bool | None = None,
        width: int | None = None,
    ) -> Iterator[Console]:
        # determine transient based on trace mode
        if transient is None:
            transient = display_type() != "conversation"
        # clear live task status and transient status
        self.live.update("", refresh=True)
        self.status.stop()
        # show cursor for input
        self.live.console.show_cursor(True)
        # set width
        old_width: int | None = None
        if width:
            old_width = self.live.console.width
            self.live.console.width = min(old_width, width)
        # record console activity for event
        self.live.console.record = True
        try:
            # print header if requested
            if header:
                style = f"{rich_theme().meta} bold"
                self.live.console.rule(f"[{style}]{header}[/{style}]", style="black")
                self.live.console.print("")
            # yield the console
            with record_console_input():
                yield self.live.console
        finally:
            # capture recording then yield input event
            input = self.live.console.export_text(clear=False, styles=False)
            input_ansi = self.live.console.export_text(clear=True, styles=True)
            self.live.console.record = False
            transcript()._event(InputEvent(input=input, input_ansi=input_ansi))
            # print one blank line
            self.live.console.print("")
            # reset width
            if old_width:
                self.live.console.width = old_width
            # disable cursor while not collecting input
            self.live.console.show_cursor(False)
            # if transient then disable live updates entirely
            if transient is False and self.live.is_started:
                self.live.stop()
            # otherwise make sure they are enabled
            elif transient is True and not self.live.is_started:
                self.live.start()
            # if not transient then display mini-status
            if not transient:
                self.status.start()
class RichTaskDisplay(TaskDisplay):
    def __init__(
        self,
        status: TaskStatus,
        show_name: bool,
        on_update: Callable[[], None] | None = None,
    ) -> None:
        self.status = status
        model = progress_model_name(self.status.profile.model)
        description = progress_description(self.status.profile)
        def task_status() -> str:
            return progress_status_icon(self.status.result)
        self.p = RichProgress(
            total=self.status.profile.steps,
            progress=self.status.progress,
            description=f"{description.markup}",
            model=f"{model.markup} ",
            status=task_status,
            on_update=on_update,
        )
    @override
    @contextlib.contextmanager
    def progress(self) -> Iterator[Progress]:
        yield self.p
    @override
    def sample_complete(self, complete: int, total: int) -> None:
        self.p.update_count(complete, total)
    @override
    def update_metrics(self, metrics: list[TaskDisplayMetric]) -> None:
        if len(metrics) > 0:
            self.p.update_score(task_metric(metrics))
    @override
    def complete(self, result: TaskResult) -> None:
        self.status.result = result
        self.p.complete()
def task_live_status(
    tasks: list[TaskStatus], progress: RProgress, counters: dict[str, str]
) -> RenderableType:
    theme = rich_theme()
    # the panel contents
    config = task_config(tasks[0].profile, style=theme.light)
    targets = task_targets(tasks[0].profile)
    subtitle = config, targets
    # the panel
    return task_panel(
        profile=tasks[0].profile,
        show_model=len(tasks) == 1,
        body=Group("", progress),
        subtitle=subtitle,
        footer=task_footer(counters, theme.light),
        log_location=None,
    )
def tasks_live_status(
    total_tasks: int,
    tasks: list[TaskStatus],
    progress: RProgress,
    counters: dict[str, str],
) -> RenderableType:
    # rendering context
    theme = rich_theme()
    console = rich.get_console()
    width = CONSOLE_DISPLAY_WIDTH if is_vscode_notebook(console) else None
    # compute completed tasks
    completed = sum(1 for task in tasks if task.result is not None)
    # get config
    config = task_config(tasks[0].profile, generate_config=False, style=theme.light)
    if config:
        config += "\n"
    # build footer table
    footer_table = Table.grid(expand=True)
    footer_table.add_column()
    footer_table.add_column(justify="right")
    footer = task_footer(counters, theme.light)
    footer_table.add_row()
    footer_table.add_row(footer[0], footer[1])
    # build a layout table
    layout_table = Table.grid(expand=True)
    layout_table.add_column()
    layout_table.add_row(config)
    layout_table.add_row(progress)
    layout_table.add_row(footer_table)
    # create panel w/ title
    panel = Panel(
        layout_table,
        title=f"[bold][{theme.meta}]{tasks_title(completed, total_tasks)}[/{theme.meta}][/bold]",
        title_align="left",
        width=width,
        expand=True,
    )
    return panel
def task_no_ansi(profile: TaskProfile) -> str:
    theme = rich_theme()
    message = f"Running task {task_title(profile, True)}"
    config = task_config(profile, style=theme.light)
    if config:
        message = f"{message} (config: {config})"
    return f"{message}...\n"

================
File: src/inspect_ai/_display/textual/app.py
================
import contextlib
from asyncio import CancelledError
from typing import (
    Any,
    AsyncIterator,
    Awaitable,
    Callable,
    ClassVar,
    Generic,
    Iterator,
    cast,
)
import anyio
import anyio.from_thread
import rich
from rich.console import Console
from textual.app import App, ComposeResult
from textual.binding import Binding, BindingType
from textual.css.query import NoMatches
from textual.events import Print
from textual.widget import Widget
from textual.widgets import TabbedContent, TabPane
from textual.widgets.tabbed_content import ContentTabs
from textual.worker import Worker, WorkerState
from typing_extensions import override
from inspect_ai._display.core.textual import textual_enable_mouse_support
from inspect_ai._util.html import as_html_id
from inspect_ai.log._samples import active_samples
from inspect_ai.log._transcript import InputEvent, transcript
from ...util._panel import InputPanel
from ..core.config import task_config
from ..core.display import (
    TP,
    TR,
    TaskDisplay,
    TaskProfile,
    TaskScreen,
    TaskSpec,
    TaskWithResult,
)
from ..core.footer import task_footer
from ..core.panel import task_targets, task_title, tasks_title
from ..core.rich import record_console_input, rich_initialise, rich_theme
from .theme import inspect_dark, inspect_light
from .widgets.console import ConsoleView
from .widgets.footer import AppFooter
from .widgets.samples import SamplesView
from .widgets.tasks import TasksView
from .widgets.titlebar import AppTitlebar
class TaskScreenResult(Generic[TR]):
    def __init__(
        self,
        value: TR | BaseException,
        tasks: list[TaskWithResult],
        output: list[str],
    ) -> None:
        self.value = value
        self.tasks = tasks
        self.output = output
class TaskScreenApp(App[TR]):
    CSS_PATH = "app.tcss"
    BINDINGS: ClassVar[list[BindingType]] = [
        Binding(
            "ctrl+c",
            "quit",
            "Interrupt",
            tooltip="Interrupt the app and return to the command prompt.",
            show=False,
            priority=True,
        )
    ]
    def __init__(self) -> None:
        # call super
        super().__init__()
        # worker and output
        self._worker: Worker[TR] | None = None
        self._error: BaseException | None = None
        self._output: list[str] = []
        # task screen
        self._total_tasks = 0
        self._parallel = False
        self._tasks: list[TaskWithResult] = []
        self._counters: dict[str, str] = {}
        # all tasks processed by app
        self._app_tasks: list[TaskWithResult] = []
        # enable rich hooks
        rich_initialise()
    def _watch_app_focus(self, focus: bool) -> None:
        super()._watch_app_focus(focus)
        if focus and self.app._driver:
            textual_enable_mouse_support(self.app._driver)
    def run_app(self, main: Callable[[], Awaitable[TR]]) -> TaskScreenResult[TR]:
        self._worker = self.run_worker(main(), start=False, exit_on_error=False)
        # run the app
        self.run()
        # determine result value
        if self.return_value is not None:
            value: TR | BaseException = self.return_value
        elif self._error is not None:
            value = self._error
        else:
            value = CancelledError()
        # return result w/ output
        return TaskScreenResult(value=value, tasks=self._app_tasks, output=self._output)
    async def on_load(self) -> None:
        # events used to synchronise loading
        self._on_load_app = anyio.Event()
        self._on_app_loaded = anyio.Event()
        # run the workers
        self.workers.start_all()
        # wait until we are given the signal to load
        # if the worker completes in the meantime then there was an error during
        # initialisation, in that case return early, which will enable delivery of
        # the worker error event and standard exit control flow
        while not self._on_load_app.is_set():
            if len(self.workers._workers) == 0:
                return
            await anyio.sleep(0.1)
    @contextlib.contextmanager
    def suspend_app(self) -> Iterator[None]:
        # suspend only if the app is already loaded
        # (otherwise its not yet displayed )
        if self._on_app_loaded.is_set():
            with self.app.suspend():
                try:
                    yield
                finally:
                    self.app.refresh(repaint=True)
        else:
            yield
    # exit the app when the worker terminates
    def on_worker_state_changed(self, event: Worker.StateChanged) -> None:
        if event.worker.state == WorkerState.ERROR:
            self._error = event.worker.error
            self.exit(None, 1)
        elif event.worker.state == WorkerState.CANCELLED:
            self._error = CancelledError()
            self.exit(None, 1)
        elif event.worker.state == WorkerState.SUCCESS:
            self.exit(event.worker.result)
    # notification that a new top level set of tasks are being run
    @contextlib.asynccontextmanager
    async def task_screen(
        self, tasks: list[TaskSpec], parallel: bool
    ) -> AsyncIterator[TaskScreen]:
        # indicate its time to load then wait on the load
        self._on_load_app.set()
        await self._on_app_loaded.wait()
        # reset state
        self._tasks = []
        self._total_tasks = len(tasks)
        self._parallel = parallel
        # clear existing task progress
        tasks_view = self.query_one(TasksView)
        tasks_view.init_tasks(tasks)
        # update display
        self.update_display()
        # force repaint
        self.refresh(repaint=True)
        # enable mouse support (this broke in textual 2.0 when running in VS Code
        # however is fixed in textual 2.1)
        assert self.app._driver
        textual_enable_mouse_support(self.app._driver)
        try:
            yield TextualTaskScreen(self)
        finally:
            self._tasks = []
            self._total_tasks = 0
            self._parallel = False
    # notification that a task is running and requires display
    @contextlib.contextmanager
    def task_display(self, profile: TaskProfile) -> Iterator[TaskDisplay]:
        # create and track task
        task = TaskWithResult(profile, None)
        self._app_tasks.append(task)
        self._tasks.append(task)
        # update display
        self.update_display()
        # add task
        try:
            task_view = self.query_one(TasksView)
            task_view.set_display_metrics(
                profile.eval_config.score_display is not False
            )
            yield task_view.add_task(task)
        finally:
            pass
    # compose use
    def compose(self) -> ComposeResult:
        yield AppTitlebar()
        yield AppFooter()
        with TabbedContent(id="tabs", initial="tasks"):
            with TabPane("Tasks", id="tasks"):
                yield TasksView()
            with TabPane("Running Samples", id="samples"):
                yield SamplesView()
            with TabPane("Console", id="console"):
                yield ConsoleView()
    def on_mount(self) -> None:
        # register and set theme
        self.register_theme(inspect_dark)
        self.register_theme(inspect_light)
        self.theme = "inspect-dark"
        # capture stdout/stderr (works w/ on_print)
        self.begin_capture_print(self)
        # handle tab activations
        self.handle_tab_activations()
        # handle console unread
        self.handle_console_unread()
        # update display every second
        self.set_interval(1, self.update_display)
        # indicate that the app is loaded
        self._on_app_loaded.set()
    # update dynamic parts of display
    def update_display(self) -> None:
        self.update_title()
        self.update_tasks()
        self.update_samples()
        self.update_footer()
        for input_panel in self.query(f".{InputPanel.DEFAULT_CLASSES}"):
            cast(InputPanel, input_panel).update()
    # update the header title
    def update_title(self) -> None:
        # determine title
        if self._worker and self._worker.is_cancelled:
            title = "eval interrupted (cancelling running samples...)"
        elif len(self._tasks) > 0:
            if self._parallel:
                completed = sum(1 for task in self._tasks if task.result is not None)
                title = f"{tasks_title(completed, self._total_tasks)}"
            else:
                title = f"{task_title(self._tasks[0].profile, show_model=len(self._tasks) == 1)}"
        else:
            title = ""
        # set if required
        header = self.query_one(AppTitlebar)
        if header.title != title:
            header.title = title
    def update_tasks(self) -> None:
        tasks = self.query_one(TasksView)
        if len(self._tasks) > 0:
            tasks.config = task_config(
                self._tasks[0].profile, generate_config=not self._parallel
            )
            if not self._parallel:
                tasks.targets = task_targets(self._tasks[0].profile)
            else:
                tasks.targets = " \n "
        else:
            tasks.config = ""
            tasks.targets = ""
    def update_samples(self) -> None:
        samples_view = self.query_one(SamplesView)
        active_and_started_samples = [
            sample for sample in active_samples() if sample.started is not None
        ]
        samples_view.set_samples(active_and_started_samples)
    def update_footer(self) -> None:
        left, right = task_footer(self._counters)
        footer = self.query_one(AppFooter)
        footer.left = left
        footer.right = right
    # track and display console unread state
    def handle_console_unread(self) -> None:
        # unread management
        tabs = self.query_one(TabbedContent)
        console_tab = tabs.get_tab("console")
        console_view = self.query_one(ConsoleView)
        def set_unread(unread: int | None) -> None:
            if unread is not None:
                console_tab.label = f"Console ({unread})"  # type: ignore[assignment]
            else:
                console_tab.label = "Console"  # type: ignore[assignment]
        self.watch(console_view, "unread", set_unread)
    # handle tab activations
    def handle_tab_activations(self) -> None:
        tabs = self.query_one(TabbedContent)
        console_view = self.query_one(ConsoleView)
        samples_view = self.query_one(SamplesView)
        async def set_active_tab(active: str) -> None:
            await console_view.notify_active(active == "console")
            await samples_view.notify_active(active == "samples")
        self.watch(tabs, "active", set_active_tab)
    # activate the tasks tab
    def activate_tasks_tab(self) -> None:
        tasks = self.query_one(TasksView)
        tasks.tasks.focus()  # force the tab to switch by focusing a child
        self.query_one(ContentTabs).focus()  # focus the tab control
    # capture output and route to console view and our buffer
    def on_print(self, event: Print) -> None:
        # remove trailing newline
        text = event.text
        if text.endswith("\n"):
            text = text[:-1]
        # track output (for printing at the end)
        self._output.append(text)
        # write to console view
        self.query_one(ConsoleView).write_ansi(text)
    # map ctrl+c to cancelling the worker
    @override
    async def action_quit(self) -> None:
        if self._worker and self._worker.is_running and not self._worker.is_cancelled:
            self._worker.cancel()
            self.update_title()
    # dynamic input panels
    async def add_input_panel(self, panel: InputPanel) -> None:
        tabs = self.query_one(TabbedContent)
        await tabs.add_pane(
            TabPane(panel.title, panel, id=as_input_panel_id(type(panel)))
        )
    def get_input_panel(self, panel_type: type) -> InputPanel | None:
        try:
            tab_pane = self.query_one(f"#{as_input_panel_id(panel_type)}")
            if len(tab_pane.children) > 0:
                return cast(InputPanel, tab_pane.children[0])
            else:
                return None
        except NoMatches:
            return None
    def display_counter(self, caption: str, value: str) -> None:
        self._counters[caption] = value
        self.update_footer()
    class InputPanelHost(InputPanel.Host):
        def __init__(self, app: "TaskScreenApp[TR]", tab_id: str) -> None:
            self.app = app
            self.tab_id = tab_id
        def set_title(self, title: str) -> None:
            tabs = self.app.query_one(TabbedContent)
            tab = tabs.get_tab(self.tab_id)
            tab.label = title  # type: ignore[assignment]
        def activate(self) -> None:
            # show the tab
            tabs = self.app.query_one(TabbedContent)
            tabs.show_tab(self.tab_id)
            # focus the first focuable child (this seems to be necessary
            # to get textual to reliably make the switch). after that, focus
            # the tabs control so the user can switch back w/ the keyboard
            tab_pane = self.app.query_one(f"#{self.tab_id}")
            panel = cast(InputPanel, tab_pane.children[0])
            for child in panel.walk_children(Widget):
                if child.focusable:
                    child.focus()
                    self.app.query_one(ContentTabs).focus()
                    break
        def deactivate(self) -> None:
            tabs = self.app.query_one(TabbedContent)
            if tabs.active == self.tab_id:
                self.app.activate_tasks_tab()
        def close(self) -> None:
            tabs = self.app.query_one(TabbedContent)
            tabs.remove_pane(self.tab_id)
            self.app.activate_tasks_tab()
class TextualTaskScreen(TaskScreen, Generic[TR]):
    def __init__(self, app: TaskScreenApp[TR]) -> None:
        self.app = app
        self.lock = anyio.Lock()
    def __exit__(self, *excinfo: Any) -> None:
        pass
    @override
    @contextlib.contextmanager
    def input_screen(
        self,
        header: str | None = None,
        transient: bool | None = None,
        width: int | None = None,
    ) -> Iterator[Console]:
        with self.app.suspend_app():
            # get rich console
            console = rich.get_console()
            # set width
            old_width: int | None = None
            if width:
                old_width = console.width
                console.width = min(old_width, width)
            # record console activity for event
            console.record = True
            try:
                # print header if requested
                if header:
                    style = f"{rich_theme().meta} bold"
                    console.rule(f"[{style}]{header}[/{style}]", style="black")
                    console.print("")
                # yield the console
                with record_console_input():
                    yield console
            finally:
                # capture recording then yield input event
                input = console.export_text(clear=False, styles=False)
                input_ansi = console.export_text(clear=True, styles=True)
                console.record = False
                transcript()._event(InputEvent(input=input, input_ansi=input_ansi))
                # print one blank line
                console.print("")
                # reset width
                if old_width:
                    console.width = old_width
    @override
    async def input_panel(self, panel_type: type[TP]) -> TP:
        async with self.lock:
            panel_widget = self.app.get_input_panel(panel_type)
            if panel_widget is None:
                panel_widget = panel_type(
                    TaskScreenApp[TR].InputPanelHost(
                        self.app, as_input_panel_id(panel_type)
                    ),
                )
                await self.app.add_input_panel(panel_widget)
            return cast(TP, panel_widget)
def as_input_panel_id(panel_type: type) -> str:
    return as_html_id("id-input-panel", panel_type.__name__)

================
File: src/inspect_ai/_display/textual/display.py
================
import contextlib
from typing import AsyncIterator, Callable, Coroutine, Iterator
import rich
from typing_extensions import override
from ..core.display import (
    TR,
    Display,
    Progress,
    TaskDisplay,
    TaskProfile,
    TaskScreen,
    TaskSpec,
)
from ..core.progress import RichProgress, rich_progress
from ..core.results import tasks_results
from .app import TaskScreenApp
class TextualDisplay(Display):
    @override
    def print(self, message: str) -> None:
        rich.get_console().print(message, markup=False, highlight=False)
    @override
    @contextlib.contextmanager
    def progress(self, total: int) -> Iterator[Progress]:
        with rich_progress() as progress:
            yield RichProgress(total, progress)
    @override
    def run_task_app(self, main: Callable[[], Coroutine[None, None, TR]]) -> TR:
        # create and run the app
        self.app = TaskScreenApp[TR]()
        result = self.app.run_app(main)
        # print output
        if result.output:
            print("\n".join(result.output))
        # print tasks
        rich.print(tasks_results(result.tasks))
        # raise error as required
        if isinstance(result.value, BaseException):
            raise result.value
        # success! return value
        else:
            return result.value
    @override
    @contextlib.contextmanager
    def suspend_task_app(self) -> Iterator[None]:
        if getattr(self, "app", None) and self.app.is_running:
            with self.app.suspend_app():
                yield
        else:
            yield
    @override
    @contextlib.asynccontextmanager
    async def task_screen(
        self, tasks: list[TaskSpec], parallel: bool
    ) -> AsyncIterator[TaskScreen]:
        async with self.app.task_screen(tasks, parallel) as task_screen:
            yield task_screen
    @override
    @contextlib.contextmanager
    def task(self, profile: TaskProfile) -> Iterator[TaskDisplay]:
        with self.app.task_display(profile) as task_display:
            yield task_display
    @override
    def display_counter(self, caption: str, value: str) -> None:
        self.app.display_counter(caption, value)

================
File: src/inspect_ai/_display/textual/theme.py
================
from textual.theme import Theme
inspect_dark = Theme(
    name="inspect-dark",
    primary="#3376CD",
    secondary="#004578",
    accent="#ffa62b",
    warning="#ffa62b",
    error="#ba3c5b",
    success="#408558",
    foreground="#e0e0e0",
)
inspect_light = Theme(
    name="inspect-light",
    primary="#4283CA",
    secondary="#0178D4",
    accent="#ffa62b",
    warning="#ffa62b",
    error="#ba3c5b",
    success="#54B98F",
    surface="#D8D8D8",
    panel="#DFDFDF",
    background="#F8F8F8",
    dark=False,
    variables={
        "footer-key-foreground": "#0178D4",
    },
)

================
File: src/inspect_ai/_display/textual/widgets/clock.py
================
from datetime import datetime
from textual.reactive import reactive
from textual.timer import Timer
from textual.widgets import Static
from inspect_ai._util.format import format_progress_time
class Clock(Static):
    DEFAULT_CSS = """
    Clock {
        color: $primary-lighten-3;
    }
    """
    time: reactive[float] = reactive(datetime.now().timestamp)
    timer: Timer | None = None
    def __init__(self, interval: int = 1) -> None:
        super().__init__()
        self.start_time: float | None = None
        self.time = datetime.now().timestamp()
        self.interval = interval
    def start(self, start_time: float) -> None:
        if start_time != self.start_time:
            self.stop()
            self.start_time = start_time
            self.update_time()
            self.timer = self.set_interval(self.interval, self.update_time)
    def stop(self) -> None:
        self.start_time = None
        if self.timer:
            self.timer.stop()
            self.timer = None
    def on_unmount(self) -> None:
        self.stop()
    def watch_start_time(self, start_time: float | None) -> None:
        if start_time is not None:
            if self.timer is None:
                self.timer = self.set_interval(self.interval, self.update_time)
            self.update(format_progress_time(start_time))
        else:
            self.stop()
    def update_time(self) -> None:
        if self.start_time is not None:
            self.time = datetime.now().timestamp() - self.start_time
    def watch_time(self, time: float) -> None:
        self.update(format_progress_time(time))

================
File: src/inspect_ai/_display/textual/widgets/console.py
================
from rich.text import Text
from textual.reactive import reactive
from textual.widgets import RichLog
class ConsoleView(RichLog):
    DEFAULT_CSS = """
    ConsoleView {
        scrollbar-size-horizontal: 1;
        scrollbar-size-vertical: 1;
        scrollbar-gutter: stable;
        background: transparent;
    }
    """
    # enable tab container to print our unread count
    unread: reactive[int | None] = reactive(None)
    def __init__(self) -> None:
        super().__init__()
        self.active = False
        self.show_horizontal_scrollbar = False
    async def notify_active(self, active: bool) -> None:
        self.active = active
        if self.active:
            self.unread = None
    def write_ansi(self, text: str) -> None:
        # process line by line
        for line in text.splitlines():
            self.write_ansi_line(line)
        # tick unread if we aren't active
        if not self.active and len(text.strip()) > 0:
            self.unread = (self.unread or 0) + 1
    def write_ansi_line(self, line: str) -> None:
        # tweak rich console lines with path at end to not go under the scrollbar
        # (remove two inner spaces and add a space at the end)
        if "[2m" in line:
            chars = list(line)
            removed = 0
            for i in range(len(chars) - 1, -1, -1):
                if chars[i].isspace():
                    chars.pop(i)
                    removed += 1
                    if removed > 1:
                        break
            line = "".join(chars) + " "
        self.write(Text.from_ansi(line))

================
File: src/inspect_ai/_display/textual/widgets/footer.py
================
from typing import cast
from rich.console import RenderableType
from textual.app import ComposeResult
from textual.reactive import reactive
from textual.widget import Widget
from textual.widgets import Static
class AppFooter(Widget):
    DEFAULT_CSS = """
    AppFooter {
        layout: grid;
        grid-size: 2 1;
        grid-columns: 1fr auto;
        grid-gutter: 2;
        background: $foreground 5%;
        color: $text-muted;
        dock: bottom;
        height: auto;
        padding: 0 1
    }
    """
    left: reactive[RenderableType] = reactive("")
    right: reactive[RenderableType] = reactive("")
    def compose(self) -> ComposeResult:
        yield Static(id="footer-left", markup=False)
        yield Static(id="footer-right", markup=False)
    def watch_left(self, new_left: RenderableType) -> None:
        footer_left = cast(Static, self.query_one("#footer-left"))
        footer_left.update(new_left)
    def watch_right(self, new_right: RenderableType) -> None:
        footer_right = cast(Static, self.query_one("#footer-right"))
        footer_right.update(new_right)
        if footer_right.tooltip is None:
            footer_right.tooltip = (
                "Execute 'inspect trace http' for a log of all HTTP requests."
            )

================
File: src/inspect_ai/_display/textual/widgets/port_mappings.py
================
from typing import Literal
from textual.app import ComposeResult
from textual.containers import HorizontalScroll
from textual.widget import Widget
from textual.widgets import Link, Static
from inspect_ai._util.port_names import get_service_by_port
from inspect_ai.util._sandbox.environment import PortMapping
class PortMappingsView(HorizontalScroll):
    DEFAULT_CSS = """
    PortMappingsView {
      layout: grid;
      height: auto;
      grid-size: 4 3;
      grid-columns: auto auto auto auto;
      grid-gutter: 0 1;
    }
    """
    def __init__(self, ports: list[PortMapping] | None) -> None:
        super().__init__()
        self.ports = ports
    def compose(self) -> ComposeResult:
        if not self.ports:
            return
        yield Static("service")
        yield Static("sandbox")
        yield Static("client")
        yield Static("endpoint")
        mappings_and_services = [
            (mapping, get_service_by_port(mapping.container_port, mapping.protocol))
            for mapping in self.ports
        ]
        remaining_widgets = [
            widget
            for mapping_and_service in mappings_and_services
            for widget in widgets_from_port_mapping(mapping_and_service)
        ]
        for widget in remaining_widgets:
            yield widget
def widgets_for_port_mappings(
    port_mappings: list[PortMapping] | None,
) -> list[Widget]:
    if port_mappings is None:
        return []
    return [
        static
        for mapping in [
            (mapping, get_service_by_port(mapping.container_port, mapping.protocol))
            for mapping in port_mappings
        ]
        for static in widgets_from_port_mapping(mapping)
    ]
def widgets_from_port_mapping(
    mapping_service_tuple: tuple[PortMapping, str | None],
) -> list[Widget]:
    port_mapping, service = mapping_service_tuple
    return [
        widget
        for host_mapping in port_mapping.mappings
        for widget in get_row_widgets(
            port_mapping.protocol,
            host_mapping.host_port,
            port_mapping.container_port,
            service,
        )
    ]
def get_row_widgets(
    protocol: Literal["tcp", "udp"],
    host_port: int,
    container_port: int,
    service: str | None,
) -> list[Widget]:
    url = get_url(
        host_port,
        service,
    )
    return [
        Static(service if service is not None else protocol),
        Static(str(container_port)),
        Static(str(host_port)),
        Link(url) if url is not None else Static("asdf"),
    ]
def get_url(
    host_port: int,
    service: str | None,
) -> str | None:
    if service is not None:
        if service == "noVNC":
            return f"http://localhost:{host_port}?view_only=true&autoconnect=true&resize=scale"
        if service.startswith("HTTP"):
            return f"https://localhost:{host_port}"
        if service.startswith("VNC"):
            return f"vnc://localhost:{host_port}"
    return None

================
File: src/inspect_ai/_display/textual/widgets/samples.py
================
import time
from typing import cast
from rich.console import RenderableType
from rich.table import Table
from rich.text import Text
from textual.app import ComposeResult
from textual.containers import Horizontal, HorizontalGroup, Vertical, VerticalGroup
from textual.css.query import NoMatches
from textual.reactive import reactive
from textual.widget import Widget
from textual.widgets import (
    Button,
    Collapsible,
    Link,
    LoadingIndicator,
    OptionList,
    Static,
)
from textual.widgets.option_list import Option
from inspect_ai._display.textual.widgets.port_mappings import get_url
from inspect_ai._util.format import format_progress_time
from inspect_ai._util.port_names import get_service_by_port
from inspect_ai._util.registry import registry_unqualified_name
from inspect_ai.log._samples import ActiveSample
from inspect_ai.log._transcript import ToolEvent
from .clock import Clock
from .sandbox import SandboxView
from .transcript import TranscriptView
class SamplesView(Widget):
    DEFAULT_CSS = """
    SamplesView {
        width: 1fr;
        height: 1fr;
        padding: 0 1 0 1;
        layout: grid;
        grid-size: 2 3;
        grid-rows: auto 1fr 3;
        grid-columns: 32 1fr;
        grid-gutter: 1;
    }
    """
    def __init__(self) -> None:
        super().__init__()
        self.samples: list[ActiveSample] = []
        self.last_updated = time.perf_counter()
    def compose(self) -> ComposeResult:
        yield SamplesList()
        yield SampleInfo()
        yield TranscriptView()
        yield SampleToolbar()
    def on_mount(self) -> None:
        self.watch(
            self.query_one(SamplesList), "highlighted", self.set_highlighted_sample
        )
    async def notify_active(self, active: bool) -> None:
        try:
            await self.query_one(TranscriptView).notify_active(active)
        except NoMatches:
            pass
    def set_samples(self, samples: list[ActiveSample]) -> None:
        # throttle to no more than 1 second per 100 samples
        throttle = round(max(len(samples) / 100, 1))
        current = time.perf_counter()
        if (current - self.last_updated) > throttle:
            self.query_one(SamplesList).set_samples(samples)
            self.last_updated = current
    async def set_highlighted_sample(self, highlighted: int | None) -> None:
        sample_info = self.query_one(SampleInfo)
        sample_vnc = self.query_one(SampleVNC)
        transcript_view = self.query_one(TranscriptView)
        sample_toolbar = self.query_one(SampleToolbar)
        if highlighted is not None:
            sample = self.query_one(SamplesList).sample_for_highlighted(highlighted)
            if sample is not None:
                sample_info.display = True
                transcript_view.display = True
                sample_toolbar.display = True
                await sample_info.sync_sample(sample)
                await sample_vnc.sync_sample(sample)
                await transcript_view.sync_sample(sample)
                await sample_toolbar.sync_sample(sample)
                return
        # otherwise hide ui
        sample_info.display = False
        sample_vnc.display = False
        transcript_view.display = False
        sample_toolbar.display = False
class SamplesList(OptionList):
    DEFAULT_CSS = """
    SamplesList {
        height: 100%;
        scrollbar-size-vertical: 1;
        margin-bottom: 1;
        row-span: 3;
        background: transparent;
    }
    SamplesList:focus > .option-list--option-highlighted {
        background: $primary 40%;
    }
    SamplesList  > .option-list--option-highlighted {
        background: $primary 40%;
    }
    """
    def __init__(self) -> None:
        super().__init__()
        self.samples: list[ActiveSample] = []
    def set_samples(self, samples: list[ActiveSample]) -> None:
        # check for a highlighted sample (make sure we don't remove it)
        highlighted_id = (
            self.get_option_at_index(self.highlighted).id
            if self.highlighted is not None
            else None
        )
        highlighted_sample = (
            sample_for_id(self.samples, highlighted_id)
            if highlighted_id is not None
            else None
        )
        # assign the new samples
        self.samples = samples.copy()
        # add the highlighted sample if its no longer in the list
        if highlighted_sample and (highlighted_sample not in self.samples):
            self.samples.append(highlighted_sample)
        # sort the samples by running time
        self.samples.sort(key=lambda sample: sample.running_time, reverse=True)
        # rebuild the list
        self.clear_options()
        options: list[Option] = []
        for sample in self.samples:
            table = Table.grid(expand=True)
            table.add_column(width=20)
            table.add_column(width=11, justify="right")
            table.add_column(width=1)
            task_name = Text.from_markup(f"{registry_unqualified_name(sample.task)}")
            task_name.truncate(18, overflow="ellipsis", pad=True)
            task_time = Text.from_markup(f"{format_progress_time(sample.running_time)}")
            table.add_row(task_name, task_time, " ")
            sample_id = Text.from_markup(f"id: {sample.sample.id}")
            sample_id.truncate(18, overflow="ellipsis", pad=True)
            sample_epoch = Text.from_markup(f"epoch: {sample.epoch:.0f}")
            table.add_row(
                sample_id,
                sample_epoch,
                " ",
            )
            table.add_row("", "", "")
            options.append(Option(table, id=sample.id))
        self.add_options(options)
        # select sample (re-select the highlighted sample if there is one)
        if len(self.samples) > 0:
            if highlighted_id is not None:
                index = sample_index_for_id(self.samples, highlighted_id)
            else:
                index = 0
            self.highlighted = index
            self.scroll_to_highlight()
    def sample_for_highlighted(self, highlighted: int) -> ActiveSample | None:
        highlighted_id = self.get_option_at_index(highlighted).id
        if highlighted_id is not None:
            return sample_for_id(self.samples, highlighted_id)
        else:
            return None
class SampleVNC(Horizontal):
    DEFAULT_CSS = """
    SampleVNC {
        layout: grid;
        grid-size: 2 1;
        grid-columns: auto 1fr;
    }
    SampleVNC Static {
        color: $secondary;
    }
    SampleVNC Link {
        color: $accent;
    }
    """
    def __init__(self) -> None:
        super().__init__()
        self._sample: ActiveSample | None = None
    def compose(self) -> ComposeResult:
        yield Static("VNC: ")
        yield Link("")
    async def sync_sample(self, sample: ActiveSample) -> None:
        if sample == self._sample:
            return
        # defult to hidden (show if we find a vnc connection)
        self.display = False
        # is there a vnc connection? if so populate
        for connection in [c for c in sample.sandboxes.values() if c.ports]:
            for port in connection.ports or []:
                service = get_service_by_port(port.container_port, port.protocol)
                if service == "noVNC" and port.mappings:
                    host_mappings = port.mappings
                    link = self.query_one(Link)
                    vnc_url = get_url(host_mappings[0].host_port, service)
                    if vnc_url:
                        link.text = vnc_url
                        link.url = link.text
                        self.display = True
                        break
class SampleInfo(Vertical):
    DEFAULT_CSS = """
    SampleInfo {
        color: $text-muted;
        layout: grid;
        grid-size: 1 2;
        grid-rows: auto 1;
        grid-gutter: 1;
    }
    SampleInfo Collapsible {
        padding: 0;
        border-top: none;
    }
    SampleInfo Collapsible CollapsibleTitle {
        padding: 0;
        color: $secondary;
        &:hover {
            background: $block-hover-background;
            color: $primary;
        }
        &:focus {
            background: $block-hover-background;
            color: $primary;
        }
    }
    SampleInfo Collapsible Contents {
        padding: 1 0 1 2;
        height: auto;
        overflow-x: auto;
    }
    SampleInfo Static {
        width: 1fr;
        background: $surface;
        color: $secondary;
    }
    """
    def __init__(self) -> None:
        super().__init__()
        self._sample: ActiveSample | None = None
        self._sandbox_count: int | None = None
    def compose(self) -> ComposeResult:
        with Collapsible(title=""):
            yield SampleLimits()
            yield SandboxesView()
        yield SampleVNC()
    async def sync_sample(self, sample: ActiveSample | None) -> None:
        if sample is None:
            self.display = False
            self._sample = None
        else:
            # update sample limits
            limits = self.query_one(SampleLimits)
            await limits.sync_sample(sample)
            new_sandbox_count = len(sample.sandboxes)
            # bail if we've already processed this sample
            if self._sample == sample and self._sandbox_count == new_sandbox_count:
                return
            # set sample
            self._sample = sample
            self._sandbox_count = new_sandbox_count
            # update UI
            self.display = True
            title = f"{registry_unqualified_name(sample.task)} (id: {sample.sample.id}, epoch {sample.epoch}): {sample.model}"
            self.query_one(Collapsible).title = title
            sandboxes = self.query_one(SandboxesView)
            await sandboxes.sync_sample(sample)
            await self.query_one(SampleVNC).sync_sample(sample)
class SampleLimits(Widget):
    DEFAULT_CSS = """
    SampleLimits {
        padding: 0 0 0 0;
        color: $secondary;
        background: transparent;
        height: auto;
    }
    SampleLimits Static {
        background: transparent;
        color: $secondary;
    }
    """
    messages = reactive(0)
    message_limit = reactive(0)
    tokens = reactive(0)
    token_limit = reactive(0)
    started = reactive(0)
    time_limit = reactive(0)
    def __init__(self) -> None:
        super().__init__()
    def render(self) -> RenderableType:
        limits = f"[bold]messages[/bold]: {self.messages}"
        if self.message_limit:
            limits = f"{limits} (limit {self.message_limit})"
        limits = f"{limits}, [bold]tokens[/bold]: {self.tokens:,}"
        if self.token_limit:
            limits = f"{limits} ({self.token_limit:,})"
        return limits
    async def sync_sample(self, sample: ActiveSample) -> None:
        self.messages = sample.total_messages
        self.message_limit = sample.message_limit or 0
        self.tokens = sample.total_tokens
        self.token_limit = sample.token_limit or 0
class SandboxesView(Vertical):
    DEFAULT_CSS = """
    SandboxesView {
        padding: 1 0 0 0;
        background: transparent;
        height: auto;
    }
    #sandboxes-list {
        height: auto;
    }
    SandboxesView Static {
        background: transparent;
    }
    .clipboard-message {
        height: auto;
        margin-top: 1;
    }
    """
    def __init__(self) -> None:
        super().__init__()
    def compose(self) -> ComposeResult:
        yield Static(id="sandboxes-caption", markup=True)
        yield Vertical(id="sandboxes-list")
    async def sync_sample(self, sample: ActiveSample) -> None:
        if len(sample.sandboxes) > 0:
            multiple_sandboxes = len(sample.sandboxes) > 1
            sandboxes_caption = cast(Static, self.query_one("#sandboxes-caption"))
            sandboxes_caption.update(
                f"[bold]sandbox container{'s' if multiple_sandboxes else ''}:[/bold]"
            )
            sandboxes_list = self.query_one("#sandboxes-list")
            await sandboxes_list.remove_children()
            await sandboxes_list.mount_all(
                [
                    SandboxView(connection, name if multiple_sandboxes else None)
                    for name, connection in sample.sandboxes.items()
                ]
            )
            await sandboxes_list.mount(
                Static(
                    "[italic]Hold down Alt (or Option) to select text for copying[/italic]",
                    classes="clipboard-message",
                    markup=True,
                )
            )
            self.display = True
        else:
            self.display = False
class SampleToolbar(Horizontal):
    STATUS_GROUP = "status_group"
    TIMEOUT_TOOL_CALL = "timeout_tool_call"
    CANCEL_SCORE_OUTPUT = "cancel_score_output"
    CANCEL_RAISE_ERROR = "cancel_raise_error"
    PENDING_STATUS = "pending_status"
    PENDING_CAPTION = "pending_caption"
    TIMEOUT_TOOL_CALL_ENABLED = (
        "Cancel the tool call and report a timeout to the model."
    )
    TIMEOUT_TOOL_CALL_DISABLED = "Cancelling tool call..."
    CANCEL_SCORE_OUTPUT_ENABLED = (
        "Cancel the sample and score whatever output has been generated so far."
    )
    CANCEL_RAISE_ERROR_ENABLED = "Cancel the sample and raise an error"
    CANCEL_DISABLED = "Cancelling sample..."
    DEFAULT_CSS = f"""
    SampleToolbar #{STATUS_GROUP} {{
        width: 22;
    }}
    SampleToolbar Button {{
        margin-bottom: 1;
        margin-right: 2;
        min-width: 18;
    }}
    SampleToolbar #{TIMEOUT_TOOL_CALL} {{
        color: $secondary-darken-3;
        min-width: 16;
    }}
    SampleToolbar #{CANCEL_SCORE_OUTPUT} {{
        color: $primary-darken-3;
    }}
    SampleToolbar #{CANCEL_RAISE_ERROR} {{
        color: $warning-darken-3;
    }}
    """
    def __init__(self) -> None:
        super().__init__()
        self.sample: ActiveSample | None = None
    def compose(self) -> ComposeResult:
        with HorizontalGroup(id=self.STATUS_GROUP):
            with VerticalGroup(id=self.PENDING_STATUS):
                yield Static("Executing...", id=self.PENDING_CAPTION)
                yield HorizontalGroup(EventLoadingIndicator(), Clock())
        yield Button(
            Text("Timeout Tool"),
            id=self.TIMEOUT_TOOL_CALL,
            tooltip=self.TIMEOUT_TOOL_CALL_ENABLED,
        )
        yield Horizontal()
        yield Button(
            Text("Cancel (Score)"),
            id=self.CANCEL_SCORE_OUTPUT,
            tooltip=self.CANCEL_SCORE_OUTPUT_ENABLED,
        )
        yield Button(
            Text("Cancel (Error)"),
            id=self.CANCEL_RAISE_ERROR,
            tooltip=self.CANCEL_RAISE_ERROR_ENABLED,
        )
    def on_mount(self) -> None:
        self.query_one("#" + self.PENDING_STATUS).visible = False
        self.query_one("#" + self.TIMEOUT_TOOL_CALL).display = False
        self.query_one("#" + self.CANCEL_SCORE_OUTPUT).display = False
        self.query_one("#" + self.CANCEL_RAISE_ERROR).display = False
    def on_button_pressed(self, event: Button.Pressed) -> None:
        if self.sample:
            if event.button.id == self.TIMEOUT_TOOL_CALL:
                last_event = (
                    self.sample.transcript.events[-1]
                    if self.sample.transcript.events
                    else None
                )
                if isinstance(last_event, ToolEvent):
                    last_event._cancel()
                    event.button.disabled = True
                    event.button.tooltip = self.TIMEOUT_TOOL_CALL_DISABLED
            else:
                if event.button.id == self.CANCEL_SCORE_OUTPUT:
                    self.sample.interrupt("score")
                elif event.button.id == self.CANCEL_RAISE_ERROR:
                    self.sample.interrupt("error")
                cancel_score_output = self.query_one("#" + self.CANCEL_SCORE_OUTPUT)
                cancel_score_output.disabled = True
                cancel_score_output.tooltip = self.CANCEL_DISABLED
                cancel_with_error = self.query_one("#" + self.CANCEL_RAISE_ERROR)
                cancel_with_error.disabled = True
                cancel_with_error.tooltip = self.CANCEL_DISABLED
    async def sync_sample(self, sample: ActiveSample | None) -> None:
        from inspect_ai.log._transcript import ModelEvent
        # is it a new sample?
        new_sample = sample != self.sample
        # track the sample
        self.sample = sample
        status_group = self.query_one("#" + self.STATUS_GROUP)
        pending_status = self.query_one("#" + self.PENDING_STATUS)
        timeout_tool = self.query_one("#" + self.TIMEOUT_TOOL_CALL)
        clock = self.query_one(Clock)
        cancel_score_output = cast(
            Button, self.query_one("#" + self.CANCEL_SCORE_OUTPUT)
        )
        cancel_with_error = cast(Button, self.query_one("#" + self.CANCEL_RAISE_ERROR))
        if sample and not sample.completed:
            # update visibility and button status
            self.display = True
            cancel_score_output.display = True
            cancel_with_error.display = not sample.fails_on_error
            # if its a new sample then reset enabled states
            if new_sample:
                cancel_score_output.disabled = False
                cancel_score_output.tooltip = self.CANCEL_SCORE_OUTPUT_ENABLED
                cancel_with_error.disabled = False
                cancel_with_error.tooltip = self.CANCEL_RAISE_ERROR_ENABLED
            # if we have a pending event then start the clock and show pending status
            last_event = (
                sample.transcript.events[-1]
                if len(sample.transcript.events) > 0
                else None
            )
            if last_event and last_event.pending:
                pending_status.visible = True
                pending_caption = cast(
                    Static, self.query_one("#" + self.PENDING_CAPTION)
                )
                if isinstance(last_event, ModelEvent):
                    # see if there are retries in play
                    if sample.retry_count > 0:
                        suffix = "retry" if sample.retry_count == 1 else "retries"
                        pending_caption_text = (
                            f"Generating ({sample.retry_count:,} {suffix})..."
                        )
                    else:
                        pending_caption_text = "Generating..."
                else:
                    pending_caption_text = "Executing..."
                status_group.styles.width = max(22, len(pending_caption_text))
                pending_caption.update(
                    Text.from_markup(f"[italic]{pending_caption_text}[/italic]")
                )
                timeout_tool.display = isinstance(last_event, ToolEvent)
                timeout_tool.disabled = False
                timeout_tool.tooltip = self.TIMEOUT_TOOL_CALL_ENABLED
                clock.start(last_event.timestamp.timestamp())
            else:
                pending_status.visible = False
                timeout_tool.display = False
                clock.stop()
        else:
            self.display = False
            pending_status.visible = False
            timeout_tool.display = False
            clock.stop()
class EventLoadingIndicator(LoadingIndicator):
    DEFAULT_CSS = """
    EventLoadingIndicator {
        width: auto;
        height: 1;
        color: $primary;
        text-style: not reverse;
        margin-right: 1;
    }
    """
    def __init__(self) -> None:
        super().__init__()
def sample_for_id(samples: list[ActiveSample], id: str) -> ActiveSample | None:
    index = sample_index_for_id(samples, id)
    if index != -1:
        return samples[index]
    else:
        return None
def sample_index_for_id(samples: list[ActiveSample], id: str) -> int:
    for i, sample in enumerate(samples):
        if sample.id == id:
            return i
    return -1

================
File: src/inspect_ai/_display/textual/widgets/sandbox.py
================
from textual.app import ComposeResult
from textual.containers import Horizontal, Vertical
from textual.widgets import Static
from inspect_ai.util._sandbox.environment import SandboxConnection
from .port_mappings import PortMappingsView
class SandboxView(Vertical):
    DEFAULT_CSS = """
    SandboxView {
        height: auto;
    }
    SandboxView * {
        height: auto;
    }
    .indent {
        width: 2;
    }
    .no_indent {
        width: 0;
    }
    """
    def __init__(
        self,
        connection: SandboxConnection,
        name: str | None,  # if None, no header or indent
    ) -> None:
        super().__init__()
        self.sandbox_name = name
        self.connection = connection
    def compose(self) -> ComposeResult:
        if self.sandbox_name:
            yield Static(self.sandbox_name)
        with Horizontal():
            yield Static("", classes="indent" if self.sandbox_name else "no_indent")
            with Vertical():
                yield Static(self.connection.command, markup=False)
                if self.connection.ports:
                    yield PortMappingsView(self.connection.ports)

================
File: src/inspect_ai/_display/textual/widgets/task_detail.py
================
import re
from dataclasses import dataclass
import numpy as np
from textual.app import ComposeResult
from textual.containers import Center, Grid, Horizontal
from textual.reactive import Reactive, reactive
from textual.widget import Widget
from textual.widgets import Static
from inspect_ai._display.core.display import TaskDisplayMetric
@dataclass
class TaskMetric:
    name: str
    value: float
class TaskDetail(Widget):
    hidden = reactive(False)
    DEFAULT_CSS = """
    TaskDetail {
        background: $boost;
        width: 100%;
        height: auto;
        padding: 1 0 1 0;
    }
    TaskDetail Grid {
        width: 100%;
        height: auto;
        grid-gutter: 1 3;
    }
    """
    def __init__(
        self,
        *,
        hidden: bool = True,
        id: str | None = None,
        classes: str | None = None,
    ) -> None:
        super().__init__(id=id, classes=classes)
        self.hidden = hidden
        self.existing_metrics: dict[str, TaskMetrics] = {}
        self.grid = Grid()
        self.by_reducer: dict[str | None, dict[str, list[TaskMetric]]] = {}
        self.metrics: list[TaskDisplayMetric] = []
    def watch_hidden(self, hidden: bool) -> None:
        """React to changes in the `visible` property."""
        if hidden:
            self.add_class("hidden")
        else:
            self.remove_class("hidden")
    def compose(self) -> ComposeResult:
        yield self.grid
    def on_mount(self) -> None:
        self.refresh_grid()
    def update_metrics(self, metrics: list[TaskDisplayMetric]) -> None:
        # Group by reducer then scorer within reducers
        self.metrics = metrics
        # clear the existing computed reducers
        self.by_reducer = {}
        for metric in metrics:
            reducer_group = (
                self.by_reducer[metric.reducer]
                if metric.reducer in self.by_reducer
                else {}
            )
            by_scorer_metrics = (
                reducer_group[metric.scorer] if metric.scorer in reducer_group else []
            )
            by_scorer_metrics.append(TaskMetric(name=metric.name, value=metric.value))
            reducer_group[metric.scorer] = by_scorer_metrics
            self.by_reducer[metric.reducer] = reducer_group
        self.refresh_grid()
    def refresh_grid(self) -> None:
        # Don't refresh the grid if not attached
        # since we may explicitly mount new widgets
        if not self.grid.is_attached:
            return
        # don't refresh the grid if there are no scores
        if len(self.by_reducer) == 0:
            return
        # Compute the row and column count
        row_count = len(self.by_reducer)
        col_count = len(next(iter(self.by_reducer.values())))
        # If this can fit in a single row, make it fit
        # otherwise place each reducer on their own row
        self.grid.styles.grid_columns = "auto"
        if row_count * col_count < 4:
            self.grid.styles.grid_size_columns = row_count * col_count
            self.grid.styles.grid_size_rows = 1
        else:
            self.grid.styles.grid_size_columns = col_count
            self.grid.styles.grid_size_rows = row_count
        # In order to reduce flashing the below tracks use of widgets
        # and updates them when possible (removing and adding them as needed)
        # Makes keys for tracking Task Metric widgets
        def metric_key(reducer: str | None, scorer: str) -> str:
            reducer = reducer or "none"
            return valid_id(f"task-{reducer}-{scorer}-tbl")
        # Remove keys that are no longer present
        existing_keys = set(self.existing_metrics.keys())
        new_keys = set(metric_key(m.reducer, m.scorer) for m in self.metrics)
        to_remove = existing_keys - new_keys
        for remove in to_remove:
            task_metric = self.existing_metrics[remove]
            task_metric.remove()
            del self.existing_metrics[remove]
        # add or update widgets with metrics
        for reducer, scorers in self.by_reducer.items():
            for scorer, scores in scorers.items():
                key = metric_key(reducer=reducer, scorer=scorer)
                if key in self.existing_metrics:
                    task_metrics = self.existing_metrics[key]
                    task_metrics.update(scores)
                else:
                    task_metrics = TaskMetrics(
                        id=key, scorer=scorer, reducer=reducer, metrics=scores
                    )
                    self.grid.mount(task_metrics)
                    self.existing_metrics[key] = task_metrics
class TaskMetrics(Widget):
    DEFAULT_CSS = """
    TaskMetrics {
        width: auto;
        height: auto;
    }
    TaskMetrics Grid {
        width: auto;
        grid-size: 2;
        grid-columns: auto;
        grid-gutter: 0 3;
        padding: 0 2 0 2;
    }
    TaskMetric Center {
        width: auto;
    }
    TaskMetrics Center Static {
        width: auto;
    }
    TaskMetrics Center Horizontal {
        width: auto;
        height: auto;
    }
    TaskMetrics Center Horizontal Static {
        width: auto;
        height: auto;
    }
    TaskMetrics .scorer {
        padding: 0 1 0 0;
        text-style: bold;
    }
    TaskMetrics .reducer {
        color: $foreground-darken-3;
    }
    """
    metrics: Reactive[list[TaskMetric]] = reactive([])
    def __init__(
        self,
        *,
        scorer: str | None,
        reducer: str | None,
        metrics: list[TaskMetric],
        id: str | None = None,
        classes: str | None = None,
    ) -> None:
        super().__init__(id=id, classes=classes)
        self.scorer = scorer
        self.reducer = reducer
        self.metrics = metrics
        self.grid: Grid = Grid()
        self.value_widgets: dict[str, Static] = {}
    def grid_id(self) -> str:
        return f"{self.id}-grid"
    def compose(self) -> ComposeResult:
        # Yield the title and base grid
        yield Center(self._title())
        yield Grid(id=self.grid_id())
    def update(self, metrics: list[TaskMetric]) -> None:
        self.metrics = metrics
        # We assume that generally the initial metric names will
        # always match future updates (so we can just update values in line)
        # but if an unrecognized metric appears on the scene, just
        # recompute the whole grid
        need_recompute = False
        for metric in metrics:
            widget = self.value_widgets.get(metric.name)
            if widget:
                # Just update the values themselves
                widget.update(content=f"{metric.value:,.3f}")
            else:
                # Don't have a widget for this, recompute the whole grid
                need_recompute = True
                break
        if need_recompute:
            self.recompute_grid()
    def on_mount(self) -> None:
        self.recompute_grid()
    def recompute_grid(self) -> None:
        if not self.is_mounted:
            return
        grid = self.query_one(f"#{self.grid_id()}")
        grid.remove_children()
        for metric in self.metrics:
            # Add the value static but keep it around
            # for future updates
            self.value_widgets[metric.name] = Static(
                self._metric_value(metric.value), markup=False
            )
            grid.mount(Static(metric.name, markup=False))
            grid.mount(self.value_widgets[metric.name])
    def _title(self) -> Widget:
        if self.scorer is None:
            return Static("")
        elif self.reducer is None:
            return Static(self.scorer, markup=False)
        else:
            return Horizontal(
                Static(self.scorer, classes="scorer", markup=False),
                Static(f"({self.reducer})", classes="reducer", markup=False),
            )
    def _metric_value(self, val: float) -> str:
        if np.isnan(val):
            return " n/a "
        else:
            return f"{val:.3f}"
def valid_id(identifier: str) -> str:
    # Remove invalid characters
    valid_part = re.sub(r"[^a-zA-Z0-9_-]", "_", identifier)
    # Ensure it doesn't start with a number
    if valid_part and valid_part[0].isdigit():
        valid_part = "_" + valid_part
    # If the string is empty return a default valid identifier
    return valid_part or "default_identifier"

================
File: src/inspect_ai/_display/textual/widgets/tasks.py
================
import contextlib
from datetime import datetime
from typing import Iterator, cast
from rich.console import RenderableType
from rich.text import Text
from textual import on
from textual.app import ComposeResult
from textual.containers import Container, ScrollableContainer
from textual.css.query import NoMatches
from textual.reactive import reactive
from textual.widget import Widget
from textual.widgets import ProgressBar, Static
from typing_extensions import override
from inspect_ai._display.core.results import task_metric
from inspect_ai._display.textual.widgets.clock import Clock
from inspect_ai._display.textual.widgets.task_detail import TaskDetail
from inspect_ai._display.textual.widgets.toggle import Toggle
from ...core.display import (
    Progress,
    TaskCancelled,
    TaskDisplay,
    TaskDisplayMetric,
    TaskError,
    TaskResult,
    TaskSpec,
    TaskWithResult,
)
from ...core.progress import (
    MAX_DESCRIPTION_WIDTH,
    MAX_MODEL_NAME_WIDTH,
    progress_count,
    progress_description,
    progress_model_name,
)
MAX_METRIC_WIDTH = 25
MAX_COUNT_WIDTH = 15
class TasksView(Container):
    DEFAULT_CSS = """
    TasksView {
        padding: 0 1;
        layout: grid;
        grid-size: 2 2;
        grid-columns: 1fr auto;
        grid-rows: auto 1fr;
    }
    #tasks-progress {
        column-span: 2;
        scrollbar-size-vertical: 1;
        margin-top: 1;
        margin-bottom: 1;
    }
    #tasks-config {
        color: $text-muted;
    }
    #tasks-targets {
        text-align: right;
        color: $text-muted;
    }
    """
    config: reactive[RenderableType] = reactive("")
    targets: reactive[RenderableType] = reactive("")
    def __init__(self) -> None:
        super().__init__()
        self.description_width = MAX_DESCRIPTION_WIDTH
        self.model_name_width = MAX_MODEL_NAME_WIDTH
        self.sample_count_width = 0
        self.display_metrics = True
    def init_tasks(self, tasks: list[TaskSpec]) -> None:
        # clear existing tasks
        self.tasks.remove_children()
        # compute the column widths by looking all of the tasks
        self.description_width = min(
            max([len(task.name) for task in tasks]), MAX_DESCRIPTION_WIDTH
        )
        self.model_name_width = min(
            max([len(str(task.model)) for task in tasks]), MAX_MODEL_NAME_WIDTH
        )
        self.update_progress_widths()
    def add_task(self, task: TaskWithResult) -> TaskDisplay:
        self.update_count_width(task.profile.samples)
        task_display = TaskProgressView(
            task,
            self.description_width,
            self.model_name_width,
            self.sample_count_width,
            self.display_metrics,
        )
        self.tasks.mount(task_display)
        self.tasks.scroll_to_widget(task_display)
        self.update_progress_widths()
        return task_display
    def set_display_metrics(self, display_metrics: bool) -> None:
        self.display_metrics = display_metrics
    def update_count_width(self, samples: int) -> None:
        sample_count_str = progress_count(samples, samples, self.sample_count_width)
        self.sample_count_width = min(
            max(self.sample_count_width, len(sample_count_str)), MAX_COUNT_WIDTH
        )
    def update_progress_widths(self) -> None:
        progress_views = self.tasks.query_children(TaskProgressView)
        metrics_size = 0
        for progress_view in progress_views:
            metrics_size = max(
                metrics_size,
                progress_view.metrics_width
                if progress_view.metrics_width is not None
                else 0,
            )
        metrics_size = min(metrics_size, MAX_METRIC_WIDTH)
        for progress_view in progress_views:
            progress_view.update_metrics_width(metrics_size)
            progress_view.update_count_width(self.sample_count_width)
    def compose(self) -> ComposeResult:
        yield Static(id="tasks-config", markup=False)
        yield Static(id="tasks-targets", markup=False)
        yield ScrollableContainer(id="tasks-progress")
    def watch_config(self, new_config: RenderableType) -> None:
        tasks_config = cast(Static, self.query_one("#tasks-config"))
        tasks_config.update(new_config)
    def watch_targets(self, new_targets: RenderableType) -> None:
        tasks_targets = cast(Static, self.query_one("#tasks-targets"))
        tasks_targets.update(new_targets)
    @property
    def tasks(self) -> ScrollableContainer:
        return cast(ScrollableContainer, self.query_one("#tasks-progress"))
class TaskProgressView(Widget):
    DEFAULT_CSS = """
    TaskProgressView {
        height: auto;
        width: 1fr;
        layout: grid;
        grid-size: 8 2;
        grid-columns: auto auto auto auto 1fr auto auto auto;
        grid-rows: auto auto;
        grid-gutter: 0 1;
    }
    TaskProgressView Bar {
        width: 1fr;
        &> .bar--bar {
            color: $warning 90%;
        }
        &> .bar--complete {
            color: $success;
        }
    }
    #task-metrics {
        color:$text-secondary;
    }
    #task-detail {
        column-span: 8;
    }
    .hidden {
        display: none;
    }
    """
    def __init__(
        self,
        task: TaskWithResult,
        description_width: int,
        model_name_width: int,
        sample_count_width: int,
        display_metrics: bool,
    ) -> None:
        super().__init__()
        self.t = task
        self.description_width = description_width
        self.model_name_width = model_name_width
        self.progress_bar = ProgressBar(total=task.profile.steps, show_eta=False)
        self.count_display = Static(markup=False)
        self.metrics_display = Static(id="task-metrics", markup=False)
        self.task_progress = TaskProgress(self.progress_bar)
        self.toggle = Toggle()
        self.task_detail = TaskDetail(id="task-detail", classes="hidden")
        self.sample_count_width: int = sample_count_width
        self.display_metrics = display_metrics
    metrics: reactive[list[TaskDisplayMetric] | None] = reactive(None)
    metrics_width: reactive[int | None] = reactive(None)
    sample_count_width: reactive[int] = reactive(0)
    samples_complete: reactive[int] = reactive(0)
    samples_total: reactive[int] = reactive(0)
    def compose(self) -> ComposeResult:
        yield (self.toggle if self.display_metrics else Static())
        yield TaskStatusIcon()
        yield Static(
            progress_description(self.t.profile, self.description_width, pad=True),
            markup=False,
        )
        yield Static(
            progress_model_name(self.t.profile.model, self.model_name_width, pad=True),
            markup=False,
        )
        yield self.progress_bar
        yield self.count_display
        yield self.metrics_display
        yield Clock()
        yield self.task_detail
    @on(Toggle.Toggled)
    def handle_title_toggle(self, event: Toggle.Toggled) -> None:
        self.task_detail.hidden = not self.toggle.toggled
        event.stop()
    def on_mount(self) -> None:
        self.query_one(Clock).start(datetime.now().timestamp())
    @contextlib.contextmanager
    def progress(self) -> Iterator[Progress]:
        yield self.task_progress
    def complete(self, result: TaskResult) -> None:
        self.t.result = result
        try:
            self.query_one(TaskStatusIcon).result = result
            self.query_one(Clock).stop()
        except NoMatches:
            pass
        self.task_progress.complete()
    def sample_complete(self, complete: int, total: int) -> None:
        self.samples_complete = complete
        self.samples_total = total
    def update_metrics(self, metrics: list[TaskDisplayMetric]) -> None:
        self.metrics = metrics
    def update_metrics_width(self, width: int) -> None:
        self.metrics_width = width
    def update_count_width(self, width: int) -> None:
        self.sample_count_width = width
    def _watch_sample_count_width(self, width: int) -> None:
        self.refresh_count()
    def _watch_samples_complete(self, complete: int) -> None:
        self.refresh_count()
    def _watch_samples_total(self, total: int) -> None:
        self.refresh_count()
    def _watch_metrics_width(self, width: int) -> None:
        self.update_metrics_label()
    def _watch_metrics(self, metrics: list[TaskDisplayMetric] | None) -> None:
        if metrics is not None and len(metrics) > 0:
            # update label
            self.update_metrics_label()
            # update details
            self.task_detail.update_metrics(metrics)
    def refresh_count(self) -> None:
        progress_label = progress_count(
            self.samples_complete, self.samples_total, self.sample_count_width
        )
        self.count_display.update(progress_label)
    def update_metrics_label(self) -> None:
        # compute the label (with a min size)
        if self.metrics is not None and self.metrics_display is not None:
            metric_label = task_metric(self.metrics, self.metrics_width)
            self.metrics_width = len(metric_label)
            self.metrics_display.update(metric_label)
class TaskStatusIcon(Static):
    result: reactive[TaskResult | None] = reactive(None)
    def __init__(self) -> None:
        super().__init__()
        self.watch_result(None)
    def watch_result(self, new_result: TaskResult | None) -> None:
        self.update(self._status_icon(new_result))
    def _status_icon(self, result: TaskResult | None) -> RenderableType:
        error = self.app.current_theme.error or ""
        succcess = self.app.current_theme.success or ""
        running = self.app.current_theme.secondary or ""
        if result:
            if isinstance(result, TaskError):
                return Text("", style=error)
            elif isinstance(result, TaskCancelled):
                return Text("", style=error)
            else:
                return Text("", style=succcess)
        else:
            return Text("", style=running)
MAX_PROGRESS_PERCENT = 0.02
MIN_PROGRESS_PERCENT = 0.98
class TaskProgress(Progress):
    def __init__(self, progress_bar: ProgressBar) -> None:
        self.progress_bar = progress_bar
        self.current_progress = 0
        # always show a minimum amount of progress
        minimum_steps = (
            MAX_PROGRESS_PERCENT * progress_bar.total
            if progress_bar.total is not None
            else 0
        )
        self.progress_bar.update(progress=minimum_steps)
    @override
    def update(self, n: int = 1) -> None:
        self.current_progress = self.current_progress + n
        # enforce a maximum cap on task progress
        max_progress = (
            MIN_PROGRESS_PERCENT * self.progress_bar.total
            if self.progress_bar.total is not None
            else 0
        )
        if (
            self.current_progress > self.progress_bar.progress
            and self.current_progress < max_progress
        ):
            self.progress_bar.update(progress=self.current_progress)
    @override
    def complete(self) -> None:
        if self.progress_bar.total is not None:
            self.progress_bar.update(progress=self.progress_bar.total)

================
File: src/inspect_ai/_display/textual/widgets/titlebar.py
================
from __future__ import annotations
from typing import Iterator
from rich.console import RenderableType
from rich.text import Text
from textual.reactive import Reactive
from textual.widget import Widget
class AppTitlebar(Widget):
    DEFAULT_CSS = """
    AppTitlebar {
        dock: top;
        width: 100%;
        background: $panel;
        color: $primary;
        height: 1;
        text-style: bold;
    }
    """
    DEFAULT_CLASSES = ""
    def __init__(
        self,
        *,
        name: str | None = None,
        id: str | None = None,
        classes: str | None = None,
    ):
        """Initialise the header widget.
        Args:
            name: The name of the header widget.
            id: The ID of the header widget in the DOM.
            classes: The CSS classes of the header widget.
        """
        super().__init__(name=name, id=id, classes=classes)
    def compose(self) -> Iterator[Widget]:
        yield AppTitlebarTitle()
    @property
    def title(self) -> str:
        return self._header_title().text
    @title.setter
    def title(self, title: str) -> None:
        self._header_title().text = title
    @property
    def sub_title(self) -> str:
        return self._header_title().sub_text
    @sub_title.setter
    def sub_title(self, sub_title: str) -> None:
        self._header_title().sub_text = sub_title
    def _header_title(self) -> AppTitlebarTitle:
        return self.query_one(AppTitlebarTitle)
class AppTitlebarTitle(Widget):
    """Display the title / subtitle in the header."""
    DEFAULT_CSS = """
    AppTitlebarTitle {
        content-align: center middle;
        width: 100%;
    }
    """
    text: Reactive[str] = Reactive("")
    """The main title text."""
    sub_text = Reactive("")
    """The sub-title text."""
    def render(self) -> RenderableType:
        """Render the title and sub-title.
        Returns:
            The value to render.
        """
        text = Text(self.text, no_wrap=True, overflow="ellipsis")
        if self.sub_text:
            text.append("  ")
            text.append(self.sub_text, "dim")
        return text

================
File: src/inspect_ai/_display/textual/widgets/toggle.py
================
from textual.events import Click
from textual.message import Message
from textual.reactive import reactive
from textual.widgets import Static
class Toggle(Static, can_focus=True):
    toggled = reactive(True)
    def __init__(
        self, on_symbol: str = "", off_symbol: str = "", toggled: bool = False
    ) -> None:
        super().__init__()
        self.on_symbol = on_symbol
        self.off_symbol = off_symbol
        self.toggled = toggled
    class Toggled(Message):
        """Request toggle."""
    async def _on_click(self, event: Click) -> None:
        """Inform ancestor we want to toggle."""
        event.stop()
        self.toggled = not self.toggled
        self.post_message(self.Toggled())
    def _watch_toggled(self, toggled: bool) -> None:
        if toggled:
            self.update(self.on_symbol)
        else:
            self.update(self.off_symbol)

================
File: src/inspect_ai/_display/textual/widgets/transcript.py
================
from typing import Any, Callable, NamedTuple, Sequence, Type
from pydantic_core import to_json
from rich.console import Group, RenderableType
from rich.markdown import Markdown
from rich.table import Table
from rich.text import Text
from textual.containers import ScrollableContainer
from textual.widget import Widget
from textual.widgets import Static
from inspect_ai._util.content import ContentReasoning, ContentText
from inspect_ai._util.rich import lines_display
from inspect_ai._util.transcript import (
    set_transcript_markdown_options,
    transcript_function,
    transcript_markdown,
    transcript_reasoning,
    transcript_separator,
)
from inspect_ai.log._samples import ActiveSample
from inspect_ai.log._transcript import (
    ApprovalEvent,
    ErrorEvent,
    Event,
    InfoEvent,
    InputEvent,
    LoggerEvent,
    ModelEvent,
    SampleInitEvent,
    SampleLimitEvent,
    ScoreEvent,
    StepEvent,
    SubtaskEvent,
    ToolEvent,
)
from inspect_ai.model._chat_message import (
    ChatMessage,
    ChatMessageUser,
)
from inspect_ai.model._render import messages_preceding_assistant
from inspect_ai.tool._tool import ToolResult
from inspect_ai.tool._tool_transcript import transcript_tool_call
class TranscriptView(ScrollableContainer):
    DEFAULT_CSS = """
    TranscriptView {
        scrollbar-size-vertical: 1;
        scrollbar-gutter: stable;
    }
    """
    def __init__(self) -> None:
        super().__init__()
        self._sample_id: str | None = None
        self._sample_events: int | None = None
        self._active = False
        self._pending_sample: ActiveSample | None = None
    async def notify_active(self, active: bool) -> None:
        self._active = active
        if self._active and self._pending_sample:
            await self.sync_sample(self._pending_sample)
            self._pending_sample = None
    async def sync_sample(self, sample: ActiveSample | None) -> None:
        # if sample is none then reset
        if sample is None:
            self._sample = None
            self._sample_events = None
            await self.remove_children()
        # process sample if we are active
        elif self._active:
            # if we have either a new sample or a new event count then proceed
            if (
                sample.id != self._sample_id
                or len(sample.transcript.events) != self._sample_events
            ):
                # update (scrolling to end if we are already close to it)
                new_sample = sample.id != self._sample_id
                scroll_to_end = (
                    new_sample or abs(self.scroll_y - self.max_scroll_y) <= 20
                )
                async with self.batch():
                    await self.remove_children()
                    await self.mount_all(
                        self._widgets_for_events(sample.transcript.events)
                    )
                if scroll_to_end:
                    self.scroll_end(animate=not new_sample)
                # set members
                self._sample_id = sample.id
                self._sample_events = len(sample.transcript.events)
        # if we aren't active then save as a pending sample
        else:
            self._pending_sample = sample
    def _widgets_for_events(self, events: Sequence[Event]) -> list[Widget]:
        widgets: list[Widget] = []
        for event in events:
            display = render_event(event)
            if display:
                for d in display:
                    if d.content:
                        widgets.append(
                            Static(
                                transcript_separator(
                                    d.title, self.app.current_theme.primary
                                )
                            )
                        )
                        if isinstance(d.content, Markdown):
                            set_transcript_markdown_options(d.content)
                        widgets.append(Static(d.content, markup=False))
                        widgets.append(Static(Text(" ")))
        return widgets
class EventDisplay(NamedTuple):
    """Display for an event group."""
    title: str
    """Text for title bar"""
    content: RenderableType | None = None
    """Optional custom content to display."""
def render_event(event: Event) -> list[EventDisplay] | None:
    # see if we have a renderer
    for event_type, renderer in _renderers:
        if isinstance(event, event_type):
            display = renderer(event)
            if display is not None:
                return display if isinstance(display, list) else [display]
    # no renderer
    return None
def render_sample_init_event(event: SampleInitEvent) -> EventDisplay:
    # alias sample
    sample = event.sample
    # input
    messages: list[ChatMessage] = (
        [ChatMessageUser(content=sample.input)]
        if isinstance(sample.input, str)
        else sample.input
    )
    content: list[RenderableType] = []
    for message in messages:
        content.extend(render_message(message))
    # target
    if sample.target:
        content.append(Text())
        content.append(Text("Target", style="bold"))
        content.append(Text())
        content.append(str(sample.target).strip())
    return EventDisplay("sample init", Group(*content))
def render_sample_limit_event(event: SampleLimitEvent) -> EventDisplay:
    return EventDisplay(f"limit: {event.type}", Text(event.message))
def render_model_event(event: ModelEvent) -> EventDisplay:
    # content
    content: list[RenderableType] = []
    def append_message(message: ChatMessage) -> None:
        content.extend(render_message(message))
    # render preceding messages
    preceding = messages_preceding_assistant(event.input)
    for message in preceding:
        append_message(message)
        content.append(Text())
    # display assistant message (note that we don't render tool calls
    # because they will be handled as part of render_tool)
    if event.output.message and event.output.message.text:
        append_message(event.output.message)
    return EventDisplay(f"model: {event.model}", Group(*content))
def render_sub_events(events: list[Event]) -> list[RenderableType]:
    content: list[RenderableType] = []
    for e in events:
        event_displays = render_event(e) or []
        for d in event_displays:
            if d.content:
                content.append(Text("  "))
                content.append(transcript_separator(d.title, "black", ""))
                if isinstance(d.content, Markdown):
                    set_transcript_markdown_options(d.content)
                content.append(d.content)
    return content
def render_tool_event(event: ToolEvent) -> list[EventDisplay]:
    # render the call
    content = transcript_tool_call(event)
    # render sub-events
    if event.events:
        content.extend(render_sub_events(event.events))
    # render the output
    if isinstance(event.result, list):
        result: ToolResult = "\n".join(
            [
                content.text
                for content in event.result
                if isinstance(content, ContentText)
            ]
        )
    else:
        result = event.result
    if result:
        content.append(Text())
        result = str(result).strip()
        content.extend(lines_display(result, 50))
    return [EventDisplay("tool call", Group(*content))]
def render_step_event(event: StepEvent) -> EventDisplay:
    if event.type == "solver":
        return render_solver_event(event)
    if event.type == "scorer":
        return render_scorer_event(event)
    else:
        return EventDisplay(step_title(event))
def render_solver_event(event: StepEvent) -> EventDisplay:
    return EventDisplay(step_title(event))
def render_scorer_event(event: StepEvent) -> EventDisplay:
    return EventDisplay(step_title(event))
def render_score_event(event: ScoreEvent) -> EventDisplay:
    table = Table(box=None, show_header=False)
    table.add_column("", min_width=10, justify="left")
    table.add_column("", justify="left")
    table.add_row("Target", str(event.target).strip())
    if event.score.answer:
        table.add_row("Answer", transcript_markdown(event.score.answer, escape=True))
    table.add_row("Score", str(event.score.value).strip())
    if event.score.explanation:
        table.add_row(
            "Explanation", transcript_markdown(event.score.explanation, escape=True)
        )
    return EventDisplay("score", table)
def render_subtask_event(event: SubtaskEvent) -> list[EventDisplay]:
    # render header
    content: list[RenderableType] = [transcript_function(event.name, event.input)]
    # render sub-events
    if event.events:
        content.extend(render_sub_events(event.events))
    if event.result:
        content.append(Text())
        if isinstance(event.result, str | int | float | bool | None):
            content.append(Text(str(event.result)))
        else:
            content.append(render_as_json(event.result))
    return [EventDisplay(f"subtask: {event.name}", Group(*content))]
def render_input_event(event: InputEvent) -> EventDisplay:
    return EventDisplay("input", Text.from_ansi(event.input_ansi.strip()))
def render_approval_event(event: ApprovalEvent) -> EventDisplay:
    content: list[RenderableType] = [
        f"[bold]{event.approver}[/bold]: {event.decision} ({event.explanation})"
    ]
    return EventDisplay("approval", Group(*content))
def render_info_event(event: InfoEvent) -> EventDisplay:
    if isinstance(event.data, str):
        content: RenderableType = transcript_markdown(event.data)
    else:
        content = render_as_json(event.data)
    return EventDisplay("info", content)
def render_logger_event(event: LoggerEvent) -> EventDisplay:
    content = event.message.level.upper()
    if event.message.name:
        content = f"{content} (${event.message.name})"
    content = f"{content}: {event.message.message}"
    return EventDisplay("logger", content)
def render_error_event(event: ErrorEvent) -> EventDisplay:
    return EventDisplay("error", event.error.traceback.strip())
def render_as_json(json: Any) -> RenderableType:
    return transcript_markdown(
        "```json\n"
        + to_json(json, indent=2, fallback=lambda _: None).decode()
        + "\n```\n"
    )
def render_message(message: ChatMessage) -> list[RenderableType]:
    content: list[RenderableType] = [
        Text(message.role.capitalize(), style="bold"),
        Text(),
    ]
    # deal with plain text or with content blocks
    if isinstance(message.content, str):
        content.extend([transcript_markdown(message.text.strip(), escape=True)])
    else:
        for c in message.content:
            if isinstance(c, ContentReasoning):
                content.extend(transcript_reasoning(c))
            elif isinstance(c, ContentText):
                content.extend([transcript_markdown(c.text.strip(), escape=True)])
    return content
def step_title(event: StepEvent) -> str:
    return f"{event.type or 'step'}: {event.name}"
EventRenderer = Callable[[Any], EventDisplay | list[EventDisplay] | None]
_renderers: list[tuple[Type[Event], EventRenderer]] = [
    (SampleInitEvent, render_sample_init_event),
    (SampleLimitEvent, render_sample_limit_event),
    (StepEvent, render_step_event),
    (ModelEvent, render_model_event),
    (ToolEvent, render_tool_event),
    (SubtaskEvent, render_subtask_event),
    (ScoreEvent, render_score_event),
    (InputEvent, render_input_event),
    (ApprovalEvent, render_approval_event),
    (InfoEvent, render_info_event),
    (LoggerEvent, render_logger_event),
    (ErrorEvent, render_error_event),
]

================
File: src/inspect_ai/_eval/context.py
================
from inspect_ai._util.dotenv import init_dotenv
from inspect_ai._util.hooks import init_hooks
from inspect_ai._util.logger import init_logger
from inspect_ai.approval._apply import have_tool_approval, init_tool_approval
from inspect_ai.approval._human.manager import init_human_approval_manager
from inspect_ai.approval._policy import ApprovalPolicy
from inspect_ai.log._samples import init_active_samples
from inspect_ai.model import GenerateConfig, Model
from inspect_ai.model._model import init_active_model, init_model_usage
from inspect_ai.util._concurrency import init_concurrency
from inspect_ai.util._subprocess import init_max_subprocesses
def init_eval_context(
    log_level: str | None,
    log_level_transcript: str | None,
    max_subprocesses: int | None = None,
) -> None:
    init_dotenv()
    init_logger(log_level, log_level_transcript)
    init_concurrency()
    init_max_subprocesses(max_subprocesses)
    init_hooks()
    init_active_samples()
    init_human_approval_manager()
def init_task_context(
    model: Model,
    approval: list[ApprovalPolicy] | None = None,
    config: GenerateConfig = GenerateConfig(),
) -> None:
    init_active_model(model, config)
    init_model_usage()
    if not have_tool_approval():
        init_tool_approval(approval)

================
File: src/inspect_ai/_eval/eval.py
================
import logging
import os
import sys
from pathlib import Path
from typing import Any, Literal
from inspect_ai._util.notgiven import NOT_GIVEN, NotGiven
if sys.version_info < (3, 11):
    from exceptiongroup import ExceptionGroup
from shortuuid import uuid
from typing_extensions import Unpack
from inspect_ai._cli.util import parse_cli_args
from inspect_ai._display.core.active import display as task_display
from inspect_ai._util.config import resolve_args
from inspect_ai._util.constants import DEFAULT_LOG_FORMAT
from inspect_ai._util.error import PrerequisiteError
from inspect_ai._util.file import absolute_file_path
from inspect_ai._util.logger import warn_once
from inspect_ai._util.platform import platform_init
from inspect_ai._util.registry import registry_lookup
from inspect_ai.approval._apply import init_tool_approval
from inspect_ai.approval._policy import (
    ApprovalPolicy,
    ApprovalPolicyConfig,
    approval_policies_from_config,
    config_from_approval_policies,
)
from inspect_ai.log import EvalConfig, EvalLog, EvalLogInfo
from inspect_ai.log._file import read_eval_log_async
from inspect_ai.log._recorders import create_recorder_for_format
from inspect_ai.model import (
    GenerateConfig,
    GenerateConfigArgs,
    Model,
)
from inspect_ai.model._model import init_active_model, resolve_models
from inspect_ai.scorer._reducer import reducer_log_names
from inspect_ai.solver._chain import chain
from inspect_ai.solver._solver import Solver, SolverSpec
from inspect_ai.util import SandboxEnvironmentType
from inspect_ai.util._display import (
    DisplayType,
    display_type,
    display_type_initialized,
    init_display_type,
)
from .context import init_eval_context
from .loader import resolve_tasks
from .run import eval_run
from .task import Epochs, PreviousTask
from .task.resolved import ResolvedTask, resolved_model_names
from .task.tasks import Tasks
log = logging.getLogger(__name__)
def eval(
    tasks: Tasks,
    model: str | Model | list[str] | list[Model] | None | NotGiven = NOT_GIVEN,
    model_base_url: str | None = None,
    model_args: dict[str, Any] | str = dict(),
    task_args: dict[str, Any] | str = dict(),
    sandbox: SandboxEnvironmentType | None = None,
    sandbox_cleanup: bool | None = None,
    solver: Solver | list[Solver] | SolverSpec | None = None,
    tags: list[str] | None = None,
    trace: bool | None = None,
    display: DisplayType | None = None,
    approval: str | list[ApprovalPolicy] | None = None,
    log_level: str | None = None,
    log_level_transcript: str | None = None,
    log_dir: str | None = None,
    log_format: Literal["eval", "json"] | None = None,
    limit: int | tuple[int, int] | None = None,
    sample_id: str | int | list[str | int] | None = None,
    epochs: int | Epochs | None = None,
    fail_on_error: bool | float | None = None,
    debug_errors: bool | None = None,
    message_limit: int | None = None,
    token_limit: int | None = None,
    time_limit: int | None = None,
    working_limit: int | None = None,
    max_samples: int | None = None,
    max_tasks: int | None = None,
    max_subprocesses: int | None = None,
    max_sandboxes: int | None = None,
    log_samples: bool | None = None,
    log_images: bool | None = None,
    log_buffer: int | None = None,
    score: bool = True,
    score_display: bool | None = None,
    **kwargs: Unpack[GenerateConfigArgs],
) -> list[EvalLog]:
    r"""Evaluate tasks using a Model.
    Args:
        tasks: Task(s) to evaluate. If None, attempt
            to evaluate a task in the current working directory
        model: Model(s) for evaluation. If not specified use the value of the INSPECT_EVAL_MODEL
            environment variable. Specify `None` to define no default model(s), which will
            leave model usage entirely up to tasks.
        model_base_url: Base URL for communicating
            with the model API.
        model_args: Model creation args
            (as a dictionary or as a path to a JSON or YAML config file)
        task_args: Task creation arguments
            (as a dictionary or as a path to a JSON or YAML config file)
        sandbox: Sandbox environment type
            (or optionally a str or tuple with a shorthand spec)
        sandbox_cleanup: Cleanup sandbox environments after task completes
            (defaults to True)
        solver: Alternative solver for task(s).
            Optional (uses task solver by default).
        tags: Tags to associate with this evaluation run.
        trace: Trace message interactions with evaluated model to terminal.
        display: Task display type (defaults to 'full').
        approval: Tool use approval policies.
            Either a path to an approval policy config file or a list of approval policies.
            Defaults to no approval policy.
        log_level: Level for logging to the console: "debug", "http", "sandbox",
            "info", "warning", "error", or "critical" (defaults to "warning")
        log_level_transcript: Level for logging to the log file (defaults to "info")
        log_dir: Output path for logging results
            (defaults to file log in ./logs directory).
        log_format: Format for writing log files (defaults
            to "eval", the native high-performance format).
        limit: Limit evaluated samples
            (defaults to all samples).
        sample_id: Evaluate specific sample(s) from the dataset.
        epochs: Epochs to repeat samples for and optional score
            reducer function(s) used to combine sample scores (defaults to "mean")
        fail_on_error: `True` to fail on first sample error
            (default); `False` to never fail on sample errors; Value between 0 and 1
            to fail if a proportion of total samples fails. Value greater than 1 to fail
            eval if a count of samples fails.
        debug_errors: Raise task errors (rather than logging them)
            so they can be debugged (defaults to False).
        message_limit: Limit on total messages used for each sample.
        token_limit: Limit on total tokens used for each sample.
        time_limit: Limit on clock time (in seconds) for samples.
        working_limit: Limit on working time (in seconds) for sample. Working
            time includes model generation, tool calls, etc. but does not include
            time spent waiting on retries or shared resources.
        max_samples: Maximum number of samples to run in parallel
            (default is max_connections)
        max_tasks: Maximum number of tasks to run in parallel
            (defaults to number of models being evaluated)
        max_subprocesses: Maximum number of subprocesses to
            run in parallel (default is os.cpu_count())
        max_sandboxes: Maximum number of sandboxes (per-provider)
            to run in parallel.
        log_samples: Log detailed samples and scores (defaults to True)
        log_images: Log base64 encoded version of images,
            even if specified as a filename or URL (defaults to False)
        log_buffer: Number of samples to buffer before writing log file.
            If not specified, an appropriate default for the format and filesystem is
            chosen (10 for most all cases, 100 for JSON logs on remote filesystems).
        score: Score output (defaults to True)
        score_display: Show scoring metrics in realtime (defaults to True)
        **kwargs: Model generation options.
    Returns:
        List of EvalLog (one for each task)
    """
    # standard platform init for top level entry points
    platform_init()
    # resolve eval trace
    max_tasks, max_samples = init_eval_display(
        display, trace, max_tasks, max_samples, model
    )
    async def run_task_app() -> list[EvalLog]:
        try:
            return await eval_async(
                tasks=tasks,
                model=model,
                model_base_url=model_base_url,
                model_args=model_args,
                task_args=task_args,
                sandbox=sandbox,
                sandbox_cleanup=sandbox_cleanup,
                solver=solver,
                tags=tags,
                approval=approval,
                log_level=log_level,
                log_level_transcript=log_level_transcript,
                log_dir=log_dir,
                log_format=log_format,
                limit=limit,
                sample_id=sample_id,
                epochs=epochs,
                fail_on_error=fail_on_error,
                debug_errors=debug_errors,
                message_limit=message_limit,
                token_limit=token_limit,
                time_limit=time_limit,
                working_limit=working_limit,
                max_samples=max_samples,
                max_tasks=max_tasks,
                max_subprocesses=max_subprocesses,
                max_sandboxes=max_sandboxes,
                log_samples=log_samples,
                log_images=log_images,
                log_buffer=log_buffer,
                score=score,
                score_display=score_display,
                **kwargs,
            )
        # exceptions can escape when debug_errors is True and that's okay
        except ExceptionGroup as ex:
            if debug_errors:
                raise ex.exceptions[0] from None
            else:
                raise
    return task_display().run_task_app(run_task_app)
# single call to eval_async at a time
_eval_async_running = False
async def eval_async(
    tasks: Tasks,
    model: str | Model | list[str] | list[Model] | None | NotGiven = NOT_GIVEN,
    model_base_url: str | None = None,
    model_args: dict[str, Any] | str = dict(),
    task_args: dict[str, Any] | str = dict(),
    sandbox: SandboxEnvironmentType | None = None,
    sandbox_cleanup: bool | None = None,
    solver: Solver | list[Solver] | SolverSpec | None = None,
    tags: list[str] | None = None,
    approval: str | list[ApprovalPolicy] | ApprovalPolicyConfig | None = None,
    log_level: str | None = None,
    log_level_transcript: str | None = None,
    log_dir: str | None = None,
    log_format: Literal["eval", "json"] | None = None,
    limit: int | tuple[int, int] | None = None,
    sample_id: str | int | list[str | int] | None = None,
    epochs: int | Epochs | None = None,
    fail_on_error: bool | float | None = None,
    debug_errors: bool | None = None,
    message_limit: int | None = None,
    token_limit: int | None = None,
    time_limit: int | None = None,
    working_limit: int | None = None,
    max_samples: int | None = None,
    max_tasks: int | None = None,
    max_subprocesses: int | None = None,
    max_sandboxes: int | None = None,
    log_samples: bool | None = None,
    log_images: bool | None = None,
    log_buffer: int | None = None,
    score: bool = True,
    score_display: bool | None = None,
    **kwargs: Unpack[GenerateConfigArgs],
) -> list[EvalLog]:
    r"""Evaluate tasks using a Model (async).
    Args:
        tasks: Task(s) to evaluate. If None, attempt
            to evaluate a task in the current working directory
        model: Model(s) for evaluation. If not specified use the value of the INSPECT_EVAL_MODEL
            environment variable. Specify `None` to define no default model(s), which will
            leave model usage entirely up to tasks.
        model_base_url: Base URL for communicating with the model API.
        model_args: Model creation args (as a dictionary or as a path to a JSON or YAML config file)
        task_args: Task creation arguments (as a dictionary or as a path to a JSON or YAML config file)
        sandbox: Sandbox environment type (or optionally a str or tuple with a shorthand spec)
        sandbox_cleanup: Cleanup sandbox environments after task completes (defaults to True)
        solver: Alternative solver for task(s).  Optional (uses task solver by default).
        tags (list[str] | None): Tags to associate with this evaluation run.
        approval: Tool use approval policies.
          Either a path to an approval policy config file or a list of approval policies.
          Defaults to no approval policy.
        log_level: Level for logging to the console: "debug", "http", "sandbox",
          "info", "warning", "error", or "critical" (defaults to "warning")
        log_level_transcript: Level for logging to the log file (defaults to "info")
        log_dir: Output path for logging results (defaults to file log in ./logs directory).
        log_format: Format for writing log files (defaults to "eval", the native high-performance format).
        limit: Limit evaluated samples (defaults to all samples).
        sample_id: Evaluate specific sample(s) from the dataset.
        epochs: Epochs to repeat samples for and optional score
            reducer function(s) used to combine sample scores (defaults to "mean")
        fail_on_error: `True` to fail on first sample error
            (default); `False` to never fail on sample errors; Value between 0 and 1
            to fail if a proportion of total samples fails. Value greater than 1 to fail eval if a count of samples fails.
        debug_errors: Raise task errors (rather than logging them) so they can be debugged (defaults to False).
        message_limit: Limit on total messages used for each sample.
        token_limit: Limit on total tokens used for each sample.
        time_limit: Limit on clock time (in seconds) for samples.
        working_limit: Limit on working time (in seconds) for sample. Working
            time includes model generation, tool calls, etc. but does not include
            time spent waiting on retries or shared resources.
        max_samples: Maximum number of samples to run in parallel (default is max_connections)
        max_tasks: Maximum number of tasks to run in parallel
            (defaults to number of models being evaluated)
        max_subprocesses: Maximum number of subprocesses to run in parallel (default is os.cpu_count())
        max_sandboxes: Maximum number of sandboxes (per-provider) to run in parallel.
        log_samples: Log detailed samples and scores (defaults to True)
        log_images: Log base64 encoded version of images, even if specified as a filename or URL (defaults to False)
        log_buffer: Number of samples to buffer before writing log file.
           If not specified, an appropriate default for the format and filesystem is
           chosen (10 for most all cases, 100 for JSON logs on remote filesystems).
        score: Score output (defaults to True)
        score_display: Show scoring metrics in realtime (defaults to True)
        **kwargs: Model generation options.
    Returns:
        List of EvalLog (one for each task)
    """
    # only a single call to eval_async can be active at a time, this is
    # because when running a task a chdir to the task's directory (and
    # similar mutation of the Python sys.path) occurs. since this is a
    # change to global process state it cannot occur in parallel. for
    # task parallelism, pass multiple tasks to eval or eval_async (which
    # will enforce the appropriate constraints on task parallelism)
    global _eval_async_running
    if _eval_async_running:
        raise RuntimeError("Multiple concurrent calls to eval_async are not allowed.")
    _eval_async_running = True
    # if we are called outside of eval() then set display type to "plain"
    if not display_type_initialized():
        init_display_type("plain")
    # resolve model and task args
    model_args = resolve_args(model_args)
    task_args = resolve_args(task_args)
    try:
        # intialise eval
        model, approval, resolved_tasks = eval_init(
            tasks=tasks,
            model=model,
            model_base_url=model_base_url,
            model_args=model_args,
            task_args=task_args,
            sandbox=sandbox,
            approval=approval,
            max_subprocesses=max_subprocesses,
            log_level=log_level,
            log_level_transcript=log_level_transcript,
            **kwargs,
        )
        # warn and return empty string if we resolved no tasks
        if len(resolved_tasks) == 0:
            log.warning("No inspect tasks were found at the specified paths.")
            return []
        # if there is no max tasks then base it on unique model names
        if max_tasks is None:
            model_count = len(resolved_model_names(resolved_tasks))
            if model_count > 1:
                max_tasks = model_count
        # apply conversation display constraints
        if display_type() == "conversation":
            # single task at a time
            if max_tasks is not None:
                max_tasks = 1
            # single sample at a time
            max_samples = 1
            # multiple models not allowed in trace mode
            if len(model) > 1:
                raise PrerequisiteError(
                    "Trace mode cannot be used when evaluating multiple models."
                )
        # resolve recorder (confirm writeable)
        log_dir = log_dir if log_dir else os.environ.get("INSPECT_LOG_DIR", "./logs")
        log_dir = absolute_file_path(log_dir)
        recorder = create_recorder_for_format(log_format or DEFAULT_LOG_FORMAT, log_dir)
        if not recorder.is_writeable():
            raise PrerequisiteError(
                f"ERROR: You do not have write permission for the log_dir '{log_dir}'"
            )
        # resolve solver
        solver = chain(solver) if isinstance(solver, list) else solver
        # ensure consistency of limit and sample_id
        if sample_id is not None and limit is not None:
            raise ValueError("You cannot specify both sample_id and limit.")
        # resolve epochs
        if isinstance(epochs, int):
            epochs = Epochs(epochs)
        if epochs is not None and epochs.epochs < 1:
            raise ValueError("epochs must be a positive integer.")
        # create config
        epochs_reducer = epochs.reducer if epochs else None
        eval_config = EvalConfig(
            limit=limit,
            sample_id=sample_id,
            epochs=epochs.epochs if epochs else None,
            epochs_reducer=reducer_log_names(epochs_reducer)
            if epochs_reducer
            else None,
            approval=config_from_approval_policies(approval) if approval else None,
            fail_on_error=fail_on_error,
            message_limit=message_limit,
            token_limit=token_limit,
            time_limit=time_limit,
            working_limit=working_limit,
            max_samples=max_samples,
            max_tasks=max_tasks,
            max_subprocesses=max_subprocesses,
            max_sandboxes=max_sandboxes,
            sandbox_cleanup=sandbox_cleanup,
            log_samples=log_samples,
            log_images=log_images,
            log_buffer=log_buffer,
            score_display=score_display,
        )
        # run tasks - 2 codepaths, one for the traditional task at a time
        # (w/ optional multiple models) and the other for true multi-task
        # (which requires different scheduling and UI)
        run_id = uuid()
        task_definitions = len(resolved_tasks) // len(model)
        parallel = 1 if (task_definitions == 1 or max_tasks is None) else max_tasks
        # single task definition (could be multi-model) or max_tasks capped to 1
        if parallel == 1:
            results: list[EvalLog] = []
            for sequence in range(0, task_definitions):
                task_batch = list(
                    filter(lambda t: t.sequence == sequence, resolved_tasks)
                )
                results.extend(
                    await eval_run(
                        run_id=run_id,
                        tasks=task_batch,
                        parallel=parallel,
                        eval_config=eval_config,
                        eval_sandbox=sandbox,
                        recorder=recorder,
                        epochs_reducer=epochs_reducer,
                        solver=solver,
                        tags=tags,
                        score=score,
                        debug_errors=debug_errors is True,
                        **kwargs,
                    )
                )
                # exit the loop if there was a cancellation
                if any([result.status == "cancelled" for result in results]):
                    break
            # return list of eval logs
            logs = EvalLogs(results)
        # multiple task definitions AND tasks not capped at 1
        else:
            results = await eval_run(
                run_id=run_id,
                tasks=resolved_tasks,
                parallel=parallel,
                eval_config=eval_config,
                eval_sandbox=sandbox,
                recorder=recorder,
                epochs_reducer=epochs_reducer,
                solver=solver,
                tags=tags,
                score=score,
                **kwargs,
            )
            logs = EvalLogs(results)
    finally:
        _eval_async_running = False
    # return logs
    return logs
def eval_retry(
    tasks: str | EvalLogInfo | EvalLog | list[str] | list[EvalLogInfo] | list[EvalLog],
    log_level: str | None = None,
    log_level_transcript: str | None = None,
    log_dir: str | None = None,
    log_format: Literal["eval", "json"] | None = None,
    max_samples: int | None = None,
    max_tasks: int | None = None,
    max_subprocesses: int | None = None,
    max_sandboxes: int | None = None,
    sandbox_cleanup: bool | None = None,
    trace: bool | None = None,
    display: DisplayType | None = None,
    fail_on_error: bool | float | None = None,
    debug_errors: bool | None = None,
    log_samples: bool | None = None,
    log_images: bool | None = None,
    log_buffer: int | None = None,
    score: bool = True,
    score_display: bool | None = None,
    max_retries: int | None = None,
    timeout: int | None = None,
    max_connections: int | None = None,
) -> list[EvalLog]:
    """Retry a previously failed evaluation task.
    Args:
        tasks: Log files for task(s) to retry.
        log_level: Level for logging to the console: "debug", "http", "sandbox",
            "info", "warning", "error", or "critical" (defaults to "warning")
        log_level_transcript: Level for logging to the log file (defaults to "info")
        log_dir: Output path for logging results
            (defaults to file log in ./logs directory).
        log_format: Format for writing log files (defaults
            to "eval", the native high-performance format).
        max_samples: Maximum number of samples to run in parallel
            (default is max_connections)
        max_tasks: Maximum number of tasks to run in parallel
            (defaults to number of models being evaluated)
        max_subprocesses: Maximum number of subprocesses to
            run in parallel (default is os.cpu_count())
        max_sandboxes: Maximum number of sandboxes (per-provider)
            to run in parallel.
        sandbox_cleanup: Cleanup sandbox environments after task completes
            (defaults to True)
        trace: Trace message interactions with evaluated model to terminal.
        display: Task display type (defaults to 'full').
        fail_on_error: `True` to fail on first sample error
            (default); `False` to never fail on sample errors; Value between 0 and 1
            to fail if a proportion of total samples fails. Value greater than 1 to fail
            eval if a count of samples fails.
        debug_errors: Raise task errors (rather than logging them)
            so they can be debugged (defaults to False).
        log_samples: Log detailed samples and scores (defaults to True)
        log_images: Log base64 encoded version of images,
            even if specified as a filename or URL (defaults to False)
        log_buffer: Number of samples to buffer before writing log file.
            If not specified, an appropriate default for the format and filesystem is
            chosen (10 for most all cases, 100 for JSON logs on remote filesystems).
        score: Score output (defaults to True)
        score_display: Show scoring metrics in realtime (defaults to True)
        max_retries:
            Maximum number of times to retry request.
        timeout:
            Request timeout (in seconds)
        max_connections:
            Maximum number of concurrent connections to Model API (default is per Model API)
    Returns:
        List of EvalLog (one for each task)
    """
    # standard platform init for top level entry points
    platform_init()
    # resolve eval trace
    max_tasks, max_samples = init_eval_display(display, trace, max_tasks, max_samples)
    async def run_task_app() -> list[EvalLog]:
        return await eval_retry_async(
            tasks=tasks,
            log_level=log_level,
            log_level_transcript=log_level_transcript,
            log_dir=log_dir,
            log_format=log_format,
            max_samples=max_samples,
            max_tasks=max_tasks,
            max_subprocesses=max_subprocesses,
            max_sandboxes=max_sandboxes,
            sandbox_cleanup=sandbox_cleanup,
            fail_on_error=fail_on_error,
            debug_errors=debug_errors,
            log_samples=log_samples,
            log_images=log_images,
            log_buffer=log_buffer,
            score=score,
            score_display=score_display,
            max_retries=max_retries,
            timeout=timeout,
            max_connections=max_connections,
        )
    return task_display().run_task_app(run_task_app)
async def eval_retry_async(
    tasks: str | EvalLogInfo | EvalLog | list[str] | list[EvalLogInfo] | list[EvalLog],
    log_level: str | None = None,
    log_level_transcript: str | None = None,
    log_dir: str | None = None,
    log_format: Literal["eval", "json"] | None = None,
    max_samples: int | None = None,
    max_tasks: int | None = None,
    max_subprocesses: int | None = None,
    max_sandboxes: int | None = None,
    sandbox_cleanup: bool | None = None,
    fail_on_error: bool | float | None = None,
    debug_errors: bool | None = None,
    log_samples: bool | None = None,
    log_images: bool | None = None,
    log_buffer: int | None = None,
    score: bool = True,
    score_display: bool | None = None,
    max_retries: int | None = None,
    timeout: int | None = None,
    max_connections: int | None = None,
) -> list[EvalLog]:
    """Retry a previously failed evaluation task.
    Args:
        tasks: (str | EvalLogInfo | EvalLog | list[str] | list[EvalLogInfo] | list[EvalLog]):
            Log files for task(s) to retry.
        log_level (str | None): Level for logging to the console: "debug", "http", "sandbox",
          "info", "warning", "error", or "critical" (defaults to "warning")
        log_level_transcript (str | None): Level for logging to the log file (defaults to "info")
        log_dir (str | None): Output path for logging results
           (defaults to file log in ./logs directory).
        log_format (Literal["eval", "json"] | None): Format for writing log files (defaults
           to "eval", the native high-performance format).
        max_samples (int | None): Maximum number of samples to run in parallel
           (default is max_connections)
        max_tasks (int | None): Maximum number of tasks to run in parallel
           (default is 1)
        max_subprocesses (int): Maximum number of subprocesses to
           run in parallel (default is os.cpu_count())
        max_sandboxes (int): Maximum number of sandboxes (per-provider) to run in parallel.
        sandbox_cleanup (bool | None): Cleanup sandbox environments after task completes
           (defaults to True)
        fail_on_error (bool | float | None): `True` to fail on first sample error
           (default); `False` to never fail on sample errors; Value between 0 and 1
           to fail if a proportion of total samples fails. Value greater than 1 to fail
           eval if a count of samples fails.
        debug_errors (bool | None): Raise task errors (rather than logging them)
           so they can be debugged (defaults to False).
        log_samples: (bool | None): Log detailed samples and scores (defaults to True)
        log_images: (bool | None): Log base64 encoded version of images,
           even if specified as a filename or URL (defaults to False)
        log_buffer: (int | None): Number of samples to buffer before writing log file.
           If not specified, an appropriate default for the format and filesystem is
           chosen (10 for most all cases, 100 for JSON logs on remote filesystems).
        score (bool): Score output (defaults to True)
        score_display (bool | None): Show scoring metrics in realtime (defaults to True)
        max_retries (int | None):
           Maximum number of times to retry request.
        timeout: (int | None):
           Request timeout (in seconds)
        max_connections (int | None):
           Maximum number of concurrent connections to Model API (default is per Model API)
    Returns:
        List of EvalLog (one for each task)
    """
    # resolve into a list of eval logs
    if isinstance(tasks, EvalLogInfo):
        tasks = [tasks]
    elif isinstance(tasks, EvalLog):
        tasks = [tasks]
    elif isinstance(tasks, str):
        tasks = [tasks]
    retry_eval_logs = [
        (
            task
            if isinstance(task, EvalLog)
            else (
                await read_eval_log_async(task.name)
                if isinstance(task, EvalLogInfo)
                else await read_eval_log_async(task)
            )
        )
        for task in tasks
    ]
    # eval them in turn
    eval_logs: list[EvalLog] = []
    for eval_log in retry_eval_logs:
        # the task needs to be either filesystem or registry
        # based in order to do a retry (we don't have enough
        # context to reconstruct ephemeral Task instances)
        task: str | None
        task_id = eval_log.eval.task_id
        task_name = eval_log.eval.task
        task_file = eval_log.eval.task_file
        if task_file:
            if not Path(task_file).exists():
                raise FileNotFoundError(f"Task file '{task_file}' not found")
            task = f"{task_file}@{task_name}"
        else:
            if registry_lookup("task", task_name) is None:
                raise FileNotFoundError(f"Task '{task_name}' not found.")
            task = task_name
        # see if there is solver spec in the eval log
        solver = (
            SolverSpec(eval_log.eval.solver, eval_log.eval.solver_args or {})
            if eval_log.eval.solver
            else None
        )
        # collect the rest of the params we need for the eval
        model = eval_log.eval.model
        model_base_url = eval_log.eval.model_base_url
        model_args = eval_log.eval.model_args
        task_args = eval_log.eval.task_args
        tags = eval_log.eval.tags
        limit = eval_log.eval.config.limit
        sample_id = eval_log.eval.config.sample_id
        epochs = (
            Epochs(eval_log.eval.config.epochs, eval_log.eval.config.epochs_reducer)
            if eval_log.eval.config.epochs
            else None
        )
        approval = eval_log.eval.config.approval
        message_limit = eval_log.eval.config.message_limit
        token_limit = eval_log.eval.config.token_limit
        time_limit = eval_log.eval.config.time_limit
        working_limit = eval_log.eval.config.working_limit
        max_samples = max_samples or eval_log.eval.config.max_samples
        max_tasks = max_tasks or eval_log.eval.config.max_tasks
        max_subprocesses = max_subprocesses or eval_log.eval.config.max_subprocesses
        max_sandboxes = max_sandboxes or eval_log.eval.config.max_sandboxes
        sandbox_cleanup = (
            sandbox_cleanup
            if sandbox_cleanup is not None
            else eval_log.eval.config.sandbox_cleanup
        )
        fail_on_error = (
            fail_on_error
            if fail_on_error is not None
            else eval_log.eval.config.fail_on_error
        )
        log_samples = (
            log_samples if log_samples is not None else eval_log.eval.config.log_samples
        )
        log_images = (
            log_images if log_images is not None else eval_log.eval.config.log_images
        )
        log_buffer = (
            log_buffer if log_buffer is not None else eval_log.eval.config.log_buffer
        )
        score_display = (
            score_display
            if score_display is not None
            else eval_log.eval.config.score_display
        )
        config = eval_log.plan.config
        config.max_retries = max_retries or config.max_retries
        config.timeout = timeout or config.timeout
        config.max_connections = max_connections or config.max_connections
        # run the eval
        log = (
            await eval_async(
                tasks=PreviousTask(
                    id=task_id, task=task, task_args=task_args, model=None, log=eval_log
                ),
                model=model,
                model_base_url=model_base_url,
                model_args=model_args,
                task_args=task_args,
                sandbox=eval_log.eval.sandbox,
                sandbox_cleanup=sandbox_cleanup,
                solver=solver,
                tags=tags,
                approval=approval,
                log_level=log_level,
                log_level_transcript=log_level_transcript,
                log_dir=log_dir,
                log_format=log_format,
                limit=limit,
                sample_id=sample_id,
                epochs=epochs,
                fail_on_error=fail_on_error,
                debug_errors=debug_errors,
                message_limit=message_limit,
                token_limit=token_limit,
                time_limit=time_limit,
                working_limit=working_limit,
                max_samples=max_samples,
                max_tasks=max_tasks,
                max_subprocesses=max_subprocesses,
                max_sandboxes=max_sandboxes,
                log_samples=log_samples,
                log_images=log_images,
                log_buffer=log_buffer,
                score=score,
                score_display=score_display,
                **dict(config),
            )
        )[0]
        # add it to our results
        eval_logs.append(log)
    return EvalLogs(eval_logs)
def eval_init(
    tasks: Tasks,
    model: str | Model | list[str] | list[Model] | None | NotGiven = NOT_GIVEN,
    model_base_url: str | None = None,
    model_args: dict[str, Any] | str = dict(),
    task_args: dict[str, Any] | str = dict(),
    sandbox: SandboxEnvironmentType | None = None,
    approval: str | list[ApprovalPolicy] | ApprovalPolicyConfig | None = None,
    max_subprocesses: int | None = None,
    log_level: str | None = None,
    log_level_transcript: str | None = None,
    **kwargs: Unpack[GenerateConfigArgs],
) -> tuple[list[Model], list[ApprovalPolicy] | None, list[ResolvedTask]]:
    # init eval context
    init_eval_context(log_level, log_level_transcript, max_subprocesses)
    # resolve model and task args
    model_args = resolve_args(model_args)
    task_args = resolve_args(task_args)
    # resolve model args from environment if not specified
    if len(model_args) == 0:
        env_model_args = os.environ.get("INSPECT_EVAL_MODEL_ARGS", None)
        if env_model_args:
            args = [arg.strip() for arg in env_model_args.split(" ")]
            model_args = parse_cli_args(args)
    # resolve models
    generate_config = GenerateConfig(**kwargs)
    models = resolve_models(model, model_base_url, model_args, generate_config)
    # resolve tasks (set active model to resolve uses of the
    # 'default' model in tools, solvers, and scorers)
    with task_display().suspend_task_app():
        resolved_tasks: list[ResolvedTask] = []
        for m in models:
            init_active_model(m, generate_config)
            resolved_tasks.extend(resolve_tasks(tasks, task_args, m, sandbox))
    # resolve approval
    if isinstance(approval, str | ApprovalPolicyConfig):
        approval = approval_policies_from_config(approval)
    init_tool_approval(approval)
    return models, approval, resolved_tasks
def init_eval_display(
    display: DisplayType | None,
    trace: bool | None,
    max_tasks: int | None,
    max_samples: int | None,
    model: Any = None,
) -> tuple[int | None, int | None]:
    # propagate any trace value to display_type
    if trace:
        warn_once(
            log,
            "WARNING: The --trace flag is deprecated (use --display=conversation instead)",
        )
        display = "conversation"
    # apply default and init
    display = display or display_type()
    init_display_type(display)
    # adapt task/samples as required if we are in conversation mode
    if display_type() == "conversation":
        # single task at a time
        if max_tasks is not None:
            max_tasks = 1
        # single sample at a time
        max_samples = 1
        # multiple models not allowed in trace mode
        if isinstance(model, list) and len(model) > 1:
            raise PrerequisiteError(
                "Conversation mode cannot be used when evaluating multiple models."
            )
    return max_tasks, max_samples
# A list of eval logs is returned from eval(). We've already displayed
# all of the output we need to to though, so we make the return
# value 'invisible'
class EvalLogs(list[EvalLog]):
    def _ipython_display_(self) -> None:
        pass
    def __repr__(self) -> str:
        return ""

================
File: src/inspect_ai/_eval/evalset.py
================
import hashlib
import logging
from copy import deepcopy
from typing import Any, Literal, NamedTuple, Set, cast
import rich
from pydantic_core import to_json
from rich.status import Status
from tenacity import (
    RetryCallState,
    Retrying,
    retry_if_not_result,
    stop_after_attempt,
    wait_exponential,
)
from typing_extensions import Unpack
from inspect_ai._util.error import PrerequisiteError
from inspect_ai._util.file import basename, filesystem
from inspect_ai._util.notgiven import NOT_GIVEN, NotGiven
from inspect_ai.approval._policy import ApprovalPolicy
from inspect_ai.log import EvalLog
from inspect_ai.log._bundle import bundle_log_dir
from inspect_ai.log._file import (
    EvalLogInfo,
    list_eval_logs,
    read_eval_log,
    read_eval_log_headers,
    write_log_dir_manifest,
)
from inspect_ai.model import (
    GenerateConfigArgs,
    Model,
)
from inspect_ai.model._generate_config import GenerateConfig
from inspect_ai.solver._solver import Solver, SolverSpec
from inspect_ai.util import DisplayType, SandboxEnvironmentType
from inspect_ai.util._display import init_display_type
from .eval import eval, eval_init
from .loader import resolve_task_args
from .task import Epochs
from .task.resolved import ResolvedTask
from .task.task import PreviousTask
from .task.tasks import Tasks
logger = logging.getLogger(__name__)
class Log(NamedTuple):
    info: EvalLogInfo
    header: EvalLog
    task_identifier: str
def eval_set(
    tasks: Tasks,
    log_dir: str,
    retry_attempts: int | None = None,
    retry_wait: float | None = None,
    retry_connections: float | None = None,
    retry_cleanup: bool | None = None,
    model: str | Model | list[str] | list[Model] | None | NotGiven = NOT_GIVEN,
    model_base_url: str | None = None,
    model_args: dict[str, Any] | str = dict(),
    task_args: dict[str, Any] | str = dict(),
    sandbox: SandboxEnvironmentType | None = None,
    sandbox_cleanup: bool | None = None,
    solver: Solver | list[Solver] | SolverSpec | None = None,
    tags: list[str] | None = None,
    trace: bool | None = None,
    display: DisplayType | None = None,
    approval: str | list[ApprovalPolicy] | None = None,
    score: bool = True,
    log_level: str | None = None,
    log_level_transcript: str | None = None,
    log_format: Literal["eval", "json"] | None = None,
    limit: int | tuple[int, int] | None = None,
    sample_id: str | int | list[str | int] | None = None,
    epochs: int | Epochs | None = None,
    fail_on_error: bool | float | None = None,
    debug_errors: bool | None = None,
    message_limit: int | None = None,
    token_limit: int | None = None,
    time_limit: int | None = None,
    working_limit: int | None = None,
    max_samples: int | None = None,
    max_tasks: int | None = None,
    max_subprocesses: int | None = None,
    max_sandboxes: int | None = None,
    log_samples: bool | None = None,
    log_images: bool | None = None,
    log_buffer: int | None = None,
    bundle_dir: str | None = None,
    bundle_overwrite: bool = False,
    **kwargs: Unpack[GenerateConfigArgs],
) -> tuple[bool, list[EvalLog]]:
    r"""Evaluate a set of tasks.
    Args:
        tasks: Task(s) to evaluate. If None, attempt
            to evaluate a task in the current working directory
        log_dir: Output path for logging results
            (required to ensure that a unique storage scope is assigned for the set).
        retry_attempts: Maximum number of retry attempts before giving up
            (defaults to 10).
        retry_wait: Time to wait between attempts, increased exponentially.
            (defaults to 30, resulting in waits of 30, 60, 120, 240, etc.). Wait time
            per-retry will in no case by longer than 1 hour.
        retry_connections: Reduce max_connections at this rate with each retry
            (defaults to 0.5)
        retry_cleanup: Cleanup failed log files after retries
            (defaults to True)
        model: Model(s) for evaluation. If not specified use the value of the INSPECT_EVAL_MODEL
            environment variable. Specify `None` to define no default model(s), which will
            leave model usage entirely up to tasks.
        model_base_url: Base URL for communicating
            with the model API.
        model_args: Model creation args
            (as a dictionary or as a path to a JSON or YAML config file)
        task_args: Task creation arguments
            (as a dictionary or as a path to a JSON or YAML config file)
        sandbox: Sandbox environment type
            (or optionally a str or tuple with a shorthand spec)
        sandbox_cleanup: Cleanup sandbox environments after task completes
            (defaults to True)
        solver: Alternative solver(s) for
            evaluating task(s). ptional (uses task solver by default).
        tags: Tags to associate with this evaluation run.
        trace: Trace message interactions with evaluated model to terminal.
        display: Task display type (defaults to 'full').
        approval: Tool use approval policies.
            Either a path to an approval policy config file or a list of approval policies.
            Defaults to no approval policy.
        score: Score output (defaults to True)
        log_level: Level for logging to the console: "debug", "http", "sandbox",
            "info", "warning", "error", or "critical" (defaults to "warning")
        log_level_transcript: Level for logging to the log file (defaults to "info")
        log_format: Format for writing
            log files (defaults to "eval", the native high-performance format).
        limit: Limit evaluated samples
            (defaults to all samples).
        sample_id: Evaluate specific sample(s) from the dataset.
        epochs: Epochs to repeat samples for and optional score
            reducer function(s) used to combine sample scores (defaults to "mean")
        fail_on_error: `True` to fail on first sample error
            (default); `False` to never fail on sample errors; Value between 0 and 1
            to fail if a proportion of total samples fails. Value greater than 1 to fail
            eval if a count of samples fails.
        debug_errors: Raise task errors (rather than logging them)
            so they can be debugged (defaults to False).
        message_limit: Limit on total messages used for each sample.
        token_limit: Limit on total tokens used for each sample.
        time_limit: Limit on clock time (in seconds) for samples.
        working_limit: Limit on working time (in seconds) for sample. Working
            time includes model generation, tool calls, etc. but does not include
            time spent waiting on retries or shared resources.
        max_samples: Maximum number of samples to run in parallel
            (default is max_connections)
        max_tasks: Maximum number of tasks to run in parallel
            (defaults to number of models being evaluated)
        max_subprocesses: Maximum number of subprocesses to
            run in parallel (default is os.cpu_count())
        max_sandboxes: Maximum number of sandboxes (per-provider)
            to run in parallel.
        log_samples: Log detailed samples and scores (defaults to True)
        log_images: Log base64 encoded version of images,
            even if specified as a filename or URL (defaults to False)
        log_buffer: Number of samples to buffer before writing log file.
            If not specified, an appropriate default for the format and filesystem is
            chosen (10 for most all cases, 100 for JSON logs on remote filesystems).
        bundle_dir: If specified, the log viewer and logs generated
            by this eval set will be bundled into this directory.
        bundle_overwrite: Whether to overwrite files in the bundle_dir.
            (defaults to False).
        **kwargs: Model generation options.
    Returns:
        Tuple of bool (whether all tasks completed successfully) and list of EvalLog
        (one for each task)
    """
    # helper function to run a set of evals
    def run_eval(tasks: list[ResolvedTask] | list[PreviousTask]) -> list[EvalLog]:
        # run evals
        results = eval(
            tasks=tasks,
            model=None,  # ResolvedTask/PreviousTask already carries its model
            model_base_url=model_base_url,
            model_args=model_args,
            task_args=task_args,
            sandbox=sandbox,
            sandbox_cleanup=sandbox_cleanup,
            solver=solver,
            tags=tags,
            trace=trace,
            display=display,
            approval=approval,
            log_level=log_level,
            log_level_transcript=log_level_transcript,
            log_dir=log_dir,
            log_format=log_format,
            limit=limit,
            sample_id=sample_id,
            epochs=epochs,
            fail_on_error=fail_on_error,
            debug_errors=debug_errors,
            message_limit=message_limit,
            token_limit=token_limit,
            time_limit=time_limit,
            working_limit=working_limit,
            max_samples=max_samples,
            max_tasks=max_tasks,
            max_subprocesses=max_subprocesses,
            max_sandboxes=max_sandboxes,
            log_samples=log_samples,
            log_images=log_images,
            log_buffer=log_buffer,
            score=score,
            **kwargs,
        )
        # check for cancelled
        if evals_cancelled(results):
            raise KeyboardInterrupt
        # if specified, bundle the output directory
        if bundle_dir:
            bundle_log_dir(
                log_dir=log_dir, output_dir=bundle_dir, overwrite=bundle_overwrite
            )
        # return results
        return results
    # initialise display (otherwise eval_init will set it to full)
    display = init_display_type(display)
    if display == "conversation":
        raise RuntimeError("eval_set cannot be used with conversation display.")
    # resolve tasks
    models, _, resolved_tasks = eval_init(
        tasks=tasks,
        model=model,
        model_base_url=model_base_url,
        model_args=model_args,
        task_args=task_args,
        sandbox=sandbox,
        max_subprocesses=max_subprocesses,
        log_level=log_level,
        log_level_transcript=log_level_transcript,
        **kwargs,
    )
    # ensure log_dir and list all logs
    fs = filesystem(log_dir)
    fs.mkdir(log_dir, exist_ok=True)
    # validate that:
    #  (1) All tasks have a unique identifier
    #  (2) All logs have identifiers that map to tasks
    validate_eval_set_prerequisites(resolved_tasks, list_all_eval_logs(log_dir))
    # resolve some parameters
    retry_connections = retry_connections or 0.5
    retry_cleanup = retry_cleanup is not False
    max_connections = starting_max_connections(models, GenerateConfig(**kwargs))
    max_tasks = max_tasks if max_tasks is not None else len(models)
    # prepare console/status
    console = rich.get_console()
    status: Status | None = None
    # before sleep
    def before_sleep(retry_state: RetryCallState) -> None:
        # compute/update next max_connections
        nonlocal max_connections
        max_connections = max(round(max_connections * retry_connections), 1)
        kwargs["max_connections"] = max_connections
        # print waiting status
        nonlocal status
        console.print("")
        msg = (
            f"Evals not complete, waiting {round(retry_state.upcoming_sleep)} "
            + "seconds before retrying...\n"
        )
        status = console.status(status_msg(msg), spinner="clock")
        status.start()
    def before(retry_state: RetryCallState) -> None:
        # clear waiting status
        nonlocal status
        if status is not None:
            status.stop()
            status = None
    # function which will be called repeatedly to attempt to complete
    # the evaluations. for this purpose we will divide tasks into:
    #   - tasks with no log at all (they'll be attempted for the first time)
    #   - tasks with a successful log (they'll just be returned)
    #   - tasks with failed logs (they'll be retried)
    def try_eval() -> list[EvalLog]:
        # list all logs currently in the log directory (update manifest if there are some)
        all_logs = list_all_eval_logs(log_dir)
        if len(all_logs) > 0:
            write_log_dir_manifest(log_dir)
        # see which tasks are yet to run (to complete successfully we need
        # a successful eval for every [task_file/]task_name/model combination)
        # for those that haven't run, schedule them into models => tasks groups
        log_task_identifiers = [log.task_identifier for log in all_logs]
        all_tasks = [(task_identifier(task), task) for task in resolved_tasks]
        pending_tasks = [
            task[1] for task in all_tasks if task[0] not in log_task_identifiers
        ]
        # we have some pending tasks yet to run, run them
        if len(pending_tasks) > 0:
            # run the tasks
            run_logs = run_eval(pending_tasks)
            # if this was the entire list of resolved tasks, return results
            if len(pending_tasks) == len(all_tasks):
                return run_logs
            # otherwise query the filesystem
            else:
                latest_logs = latest_completed_task_eval_logs(
                    logs=list_all_eval_logs(log_dir), cleanup_older=False
                )
                return [log.header for log in latest_logs]
        # all tasks have had an initial run, perform retries
        else:
            # look for retryable eval logs and cleave them into success/failed
            success_logs, failed_logs = list_latest_eval_logs(all_logs, retry_cleanup)
            # retry the failed logs (look them up in resolved_tasks)
            if len(failed_logs) > 0:
                # schedule the re-execution of the failed tasks
                failed_task_identifiers = [log.task_identifier for log in failed_logs]
                failed_tasks = [
                    task
                    for task in resolved_tasks
                    if task_identifier(task) in failed_task_identifiers
                ]
                # run previous tasks (no models passed b/c previous task already carries its model)
                retried_logs = run_eval(
                    tasks=as_previous_tasks(failed_tasks, failed_logs)
                )
                # return success
                return [log.header for log in success_logs] + retried_logs
            # no failed logs to retry, just return sucesss logs
            else:
                return [log.header for log in success_logs]
    # create retry policy
    retry = Retrying(
        retry=retry_if_not_result(all_evals_succeeded),
        retry_error_callback=return_last_value,
        reraise=True,
        stop=stop_after_attempt(10 if retry_attempts is None else retry_attempts),
        wait=wait_exponential(retry_wait or 30, max=(60 * 60)),
        before_sleep=before_sleep,
        before=before,
    )
    # execute w/ retry
    results = retry(try_eval)
    # final sweep to remove failed log files
    if retry_cleanup:
        cleanup_older_eval_logs(log_dir)
    # report final status
    success = all_evals_succeeded(results)
    if success:
        msg = status_msg(f"Completed all tasks in '{log_dir}' successfully")
    else:
        msg = status_msg(f"Did not successfully complete all tasks in '{log_dir}'.")
    console.print(f"{msg}")
    # update manifest
    write_log_dir_manifest(log_dir)
    # return status + results
    return success, results
# convert resolved tasks to previous tasks
def as_previous_tasks(
    tasks: list[ResolvedTask], failed_logs: list[Log]
) -> list[PreviousTask]:
    def task_to_failed_log(task: ResolvedTask) -> Log:
        resolved_task_identifier = task_identifier(task)
        return next(
            log
            for log in failed_logs
            if log.task_identifier == resolved_task_identifier
        )
    previous_tasks: list[PreviousTask] = []
    for task, log in zip(tasks, map(task_to_failed_log, tasks)):
        # NOTE: we used to try to recreate registry objects by
        # by just passing the task name, but that didn't work
        # when evals were run from another directory. we may
        # want to bring this back but we'd need to resolve the
        # directory issues.
        # deepcopy so the same instance is not run twice
        prev_task = deepcopy(task.task)
        previous_tasks.append(
            PreviousTask(
                id=log.header.eval.task_id,
                task=prev_task,
                task_args=resolve_task_args(task.task),
                model=task.model,
                log=read_eval_log(log.info),
            )
        )
    return previous_tasks
# filters to determine when we are done
def all_evals_succeeded(logs: list[EvalLog]) -> bool:
    return all([log.status == "success" for log in logs])
# filter for determining when we are done
def evals_cancelled(logs: list[EvalLog]) -> bool:
    return any([log.status == "cancelled" for log in logs])
# return last value if we get to the end
def return_last_value(retry_state: RetryCallState) -> list[EvalLog]:
    if retry_state.outcome:
        return cast(list[EvalLog], retry_state.outcome.result())
    else:
        return []
# list all eval logs
def list_all_eval_logs(log_dir: str) -> list[Log]:
    log_files = list_eval_logs(log_dir)
    log_headers = read_eval_log_headers(log_files)
    task_identifiers = [task_identifier(log_header) for log_header in log_headers]
    return [
        Log(info=info, header=header, task_identifier=task_identifier)
        for info, header, task_identifier in zip(
            log_files, log_headers, task_identifiers
        )
    ]
# get the latest logs (cleaning if requested). returns tuple of successful/unsuccessful
def list_latest_eval_logs(
    logs: list[Log], cleanup_older: bool
) -> tuple[list[Log], list[Log]]:
    latest_logs = latest_completed_task_eval_logs(
        logs=logs, cleanup_older=cleanup_older
    )
    success_logs = [log for log in latest_logs if log.header.status == "success"]
    failed_logs = [log for log in latest_logs if log.header.status != "success"]
    return (success_logs, failed_logs)
# cleanup logs that aren't the latest
def cleanup_older_eval_logs(log_dir: str) -> None:
    latest_completed_task_eval_logs(
        logs=list_all_eval_logs(log_dir), cleanup_older=True
    )
def latest_completed_task_eval_logs(
    logs: list[Log], cleanup_older: bool = False
) -> list[Log]:
    # collect logs by id
    logs_by_id: dict[str, list[Log]] = {}
    for log in logs:
        id = log.header.eval.task_id
        if id not in logs_by_id:
            logs_by_id[id] = []
        logs_by_id[id].append(log)
    # take the most recent completed log for each id
    latest_completed_logs: list[Log] = []
    for id, id_logs in logs_by_id.items():
        # continue if there are no target logs
        if len(id_logs) == 0:
            continue
        # sort by last file write time
        id_logs.sort(
            key=lambda id_log: (id_log[0].mtime if id_log[0].mtime else 0), reverse=True
        )
        # take the most recent
        latest_completed_logs.append(id_logs[0])
        # remove the rest if requested
        # (don't remove 'started' in case its needed for post-mortum debugging)
        if cleanup_older:
            fs = filesystem(id_logs[0][0].name)
            for id_log in id_logs[1:]:
                try:
                    if id_log.header.status != "started":
                        fs.rm(id_log.info.name)
                except Exception as ex:
                    logger.warning(f"Error attempt to remove '{id_log[0].name}': {ex}")
    return latest_completed_logs
# ensure that preconditions for eval_set are met:
#  (1) all tasks have unique identfiers (so we can pair task -> log file)
#  (2) all log files have identifiers that map to tasks (so we know we
#      are running in a log dir created for this eval_set)
def validate_eval_set_prerequisites(
    resolved_tasks: list[ResolvedTask], all_logs: list[Log]
) -> None:
    # do all resolved tasks have unique identfiers?
    task_identifiers: Set[str] = set()
    for task in resolved_tasks:
        identifier = task_identifier(task)
        if identifier in task_identifiers:
            raise PrerequisiteError(
                f"[bold]ERROR[/bold]: The task '{task.task.name}' is not distinct.\n\nTasks in an eval_set must have distinct names OR use the @task decorator and have distinct combinations of name and task args. Solvers passed to tasks should also use the @solver decorator."
            )
        else:
            task_identifiers.add(identifier)
    # do all logs in the log directory correspond to task identifiers?
    for log in all_logs:
        if log.task_identifier not in task_identifiers:
            raise PrerequisiteError(
                f"[bold]ERROR[/bold]: Existing log file '{basename(log.info.name)}' in log_dir is not "
                + "associated with a task passed to eval_set (you must run eval_set "
                + "in a fresh log directory)"
            )
# yield a unique identifier for a task (used to pair resolved tasks to log files)
def task_identifier(task: ResolvedTask | EvalLog) -> str:
    if isinstance(task, ResolvedTask):
        task_file = task.task_file or ""
        task_name = task.task.name
        task_args = task.task_args
        model = str(task.model)
    else:
        task_file = task.eval.task_file or ""
        task_name = task.eval.task
        task_args = task.eval.task_args
        model = str(task.eval.model)
    # hash for task args
    task_args_hash = hashlib.sha256(
        to_json(task_args, exclude_none=True, fallback=lambda _x: None)
    ).hexdigest()
    if task_file:
        return f"{task_file}@{task_name}#{task_args_hash}/{model}"
    else:
        return f"{task_name}#{task_args_hash}/{model}"
def task_identifier_without_model(identifier: str) -> str:
    parts = identifier.split("/")
    parts = parts[:-2]
    identifier = "/".join(parts)
    return identifier
class ModelList:
    def __init__(self, models: list[Model]) -> None:
        self.models = models
    def __hash__(self) -> int:
        # Hash based on the result of the key function
        return hash(self._key())
    def __eq__(self, other: object) -> bool:
        # Compare based on the result of the key function
        if not isinstance(other, ModelList):
            return False
        return self._key() == other._key()
    def __str__(self) -> str:
        return ",".join([str(model) for model in self.models])
    def _key(self) -> str:
        model_names = [str(model) for model in self.models]
        model_names.sort()
        return ",".join(model_names)
def starting_max_connections(models: list[Model], config: GenerateConfig) -> int:
    # if there is an explicit config use that
    if config.max_connections is not None:
        return config.max_connections
    # else take the smallest model max connections
    else:
        return min(
            models, key=lambda model: model.api.max_connections()
        ).api.max_connections()
def status_msg(msg: str) -> str:
    STATUS_FMT = "blue bold"
    return f"[{STATUS_FMT}]{msg}[/{STATUS_FMT}]"

================
File: src/inspect_ai/_eval/list.py
================
import os
import re
from logging import getLogger
from pathlib import Path
from typing import Callable
from inspect_ai._util.decorator import parse_decorators
from .task import TaskInfo
logger = getLogger(__name__)
def list_tasks(
    globs: str | list[str] = [],
    absolute: bool = False,
    root_dir: Path = Path.cwd(),
    filter: Callable[[TaskInfo], bool] | None = None,
) -> list[TaskInfo]:
    """List the tasks located at the specified locations.
    Args:
        globs (str | list[str]): File location(s). Can be
           globs (e.g. have bash-style wildcards).
        absolute (bool): Return absolute paths (defaults
           to False)
        root_dir (Path): Base directory to scan from
           (defaults to current working directory)
        filter (Callable[[TaskInfo], bool] | None):
           Filtering function.
    Returns:
        List of TaskInfo
    """
    # resolve globs
    globs = globs if isinstance(globs, list) else [globs]
    # build list of tasks to return
    tasks: list[TaskInfo] = []
    files = task_files(globs, root_dir)
    for task_file in files:
        tasks.extend(parse_tasks(task_file, root_dir, absolute))
    # filter if necessary
    tasks = [task for task in tasks if filter is None or filter(task)]
    # return sorted
    return sorted(tasks, key=lambda t: f"{t.file}@{t.name}")
def task_files(globs: list[str] = [], root_dir: Path | None = None) -> list[Path]:
    # root dir
    root_dir = root_dir if root_dir else Path.cwd()
    # no globs is cwds
    if len(globs) == 0:
        return tasks_in_dir(root_dir)
    # resolve the first level of globs
    paths: list[Path] = []
    for glob in globs:
        # we will have matched a set of directories and files
        # (depending on how the user wrote the globs). for
        # each file, add it to to our list if its a task file;
        # for each dir, recursively search it for task files
        expanded = list(root_dir.glob(glob))
        for path in expanded:
            if path.is_dir():
                paths.extend(tasks_in_dir(path))
            elif is_task_path(path):
                paths.append(path)
    return [path.absolute() for path in paths]
def tasks_in_dir(path: Path) -> list[Path]:
    paths: list[Path] = []
    for dir, dirnames, filenames in os.walk(path):
        # compute dir_path
        dir_path = Path(dir)
        # remove dirs that start with . or _
        dirnames[:] = [
            dirname for dirname in dirnames if not is_task_path_excluded(dirname)
        ]
        # select files w/ the right extension
        for filename in filenames:
            file_path = dir_path / filename
            if is_task_path(file_path):
                paths.append(file_path)
    return paths
excluded_pattern = re.compile("^[_\\.].*$")
def is_task_path_excluded(path: str) -> bool:
    return (
        re.match(excluded_pattern, path) is not None
        or path == "env"
        or path == "venv"
        or path == "tests"
    )
def is_task_path(path: Path) -> bool:
    return (
        path.suffix == ".py" or path.suffix == ".ipynb"
    ) and not is_task_path_excluded(path.name)
def parse_tasks(path: Path, root_dir: Path, absolute: bool) -> list[TaskInfo]:
    task_decorators = parse_decorators(path, "task")
    return [
        TaskInfo(
            file=task_path(path, root_dir, absolute),
            name=decorator[0],
            attribs=decorator[1],
        )
        for decorator in task_decorators
    ]
# manage relative vs. absolute paths
def task_path(path: Path, root_dir: Path, absolute: bool) -> str:
    if absolute:
        return path.resolve().as_posix()
    else:
        return path.relative_to(root_dir.resolve()).as_posix()

================
File: src/inspect_ai/_eval/loader.py
================
import ast
import contextlib
import inspect
import os
from importlib.machinery import SourceFileLoader
from importlib.util import module_from_spec, spec_from_loader
from logging import getLogger
from pathlib import Path
from types import ModuleType
from typing import Any, Callable, Tuple, cast
from typing_extensions import overload
from inspect_ai._eval.task.resolved import ResolvedTask
from inspect_ai._eval.task.util import task_file, task_run_dir
from inspect_ai._util._async import configured_async_backend
from inspect_ai._util.decorator import parse_decorators
from inspect_ai._util.error import PrerequisiteError
from inspect_ai._util.logger import warn_once
from inspect_ai._util.path import chdir_python, cwd_relative_path
from inspect_ai._util.registry import (
    RegistryInfo,
    is_registry_object,
    registry_create,
    registry_info,
    registry_lookup,
    registry_params,
)
from inspect_ai.model import Model
from inspect_ai.scorer._scorer import Scorer, ScorerSpec, scorer_create
from inspect_ai.solver._bridge import bridge
from inspect_ai.solver._solver import Solver, SolverSpec
from inspect_ai.util import SandboxEnvironmentSpec, SandboxEnvironmentType
from inspect_ai.util._sandbox.environment import (
    resolve_sandbox_environment,
)
from inspect_ai.util._sandbox.registry import registry_find_sandboxenv
from .list import task_files
from .registry import task_create
from .task import PreviousTask, Task, TaskInfo
from .task.constants import TASK_FILE_ATTR, TASK_RUN_DIR_ATTR
from .task.run import eval_log_sample_source
from .task.tasks import Tasks
logger = getLogger(__name__)
def resolve_tasks(
    tasks: Tasks,
    task_args: dict[str, Any],
    model: Model,
    sandbox: SandboxEnvironmentType | None,
) -> list[ResolvedTask]:
    def as_resolved_tasks(tasks: list[Task]) -> list[ResolvedTask]:
        return [
            ResolvedTask(
                task=task,
                task_args=resolve_task_args(task),
                task_file=task_file(task, relative=True),
                model=task.model or model,
                sandbox=resolve_task_sandbox(task, sandbox),
                sequence=sequence,
            )
            for sequence, task in enumerate(tasks)
        ]
    # reflect resolved tasks right back
    if isinstance(tasks, ResolvedTask):
        return [tasks]
    elif isinstance(tasks, list) and isinstance(tasks[0], ResolvedTask):
        return cast(list[ResolvedTask], tasks)
    # take empty lists out of play
    if isinstance(tasks, list) and len(tasks) == 0:
        return as_resolved_tasks(load_tasks(None, task_args))
    # simple cases of passing us Task objects
    if isinstance(tasks, Task):
        return as_resolved_tasks([tasks])
    elif isinstance(tasks, list) and isinstance(tasks[0], Task):
        return as_resolved_tasks(cast(list[Task], tasks))
    # simple case of passing us PreviousTask
    if isinstance(tasks, PreviousTask):
        tasks = [tasks]
    if isinstance(tasks, list) and isinstance(tasks[0], PreviousTask):
        # for previous tasks, prefer recreating from the registry (so we have
        # a fresh instance) but also allow recycling of task instances for
        # fully dynamic tasks
        previous_tasks = cast(list[PreviousTask], tasks)
        loaded_tasks: list[Task] = []
        loaded_tasks_args: list[dict[str, Any]] = []
        for previous_task in previous_tasks:
            if isinstance(previous_task.task, Task):
                loaded_task_args = previous_task.task_args
                loaded_task = previous_task.task
            else:
                loaded_task_args = previous_task.task_args
                loaded_task = load_tasks([previous_task.task], loaded_task_args)[0]
            loaded_tasks.append(loaded_task)
            loaded_tasks_args.append(loaded_task_args)
        return [
            ResolvedTask(
                task=loaded_task,
                task_args=loaded_task_args,
                task_file=previous_task.log.eval.task_file,
                model=previous_task.model or loaded_task.model or model,
                sandbox=previous_task.log.eval.sandbox,
                sequence=sequence,
                id=previous_task.id,
                sample_source=eval_log_sample_source(
                    previous_task.log, loaded_task.dataset
                ),
            )
            for sequence, loaded_task, loaded_task_args, previous_task in zip(
                range(0, len(loaded_tasks)),
                loaded_tasks,
                loaded_tasks_args,
                previous_tasks,
            )
        ]
    # convert TaskInfo to str
    if isinstance(tasks, TaskInfo):
        tasks = [tasks]
    if isinstance(tasks, list) and isinstance(tasks[0], TaskInfo):
        tasks = [f"{task.file}@{task.name}" for task in cast(list[TaskInfo], tasks)]
    # handle functions that return tasks (we get their registry name)
    if isinstance(tasks, list) and callable(tasks[0]):
        tasks = [registry_info(task).name for task in tasks]
    elif callable(tasks):
        tasks = [registry_info(tasks).name]
    # str to list[str]
    if isinstance(tasks, str):
        tasks = [tasks]
    # done! let's load the tasks
    return as_resolved_tasks(load_tasks(cast(list[str] | None, tasks), task_args))
def resolve_task_args(task: Task) -> dict[str, Any]:
    # was the task instantiated via the registry or a decorator?
    # if so then we can get the task_args from the registry.
    try:
        task_args = dict(registry_params(task))
        return task_args
    # if it wasn't instantiated via the registry or a decorator
    # then it will not be in the registy and not have formal
    # task args (as it was simply synthesized via ad-hoc code)
    except ValueError:
        return {}
def resolve_task_sandbox(
    task: Task, sandbox: SandboxEnvironmentType | None
) -> SandboxEnvironmentSpec | None:
    # do the resolution
    resolved_sandbox = resolve_sandbox_environment(sandbox) or task.sandbox
    # if we have a sandbox with no config, see if there are implcit
    # config files available for the provider
    if resolved_sandbox is not None:
        # look for default
        if resolved_sandbox.config is None:
            # get config files for this type
            sandboxenv_type = registry_find_sandboxenv(resolved_sandbox.type)
            config_files_fn = cast(
                Callable[..., list[str]], getattr(sandboxenv_type, "config_files")
            )
            config_files = config_files_fn()
            # probe for them in task src dir
            src_dir = task_run_dir(task)
            for config_file in config_files:
                config_file_path = os.path.join(src_dir, config_file)
                if os.path.isfile(config_file_path):
                    resolved_sandbox = SandboxEnvironmentSpec(
                        resolved_sandbox.type, config_file
                    )
                    break
        # resolve relative paths
        if isinstance(resolved_sandbox.config, str):
            file_path = Path(resolved_sandbox.config)
            if not file_path.is_absolute():
                file_path = Path(task_run_dir(task)) / file_path
                resolved_sandbox = SandboxEnvironmentSpec(
                    resolved_sandbox.type, file_path.as_posix()
                )
    # return resolved sandbox
    return resolved_sandbox
def load_tasks(
    task_specs: list[str] | None, task_args: dict[str, Any] = {}
) -> list[Task]:
    """Load one more more tasks (if no tasks are specified, load from the current working directory"""
    # load tasks
    return [
        spec
        for task_spec in (task_specs if task_specs else [Path.cwd().as_posix()])
        for spec in load_task_spec(task_spec, task_args)
    ]
def load_task_spec(task_spec: str, task_args: dict[str, Any] = {}) -> list[Task]:
    # task in a python package
    if registry_lookup("task", task_spec) is not None:
        # create the task from a python package
        return [task_create(task_spec, **task_args)]
    else:
        # load tasks from glob
        return create_tasks([task_spec], task_args)
def create_tasks(
    globs: list[str],
    task_args: dict[str, Any] = {},
    root_dir: Path | None = None,
) -> list[Task]:
    tasks: list[Task] = []
    root_dir = root_dir if root_dir is not None else Path.cwd()
    for glob in globs:
        # sometimes globs are direct references to files
        # that include an @ index. for this case directly
        # create the task (we also need to load the file
        # so the task is registered before we create it)
        spec_split = split_spec(glob)
        if spec_split[1] is not None:
            task_path = Path(spec_split[0])
            load_file_tasks(task_path.absolute())
            tasks.extend(create_file_tasks(task_path, [spec_split[1]], task_args))
        else:
            # if the glob is the root dir then set it to empty (will result in
            # enumeration of the root dir)
            target = [] if Path(glob).resolve() == root_dir.resolve() else [glob]
            files = task_files(target, root_dir)
            files = sorted(files, key=lambda f: f.as_posix())
            for file in files:
                tasks.extend(create_file_tasks(file, None, task_args))
    return tasks
def load_file_tasks(file: Path) -> None:
    with chdir_python(file.parent.as_posix()):
        _load_task_specs(file)
def create_file_tasks(
    file: Path,
    task_specs: list[str] | list[RegistryInfo] | None = None,
    task_args: dict[str, Any] = {},
) -> list[Task]:
    run_dir = file.parent.resolve().as_posix()
    with chdir_python(file.parent.as_posix()):
        # if we don't have task specs then go get them (also,
        # turn them into plain names)
        if task_specs is None:
            task_specs = _load_task_specs(file)
        # convert to plain names
        task_specs = [
            spec if isinstance(spec, str) else spec.name for spec in task_specs
        ]
        tasks: list[Task] = []
        for task_spec in task_specs:
            # create the task from the loaded source file and
            # note that it was loaded from this directory
            # (will be used later to ensure it runs in the directory)
            task = task_create(task_spec, **task_args)
            setattr(task, TASK_FILE_ATTR, file.as_posix())
            setattr(task, TASK_RUN_DIR_ATTR, run_dir)
            tasks.append(task)
            # warn that chdir is deprecated
            if "chdir" in task.attribs:
                if configured_async_backend() == "trio":
                    raise RuntimeError(
                        "The task 'chdir' attribute is not compatible with the trio async backend."
                    )
                warn_once(
                    logger,
                    "The 'chdir' task attribute is deprecated and will be removed in a future release "
                    + "(you should write your tasks to not depend on their runtime working directory)",
                )
        return tasks
# don't call this function directly, rather, call one of the
# higher level loading functions above (those functions
# change the working directory, this one does not b/c it is
# intended as a helper function)
def _load_task_specs(task_path: Path) -> list[str]:
    # load the module
    module = load_module(task_path, code_has_task)
    if module:
        # find the tasks in the module
        tasks = parse_decorators(task_path, "task")
        return [task[0] for task in tasks]
    else:
        return []
def split_spec(spec: str) -> tuple[str, str | None]:
    parts = spec.rsplit("@", 1)
    if len(parts) == 2:
        return parts[0], parts[1]
    else:
        return spec, None
@overload
def load_module(
    module_path: Path, filter: Callable[[str], bool]
) -> ModuleType | None: ...
@overload
def load_module(module_path: Path, filter: None = None) -> ModuleType: ...
def load_module(
    module_path: Path, filter: Callable[[str], bool] | None = None
) -> ModuleType | None:
    if module_path.suffix == ".py":
        # bail if the code doesn't pass the filter
        with open(module_path, "r", encoding="utf-8") as file:
            if filter and not filter(file.read()):
                return None
        module_name = module_path.as_posix()
        loader = SourceFileLoader(module_name, module_path.absolute().as_posix())
        spec = spec_from_loader(loader.name, loader)
        if not spec:
            raise ModuleNotFoundError(f"Module {module_name} not found")
        module = module_from_spec(spec)
        loader.exec_module(module)
        return module
    elif module_path.suffix == ".ipynb":
        try:
            from inspect_ai._util.notebook import NotebookLoader
        except ImportError:
            return None
        # bail if the code doesn't pass the filter
        def exec_filter(cells: list[str]) -> bool:
            code = "\n\n".join(cells)
            return not filter or filter(code)
        notebook_loader = NotebookLoader(exec_filter)
        return notebook_loader.load_module(module_path.as_posix())
    else:
        raise ModuleNotFoundError(
            f"Invalid extension for task file: {module_path.suffix}"
        )
def code_has_decorator(code: str, decorator: str) -> bool:
    try:
        tree = ast.parse(code)
        for node in ast.iter_child_nodes(tree):
            if isinstance(node, ast.FunctionDef):
                for dec in node.decorator_list:
                    if isinstance(dec, ast.Name):
                        if str(dec.id) == decorator:
                            return True
                    elif (
                        isinstance(dec, ast.Call)
                        and isinstance(dec.func, ast.Name)
                        and str(dec.func.id) == decorator
                    ):
                        return True
    except SyntaxError:
        pass
    return False
def code_has_task(code: str) -> bool:
    return code_has_decorator(code, "task")
def as_solver_spec(solver: Solver) -> SolverSpec:
    if not is_registry_object(solver):
        raise PrerequisiteError(
            f"The solver {getattr(solver, '__name__', '<unknown>')} was not created by a function decorated with @solver so cannot be recorded."
        )
    return SolverSpec(solver=registry_info(solver).name, args=registry_params(solver))
def solver_from_spec(spec: SolverSpec) -> Solver:
    # resolve @ reference
    solver_file, solver_name = parse_spec_str(spec.solver)
    # switch contexts if we are loading from a file
    create_cm = (
        chdir_python(solver_file.parent.as_posix())
        if solver_file is not None
        else contextlib.nullcontext()
    )
    # pretty solver name for error messages
    pretty_solver_file = (
        cwd_relative_path(solver_file.as_posix()) if solver_file else None
    )
    with create_cm:
        # if there is no solver file then just create from the registry by name
        if solver_file is None:
            if solver_name is None:
                raise ValueError(f"Unable to resolve solver name from {spec.solver}")
            return cast(Solver, registry_create("solver", solver_name, **spec.args))
        # we do have a solver file
        else:
            # load the module and parse decorators
            solver_module = load_module(solver_file)
            decorators = parse_decorators(solver_file, "solver")
            # if there is no solver_name see if we can discover it
            if solver_name is None:
                if len(decorators) == 1:
                    # decorator based solver
                    solver_name = decorators[0][0]
                elif len(decorators) == 0:
                    # see if we can find an agent based solver
                    functions = [
                        function
                        for function in inspect.getmembers(
                            solver_module, inspect.isfunction
                        )
                        if function[1].__module__ == solver_module.__name__
                    ]
                    agent_functions = [
                        function
                        for function in functions
                        if "agent" in function[0] and not function[0].startswith("_")
                    ]
                    if len(agent_functions) == 1:
                        # agent based solver
                        solver_name = agent_functions[0][0]
                    elif len(agent_functions) == 0:
                        raise PrerequisiteError(
                            f"The source file {pretty_solver_file} does not contain any @solver functions or agent functions."
                        )
                    else:
                        raise PrerequisiteError(
                            f"The source file {pretty_solver_file} has more than one agent function (qualify which agent using e.g. '{solver_file.name}@agent_fn')"
                        )
                else:
                    raise PrerequisiteError(
                        f"The source file {pretty_solver_file} has more than one @solver function (qualify which solver using e.g. '{solver_file.name}y@solver_fn')"
                    )
            # create decorator based solvers using the registry
            if any(solver[0] == solver_name for solver in decorators):
                return cast(Solver, registry_create("solver", solver_name, **spec.args))
            # create agent based solvers by calling the function and wrapping it in bridge()
            else:
                agent_fn = getattr(solver_module, solver_name, None)
                if inspect.isfunction(agent_fn):
                    return bridge.bridge(agent_fn(**spec.args))
                elif agent_fn is not None:
                    raise PrerequisiteError(
                        f"The object {solver_name} in file {pretty_solver_file} is not a Python function."
                    )
                else:
                    raise PrerequisiteError(
                        f"The function {solver_name} was not found in file {pretty_solver_file}."
                    )
def scorer_from_spec(spec: ScorerSpec, task_path: Path | None, **kwargs: Any) -> Scorer:
    """
    Load a scorer
    Args:
        spec: The scorer spec
        task_path: An optional path to the task file
        **kwargs: Additional keyword arguments passed to the scorer initialization
    Returns:
        Scorer: the loaded scorer
    Raises:
        PrerequisiteError: If the scorer cannot be found, loaded, or lacks required type annotations
    """
    # resolve @ reference
    scorer_file, scorer_name = parse_spec_str(spec.scorer)
    # switch contexts if we are loading from a file
    create_cm = (
        chdir_python(scorer_file.parent.as_posix())
        if scorer_file is not None
        else contextlib.nullcontext()
    )
    # pretty solver name for error messages
    pretty_scorer_file = (
        cwd_relative_path(scorer_file.as_posix()) if scorer_file else None
    )
    with create_cm:
        # is there a scorer file being provided? if not, load from registry
        if scorer_file is None:
            if scorer_name is None:
                raise ValueError(f"Unable to resolve scorer name from {spec.scorer}")
            try:
                return scorer_create(scorer_name, **kwargs)
            except ValueError:
                # We need a valid path to a scorer file to try to load the scorer from there
                if not task_path:
                    raise PrerequisiteError(
                        f"The scorer '{scorer_name}' couldn't be loaded. Please provide a path to the file containing the scorer using the '--scorer' parameter"
                    )
                task_pretty_path = task_path.as_posix()
                if not task_path.exists():
                    raise PrerequisiteError(
                        f"The scorer `{scorer_name}` couldn't be loaded. The file '{task_pretty_path}' was not found. Please provide a path to the file containing the scorer using the '--scorer' parameter"
                    )
                # We have the path to a file, so load that and try again
                try:
                    load_module(task_path)
                    scorer_fn = scorer_create(scorer_name, **kwargs)
                    # See if the scorer doesn't have type annotations. Currently the registry will not load
                    # the function without type annotations.
                    # TODO: We could consider calling this ourselves if we're certain it is what we're looking for
                    signature = inspect.signature(scorer_fn)
                    if signature.return_annotation is inspect.Signature.empty:
                        raise PrerequisiteError(
                            f"The scorer '{scorer_name}' in the file '{task_pretty_path}' requires return type annotations. Please add type annotations to load the scorer."
                        )
                    return scorer_fn
                except ValueError:
                    # we still couldn't load this, request the user provide a path
                    raise PrerequisiteError(
                        f"The scorer '{scorer_name}' in the file '{task_pretty_path}' couldn't be loaded. Please provide a path to the file containing the scorer using the '--scorer' parameter."
                    )
                except ModuleNotFoundError:
                    # we still couldn't load this, request the user provide a path
                    raise PrerequisiteError(
                        f"The scorer '{scorer_name}' in the file '{task_pretty_path}' couldn't be loaded. Please provide a path to the file containing the scorer using the '--scorer' parameter."
                    )
        # solver is a path, so load it that way
        else:
            load_module(scorer_file)
            decorators = parse_decorators(scorer_file, "scorer")
            # if there is no solver_name see if we can discover it
            if scorer_name is None:
                if len(decorators) == 1:
                    # decorator based solver
                    scorer_name = decorators[0][0]
                elif len(decorators) == 0:
                    raise PrerequisiteError(
                        f"The source file {pretty_scorer_file} does not contain any @scorer functions."
                    )
                else:
                    raise PrerequisiteError(
                        f"The source file {pretty_scorer_file} has more than one @solver function (qualify which solver using e.g. '{scorer_file.name}y@solver_fn')"
                    )
            # create decorator based solvers using the registry
            if any(solver[0] == scorer_name for solver in decorators):
                return scorer_create(scorer_name, **kwargs)
            else:
                raise PrerequisiteError(
                    f"The function {scorer_name} was not found in file {pretty_scorer_file}."
                )
def parse_spec_str(spec_str: str) -> Tuple[Path | None, str | None]:
    spec_split = split_spec(spec_str)
    if spec_split[1] is not None:
        file: Path | None = Path(spec_split[0]).resolve()
        name: str | None = spec_split[1]
    elif Path(spec_split[0]).exists():
        file = Path(spec_split[0]).resolve()
        name = None
    else:
        file = None
        name = spec_split[0]
    return file, name

================
File: src/inspect_ai/_eval/registry.py
================
import inspect
import logging
from functools import wraps
from pathlib import Path
from typing import Any, Callable, TypeVar, cast, overload
from inspect_ai._util.error import PrerequisiteError
from inspect_ai._util.package import get_installed_package_name
from inspect_ai._util.registry import (
    RegistryInfo,
    registry_add,
    registry_create,
    registry_info,
    registry_lookup,
    registry_name,
    registry_tag,
)
from .task import Task
from .task.constants import TASK_FILE_ATTR, TASK_RUN_DIR_ATTR
MODEL_PARAM = "model"
logger = logging.getLogger(__name__)
TaskType = TypeVar("TaskType", bound=Callable[..., Task])
def task_register(
    task: TaskType, name: str, attribs: dict[str, Any], params: list[str]
) -> TaskType:
    r"""Register a task.
    Args:
        task (TaskType):
            function that returns a Task or class
            deriving from Task
        name (str): Name of task
        attribs (dict[str,Any]): Attributes of task decorator
        params (list[str]): Task parameter names
    Returns:
        Task with registry attributes.
    """
    registry_add(
        task,
        RegistryInfo(
            type="task", name=name, metadata=dict(attribs=attribs, params=params)
        ),
    )
    return task
def task_create(name: str, **kwargs: Any) -> Task:
    r"""Create a Task based on its registered name.
    Tasks can be a function that returns a Task or a
    class deriving from Task.
    Args:
        name (str): Name of task (Optional, defaults to object name)
        **kwargs (dict): Optional creation arguments for the task
    Returns:
        Task with registry info attribute
    """
    # match kwargs params to signature (warn if param not found)
    # (note that we always pass the 'model' param but tasks aren't
    # required to consume it, so we don't warn for 'model')
    task = registry_lookup("task", name)
    if not task:
        raise PrerequisiteError(f"Task named '{name}' not found.")
    task_info = registry_info(task)
    task_params: list[str] = task_info.metadata["params"]
    task_args: dict[str, Any] = {}
    for param in kwargs.keys():
        if param in task_params:
            task_args[param] = kwargs[param]
            if "kwargs" in task_params:
                task_args[param] = kwargs[param]
            else:
                logger.warning(f"param '{param}' not used by task '{name}'")
    return cast(Task, registry_create("task", name, **task_args))
@overload
def task(func: TaskType) -> TaskType: ...
@overload
def task(
    *, name: str | None = ..., **attribs: Any
) -> Callable[[TaskType], TaskType]: ...
def task(*args: Any, name: str | None = None, **attribs: Any) -> Any:
    r"""Decorator for registering tasks.
    Args:
      *args: Function returning `Task` targeted by
        plain task decorator without attributes (e.g. `@task`)
      name (str | None):
        Optional name for task. If the decorator has no name
        argument then the name of the function
        will be used to automatically assign a name.
      **attribs: (dict[str,Any]): Additional task attributes.
    Returns:
        Task with registry attributes.
    """
    def create_task_wrapper(task_type: TaskType) -> TaskType:
        # Get the name and parameters of the task
        task_name = registry_name(task_type, name or getattr(task_type, "__name__"))
        params = list(inspect.signature(task_type).parameters.keys())
        # Create and return the wrapper function
        @wraps(task_type)
        def wrapper(*w_args: Any, **w_kwargs: Any) -> Task:
            # Create the task
            task_instance = task_type(*w_args, **w_kwargs)
            # Tag the task with registry information
            registry_tag(
                task_type,
                task_instance,
                RegistryInfo(
                    type="task",
                    name=task_name,
                    metadata=dict(attribs=attribs, params=params),
                ),
                *w_args,
                **w_kwargs,
            )
            # if its not from an installed package then it is a "local"
            # module import, so set its task file and run dir
            if get_installed_package_name(task_type) is None:
                module = inspect.getmodule(task_type)
                if module and hasattr(module, "__file__") and module.__file__:
                    file = Path(getattr(module, "__file__"))
                    setattr(task_instance, TASK_FILE_ATTR, file.as_posix())
                    setattr(task_instance, TASK_RUN_DIR_ATTR, file.parent.as_posix())
            # Return the task instance
            return task_instance
        # functools.wraps overrides the return type annotation of the inner function, so
        # we explicitly set it again
        wrapper.__annotations__["return"] = Task
        # Register the task and return the wrapper
        return task_register(
            task=cast(TaskType, wrapper), name=task_name, attribs=attribs, params=params
        )
    if args:
        # The decorator was used without arguments: @task
        func = args[0]
        return create_task_wrapper(func)
    else:
        # The decorator was used with arguments: @task(name="foo")
        def decorator(func: TaskType) -> TaskType:
            return create_task_wrapper(func)
        return decorator

================
File: src/inspect_ai/_eval/run.py
================
import functools
import logging
import os
import sys
from typing import Awaitable, Callable, Set, cast
from inspect_ai._util.trace import trace_action
if sys.version_info < (3, 11):
    from exceptiongroup import ExceptionGroup
import anyio
from shortuuid import uuid
from typing_extensions import Unpack
from inspect_ai._display import display
from inspect_ai._display.core.active import (
    clear_task_screen,
    init_task_screen,
)
from inspect_ai._display.core.display import TaskSpec
from inspect_ai._util._async import tg_collect
from inspect_ai._util.error import PrerequisiteError, exception_message
from inspect_ai._util.path import chdir
from inspect_ai._util.registry import registry_unqualified_name
from inspect_ai.dataset._dataset import Dataset
from inspect_ai.log import EvalConfig, EvalLog
from inspect_ai.log._recorders import Recorder
from inspect_ai.model import GenerateConfigArgs
from inspect_ai.model._model import ModelName
from inspect_ai.scorer._metric import to_metric_specs
from inspect_ai.scorer._reducer import ScoreReducer, reducer_log_names
from inspect_ai.scorer._reducer.registry import validate_reducer
from inspect_ai.scorer._scorer import as_scorer_spec
from inspect_ai.solver._solver import Solver, SolverSpec
from inspect_ai.util._sandbox.environment import (
    SandboxEnvironmentConfigType,
    SandboxEnvironmentSpec,
    SandboxEnvironmentType,
    TaskCleanup,
    TaskInit,
    resolve_sandbox_environment,
)
from inspect_ai.util._sandbox.registry import registry_find_sandboxenv
from .loader import (
    as_solver_spec,
    solver_from_spec,
)
from .task.log import TaskLogger
from .task.resolved import ResolvedTask
from .task.run import TaskRunOptions, task_run
from .task.rundir import task_run_dir_switching
from .task.sandbox import TaskSandboxEnvironment, resolve_sandbox_for_task
from .task.util import slice_dataset, task_chdir, task_run_dir
log = logging.getLogger(__name__)
async def eval_run(
    run_id: str,
    tasks: list[ResolvedTask],
    parallel: int,
    eval_config: EvalConfig,
    eval_sandbox: SandboxEnvironmentType | None,
    recorder: Recorder,
    epochs_reducer: list[ScoreReducer] | None = None,
    solver: Solver | SolverSpec | None = None,
    tags: list[str] | None = None,
    debug_errors: bool = False,
    score: bool = True,
    **kwargs: Unpack[GenerateConfigArgs],
) -> list[EvalLog]:
    # see if we need to use run_dir switching
    run_dir = task_run_dir(tasks[0].task)
    multiple_run_dirs = any([task_run_dir(task.task) != run_dir for task in tasks])
    tasks_chdir = any([task_chdir(task.task) is not None for task in tasks])
    has_sandbox = next((task.has_sandbox for task in tasks), None)
    # get cwd before switching to task dir
    eval_wd = os.getcwd()
    # ensure sample ids
    for resolved_task in tasks:
        # add sample ids to dataset if they aren't there (start at 1 not 0)
        task = resolved_task.task
        for id, sample in enumerate(task.dataset):
            if sample.id is None:
                sample.id = id + 1
        # Ensure sample ids are unique
        ensure_unique_ids(task.dataset)
    # run startup pass for the sandbox environments
    shutdown_sandbox_environments: Callable[[], Awaitable[None]] | None = None
    if has_sandbox:
        cleanup = eval_config.sandbox_cleanup is not False
        shutdown_sandbox_environments = await startup_sandbox_environments(
            resolve_sandbox_environment(eval_sandbox), tasks, eval_config, cleanup
        )
    # resolve solver and solver spec
    if isinstance(solver, Solver):
        eval_solver = solver
        eval_solver_spec = as_solver_spec(solver)
    elif isinstance(solver, SolverSpec):
        eval_solver = solver_from_spec(solver)
        eval_solver_spec = solver
    else:
        eval_solver = None
        eval_solver_spec = None
    # resolve the task scorers
    eval_scorer_specs = (
        [as_scorer_spec(scorer) for scorer in task.scorer]
        if task.scorer is not None
        else None
    )
    # resolve task metrics
    eval_metrics = to_metric_specs(task.metrics) if task.metrics is not None else None
    try:
        # create run tasks
        task_run_options: list[TaskRunOptions] = []
        for resolved_task in tasks:
            with chdir(task_run_dir(resolved_task.task)):
                # tasks can provide their epochs, message_limit,
                # token_limit, time_limit, and fail_on_error so broadcast these
                # into the eval config (so long as they aren't overriding a
                # value specified from eval() or the CLI)
                task = resolved_task.task
                task_eval_config = eval_config.model_copy()
                # epochs
                if task_eval_config.epochs is None:
                    task_eval_config.epochs = task.epochs
                else:
                    task.epochs = task_eval_config.epochs
                # epochs reducer
                if epochs_reducer is not None:
                    # override task (eval_config already reflects epochs_reducer)
                    task.epochs_reducer = epochs_reducer
                else:
                    # use task (eval_config needs to be updated to reflect task reducer)
                    task_eval_config.epochs_reducer = reducer_log_names(
                        task.epochs_reducer
                    )
                # validate task epochs
                if task.epochs and task.epochs_reducer:
                    for reducer in task.epochs_reducer:
                        validate_reducer(task.epochs, reducer)
                # sample message limit
                if task_eval_config.message_limit is None:
                    task_eval_config.message_limit = task.message_limit
                else:
                    task.message_limit = task_eval_config.message_limit
                # sample token limit
                if task_eval_config.token_limit is None:
                    task_eval_config.token_limit = task.token_limit
                else:
                    task.token_limit = task_eval_config.token_limit
                # sample time limit
                if task_eval_config.time_limit is None:
                    task_eval_config.time_limit = task.time_limit
                else:
                    task.time_limit = task_eval_config.time_limit
                # sample execution limit
                if task_eval_config.working_limit is None:
                    task_eval_config.working_limit = task.working_limit
                else:
                    task.working_limit = task_eval_config.working_limit
                # fail_on_error
                if task_eval_config.fail_on_error is None:
                    task_eval_config.fail_on_error = task.fail_on_error
                else:
                    task.fail_on_error = task_eval_config.fail_on_error
                # create and track the logger
                logger = TaskLogger(
                    task_name=task.name,
                    task_version=task.version,
                    task_file=resolved_task.task_file,
                    task_id=resolved_task.id if resolved_task.id else uuid(),
                    run_id=run_id,
                    solver=eval_solver_spec,
                    tags=tags,
                    model=resolved_task.model,
                    dataset=task.dataset,
                    scorer=eval_scorer_specs,
                    metrics=eval_metrics,
                    sandbox=resolved_task.sandbox,
                    task_attribs=task.attribs,
                    task_args=resolved_task.task_args,
                    model_args=resolved_task.model.model_args,
                    eval_config=task_eval_config,
                    metadata=task.metadata,
                    recorder=recorder,
                )
                await logger.init()
                # append task
                task_run_options.append(
                    TaskRunOptions(
                        task=task,
                        model=resolved_task.model,
                        sandbox=resolved_task.sandbox,
                        logger=logger,
                        eval_wd=eval_wd,
                        config=task_eval_config,
                        solver=eval_solver,
                        tags=tags,
                        score=score,
                        debug_errors=debug_errors,
                        sample_source=resolved_task.sample_source,
                        kwargs=kwargs,
                    )
                )
        # multiple mode is for running/displaying multiple
        # task definitions, which requires some smart scheduling
        # to ensure that we spread work among models
        if tasks_chdir:
            if parallel > 1:
                if multiple_run_dirs:
                    with task_run_dir_switching():
                        return await run_multiple(task_run_options, parallel)
                else:
                    with chdir(run_dir):
                        return await run_multiple(task_run_options, parallel)
            # single mode is for a single task definitions (which
            # could in turn be executed for multiple models)
            else:
                with chdir(run_dir):
                    return await run_single(task_run_options, debug_errors)
        else:
            if parallel > 1:
                return await run_multiple(task_run_options, parallel)
            else:
                return await run_single(task_run_options, debug_errors)
    finally:
        # shutdown sandbox environments
        if shutdown_sandbox_environments:
            try:
                await shutdown_sandbox_environments()
            except BaseException as ex:
                log.warning(
                    f"Error occurred shutting down sandbox environments: {exception_message(ex)}"
                )
# single mode -- run a single logical task (could consist of multiple
# executable tasks if we are evaluating against multiple models)
async def run_single(tasks: list[TaskRunOptions], debug_errors: bool) -> list[EvalLog]:
    async with display().task_screen(task_specs(tasks), parallel=False) as screen:
        # init ui
        init_task_screen(screen)
        results: list[tuple[int, EvalLog]] = []
        try:
            async with anyio.create_task_group() as tg:
                async def run_task(index: int) -> None:
                    result = await task_run(tasks[index])
                    results.append((index, result))
                for i in range(0, len(tasks)):
                    tg.start_soon(run_task, i)
        # exceptions can escape when debug_errors is True and that's okay
        except ExceptionGroup as ex:
            if debug_errors:
                raise ex.exceptions[0]
            else:
                raise
        except anyio.get_cancelled_exc_class():
            # child tasks have already each handled this and updated results
            pass
        finally:
            # clear ui
            clear_task_screen()
        # sort results by original index and return just the values
        return [r for _, r in sorted(results)]
# multiple mode -- run multiple logical tasks (requires some smart
# schedluing to ensure that we are spreading work among models)
async def run_multiple(tasks: list[TaskRunOptions], parallel: int) -> list[EvalLog]:
    # track current usage of each model
    models: Set[str] = set()
    for task in tasks:
        models.add(str(task.model))
    model_counts = {model: 0 for model in models}
    # setup pending tasks, queue, and results
    pending_tasks = tasks.copy()
    results: list[EvalLog] = []
    tasks_completed = 0
    total_tasks = len(tasks)
    # produce/consume tasks
    send_channel, receive_channel = anyio.create_memory_object_stream[TaskRunOptions](
        parallel * 2
    )
    # find a task that keeps as many different models as possible running concurrently
    async def enque_next_task() -> bool:
        if tasks_completed < total_tasks:
            # filter out models that have no pending tasks
            models_with_pending = {
                model
                for model in model_counts
                if any(str(t.model) == model for t in pending_tasks)
            }
            if not models_with_pending:
                return False
            # among those models, pick one with the least usage
            model = min(models_with_pending, key=lambda m: model_counts[m])
            # now we know theres at least one pending task for this model so its safe to pick it
            next_task = next(t for t in pending_tasks if str(t.model) == model)
            pending_tasks.remove(next_task)
            model_counts[str(next_task.model)] += 1
            with trace_action(
                log, "Enque Task", f"task: {next_task.task.name} ({next_task.model})"
            ):
                await send_channel.send(next_task)
            return True
        else:
            return False
    async def worker() -> None:
        try:
            nonlocal tasks_completed
            async for task_options in receive_channel:
                result: EvalLog | None = None
                # run the task
                try:
                    with trace_action(
                        log,
                        "Run Task",
                        f"task: {task_options.task.name} ({task_options.model})",
                    ):
                        tg_results = await tg_collect(
                            [functools.partial(task_run, task_options)]
                        )
                    # check for empty results list (indicates cancellation)
                    if len(tg_results) == 0:
                        # task was cancelled, break out of the worker loop
                        result = None
                    else:
                        result = tg_results[0]
                        results.append(result)
                except Exception as ex:
                    # errors generally don't escape from tasks (the exception being if an error
                    # occurs during the final write of the log)
                    log.error(
                        f"Task '{task_options.task.name}' encountered an error during finalisation: {ex}"
                    )
                # tracking
                tasks_completed += 1
                model_counts[str(task_options.model)] -= 1
                # if a task was cancelled we are done
                if not result or result.status == "cancelled":
                    break
                # check if there are more tasks to process
                if tasks_completed < total_tasks:
                    await enque_next_task()
                elif tasks_completed == total_tasks:
                    # all tasks are complete, close the stream
                    try:
                        await send_channel.aclose()
                    except anyio.ClosedResourceError:
                        # another worker might have already closed it
                        pass
        except anyio.EndOfStream:
            pass
    # with task display
    async with display().task_screen(task_specs(tasks), parallel=True) as screen:
        # init screen
        init_task_screen(screen)
        # Use anyio task group instead of manual task management
        try:
            async with anyio.create_task_group() as tg:
                # start worker tasks
                for _ in range(parallel):
                    tg.start_soon(worker)
                # enqueue initial set of tasks
                for _ in range(min(parallel, total_tasks)):
                    await enque_next_task()
        except anyio.get_cancelled_exc_class():
            pass
        finally:
            # Always ensure channels are closed
            try:
                await send_channel.aclose()
            except anyio.ClosedResourceError:
                pass
            try:
                await receive_channel.aclose()
            except anyio.ClosedResourceError:
                pass
            clear_task_screen()
        return results
async def startup_sandbox_environments(
    eval_sandbox: SandboxEnvironmentSpec | None,
    tasks: list[ResolvedTask],
    config: EvalConfig,
    cleanup: bool,
) -> Callable[[], Awaitable[None]]:
    # find unique sandboxenvs
    sandboxenvs: Set[TaskSandboxEnvironment] = set()
    for task in tasks:
        # resolve each sample and add to sandboxenvs
        dataset = slice_dataset(task.task.dataset, config.limit, config.sample_id)
        for sample in dataset:
            sandbox = resolve_sandbox_for_task(eval_sandbox, task.task, sample)
            if sandbox is not None and sandbox not in sandboxenvs:
                sandboxenvs.add(sandbox)
    # initialiase sandboxenvs (track cleanups)
    cleanups: list[tuple[TaskCleanup, SandboxEnvironmentConfigType | None, str]] = []
    with display().suspend_task_app():
        for sandboxenv in sandboxenvs:
            # find type
            sandboxenv_type = registry_find_sandboxenv(sandboxenv.sandbox.type)
            # run startup
            task_init = cast(TaskInit, getattr(sandboxenv_type, "task_init"))
            with chdir(sandboxenv.run_dir):
                await task_init("startup", sandboxenv.sandbox.config)
            # append cleanup method
            task_cleanup = cast(TaskCleanup, getattr(sandboxenv_type, "task_cleanup"))
            cleanups.append(
                (task_cleanup, sandboxenv.sandbox.config, sandboxenv.run_dir)
            )
    # return shutdown method
    async def shutdown() -> None:
        for cleanup_jobs in cleanups:
            try:
                cleanup_fn, config, task_run_dir = cleanup_jobs
                with chdir(task_run_dir):
                    await cleanup_fn("shutdown", config, cleanup)
            except BaseException as ex:
                log.warning(
                    f"Error occurred shutting down sandbox environments: {exception_message(ex)}"
                )
    return shutdown
def task_specs(tasks: list[TaskRunOptions]) -> list[TaskSpec]:
    return [
        TaskSpec(registry_unqualified_name(task.task.name), ModelName(task.model))
        for task in tasks
    ]
def ensure_unique_ids(dataset: Dataset) -> None:
    """
    Validates that all samples in the dataset have unique IDs.
    Raises a error if duplicates are found.
    Args:
        dataset (Datatset): The dataset
    Raises:
        PrerequisiteError: If duplicate IDs are found in the dataset.
    """
    seen_ids = set()
    for sample in dataset:
        if sample.id in seen_ids:
            raise PrerequisiteError(
                f"The dataset contains duplicate sample ids (duplicate id: {sample.id}). Please ensure each sample has a unique id."
            )
        seen_ids.add(sample.id)

================
File: src/inspect_ai/_eval/score.py
================
import functools
from copy import deepcopy
from pathlib import Path
from typing import Any, Callable, Literal, cast
import anyio
from inspect_ai._display import display
from inspect_ai._eval.loader import scorer_from_spec
from inspect_ai._util._async import tg_collect
from inspect_ai._util.platform import platform_init
from inspect_ai._util.registry import registry_create, registry_unqualified_name
from inspect_ai.log import (
    EvalLog,
)
from inspect_ai.log._log import EvalMetricDefinition
from inspect_ai.model import ModelName
from inspect_ai.scorer import Metric, Scorer, Target
from inspect_ai.scorer._metric import SampleScore
from inspect_ai.scorer._reducer import (
    ScoreReducer,
    ScoreReducers,
    create_reducers,
    reducer_log_names,
)
from inspect_ai.scorer._scorer import ScorerSpec, unique_scorer_name
from inspect_ai.solver import TaskState
from .task.results import eval_results
ScoreAction = Literal["append", "overwrite"]
def score(
    log: EvalLog,
    scorers: Scorer | list[Scorer],
    epochs_reducer: ScoreReducers | None = None,
    action: ScoreAction | None = None,
) -> EvalLog:
    """Score an evaluation log.
    Args:
       log (EvalLog): Evaluation log.
       scorers (Scorer): List of Scorers to apply to log
       epochs_reducer (ScoreReducers | None):
           Reducer function(s) for aggregating scores in each sample.
           Defaults to previously used reducer(s).
       action: Whether to append or overwrite this score
    Returns:
       Log with scores yielded by scorer.
    """
    # standard platform init for top level entry points
    platform_init()
    # resolve scorers into a list
    scorers = [scorers] if isinstance(scorers, Scorer) else scorers
    return anyio.run(score_async, log, scorers, epochs_reducer, action)
async def score_async(
    log: EvalLog,
    scorers: list[Scorer],
    epochs_reducer: ScoreReducers | None = None,
    action: ScoreAction | None = None,
) -> EvalLog:
    """Score an evaluation log.
    Args:
       log (EvalLog): Evaluation log.
       scorers (list[Scorer]): Scorers to apply to log
       epochs_reducer (ScoreReducers  | None):
         Reducer function(s) for aggregating scores in each sample.
         Defaults to previously used reducer(s).
       action: Whether to append or overwrite this score
    Returns:
       Log with scores yielded by scorer.
    """
    # deepcopy so we don't mutate the passed log
    log = deepcopy(log)
    # confirm we have samples
    if log.samples is None or len(log.samples) == 0:
        raise ValueError("There are no samples to score in the log.")
    # prime the scoring tasks
    states = [
        TaskState(
            model=ModelName(log.eval.model),
            sample_id=sample.id,
            epoch=sample.epoch,
            input=sample.input,
            target=Target(sample.target),
            choices=sample.choices,
            messages=sample.messages,
            output=sample.output,
            completed=True,
            metadata=sample.metadata,
        )
        for sample in log.samples
    ]
    with display().progress(total=len(states)) as p:
        def progress() -> None:
            p.update(1)
        # do scoring
        scores: list[dict[str, SampleScore]] = await tg_collect(
            [
                functools.partial(
                    run_score_task, state, Target(sample.target), scorers, progress
                )
                for (sample, state) in zip(log.samples, states)
            ]
        )
        # write them back (gather ensures that they come back in the same order)
        for index, score in enumerate(scores):
            if action == "overwrite":
                log.samples[index].scores = {k: v.score for k, v in score.items()}
            else:
                existing_scores = log.samples[index].scores or {}
                new_scores = {k: v.score for k, v in score.items()}
                for key, value in new_scores.items():
                    if key not in existing_scores:
                        existing_scores[key] = value
                    else:
                        # This key already exists, dedupe its name
                        count = 1
                        while f"{key}-{count}" in existing_scores.keys():
                            count = count + 1
                        existing_scores[f"{key}-{count}"] = value
                log.samples[index].scores = existing_scores
        # collect metrics from EvalLog (they may overlap w/ the scorer metrics,
        # that will be taken care of in eval_results)
        log_metrics = metrics_from_log(log)
        # override epochs_reducer if specified
        epochs_reducer = create_reducers(epochs_reducer)
        if epochs_reducer:
            log.eval.config.epochs_reducer = reducer_log_names(epochs_reducer)
        else:
            epochs_reducer = reducers_from_log(log)
        # compute metrics
        log.results, log.reductions = eval_results(
            len(log.samples), scores, epochs_reducer, scorers, log_metrics
        )
    return log
async def task_score(
    log: EvalLog,
    scorer: str | None = None,
    scorer_args: dict[str, Any] | None = None,
    action: ScoreAction | None = None,
) -> EvalLog:
    # confirm we have a scorer
    scorers = resolve_scorers(log, scorer, scorer_args)
    if len(scorers) == 0:
        raise ValueError(
            "Unable to resolve any scorers for this log. Please specify a scorer using the '--scorer' param."
        )
    # confirm we have samples
    if log.samples is None or len(log.samples) == 0:
        raise ValueError("There are no samples to score in the log.")
    task_name = log.eval.task
    display().print(f"\nScoring {task_name} ({len(log.samples)} samples)")
    # perform scoring
    log = await score_async(log=log, scorers=scorers, action=action)
    # compute and log metrics
    if log.samples:
        sample_scores = [
            {
                score_key: SampleScore(
                    score=score,
                    sample_id=sample.id,
                    sample_metadata=sample.metadata,
                )
                for score_key, score in sample.scores.items()
            }
            for sample in log.samples
            if sample.scores is not None
        ]
        epochs_reducer = reducers_from_log(log)
        metrics = metrics_from_log(log)
        log.results, log.reductions = eval_results(
            log.results.total_samples if log.results else 0,
            sample_scores,
            epochs_reducer,
            scorers,
            metrics,
        )
    return log
async def run_score_task(
    state: TaskState,
    target: Target,
    scorers: list[Scorer],
    progress: Callable[..., None],
) -> dict[str, SampleScore]:
    results: dict[str, SampleScore] = {}
    for scorer in scorers:
        result = await scorer(state, target)
        scorer_name = unique_scorer_name(scorer, list(results.keys()))
        results[scorer_name] = SampleScore(
            score=result,
            sample_id=state.sample_id,
            sample_metadata=state.metadata,
            scorer=registry_unqualified_name(scorer),
        )
    progress()
    return results
def metrics_from_log(log: EvalLog) -> list[Metric] | dict[str, list[Metric]] | None:
    # See if we have metrics in the eval itself
    if log.eval.metrics:
        if isinstance(log.eval.metrics, list):
            return [metric_from_log(metric) for metric in log.eval.metrics]
        else:
            return {
                key: [metric_from_log(metric) for metric in metrics]
                for key, metrics in log.eval.metrics.items()
            }
    return None
def metric_from_log(metric: EvalMetricDefinition) -> Metric:
    return cast(
        Metric, registry_create("metric", metric.name, **(metric.options or {}))
    )
def reducers_from_log(log: EvalLog) -> list[ScoreReducer] | None:
    return create_reducers(log.eval.config.epochs_reducer)
def resolve_scorers(
    log: EvalLog, scorer: str | None, scorer_args: dict[str, Any] | None
) -> list[Scorer]:
    """
    Create a list of Scorer objects from an evaluation log.
    Args:
        log: EvalLog object containing evaluation configuration and results
        scorer:: Scorer name (simple name or file.py@name).
        scorer_args: Dictionary of scorer arguments
    Returns:
        list[Scorer]: List of initialized scorers
    """
    # resolve the scorer path
    task_path = Path(log.eval.task_file) if log.eval.task_file else None
    # If there is an explicit scorer
    if scorer:
        return [
            scorer_from_spec(
                spec=ScorerSpec(scorer=scorer),
                task_path=task_path,
                **(scorer_args or {}),
            )
        ]
    # See if we can create scorers from the eval itself
    elif log.eval.scorers is not None:
        return (
            [
                scorer_from_spec(
                    spec=ScorerSpec(scorer=score.name),
                    task_path=task_path,
                    **(score.options or {}),
                )
                for score in log.eval.scorers
            ]
            if log.results
            else []
        )
    # Otherwise, perhaps we can re-create them from the results
    return (
        [
            scorer_from_spec(
                spec=ScorerSpec(scorer=score.name), task_path=task_path, **score.params
            )
            for score in log.results.scores
        ]
        if log.results
        else []
    )

================
File: src/inspect_ai/_eval/task/__init__.py
================
from .task import Task, TaskInfo, PreviousTask, task_with  # noqa: I001, F401
from .epochs import Epochs
__all__ = [
    "Epochs",
    "Task",
    "TaskInfo",
    "PreviousTask",
    "task_with",
]

================
File: src/inspect_ai/_eval/task/constants.py
================
TASK_FILE_ATTR = "__task_file__"
TASK_RUN_DIR_ATTR = "__task_run_dir__"

================
File: src/inspect_ai/_eval/task/epochs.py
================
from inspect_ai.scorer._reducer import ScoreReducers, create_reducers
class Epochs:
    """Task epochs.
    Number of epochs to repeat samples over and optionally one or more
    reducers used to combine scores from samples across epochs. If not
    specified the "mean" score reducer is used.
    """
    def __init__(self, epochs: int, reducer: ScoreReducers | None = None) -> None:
        """Task epochs.
        Args:
           epochs (int): Number of epochs
           reducer (ScoreReducers): One or more reducers used to combine
              scores from samples across epochs (defaults to "mean)
        """
        self.epochs = epochs
        self.reducer = create_reducers(reducer)

================
File: src/inspect_ai/_eval/task/error.py
================
from inspect_ai._util.error import EvalError
from inspect_ai.log._log import eval_error
class SampleErrorHandler:
    def __init__(self, fail_on_error: bool | float | None, total_samples: int) -> None:
        self.error_count = 0.0
        self.fail_on_error = True if fail_on_error is None else fail_on_error
        self.total_samples = float(total_samples)
    def __call__(self, ex: BaseException) -> tuple[EvalError, BaseException | None]:
        # increment error count
        self.error_count += 1
        # create error (we may return it)
        def sample_error(
            *, raise_error: bool
        ) -> tuple[EvalError, BaseException | None]:
            return eval_error(
                ex, type(ex), ex, ex.__traceback__
            ), ex if raise_error else None
        # check against limits
        if isinstance(self.fail_on_error, bool):
            return sample_error(raise_error=self.fail_on_error)
        else:
            if self.fail_on_error < 1:
                max_errors = self.fail_on_error * self.total_samples
                return sample_error(raise_error=self.error_count >= max_errors)
            else:
                return sample_error(raise_error=self.error_count >= self.fail_on_error)

================
File: src/inspect_ai/_eval/task/generate.py
================
from typing import Literal
from inspect_ai.model import (
    CachePolicy,
    GenerateConfig,
    Model,
    call_tools,
)
from inspect_ai.model._cache import epoch
from inspect_ai.solver import TaskState
from inspect_ai.solver._limit import SampleLimitExceededError
from inspect_ai.tool import ToolFunction
async def task_generate(
    model: Model,
    state: TaskState,
    tool_calls: Literal["loop", "single", "none"],
    cache: bool | CachePolicy,
    config: GenerateConfig,
) -> TaskState:
    # track tool_choice (revert to "auto" after first forced call of a tool)
    tool_choice = state.tool_choice
    try:
        while True:
            # If we don't update the epoch here as we go, it's entirely possible
            # we'd cache the same response for every single epoch, which would
            # completely defeat the point!
            epoch.set(state.epoch)
            # call the model
            state.output = await model.generate(
                input=state.messages,
                tools=state.tools,
                tool_choice=tool_choice,
                config=config,
                cache=cache,
            )
            # append the assistant message
            message = state.output.message
            state.messages.append(message)
            # check for completed
            if state.completed:
                return state
            # resolve tool calls if necessary
            if tool_calls != "none" and message.tool_calls:
                # call tools and append messages to state
                state.messages.extend(
                    await call_tools(message, state.tools, config.max_tool_output)
                )
                # check for completed or only executing a single tool call
                if state.completed or tool_calls == "single":
                    return state
                # if a tool_call was forced set tool_choice to 'auto'
                # (otherwise it will get forced over and over again)
                if isinstance(tool_choice, ToolFunction):
                    tool_choice = "auto"
            # no tool calls or not resolving tool calls, we are done!
            else:
                return state
    # propagate current state along with sample limit exceeded
    except SampleLimitExceededError as ex:
        raise ex.with_state(state)

================
File: src/inspect_ai/_eval/task/images.py
================
import functools
from inspect_ai._util._async import tg_collect
from inspect_ai._util.constants import BASE_64_DATA_REMOVED
from inspect_ai._util.content import Content, ContentAudio, ContentImage, ContentVideo
from inspect_ai._util.images import file_as_data_uri
from inspect_ai._util.url import is_data_uri
from inspect_ai.dataset import Sample
from inspect_ai.model import ChatMessage
from inspect_ai.solver import TaskState
async def states_with_base64_content(states: list[TaskState]) -> list[TaskState]:
    return await tg_collect(
        [functools.partial(state_with_base64_content, state) for state in states]
    )
async def state_with_base64_content(state: TaskState) -> TaskState:
    state.messages = await messages_with_base64_content(state.messages)
    return state
def state_without_base64_content(state: TaskState) -> TaskState:
    state.messages = messages_without_base64_content(state.messages)
    return state
async def samples_with_base64_content(samples: list[Sample]) -> list[Sample]:
    return await tg_collect(
        [functools.partial(sample_with_base64_content, sample) for sample in samples]
    )
async def sample_with_base64_content(sample: Sample) -> Sample:
    if isinstance(sample.input, list):
        return sample.model_copy(
            update={"input": await messages_with_base64_content(sample.input)}
        )
    else:
        return sample
def sample_without_base64_content(sample: Sample) -> Sample:
    if isinstance(sample.input, list):
        return sample.model_copy(
            update={"input": messages_without_base64_content(sample.input)}
        )
    else:
        return sample
async def messages_with_base64_content(
    messages: list[ChatMessage],
) -> list[ChatMessage]:
    return await tg_collect(
        [
            functools.partial(message_with_base64_content, message)
            for message in messages
        ]
    )
def messages_without_base64_content(messages: list[ChatMessage]) -> list[ChatMessage]:
    return [message_without_base64_content(message) for message in messages]
async def message_with_base64_content(message: ChatMessage) -> ChatMessage:
    if not isinstance(message.content, str):
        return message.model_copy(
            update=dict(
                content=[
                    await chat_content_with_base64_content(content)
                    for content in message.content
                ]
            )
        )
    else:
        return message
def message_without_base64_content(message: ChatMessage) -> ChatMessage:
    if not isinstance(message.content, str):
        return message.model_copy(
            update=dict(
                content=[
                    chat_content_without_base64_content(content)
                    for content in message.content
                ]
            )
        )
    else:
        return message
async def chat_content_with_base64_content(content: Content) -> Content:
    if isinstance(content, ContentImage):
        return ContentImage(
            image=await file_as_data_uri(content.image),
            detail=content.detail,
        )
    elif isinstance(content, ContentAudio):
        return ContentAudio(
            audio=await file_as_data_uri(content.audio), format=content.format
        )
    elif isinstance(content, ContentVideo):
        return ContentVideo(
            video=await file_as_data_uri(content.video), format=content.format
        )
    else:
        return content
def chat_content_without_base64_content(content: Content) -> Content:
    if isinstance(content, ContentImage) and is_data_uri(content.image):
        return ContentImage(image=BASE_64_DATA_REMOVED, detail=content.detail)
    elif isinstance(content, ContentAudio) and is_data_uri(content.audio):
        return ContentAudio(audio=BASE_64_DATA_REMOVED, format="mp3")
    elif isinstance(content, ContentVideo) and is_data_uri(content.video):
        return ContentVideo(video=BASE_64_DATA_REMOVED, format="mp4")
    else:
        return content

================
File: src/inspect_ai/_eval/task/log.py
================
from importlib import metadata as importlib_metadata
from inspect import isgenerator
from typing import Any, Iterator, Literal, cast
from shortuuid import uuid
from inspect_ai._eval.task.util import slice_dataset
from inspect_ai._util.constants import PKG_NAME
from inspect_ai._util.datetime import iso_now
from inspect_ai._util.git import git_context
from inspect_ai._util.path import cwd_relative_path
from inspect_ai._util.registry import (
    registry_log_name,
    registry_params,
)
from inspect_ai.dataset import Dataset
from inspect_ai.log import (
    EvalConfig,
    EvalDataset,
    EvalError,
    EvalPlan,
    EvalPlanStep,
    EvalResults,
    EvalRevision,
    EvalSample,
    EvalSpec,
    EvalStats,
)
from inspect_ai.log._log import (
    EvalLog,
    EvalMetricDefinition,
    EvalSampleReductions,
    EvalScorer,
    eval_config_defaults,
)
from inspect_ai.log._recorders import Recorder
from inspect_ai.model import (
    GenerateConfig,
    Model,
    ModelName,
)
from inspect_ai.model._model import model_usage
from inspect_ai.scorer._metric import MetricSpec
from inspect_ai.scorer._scorer import ScorerSpec
from inspect_ai.solver._plan import Plan
from inspect_ai.solver._solver import Solver, SolverSpec
from inspect_ai.util._sandbox.environment import SandboxEnvironmentSpec
class TaskLogger:
    def __init__(
        self,
        task_name: str,
        task_version: int,
        task_file: str | None,
        task_id: str | None,
        run_id: str,
        solver: SolverSpec | None,
        tags: list[str] | None,
        model: Model,
        dataset: Dataset,
        scorer: list[ScorerSpec] | None,
        metrics: list[MetricSpec] | dict[str, list[MetricSpec]] | None,
        sandbox: SandboxEnvironmentSpec | None,
        task_attribs: dict[str, Any],
        task_args: dict[str, Any],
        model_args: dict[str, Any],
        eval_config: EvalConfig,
        metadata: dict[str, Any] | None,
        recorder: Recorder,
    ) -> None:
        # determine versions
        git = git_context()
        revision = (
            EvalRevision(type="git", origin=git.origin, commit=git.commit)
            if git
            else None
        )
        packages = {PKG_NAME: importlib_metadata.version(PKG_NAME)}
        # redact authentication oriented model_args
        model_args = model_args.copy()
        if "api_key" in model_args:
            del model_args["api_key"]
        model_args = {k: v for k, v in model_args.items() if not k.startswith("aws_")}
        # don't try to serialise generators
        model_args = {
            k: v
            for k, v in model_args.items()
            if not isgenerator(v) and not isinstance(v, Iterator)
        }
        # cwd_relative_path for sandbox config
        if sandbox and isinstance(sandbox.config, str):
            sandbox = SandboxEnvironmentSpec(
                sandbox.type, cwd_relative_path(sandbox.config)
            )
        # ensure that the dataset has sample ids and record them
        sample_ids = cast(
            list[int | str],
            [
                sample.id
                for sample in slice_dataset(
                    dataset, eval_config.limit, eval_config.sample_id
                )
            ],
        )
        # write defaults for unspecified config
        for name, value in eval_config_defaults().items():
            if getattr(eval_config, name, None) is None:
                setattr(eval_config, name, value)
        # resolve scorers
        eval_scorers = resolve_eval_scorers(scorer)
        # resolve metrics
        eval_metrics = resolve_eval_metrics(metrics)
        # create eval spec
        self.eval = EvalSpec(
            run_id=run_id,
            created=iso_now(),
            task=f"{task_name}",
            task_id=task_id if task_id else uuid(),
            task_version=task_version,
            task_file=task_file,
            task_attribs=task_attribs,
            task_args=task_args,
            solver=solver.solver if solver else None,
            tags=tags,
            solver_args=solver.args if solver else None,
            model=str(ModelName(model)),
            model_base_url=model.api.base_url,
            dataset=EvalDataset(
                name=dataset.name,
                location=cwd_relative_path(dataset.location),
                samples=len(dataset),
                sample_ids=sample_ids,
                shuffled=dataset.shuffled,
            ),
            scorers=eval_scorers,
            metrics=eval_metrics,
            sandbox=sandbox,
            model_args=model_args,
            config=eval_config,
            revision=revision,
            packages=packages,
            metadata=metadata,
        )
        # stack recorder and location
        self.recorder = recorder
        # number of samples logged
        self._samples_completed = 0
        # size of flush buffer (how many samples we buffer before hitting storage)
        self.flush_buffer = eval_config.log_buffer or recorder.default_log_buffer()
        self.flush_pending = 0
    async def init(self) -> None:
        self._location = await self.recorder.log_init(self.eval)
    @property
    def location(self) -> str:
        return self._location
    @property
    def samples_completed(self) -> int:
        return self._samples_completed
    async def log_start(self, plan: EvalPlan) -> None:
        await self.recorder.log_start(self.eval, plan)
    async def log_sample(self, sample: EvalSample, *, flush: bool) -> None:
        # log the sample
        await self.recorder.log_sample(self.eval, sample)
        # flush if requested
        if flush:
            self.flush_pending += 1
            if self.flush_pending >= self.flush_buffer:
                await self.recorder.flush(self.eval)
                self.flush_pending = 0
        # track sucessful samples logged
        if sample.error is None:
            self._samples_completed += 1
    async def log_finish(
        self,
        status: Literal["success", "cancelled", "error"],
        stats: EvalStats,
        results: EvalResults | None = None,
        reductions: list[EvalSampleReductions] | None = None,
        error: EvalError | None = None,
    ) -> EvalLog:
        return await self.recorder.log_finish(
            self.eval, status, stats, results, reductions, error
        )
async def log_start(
    logger: TaskLogger,
    plan: Plan,
    config: GenerateConfig,
) -> None:
    def eval_plan_step(solver: Solver) -> EvalPlanStep:
        return EvalPlanStep(
            solver=registry_log_name(solver), params=registry_params(solver)
        )
    eval_plan = EvalPlan(
        name=plan.name,
        steps=[eval_plan_step(solver) for solver in plan.steps],
        finish=eval_plan_step(plan.finish) if plan.finish else None,
        config=config,
    )
    if plan.finish:
        eval_plan.steps.append(eval_plan_step(plan.finish))
    await logger.log_start(eval_plan)
def collect_eval_data(stats: EvalStats) -> None:
    # collect stats
    stats.completed_at = iso_now()
    stats.model_usage = model_usage()
def resolve_eval_metrics(
    metrics: list[MetricSpec] | dict[str, list[MetricSpec]] | None,
) -> list[EvalMetricDefinition] | dict[str, list[EvalMetricDefinition]] | None:
    if metrics is None:
        return None
    elif isinstance(metrics, list):
        return [EvalMetricDefinition(name=m.metric, options=m.args) for m in metrics]
    else:
        return {
            k: [
                EvalMetricDefinition(name=v.metric, options=v.args) for v in metric_list
            ]
            for k, metric_list in metrics.items()
        }
def resolve_eval_scorers(scorers: list[ScorerSpec] | None) -> list[EvalScorer] | None:
    if scorers is None:
        return None
    else:
        results = []
        for scorer in scorers:
            results.append(
                EvalScorer(
                    name=scorer.scorer,
                    metrics=resolve_scorer_metrics(scorer.metrics),
                    options=scorer.args,
                    metadata=scorer.metadata,
                )
            )
        return results
def resolve_scorer_metrics(
    metrics: list[MetricSpec | dict[str, list[MetricSpec]]]
    | dict[str, list[MetricSpec]]
    | None,
) -> (
    list[EvalMetricDefinition | dict[str, list[EvalMetricDefinition]]]
    | dict[str, list[EvalMetricDefinition]]
    | None
):
    if metrics is None:
        return None
    elif isinstance(metrics, list):
        resolved_metrics: list[
            EvalMetricDefinition | dict[str, list[EvalMetricDefinition]]
        ] = []
        for metric_item in metrics:
            if isinstance(metric_item, MetricSpec):
                resolved_metrics.append(
                    EvalMetricDefinition(
                        name=metric_item.metric, options=metric_item.args
                    )
                )
            elif isinstance(metric_item, dict):
                resolved_metrics.append(
                    {
                        metric_group: [
                            EvalMetricDefinition(
                                name=metric_spec.metric, options=metric_spec.args
                            )
                            for metric_spec in metric_specs
                        ]
                        for metric_group, metric_specs in metric_item.items()
                    }
                )
            else:
                raise TypeError(f"Unexpected item in list: {metric_item}")
        return resolved_metrics
    else:
        return {
            metric_group: [
                EvalMetricDefinition(name=metric_spec.metric, options=metric_spec.args)
                for metric_spec in metric_specs
            ]
            for metric_group, metric_specs in metrics.items()
        }

================
File: src/inspect_ai/_eval/task/resolved.py
================
from dataclasses import dataclass, field
from typing import Any, Set
from inspect_ai._eval.task import Task
from inspect_ai._eval.task.run import EvalSampleSource
from inspect_ai.model import Model
from inspect_ai.util import SandboxEnvironmentSpec
@dataclass(frozen=True)
class ResolvedTask:
    task: Task
    task_args: dict[str, Any]
    task_file: str | None
    model: Model
    sandbox: SandboxEnvironmentSpec | None
    sequence: int
    id: str | None = field(default=None)
    sample_source: EvalSampleSource | None = field(default=None)
    @property
    def has_sandbox(self) -> bool:
        if self.sandbox:
            return True
        else:
            return any(
                [True if sample.sandbox else False for sample in self.task.dataset]
            )
def resolved_model_names(tasks: list[ResolvedTask]) -> list[str]:
    models: Set[str] = set()
    for task in tasks:
        models.add(str(task.model))
    return list(models)

================
File: src/inspect_ai/_eval/task/results.py
================
import fnmatch
import inspect
import logging
import re
from collections import defaultdict
from copy import deepcopy
from dataclasses import dataclass, field
from typing import Any, Tuple, TypeGuard, cast, get_args, get_origin, get_type_hints
from inspect_ai._util.logger import warn_once
from inspect_ai._util.registry import (
    registry_info,
    registry_log_name,
    registry_params,
    registry_unqualified_name,
)
from inspect_ai.log import (
    EvalMetric,
    EvalResults,
    EvalSampleScore,
    EvalScore,
)
from inspect_ai.log._log import EvalSampleReductions
from inspect_ai.scorer import Metric, Score, Scorer
from inspect_ai.scorer._metric import (
    MetricDeprecated,
    MetricProtocol,
    SampleScore,
    Value,
)
from inspect_ai.scorer._metrics.accuracy import accuracy
from inspect_ai.scorer._metrics.std import stderr
from inspect_ai.scorer._reducer import ScoreReducer, mean_score, reducer_log_name
from inspect_ai.scorer._scorer import (
    SCORER_METRICS,
    scorer_metrics,
    unique_scorer_name,
)
logger = logging.getLogger(__name__)
@dataclass
class ScorerInfo:
    name: str
    metrics: list[Metric | dict[str, list[Metric]]] | dict[str, list[Metric]]
    params: dict[str, Any] = field(default_factory=dict)
    metadata: dict[str, Any] = field(default_factory=dict)
    @staticmethod
    def from_scorer(scorer: Scorer) -> "ScorerInfo":
        name = registry_unqualified_name(scorer)
        metrics = scorer_metrics(scorer)
        metadata = deepcopy(registry_info(scorer).metadata)
        del metadata[SCORER_METRICS]
        params = registry_params(scorer)
        return ScorerInfo(name=name, metrics=metrics, params=params, metadata=metadata)
    @staticmethod
    def from_name(name: str) -> "ScorerInfo":
        return ScorerInfo(name=name, metrics=[accuracy(), stderr()])
def eval_results(
    samples: int,
    scores: list[dict[str, SampleScore]],
    reducers: ScoreReducer | list[ScoreReducer] | None,
    scorers: list[Scorer] | None,
    metrics: list[Metric] | dict[str, list[Metric]] | None,
) -> Tuple[EvalResults, list[EvalSampleReductions] | None]:
    # initialise results
    results = EvalResults(total_samples=samples, completed_samples=len(scores))
    reductions = None
    # extract scorers info from scorers then create scorers info for any
    # scores not already accounted for by a scorer name
    scorers_info = [ScorerInfo.from_scorer(scorer) for scorer in (scorers or [])]
    scorer_names = {info.name for info in scorers_info}
    for sample_scores in scores:
        for name, sample_score in sample_scores.items():
            if sample_score.scorer is None and name not in scorer_names:
                scorers_info.append(ScorerInfo.from_name(name))
                scorer_names.add(name)
    # record scorer
    if len(scorers_info) > 0:
        result_scores: list[EvalScore] = []
        sample_reductions: list[EvalSampleReductions] = []
        for scorer_info in scorers_info:
            # this scorer
            scorer_name = unique_scorer_name(
                scorer_info.name, [eval_score.name for eval_score in result_scores]
            )
            # scores for this scorer
            resolved_scores = [
                score[scorer_name] for score in scores if scorer_name in score
            ]
            # Group the scores by sample_id
            reducers, use_reducer_name = resolve_reducer(reducers)
            for reducer in reducers:
                reducer_display_nm = (
                    reducer_log_name(reducer) if use_reducer_name else None
                )
                reduced_scores = reduce_scores(resolved_scores, reducer=reducer)
                # record this scorer's intermediate results
                reduced_samples = EvalSampleReductions(
                    scorer=scorer_name,
                    reducer=reducer_display_nm,
                    samples=[
                        EvalSampleScore(**ss.score.__dict__, sample_id=ss.sample_id)
                        for ss in reduced_scores
                    ],
                )
                sample_reductions.append(reduced_samples)
                # Compute metrics for this scorer
                targets = metrics if metrics is not None else scorer_info.metrics
                if isinstance(targets, list):
                    ## split the metrics into the simple metrics and any dictionary
                    ## metrics, to be processed independently
                    simple_metrics, dict_metrics = split_metrics(
                        cast(list[Metric | dict[str, list[Metric]]], targets)
                    )
                    # If there is a simple list of metrics
                    # just compute the metrics for this scorer
                    result_scores.extend(
                        scorer_for_metrics(
                            scorer_name=scorer_name,
                            scorer_info=scorer_info,
                            sample_scores=reduced_scores,
                            metrics=simple_metrics,
                            reducer_name=reducer_display_nm,
                        )
                    )
                    for dict_metric in dict_metrics:
                        result_scores.extend(
                            scorers_from_metric_dict(
                                scorer_name=scorer_name,
                                scorer_info=scorer_info,
                                sample_scores=reduced_scores,
                                metrics=dict_metric,
                                reducer_name=reducer_display_nm,
                            )
                        )
                else:
                    # If there is a dictionary of metrics, apply
                    # the metrics to the values within the scores
                    # (corresponding by key) and emit an EvalScorer for
                    # each key (which effectively creates multiple scorers
                    # by expanding a dictionary score value into multiple
                    # results with metrics)
                    result_scores.extend(
                        scorers_from_metric_dict(
                            scorer_name=scorer_name,
                            scorer_info=scorer_info,
                            sample_scores=reduced_scores,
                            metrics=targets,
                            reducer_name=reducer_display_nm,
                        )
                    )
            # build results
        results.scores = result_scores
        reductions = sample_reductions
    return results, reductions
def resolve_reducer(
    reducers: ScoreReducer | list[ScoreReducer] | None,
) -> tuple[list[ScoreReducer], bool]:
    if reducers is None:
        return ([mean_score()], False)
    else:
        return (reducers if isinstance(reducers, list) else [reducers], True)
def split_metrics(
    metrics: list[Metric | dict[str, list[Metric]]],
) -> tuple[list[Metric], list[dict[str, list[Metric]]]]:
    metric_list: list[Metric] = []
    dict_list: list[dict[str, list[Metric]]] = []
    for metric in metrics:
        if isinstance(metric, Metric):
            metric_list.append(metric)
        elif isinstance(metric, dict):
            dict_list.append(metric)
    return metric_list, dict_list
def scorer_for_metrics(
    scorer_name: str,
    scorer_info: ScorerInfo,
    sample_scores: list[SampleScore],
    metrics: list[Metric],
    reducer_name: str | None = None,
) -> list[EvalScore]:
    results: list[EvalScore] = []
    # we want to use simple names for metrics in the metrics dict
    # (i.e. without package prefixes). we do this by getting the
    # unqualified name, then appending a suffix if there are duplicates
    # this keeps the code straightforward and intuitive for users
    # programming against the log (e.g. metrics["accuracy"]) vs.
    # metrics["pkgname/accuracy"])
    list_metrics: dict[str, EvalMetric] = {}
    for metric in metrics:
        key = metrics_unique_key(
            registry_unqualified_name(metric), list(list_metrics.keys())
        )
        params = registry_params(metric)
        # process metric values
        if len(sample_scores) > 0:
            metric_value = call_metric(metric, sample_scores)
        else:
            metric_value = float("Nan")
        base_metric_name = registry_log_name(metric)
        # If the metric value is a dictionary, turn each of the entries
        # in the dictionary into a result
        if isinstance(metric_value, dict):
            for metric_key, value in metric_value.items():
                if value is not None:
                    name = metrics_unique_key(metric_key, list(list_metrics.keys()))
                    list_metrics[name] = EvalMetric(
                        name=name, value=float(value), params=params
                    )
        # If the metric value is a list, turn each element in the list
        # into a result
        elif isinstance(metric_value, list):
            for index, value in enumerate(metric_value):
                if value is not None:
                    count = str(index + 1)
                    name = metrics_unique_key(
                        with_suffix(key, count), list(list_metrics.keys())
                    )
                    list_metrics[name] = EvalMetric(
                        name=name, value=float(value), params=params
                    )
        # the metric is a float, str, or int
        else:
            list_metrics[key] = EvalMetric(
                name=base_metric_name, value=float(metric_value), params=params
            )
    # build results
    results.append(
        EvalScore(
            scorer=scorer_name,
            reducer=reducer_name,
            name=scorer_name,
            params=scorer_info.params,
            metadata=scorer_info.metadata
            if len(scorer_info.metadata.keys()) > 0
            else None,
            metrics=list_metrics,
        )
    )
    return results
def scorers_from_metric_dict(
    scorer_name: str,
    scorer_info: ScorerInfo,
    sample_scores: list[SampleScore],
    metrics: dict[str, list[Metric]],
    reducer_name: str | None = None,
) -> list[EvalScore]:
    results: list[EvalScore] = []
    # Expand any metric keys
    resolved_metrics = (
        resolve_glob_metric_keys(metrics, sample_scores[0].score)
        if len(sample_scores) > 0
        else metrics
    )
    for metric_key, metric_list in resolved_metrics.items():
        # filter scores to a list of scalars with the value of the metric name
        metric_scores: list[SampleScore] = []
        for sample_score in sample_scores:
            if isinstance(sample_score.score.value, dict):
                if metric_key in sample_score.score.value:
                    # Convert the score into a simple scalar value to apply metrics
                    metric_score = deepcopy(sample_score)
                    metric_score.score.value = cast(
                        float, sample_score.score.value[metric_key]
                    )
                    metric_scores.append(metric_score)
                else:
                    raise TypeError(
                        f"key '{metric_key}' isn't present in the score value dictionary"
                    )
            else:
                raise TypeError(
                    "A dictionary of metrics specified for a non-dictionary score"
                )
        result_metrics: dict[str, EvalMetric] = {}
        for target_metric in metric_list:
            # compute the metric value
            metric_name = registry_log_name(target_metric)
            metric_params = registry_params(target_metric)
            if len(metric_scores) > 0:
                value = call_metric(target_metric, metric_scores)
            else:
                value = float("Nan")
            # convert the value to a float (either by expanding the dict or array)
            # or by casting to a float
            if isinstance(value, dict):
                for key, val in value.items():
                    name = f"{metric_name}_{key}"
                    result_metrics[name] = EvalMetric(
                        name=name, value=cast(float, val), params=metric_params
                    )
            elif isinstance(value, list):
                for idx, item in enumerate(value):
                    name = f"{metric_name}_{idx}"
                    result_metrics[name] = EvalMetric(
                        name=name, value=cast(float, item), params=metric_params
                    )
            else:
                result_metrics[metric_name] = EvalMetric(
                    name=metric_name, value=cast(float, value), params=metric_params
                )
        # create a scorer result for this metric
        # TODO: What if there is separate simple scorer which has a name collision with
        # a score created by this scorer
        results.append(
            EvalScore(
                scorer=scorer_name,
                reducer=reducer_name,
                name=metric_key,
                params=scorer_info.params,
                metadata=scorer_info.metadata
                if len(scorer_info.metadata.keys()) > 0
                else None,
                metrics=result_metrics,
            )
        )
    return results
def call_metric(metric: Metric, sample_scores: list[SampleScore]) -> Value:
    if is_metric_deprecated(metric):
        warn_once(
            logger,
            f"Metric {registry_log_name(metric)} should be updated to take list[SampleScore]. "
            f"Metrics with list[Score] are deprecated.",
        )
        scores = [sample_score.score for sample_score in sample_scores]
        return metric(scores)
    else:
        metric = cast(MetricProtocol, metric)
        return metric(sample_scores)
def is_metric_deprecated(metric: Metric) -> TypeGuard[MetricDeprecated]:
    """Type guard to check if a metric follows the deprecated signature."""
    try:
        # signature and params
        sig = inspect.signature(metric)
        param_types = get_type_hints(metric)
        # there should be only one param, check it
        first_param = next(iter(sig.parameters.values()), None)
        if first_param is None:
            # No parameters, who knows what this is, treat it as deprecated
            return True
        expected_type: Any = param_types.get(first_param.name, None)
        if expected_type is None or expected_type is Any:
            # no helpful type info, treat it as deprecated
            return True
        # Extract generic base type and arguments to check if it matches list[Score]
        origin = get_origin(expected_type)
        args = get_args(expected_type)
        return origin is list and args == (Score,)
    except (AttributeError, ValueError, TypeError):
        return False
def resolve_glob_metric_keys(
    metrics: dict[str, list[Metric]], base_score: Score
) -> dict[str, list[Metric]]:
    if not isinstance(base_score.value, dict):
        # this value isn't a dictionary (unexpected)
        raise TypeError(
            "A dictionary of metrics was specified for a non-dictionary score. Dictionaries of metrics are only valid when the score value is a dictionary."
        )
    # Expand any metric keys
    resolved_metrics: dict[str, list[Metric]] = {}
    # the value is a dictionary, so go through the dictionary
    # and expand any metric globs into their literal values
    # and apply matching metrics to those keys
    for metric_key, metric_list in metrics.items():
        # compile the key as a glob into a regex and use that to match keys
        key_glob_re = re.compile(fnmatch.translate(metric_key))
        for score_key in base_score.value.keys():
            if key_glob_re.match(score_key):
                # The key matched, so either create a new entry for it and add metrics
                # or add metrics to the existing key
                resolved_metrics.setdefault(score_key, [])
                existing_metric_names = {
                    registry_log_name(m) for m in resolved_metrics[score_key]
                }
                # Add metrics that aren't already in the list
                for metric in metric_list:
                    metric_name = registry_log_name(metric)
                    if metric_name not in existing_metric_names:
                        resolved_metrics[score_key].append(metric)
                        existing_metric_names.add(metric_name)
    return resolved_metrics
def reduce_scores(
    scores: list[SampleScore], reducer: ScoreReducer
) -> list[SampleScore]:
    # Group the scores by sample_id
    grouped_scores: dict[str, list[SampleScore]] = defaultdict(list)
    for sample_score in scores:
        if sample_score.sample_id is not None:
            grouped_scores[str(sample_score.sample_id)].append(sample_score)
    # reduce the scores
    reduced_scores: list[SampleScore] = []
    for scores in grouped_scores.values():
        reduced = reducer([score.score for score in scores])
        reduced_scores.append(
            SampleScore(
                sample_id=scores[0].sample_id,
                sample_metadata=scores[0].sample_metadata,
                score=reduced,
            )
        )
    return reduced_scores
def metrics_unique_key(key: str, existing: list[str]) -> str:
    if key not in existing:
        return key
    else:
        key_index = 2
        pattern = re.compile(f"{re.escape(key)}(\\d+)")
        for existing_key in existing:
            match = pattern.match(existing_key)
            index = int(match.group(1)) if match else None
            if index and (index >= key_index):
                key_index = index + 1
        return f"{key}{key_index}"
def with_suffix(prefix: str, suffix: str) -> str:
    return prefix + "-" + suffix

================
File: src/inspect_ai/_eval/task/run.py
================
import contextlib
import functools
import sys
import time
from copy import deepcopy
from dataclasses import dataclass, field
from datetime import datetime
from logging import getLogger
from pathlib import PurePath
from typing import Callable, Literal
import anyio
from typing_extensions import Unpack
from inspect_ai._display import (
    TaskCancelled,
    TaskError,
    TaskProfile,
    TaskSuccess,
    display,
)
from inspect_ai._display.core.display import TaskDisplay, TaskDisplayMetric
from inspect_ai._util._async import tg_collect
from inspect_ai._util.constants import (
    DEFAULT_EPOCHS,
    DEFAULT_MAX_CONNECTIONS,
    SAMPLE_SUBTASK,
)
from inspect_ai._util.datetime import iso_now
from inspect_ai._util.error import exception_message
from inspect_ai._util.hooks import send_telemetry
from inspect_ai._util.registry import (
    is_registry_object,
    registry_log_name,
    registry_unqualified_name,
)
from inspect_ai._util.working import (
    init_sample_working_limit,
    sample_waiting_time,
)
from inspect_ai._view.notify import view_notify_eval
from inspect_ai.dataset import Dataset, Sample
from inspect_ai.log import (
    EvalConfig,
    EvalError,
    EvalLog,
    EvalResults,
    EvalSample,
    EvalStats,
)
from inspect_ai.log._condense import condense_sample
from inspect_ai.log._file import eval_log_json_str
from inspect_ai.log._log import EvalSampleLimit, EvalSampleReductions, eval_error
from inspect_ai.log._samples import active_sample
from inspect_ai.log._transcript import (
    ErrorEvent,
    SampleInitEvent,
    SampleLimitEvent,
    ScoreEvent,
    StepEvent,
    transcript,
)
from inspect_ai.model import (
    CachePolicy,
    GenerateConfig,
    GenerateConfigArgs,
    Model,
    ModelAPI,
    ModelName,
)
from inspect_ai.model._model import init_sample_model_usage, sample_model_usage
from inspect_ai.scorer import Scorer, Target
from inspect_ai.scorer._metric import Metric, SampleScore
from inspect_ai.scorer._reducer.types import ScoreReducer
from inspect_ai.scorer._score import init_scoring_context
from inspect_ai.scorer._scorer import unique_scorer_name
from inspect_ai.solver import Generate, Plan, TaskState
from inspect_ai.solver._chain import Chain, unroll
from inspect_ai.solver._fork import set_task_generate
from inspect_ai.solver._limit import SampleLimitExceededError
from inspect_ai.solver._solver import Solver
from inspect_ai.solver._task_state import sample_state, set_sample_state, state_jsonable
from inspect_ai.util._sandbox.context import sandbox_connections
from inspect_ai.util._sandbox.environment import SandboxEnvironmentSpec
from inspect_ai.util._subtask import init_subtask
from ..context import init_task_context
from ..task import Task
from .error import SampleErrorHandler
from .generate import task_generate
from .images import (
    sample_without_base64_content,
    samples_with_base64_content,
    state_without_base64_content,
    states_with_base64_content,
)
from .log import TaskLogger, collect_eval_data, log_start
from .results import eval_results
from .rundir import set_task_chdir
from .sandbox import sandboxenv_context
from .util import sample_messages, slice_dataset
py_logger = getLogger(__name__)
EvalSampleSource = Callable[[int | str, int], EvalSample | None]
# Units allocated for sample progress - the total units
# represents the total units of progress for an individual sample
# the remainder are increments of progress within a sample (and
# must sum to the total_progress_units when the sample is complete)
SAMPLE_TOTAL_PROGRESS_UNITS = 1
@dataclass
class TaskRunOptions:
    task: Task
    model: Model
    sandbox: SandboxEnvironmentSpec | None
    logger: TaskLogger
    eval_wd: str
    config: EvalConfig = field(default_factory=EvalConfig)
    solver: Solver | None = field(default=None)
    tags: list[str] | None = field(default=None)
    score: bool = field(default=True)
    debug_errors: bool = field(default=False)
    sample_source: EvalSampleSource | None = field(default=None)
    kwargs: GenerateConfigArgs = field(default_factory=lambda: GenerateConfigArgs())
async def task_run(options: TaskRunOptions) -> EvalLog:
    # destructure options
    task = options.task
    model = options.model
    sandbox = options.sandbox
    logger = options.logger
    eval_wd = options.eval_wd
    config = options.config
    solver = options.solver
    tags = options.tags
    score = options.score
    sample_source = options.sample_source
    kwargs = options.kwargs
    # resolve default generate_config for task
    generate_config = task.config.merge(GenerateConfigArgs(**kwargs))
    # init task context
    init_task_context(model, options.task.approval, generate_config)
    # establish chdir for duration of execution (if a task has chdir=True)
    with set_task_chdir(task):
        # track stats and error
        results: EvalResults | None = None
        reductions: list[EvalSampleReductions] | None = None
        stats = EvalStats(started_at=iso_now())
        # handle sample errors (raise as required)
        sample_error_handler = SampleErrorHandler(
            config.fail_on_error, len(task.dataset)
        )
        # resolve some config
        model_name = ModelName(model)
        epochs = config.epochs if config.epochs else DEFAULT_EPOCHS
        sandbox_cleanup = config.sandbox_cleanup is not False
        log_images = config.log_images is not False
        log_samples = config.log_samples is not False
        # resolve dataset
        _, samples, states = await resolve_dataset(
            dataset=task.dataset,
            model_name=model_name,
            limit=config.limit,
            sample_id=config.sample_id,
            epochs=epochs,
            log_images=log_images,
            message_limit=config.message_limit,
            token_limit=config.token_limit,
        )
        # resolve the plan (unroll chains)
        solver = solver or task.solver
        if isinstance(solver, Plan):
            plan = solver
        elif isinstance(solver, Chain):
            plan = Plan(list(solver), cleanup=task.cleanup, internal=True)
        else:
            plan = Plan(unroll(solver), cleanup=task.cleanup, internal=True)
        # add setup solver(s) if specified
        if task.setup:
            plan.steps = unroll(task.setup) + plan.steps
        # resolve the scorer
        score = score and task.scorer is not None
        scorers: list[Scorer] | None = task.scorer if (score and task.scorer) else None
        scorer_profiles = (
            [
                registry_log_name(scorer)
                for scorer in scorers
                if is_registry_object(scorer)
            ]
            if scorers is not None
            else ["(none)"]
        )
        # compute an eval directory relative log location if we can
        if PurePath(logger.location).is_relative_to(PurePath(eval_wd)):
            log_location = PurePath(logger.location).relative_to(eval_wd).as_posix()
        else:
            log_location = logger.location
        # create task profile for display
        profile = TaskProfile(
            name=task.name,
            file=logger.eval.task_file,
            model=model_name,
            dataset=task.dataset.name or "(samples)",
            scorer=", ".join(scorer_profiles),
            samples=len(samples),
            steps=len(samples) * SAMPLE_TOTAL_PROGRESS_UNITS,
            eval_config=config,
            task_args=logger.eval.task_args,
            generate_config=generate_config,
            tags=tags,
            log_location=log_location,
        )
        with display().task(
            profile,
        ) as td:
            try:
                # start the log
                await log_start(logger, plan, generate_config)
                with td.progress() as p:
                    # forward progress
                    def progress(number: int) -> None:
                        p.update(number)
                    # provide solvers a function that they can use to generate output
                    async def generate(
                        state: TaskState,
                        tool_calls: Literal["loop", "single", "none"] = "loop",
                        cache: bool | CachePolicy = False,
                        **kwargs: Unpack[GenerateConfigArgs],
                    ) -> TaskState:
                        return await task_generate(
                            model=model,
                            state=state,
                            tool_calls=tool_calls,
                            cache=cache,
                            config=generate_config.merge(kwargs),
                        )
                    # set generate for fork module
                    set_task_generate(generate)
                    # semaphore to limit concurrency
                    sample_semaphore = create_sample_semaphore(
                        config, generate_config, model.api
                    )
                    # track when samples complete and update progress as we go
                    progress_results: list[dict[str, SampleScore]] = []
                    update_metrics_display = update_metrics_display_fn(
                        td,
                        display_metrics=profile.eval_config.score_display is not False,
                    )
                    def sample_complete(sample_score: dict[str, SampleScore]) -> None:
                        # Capture the result
                        progress_results.append(sample_score)
                        # Increment the segment progress
                        td.sample_complete(
                            complete=len(progress_results), total=len(samples)
                        )
                        # Update metrics
                        update_metrics_display(
                            len(progress_results),
                            progress_results,
                            scorers,
                            task.epochs_reducer,
                            task.metrics,
                        )
                    # initial progress
                    td.sample_complete(complete=0, total=len(samples))
                    # Update metrics to empty state
                    update_metrics_display(
                        len(progress_results),
                        progress_results,
                        scorers,
                        task.epochs_reducer,
                        task.metrics,
                    )
                    sample_results = await tg_collect(
                        [
                            functools.partial(
                                task_run_sample,
                                task_name=task.name,
                                sample=sample,
                                state=state,
                                sandbox=sandbox,
                                max_sandboxes=config.max_sandboxes,
                                sandbox_cleanup=sandbox_cleanup,
                                plan=plan,
                                scorers=scorers,
                                generate=generate,
                                progress=progress,
                                logger=logger if log_samples else None,
                                log_images=log_images,
                                sample_source=sample_source,
                                sample_error=sample_error_handler,
                                sample_complete=sample_complete,
                                fails_on_error=(
                                    config.fail_on_error is None
                                    or config.fail_on_error is True
                                ),
                                time_limit=config.time_limit,
                                working_limit=config.working_limit,
                                semaphore=sample_semaphore,
                            )
                            for (sample, state) in zip(samples, states)
                        ]
                    )
                # compute and record metrics if we have scores
                completed_scores = [
                    score_dict
                    for score_dict in sample_results
                    if isinstance(score_dict, dict)
                ]
                if len(completed_scores) > 0:
                    results, reductions = eval_results(
                        samples=profile.samples,
                        scores=completed_scores,
                        reducers=task.epochs_reducer,
                        scorers=scorers,
                        metrics=task.metrics,
                    )
                # collect eval data
                collect_eval_data(stats)
                # finish w/ success status
                eval_log = await logger.log_finish(
                    "success", stats, results, reductions
                )
                # display task summary
                td.complete(
                    TaskSuccess(
                        samples_completed=logger.samples_completed,
                        stats=stats,
                        results=results or EvalResults(),
                    )
                )
            except anyio.get_cancelled_exc_class():
                with anyio.CancelScope(shield=True):
                    # collect eval data
                    collect_eval_data(stats)
                    # finish w/ cancelled status
                    eval_log = await logger.log_finish(
                        "cancelled", stats, results, reductions
                    )
                    # display task cancelled
                    td.complete(TaskCancelled(logger.samples_completed, stats))
            except BaseException as ex:
                if options.debug_errors:
                    raise
                else:
                    # get exception info
                    type, value, traceback = sys.exc_info()
                    type = type if type else BaseException
                    value = value if value else ex
                    # build eval error
                    error = eval_error(ex, type, value, traceback)
                    # collect eval data
                    collect_eval_data(stats)
                    # finish with error status
                    eval_log = await logger.log_finish(
                        "error", stats, results, reductions, error
                    )
                    # display it
                    td.complete(
                        TaskError(logger.samples_completed, type, value, traceback)
                    )
        # notify the view module that an eval just completed
        # (in case we have a view polling for new evals)
        view_notify_eval(logger.location)
        try:
            if (
                await send_telemetry("eval_log_location", eval_log.location)
                == "not_handled"
            ):
                # Converting the eval log to JSON is expensive. Only do so if
                # eval_log_location was not handled.
                await send_telemetry("eval_log", eval_log_json_str(eval_log))
        except Exception as ex:
            py_logger.warning(
                f"Error occurred sending telemetry: {exception_message(ex)}"
            )
        # return eval log
        return eval_log
def update_metrics_display_fn(
    td: TaskDisplay,
    initial_interval: float = 0,
    min_interval: float = 0.9,
    display_metrics: bool = True,
) -> Callable[
    [
        int,
        list[dict[str, SampleScore]],
        list[Scorer] | None,
        ScoreReducer | list[ScoreReducer] | None,
        list[Metric] | dict[str, list[Metric]] | None,
    ],
    None,
]:
    next_compute_time = time.perf_counter() + initial_interval
    def compute(
        sample_count: int,
        sample_scores: list[dict[str, SampleScore]],
        scorers: list[Scorer] | None,
        reducers: ScoreReducer | list[ScoreReducer] | None,
        metrics: list[Metric] | dict[str, list[Metric]] | None,
    ) -> None:
        # Don't compute metrics if they are not being displayed
        if not display_metrics:
            return None
        nonlocal next_compute_time
        time_start = time.perf_counter()
        if time_start >= next_compute_time:
            # compute metrics
            results, reductions = eval_results(
                samples=sample_count,
                scores=sample_scores,
                reducers=reducers,
                scorers=scorers,
                metrics=metrics,
            )
            # Name, reducer, value
            task_metrics = []
            if len(results.scores) > 0:
                for score in results.scores:
                    for key, metric in score.metrics.items():
                        task_metrics.append(
                            TaskDisplayMetric(
                                scorer=score.name,
                                name=metric.name,
                                value=metric.value,
                                reducer=score.reducer,
                            )
                        )
                td.update_metrics(task_metrics)
            # determine how long to wait before recomputing metrics
            time_end = time.perf_counter()
            elapsed_time = time_end - time_start
            wait = max(min_interval, elapsed_time * 10)
            next_compute_time = time_end + wait
    return compute
async def task_run_sample(
    task_name: str,
    sample: Sample,
    state: TaskState,
    sandbox: SandboxEnvironmentSpec | None,
    max_sandboxes: int | None,
    sandbox_cleanup: bool,
    plan: Plan,
    scorers: list[Scorer] | None,
    generate: Generate,
    progress: Callable[[int], None],
    logger: TaskLogger | None,
    log_images: bool,
    sample_source: EvalSampleSource | None,
    sample_error: SampleErrorHandler,
    sample_complete: Callable[[dict[str, SampleScore]], None],
    fails_on_error: bool,
    time_limit: int | None,
    working_limit: int | None,
    semaphore: anyio.Semaphore | None,
) -> dict[str, SampleScore] | None:
    # if there is an existing sample then tick off its progress, log it, and return it
    if sample_source and sample.id is not None:
        previous_sample = sample_source(sample.id, state.epoch)
        if previous_sample:
            # tick off progress for this sample
            progress(SAMPLE_TOTAL_PROGRESS_UNITS)
            # log if requested
            if logger:
                await logger.log_sample(previous_sample, flush=False)
            # return score
            sample_scores = (
                {
                    key: SampleScore(
                        score=score,
                        sample_id=previous_sample.id,
                        sample_metadata=previous_sample.metadata,
                    )
                    for key, score in previous_sample.scores.items()
                }
                if previous_sample.scores
                else {}
            )
            sample_complete(sample_scores)
            return sample_scores
    # use semaphore if provided
    semaphore_cm: anyio.Semaphore | contextlib.AbstractAsyncContextManager[None] = (
        semaphore if semaphore else contextlib.nullcontext()
    )
    # initialise subtask and scoring context
    init_sample_model_usage()
    set_sample_state(state)
    sample_transcript = init_subtask(SAMPLE_SUBTASK, state.store)
    if scorers:
        init_scoring_context(scorers, Target(sample.target))
    # use sandbox if provided
    sandboxenv_cm = (
        sandboxenv_context(task_name, sandbox, max_sandboxes, sandbox_cleanup, sample)
        if sandbox or sample.sandbox is not None
        else contextlib.nullcontext()
    )
    # helper to handle exceptions (will throw if we've exceeded the limit)
    def handle_error(ex: BaseException) -> tuple[EvalError, BaseException | None]:
        err = sample_error(ex)
        # if we aren't raising the error then print a warning
        if err[1] is None:
            py_logger.warning(
                f"Sample error (id: {sample.id}, epoch: {state.epoch}): {exception_message(ex)})"
            )
        transcript()._event(ErrorEvent(error=err[0]))
        return err
    # solver loop
    async with (
        semaphore_cm,
        active_sample(
            task=task_name,
            model=str(state.model),
            sample=sample,
            epoch=state.epoch,
            message_limit=state.message_limit,
            token_limit=state.token_limit,
            time_limit=time_limit,
            working_limit=working_limit,
            fails_on_error=fails_on_error,
            transcript=sample_transcript,
        ) as active,
    ):
        start_time: float | None = None
        error: EvalError | None = None
        raise_error: BaseException | None = None
        results: dict[str, SampleScore] = {}
        try:
            # begin init
            transcript()._event(StepEvent(action="begin", name="init"))
            # sample init event (remove file bodies as they have content or absolute paths)
            event_sample = sample.model_copy(
                update=dict(files={k: "" for k in sample.files.keys()})
                if sample.files
                else None
            )
            transcript()._event(
                SampleInitEvent(sample=event_sample, state=state_jsonable(state))
            )
            async with sandboxenv_cm:
                try:
                    # update active sample wth sandboxes now that we are initialised
                    active.sandboxes = await sandbox_connections()
                    # end init
                    transcript()._event(StepEvent(action="end", name="init"))
                    # initialise timeout context manager
                    timeout_cm = (
                        anyio.fail_after(time_limit)
                        if time_limit is not None
                        else contextlib.nullcontext()
                    )
                    # record start time
                    start_time = time.monotonic()
                    init_sample_working_limit(start_time, working_limit)
                    # run sample w/ optional timeout
                    with timeout_cm:
                        # mark started
                        active.started = datetime.now().timestamp()
                        # set progress for plan then run it
                        state = await plan(state, generate)
                except TimeoutError:
                    if time_limit is not None:
                        transcript()._event(
                            SampleLimitEvent(
                                type="time",
                                message=f"Sample completed: exceeded time limit ({time_limit:,} seconds)",
                                limit=time_limit,
                            )
                        )
                    else:
                        py_logger.warning(
                            "Unexpected timeout error reached top of sample stack. Are you handling TimeoutError when applying timeouts?"
                        )
                    # capture most recent state for scoring
                    state = sample_state() or state
                except anyio.get_cancelled_exc_class() as ex:
                    if active.interrupt_action:
                        # record event
                        transcript()._event(
                            SampleLimitEvent(
                                type="operator",
                                message="Sample completed: interrupted by operator",
                            )
                        )
                        # handle the action
                        match active.interrupt_action:
                            case "score":
                                # continue to scoring (capture the most recent state)
                                state = sample_state() or state
                            case "error":
                                # default error handling
                                error, raise_error = handle_error(ex)
                    else:
                        # task group provided by tg_collect will automatically
                        # handle the cancel exception
                        raise
                except SampleLimitExceededError as ex:
                    # sample limit event
                    transcript()._event(
                        SampleLimitEvent(
                            type=ex.type,
                            limit=ex.limit,
                            message=f"Sample completed: {ex.message}",
                        )
                    )
                    # capture most recent state for scoring
                    state = ex.state or sample_state() or state
                except BaseException as ex:
                    error, raise_error = handle_error(ex)
                # mark completed
                state.completed = True
                # set timeout for scoring. if the original timeout was hit we still
                # want to provide opportunity for scoring, but we don't necessarily
                # want to wait the full timeout again (especially in the case where
                # the cause of the timeout is a hung container and scoring requires
                # interacting with the container). as a middle ground we use half
                # of the original timeout value for scoring.
                if time_limit is not None:
                    timeout_cm = anyio.fail_after(time_limit / 2)
                # turn off message and token limits
                state.message_limit = None
                state.token_limit = None
                set_sample_state(state)
                # scoring
                try:
                    # timeout during scoring will result in an ordinary sample error
                    with timeout_cm:
                        if error is None:
                            for scorer in scorers or []:
                                scorer_name = unique_scorer_name(
                                    scorer, list(results.keys())
                                )
                                with transcript().step(name=scorer_name, type="scorer"):
                                    score_result = (
                                        await scorer(state, Target(sample.target))
                                        if scorer
                                        else None
                                    )
                                    if score_result is not None:
                                        sample_score = SampleScore(
                                            score=score_result,
                                            sample_id=sample.id,
                                            sample_metadata=sample.metadata,
                                            scorer=registry_unqualified_name(scorer),
                                        )
                                        transcript()._event(
                                            ScoreEvent(
                                                score=score_result, target=sample.target
                                            )
                                        )
                                        results[scorer_name] = sample_score
                            # add scores returned by solvers
                            if state.scores is not None:
                                for name, score in state.scores.items():
                                    results[name] = SampleScore(
                                        score=score,
                                        sample_id=state.sample_id,
                                        sample_metadata=state.metadata,
                                    )
                                    transcript()._event(
                                        ScoreEvent(score=score, target=sample.target)
                                    )
                            # propagate results into scores
                            state.scores = {k: v.score for k, v in results.items()}
                except anyio.get_cancelled_exc_class():
                    if active.interrupt_action:
                        transcript()._event(
                            SampleLimitEvent(
                                type="operator",
                                message="Unable to score sample due to operator interruption",
                            )
                        )
                    raise
                except BaseException as ex:
                    # note timeout
                    if isinstance(ex, TimeoutError):
                        transcript()._event(
                            SampleLimitEvent(
                                type="time",
                                message=f"Unable to score sample due to exceeded time limit ({time_limit:,} seconds)",
                                limit=time_limit,
                            )
                        )
                    # handle error (this will throw if we've exceeded the limit)
                    error, raise_error = handle_error(ex)
        except Exception as ex:
            error, raise_error = handle_error(ex)
        # complete the sample
        progress(SAMPLE_TOTAL_PROGRESS_UNITS)
        # log it
        if logger is not None:
            # if we are logging images then be sure to base64 images injected by solvers
            if log_images:
                state = (await states_with_base64_content([state]))[0]
            # otherwise ensure there are no base64 images in sample or messages
            else:
                sample = sample_without_base64_content(sample)
                state = state_without_base64_content(state)
            # log the sample
            await log_sample(
                start_time=start_time,
                logger=logger,
                sample=sample,
                state=state,
                scores=results,
                error=error,
                log_images=log_images,
            )
        # return
        if error is None:
            if results is not None:
                sample_complete(results)
            return results
        elif raise_error:
            raise raise_error
        else:
            return None
async def log_sample(
    start_time: float | None,
    logger: TaskLogger,
    sample: Sample,
    state: TaskState,
    scores: dict[str, SampleScore],
    error: EvalError | None,
    log_images: bool,
) -> None:
    # sample must have id to be logged
    id = sample.id
    if id is None:
        raise ValueError(
            f"Samples without IDs cannot be logged: {sample.model_dump_json()}"
        )
    # construct sample for logging
    # compute total time if we can
    total_time = time.monotonic() - start_time if start_time is not None else None
    # if a limit was hit, note that in the Eval Sample
    limit = None
    for e in transcript().events:
        if e.event == "sample_limit":
            limit = EvalSampleLimit(
                type=e.type, limit=e.limit if e.limit is not None else -1
            )
            break
    eval_sample = EvalSample(
        id=id,
        epoch=state.epoch,
        input=sample.input,
        choices=sample.choices,
        target=sample.target,
        metadata=state.metadata if state.metadata else {},
        sandbox=sample.sandbox,
        files=list(sample.files.keys()) if sample.files else None,
        setup=sample.setup,
        messages=state.messages,
        output=state.output,
        scores={k: v.score for k, v in scores.items()},
        store=dict(state.store.items()),
        uuid=state.uuid,
        events=list(transcript().events),
        model_usage=sample_model_usage(),
        total_time=round(total_time, 3) if total_time is not None else None,
        working_time=round(total_time - sample_waiting_time(), 3)
        if total_time is not None
        else None,
        error=error,
        limit=limit,
    )
    await logger.log_sample(condense_sample(eval_sample, log_images), flush=True)
async def resolve_dataset(
    dataset: Dataset,
    model_name: ModelName,
    limit: int | tuple[int, int] | None,
    sample_id: str | int | list[str | int] | None,
    epochs: int,
    log_images: bool,
    message_limit: int | None,
    token_limit: int | None,
) -> tuple[Dataset, list[Sample], list[TaskState]]:
    # slice dataset
    dataset = slice_dataset(dataset, limit, sample_id)
    # apply epochs (deepcopy the samples so they remain independent)
    samples: list[Sample] = []
    for _ in range(0, epochs):
        samples.extend([deepcopy(sample) for sample in dataset])
    # if we are logging images then resolve sample images here
    if log_images:
        samples = await samples_with_base64_content(samples)
    # prime the eval tasks (deep copy so they share no state w/ sample)
    sample_epochs: list[int] = []
    for e in range(0, epochs):
        sample_epochs.extend([e + 1] * len(dataset))
    states = [
        deepcopy(
            TaskState(
                sample_id=sample.id or 0,
                epoch=epoch,
                model=model_name,
                input=sample.input,
                target=Target(sample.target),
                choices=sample.choices,
                messages=sample_messages(sample),
                message_limit=message_limit,
                token_limit=token_limit,
                completed=False,
                metadata=sample.metadata if sample.metadata else {},
            )
        )
        for epoch, sample in zip(sample_epochs, samples)
    ]
    return (dataset, samples, states)
# we can reuse samples from a previous eval_log if and only if:
#   - The datasets have not been shuffled OR the samples in the dataset have unique ids
#   - The datasets have the exact same length
def eval_log_sample_source(
    eval_log: EvalLog | None, dataset: Dataset
) -> EvalSampleSource:
    # return dummy function for no sample source
    def no_sample_source(id: int | str, epoch: int) -> None:
        return None
    # take care of no log or no samples in log
    if not eval_log:
        return no_sample_source
    elif not eval_log.samples or len(eval_log.samples) == 0:
        return no_sample_source
    # determine whether all samples in the dataset have ids (if not, then we can't
    # provide a sample source in the case where either dataset is shuffled, as the ids
    # will be auto-assigned based on position, and therefore not stable)
    samples_have_ids = (
        next((sample for sample in dataset if sample.id is None), None) is None
    )
    if (eval_log.eval.dataset.shuffled or dataset.shuffled) and not samples_have_ids:
        py_logger.warning(
            "Unable to re-use samples from retry log file because the dataset was shuffled "
            + "and some samples in the dataset do not have an 'id' field."
        )
        return no_sample_source
    elif eval_log.eval.dataset.samples != len(dataset):
        py_logger.warning(
            "Unable to re-use samples from retry log file because the dataset size changed "
            + f"(log samples {eval_log.eval.dataset.samples}, dataset samples {len(dataset)})"
        )
        return no_sample_source
    else:
        def previous(id: int | str, epoch: int) -> EvalSample | None:
            return next(
                (
                    sample
                    for sample in (eval_log.samples or [])
                    if sample.id == id
                    and sample.epoch == epoch
                    and sample.error is None
                ),
                None,
            )
        return previous
# semaphore to limit concurrency. default max_samples to
# max_connections + 1 if not explicitly specified (this is
# to make sure it always saturates the connection pool)
def create_sample_semaphore(
    config: EvalConfig,
    generate_config: GenerateConfig,
    modelapi: ModelAPI | None = None,
) -> anyio.Semaphore:
    # if the user set max_samples then use that
    if config.max_samples is not None:
        return anyio.Semaphore(config.max_samples)
    # use max_connections
    max_samples = (
        generate_config.max_connections
        if generate_config.max_connections is not None
        else modelapi.max_connections()
        if modelapi
        else DEFAULT_MAX_CONNECTIONS
    )
    # return the semaphore
    return anyio.Semaphore(max_samples)

================
File: src/inspect_ai/_eval/task/rundir.py
================
import asyncio
import os
import sys
from contextlib import contextmanager
from contextvars import ContextVar
from functools import wraps
from typing import Any, Callable, Iterator, TypeVar
from inspect_ai._eval.task.task import Task
from inspect_ai._eval.task.util import task_chdir
TASK_DIRECTORY_ATTRIB = "task_directory"
_task_chdir = ContextVar[str | None]("_task_chdir", default=None)
T = TypeVar("T", bound="asyncio.BaseEventLoop")
@contextmanager
def task_run_dir_switching() -> Iterator[None]:
    # get the beginning working dir
    cwd = os.getcwd()
    # get the class for the current event loop instance
    loop = asyncio.get_event_loop()
    if not isinstance(loop, asyncio.BaseEventLoop):
        raise ValueError(
            "Can't run tasks in multiple directories with loop of type %s" % type(loop)
        )
    cls: type[asyncio.BaseEventLoop] = loop.__class__
    # patch call_soon with directory switching version
    original_call_soon = cls.call_soon
    def patched_call_soon(
        self: T, callback: Callable[..., Any], *args: Any, **kwargs: Any
    ) -> asyncio.Handle:
        wrapped_callback = _wrap_callback(callback)
        return original_call_soon(self, wrapped_callback, *args, **kwargs)
    cls.call_soon = patched_call_soon  # type: ignore
    # execute and restore original call_soon at the end
    try:
        yield
    finally:
        cls.call_soon = original_call_soon  # type: ignore
        os.chdir(cwd)
@contextmanager
def set_task_chdir(task: Task) -> Iterator[None]:
    chdir = task_chdir(task)
    if chdir is not None:
        token = _task_chdir.set(chdir)
        try:
            yield
        finally:
            _task_chdir.reset(token)
    else:
        yield
if sys.platform == "win32":
    BaseEventLoop = asyncio.ProactorEventLoop
else:
    BaseEventLoop = asyncio.SelectorEventLoop
def _wrap_callback(callback: Callable[..., Any]) -> Callable[..., Any]:
    @wraps(callback)
    def wrapper(*args: Any, **kwargs: Any) -> Any:
        chdir = _task_chdir.get(None)
        if chdir is not None and chdir != os.getcwd():
            os.chdir(chdir)
        return callback(*args, **kwargs)
    return wrapper

================
File: src/inspect_ai/_eval/task/sandbox.py
================
import base64
import contextlib
from random import random
from typing import AsyncGenerator, Callable, NamedTuple, cast
import anyio
import httpx
from tenacity import (
    retry,
    retry_if_exception,
    stop_after_attempt,
    stop_after_delay,
    wait_exponential_jitter,
)
from inspect_ai._eval.task.task import Task
from inspect_ai._eval.task.util import task_run_dir
from inspect_ai._util.file import file, filesystem
from inspect_ai._util.httpx import httpx_should_retry, log_httpx_retry_attempt
from inspect_ai._util.registry import registry_unqualified_name
from inspect_ai._util.url import data_uri_to_base64, is_data_uri, is_http_url
from inspect_ai.dataset import Sample
from inspect_ai.util._concurrency import concurrency
from inspect_ai.util._sandbox.context import (
    cleanup_sandbox_environments_sample,
    init_sandbox_environments_sample,
)
from inspect_ai.util._sandbox.environment import (
    SandboxEnvironment,
    SandboxEnvironmentConfigType,
    SandboxEnvironmentSpec,
)
from inspect_ai.util._sandbox.registry import registry_find_sandboxenv
@contextlib.asynccontextmanager
async def sandboxenv_context(
    task_name: str,
    sandbox: SandboxEnvironmentSpec | None,
    max_sandboxes: int | None,
    cleanup: bool,
    sample: Sample,
) -> AsyncGenerator[None, None]:
    # resolve sandbox
    sandbox = resolve_sandbox(sandbox, sample)
    if not sandbox:
        raise ValueError("sandboxenv_context called with no sandbox specified")
    # get sandboxenv_type
    sandboxenv_type = registry_find_sandboxenv(sandbox.type)
    # see if there is a max_sandboxes in play (passed or from type)
    if max_sandboxes is None:
        default_concurrency_fn = cast(
            Callable[[], int | None], getattr(sandboxenv_type, "default_concurrency")
        )
        max_sandboxes = default_concurrency_fn()
    # if we are enforcing max_sandboxes, then when samples are scheduled they may
    # not get interleaved properly across tasks (because the first task will come
    # in and grab all of the sandboxes). Therefore, in this case we wait a random
    # delay so that all tasks/samples have an equal shot at getting scheduled.
    if max_sandboxes is not None:
        await anyio.sleep(random())
    # enforce concurrency if required
    sandboxes_cm = (
        concurrency(sandbox.type, max_sandboxes, f"sandboxes/{sandbox.type}")
        if max_sandboxes is not None
        else contextlib.nullcontext()
    )
    async with sandboxes_cm:
        # read files from sample
        files: dict[str, bytes] = {}
        if sample.files:
            for path, contents in sample.files.items():
                files[path] = await read_sandboxenv_file(contents)
        # read setup script from sample (add bash shebang if necessary)
        setup: bytes | None = None
        if sample.setup:
            setup = await read_sandboxenv_file(sample.setup)
            setup_str = setup.decode(encoding="utf-8")
            if not setup_str.strip().startswith("#!"):
                setup_str = f"#!/usr/bin/env bash\n\n{setup_str}"
                setup = setup_str.encode(encoding="utf-8")
        interrupted = False
        environments: dict[str, SandboxEnvironment] | None = None
        try:
            # initialize sandbox environment,
            environments = await init_sandbox_environments_sample(
                sandboxenv_type=sandboxenv_type,
                task_name=registry_unqualified_name(task_name),
                config=sandbox.config,
                files=files,
                setup=setup,
                metadata=sample.metadata if sample.metadata else {},
            )
            # run sample
            yield
        except anyio.get_cancelled_exc_class() as ex:
            interrupted = True
            raise ex
        finally:
            # cleanup sandbox environment
            if environments and cleanup:
                await cleanup_sandbox_environments_sample(
                    type=sandbox.type,
                    task_name=task_name,
                    config=sandbox.config,
                    environments=environments,
                    interrupted=interrupted,
                )
async def read_sandboxenv_file(contents: str) -> bytes:
    if is_data_uri(contents):
        contents_base64 = data_uri_to_base64(contents)
        file_bytes = base64.b64decode(contents_base64)
    elif is_http_url(contents):
        file_bytes = await _retrying_httpx_get(contents)
    else:
        # try to read as a file (if it doesn't exist or has a path not cool w/
        # the filesystem then we fall back to contents)
        try:
            fs = filesystem(contents)
            if fs.exists(contents):
                with file(contents, "rb") as f:
                    file_bytes = f.read()
            else:
                file_bytes = contents.encode("utf-8")
        except Exception:
            file_bytes = contents.encode("utf-8")
    return file_bytes
class TaskSandboxEnvironment(NamedTuple):
    sandbox: SandboxEnvironmentSpec
    run_dir: str
def resolve_sandbox_for_task(
    eval_sandbox: SandboxEnvironmentSpec | None,
    task: Task,
    sample: Sample,
) -> TaskSandboxEnvironment | None:
    # eval_sandbox overrides task or sample sandbox
    sandbox = eval_sandbox or resolve_sandbox(task.sandbox, sample)
    if sandbox is not None:
        return TaskSandboxEnvironment(sandbox, task_run_dir(task))
    else:
        return None
def resolve_sandbox(
    sandbox: SandboxEnvironmentSpec | None,
    sample: Sample,
) -> SandboxEnvironmentSpec | None:
    # resolve sandbox (task type overrides sample type, but sample config
    # file overrides task config file if they have the same type)
    task_sandbox = sandbox
    if task_sandbox is not None:
        if (
            sample.sandbox
            and sample.sandbox.type == task_sandbox.type
            and sample.sandbox.config is not None
        ):
            sandbox_config: SandboxEnvironmentConfigType | None = sample.sandbox.config
        else:
            sandbox_config = task_sandbox.config
        return SandboxEnvironmentSpec(task_sandbox.type, sandbox_config)
    elif sample.sandbox is not None:
        return sample.sandbox
    else:
        return None
async def _retrying_httpx_get(
    url: str,
    client: httpx.AsyncClient = httpx.AsyncClient(),
    timeout: int = 30,  # per-attempt timeout
    max_retries: int = 10,
    total_timeout: int = 120,  #  timeout for the whole retry loop. not for an individual attempt
) -> bytes:
    @retry(
        wait=wait_exponential_jitter(),
        stop=(stop_after_attempt(max_retries) | stop_after_delay(total_timeout)),
        retry=retry_if_exception(httpx_should_retry),
        before_sleep=log_httpx_retry_attempt(url),
    )
    async def do_get() -> bytes:
        response = await client.get(
            url=url,
            follow_redirects=True,
            timeout=(timeout, timeout, timeout, timeout),
        )
        response.raise_for_status()
        return response.content
    return await do_get()

================
File: src/inspect_ai/_eval/task/task.py
================
from copy import deepcopy
from dataclasses import dataclass
from logging import getLogger
from typing import Any, Awaitable, Callable, Sequence, cast
from pydantic import BaseModel
from typing_extensions import TypedDict, Unpack
from inspect_ai._util.logger import warn_once
from inspect_ai._util.notgiven import NOT_GIVEN, NotGiven
from inspect_ai._util.registry import is_registry_object, registry_info
from inspect_ai.approval._policy import ApprovalPolicy, approval_policies_from_config
from inspect_ai.dataset import Dataset, MemoryDataset, Sample
from inspect_ai.log import EvalLog
from inspect_ai.model import GenerateConfig
from inspect_ai.model._model import Model, get_model
from inspect_ai.scorer import Metric, Scorer
from inspect_ai.scorer._reducer import ScoreReducers, create_reducers
from inspect_ai.solver import Plan, Solver, generate
from inspect_ai.solver._chain import chain
from inspect_ai.solver._task_state import TaskState
from inspect_ai.util._sandbox.environment import (
    SandboxEnvironmentSpec,
    SandboxEnvironmentType,
    resolve_sandbox_environment,
)
from .epochs import Epochs
logger = getLogger(__name__)
class TaskDeprecatedArgs(TypedDict, total=False):
    plan: Plan | Solver | list[Solver]
    tool_environment: str | SandboxEnvironmentSpec | None
    epochs_reducer: ScoreReducers | None
    max_messages: int | None
class Task:
    r"""Evaluation task.
    Tasks are the basis for defining and running evaluations.
    """
    def __init__(
        self,
        dataset: Dataset | Sequence[Sample] | None = None,
        setup: Solver | list[Solver] | None = None,
        solver: Solver | list[Solver] = generate(),
        cleanup: Callable[[TaskState], Awaitable[None]] | None = None,
        scorer: Scorer | list[Scorer] | None = None,
        metrics: list[Metric] | dict[str, list[Metric]] | None = None,
        model: str | Model | None = None,
        config: GenerateConfig = GenerateConfig(),
        sandbox: SandboxEnvironmentType | None = None,
        approval: str | list[ApprovalPolicy] | None = None,
        epochs: int | Epochs | None = None,
        fail_on_error: bool | float | None = None,
        message_limit: int | None = None,
        token_limit: int | None = None,
        time_limit: int | None = None,
        working_limit: int | None = None,
        name: str | None = None,
        version: int = 0,
        metadata: dict[str, Any] | None = None,
        **kwargs: Unpack[TaskDeprecatedArgs],
    ) -> None:
        """Create a task.
        Args:
            dataset: Dataset to evaluate
            setup: Setup step (always run even when the main `solver` is replaced).
            solver: Solver or list of solvers. Defaults to generate(), a normal call to the model.
            cleanup: Optional cleanup function for task. Called after
                all solvers have run for each sample (including if an
                exception occurs during the run)
            scorer: Scorer used to evaluate model output.
            metrics: Alternative metrics (overrides the metrics provided by the specified scorer).
            model: Default model for task (Optional, defaults to eval model).
            config: Model generation config.
            sandbox: Sandbox environment type (or optionally a str or tuple with a shorthand spec)
            approval: Tool use approval policies.
                Either a path to an approval policy config file or a list of approval policies. Defaults to no approval policy.
            epochs: Epochs to repeat samples for and optional score
                reducer function(s) used to combine sample scores (defaults to "mean")
            fail_on_error: `True` to fail on first sample error
                (default); `False` to never fail on sample errors; Value between 0 and 1
                to fail if a proportion of total samples fails. Value greater than 1 to fail
                eval if a count of samples fails.
            message_limit: Limit on total messages used for each sample.
            token_limit: Limit on total tokens used for each sample.
            time_limit: Limit on clock time (in seconds) for samples.
            working_limit: Limit on working time (in seconds) for sample. Working
                time includes model generation, tool calls, etc. but does not include
                time spent waiting on retries or shared resources.
            name: Task name. If not specified is automatically
                determined based on the name of the task directory (or "task")
                if its anonymous task (e.g. created in a notebook and passed to
                eval() directly)
            version: Version of task (to distinguish evolutions
                of the task spec or breaking changes to it)
            metadata:  Additional metadata to associate with the task.
            **kwargs: Deprecated arguments.
        """
        # handle deprecated args
        for arg, value in kwargs.items():
            newarg = ""
            if arg == "tool_environment":
                newarg = "sandbox"
                sandbox = cast(str | SandboxEnvironmentSpec | None, value)
            elif arg == "epochs_reducer":
                newarg = "epochs"
                if isinstance(epochs, int):
                    epochs = Epochs(
                        epochs, create_reducers(cast(ScoreReducers | None, value))
                    )
            elif arg == "plan":
                # no deprecation warning (yet) as it would affect 100% of evals in the wild
                solver = cast(Solver, value)
            elif arg == "max_messages":
                # no deprecation warning (yet) as many tasks set this
                message_limit = int(cast(int, value))
            if newarg:
                warn_once(
                    logger,
                    f"DEPRECATED: the '{arg}' parameter is deprecated (please use the '{newarg}' parameter instead)",
                )
        self.dataset = resolve_dataset(dataset)
        self.setup = setup
        self.solver = resolve_solver(solver)
        self.cleanup = cleanup
        self.scorer = resolve_scorer(scorer)
        self.metrics = metrics
        self.model = resolve_model(model)
        self.config = config
        self.sandbox = resolve_sandbox_environment(sandbox)
        self.approval = resolve_approval(approval)
        epochs = resolve_epochs(epochs)
        self.epochs = epochs.epochs if epochs else None
        self.epochs_reducer = epochs.reducer if epochs else None
        self.fail_on_error = fail_on_error
        self.message_limit = message_limit
        self.token_limit = token_limit
        self.time_limit = time_limit
        self.working_limit = working_limit
        self.version = version
        self._name = name
        self.metadata = metadata
    @property
    def name(self) -> str:
        if self._name is not None:
            return self._name
        elif is_registry_object(self):
            return registry_info(self).name
        else:
            return "task"
    @property
    def attribs(self) -> dict[str, Any]:
        if is_registry_object(self):
            return cast(dict[str, Any], registry_info(self).metadata.get("attribs", {}))
        else:
            return dict()
def task_with(
    task: Task,
    *,
    dataset: Dataset | Sequence[Sample] | None | NotGiven = NOT_GIVEN,
    setup: Solver | list[Solver] | None | NotGiven = NOT_GIVEN,
    solver: Solver | list[Solver] | NotGiven = NOT_GIVEN,
    cleanup: Callable[[TaskState], Awaitable[None]] | None | NotGiven = NOT_GIVEN,
    scorer: Scorer | list[Scorer] | None | NotGiven = NOT_GIVEN,
    metrics: list[Metric] | dict[str, list[Metric]] | None | NotGiven = NOT_GIVEN,
    model: str | Model | NotGiven = NOT_GIVEN,
    config: GenerateConfig | NotGiven = NOT_GIVEN,
    sandbox: SandboxEnvironmentType | None | NotGiven = NOT_GIVEN,
    approval: str | list[ApprovalPolicy] | None | NotGiven = NOT_GIVEN,
    epochs: int | Epochs | None | NotGiven = NOT_GIVEN,
    fail_on_error: bool | float | None | NotGiven = NOT_GIVEN,
    message_limit: int | None | NotGiven = NOT_GIVEN,
    token_limit: int | None | NotGiven = NOT_GIVEN,
    time_limit: int | None | NotGiven = NOT_GIVEN,
    working_limit: int | None | NotGiven = NOT_GIVEN,
    name: str | None | NotGiven = NOT_GIVEN,
    version: int | NotGiven = NOT_GIVEN,
    metadata: dict[str, Any] | None | NotGiven = NOT_GIVEN,
) -> Task:
    """Task adapted with alternate values for one or more options.
    Args:
        task: Task to adapt (it is deep copied prior to mutating options)
        dataset: Dataset to evaluate
        setup: Setup step (always run even when the main `solver` is replaced).
        solver: Solver or list of solvers. Defaults to generate(), a normal call to the model.
        cleanup: Optional cleanup function for task. Called after
            all solvers have run for each sample (including if an
            exception occurs during the run)
        scorer: Scorer used to evaluate model output.
        metrics: Alternative metrics (overrides the metrics provided by the specified scorer).
        model: Default model for task (Optional, defaults to eval model).
        config: Model generation config.
        sandbox: Sandbox environment type (or optionally a str or tuple with a shorthand spec)
        approval: Tool use approval policies.
            Either a path to an approval policy config file or a list of approval policies. Defaults to no approval policy.
        epochs: Epochs to repeat samples for and optional score
            reducer function(s) used to combine sample scores (defaults to "mean")
        fail_on_error: `True` to fail on first sample error
            (default); `False` to never fail on sample errors; Value between 0 and 1
            to fail if a proportion of total samples fails. Value greater than 1 to fail
            eval if a count of samples fails.
        message_limit: Limit on total messages used for each sample.
        token_limit: Limit on total tokens used for each sample.
        time_limit: Limit on clock time (in seconds) for samples.
        working_limit: Limit on working time (in seconds) for sample. Working
            time includes model generation, tool calls, etc. but does not include
            time spent waiting on retries or shared resources.
        name: Task name. If not specified is automatically
            determined based on the name of the task directory (or "task")
            if its anonymous task (e.g. created in a notebook and passed to
            eval() directly)
        version: Version of task (to distinguish evolutions
            of the task spec or breaking changes to it)
        metadata:  Additional metadata to associate with the task.
    Returns:
        Task: Task adapted with alternate options.
    """
    # deep copy the task
    task = deepcopy(task)
    if not isinstance(dataset, NotGiven):
        task.dataset = resolve_dataset(dataset)
    if not isinstance(setup, NotGiven):
        task.setup = setup
    if not isinstance(solver, NotGiven):
        task.solver = resolve_solver(solver)
    if not isinstance(cleanup, NotGiven):
        task.cleanup = cleanup
    if not isinstance(scorer, NotGiven):
        task.scorer = resolve_scorer(scorer)
    if not isinstance(metrics, NotGiven):
        task.metrics = metrics
    if not isinstance(model, NotGiven):
        task.model = resolve_model(model)
    if not isinstance(config, NotGiven):
        task.config = config
    if not isinstance(sandbox, NotGiven):
        task.sandbox = resolve_sandbox_environment(sandbox)
    if not isinstance(approval, NotGiven):
        task.approval = resolve_approval(approval)
    if not isinstance(epochs, NotGiven):
        epochs = resolve_epochs(epochs)
        task.epochs = epochs.epochs if epochs else None
        task.epochs_reducer = epochs.reducer if epochs else None
    if not isinstance(fail_on_error, NotGiven):
        task.fail_on_error = fail_on_error
    if not isinstance(message_limit, NotGiven):
        task.message_limit = message_limit
    if not isinstance(token_limit, NotGiven):
        task.token_limit = token_limit
    if not isinstance(time_limit, NotGiven):
        task.time_limit = time_limit
    if not isinstance(working_limit, NotGiven):
        task.working_limit = working_limit
    if not isinstance(version, NotGiven):
        task.version = version
    if not isinstance(name, NotGiven):
        task._name = name
    if not isinstance(metadata, NotGiven):
        task.metadata = metadata
    # return modified task
    return task
class TaskInfo(BaseModel):
    """Task information (file, name, and attributes)."""
    file: str
    """File path where task was loaded from."""
    name: str
    """Task name (defaults to function name)"""
    attribs: dict[str, Any]
    """Task attributes (arguments passed to `@task`)"""
    def __str__(self) -> str:
        return f"{self.file}@{self.name}"
    def __hash__(self) -> int:
        return hash(
            (self.file, self.name)
            + tuple(self.attribs.keys())
            + tuple(self.attribs.values())
        )
@dataclass
class PreviousTask:
    id: str
    task: str | Task
    task_args: dict[str, Any]
    model: Model | None
    log: EvalLog
def resolve_approval(
    approval: str | list[ApprovalPolicy] | None,
) -> list[ApprovalPolicy] | None:
    return (
        approval_policies_from_config(approval)
        if isinstance(approval, str)
        else approval
    )
def resolve_epochs(epochs: int | Epochs | None) -> Epochs | None:
    if isinstance(epochs, int):
        epochs = Epochs(epochs)
    if epochs is not None and epochs.epochs < 1:
        raise ValueError("epochs must be a positive integer.")
    return epochs
def resolve_dataset(dataset: Dataset | Sequence[Sample] | None) -> Dataset:
    # this is a convenience for tests that don't want to define a dummy sample
    if dataset is None:
        dataset = [Sample(input="prompt")]
    # raise error if the dataset is empty
    if len(dataset) == 0:
        raise ValueError("The specified dataset is empty (has no samples)")
    # resolve sequence to dataset if necessary
    return dataset if isinstance(dataset, Dataset) else MemoryDataset(list(dataset))
def resolve_solver(solver: Solver | list[Solver]) -> Solver:
    return chain(solver) if isinstance(solver, list) else solver
def resolve_model(model: str | Model | None) -> Model | None:
    if isinstance(model, str):
        return get_model(model)
    else:
        return model
def resolve_scorer(scorer: Scorer | list[Scorer] | None) -> list[Scorer] | None:
    return (
        scorer if isinstance(scorer, list) else [scorer] if scorer is not None else None
    )

================
File: src/inspect_ai/_eval/task/tasks.py
================
from typing import Callable, TypeAlias
from .resolved import ResolvedTask
from .task import PreviousTask, Task, TaskInfo
Tasks: TypeAlias = (
    str
    | PreviousTask
    | ResolvedTask
    | TaskInfo
    | Task
    | Callable[..., Task]
    | type[Task]
    | list[str]
    | list[PreviousTask]
    | list[ResolvedTask]
    | list[TaskInfo]
    | list[Task]
    | list[Callable[..., Task]]
    | list[type[Task]]
    | None
)
r"""One or more tasks.
Tasks to be evaluated. Many forms of task specification are
supported including directory names, task functions, task
classes, and task instances (a single task or list of tasks
can be specified). None is a request to read a task out
of the current working directory.
"""

================
File: src/inspect_ai/_eval/task/util.py
================
import os
from copy import deepcopy
from typing import cast
from inspect_ai._util.path import cwd_relative_path
from inspect_ai.dataset import Sample
from inspect_ai.dataset._dataset import Dataset
from inspect_ai.model import ChatMessage, ChatMessageUser
from ..task import Task
from .constants import TASK_FILE_ATTR, TASK_RUN_DIR_ATTR
def sample_messages(sample: Sample) -> list[ChatMessage]:
    if isinstance(sample.input, str):
        return [ChatMessageUser(content=sample.input, source="input")]
    else:
        messages = deepcopy(sample.input)
        for message in messages:
            message.source = "input"
        return messages
def task_run_dir(task: Task) -> str:
    return getattr(task, TASK_RUN_DIR_ATTR, os.getcwd())
def task_chdir(task: Task) -> str | None:
    if task.attribs.get("chdir", False) is True:
        return task_run_dir(task)
    else:
        return None
def task_file(task: Task, relative: bool = False) -> str | None:
    file = cast(str | None, getattr(task, TASK_FILE_ATTR, None))
    if file:
        if relative:
            return cwd_relative_path(file)
        else:
            return file
    else:
        return None
def slice_dataset(
    dataset: Dataset,
    limit: int | tuple[int, int] | None,
    sample_id: str | int | list[str | int] | None,
) -> Dataset:
    def normalise(id: str | int | None) -> str:
        if isinstance(id, str) and id.isdigit():
            id = int(id)
        return id if isinstance(id, str) else str(id).zfill(20)
    if sample_id is not None:
        sample_id = sample_id if isinstance(sample_id, list) else [sample_id]
        sample_id = [normalise(id) for id in sample_id]
        return dataset.filter(lambda sample: normalise(sample.id) in sample_id)
    else:
        dataset_limit = (
            slice(0, len(dataset))
            if limit is None
            else (slice(*limit) if isinstance(limit, tuple) else slice(0, limit))
        )
        return dataset[dataset_limit]

================
File: src/inspect_ai/_util/_async.py
================
import asyncio
import inspect
import os
import sys
from logging import Logger
from typing import Any, Awaitable, Callable, Coroutine, Literal, TypeVar, cast
import anyio
import nest_asyncio  # type: ignore
import sniffio
if sys.version_info >= (3, 11):
    from typing import TypeVarTuple, Unpack
else:
    from exceptiongroup import ExceptionGroup
    from typing_extensions import TypeVarTuple, Unpack
PosArgsT = TypeVarTuple("PosArgsT")
def is_callable_coroutine(func_or_cls: Any) -> bool:
    if inspect.iscoroutinefunction(func_or_cls):
        return True
    elif callable(func_or_cls):
        return inspect.iscoroutinefunction(func_or_cls.__call__)
    return False
T = TypeVar("T")
async def tg_collect(
    funcs: list[Callable[[], Awaitable[T]]], exception_group: bool = False
) -> list[T]:
    """Runs all of the pased async functions and collects their results.
    The results will be returned in the same order as the input `funcs`.
    Args:
       funcs: List of async functions.
       exception_group: `True` to raise an ExceptionGroup or
          `False` (the default) to raise only the first exception.
    Returns:
       List of results of type T.
    Raises:
       Exception: First exception occurring in failed tasks
          (for `exception_group == False`, the default)
       ExceptionGroup: Exceptions that occurred in failed tasks
         (for `exception_group == True`)
    """
    try:
        results: list[tuple[int, T]] = []
        async with anyio.create_task_group() as tg:
            async def run_task(index: int) -> None:
                result = await funcs[index]()
                results.append((index, result))
            for i in range(0, len(funcs)):
                tg.start_soon(run_task, i)
        # sort results by original index and return just the values
        return [r for _, r in sorted(results)]
    except ExceptionGroup as ex:
        if exception_group:
            raise
        else:
            raise ex.exceptions[0]
async def coro_print_exceptions(
    context: str,
    func: Callable[[Unpack[PosArgsT]], Awaitable[Any]],
    *args: Unpack[PosArgsT],
) -> None:
    try:
        await func(*args)
    except Exception as ex:
        print(f"Error {context}: {ex}")
async def coro_log_exceptions(
    logger: Logger,
    context: str,
    func: Callable[[Unpack[PosArgsT]], Awaitable[Any]],
    *args: Unpack[PosArgsT],
) -> None:
    try:
        await func(*args)
    except Exception as ex:
        logger.warning(f"Error {context}: {ex}")
_initialised_nest_asyncio: bool = False
def init_nest_asyncio() -> None:
    global _initialised_nest_asyncio
    if not _initialised_nest_asyncio:
        nest_asyncio.apply()
        _initialised_nest_asyncio = True
def run_coroutine(coroutine: Coroutine[None, None, T]) -> T:
    from inspect_ai._util.platform import running_in_notebook
    if current_async_backend() == "trio":
        raise RuntimeError("run_coroutine cannot be used with trio")
    if running_in_notebook():
        init_nest_asyncio()
        return asyncio.run(coroutine)
    else:
        try:
            # this will throw if there is no running loop
            asyncio.get_running_loop()
            # initialiase nest_asyncio then we are clear to run
            init_nest_asyncio()
            return asyncio.run(coroutine)
        except RuntimeError:
            # No running event loop so we are clear to run
            return asyncio.run(coroutine)
def current_async_backend() -> Literal["asyncio", "trio"] | None:
    try:
        return _validate_backend(sniffio.current_async_library().lower())
    except sniffio.AsyncLibraryNotFoundError:
        return None
def configured_async_backend() -> Literal["asyncio", "trio"]:
    backend = os.environ.get("INSPECT_ASYNC_BACKEND", "asyncio").lower()
    return _validate_backend(backend)
def _validate_backend(backend: str) -> Literal["asyncio", "trio"]:
    if backend in ["asyncio", "trio"]:
        return cast(Literal["asyncio", "trio"], backend)
    else:
        raise RuntimeError(f"Unknown async backend: {backend}")

================
File: src/inspect_ai/_util/ansi.py
================
import os
from typing import Any
from rich.console import Console, RenderableType
def render_text(
    text: RenderableType | list[RenderableType], styles: bool = True, **options: Any
) -> str:
    """Render text from Rich renderables.
    Args:
      text (RenderableType | list[RenderableType]): Renderables.
      styles (bool): If True, ansi escape codes will be included. False for plain text.
        Defaults to True.
      **options (Any): Additonal keyword arguments to pass to `Console` constructor.
    Returns:
       str: Rendered text (with ansi codes if `styles=True`)
    """
    # resolve to text
    text = text if isinstance(text, list) else [text]
    # print to console attached to /dev/null
    with open(os.devnull, "w") as f:
        console = Console(file=f, record=True, force_terminal=True, **options)
        for t in text:
            console.print(t)
    # export (optionally w/ ansi styles)
    return console.export_text(styles=styles).strip()

================
File: src/inspect_ai/_util/appdirs.py
================
from pathlib import Path
from platformdirs import user_cache_path, user_data_path
from inspect_ai._util.constants import PKG_NAME
def inspect_data_dir(subdir: str | None) -> Path:
    data_dir = user_data_path(PKG_NAME)
    if subdir:
        data_dir = data_dir / subdir
    data_dir.mkdir(parents=True, exist_ok=True)
    return data_dir
def inspect_cache_dir(subdir: str | None) -> Path:
    cache_dir = user_cache_path(PKG_NAME)
    if subdir:
        cache_dir = cache_dir / subdir
    cache_dir.mkdir(parents=True, exist_ok=True)
    return cache_dir

================
File: src/inspect_ai/_util/config.py
================
import json
from typing import Any
import yaml
from inspect_ai.util._resource import resource
from .error import PrerequisiteError
from .file import filesystem
def resolve_args(args: dict[str, Any] | str) -> dict[str, Any]:
    # if its a file, read as JSON or YAML
    if isinstance(args, str):
        fs = filesystem(args)
        if not fs.exists(args):
            raise PrerequisiteError(f"The config file {args} does not exist.")
        args = read_config_object(resource(args, type="file"))
    return args
def read_config_object(obj: str) -> dict[str, Any]:
    # detect json vs. yaml
    is_json = obj.strip().startswith("{")
    config = json.loads(obj) if is_json else yaml.safe_load(obj)
    if not isinstance(config, dict):
        raise ValueError(f"The config is not a valid object: {obj}")
    else:
        return config

================
File: src/inspect_ai/_util/constants.py
================
from pathlib import Path
from typing import Literal
PKG_AUTHOR = "UK AI Security Institute"
PKG_AUTHOR_DIR = "UK-AISI"
PKG_NAME = Path(__file__).parent.parent.stem
PKG_PATH = Path(__file__).parent.parent
DEFAULT_EPOCHS = 1
DEFAULT_MAX_CONNECTIONS = 10
DEFAULT_MAX_TOKENS = 2048
DEFAULT_VIEW_PORT = 7575
DEFAULT_SERVER_HOST = "127.0.0.1"
HTTP = 15
HTTP_LOG_LEVEL = "HTTP"
TRACE = 13
TRACE_LOG_LEVEL = "TRACE"
ALL_LOG_LEVELS = [
    "DEBUG",
    TRACE_LOG_LEVEL,
    HTTP_LOG_LEVEL,
    "INFO",
    "WARNING",
    "ERROR",
    "CRITICAL",
]
DEFAULT_LOG_LEVEL = "warning"
DEFAULT_LOG_LEVEL_TRANSCRIPT = "info"
ALL_LOG_FORMATS = ["eval", "json"]
DEFAULT_LOG_FORMAT: Literal["eval", "json"] = "eval"
EVAL_LOG_FORMAT = "eval"
DEFAULT_DISPLAY = "full"
LOG_SCHEMA_VERSION = 2
SCORED_SUFFIX = "-scored"
SAMPLE_SUBTASK = "sample"
CONSOLE_DISPLAY_WIDTH = 120
BASE_64_DATA_REMOVED = "<base64-data-removed>"
SANDBOX_SETUP_TIMEOUT = 300
NO_CONTENT = "(no content)"
DESERIALIZING = "deserializing"
DESERIALIZING_CONTEXT = {DESERIALIZING: True}

================
File: src/inspect_ai/_util/content.py
================
from typing import Literal, Union
from pydantic import BaseModel, Field
class ContentText(BaseModel):
    """Text content."""
    type: Literal["text"] = Field(default="text")
    """Type."""
    text: str
    """Text content."""
class ContentReasoning(BaseModel):
    """Reasoning content.
    See the specification for [thinking blocks](https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking#understanding-thinking-blocks) for Claude models.
    """
    type: Literal["reasoning"] = Field(default="reasoning")
    """Type."""
    reasoning: str
    """Reasoning content."""
    signature: str | None = Field(default=None)
    """Signature for reasoning content (used by some models to ensure that reasoning content is not modified for replay)"""
    redacted: bool = Field(default=False)
    """Indicates that the explicit content of this reasoning block has been redacted."""
class ContentImage(BaseModel):
    """Image content."""
    type: Literal["image"] = Field(default="image")
    """Type."""
    image: str
    """Either a URL of the image or the base64 encoded image data."""
    detail: Literal["auto", "low", "high"] = Field(default="auto")
    """Specifies the detail level of the image.
    Currently only supported for OpenAI. Learn more in the    [Vision guide](https://platform.openai.com/docs/guides/vision/low-or-high-fidelity-image-understanding).
    """
class ContentAudio(BaseModel):
    """Audio content."""
    type: Literal["audio"] = Field(default="audio")
    """Type."""
    audio: str
    """Audio file path or base64 encoded data URL."""
    format: Literal["wav", "mp3"]
    """Format of audio data ('mp3' or 'wav')"""
class ContentVideo(BaseModel):
    """Video content."""
    type: Literal["video"] = Field(default="video")
    """Type."""
    video: str
    """Audio file path or base64 encoded data URL."""
    format: Literal["mp4", "mpeg", "mov"]
    """Format of video data ('mp4', 'mpeg', or 'mov')"""
Content = Union[ContentText, ContentReasoning, ContentImage, ContentAudio, ContentVideo]
"""Content sent to or received from a model."""

================
File: src/inspect_ai/_util/datetime.py
================
from datetime import datetime
from typing import Literal
def iso_now(
    timespec: Literal[
        "auto", "hours", "minutes", "seconds", "milliseconds", "microseconds"
    ] = "seconds",
) -> str:
    return datetime.now().astimezone().isoformat(timespec=timespec)

================
File: src/inspect_ai/_util/decorator.py
================
import ast
from logging import getLogger
from pathlib import Path
from typing import Any
from .error import exception_message
from .file import file
logger = getLogger(__name__)
def parse_decorators(
    path: Path, decorator_name: str
) -> list[tuple[str, dict[str, Any]]]:
    # read code from python source file
    if path.suffix.lower() == ".py":
        with file(path.as_posix(), "r", encoding="utf-8") as f:
            code = f.read()
    # read code from notebook
    elif path.suffix.lower() == ".ipynb":
        try:
            from inspect_ai._util.notebook import read_notebook_code
        except ImportError:
            return []
        code = read_notebook_code(path)
    # unsupported file type
    else:
        raise ModuleNotFoundError(f"Invalid extension for module file: {path.suffix}")
    # parse the top level decorators out of the code
    decorators: list[tuple[str, dict[str, Any]]] = []
    try:
        tree = ast.parse(code)
        for node in ast.iter_child_nodes(tree):
            if isinstance(node, ast.FunctionDef):
                for decorator in node.decorator_list:
                    result = parse_decorator(node, decorator, decorator_name)
                    if result:
                        decorators.append(result)
    except SyntaxError:
        pass
    return decorators
def parse_decorator(
    node: ast.FunctionDef, decorator: ast.expr, decorator_name: str
) -> tuple[str, dict[str, Any]] | None:
    if isinstance(decorator, ast.Name):
        if str(decorator.id) == decorator_name:
            return node.name, {}
    elif isinstance(decorator, ast.Call):
        if isinstance(decorator.func, ast.Name):
            if str(decorator.func.id) == decorator_name:
                return parse_decorator_name_and_params(node, decorator)
    return None
def parse_decorator_name_and_params(
    node: ast.FunctionDef, decorator: ast.Call
) -> tuple[str, dict[str, Any]]:
    name = node.name
    attribs: dict[str, Any] = {}
    for arg in decorator.keywords:
        if arg.arg is not None:
            try:
                value = ast.literal_eval(arg.value)
                if arg.arg == "name":
                    name = value
                else:
                    attribs[arg.arg] = value
            except ValueError as ex:
                # when parsing, we can't provide the values of expressions that execute code
                logger.debug(
                    f"Error parsing attribute {arg.arg} of {node.name}: {exception_message(ex)}"
                )
                pass
    return name, attribs

================
File: src/inspect_ai/_util/deprecation.py
================
# from https://pyomo.readthedocs.io/en/stable/_modules/pyomo/common/deprecation.html
import inspect
import logging
import textwrap
from types import FrameType
from typing import Any, Set
def relocated_module_attribute(
    local: str,
    target: str,
    version: str | None = None,
    remove_in: str | None = None,
    msg: str | None = None,
    f_globals: dict[str, Any] | None = None,
) -> None:
    """Provide a deprecation path for moved / renamed module attributes
    This function declares that a local module attribute has been moved
    to another location.  For Python 3.7+, it leverages a
    module.__getattr__ method to manage the deferred import of the
    object from the new location (on request), as well as emitting the
    deprecation warning.
    Params:
        local (str): The original (local) name of the relocated attribute
        target (str): The new absolute import name of the relocated attribute
        version (str): The Pyomo version when this move was released
          (passed to deprecation_warning)
        remove_in (str | None): The Pyomo version when this deprecation path
          will be removed (passed to deprecation_warning)
        msg (str | None): If not None, then this specifies a custom deprecation
          message to be emitted when the attribute is accessed from its original
         location.
    """
    # Historical note: This method only works for Python >= 3.7.  There
    # were backports to previous Python interpreters, but were removed
    # after SHA 4e04819aaeefc2c08b7718460918885e12343451
    if f_globals is None:
        frame = inspect.currentframe()
        if frame:
            back = frame.f_back
            if back:
                f_globals = back.f_globals
                if f_globals["__name__"].startswith("importlib."):
                    raise RuntimeError(
                        "relocated_module_attribute() called from a cythonized "
                        "module without passing f_globals"
                    )
    if f_globals is None:
        raise RuntimeError("No f_globals available.")
    _relocated = f_globals.get("__relocated_attrs__", None)
    if _relocated is None:
        f_globals["__relocated_attrs__"] = _relocated = {}
        _mod_getattr = f_globals.get("__getattr__", None)
        def __getattr__(name: str) -> Any:
            info = _relocated.get(name, None)
            if info is not None:
                target_obj = _import_object(name, *info)
                f_globals[name] = target_obj
                return target_obj
            elif _mod_getattr is not None:
                return _mod_getattr(name)
            raise AttributeError(
                "module '%s' has no attribute '%s'" % (f_globals["__name__"], name)
            )
        f_globals["__getattr__"] = __getattr__
    _relocated[local] = (target, version, remove_in, msg)
def _import_object(
    name: str,
    target: str,
    version: str | None = None,
    remove_in: str | None = None,
    msg: str | None = None,
) -> Any:
    from importlib import import_module
    modname, targetname = target.rsplit(".", 1)
    _object = getattr(import_module(modname), targetname)
    if msg is None:
        if inspect.isclass(_object):
            _type = "class"
        elif inspect.isfunction(_object):
            _type = "function"
        else:
            _type = "attribute"
        msg = (
            f"the '{name}' {_type} has been moved to '{target}'."
            " Please update your import."
        )
    deprecation_warning(msg, version=version, remove_in=remove_in)
    return _object
def deprecation_warning(
    msg: str,
    version: str | None = None,
    remove_in: str | None = None,
    calling_frame: FrameType | None = None,
) -> None:
    logger = logging.getLogger(__name__)
    msg = textwrap.fill(
        f"DEPRECATED: {default_deprecation_msg(None, msg, version, remove_in)}",
        width=70,
    )
    if calling_frame is None:
        # The useful thing to let the user know is what called the
        # function that generated the deprecation warning.  The current
        # globals() is *this* module.  Walking up the stack to find the
        # frame where the globals() changes tells us the module that is
        # issuing the deprecation warning.  As we assume that *that*
        # module will not trigger its own deprecation warnings, we will
        # walk farther up until the globals() changes again.
        calling_frame = _find_calling_frame(2)
    if calling_frame is not None:
        info = inspect.getframeinfo(calling_frame)
        msg += "\n(called from %s:%s)" % (info.filename.strip(), info.lineno)
        if msg in _emitted_warnings:
            return
        _emitted_warnings.add(msg)
    from inspect_ai._util.logger import warn_once
    warn_once(logger, msg)
_emitted_warnings: Set[str] = set()
def _find_calling_frame(module_offset: int) -> FrameType | None:
    g = [globals()]
    frame = inspect.currentframe()
    if frame is not None:
        calling_frame = frame.f_back
        while calling_frame is not None:
            if calling_frame.f_globals is g[-1]:
                calling_frame = calling_frame.f_back
            elif len(g) < module_offset:
                g.append(calling_frame.f_globals)
            else:
                break
        return calling_frame
    else:
        return None
def default_deprecation_msg(
    obj: Any, user_msg: str, version: str | None, remove_in: str | None
) -> str:
    """Generate the default deprecation message.
    See deprecated() function for argument details.
    """
    if user_msg is None:
        if inspect.isclass(obj):
            _obj = " class"
        elif inspect.ismethod(obj):
            _obj = " method"
        elif inspect.isfunction(obj) or inspect.isbuiltin(obj):
            _obj = " function"
        else:
            # either @deprecated() an unknown type or called from
            # deprecation_warning()
            _obj = ""
        _qual = getattr(obj, "__qualname__", "") or ""
        if _qual.endswith(".__init__") or _qual.endswith(".__new__"):
            _obj = f" class ({_qual.rsplit('.', 1)[0]})"
        elif _qual and _obj:
            _obj += f" ({_qual})"
        user_msg = (
            "This%s has been deprecated and may be removed in a "
            "future release." % (_obj,)
        )
    comment = []
    if version:
        comment.append("deprecated in %s" % (version,))
    if remove_in:
        comment.append("will be removed in %s" % (remove_in))
    if comment:
        return user_msg + "  (%s)" % (", ".join(comment),)
    else:
        return user_msg

================
File: src/inspect_ai/_util/dev.py
================
import os
def is_dev_mode() -> bool:
    return os.environ.get("INSPECT_DEV_MODE", None) is not None

================
File: src/inspect_ai/_util/dict.py
================
from typing import Any
def omit(x: dict[str, Any], vars: list[str]) -> dict[str, Any]:
    x = x.copy()
    for var in vars:
        if var in x:
            del x[var]
    return x

================
File: src/inspect_ai/_util/dotenv.py
================
import contextlib
import os
from pathlib import Path
from typing import Any, Generator
from urllib.parse import urlparse
from dotenv import dotenv_values, find_dotenv, load_dotenv
from .file import absolute_file_path
from .platform import is_running_in_vscode
INSPECT_LOG_DIR_VAR = "INSPECT_LOG_DIR"
def init_dotenv() -> None:
    # if we are running in vscode, the vscode python extension is already reading in the
    # .env file. This means that editing the .env file within a given session does not
    # actually work! (since load_dotenv doesn't overwrite existing vars by default).
    # so, in this case we actually specify override so we get the more intuitive behavior
    override = is_running_in_vscode()
    # look up the directory tree for a .env file
    dotenv_file = find_dotenv(usecwd=True)
    # we found one, process it
    if dotenv_file:
        # is there an INSPECT_LOG_DIR currently in the environment? (we will give it preference)
        environment_log_dir = os.environ.get(INSPECT_LOG_DIR_VAR, None)
        dotenv_log_dir = dotenv_values(dotenv_file).get(INSPECT_LOG_DIR_VAR, None)
        # If the environment is providing a log_dir that is the same as what is configured
        # in the environment, treat this as if the environment is providing the log_dir
        # (which means it will be interpreted relative to the env file)
        #
        # If the log_dir is coming from the environment, interpret it relative to the cwd
        inspect_log_dir = None
        if environment_log_dir == dotenv_log_dir or not environment_log_dir:
            # check for a relative dir, if we find one then resolve to absolute
            if dotenv_log_dir:
                fs_scheme = urlparse(dotenv_log_dir).scheme
                if not fs_scheme and not os.path.isabs(dotenv_log_dir):
                    inspect_log_dir = (
                        (Path(dotenv_file).parent / dotenv_log_dir).resolve().as_posix()
                    )
        elif environment_log_dir:
            inspect_log_dir = absolute_file_path(environment_log_dir)
        # do the load, overriding as necessary if we are in vscode
        load_dotenv(dotenv_file, override=override)
        # apply the log_dir, giving preference to the existing environment var
        if inspect_log_dir:
            os.environ[INSPECT_LOG_DIR_VAR] = inspect_log_dir
        # re-apply any env vars specified at the cli w/ --env
        apply_cli_env()
@contextlib.contextmanager
def dotenv_environ(
    override: bool = is_running_in_vscode(),
) -> Generator[Any, Any, None]:
    # determine values to update
    update: dict[str, str] = {}
    values = dotenv_values(".env")
    for key, value in values.items():
        if value is not None and (override or (key not in os.environ.keys())):
            update[key] = value
    # vars to restore and remove on exit
    stomped = set(update.keys()) & set(os.environ.keys())
    update_after = {k: os.environ[k] for k in stomped}
    remove_after = frozenset(k for k in update if k not in os.environ)
    # do the thing
    try:
        os.environ.update(update)
        yield
    finally:
        os.environ.update(update_after)
        [os.environ.pop(k) for k in remove_after]
_cli_env: dict[str, Any] = {}
def init_cli_env(env: dict[str, Any]) -> None:
    global _cli_env
    _cli_env = env
    apply_cli_env()
def apply_cli_env() -> None:
    for var, value in _cli_env.items():
        os.environ[var] = str(value)

================
File: src/inspect_ai/_util/entrypoints.py
================
from importlib.metadata import entry_points
from logging import getLogger
logger = getLogger(__name__)
def ensure_entry_points(package: str | None = None) -> None:
    # have we already loaded all entry points?
    global _inspect_ai_eps_loaded_all
    if _inspect_ai_eps_loaded_all:
        return None
    # have we already loaded entry points for this package?
    if package in _inspect_ai_eps_loaded:
        return None
    # enumerate entry points
    eps = entry_points(group="inspect_ai")
    for ep in eps:
        try:
            # if there is a package filter then condition on that
            if package is not None:
                if ep.name and (ep.name == package):
                    ep.load()
                    _inspect_ai_eps_loaded.append(package)
            # if there is no package filter then load unconditionally
            # and mark us as fully loaded
            else:
                ep.load()
                _inspect_ai_eps_loaded_all = True
        except Exception as ex:
            logger.warning(
                f"Unexpected exception loading entrypoints from '{ep.value}': {ex}"
            )
# inspect extension entry points
_inspect_ai_eps_loaded: list[str] = []
_inspect_ai_eps_loaded_all: bool = False

================
File: src/inspect_ai/_util/environ.py
================
import os
from contextlib import contextmanager
from typing import Iterator
@contextmanager
def environ_var(name: str, value: str) -> Iterator[None]:
    """
    Temporarily set an environment variable within a context.
    Args:
        name: Name of the environment variable to set
        value: Value to set the environment variable to
    Yields:
        None
    """
    previous_value = os.environ.get(name)
    os.environ[name] = value
    try:
        yield
    finally:
        if previous_value is None:
            os.environ.pop(name, None)
        else:
            os.environ[name] = previous_value

================
File: src/inspect_ai/_util/error.py
================
import sys
from importlib.metadata import version
from types import TracebackType
from typing import Callable
from pydantic import BaseModel
from rich import print
from rich.console import RenderableType
class EvalError(BaseModel):
    """Eval error details."""
    message: str
    """Error message."""
    traceback: str
    """Error traceback."""
    traceback_ansi: str
    """Error traceback with ANSI color codes."""
def pip_dependency_error(feature: str, dependencies: list[str]) -> Exception:
    return PrerequisiteError(
        f"[bold]ERROR[/bold]: {feature} requires optional dependencies. "
        f"Install with:\n\n[bold]pip install {' '.join(dependencies)}[/bold]"
    )
def module_version_error(
    feature: str, package: str, required_version: str
) -> Exception:
    return PrerequisiteError(
        f"[bold]ERROR[/bold]: {feature} requires at least version {required_version} of package {package} "
        f"(you have version {version(package)} installed).\n\n"
        f"Upgrade with:\n\n[bold]pip install --upgrade {package}[/bold]"
    )
def module_max_version_error(feature: str, package: str, max_version: str) -> Exception:
    return PrerequisiteError(
        f"[bold]ERROR[/bold]: {feature} supports only version {max_version} and earlier of package {package} "
        f"(you have version {version(package)} installed).\n\n"
        f"Install the older version with with:\n\n[bold]pip install {package}=={max_version}[/bold]"
    )
def exception_message(ex: BaseException) -> str:
    return getattr(ex, "message", repr(ex))
class PrerequisiteError(Exception):
    def __init__(self, message: RenderableType) -> None:
        self.message = message
def exception_hook() -> Callable[..., None]:
    sys_handler = sys.excepthook
    def handler(
        exception_type: type[BaseException],
        exception: BaseException,
        traceback: TracebackType,
    ) -> None:
        if isinstance(exception, PrerequisiteError):
            print(f"\n{exception.message}\n")
        else:
            sys_handler(exception_type, exception, traceback)
    return handler
_exception_hook_set: bool = False
def set_exception_hook() -> None:
    global _exception_hook_set
    if not _exception_hook_set:
        sys.excepthook = exception_hook()
        _exception_hook_set = True

================
File: src/inspect_ai/_util/file.py
================
import datetime
import io
import os
import re
import string
import unicodedata
from contextlib import contextmanager
from copy import deepcopy
from pathlib import Path
from typing import Any, BinaryIO, Iterator, Literal, cast, overload
from urllib.parse import urlparse
import fsspec  # type: ignore  # type: ignore
from fsspec.core import split_protocol  # type: ignore  # type: ignore
from fsspec.implementations.local import make_path_posix  # type: ignore
from pydantic import BaseModel
from s3fs import S3FileSystem  # type: ignore
from shortuuid import uuid
from inspect_ai._util._async import configured_async_backend, current_async_backend
from inspect_ai._util.error import PrerequisiteError
# https://filesystem-spec.readthedocs.io/en/latest/_modules/fsspec/spec.html#AbstractFileSystem
# https://filesystem-spec.readthedocs.io/en/latest/api.html#fsspec.generic.GenericFileSystem
OpenTextMode = Literal["r", "a", "w"]
OpenBinaryMode = Literal["rb", "ab", "wb"]
@overload
@contextmanager
def file(
    file: str,
    mode: OpenTextMode,
    encoding: str = "utf-8",
    fs_options: dict[str, Any] = {},
) -> Iterator[io.TextIOWrapper]: ...
@overload
@contextmanager
def file(
    file: str,
    mode: OpenBinaryMode,
    encoding: str = "utf-8",
    fs_options: dict[str, Any] = {},
) -> Iterator[BinaryIO]: ...
@contextmanager
def file(
    file: str,
    mode: OpenTextMode | OpenBinaryMode,
    encoding: str = "utf-8",
    fs_options: dict[str, Any] = {},
) -> Iterator[io.TextIOWrapper] | Iterator[BinaryIO]:
    """Open local or remote file stream.
    Open a file stream for reading or writing. Refer to a local file or
    use a URI with a remove filesystem prefix (e.g. 's3://'). The
    `fsspec` package is used to resolve filesystem URLs.
    Args:
        file (str):
          Local file path or remove filesystem URL (e.g. 's3://')
        mode (str): Mode for accessing file ("r", "rb", "w", "wb", etc.).
        encoding: (str): Encoding for text files (defaults to "utf-8")
        fs_options (dict[str, Any]): Optional. Addional arguments to pass through
          to the filesystem provider (e.g. `S3FileSystem`). Use `{"anon": True }`
          if you are accessing a public S3 bucket with no credentials.
    """
    # get the default storage options for the scheme then apply passed options
    options = default_fs_options(file)
    options.update(fs_options)
    # open the file
    open_file = fsspec.open(file, mode=mode, encoding=encoding, **options)
    # yield the file and ensure it is closed when we exit the context
    with open_file as f:
        try:
            yield f
        finally:
            f.close()
def open_file(
    file: str,
    mode: OpenTextMode | OpenBinaryMode,
    encoding: str = "utf-8",
    fs_options: dict[str, Any] = {},
) -> fsspec.core.OpenFile:
    # get the default storage options for the scheme then apply passed options
    options = default_fs_options(file)
    options.update(fs_options)
    # open the file and return the stream
    return fsspec.open(file, mode=mode, encoding=encoding, **options)
# utility to copy a file
def copy_file(
    input_file: str,
    output_file: str,
    buffer_size: int = 1024 * 1024,
) -> None:
    """Copy a file across filesystems."""
    with file(input_file, "rb") as fin, file(output_file, "wb") as fout:
        while True:
            chunk = fin.read(buffer_size)
            if not chunk:
                break
            fout.write(chunk)
def basename(file: str) -> str:
    """Get the base name of the file.
    Works for all variations of fsspec providers, posix/windows/etc.
    Args:
       file (str): File name
    Returns:
       Base name for file
    """
    # windows paths aren't natively handled on posix so flip backslashes
    if os.sep == "/":
        file = file.replace("\\", "/")
    normalized_path = make_path_posix(file)
    _, path_without_protocol = split_protocol(normalized_path)
    name: str = path_without_protocol.rstrip("/").split("/")[-1]
    return name
def dirname(file: str) -> str:
    base = basename(file)
    return file[: -(len(base) + 1)]
def exists(file: str) -> bool:
    fs = filesystem(file)
    return fs.exists(file)
class FileInfo(BaseModel):
    name: str
    """Name of file."""
    type: str
    """Type of file (file or directory)"""
    size: int
    """File size in bytes."""
    mtime: float | None
    """File modification time (None if the file is a directory on S3)."""
class FileSystem:
    def __init__(self, fs: Any) -> None:
        self.fs = fs
    @property
    def sep(self) -> str:
        return cast(str, self.fs.sep)
    def exists(self, path: str) -> bool:
        return self.fs.exists(path) is True
    def touch(self, path: str) -> None:
        self.fs.touch(path)
    def rm(
        self, path: str, recursive: bool = False, maxdepth: int | None = None
    ) -> None:
        self.fs.rm(path, recursive=recursive, maxdepth=maxdepth)
    def mkdir(self, path: str, exist_ok: bool = False) -> None:
        if self.is_s3():
            # try to avoid calling create_bucket on s3 filesystems (as that requires distinct
            # privileges from being able to write to an existing bucket). we do this by
            # first calling mkdir w/ create_parents=False and then only if that fails
            # with FileNotFound do we attempt to create the bucket by calling mkdirs
            try:
                self.fs.makedir(path, create_parents=False)
            except FileExistsError:
                if exist_ok:
                    pass
                else:
                    raise
            except FileNotFoundError:
                self.fs.makedirs(path, exist_ok=exist_ok)
        else:
            self.fs.makedirs(path, exist_ok=exist_ok)
    def info(self, path: str, **kwargs: dict[str, Any]) -> FileInfo:
        return self._file_info(self.fs.info(path, **kwargs))
    def ls(
        self, path: str, recursive: bool = False, **kwargs: dict[str, Any]
    ) -> list[FileInfo]:
        # prevent caching of listings
        self.fs.invalidate_cache(path)
        # enumerate the files
        if recursive:
            files: list[dict[str, Any]] = []
            for _, _, filenames in self.fs.walk(path=path, detail=True, **kwargs):
                files.extend(filenames.values())
        else:
            files = cast(
                list[dict[str, Any]],
                self.fs.ls(path, detail=True, **kwargs),
            )
        # return FileInfo
        return [self._file_info(file) for file in files]
    def is_local(self) -> bool:
        return isinstance(self.fs, fsspec.implementations.local.LocalFileSystem)
    def is_writeable(self, path: str) -> bool:
        try:
            path = path.rstrip("/\\")
            touch_file = f"{path}{self.fs.sep}{uuid()}"
            self.touch(touch_file)
            self.rm(touch_file)
            return True
        except PermissionError:
            return False
    def is_async(self) -> bool:
        return isinstance(self.fs, fsspec.asyn.AsyncFileSystem)
    def is_s3(self) -> bool:
        return isinstance(self.fs, S3FileSystem)
    def put_file(self, lpath: str, rpath: str) -> None:
        self.fs.put_file(lpath, rpath)
    def get_file(self, rpath: str, lpath: str) -> None:
        self.fs.get_file(rpath, lpath)
    def read_bytes(self, path: str, start: int, end: int) -> bytes:
        return cast(bytes, self.fs.read_bytes(path, start, end))
    def _file_info(self, info: dict[str, Any]) -> FileInfo:
        # name needs the protocol prepended
        file = info.copy()
        file["name"] = self.fs.unstrip_protocol(file["name"])
        # S3 filesystems use "LastModified"
        if "LastModified" in file.keys():
            file["mtime"] = cast(
                datetime.datetime, cast(Any, file)["LastModified"]
            ).timestamp()
        # if we don't yet have an mtime key then fetch created explicitly
        # note: S3 doesn't give you a directory modification time
        if "mtime" not in file.keys() and file["type"] == "file":
            file["mtime"] = self.fs.created(file).timestamp()
        if "mtime" in file.keys():
            file["mtime"] = file["mtime"] * 1000
        else:
            file["mtime"] = None
        return FileInfo(
            name=file["name"],
            type=file["type"],
            size=file["size"],
            mtime=file["mtime"],
        )
def filesystem(path: str, fs_options: dict[str, Any] = {}) -> FileSystem:
    """Return the filesystem used to host the specified path.
    Args:
      path (str): Local path or remote URL e.g. s3://).  The
        `fsspec` package is used to resolve filesystem URLs.
      fs_options (dict[str, Any]): Optional. Additional arguments to pass through
        to the filesystem provider (e.g. `S3FileSystem`). Use `{"anon": True }`
        if you are accessing a public S3 bucket with no credentials.
    Returns:
       An tuple with an `fsspec` compatible filesystem and the
       file-systems-specific URL for file.
    """
    # determine options
    options = default_fs_options(path)
    options.update(fs_options)
    # create filesystem
    fs, path = fsspec.core.url_to_fs(path, **options)
    return FileSystem(fs)
def absolute_file_path(file: str) -> str:
    # check for a relative dir, if we find one then resolve to absolute
    fs_scheme = urlparse(file).scheme
    if not fs_scheme and not os.path.isabs(file):
        file = Path(file).resolve().as_posix()
    return file
def default_fs_options(file: str) -> dict[str, Any]:
    scheme = urlparse(file).scheme
    if (
        scheme == "s3"
        and configured_async_backend() == "trio"
        and current_async_backend() == "trio"
    ):
        raise PrerequisiteError(
            "ERROR: The s3 interface is not supported when running under the trio async backend."
        )
    options = deepcopy(DEFAULT_FS_OPTIONS.get(scheme, {}))
    # disable caching for all filesystems
    options.update(
        dict(
            skip_instance_cache=False,
            use_listings_cache=False,
        )
    )
    return options
def size_in_mb(file: str) -> float:
    # Open the file using fsspec and retrieve the file's information
    fs, path = fsspec.core.url_to_fs(file)
    # Use the filesystem's info method to get the size
    file_info = fs.info(path)
    # Extract the size from the file information
    file_size_in_bytes = cast(float, file_info["size"])
    # Get the size in megabytes
    file_size_in_mb = file_size_in_bytes / (1024 * 1024)
    return file_size_in_mb
def safe_filename(s: str, max_length: int = 255) -> str:
    """
    Convert a string into a safe filename by removing or replacing unsafe characters.
    Args:
        s (str): The input string to convert
        max_length (int): Maximum length of the resulting filename (default 255)
    Returns:
        str: A safe filename string
    Examples:
        >>> safe_filename("Hello/World?.txt")
        'Hello_World.txt'
    """
    # normalize unicode characters
    s = unicodedata.normalize("NFKD", s)
    s = s.encode("ASCII", "ignore").decode("ASCII")
    # remove or replace unsafe characters
    # Keep only alphanumeric characters, dots, dashes, and underscores
    safe_chars = string.ascii_letters + string.digits + ".-_"
    s = "".join(c if c in safe_chars else "_" for c in s)
    # remove consecutive underscores
    s = re.sub(r"_+", "_", s)
    # remove leading/trailing periods and underscores
    s = s.strip("._")
    # handle empty string case
    if not s:
        s = "untitled"
    # handle starting with a period (hidden files)
    if s.startswith("."):
        s = "_" + s
    # enforce length limit
    if len(s) > max_length:
        # If we need to truncate, preserve the file extension if present
        name, ext = os.path.splitext(s)
        ext_len = len(ext)
        if ext_len > 0:
            max_name_length = max_length - ext_len
            s = name[:max_name_length] + ext
        else:
            s = s[:max_length]
    return s
DEFAULT_FS_OPTIONS: dict[str, dict[str, Any]] = dict(
    # disable all S3 native caching
    s3=dict(default_fill_cache=False, default_cache_type="none", cache_regions=False)
)

================
File: src/inspect_ai/_util/format.py
================
import pprint
from string import Formatter
from textwrap import indent
from typing import Any
def format_function_call(
    func_name: str, args_dict: dict[str, Any], indent_spaces: int = 4, width: int = 80
) -> str:
    formatted_args = []
    for key, value in args_dict.items():
        formatted_value = format_value(value, width)
        formatted_args.append(f"{key}={formatted_value}")
    args_str = ", ".join(formatted_args)
    if len(args_str) <= width - 1 - len(func_name) - 2:  # 2 for parentheses
        return f"{func_name}({args_str})"
    else:
        indented_args = indent(",\n".join(formatted_args), " " * indent_spaces)
        return f"{func_name}(\n{indented_args}\n)"
def format_value(value: object, width: int) -> str:
    if isinstance(value, str):
        return f"'{value}'"
    elif isinstance(value, list | tuple | dict):
        return pprint.pformat(value, width=width)
    return str(value)
def format_progress_time(time: float, pad_hours: bool = True) -> str:
    minutes, seconds = divmod(time, 60)
    hours, minutes = divmod(minutes, 60)
    hours_fmt = f"{hours:2.0f}" if pad_hours else f"{hours:.0f}"
    return f"{hours_fmt}:{minutes:02.0f}:{seconds:02.0f}"
def format_template(
    template: str,
    params: dict[str, Any],
    skip_unknown: bool = True,
) -> str:
    """Format a template string, optionally preserving unknown placeholders.
    Args:
        template: A string containing {placeholders} to be formatted
        params: Dictionary of parameters to substitute into the template
        skip_unknown: If True, preserve unknown placeholders; if False, raise KeyError
    Returns:
        The formatted string with parameters substituted
    Examples:
        >>> format_template("Hello {name}!", {"name": "World"})
        'Hello World!'
        >>> format_template("Hello {name}!", {}, skip_unknown=True)
        'Hello {name}!'
    """
    class SafeFormatter(Formatter):
        def get_field(self, field_name: str, args: Any, kwargs: Any) -> Any:
            try:
                # Handle array indexing and nested attributes
                first, rest = (
                    field_name.split(".", 1)
                    if "." in field_name
                    else (field_name, None)
                )
                first = first.split("[")[0]  # Remove any array indexing for the check
                if first not in params and skip_unknown:
                    return "{" + field_name + "}", field_name
                obj = params.get(first)
                if obj is None and skip_unknown:
                    return "{" + field_name + "}", field_name
                return super().get_field(field_name, args, kwargs)
            except (AttributeError, KeyError, IndexError) as e:
                if skip_unknown:
                    return "{" + field_name + "}", field_name
                raise KeyError(f"Failed to format field '{field_name}'") from e
        def format_field(self, value: Any, format_spec: str) -> Any:
            try:
                return super().format_field(value, format_spec)
            except (ValueError, TypeError):
                if skip_unknown:
                    return "{" + str(value) + ":" + format_spec + "}"
                raise
    return SafeFormatter().format(template, **params)

================
File: src/inspect_ai/_util/future.py
================
from typing import Generic, TypeVar
import anyio
T = TypeVar("T")
class Future(Generic[T]):
    def __init__(self) -> None:
        self._result: T | None = None
        self._ex: Exception | None = None
        self._event = anyio.Event()
    def set_result(self, result: T) -> None:
        self._result = result
        self._event.set()
    def set_exception(self, ex: Exception) -> None:
        self._ex = ex
        self._event.set()
    async def result(self) -> T:
        await self._event.wait()
        if self._result is not None:
            return self._result
        elif self._ex is not None:
            raise self._ex
        else:
            raise RuntimeError("Future completed without a result or error")
    @staticmethod
    def set_future_result(future: "Future[T]", result: T) -> None:
        future.set_result(result)
    @staticmethod
    def set_future_exception(future: "Future[T]", error: Exception) -> None:
        future.set_exception(error)

================
File: src/inspect_ai/_util/git.py
================
import shutil
import subprocess
from pydantic import BaseModel
class GitContext(BaseModel):
    origin: str
    commit: str
def git_context() -> GitContext | None:
    # check for git
    git = shutil.which("git")
    if not git:
        return None
    # check for a git revision in this directory
    commit_result = subprocess.run(
        [git, "rev-parse", "--short", "HEAD"], capture_output=True, text=True
    )
    if commit_result.returncode != 0:
        return None
    # check for git origin (if any)
    origin = subprocess.run(
        [git, "remote", "get-url", "origin"],
        capture_output=True,
        text=True,
    ).stdout.strip()
    # return context
    return GitContext(origin=origin, commit=commit_result.stdout.strip())

================
File: src/inspect_ai/_util/hash.py
================
import mmh3
def mm3_hash(message: str) -> str:
    # Generate the 128-bit hash as two 64-bit integers
    h1, h2 = mmh3.hash64(message.encode("utf-8"))  # pylint: disable=E0633
    # Convert to unsigned integers and then to hexadecimal
    return f"{h1 & 0xFFFFFFFFFFFFFFFF:016x}{h2 & 0xFFFFFFFFFFFFFFFF:016x}"

================
File: src/inspect_ai/_util/hooks.py
================
import importlib
import os
from typing import Any, Awaitable, Callable, Literal, cast
from rich import print
from .constants import PKG_NAME
from .error import PrerequisiteError
# Hooks are functions inside packages that are installed with an
# environment variable (e.g. INSPECT_TELEMETRY='mypackage.send_telemetry')
# If one or more hooks are enabled a message will be printed at startup
# indicating this, as well as which package/function implements each hook
# Telemetry (INSPECT_TELEMETRY)
#
# Telemetry can be optionally enabled by setting an INSPECT_TELEMETRY
# environment variable that points to a function in a package which
# conforms to the TelemetrySend signature below. A return value of True
# indicates that the telemetry event was handled.
# There are currently three types of telemetry sent:
#    - model_usage       (JSON string of the model usage)
#    - eval_log_location (file path or URL string of the eval log)
#    - eval_log          (JSON string of the eval log)
#                        [only sent if eval_log_location unhandled]
# The eval_log_location type is preferred over eval_log as it means we can take
# advantage of the .eval format and avoid loading the whole log into memory.
TelemetrySend = Callable[[str, str], Awaitable[bool]]
async def send_telemetry(
    type: Literal["model_usage", "eval_log", "eval_log_location"], json: str
) -> Literal["handled", "not_handled", "no_subscribers"]:
    global _send_telemetry
    if _send_telemetry:
        if await _send_telemetry(type, json):
            return "handled"
        return "not_handled"
    return "no_subscribers"
_send_telemetry: TelemetrySend | None = None
# API Key Override (INSPECT_API_KEY_OVERRIDE)
#
# API Key overrides can be optionally enabled by setting an
# INSPECT_API_KEY_OVERRIDE environment variable which conforms to the
# ApiKeyOverride signature below.
#
# The api key override function will be called with the name and value
# of provider specified environment variables that contain api keys,
# and it can optionally return an override value.
ApiKeyOverride = Callable[[str, str], str | None]
def override_api_key(var: str, value: str) -> str | None:
    global _override_api_key
    if _override_api_key:
        return _override_api_key(var, value)
    else:
        return None
_override_api_key: ApiKeyOverride | None = None
def init_hooks() -> None:
    # messages we'll print for hooks if we have them
    messages: list[str] = []
    # telemetry
    global _send_telemetry
    if not _send_telemetry:
        result = init_hook(
            "telemetry",
            "INSPECT_TELEMETRY",
            "(eval logs and token usage will be recorded by the provider)",
        )
        if result:
            _send_telemetry, message = result
            messages.append(message)
    # api key override
    global _override_api_key
    if not _override_api_key:
        result = init_hook(
            "api key override",
            "INSPECT_API_KEY_OVERRIDE",
            "(api keys will be read and modified by the provider)",
        )
        if result:
            _override_api_key, message = result
            messages.append(message)
    # if any hooks are enabled, let the user know
    if len(messages) > 0:
        version = importlib.metadata.version(PKG_NAME)
        all_messages = "\n".join([f"- {message}" for message in messages])
        print(
            f"[blue][bold]inspect_ai v{version}[/bold][/blue]\n[bright_black]{all_messages}[/bright_black]\n"
        )
def init_hook(
    name: str, env: str, message: str
) -> tuple[Callable[..., Any], str] | None:
    hook = os.environ.get(env, "")
    if hook:
        # parse module/function
        module_name, function_name = hook.strip().rsplit(".", 1)
        # load (fail gracefully w/ clear error)
        try:
            module = importlib.import_module(module_name)
            return (
                cast(Callable[..., Any], getattr(module, function_name)),
                f"[bold]{name} enabled: {hook}[/bold]\n  {message}",
            )
        except (AttributeError, ModuleNotFoundError):
            raise PrerequisiteError(
                f"{env} provider not found: {hook}\n"
                + "Please correct (or undefine) this environment variable before proceeding."
            )
    else:
        return None

================
File: src/inspect_ai/_util/html.py
================
def as_html_id(prefix: str, text: str) -> str:
    id = "".join(c if c.isalnum() else "-" for c in text.lower())
    return f"{prefix}-{id}" if id[0].isdigit() else id

================
File: src/inspect_ai/_util/http.py
================
# see https://cloud.google.com/storage/docs/retry-strategy
def is_retryable_http_status(status_code: int) -> bool:
    return status_code in [408, 429] or (500 <= status_code < 600)

================
File: src/inspect_ai/_util/httpx.py
================
import logging
from typing import Callable
from httpx import ConnectError, ConnectTimeout, HTTPStatusError, ReadTimeout
from tenacity import RetryCallState
from inspect_ai._util.constants import HTTP
logger = logging.getLogger(__name__)
def httpx_should_retry(ex: BaseException) -> bool:
    """Check whether an exception raised from httpx should be retried.
    Implements the strategy described here: https://cloud.google.com/storage/docs/retry-strategy
    Args:
      ex (BaseException): Exception to examine for retry behavior
    Returns:
      True if a retry should occur
    """
    # httpx status exception
    if isinstance(ex, HTTPStatusError):
        # request timeout
        if ex.response.status_code == 408:
            return True
        # lock timeout
        elif ex.response.status_code == 409:
            return True
        # rate limit
        elif ex.response.status_code == 429:
            return True
        # internal errors
        elif ex.response.status_code >= 500:
            return True
        else:
            return False
    # connection error
    elif is_httpx_connection_error(ex):
        return True
    # don't retry
    else:
        return False
def log_httpx_retry_attempt(context: str) -> Callable[[RetryCallState], None]:
    def log_attempt(retry_state: RetryCallState) -> None:
        logger.log(
            HTTP,
            f"{context} connection retry {retry_state.attempt_number} after waiting for {retry_state.idle_for}",
        )
    return log_attempt
def is_httpx_connection_error(ex: BaseException) -> bool:
    return isinstance(ex, ConnectTimeout | ConnectError | ConnectionError | ReadTimeout)

================
File: src/inspect_ai/_util/images.py
================
import base64
import mimetypes
import httpx
from .file import file as open_file
from .url import (
    data_uri_mime_type,
    data_uri_to_base64,
    is_data_uri,
    is_http_url,
)
async def file_as_data(file: str) -> tuple[bytes, str]:
    if is_data_uri(file):
        # resolve mime type and base64 content
        mime_type = data_uri_mime_type(file) or "image/png"
        file_base64 = data_uri_to_base64(file)
        file_bytes = base64.b64decode(file_base64)
    else:
        # guess mime type; need strict=False for webp images
        type, _ = mimetypes.guess_type(file, strict=False)
        if type:
            mime_type = type
        else:
            mime_type = "image/png"
        # handle url or file
        if is_http_url(file):
            client = httpx.AsyncClient()
            file_bytes = (await client.get(file)).content
        else:
            with open_file(file, "rb") as f:
                file_bytes = f.read()
    # return bytes and type
    return file_bytes, mime_type
async def file_as_data_uri(file: str) -> str:
    if is_data_uri(file):
        return file
    else:
        bytes, mime_type = await file_as_data(file)
        base64_file = base64.b64encode(bytes).decode("utf-8")
        file = f"data:{mime_type};base64,{base64_file}"
        return file

================
File: src/inspect_ai/_util/interrupt.py
================
import anyio
from .working import check_sample_working_limit
def check_sample_interrupt() -> None:
    from inspect_ai.log._samples import sample_active
    # check for user interrupt
    sample = sample_active()
    if sample and sample.interrupt_action:
        raise anyio.get_cancelled_exc_class()
    # check for working_limit
    check_sample_working_limit()

================
File: src/inspect_ai/_util/json.py
================
from typing import (
    Any,
    Literal,
    cast,
)
import jsonpatch
from pydantic import BaseModel, Field, JsonValue
from pydantic_core import to_jsonable_python
def jsonable_python(x: Any) -> Any:
    return to_jsonable_python(x, exclude_none=True, fallback=lambda _x: None)
def jsonable_dict(x: Any) -> dict[str, JsonValue]:
    x = to_jsonable_python(x, exclude_none=True, fallback=lambda _x: None)
    if isinstance(x, dict):
        return x
    else:
        raise TypeError(
            f"jsonable_dict must be passed an object with fields (type passed was {type(x)})"
        )
class JsonChange(BaseModel):
    """Describes a change to data using JSON Patch format."""
    op: Literal["remove", "add", "replace", "move", "test", "copy"]
    """Change operation."""
    path: str
    """Path within object that was changed (uses / to delimit levels)."""
    from_: str | None = Field(default=None, alias="from")
    """Location from which data was moved or copied."""
    value: JsonValue = Field(default=None, exclude=False)
    """Changed value."""
    replaced: JsonValue = Field(default=None, exclude=False)
    """Replaced value."""
    model_config = {"populate_by_name": True}
def json_changes(
    before: dict[str, Any], after: dict[str, Any]
) -> list[JsonChange] | None:
    patch = jsonpatch.make_patch(before, after)
    if patch:
        changes: list[JsonChange] = []
        for change in cast(list[Any], patch):
            json_change = JsonChange(**change)
            if json_change.op == "replace":
                paths = json_change.path.split("/")[1:]
                replaced = before
                for path in paths:
                    decoded_path = decode_json_pointer_segment(path)
                    index: Any = (
                        int(decoded_path) if decoded_path.isnumeric() else decoded_path
                    )
                    replaced = replaced[index]
                json_change.replaced = replaced
            changes.append(json_change)
        return changes
    else:
        return None
def decode_json_pointer_segment(segment: str) -> str:
    """Decode a single JSON Pointer segment."""
    # JSON points encode ~ and / because they are special characters
    # this decodes these values (https://www.rfc-editor.org/rfc/rfc6901)
    return segment.replace("~1", "/").replace("~0", "~")

================
File: src/inspect_ai/_util/kvstore.py
================
import sqlite3
from contextlib import AbstractContextManager
from typing import Any, Optional, cast
from .appdirs import inspect_data_dir
class KVStore(AbstractContextManager["KVStore"]):
    def __init__(self, filename: str, max_entries: int | None = None):
        self.filename = filename
        self.max_entries = max_entries
    def __enter__(self) -> "KVStore":
        self.conn = sqlite3.connect(self.filename)
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS kv_store (
                key TEXT PRIMARY KEY,
                value TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        self.conn.commit()
        return self
    def __exit__(self, *excinfo: Any) -> None:
        self.conn.close()
    def put(self, key: str, value: str) -> None:
        # Insert or update the value
        self.conn.execute(
            """
            INSERT OR REPLACE INTO kv_store (key, value, created_at)
            VALUES (?, ?, CURRENT_TIMESTAMP)
        """,
            (key, value),
        )
        # If we have a max_entries limit, remove oldest entries
        if self.max_entries:
            count = self.count()
            if count > self.max_entries:
                self.conn.execute(
                    """
                    DELETE FROM kv_store
                    WHERE key IN (
                        SELECT key FROM kv_store
                        ORDER BY created_at ASC
                        LIMIT ?
                    )
                    """,
                    (max(0, count - self.max_entries),),
                )
        self.conn.commit()
    def get(self, key: str) -> Optional[str]:
        cursor = self.conn.execute("SELECT value FROM kv_store WHERE key = ?", (key,))
        result = cursor.fetchone()
        return result[0] if result else None
    def delete(self, key: str) -> bool:
        cursor = self.conn.execute("DELETE FROM kv_store WHERE key = ?", (key,))
        self.conn.commit()
        return cursor.rowcount > 0
    def count(self) -> int:
        cursor = self.conn.execute("SELECT COUNT(*) FROM kv_store")
        return cast(int, cursor.fetchone()[0])
def inspect_kvstore(name: str, max_entries: int | None = None) -> KVStore:
    filename = inspect_data_dir("kvstore") / f"{name}.db"
    return KVStore(filename.as_posix(), max_entries=max_entries)

================
File: src/inspect_ai/_util/list.py
================
from typing import Callable, TypeVar
T = TypeVar("T")
def remove_last_match_and_after(
    lst: list[T], predicate: Callable[[T], bool]
) -> list[T]:
    last_match_index = max((i for i, x in enumerate(lst) if predicate(x)), default=-1)
    return lst[: last_match_index + 1]

================
File: src/inspect_ai/_util/logger.py
================
import atexit
import os
from logging import (
    INFO,
    WARNING,
    FileHandler,
    Formatter,
    Logger,
    LogRecord,
    addLevelName,
    getLevelName,
    getLogger,
)
from pathlib import Path
import rich
from rich.console import ConsoleRenderable
from rich.logging import RichHandler
from rich.text import Text
from typing_extensions import override
from .constants import (
    ALL_LOG_LEVELS,
    DEFAULT_LOG_LEVEL,
    DEFAULT_LOG_LEVEL_TRANSCRIPT,
    HTTP,
    HTTP_LOG_LEVEL,
    PKG_NAME,
    TRACE,
    TRACE_LOG_LEVEL,
)
from .error import PrerequisiteError
from .trace import (
    TraceFormatter,
    compress_trace_log,
    inspect_trace_file,
    rotate_trace_files,
)
TRACE_FILE_NAME = "trace.log"
# log handler that filters messages to stderr and the log file
class LogHandler(RichHandler):
    def __init__(
        self, capture_levelno: int, display_levelno: int, transcript_levelno: int
    ) -> None:
        super().__init__(capture_levelno, console=rich.get_console())
        self.transcript_levelno = transcript_levelno
        self.display_level = display_levelno
        # log into an external file if requested via env var
        file_logger = os.environ.get("INSPECT_PY_LOGGER_FILE", None)
        self.file_logger = FileHandler(file_logger) if file_logger else None
        if self.file_logger:
            self.file_logger.setFormatter(
                Formatter("%(asctime)s - %(levelname)s - %(message)s")
            )
        # see if the user has a special log level for the file
        file_logger_level = os.environ.get("INSPECT_PY_LOGGER_LEVEL", "")
        if file_logger_level:
            self.file_logger_level = int(getLevelName(file_logger_level.upper()))
        else:
            self.file_logger_level = 0
        # add a trace file handler
        rotate_trace_files()  # remove oldest if > 10 trace files
        env_trace_file = os.environ.get("INSPECT_TRACE_FILE", None)
        trace_file = Path(env_trace_file) if env_trace_file else inspect_trace_file()
        self.trace_logger = FileHandler(trace_file)
        self.trace_logger.setFormatter(TraceFormatter())
        atexit.register(compress_trace_log(self.trace_logger))
        # set trace level
        trace_level = os.environ.get("INSPECT_TRACE_LEVEL", TRACE_LOG_LEVEL)
        self.trace_logger_level = int(getLevelName(trace_level.upper()))
    @override
    def emit(self, record: LogRecord) -> None:
        # write to stderr if we are at or above the threshold
        if record.levelno >= self.display_level:
            super().emit(record)
        # write to file if the log file level matches. if the
        # user hasn't explicitly specified a level then we
        # take the minimum of 'info' and the display level
        if self.file_logger and record.levelno >= (
            self.file_logger_level or min(self.display_level, INFO)
        ):
            self.file_logger.emit(record)
        # write to trace if the trace level matches.
        if self.trace_logger and record.levelno >= self.trace_logger_level:
            self.trace_logger.emit(record)
        # eval log gets transcript level or higher
        if record.levelno >= self.transcript_levelno:
            log_to_transcript(record)
    @override
    def render_message(self, record: LogRecord, message: str) -> ConsoleRenderable:
        return Text.from_ansi(message)
# initialize logging -- this function can be called multiple times
# in the lifetime of the process (the levelno will update globally)
def init_logger(log_level: str | None, log_level_transcript: str | None = None) -> None:
    # backwards compatibility for 'tools'
    if log_level == "sandbox" or log_level == "tools":
        log_level = "trace"
    # register http and tools levels
    addLevelName(HTTP, HTTP_LOG_LEVEL)
    addLevelName(TRACE, TRACE_LOG_LEVEL)
    def validate_level(option: str, level: str) -> None:
        if level not in ALL_LOG_LEVELS:
            log_levels = ", ".join([level.lower() for level in ALL_LOG_LEVELS])
            raise PrerequisiteError(
                f"Invalid {option} '{level.lower()}'. Log level must be one of {log_levels}"
            )
    # resolve default log level
    log_level = (
        log_level if log_level else os.getenv("INSPECT_LOG_LEVEL", DEFAULT_LOG_LEVEL)
    ).upper()
    validate_level("log level", log_level)
    # reolve transcript log level
    log_level_transcript = (
        log_level_transcript
        if log_level_transcript
        else os.getenv("INSPECT_LOG_LEVEL_TRANSCRIPT", DEFAULT_LOG_LEVEL_TRANSCRIPT)
    ).upper()
    validate_level("log file level", log_level_transcript)
    # convert to integer
    levelno = getLevelName(log_level)
    transcript_levelno = getLevelName(log_level_transcript)
    # set capture level for our logs (we won't actually display/write all of them)
    capture_level = min(TRACE, levelno, transcript_levelno)
    # init logging handler on demand
    global _logHandler
    if not _logHandler:
        _logHandler = LogHandler(
            capture_levelno=capture_level,
            display_levelno=levelno,
            transcript_levelno=transcript_levelno,
        )
        # set the log level for our package
        getLogger(PKG_NAME).setLevel(capture_level)
        getLogger(PKG_NAME).addHandler(_logHandler)
        getLogger(PKG_NAME).propagate = False
        # add our logger to the global handlers
        getLogger().addHandler(_logHandler)
        # httpx currently logs all requests at the INFO level
        # this is a bit aggressive and we already do this at
        # our own HTTP level
        getLogger("httpx").setLevel(WARNING)
_logHandler: LogHandler | None = None
def log_to_transcript(record: LogRecord) -> None:
    from inspect_ai.log._message import LoggingMessage
    from inspect_ai.log._transcript import LoggerEvent, transcript
    transcript()._event(LoggerEvent(message=LoggingMessage._from_log_record(record)))
def warn_once(logger: Logger, message: str) -> None:
    if message not in _warned:
        logger.warning(message)
        _warned.append(message)
_warned: list[str] = []

================
File: src/inspect_ai/_util/notebook.py
================
import io
import sys
import types
from pathlib import Path
from typing import Callable
from IPython import get_ipython  # type: ignore
from IPython.core.interactiveshell import InteractiveShell
from nbformat import NBFormatError, ValidationError, read
from nbformat.reader import NotJSONError
# from https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Importing%20Notebooks.html
class NotebookLoader(object):
    """Module Loader for Jupyter Notebooks"""
    def __init__(self, exec_filter: Callable[[list[str]], bool] | None = None) -> None:
        self.shell = InteractiveShell.instance()
        self.exec_filter = exec_filter
    def load_module(self, fullname: str) -> types.ModuleType:
        # load the notebook object
        with io.open(fullname, "r", encoding="utf-8") as f:
            nb = read(f, 4)  # type: ignore
        # create the module and add it to sys.modules
        # if name in sys.modules:
        #    return sys.modules[name]
        mod = types.ModuleType(fullname)
        mod.__file__ = fullname
        mod.__loader__ = self
        mod.__dict__["get_ipython"] = get_ipython
        sys.modules[fullname] = mod
        # extra work to ensure that magics that would affect the user_ns
        # actually affect the notebook module's ns
        save_user_ns = self.shell.user_ns
        self.shell.user_ns = mod.__dict__
        try:
            # get source code for all the calls
            cells_code: list[str] = []
            for cell in nb.cells:
                # transform the input to executable Python for each cell
                if cell.cell_type == "code":
                    code = self.shell.input_transformer_manager.transform_cell(
                        cell.source
                    )
                    cells_code.append(code)
            # check the exec filter to make sure we should execute the
            # notebook cells, if not just return an empty module
            if self.exec_filter and not self.exec_filter(cells_code):
                del sys.modules[fullname]
                return mod
            # run the code in each cell
            for code in cells_code:
                exec(code, mod.__dict__)
            return mod
        finally:
            self.shell.user_ns = save_user_ns
def read_notebook_code(path: Path) -> str:
    try:
        # load the notebook object
        with io.open(path, "r", encoding="utf-8") as f:
            nb = read(f, 4)  # type: ignore
    except NotJSONError:
        return ""
    except ValidationError:
        return ""
    except NBFormatError:
        return ""
    # for dealing w/ magics
    shell = InteractiveShell.instance()
    # get the code
    lines: list[str] = []
    for cell in nb.cells:
        # transform the input to executable Python for each cell
        if cell.cell_type == "code":
            code = shell.input_transformer_manager.transform_cell(cell.source)
            lines.append(code)
    return "\n".join(lines)

================
File: src/inspect_ai/_util/notgiven.py
================
# Sentinel class used until PEP 0661 is accepted
from typing import Literal
from typing_extensions import override
class NotGiven:
    """A sentinel singleton class used to distinguish omitted keyword arguments from those passed in with the value None (which may have different behavior)."""
    def __bool__(self) -> Literal[False]:
        return False
    @override
    def __repr__(self) -> str:
        return "NOT_GIVEN"
NOT_GIVEN = NotGiven()

================
File: src/inspect_ai/_util/package.py
================
import importlib.util
import inspect
import json
import site
import sys
from functools import lru_cache
from importlib.metadata import Distribution, PackageNotFoundError
from typing import Any
def get_installed_package_name(obj: Any) -> str | None:
    # special handling for built-in functions
    if inspect.isbuiltin(obj):
        # try to get the module name
        if getattr(obj, "__module__") is not None:
            module_name = obj.__module__
            if module_name:
                return module_name.split(".")[0]
        # try to get the class that defines this method
        if hasattr(obj, "__objclass__"):
            cls = obj.__objclass__
        elif hasattr(obj, "__self__"):
            cls = type(obj.__self__)
        else:
            return None
        for base_cls in inspect.getmro(cls):
            module = inspect.getmodule(base_cls)
            if module:
                return module.__name__.split(".")[0]
    # get the module of the object
    module = inspect.getmodule(obj)
    if module is None:
        return None
    # find the origin (install path) for the module
    module_name = module.__name__
    try:
        spec = importlib.util.find_spec(module_name)
    except Exception:
        return None
    if spec is None or spec.origin is None:
        return None
    # check if this is a package (either in library or installed editable)
    package_name = module_name.split(".")[0]
    if package_path_is_in_site_packages(spec.origin):
        return package_name
    if package_is_installed_editable(package_name):
        return package_name
    else:
        return None
@lru_cache(maxsize=None)
def package_path_is_in_site_packages(path: str) -> bool:
    path = path.lower()
    return (
        any(path.startswith(p.lower()) for p in site.getsitepackages())
        or path.startswith(site.getusersitepackages().lower())
        or any(
            "site-packages" in p.lower() and path.startswith(p.lower())
            for p in sys.path
        )
    )
@lru_cache(maxsize=None)
def package_is_installed_editable(package: str) -> bool:
    # get the distribution
    try:
        distribution = Distribution.from_name(package)
    except (ValueError, PackageNotFoundError):
        return False
    # read the direct_url json
    direct_url_json = distribution.read_text("direct_url.json")
    if not direct_url_json:
        return False
    # parse the json
    try:
        direct_url = json.loads(direct_url_json)
        if not isinstance(direct_url, dict):
            return False
    except json.JSONDecodeError:
        return False
    # read the editable property
    return direct_url.get("dir_info", {}).get("editable", False) is not False

================
File: src/inspect_ai/_util/path.py
================
import os
import sys
from collections import deque
from contextlib import AbstractContextManager, contextmanager
from copy import deepcopy
from pathlib import PurePath
from typing import Any, Iterator, overload
@contextmanager
def add_to_path(p: str) -> Iterator[None]:
    old_path = sys.path
    sys.path = sys.path[:]
    sys.path.insert(0, p)
    try:
        yield
    finally:
        sys.path = old_path
# NOTE: this code is adapted from
# https://github.com/python/cpython/blob/b3722ca058f6a6d6505cf2ea9ffabaf7fb6b6e19/Lib/contextlib.py#L767-L779)
class chdir(AbstractContextManager[None]):
    """Non thread-safe context manager to change the working directory.
    Changes the current working directory
    """
    def __init__(self, path: str):
        self.path = path
        self._old_cwd: list[str] = []
    def __enter__(self) -> None:
        self._old_cwd.append(os.getcwd())
        os.chdir(self.path)
    def __exit__(self, *excinfo: Any) -> None:
        os.chdir(self._old_cwd.pop())
class add_to_syspath(AbstractContextManager[None]):
    """Non thread-safe context manager to add to the python syspath."""
    def __init__(self, path: str):
        self.path = path
        self._old_sys_path: list[list[str]] = []
    def __enter__(self) -> None:
        self._old_sys_path.append(deepcopy(sys.path))
        sys.path.append(self.path)
    def __exit__(self, *excinfo: Any) -> None:
        sys.path = self._old_sys_path.pop()
class chdir_python(AbstractContextManager[None]):
    """Non thread-safe context manager to change the runtime Python directory.
    Changes the current working directory and adds the directory to
    the Python sys.path (so local module references resolve correctly).
    """
    def __init__(self, path: str):
        self.path = path
        self._old_sys_path: list[list[str]] = []
        self._old_cwd: list[str] = []
    def __enter__(self) -> None:
        self._old_cwd.append(os.getcwd())
        self._old_sys_path.append(deepcopy(sys.path))
        os.chdir(self.path)
        sys.path.append(self.path)
    def __exit__(self, *excinfo: Any) -> None:
        os.chdir(self._old_cwd.pop())
        sys.path = self._old_sys_path.pop()
@overload
def cwd_relative_path(file: str, walk_up: bool = False) -> str: ...
@overload
def cwd_relative_path(file: None, walk_up: bool = False) -> None: ...
def cwd_relative_path(file: str | None, walk_up: bool = False) -> str | None:
    if file:
        cwd = PurePath(os.getcwd())
        task_path = PurePath(file)
        if task_path.is_relative_to(cwd):
            return task_path.relative_to(cwd).as_posix()
        elif walk_up:
            return relative_walk(cwd, task_path)
        else:
            return file
    else:
        return None
# A slightly modified implementation of task_path.relative(d, walk_up=True)
# since that wasn't introduced until python 3.12
def relative_walk(from_path: PurePath, to_path: PurePath) -> str:
    if from_path.anchor != to_path.anchor:
        raise ValueError(
            f"{str(from_path)!r} and {str(to_path)!r} have different anchors"
        )
    from_parts = deque(from_path.parts)
    to_parts = deque(to_path.parts)
    while from_parts and to_parts and from_parts[0] == to_parts[0]:
        from_parts.popleft()
        to_parts.popleft()
    # Process the remaining segments in the base_parts
    relative_parts: list[str] = []
    for part in from_parts:
        if not part or part == ".":
            pass
        elif part == "..":
            raise ValueError(f"'..' segment in {str(from_path)!r} cannot be walked")
        else:
            relative_parts.append("..")
    # Add the remaining parts of other_parts
    relative_parts.extend(to_parts)
    return str(PurePath(*relative_parts))

================
File: src/inspect_ai/_util/pattern.py
================
ANSWER_PATTERN_LETTER = r"(?i)ANSWER\s*:\s*([A-Za-z])(?:[^\w]|\n|$)"
ANSWER_PATTERN_WORD = r"(?i)ANSWER\s*:\s*(\w+)(?:\n|$)"
ANSWER_PATTERN_LINE = r"(?i)ANSWER\s*:\s*([^\n]+)"

================
File: src/inspect_ai/_util/platform.py
================
import importlib.util
import os
from inspect_ai._util._async import init_nest_asyncio
from .error import set_exception_hook
def running_in_notebook() -> bool:
    try:
        from IPython import get_ipython  # type: ignore
        if "IPKernelApp" not in get_ipython().config:  # type: ignore
            return False
    except ImportError:
        return False
    except AttributeError:
        return False
    return True
def platform_init() -> None:
    # set exception hook if we haven't already
    set_exception_hook()
    # if we are running in a notebook...
    if running_in_notebook():
        # check for required packages
        if not have_package("ipywidgets"):
            raise ModuleNotFoundError(
                "To using inspect_ai within a notebook, please install ipywidgets with:\n\n"
                + "pip install ipywidgets\n"
            )
        # setup nested asyncio
        init_nest_asyncio()
def have_package(package: str) -> bool:
    return importlib.util.find_spec(package) is not None
def is_running_in_jupyterlab() -> bool:
    return os.getenv("JPY_SESSION_NAME", None) is not None
def is_running_in_vscode() -> bool:
    # Check if running in VS Code Jupyter notebook or interactive window
    if (
        os.getenv("VSCODE_IPYTHON_KERNEL") is not None
        or os.getenv("VSCODE_CLI_REQUIRE_TOKEN") is not None
        or os.getenv("VSCODE_PID") is not None
        or os.getenv("VSCODE_CWD") is not None
    ):
        return True
    # Check if running in a VS Code terminal
    if os.getenv("TERM_PROGRAM") == "vscode":
        return True
    # If none of the conditions are met, we assume it's not running in VS Code
    return False
def is_windows() -> bool:
    return os.name == "nt"

================
File: src/inspect_ai/_util/port_names.py
================
from typing import Literal
def get_service_by_port(port: int, protocol: Literal["tcp", "udp"]) -> str | None:
    """
    Returns the likely service running on a given port number.
    Args:
        port (int): The port number to look up
        protocol (str): Either 'tcp' or 'udp'
    Returns:
        str: Description of the likely service, or None if not found
    """
    # Common port mappings based on IANA assignments and common usage
    port_mappings = {
        "tcp": {
            20: "FTP (Data)",
            21: "FTP (Control)",
            22: "SSH",
            23: "Telnet",
            25: "SMTP",
            53: "DNS",
            80: "HTTP",
            110: "POP3",
            143: "IMAP",
            443: "HTTPS",
            445: "Microsoft-DS (SMB)",
            587: "SMTP (Submission)",
            993: "IMAPS",
            995: "POP3S",
            1433: "Microsoft SQL Server",
            1521: "Oracle Database",
            3306: "MySQL",
            3389: "RDP (Remote Desktop)",
            5432: "PostgreSQL",
            5900: "VNC",
            5901: "VNC Display :1",
            5902: "VNC Display :2",
            6080: "noVNC",
            8080: "HTTP Alternate",
            8443: "HTTPS Alternate",
            27017: "MongoDB",
            27018: "MongoDB Shard",
            27019: "MongoDB Config Server",
        },
        "udp": {
            53: "DNS",
            67: "DHCP Server",
            68: "DHCP Client",
            69: "TFTP",
            123: "NTP",
            161: "SNMP",
            162: "SNMP Trap",
            514: "Syslog",
            1194: "OpenVPN",
            5353: "mDNS",
        },
    }
    return port_mappings.get(protocol, {}).get(port, None)

================
File: src/inspect_ai/_util/registry.py
================
import inspect
from inspect import get_annotations, isclass
from typing import Any, Callable, Literal, TypedDict, TypeGuard, cast
from pydantic import BaseModel, Field
from pydantic_core import to_jsonable_python
from inspect_ai._util.json import jsonable_python
from inspect_ai._util.package import get_installed_package_name
from .constants import PKG_NAME
from .entrypoints import ensure_entry_points
obj_type = type
RegistryType = Literal[
    "modelapi",
    "task",
    "solver",
    "plan",
    "scorer",
    "metric",
    "tool",
    "sandboxenv",
    "score_reducer",
    "approver",
]
class RegistryInfo(BaseModel):
    type: RegistryType
    name: str
    metadata: dict[str, Any] = Field(default_factory=dict)
def registry_add(o: object, info: RegistryInfo) -> None:
    r"""Add an object to the registry.
    Add the passed object to the registry using the RegistryInfo
    to index it for retrieval. The RegistryInfo is also added
    to the object as an attribute, which can retrieved by calling
    registry_info() on an object instance.
    Args:
        o (object): Object to be registered (Metric, Solver, etc.)
        info (RegistryInfo): Metadata (name, etc.) for object.
    """
    # tag the object
    setattr(o, REGISTRY_INFO, info)
    # add to registry
    _registry[registry_key(info.type, info.name)] = o
def registry_tag(
    type: Callable[..., Any],
    o: object,
    info: RegistryInfo,
    *args: Any,
    **kwargs: Any,
) -> None:
    r"""Tag an object w/ registry info.
    Tag the passed object with RegistryInfo. This function DOES NOT
    add the object to the registry (call registry_add() to both
    tag and add an object to the registry). Call registry_info()
    on a tagged/registered object to retrieve its info
    Args:
        type (T): type of object being tagged
        o (object): Object to be registered (Metric, Solver, etc.)
        info (RegistryInfo): Metadata (name, etc.) for object.
        *args (list[Any]): Creation arguments
        **kwargs (dict[str,Any]): Creation keyword arguments
    """
    # bind arguments to params
    named_params: dict[str, Any] = {}
    bound_params = inspect.signature(type).bind(*args, **kwargs)
    for param, value in bound_params.arguments.items():
        named_params[param] = registry_value(value)
    # callables are not serializable so use their names
    for param in named_params.keys():
        if is_registry_object(named_params[param]):
            named_params[param] = registry_info(named_params[param]).name
        elif callable(named_params[param]) and hasattr(named_params[param], "__name__"):
            named_params[param] = getattr(named_params[param], "__name__")
        elif isinstance(named_params[param], dict | list | BaseModel):
            named_params[param] = to_jsonable_python(
                named_params[param], fallback=lambda x: getattr(x, "__name__", None)
            )
        elif isinstance(named_params[param], str | int | float | str | bool | None):
            named_params[param] = named_params[param]
        else:
            named_params[param] = (
                getattr(named_params[param], "name", None)
                or getattr(named_params[param], "__name__", None)
                or getattr(obj_type(named_params[param]), "__name__", None)
                or "<unknown>"
            )
    # set attribute
    setattr(o, REGISTRY_INFO, info)
    setattr(o, REGISTRY_PARAMS, named_params)
def registry_name(o: object, name: str) -> str:
    r"""Compute the registry name of an object.
    This function checks whether the passed object is in a package,
    and if it is, prepends the package name as a namespace
    """
    package = get_installed_package_name(o)
    return f"{package}/{name}" if package else name
def registry_lookup(type: RegistryType, name: str) -> object | None:
    r"""Lookup an object in the registry by type and name.
    Objects that defined in inspect extension packages (i.e. not
    directly within the core inspect_ai package) must be namespaced
    (e.g. "fancy_prompts/jailbreaker")
    Args:
        type: Type of object to find
        name: Name of object to find
    Returns:
        Object or None if not found.
    """
    def _lookup() -> object | None:
        # first try
        object = _registry.get(registry_key(type, name))
        if object:
            return object
        # unnamespaced objects can also be found in inspect_ai
        elif name.find("/") == -1:
            return _registry.get(registry_key(type, f"{PKG_NAME}/{name}"))
        else:
            return None
    o = _lookup()
    # try to recover
    if o is None:
        # load entry points for this package as required
        if name.find("/") != -1 and name.find(".") == -1:
            package = name.split("/")[0]
            ensure_entry_points(package)
        return _lookup()
    else:
        return o
def registry_find(predicate: Callable[[RegistryInfo], bool]) -> list[object]:
    r"""Find objects in the registry that match the passed predicate.
    Args:
        predicate (Callable[[RegistryInfo], bool]): Predicate to find
    Returns:
        List of registry objects found
    """
    def _find() -> list[object]:
        return [
            object for object in _registry.values() if predicate(registry_info(object))
        ]
    o = _find()
    if len(o) == 0:
        ensure_entry_points()
        return _find()
    else:
        return o
def registry_create(type: RegistryType, name: str, **kwargs: Any) -> object:
    r"""Create a registry object.
    Registry objects can be ordinary functions that implement a protocol,
    factory functions that return a function based on **kwargs, or classes
    deriving that can be created using **kwargs
    Args:
        type (RegistryType): Type of registry object to create
        name (str): Name of registry options to create
        **kwargs (Any): Optional creation arguments
    Returns:
        Registry object with registry info attribute
    """
    # lookup the object
    obj = registry_lookup(type, name)
    # forward registry info to the instantiated object
    def with_registry_info(o: object) -> object:
        return set_registry_info(o, registry_info(obj))
    # instantiate registry and model objects
    for param in kwargs.keys():
        value = kwargs[param]
        if is_registry_dict(value):
            kwargs[param] = registry_create(
                value["type"], value["name"], **value["params"]
            )
        elif is_model_dict(value):
            kwargs[param] = model_create_from_dict(value)
    if isclass(obj):
        return with_registry_info(obj(**kwargs))
    elif callable(obj):
        return_type = get_annotations(obj).get("return")
        # Until we remove the MetricDeprecated symbol we need this extra
        # bit to map the Metric union back to Metric
        if "_metric.Metric" in str(return_type):
            return_type = "Metric"
        else:
            return_type = getattr(return_type, "__name__", None)
        if return_type and return_type.lower() == type:
            return with_registry_info(obj(**kwargs))
        else:
            return obj
    else:
        raise ValueError(f"{name} was not found in the registry")
def registry_info(o: object) -> RegistryInfo:
    r"""Lookup RegistryInfo for an object.
    Args:
        o (object): Object to lookup info for
    Returns:
        RegistryInfo for object.
    """
    info = getattr(o, REGISTRY_INFO, None)
    if info is not None:
        return cast(RegistryInfo, info)
    else:
        name = getattr(o, "__name__", "unknown")
        decorator = " @solver " if name == "solve" else " "
        raise ValueError(
            f"Object '{name}' does not have registry info. Did you forget to add a{decorator}decorator somewhere?"
        )
def registry_params(o: object) -> dict[str, Any]:
    r"""Lookup parameters used to instantiate a registry object.
    Args:
        o (object): Object to lookup info for
    Returns:
        Dictionary of parameters used to instantiate object.
    """
    params = getattr(o, REGISTRY_PARAMS, None)
    if params is not None:
        return cast(dict[str, Any], params)
    else:
        raise ValueError("Object does not have registry info")
def registry_log_name(o: str | object) -> str:
    r"""Name of object for logging.
    Registry objects defined by the inspect_ai package have their
    prefix stripped when written to the log (they in turn can also
    be created/referenced without the prefix).
    Args:
        o (str | object): Name or object to get name for
    Returns:
        Name of object for logging.
    """
    name = o if isinstance(o, str) else registry_info(o).name
    return name.replace(f"{PKG_NAME}/", "", 1)
def registry_unqualified_name(o: str | object | RegistryInfo) -> str:
    r"""Unqualified name of object (i.e. without package prefix).
    Args:
        o (str | object | RegistryInfo): string, registry object, or RegistryInfo to get unqualified name for.
    Returns:
        Unqualified name of object
    """
    if isinstance(o, str):
        name = o
    else:
        info = o if isinstance(o, RegistryInfo) else registry_info(o)
        name = info.name
    parts = name.split("/")
    if len(parts) == 1:
        return parts[0]
    else:
        return "/".join(parts[1:])
def is_registry_object(o: object, type: RegistryType | None = None) -> bool:
    r"""Check if an object is a registry object.
    Args:
        o (object): Object to lookup info for
        type: (RegistryType | None): Optional. Check for a specific type
    Returns:
        True if the object is a registry object (optionally of the specified
        type). Otherwise, False
    """
    info = getattr(o, REGISTRY_INFO, None)
    if info:
        reg_info = cast(RegistryInfo, info)
        if type:
            return reg_info.type == type
        else:
            return True
    else:
        return False
def set_registry_info(o: object, info: RegistryInfo) -> object:
    r"""Set the RegistryInfo for an object.
    Args:
        o (object): Object to set the registry info for
        info: (object): Registry info
    Returns:
        Passed object, with RegistryInfo attached
    """
    setattr(o, REGISTRY_INFO, info)
    return o
def set_registry_params(o: object, params: dict[str, Any]) -> object:
    r"""Set the registry params for an object.
    Args:
        o (object): Object to set the registry params for
        params: (dict[str, Any]): Registry params
    Returns:
        Passed object, with registry params attached
    """
    setattr(o, REGISTRY_PARAMS, params)
    return o
def has_registry_params(o: object) -> bool:
    r"""Check if the object has registry params.
    Args:
        o (object): Object to check.
    Returns:
        True if the object has registry params, else False.
    """
    return is_registry_object(o) and hasattr(o, REGISTRY_PARAMS)
def registry_key(type: RegistryType, name: str) -> str:
    return f"{type}:{name}"
REGISTRY_INFO = "__registry_info__"
REGISTRY_PARAMS = "__registry_params__"
_registry: dict[str, object] = {}
class RegistryDict(TypedDict):
    type: RegistryType
    name: str
    params: dict[str, Any]
def is_registry_dict(o: object) -> TypeGuard[RegistryDict]:
    return isinstance(o, dict) and "type" in o and "name" in o and "params" in o
def registry_value(o: object) -> Any:
    from inspect_ai.model._model import Model
    # treat tuple as list
    if isinstance(o, tuple):
        o = list(o)
    # recurse through collection types
    if isinstance(o, list):
        return [registry_value(x) for x in o]
    elif isinstance(o, dict):
        return {k: registry_value(v) for k, v in o.items()}
    elif has_registry_params(o):
        return RegistryDict(
            type=registry_info(o).type,
            name=registry_log_name(o),
            params=registry_params(o),
        )
    elif isinstance(o, Model):
        return ModelDict(
            model=str(o),
            config=jsonable_python(o.config),
            base_url=o.api.base_url,
            model_args=o.model_args,
        )
    else:
        return o
def registry_create_from_dict(d: RegistryDict) -> object:
    return registry_create(d["type"], d["name"], **d["params"])
class ModelDict(TypedDict):
    model: str
    config: dict[str, Any]
    base_url: str | None
    model_args: dict[str, Any]
def is_model_dict(o: object) -> TypeGuard[ModelDict]:
    return (
        isinstance(o, dict)
        and "model" in o
        and "config" in o
        and "base_url" in o
        and "model_args" in o
    )
def model_create_from_dict(d: ModelDict) -> object:
    from inspect_ai.model._generate_config import GenerateConfig
    from inspect_ai.model._model import get_model
    return get_model(
        d["model"],
        config=GenerateConfig(**d["config"]),
        base_url=d["base_url"],
        **d["model_args"],
    )

================
File: src/inspect_ai/_util/retry.py
================
_http_retries_count: int = 0
def report_http_retry() -> None:
    from inspect_ai.log._samples import report_active_sample_retry
    # bump global counter
    global _http_retries_count
    _http_retries_count = _http_retries_count + 1
    # report sample retry
    report_active_sample_retry()
def http_retries_count() -> int:
    return _http_retries_count

================
File: src/inspect_ai/_util/rich.py
================
from rich.console import RenderableType
from rich.style import Style
from rich.text import Text
from inspect_ai._util.text import truncate_lines
def lines_display(
    text: str, max_lines: int = 100, style: str | Style = ""
) -> list[RenderableType]:
    lines, truncated = truncate_lines(text, max_lines)
    content: list[RenderableType] = [Text(lines, style=style)]
    if truncated is not None:
        content.append(Text())
        content.append(
            Text.from_markup(
                f"[italic]Output truncated ({truncated} additional lines)...[/italic]",
                style=style,
            )
        )
    return content

================
File: src/inspect_ai/_util/samples.py
================
def parse_samples_limit(limit: str | None) -> int | tuple[int, int] | None:
    if limit is not None:
        if "-" not in limit:
            return int(limit)
        else:
            limit_split = [int(r) for r in limit.split("-")]
            return (limit_split[0] - 1, limit_split[1])
    else:
        return None
def parse_sample_id(sample_id: str | None) -> list[str] | None:
    if sample_id is not None:
        return [id.strip() for id in sample_id.split(",")]
    else:
        return None

================
File: src/inspect_ai/_util/terminal.py
================
import functools
import re
import select
import sys
from dataclasses import dataclass
from logging import getLogger
from typing import Any
logger = getLogger(__name__)
@dataclass
class RGB:
    r: int
    g: int
    b: int
@dataclass
class TerminalBackground:
    color: RGB
    brightness: float
    dark: bool
@functools.cache
def detect_terminal_background(
    default_color: RGB = RGB(0, 0, 0),
) -> TerminalBackground:
    """Query the terminal background color using OSC escape sequence.
    Based on https://dystroy.org/blog/terminal-light/#detect-whether-the-terminal-is-dark-or-light
    and https://github.com/Canop/terminal-light/blob/main/src/xterm.rs
    The `default_color` parameter ensures that you always get back a color
    even if when on windows or if an error occurs while querying the terminal
    (dark terminal is detected in this case).
    Args:
        default_color (Rgb): Default color in the case that we
          are unable to successfully query for colors.
    Returns:
        TerminalBackground: Terminal background color, brightness, and type.
    """
    def background_from_color(color: RGB) -> TerminalBackground:
        # compute brightness
        brightness = (color.r * 299 + color.g * 587 + color.b * 114) / 1000
        # return background
        return TerminalBackground(
            color=color, brightness=brightness, dark=brightness <= 128
        )
    # this does not work on windows so in that case we return the default
    if sys.platform == "win32":
        return background_from_color(default_color)
    try:
        # Send OSC 11 query for background color
        response = _query("\x1b]11;?\x07", 500)
        # Parse the response
        # Expected format: ]11;rgb:RRRR/GGGG/BBBB
        match = re.search(
            r"]11;rgb:([0-9a-fA-F]{2,4})/([0-9a-fA-F]{2,4})/([0-9a-fA-F]{2,4})",
            response,
        )
        if not match:
            raise RuntimeError(f"Unexpected OSC response format: {response}")
        # Extract RGB values (using first 2 digits of each component)
        r = int(match.group(1)[:2], 16)
        g = int(match.group(2)[:2], 16)
        b = int(match.group(3)[:2], 16)
        color = RGB(r, g, b)
        # return background
        return background_from_color(color)
    except Exception as e:
        logger.debug("Error attempting to query terminal background color: " + str(e))
        return background_from_color(default_color)
if sys.platform != "win32":
    import termios
    import tty
    def _query(query_str: str, timeout_ms: int) -> str:
        """Send a query to the terminal and wait for response"""
        old_settings = None
        try:
            switch_to_raw = not _is_raw_mode_enabled()
            if switch_to_raw:
                old_settings = _enable_raw_mode()
            # Send the query
            sys.stdout.write(query_str)
            sys.stdout.flush()
            # Wait for response
            readable, _, _ = select.select([sys.stdin], [], [], timeout_ms / 1000.0)
            if not readable:
                raise RuntimeError("Timeout waiting for terminal query response")
            # Read response
            response: str = ""
            while True:
                char = sys.stdin.read(1)
                response += char
                if char == "\\" or (len(response) > 1 and response[-2:] == "\x1b\\"):
                    break
            return response
        finally:
            if old_settings is not None:
                _disable_raw_mode(old_settings)
    def _is_raw_mode_enabled() -> bool:
        """Check if the terminal is in raw mode"""
        mode = termios.tcgetattr(sys.stdin.fileno())
        return not bool(mode[3] & termios.ICANON)
    def _enable_raw_mode() -> Any:
        """Enable raw mode for the terminal"""
        fd = sys.stdin.fileno()
        old_settings = termios.tcgetattr(fd)
        tty.setraw(fd)
        return old_settings
    def _disable_raw_mode(old_settings: Any) -> None:
        """Disable raw mode for the terminal"""
        fd = sys.stdin.fileno()
        termios.tcsetattr(fd, termios.TCSADRAIN, old_settings)

================
File: src/inspect_ai/_util/text.py
================
import random
import re
import string
from logging import getLogger
from typing import List, NamedTuple
logger = getLogger(__name__)
def strip_punctuation(s: str) -> str:
    return s.strip(string.whitespace + string.punctuation)
def strip_numeric_punctuation(s: str) -> str:
    # strip $, , , and ,
    stripped = re.sub(r"[$,,]", "", s)
    # strip . if it's followed by a space, the end of the string,
    # or a non-digit character
    stripped = re.sub(r"\.(?=\s|$|\D)", "", stripped)
    return stripped
class TruncatedOutput(NamedTuple):
    output: str
    original_bytes: int
def truncate_string_to_bytes(input: str, max_bytes: int) -> TruncatedOutput | None:
    """Truncate a string to a maximum number of bytes.
    Args:
       input (str): String to truncate
       max_bytes (int): Maximum number of bytes
    Returns:
       Tuple of truncated string, original size if truncation occurred,
       otherwise None
    """
    # early return for empty string or max_bytes of 0
    if not input or max_bytes <= 0:
        return None
    # fast path for ASCII strings
    if input.isascii():
        if len(input) <= max_bytes:
            return None
        else:
            return TruncatedOutput(input[:max_bytes], len(input))
    # fast path for smaller strings (4 bytes/char is max for unicode)
    if len(input) * 4 <= max_bytes:
        return None
    # encode and truncate (never fail, just warn)
    try:
        encoded = input.encode("utf-8", errors="replace")
        if len(encoded) <= max_bytes:
            return None
        else:
            return TruncatedOutput(
                encoded[:max_bytes].decode("utf-8", errors="replace"), len(encoded)
            )
    except Exception as ex:
        logger.warning(f"Unexpected error occurred truncating string: {ex}")
        return None
def str_to_float(s: str) -> float:
    """Convert a str to float, including handling exponent characters.
    The Python isnumeric() function returns True for strings that include exponents
    (e.g. 5) however the float() function doesn't handle exponents. This function
    will correctly handle these exponents when converting from str to float.
    Args:
       s (str): String to convert to float
    Returns:
       float: Converted value
    Raises:
       ValueError: If the string is not a valid numeric value.
    """
    # handle empty input
    if not s:
        raise ValueError("Input string is empty.")
    superscript_map = str.maketrans("", "0123456789")
    superscript_chars = ""
    base_part = ""
    exponent_part = ""
    for idx, char in enumerate(s):
        if char in superscript_chars:
            base_part = s[:idx]
            exponent_part = s[idx:]
            break
    else:
        base_part = s
    # handle empty base (e.g., '')
    base = float(base_part) if base_part else 1.0
    # handle exponent part
    if exponent_part:
        exponent_str = exponent_part.translate(superscript_map)
        exponent = int(exponent_str)
    else:
        exponent = 1  # Default exponent is 1 if no superscript is present
    return base**exponent
def truncate(text: str, length: int, overflow: str = "...", pad: bool = True) -> str:
    """
    Truncate text to specified length with optional padding and overflow indicator.
    Args:
        text (str): Text to truncate
        length (int): Maximum length including overflow indicator
        overflow (str): String to indicate truncation (defaults to '...')
        pad (bool): Whether to pad the result to full length (defaults to padding)
    Returns:
        Truncated string, padded if requested
    """
    if len(text) <= length:
        return text + (" " * (length - len(text))) if pad else text
    overflow_length = len(overflow)
    truncated = text[: length - overflow_length] + overflow
    return truncated
def truncate_lines(
    text: str, max_lines: int = 100, max_characters: int | None = 100 * 100
) -> tuple[str, int | None]:
    if max_characters is not None:
        text = truncate(text, max_characters)
    lines = text.splitlines()
    if len(lines) > max_lines:
        output = "\n".join(lines[0:max_lines])
        return output, len(lines) - max_lines
    else:
        return text, None
def generate_large_text(target_tokens: int) -> str:
    """Generate a large amount of text with approximately the target number of tokens"""
    generated_text = []
    estimated_tokens = 0
    while estimated_tokens < target_tokens:
        sentence = generate_sentence()
        # Add paragraph breaks occasionally
        if random.random() < 0.1:
            sentence += "\n\n"
        generated_text.append(sentence)
        # Rough estimate of tokens (words + punctuation)
        estimated_tokens += len(sentence.split()) + 2
    return " ".join(generated_text)
def generate_sentence() -> str:
    """Generate a random sentence using predefined templates"""
    adjectives, nouns, verbs = create_word_lists()
    templates = [
        f"The {random.choice(adjectives)} {random.choice(nouns)} {random.choice(verbs)} the {random.choice(adjectives)} {random.choice(nouns)}.",
        f"A {random.choice(adjectives)} {random.choice(nouns)} {random.choice(verbs)} near the {random.choice(nouns)}.",
        f"In the {random.choice(adjectives)} {random.choice(nouns)}, the {random.choice(nouns)} {random.choice(verbs)} {random.choice(adjectives)}.",
        f"When the {random.choice(nouns)} {random.choice(verbs)}, a {random.choice(adjectives)} {random.choice(nouns)} {random.choice(verbs)}.",
        f"The {random.choice(nouns)} {random.choice(verbs)} while the {random.choice(adjectives)} {random.choice(nouns)} {random.choice(verbs)}.",
    ]
    return random.choice(templates)
def create_word_lists() -> tuple[List[str], List[str], List[str]]:
    """Create basic word lists for sentence generation"""
    # Common adjectives
    adjectives = [
        "red",
        "blue",
        "green",
        "dark",
        "bright",
        "quiet",
        "loud",
        "small",
        "large",
        "quick",
        "slow",
        "happy",
        "sad",
        "clever",
        "wise",
        "ancient",
        "modern",
        "complex",
        "simple",
        "elegant",
        "rough",
        "smooth",
        "sharp",
        "dull",
        "fresh",
        "stale",
        "clean",
        "dirty",
        "heavy",
        "light",
        "hot",
        "cold",
        "dry",
        "wet",
        "rich",
        "poor",
        "thick",
        "thin",
        "strong",
        "weak",
        "early",
        "late",
        "young",
        "old",
        "good",
        "bad",
        "high",
        "low",
        "long",
        "short",
        "deep",
        "shallow",
        "hard",
        "soft",
        "near",
        "far",
        "wide",
        "narrow",
        "big",
        "little",
        "fast",
        "slow",
        "busy",
        "lazy",
        "new",
        "old",
        "full",
        "empty",
        "loud",
        "quiet",
        "sweet",
        "sour",
        "brave",
        "scared",
    ]
    # Common nouns
    nouns = [
        "time",
        "person",
        "year",
        "way",
        "day",
        "thing",
        "man",
        "world",
        "life",
        "hand",
        "part",
        "child",
        "eye",
        "woman",
        "place",
        "work",
        "week",
        "case",
        "point",
        "group",
        "number",
        "room",
        "fact",
        "idea",
        "water",
        "money",
        "month",
        "book",
        "line",
        "city",
        "business",
        "night",
        "question",
        "story",
        "job",
        "word",
        "house",
        "power",
        "game",
        "country",
        "plant",
        "animal",
        "tree",
        "stone",
        "river",
        "fire",
        "problem",
        "theory",
        "street",
        "family",
        "history",
        "mind",
        "car",
        "music",
        "art",
        "nation",
        "science",
        "nature",
        "truth",
        "peace",
        "voice",
        "class",
        "paper",
        "space",
        "ground",
        "market",
        "court",
        "force",
        "price",
        "action",
        "reason",
        "love",
        "law",
        "bird",
        "literature",
        "knowledge",
        "society",
        "valley",
        "ocean",
        "machine",
        "energy",
        "metal",
        "mountain",
    ]
    # Common verbs (present tense)
    verbs = [
        "run",
        "walk",
        "jump",
        "sing",
        "dance",
        "write",
        "read",
        "speak",
        "listen",
        "watch",
        "think",
        "grow",
        "live",
        "play",
        "work",
        "move",
        "stop",
        "start",
        "create",
        "destroy",
        "build",
        "break",
        "push",
        "pull",
        "open",
        "close",
        "rise",
        "fall",
        "increase",
        "decrease",
        "begin",
        "end",
        "love",
        "hate",
        "help",
        "hurt",
        "make",
        "take",
        "give",
        "receive",
        "buy",
        "sell",
        "eat",
        "drink",
        "sleep",
        "wake",
        "laugh",
        "cry",
        "learn",
        "teach",
        "change",
        "stay",
        "come",
        "go",
        "arrive",
        "leave",
        "enter",
        "exit",
        "succeed",
        "fail",
        "win",
        "lose",
        "fight",
        "defend",
        "attack",
        "protect",
        "save",
        "waste",
        "gather",
        "scatter",
        "collect",
        "distribute",
        "join",
        "separate",
        "unite",
        "divide",
        "share",
    ]
    return adjectives, nouns, verbs

================
File: src/inspect_ai/_util/thread.py
================
import threading
def is_main_thread() -> bool:
    return threading.current_thread() is threading.main_thread()

================
File: src/inspect_ai/_util/throttle.py
================
import time
from functools import wraps
from typing import Any, Callable
def throttle(seconds: float) -> Callable[..., Any]:
    """Throttle a function to ensure it is called no more than every n seconds.
    Args:
       seconds (float): Throttle time.
    Returns:
       Callable: Throttled function.
    """
    def decorator(func: Callable[..., Any]) -> Callable[..., Any]:
        last_called: float = 0
        last_result: Any = None
        @wraps(func)
        def wrapped(*args: Any, **kwargs: Any) -> Any:
            nonlocal last_called
            nonlocal last_result
            current_time = time.time()
            if current_time - last_called >= seconds:
                last_result = func(*args, **kwargs)
                last_called = current_time
            return last_result
        return wrapped
    return decorator

================
File: src/inspect_ai/_util/trace.py
================
import datetime
import gzip
import json
import logging
import os
import shutil
import time
import traceback
from contextlib import contextmanager
from dataclasses import dataclass
from logging import FileHandler, Logger
from pathlib import Path
from typing import Any, Callable, Generator, Literal, TextIO
import anyio
import jsonlines
from pydantic import BaseModel, Field, JsonValue
from shortuuid import uuid
from .appdirs import inspect_data_dir
from .constants import TRACE
def inspect_trace_dir() -> Path:
    return inspect_data_dir("traces")
def inspect_trace_file() -> Path:
    return inspect_trace_dir() / f"trace-{os.getpid()}.log"
@contextmanager
def trace_action(
    logger: Logger, action: str, message: str, *args: Any, **kwargs: Any
) -> Generator[None, None, None]:
    """Trace a long running or poentially unreliable action.
    Trace actions for which you want to collect data on the resolution
    (e.g. succeeded, cancelled, failed, timed out, etc.) and duration of.
    Traces are written to the `TRACE` log level (which is just below
    `HTTP` and `INFO`). List and read trace logs with `inspect trace list`
    and related commands (see `inspect trace --help` for details).
    Args:
       logger (Logger): Logger to use for tracing (e.g. from `getLogger(__name__)`)
       action (str): Name of action to trace (e.g. 'Model', 'Subprocess', etc.)
       message (str): Message describing action (can be a format string w/ args or kwargs)
       *args (Any): Positional arguments for `message` format string.
       **kwargs (Any): Named args for `message` format string.
    """
    trace_id = uuid()
    start_monotonic = time.monotonic()
    start_wall = time.time()
    detail = message % args if args else message % kwargs if kwargs else message
    def trace_message(event: str) -> str:
        return f"{action}: {detail} ({event})"
    logger.log(
        TRACE,
        trace_message("enter"),
        extra={
            "action": action,
            "detail": detail,
            "event": "enter",
            "trace_id": str(trace_id),
            "start_time": start_wall,
        },
    )
    try:
        yield
        duration = time.monotonic() - start_monotonic
        logger.log(
            TRACE,
            trace_message("exit"),
            extra={
                "action": action,
                "detail": detail,
                "event": "exit",
                "trace_id": str(trace_id),
                "duration": duration,
            },
        )
    except (KeyboardInterrupt, anyio.get_cancelled_exc_class()):
        duration = time.monotonic() - start_monotonic
        logger.log(
            TRACE,
            trace_message("cancel"),
            extra={
                "action": action,
                "detail": detail,
                "event": "cancel",
                "trace_id": str(trace_id),
                "duration": duration,
            },
        )
        raise
    except TimeoutError:
        duration = time.monotonic() - start_monotonic
        logger.log(
            TRACE,
            trace_message("timeout"),
            extra={
                "action": action,
                "detail": detail,
                "event": "timeout",
                "trace_id": str(trace_id),
                "duration": duration,
            },
        )
        raise
    except Exception as ex:
        duration = time.monotonic() - start_monotonic
        logger.log(
            TRACE,
            trace_message("error"),
            extra={
                "action": action,
                "detail": detail,
                "event": "error",
                "trace_id": str(trace_id),
                "duration": duration,
                "error": getattr(ex, "message", str(ex)) or repr(ex),
                "error_type": type(ex).__name__,
                "stacktrace": traceback.format_exc(),
            },
        )
        raise
def trace_message(
    logger: Logger, category: str, message: str, *args: Any, **kwargs: Any
) -> None:
    """Log a message using the TRACE log level.
    The `TRACE` log level is just below `HTTP` and `INFO`). List and
    read trace logs with `inspect trace list` and related commands
    (see `inspect trace --help` for details).
    Args:
       logger (Logger): Logger to use for tracing (e.g. from `getLogger(__name__)`)
       category (str): Category of trace message.
       message (str): Trace message (can be a format string w/ args or kwargs)
       *args (Any): Positional arguments for `message` format string.
       **kwargs (Any): Named args for `message` format string.
    """
    logger.log(TRACE, f"[{category}] {message}", *args, **kwargs)
class TraceFormatter(logging.Formatter):
    def format(self, record: logging.LogRecord) -> str:
        # Base log entry with standard fields
        output: dict[str, JsonValue] = {
            "timestamp": self.formatTime(record),
            "level": record.levelname,
            "message": record.getMessage(),  # This handles the % formatting of the message
        }
        # Add basic context its not a TRACE message
        if record.levelname != "TRACE":
            if hasattr(record, "module"):
                output["module"] = record.module
            if hasattr(record, "funcName"):
                output["function"] = record.funcName
            if hasattr(record, "lineno"):
                output["line"] = record.lineno
        # Add any structured fields from extra
        elif hasattr(record, "action"):
            # This is a trace_action log
            for key in [
                "action",
                "detail",
                "event",
                "trace_id",
                "start_time",
                "duration",
                "error",
                "error_type",
                "stacktrace",
            ]:
                if hasattr(record, key):
                    output[key] = getattr(record, key)
        # Handle any unexpected extra attributes
        for key, value in record.__dict__.items():
            if key not in output and key not in (
                "args",
                "lineno",
                "funcName",
                "module",
                "asctime",
                "created",
                "exc_info",
                "exc_text",
                "filename",
                "levelno",
                "levelname",
                "msecs",
                "msg",
                "name",
                "pathname",
                "process",
                "processName",
                "relativeCreated",
                "stack_info",
                "thread",
                "threadName",
            ):
                output[key] = value
        return json.dumps(
            output, default=str
        )  # default=str handles non-serializable objects
    def formatTime(self, record: logging.LogRecord, datefmt: str | None = None) -> str:
        # ISO format with timezone
        dt = datetime.datetime.fromtimestamp(record.created)
        return dt.isoformat()
class TraceRecord(BaseModel):
    timestamp: str
    level: str
    message: str
class SimpleTraceRecord(TraceRecord):
    action: None = Field(default=None)
class ActionTraceRecord(TraceRecord):
    action: str
    event: Literal["enter", "cancel", "error", "timeout", "exit"]
    trace_id: str
    detail: str = Field(default="")
    start_time: float | None = Field(default=None)
    duration: float | None = Field(default=None)
    error: str | None = Field(default=None)
    error_type: str | None = Field(default=None)
    stacktrace: str | None = Field(default=None)
@dataclass
class TraceFile:
    file: Path
    mtime: float
def list_trace_files() -> list[TraceFile]:
    trace_files: list[TraceFile] = [
        TraceFile(file=f, mtime=f.lstat().st_mtime)
        for f in inspect_trace_dir().iterdir()
        if f.is_file()
    ]
    trace_files.sort(key=lambda f: f.mtime, reverse=True)
    return trace_files
def read_trace_file(file: Path) -> list[TraceRecord]:
    def read_file(f: TextIO) -> list[TraceRecord]:
        jsonlines_reader = jsonlines.Reader(f)
        trace_records: list[TraceRecord] = []
        for trace in jsonlines_reader.iter(type=dict):
            if "action" in trace:
                trace_records.append(ActionTraceRecord(**trace))
            else:
                trace_records.append(SimpleTraceRecord(**trace))
        return trace_records
    if file.name.endswith(".gz"):
        with gzip.open(file, "rt") as f:
            return read_file(f)
    else:
        with open(file, "r") as f:
            return read_file(f)
def rotate_trace_files() -> None:
    # if multiple inspect processes start up at once they
    # will all be attempting to rotate at the same time,
    # which can lead to FileNotFoundError -- ignore these
    # errors if they occur
    try:
        rotate_files = list_trace_files()[10:]
        for file in rotate_files:
            file.file.unlink(missing_ok=True)
    except FileNotFoundError:
        pass
def compress_trace_log(log_handler: FileHandler) -> Callable[[], None]:
    def compress() -> None:
        # ensure log is closed
        log_handler.close()
        # compress
        trace_file = Path(log_handler.baseFilename)
        if trace_file.exists():
            with open(trace_file, "rb") as f_in:
                with gzip.open(trace_file.with_suffix(".log.gz"), "wb") as f_out:
                    shutil.copyfileobj(f_in, f_out)
            trace_file.unlink()
    return compress

================
File: src/inspect_ai/_util/transcript.py
================
import html
import re
from typing import Any
from rich.align import AlignMethod
from rich.box import ROUNDED, Box
from rich.console import Group, RenderableType
from rich.markdown import Markdown
from rich.panel import Panel
from rich.rule import Rule
from rich.text import Text
from inspect_ai._util.content import ContentReasoning
from .format import format_function_call
def transcript_code_theme() -> str:
    return "github-dark"
def transcript_markdown(content: str, *, escape: bool = False) -> Markdown:
    code_theme = transcript_code_theme()
    return Markdown(
        html_escape_markdown(content) if escape else content,
        code_theme=code_theme,
        inline_code_lexer="python",
        inline_code_theme=code_theme,
    )
def html_escape_markdown(content: str) -> str:
    """Escape markdown lines that aren't in a code block."""
    codeblock_pattern = re.compile("`{3,}")
    current_codeblock = ""
    escaped: list[str] = []
    lines = content.splitlines()
    for line in lines:
        # look for matching end of codeblock
        if current_codeblock:
            if current_codeblock in line:
                current_codeblock = ""
                escaped.append(line)
                continue
        # look for beginning of codeblock
        match = codeblock_pattern.search(line)
        if match:
            current_codeblock = match[0]
            escaped.append(line)
            continue
        # escape if we are not in a codeblock
        if current_codeblock:
            escaped.append(line)
        else:
            escaped.append(html.escape(line, quote=False))
    return "\n".join(escaped)
def set_transcript_markdown_options(markdown: Markdown) -> None:
    code_theme = transcript_code_theme()
    markdown.code_theme = code_theme
    markdown.inline_code_lexer = "python"
    markdown.inline_code_theme = code_theme
def transcript_panel(
    title: str,
    subtitle: str | None = None,
    content: RenderableType | list[RenderableType] = [],
    level: int = 1,
) -> Panel:
    # resolve content to list
    content = content if isinstance(content, list) else [content]
    # no padding if there is no content
    padding = (0, 1) if content else (0, 0)
    # handle title/level
    if level == 1:
        title = f"[bold][blue]{title}[/blue][/bold]"
        title_align: AlignMethod = "left"
        # box if content, else line
        box = ROUNDED if content else LINE
    else:
        title = f"[bold]{title}[/bold]"
        title_align = "center"
        if level == 2:
            box = LINE
        else:
            box = DOTTED
    # inject subtitle
    if subtitle:
        content.insert(0, Text())
        content.insert(0, Text.from_markup(f"[bold]{subtitle}[/bold]"))
    # use xcode theme for markdown code
    for c in content:
        if isinstance(c, Markdown):
            set_transcript_markdown_options(c)
    return Panel(
        Group(*content),
        title=title,
        title_align=title_align,
        box=box,
        padding=padding,
        highlight=True,
        expand=True,
    )
def transcript_reasoning(reasoning: ContentReasoning) -> list[RenderableType]:
    content: list[RenderableType] = []
    text = (
        reasoning.reasoning
        if not reasoning.redacted
        else "Reasoning encrypted by model provider."
    )
    content.append(
        transcript_markdown(f"**<think>**  \n{text}  \n**</think>**\n\n", escape=True)
    )
    content.append(Text())
    return content
def transcript_separator(
    title: str, color: str, characters: str = ""
) -> RenderableType:
    return Rule(
        title=title,
        characters=characters,
        style=f"{color} bold",
        align="center",
        end="\n\n",
    )
def transcript_function(function: str, arguments: dict[str, Any]) -> RenderableType:
    call = format_function_call(function, arguments)
    return transcript_markdown("```python\n" + call + "\n```\n")
DOUBLE_LINE = Box("  \n    \n    \n    \n    \n    \n    \n    \n")
LINE = Box("  \n    \n    \n    \n    \n    \n    \n    \n")
DOTTED = Box("  \n    \n    \n    \n    \n    \n    \n    \n")
NOBORDER = Box("    \n    \n    \n    \n    \n    \n    \n    \n")

================
File: src/inspect_ai/_util/url.py
================
import re
def is_http_url(url: str) -> bool:
    return url.startswith("http://") or url.startswith("https://")
def is_data_uri(url: str) -> bool:
    pattern = r"^data:([^;]+);base64,.*"
    return re.match(pattern, url) is not None
def data_uri_mime_type(data_url: str) -> str | None:
    pattern = r"^data:([^;]+);.*"
    match = re.match(pattern, data_url)
    if match:
        mime_type = match.group(1)
        return mime_type
    else:
        return None
def data_uri_to_base64(data_uri: str) -> str:
    pattern = r"^data:[^,]+,"
    stripped_uri = re.sub(pattern, "", data_uri)
    return stripped_uri

================
File: src/inspect_ai/_util/version.py
================
from importlib.metadata import version
import semver
from .error import module_max_version_error, module_version_error
def verify_required_version(feature: str, package: str, version: str) -> None:
    if not has_required_version(package, version):
        raise module_version_error(feature, package, version)
def verify_max_version(feature: str, package: str, max_version: str) -> None:
    if semver.Version.parse(version(package)).compare(max_version) > 0:
        raise module_max_version_error(feature, package, max_version)
def has_required_version(package: str, required_version: str) -> bool:
    if semver.Version.parse(version(package)).compare(required_version) >= 0:
        return True
    else:
        return False

================
File: src/inspect_ai/_util/vscode.py
================
import os
from pathlib import Path
from typing import Any
from pydantic import BaseModel, Field
from pydantic_core import to_json
from shortuuid import uuid
from .appdirs import inspect_data_dir
class VSCodeCommand(BaseModel):
    command: str
    args: list[Any] = Field(default_factory=list)
def execute_vscode_commands(commands: VSCodeCommand | list[VSCodeCommand]) -> None:
    # resolve to list
    commands = commands if isinstance(commands, list) else [commands]
    # ensure there is someone listening
    command_dir = vs_code_commands_dir()
    if command_dir is None:
        raise NotImplementedError(
            "Not running in VS Code session or have older version of Inspect AI extension"
        )
    command_file = command_dir / uuid()
    with open(command_file, "w") as f:
        f.write(to_json(commands).decode())
def can_execute_vscode_commands() -> bool:
    return vs_code_commands_dir() is not None
def vs_code_commands_dir() -> Path | None:
    workspace_id = vscode_workspace_id()
    if workspace_id:
        workspace_dir = inspect_data_dir(os.path.join("vscode", workspace_id))
        if workspace_dir.exists():
            commands_dir = workspace_dir / "commands"
            return commands_dir if commands_dir.exists() else None
        else:
            return None
    else:
        return None
def vscode_workspace_id() -> str | None:
    return os.environ.get("INSPECT_WORKSPACE_ID", None)

================
File: src/inspect_ai/_util/working.py
================
import time
from contextvars import ContextVar
def init_sample_working_limit(start_time: float, working_limit: float | None) -> None:
    _sample_working_limit.set(working_limit)
    _sample_start_time.set(start_time)
    _sample_waiting_time.set(0)
def sample_waiting_time() -> float:
    return _sample_waiting_time.get()
def sample_working_time() -> float:
    return time.monotonic() - _sample_start_time.get() - sample_waiting_time()
def report_sample_waiting_time(waiting_time: float) -> None:
    _sample_waiting_time.set(_sample_waiting_time.get() + waiting_time)
    check_sample_working_limit()
def check_sample_working_limit() -> None:
    # no check if we don't have a limit
    working_limit = _sample_working_limit.get()
    if working_limit is None:
        return
    # are we over the limit?
    running_time = time.monotonic() - _sample_start_time.get()
    working_time = running_time - sample_waiting_time()
    if working_time > working_limit:
        from inspect_ai.solver._limit import SampleLimitExceededError
        raise SampleLimitExceededError(
            type="working",
            value=int(working_time),
            limit=int(working_limit),
            message=f"Exceeded working time limit ({working_limit:,} seconds)",
        )
_sample_working_limit: ContextVar[float | None] = ContextVar(
    "sample_working_limit", default=None
)
_sample_start_time: ContextVar[float] = ContextVar("sample_start_time", default=0)
_sample_waiting_time: ContextVar[float] = ContextVar("sample_waiting_time", default=0)

================
File: src/inspect_ai/_view/notify.py
================
import json
import os
from pathlib import Path
from urllib.parse import urlparse
from inspect_ai._util.appdirs import inspect_data_dir
from inspect_ai._util.vscode import vscode_workspace_id
# lightweight tracking of when the last eval task completed
# this enables the view client to poll for changes frequently
# (e.g. every 1 second) with very minimal overhead.
def view_notify_eval(location: str) -> None:
    # do not do this when running under pytest
    if os.environ.get("PYTEST_VERSION", None) is not None:
        return
    file = view_last_eval_file()
    with open(file, "w", encoding="utf-8") as f:
        if not urlparse(location).scheme:
            location = Path(location).absolute().as_posix()
        # Construct a payload with context for the last eval
        payload = {
            "location": location,
        }
        workspace_id = vscode_workspace_id()
        if workspace_id:
            payload["workspace_id"] = workspace_id
        # Serialize the payload and write it to the signal file
        payload_json = json.dumps(payload, indent=2)
        f.write(payload_json)
def view_last_eval_time() -> int:
    file = view_last_eval_file()
    if file.exists():
        return int(file.stat().st_mtime * 1000)
    else:
        return 0
def view_data_dir() -> Path:
    return inspect_data_dir("view")
def view_last_eval_file() -> Path:
    return view_data_dir() / "last-eval-result"

================
File: src/inspect_ai/_view/schema.py
================
import json
import os
import shutil
import subprocess
from pathlib import Path
from typing import Any
from inspect_ai.log import EvalLog
WWW_DIR = os.path.abspath((Path(__file__).parent / "www").as_posix())
def sync_view_schema() -> None:
    """Generate a JSON schema and Typescript types for EvalLog.
    This is useful for keeping log file viewer JS development
    in sync w/ Python development
    """
    # export schema file
    schema_path = Path(WWW_DIR, "log-schema.json")
    types_path = Path(WWW_DIR, "src", "types", "log.d.ts")
    vs_code_types_path = Path(
        WWW_DIR, "..", "..", "..", "..", "tools", "vscode", "src", "@types", "log.d.ts"
    )
    with open(schema_path, "w", encoding="utf-8") as f:
        # make everything required
        schema = EvalLog.model_json_schema()
        defs: dict[str, Any] = schema["$defs"]
        for key in defs.keys():
            defs[key] = schema_to_strict(defs[key])
        f.write(json.dumps(schema, indent=2))
        # generate types w/ json-schema-to-typescript
        subprocess.run(
            [
                "yarn",
                "json2ts",
                "--input",
                schema_path,
                "--output",
                types_path,
                "--additionalProperties",
                "false",
            ],
            cwd=WWW_DIR,
        )
        subprocess.run(["yarn", "prettier:write"], cwd=types_path.parent)
        shutil.copyfile(types_path, vs_code_types_path)
def schema_to_strict(schema: dict[str, Any]) -> dict[str, Any]:
    properties = schema.get("properties", None)
    if properties:
        schema["required"] = list(properties.keys())
        schema["additionalProperties"] = False
    return schema
if __name__ == "__main__":
    sync_view_schema()

================
File: src/inspect_ai/_view/server.py
================
import asyncio
import contextlib
import logging
import os
import urllib.parse
from logging import LogRecord, getLogger
from pathlib import Path
from typing import Any, AsyncIterator, Awaitable, Callable, Literal, cast
import fsspec  # type: ignore
from aiohttp import web
from fsspec.asyn import AsyncFileSystem  # type: ignore
from fsspec.core import split_protocol  # type: ignore
from pydantic_core import to_jsonable_python
from s3fs import S3FileSystem  # type: ignore
from inspect_ai._display import display
from inspect_ai._util.constants import DEFAULT_SERVER_HOST, DEFAULT_VIEW_PORT
from inspect_ai._util.file import default_fs_options, filesystem, size_in_mb
from inspect_ai.log._file import (
    EvalLogInfo,
    eval_log_json,
    list_eval_logs,
    log_files_from_ls,
    read_eval_log_async,
    read_eval_log_headers_async,
)
from .notify import view_last_eval_time
logger = getLogger(__name__)
def view_server(
    log_dir: str,
    recursive: bool = True,
    host: str = DEFAULT_SERVER_HOST,
    port: int = DEFAULT_VIEW_PORT,
    authorization: str | None = None,
    fs_options: dict[str, Any] = {},
) -> None:
    # route table
    routes = web.RouteTableDef()
    # get filesystem and resolve log_dir to full path
    fs = filesystem(log_dir)
    if not fs.exists(log_dir):
        fs.mkdir(log_dir, True)
    log_dir = fs.info(log_dir).name
    # validate log file requests (must be in the log_dir
    # unless authorization has been provided)
    def validate_log_file_request(log_file: str) -> None:
        if not authorization and (not log_file.startswith(log_dir) or ".." in log_file):
            raise web.HTTPUnauthorized()
    @routes.get("/api/logs/{log}")
    async def api_log(request: web.Request) -> web.Response:
        # log file requested
        file = request.match_info["log"]
        file = urllib.parse.unquote(file)
        validate_log_file_request(file)
        # header_only is based on a size threshold
        header_only = request.query.get("header-only", None)
        return await log_file_response(file, header_only)
    @routes.get("/api/log-size/{log}")
    async def api_log_size(request: web.Request) -> web.Response:
        # log file requested
        file = request.match_info["log"]
        file = urllib.parse.unquote(file)
        validate_log_file_request(file)
        return await log_size_response(file)
    @routes.get("/api/log-delete/{log}")
    async def api_log_delete(request: web.Request) -> web.Response:
        # log file requested
        file = request.match_info["log"]
        file = urllib.parse.unquote(file)
        validate_log_file_request(file)
        return await log_delete_response(file)
    @routes.get("/api/log-bytes/{log}")
    async def api_log_bytes(request: web.Request) -> web.Response:
        # log file requested
        file = request.match_info["log"]
        file = urllib.parse.unquote(file)
        validate_log_file_request(file)
        # header_only is based on a size threshold
        start = request.query.get("start", None)
        if start is None:
            return web.HTTPBadRequest(reason="No 'start' query param.")
        end = request.query.get("end", None)
        if end is None:
            return web.HTTPBadRequest(reason="No 'end' query param")
        return await log_bytes_response(file, int(start), int(end))
    @routes.get("/api/logs")
    async def api_logs(request: web.Request) -> web.Response:
        # log dir can optionally be overridden by the request
        if authorization:
            request_log_dir = request.query.getone("log_dir", None)
            if request_log_dir:
                request_log_dir = urllib.parse.unquote(request_log_dir)
            else:
                request_log_dir = log_dir
        else:
            request_log_dir = log_dir
        # list logs
        logs = await list_eval_logs_async(
            log_dir=request_log_dir, recursive=recursive, fs_options=fs_options
        )
        return log_listing_response(logs, request_log_dir)
    @routes.get("/api/log-headers")
    async def api_log_headers(request: web.Request) -> web.Response:
        files = request.query.getall("file", [])
        files = [urllib.parse.unquote(file) for file in files]
        map(validate_log_file_request, files)
        return await log_headers_response(files)
    @routes.get("/api/events")
    async def api_events(request: web.Request) -> web.Response:
        last_eval_time = request.query.get("last_eval_time", None)
        actions = (
            ["refresh-evals"]
            if last_eval_time and view_last_eval_time() > int(last_eval_time)
            else []
        )
        return web.json_response(actions)
    # optional auth middleware
    @web.middleware
    async def authorize(
        request: web.Request,
        handler: Callable[[web.Request], Awaitable[web.StreamResponse]],
    ) -> web.StreamResponse:
        if request.headers.get("Authorization", None) != authorization:
            return web.HTTPUnauthorized()
        else:
            return await handler(request)
    # setup server
    app = web.Application(middlewares=[authorize] if authorization else [])
    app.router.add_routes(routes)
    app.router.register_resource(WWWResource())
    # filter request log (remove /api/events)
    filter_aiohttp_log()
    # run app
    display().print(f"Inspect View: {log_dir}")
    web.run_app(
        app=app,
        host=host,
        port=port,
        print=display().print,
        access_log_format='%a %t "%r" %s %b (%Tf)',
        shutdown_timeout=1,
    )
def log_listing_response(logs: list[EvalLogInfo], log_dir: str) -> web.Response:
    response = dict(
        log_dir=aliased_path(log_dir),
        files=[
            dict(
                name=log.name,
                size=log.size,
                mtime=log.mtime,
                task=log.task,
                task_id=log.task_id,
            )
            for log in logs
        ],
    )
    return web.json_response(response)
async def log_file_response(file: str, header_only_param: str | None) -> web.Response:
    # resolve header_only
    header_only_mb = int(header_only_param) if header_only_param is not None else None
    header_only = resolve_header_only(file, header_only_mb)
    try:
        contents: bytes | None = None
        if header_only:
            try:
                log = await read_eval_log_async(file, header_only=True)
                contents = eval_log_json(log)
            except ValueError as ex:
                logger.info(
                    f"Unable to read headers from log file {file}: {ex}. "
                    + "The file may include a NaN or Inf value. Falling back to reading entire file."
                )
        if contents is None:  # normal read
            log = await read_eval_log_async(file, header_only=False)
            contents = eval_log_json(log)
        return web.Response(body=contents, content_type="application/json")
    except Exception as error:
        logger.exception(error)
        return web.Response(status=500, reason="File not found")
async def log_size_response(log_file: str) -> web.Response:
    fs = filesystem(log_file)
    if fs.is_async():
        info = fs._file_info(await async_connection(log_file)._info(log_file))
    else:
        info = fs.info(log_file)
    return web.json_response(info.size)
async def log_delete_response(log_file: str) -> web.Response:
    fs = filesystem(log_file)
    fs.rm(log_file)
    return web.json_response(True)
async def log_bytes_response(log_file: str, start: int, end: int) -> web.Response:
    # build headers
    content_length = end - start + 1
    headers = {
        "Content-Type": "application/octet-stream",
        "Content-Length": str(content_length),
    }
    # fetch bytes
    fs = filesystem(log_file)
    if fs.is_async():
        bytes = await async_connection(log_file)._cat_file(
            log_file, start=start, end=end + 1
        )
    else:
        bytes = fs.read_bytes(log_file, start, end + 1)
    # return response
    return web.Response(status=200, body=bytes, headers=headers)
async def log_headers_response(files: list[str]) -> web.Response:
    headers = await read_eval_log_headers_async(files)
    return web.json_response(to_jsonable_python(headers, exclude_none=True))
class WWWResource(web.StaticResource):
    def __init__(self) -> None:
        super().__init__(
            "", os.path.abspath((Path(__file__).parent / "www" / "dist").as_posix())
        )
    async def _handle(self, request: web.Request) -> web.StreamResponse:
        # serve /index.html for /
        filename = request.match_info["filename"]
        if not filename:
            request.match_info["filename"] = "index.html"
        # call super
        response = await super()._handle(request)
        # disable caching as this is only ever served locally
        # and w/ caching sometimes we get stale assets
        response.headers.update(
            {
                "Expires": "Fri, 01 Jan 1990 00:00:00 GMT",
                "Pragma": "no-cache",
                "Cache-Control": "no-cache, no-store, max-age=0, must-revalidate",
            }
        )
        # return response
        return response
def aliased_path(path: str) -> str:
    home_dir = os.path.expanduser("~")
    if path.startswith(home_dir):
        return path.replace(home_dir, "~", 1)
    else:
        return path
def resolve_header_only(path: str, header_only: int | None) -> bool:
    # if there is a max_size passed, respect that and switch to
    # header_only mode if the file is too large
    if header_only == 0:
        return True
    if header_only is not None and size_in_mb(path) > int(header_only):
        return True
    else:
        return False
async def list_eval_logs_async(
    log_dir: str = os.environ.get("INSPECT_LOG_DIR", "./logs"),
    formats: list[Literal["eval", "json"]] | None = None,
    recursive: bool = True,
    descending: bool = True,
    fs_options: dict[str, Any] = {},
) -> list[EvalLogInfo]:
    """List all eval logs in a directory.
    Will be async for filesystem providers that support async (e.g. s3, gcs, etc.)
    otherwise will fallback to sync implementation.
    Args:
      log_dir (str): Log directory (defaults to INSPECT_LOG_DIR)
      formats (Literal["eval", "json"]): Formats to list (default
        to listing all formats)
      recursive (bool): List log files recursively (defaults to True).
      descending (bool): List in descending order.
      fs_options (dict[str, Any]): Optional. Additional arguments to pass through
          to the filesystem provider (e.g. `S3FileSystem`).
    Returns:
       List of EvalLog Info.
    """
    # async filesystem if we can
    fs = filesystem(log_dir, fs_options)
    if fs.is_async():
        async with async_fileystem(log_dir, fs_options=fs_options) as async_fs:
            if await async_fs._exists(log_dir):
                # prevent caching of listings
                async_fs.invalidate_cache(log_dir)
                # list logs
                if recursive:
                    files: list[dict[str, Any]] = []
                    async for _, _, filenames in async_fs._walk(log_dir, detail=True):
                        files.extend(filenames.values())
                else:
                    files = cast(
                        list[dict[str, Any]],
                        await async_fs._ls(log_dir, detail=True),
                    )
                logs = [fs._file_info(file) for file in files]
                # resolve to eval logs
                return log_files_from_ls(logs, formats, descending)
            else:
                return []
    else:
        return list_eval_logs(
            log_dir=log_dir,
            formats=formats,
            recursive=recursive,
            descending=descending,
            fs_options=fs_options,
        )
def filter_aiohttp_log() -> None:
    #  filter overly chatty /api/events messages
    class RequestFilter(logging.Filter):
        def filter(self, record: LogRecord) -> bool:
            return "/api/events" not in record.getMessage()
    # don't add if we already have
    access_logger = getLogger("aiohttp.access")
    for existing_filter in access_logger.filters:
        if isinstance(existing_filter, RequestFilter):
            return
    # add the filter
    access_logger.addFilter(RequestFilter())
_async_connections: dict[str, AsyncFileSystem] = {}
def async_connection(log_file: str) -> AsyncFileSystem:
    # determine protocol
    protocol, _ = split_protocol(log_file)
    protocol = protocol or "file"
    # create connection if required
    if protocol not in _async_connections.keys():
        _async_connections[protocol] = fsspec.filesystem(
            protocol, asynchronous=True, loop=asyncio.get_event_loop()
        )
    # return async file-system
    return _async_connections.get(protocol)
@contextlib.asynccontextmanager
async def async_fileystem(
    location: str, fs_options: dict[str, Any] = {}
) -> AsyncIterator[AsyncFileSystem]:
    # determine protocol
    protocol, _ = split_protocol(location)
    protocol = protocol or "file"
    # build options
    options = default_fs_options(location)
    options.update(fs_options)
    if protocol == "s3":
        s3 = S3FileSystem(asynchronous=True, **options)
        session = await s3.set_session()
        try:
            yield s3
        finally:
            await session.close()
    else:
        options.update({"asynchronous": True, "loop": asyncio.get_event_loop()})
        yield fsspec.filesystem(protocol, **options)

================
File: src/inspect_ai/_view/view.py
================
import atexit
import logging
import os
from pathlib import Path
from typing import Any
import psutil
from inspect_ai._display import display
from inspect_ai._util.constants import (
    DEFAULT_SERVER_HOST,
    DEFAULT_VIEW_PORT,
)
from inspect_ai._util.dotenv import init_dotenv
from inspect_ai._util.error import exception_message
from inspect_ai._util.logger import init_logger
from inspect_ai._view.server import view_server
from .notify import view_data_dir
logger = logging.getLogger(__name__)
def view(
    log_dir: str | None = None,
    recursive: bool = True,
    host: str = DEFAULT_SERVER_HOST,
    port: int = DEFAULT_VIEW_PORT,
    authorization: str | None = None,
    log_level: str | None = None,
    fs_options: dict[str, Any] = {},
) -> None:
    init_dotenv()
    init_logger(log_level)
    # initialize the log_dir
    log_dir = log_dir if log_dir else os.getenv("INSPECT_LOG_DIR", "./logs")
    # acquire the requested port
    view_acquire_port(port)
    # run server
    view_server(
        log_dir=log_dir,
        recursive=recursive,
        host=host,
        port=port,
        authorization=authorization,
        fs_options=fs_options,
    )
def view_port_pid_file(port: int) -> Path:
    ports_dir = view_data_dir() / "ports"
    ports_dir.mkdir(parents=True, exist_ok=True)
    return ports_dir / str(port)
def view_acquire_port(port: int) -> None:
    # pid file name
    pid_file = view_port_pid_file(port)
    # does it already exist? if so terminate that process
    if pid_file.exists():
        WAIT_SECONDS = 5
        with open(pid_file, "r", encoding="utf-8") as f:
            pid = int(f.read().strip())
        try:
            p = psutil.Process(pid)
            p.terminate()
            display().print(
                f"Terminating existing inspect view command using port {port}"
            )
            p.wait(WAIT_SECONDS)
        except psutil.NoSuchProcess:
            # expected error for crufty pid files
            pass
        except psutil.TimeoutExpired:
            logger.warning(
                f"Timed out waiting for process to exit for {WAIT_SECONDS} seconds."
            )
        except psutil.AccessDenied:
            logger.warning(
                "Attempted to kill existing view command on "
                + f"port {port} but access was denied."
            )
        except Exception as ex:
            logger.warning(
                "Attempted to kill existing view command on "
                + f"port {port} but error occurred: {exception_message(ex)}"
            )
    # write our pid to the file
    with open(pid_file, "w", encoding="utf-8") as f:
        f.write(str(os.getpid()))
    # arrange to release on exit
    def release_lock_file() -> None:
        try:
            pid_file.unlink(True)
        except Exception:
            pass
    atexit.register(release_lock_file)

================
File: src/inspect_ai/_view/www/README.md
================
The Inspect log viewer is now built into a bundled js file using `vite`, but it is important that users who clone or install from the repo directly are able to run the viewer without needing an explicit build step. To support this, we keep a bundled version of the viewer committed into the repo in the `dist` folder. **When you make changes, your commits/PRs must include updates to the bundled files in the `dist` folder as well as the source code changes.** 


## Before You Commit:

Use the following commands (run in the `src/inspect_ai/_view/www` dir) to ensure your commits are ready to go:

1. Run eslint to catch any linting errors:

   ```bash
   yarn eslint .
   ```

   Fix any errors reported.

2. Run prettier to ensure formatting:

   ```bash
   yarn prettier:write
   ```

3. Build the bundled output into the `dist` directory.

   ```bash
   yarn build
   ```
   
   Don't forget to stage newly updated changes in the `dist` folder.

================
File: src/inspect_ai/approval/__init__.py
================
from ._approval import Approval, ApprovalDecision
from ._approver import Approver
from ._auto import auto_approver
from ._human.approver import human_approver
from ._policy import ApprovalPolicy
from ._registry import approver
__all__ = [
    "Approver",
    "Approval",
    "ApprovalDecision",
    "ApprovalPolicy",
    "approver",
    "human_approver",
    "auto_approver",
]

================
File: src/inspect_ai/approval/_apply.py
================
from contextvars import ContextVar
from inspect_ai._util.format import format_function_call
from inspect_ai.approval._approval import Approval
from inspect_ai.tool._tool_call import (
    ToolCall,
    ToolCallContent,
    ToolCallView,
    ToolCallViewer,
)
from ._approver import Approver
from ._policy import ApprovalPolicy, policy_approver
async def apply_tool_approval(
    message: str, call: ToolCall, viewer: ToolCallViewer | None
) -> tuple[bool, Approval | None]:
    from inspect_ai.solver._task_state import sample_state
    approver = _tool_approver.get(None)
    if approver:
        # resolve view
        if viewer:
            view = viewer(call)
            if not view.call:
                view.call = default_tool_call_viewer(call).call
        else:
            view = default_tool_call_viewer(call)
        # current sample state
        state = sample_state()
        # call approver
        approval = await approver(
            message=message,
            call=call,
            view=view,
            state=state,
        )
        # process decision
        match approval.decision:
            case "approve" | "modify":
                return True, approval
            case "reject":
                return False, approval
            case "terminate":
                if state:
                    state.completed = True
                return False, approval
            case "escalate":
                raise RuntimeError("Unexpected 'escalate' from policy approver.")
    # no approval system registered
    else:
        return True, None
def default_tool_call_viewer(call: ToolCall) -> ToolCallView:
    return ToolCallView(
        call=ToolCallContent(
            format="markdown",
            content="```python\n"
            + format_function_call(call.function, call.arguments)
            + "\n```\n",
        )
    )
def init_tool_approval(approval: list[ApprovalPolicy] | None) -> None:
    if approval:
        _tool_approver.set(policy_approver(approval))
    else:
        _tool_approver.set(None)
def have_tool_approval() -> bool:
    return _tool_approver.get(None) is not None
_tool_approver: ContextVar[Approver | None] = ContextVar("tool_approver", default=None)

================
File: src/inspect_ai/approval/_approval.py
================
from typing import Literal
from pydantic import BaseModel, Field
from inspect_ai.tool._tool_call import ToolCall
ApprovalDecision = Literal["approve", "modify", "reject", "terminate", "escalate"]
"""Represents the possible decisions in an approval.
Possible values:
  "approve": The action is approved.
  "modify": The action is approved with modification.
  "reject": The action is rejected.
  "terminate": Evaluation of the sample should be terminated.
  "escalate": The decision is escalated to another approver.
"""
class Approval(BaseModel):
    """Approval details (decision, explanation, etc.)"""
    decision: ApprovalDecision
    """Approval decision."""
    modified: ToolCall | None = Field(default=None)
    """Modified tool call for decision 'modify'."""
    explanation: str | None = Field(default=None)
    """Explanation for decision."""

================
File: src/inspect_ai/approval/_approver.py
================
from typing import Protocol
from inspect_ai.solver._task_state import TaskState
from inspect_ai.tool._tool_call import ToolCall, ToolCallView
from ._approval import Approval
class Approver(Protocol):
    """Protocol for approvers."""
    async def __call__(
        self,
        message: str,
        call: ToolCall,
        view: ToolCallView,
        state: TaskState | None = None,
    ) -> Approval:
        """
        Approve or reject a tool call.
        Args:
            message: Message genreated by the model along with the tool call.
            call: The tool call to be approved.
            view: Custom rendering of tool context and call.
            state: The current task state, if available.
        Returns:
            Approval: An Approval object containing the decision and explanation.
        """
        ...

================
File: src/inspect_ai/approval/_auto.py
================
from inspect_ai.solver._task_state import TaskState
from inspect_ai.tool._tool_call import ToolCall, ToolCallView
from ._approval import Approval, ApprovalDecision
from ._approver import Approver
from ._registry import approver
@approver(name="auto")
def auto_approver(decision: ApprovalDecision = "approve") -> Approver:
    """Automatically apply a decision to tool calls.
    Args:
       decision: Decision to apply.
    Returns:
       Approver: Auto approver.
    """
    async def approve(
        message: str,
        call: ToolCall,
        view: ToolCallView,
        state: TaskState | None = None,
    ) -> Approval:
        return Approval(decision=decision, explanation="Automatic decision.")
    return approve

================
File: src/inspect_ai/approval/_call.py
================
from inspect_ai._util.registry import registry_log_name
from inspect_ai.solver._task_state import TaskState
from inspect_ai.tool._tool_call import ToolCall, ToolCallView
from ._approval import Approval
from ._approver import Approver
async def call_approver(
    approver: Approver,
    message: str,
    call: ToolCall,
    view: ToolCallView,
    state: TaskState | None = None,
) -> Approval:
    # run approver
    approval = await approver(message, call, view, state)
    # record
    record_approval(registry_log_name(approver), message, call, view, approval)
    # return approval
    return approval
def record_approval(
    approver_name: str,
    message: str,
    call: ToolCall,
    view: ToolCallView | None,
    approval: Approval,
) -> None:
    from inspect_ai.log._transcript import ApprovalEvent, transcript
    transcript()._event(
        ApprovalEvent(
            message=message,
            call=call,
            view=view,
            approver=approver_name,
            decision=approval.decision,
            modified=approval.modified,
            explanation=approval.explanation,
        )
    )

================
File: src/inspect_ai/approval/_human/approver.py
================
from inspect_ai.solver._task_state import TaskState
from inspect_ai.tool._tool_call import ToolCall, ToolCallView
from .._approval import Approval, ApprovalDecision
from .._approver import Approver
from .._registry import approver
from .console import console_approval
from .panel import panel_approval
@approver(name="human")
def human_approver(
    choices: list[ApprovalDecision] = ["approve", "reject", "terminate"],
) -> Approver:
    """Interactive human approver.
    Args:
       choices: Choices to present to human.
    Returns:
       Approver: Interactive human approver.
    """
    async def approve(
        message: str,
        call: ToolCall,
        view: ToolCallView,
        state: TaskState | None = None,
    ) -> Approval:
        # try to use the panel approval (available in fullscreen display)
        try:
            return await panel_approval(message, call, view, state, choices)
        # fallback to plain console approval (available in all displays)
        except NotImplementedError:
            return console_approval(message, view, choices)
    return approve

================
File: src/inspect_ai/approval/_human/console.py
================
from rich.prompt import Prompt
from inspect_ai._util.transcript import transcript_panel
from inspect_ai.tool._tool_call import ToolCallView
from inspect_ai.util._console import input_screen
from .._approval import Approval, ApprovalDecision
from .util import (
    HUMAN_APPROVED,
    HUMAN_ESCALATED,
    HUMAN_REJECTED,
    HUMAN_TERMINATED,
    render_tool_approval,
)
def console_approval(
    message: str, view: ToolCallView, choices: list[ApprovalDecision]
) -> Approval:
    with input_screen(width=None) as console:
        console.print(
            transcript_panel(
                title="Approve Tool", content=render_tool_approval(message, view)
            )
        )
        # provide choices
        prompts: dict[str, str] = {}
        for choice in choices:
            prompts[choice[0]] = f"{choice.capitalize()} ({choice[0]})"
        values = list(prompts.values())
        prompt = ", ".join(values[:-1])
        prompt = f"{prompt}, or {values[-1]}"
        def render_approval(approval: Approval) -> Approval:
            console.print(f"Decision: {approval.decision.capitalize()}")
            return approval
        while True:
            decision = Prompt.ask(
                prompt=prompt,
                console=console,
                choices=list(prompts.keys()),
                default="a",
            ).lower()
            if decision == "a":
                return render_approval(
                    Approval(decision="approve", explanation=HUMAN_APPROVED)
                )
            elif decision == "r":
                return render_approval(
                    Approval(decision="reject", explanation=HUMAN_REJECTED)
                )
            elif decision == "t":
                return render_approval(
                    Approval(decision="terminate", explanation=HUMAN_TERMINATED)
                )
            elif decision == "e":
                return render_approval(
                    Approval(decision="escalate", explanation=HUMAN_ESCALATED)
                )

================
File: src/inspect_ai/approval/_human/manager.py
================
import uuid
from contextvars import ContextVar
from typing import Callable, Literal, NamedTuple
from inspect_ai._util.future import Future
from inspect_ai.solver._task_state import TaskState
from inspect_ai.tool._tool_call import ToolCall, ToolCallView
from .._approval import Approval, ApprovalDecision
class ApprovalRequest(NamedTuple):
    message: str
    call: ToolCall
    view: ToolCallView
    state: TaskState | None
    choices: list[ApprovalDecision]
class PendingApprovalRequest(NamedTuple):
    request: ApprovalRequest
    task: str
    model: str
    id: int | str
    epoch: int
class HumanApprovalManager:
    def __init__(self) -> None:
        self._approval_requests: dict[
            str, tuple[PendingApprovalRequest, Future[Approval]]
        ] = {}
        self._change_callbacks: list[Callable[[Literal["add", "remove"]], None]] = []
    def request_approval(self, request: ApprovalRequest) -> str:
        from inspect_ai.log._samples import sample_active
        id = str(uuid.uuid4())
        sample = sample_active()
        assert sample
        assert sample.sample.id is not None
        pending = PendingApprovalRequest(
            request=request,
            task=sample.task,
            model=sample.model,
            id=sample.sample.id,
            epoch=sample.epoch,
        )
        self._approval_requests[id] = (pending, Future[Approval]())
        self._notify_change("add")
        return id
    def withdraw_request(self, id: str) -> None:
        del self._approval_requests[id]
        self._notify_change("remove")
    async def wait_for_approval(self, id: str) -> Approval:
        _, future = self._approval_requests[id]
        return await future.result()
    def on_change(
        self, callback: Callable[[Literal["add", "remove"]], None]
    ) -> Callable[[], None]:
        self._change_callbacks.append(callback)
        def unsubscribe() -> None:
            if callback in self._change_callbacks:
                self._change_callbacks.remove(callback)
        return unsubscribe
    def approval_requests(self) -> list[tuple[str, PendingApprovalRequest]]:
        return [(aid, data) for aid, (data, _) in self._approval_requests.items()]
    def complete_approval(self, id: str, result: Approval) -> None:
        if id in self._approval_requests:
            _, future = self._approval_requests[id]
            future.set_result(result)
            del self._approval_requests[id]
            self._notify_change("remove")
    def fail_approval(self, id: str, error: Exception) -> None:
        if id in self._approval_requests:
            _, future = self._approval_requests[id]
            future.set_exception(error)
            del self._approval_requests[id]
            self._notify_change("remove")
    def _notify_change(self, action: Literal["add", "remove"]) -> None:
        for callback in self._change_callbacks:
            callback(action)
def human_approval_manager() -> HumanApprovalManager:
    return _human_approval_manager.get()
def init_human_approval_manager() -> None:
    _human_approval_manager.set(HumanApprovalManager())
_human_approval_manager: ContextVar[HumanApprovalManager] = ContextVar(
    "_human_approval_manager"
)

================
File: src/inspect_ai/approval/_human/panel.py
================
from typing import Callable, Literal
import anyio
from rich.console import RenderableType
from rich.text import Text
from textual.app import ComposeResult
from textual.containers import Horizontal, ScrollableContainer
from textual.reactive import reactive
from textual.widgets import Button, Static
from typing_extensions import override
from inspect_ai._util.registry import registry_unqualified_name
from inspect_ai.solver._task_state import TaskState
from inspect_ai.tool._tool_call import ToolCall, ToolCallView
from inspect_ai.util._panel import InputPanel, input_panel
from .._approval import Approval, ApprovalDecision
from .manager import ApprovalRequest, PendingApprovalRequest, human_approval_manager
from .util import (
    HUMAN_APPROVED,
    HUMAN_ESCALATED,
    HUMAN_REJECTED,
    HUMAN_TERMINATED,
    render_tool_approval,
)
async def panel_approval(
    message: str,
    call: ToolCall,
    view: ToolCallView,
    state: TaskState | None,
    choices: list[ApprovalDecision],
) -> Approval:
    # ensure the approvals panel is shown
    await input_panel(ApprovalInputPanel)
    # submit to human approval manager (will be picked up by panel)
    approvals = human_approval_manager()
    id = approvals.request_approval(
        ApprovalRequest(
            message=message, call=call, view=view, state=state, choices=choices
        )
    )
    try:
        return await approvals.wait_for_approval(id)
    except anyio.get_cancelled_exc_class():
        approvals.withdraw_request(id)
        raise
class ApprovalInputPanel(InputPanel):
    DEFAULT_TITLE = "Approval"
    DEFAULT_CSS = """
    ApprovalInputPanel {
        layout: grid;
        grid-size: 1 3;
        grid-rows: auto 1fr auto;
    }
    """
    _approvals: list[tuple[str, PendingApprovalRequest]] = []
    _unsubscribe: Callable[[], None] | None = None
    @override
    def compose(self) -> ComposeResult:
        yield ApprovalRequestHeading()
        yield ApprovalRequestContent()
        yield ApprovalRequestActions()
    def on_mount(self) -> None:
        self._unsubscribe = human_approval_manager().on_change(
            self.on_approvals_changed
        )
    def on_unmount(self) -> None:
        if self._unsubscribe is not None:
            self._unsubscribe()
    def on_approvals_changed(self, action: Literal["add", "remove"]) -> None:
        heading = self.query_one(ApprovalRequestHeading)
        content = self.query_one(ApprovalRequestContent)
        actions = self.query_one(ApprovalRequestActions)
        self._approvals = human_approval_manager().approval_requests()
        if len(self._approvals) > 0:
            approval_id, approval_request = self._approvals[0]
            self.title = f"{self.DEFAULT_TITLE} ({len(self._approvals):,})"
            heading.request = approval_request
            content.approval = approval_request.request
            actions.approval_request = approval_id, approval_request
            if action == "add":
                self.activate()
                actions.activate()
            self.visible = True
        else:
            self.title = self.DEFAULT_TITLE
            heading.request = None
            content.approval = None
            actions.approval_request = None
            self.deactivate()
            self.visible = False
class ApprovalRequestHeading(Static):
    DEFAULT_CSS = """
    ApprovalRequestHeading {
        width: 1fr;
        background: $surface;
        color: $secondary;
        margin-left: 1;
    }
    """
    request: reactive[PendingApprovalRequest | None] = reactive(None)
    def render(self) -> RenderableType:
        if self.request is not None:
            return f"{registry_unqualified_name(self.request.task)} (id: {self.request.id}, epoch {self.request.epoch}): {self.request.model}"
        else:
            return ""
class ApprovalRequestContent(ScrollableContainer):
    DEFAULT_CSS = """
    ApprovalRequestContent {
        scrollbar-size-vertical: 1;
        scrollbar-gutter: stable;
        border: solid $foreground 20%;
        padding: 0 1 0 1;
    }
    """
    approval: reactive[ApprovalRequest | None] = reactive(None)
    async def watch_approval(self, approval: ApprovalRequest | None) -> None:
        await self.remove_children()
        if approval:
            self.mount_all(
                Static(r) for r in render_tool_approval(approval.message, approval.view)
            )
            self.scroll_end(animate=False)
class ApprovalRequestActions(Horizontal):
    APPROVE_TOOL_CALL = "approve-tool-call"
    REJECT_TOOL_CALL = "reject-tool-call"
    ESCALATE_TOOL_CALL = "escalate-tool-call"
    TERMINATE_TOOL_CALL_SAMPLE = "terminate-tool-call-sample"
    DEFAULT_CSS = f"""
    ApprovalRequestActions Button {{
        margin-right: 1;
        min-width: 20;
    }}
    ApprovalRequestActions #{APPROVE_TOOL_CALL} {{
        color: $success;
    }}
    ApprovalRequestActions #{REJECT_TOOL_CALL} {{
        color: $warning-darken-3;
    }}
    ApprovalRequestActions #{ESCALATE_TOOL_CALL} {{
        color: $primary-darken-3;
        margin-left: 3;
    }}
    ApprovalRequestActions #{TERMINATE_TOOL_CALL_SAMPLE} {{
        color: $error-darken-1;
        margin-left: 3;
    }}
    """
    approval_request: reactive[tuple[str, PendingApprovalRequest] | None] = reactive(
        None
    )
    def compose(self) -> ComposeResult:
        yield Button(
            Text("Approve"),
            id=self.APPROVE_TOOL_CALL,
            tooltip="Approve the tool call.",
        )
        yield Button(
            Text("Reject"),
            id=self.REJECT_TOOL_CALL,
            tooltip="Reject the tool call.",
        )
        yield Button(
            Text("Escalate"),
            id=self.ESCALATE_TOOL_CALL,
            tooltip="Escalate the tool call to another approver.",
        )
        yield Button(
            Text("Terminate"),
            id=self.TERMINATE_TOOL_CALL_SAMPLE,
            tooltip="Terminate the sample.",
        )
    def activate(self) -> None:
        approve = self.query_one(f"#{self.APPROVE_TOOL_CALL}")
        approve.focus()
    def on_button_pressed(self, event: Button.Pressed) -> None:
        if self.approval_request is not None:
            id, _ = self.approval_request
            if event.button.id == self.APPROVE_TOOL_CALL:
                approval = Approval(decision="approve", explanation=HUMAN_APPROVED)
            elif event.button.id == self.REJECT_TOOL_CALL:
                approval = Approval(decision="reject", explanation=HUMAN_REJECTED)
            elif event.button.id == self.ESCALATE_TOOL_CALL:
                approval = Approval(decision="escalate", explanation=HUMAN_ESCALATED)
            elif event.button.id == self.TERMINATE_TOOL_CALL_SAMPLE:
                approval = Approval(decision="terminate", explanation=HUMAN_TERMINATED)
            else:
                raise ValueError(f"Unexpected button id: {event.button.id}")
            human_approval_manager().complete_approval(id, approval)
    def watch_approval_request(
        self, approval_request: tuple[str, PendingApprovalRequest] | None
    ) -> None:
        choices = (
            approval_request[1].request.choices if approval_request is not None else []
        )
        def update_visible(id: str, choice: ApprovalDecision) -> None:
            self.query_one(f"#{id}").display = choice in choices
        update_visible(self.APPROVE_TOOL_CALL, "approve")
        update_visible(self.REJECT_TOOL_CALL, "reject")
        update_visible(self.ESCALATE_TOOL_CALL, "escalate")
        update_visible(self.TERMINATE_TOOL_CALL_SAMPLE, "terminate")

================
File: src/inspect_ai/approval/_human/util.py
================
from rich.console import RenderableType
from rich.highlighter import ReprHighlighter
from rich.rule import Rule
from rich.text import Text
from inspect_ai._util.transcript import transcript_markdown
from inspect_ai.tool._tool_call import ToolCallContent, ToolCallView
from inspect_ai.util._display import display_type
HUMAN_APPROVED = "Human operator approved tool call."
HUMAN_REJECTED = "Human operator rejected the tool call."
HUMAN_TERMINATED = "Human operator asked that the sample be terminated."
HUMAN_ESCALATED = "Human operator escalated the tool call approval."
def render_tool_approval(message: str, view: ToolCallView) -> list[RenderableType]:
    renderables: list[RenderableType] = []
    text_highlighter = ReprHighlighter()
    # ignore content if trace enabled
    message = message.strip() if display_type() != "conversation" else ""
    def add_view_content(view_content: ToolCallContent) -> None:
        if view_content.title:
            renderables.append(Text.from_markup(f"[bold]{view_content.title}[/bold]\n"))
        if view_content.format == "markdown":
            renderables.append(transcript_markdown(view_content.content))
        else:
            text_content = text_highlighter(Text(view_content.content))
            renderables.append(text_content)
    # assistant content (don't add if trace_enabled as we already have it in that case)
    if message:
        renderables.append(Text.from_markup("[bold]Assistant[/bold]\n"))
        renderables.append(Text(f"{message.strip()}"))
    # extra context provided by tool view
    if view.context:
        renderables.append(Text())
        add_view_content(view.context)
        renderables.append(Text())
    # tool call view
    if view.call:
        if message or view.context:
            renderables.append(Rule("", style="#282c34", align="left", characters=""))
        renderables.append(Text())
        add_view_content(view.call)
        renderables.append(Text())
    return renderables

================
File: src/inspect_ai/approval/_policy.py
================
import fnmatch
import sys
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Generator, cast
from pydantic import BaseModel, Field, model_validator
from inspect_ai._util.config import read_config_object
from inspect_ai._util.format import format_function_call
from inspect_ai._util.registry import registry_create, registry_lookup
from inspect_ai.solver._task_state import TaskState
from inspect_ai.tool._tool_call import ToolCall, ToolCallView
from inspect_ai.util._resource import resource
from ._approval import Approval
from ._approver import Approver
from ._call import call_approver, record_approval
@dataclass
class ApprovalPolicy:
    """Policy mapping approvers to tools."""
    approver: Approver
    """Approver for policy."""
    tools: str | list[str]
    """Tools to use this approver for (can be full tool names or globs)."""
def policy_approver(policies: str | list[ApprovalPolicy]) -> Approver:
    # if policies is a str, it is a config file or an approver
    if isinstance(policies, str):
        policies = approval_policies_from_config(policies)
    # compile policy into approvers and regexes for matching
    policy_matchers: list[tuple[list[str], Approver]] = []
    for policy in policies:
        tools = [policy.tools] if isinstance(policy.tools, str) else policy.tools
        globs = [f"{tool}*" for tool in tools]
        policy_matchers.append((globs, policy.approver))
    # generator for policies that match a tool_call
    def tool_approvers(tool_call: ToolCall) -> Generator[Approver, None, None]:
        for policy_matcher in iter(policy_matchers):
            function_call = format_function_call(
                tool_call.function, tool_call.arguments, width=sys.maxsize
            )
            if any(
                [
                    fnmatch.fnmatch(function_call, pattern)
                    for pattern in policy_matcher[0]
                ]
            ):
                yield policy_matcher[1]
    async def approve(
        message: str,
        call: ToolCall,
        view: ToolCallView,
        state: TaskState | None = None,
    ) -> Approval:
        # process approvers for this tool call (continue loop on "escalate")
        has_approver = False
        for approver in tool_approvers(call):
            has_approver = True
            approval = await call_approver(approver, message, call, view, state)
            if approval.decision != "escalate":
                return approval
        # if there are no approvers then we reject
        reject = Approval(
            decision="reject",
            explanation=f"No {'approval granted' if has_approver else 'approvers registered'} for tool {call.function}",
        )
        # record and return the rejection
        record_approval("policy", message, call, view, reject)
        return reject
    return approve
class ApproverPolicyConfig(BaseModel):
    """
    Configuration format for approver policies.
    For example, here is a configuration in YAML:
    ```yaml
    approvers:
      - name: human
        tools: web_browser*, bash, pyhton
        choices: [approve, reject]
      - name: auto
        tools: *
        decision: approve
    ```
    """
    name: str
    tools: str | list[str]
    params: dict[str, Any] = Field(default_factory=dict)
    model_config = {
        "extra": "allow",
    }
    @model_validator(mode="before")
    @classmethod
    def collect_unknown_fields(cls, data: Any) -> Any:
        if not isinstance(data, dict):
            return data
        known_fields = set(["name", "tools", "params"])
        unknown_fields = {k: v for k, v in data.items() if k not in known_fields}
        if unknown_fields:
            data["params"] = data.get("params", {}) | unknown_fields
            for k in unknown_fields:
                data.pop(k, None)
        return data
class ApprovalPolicyConfig(BaseModel):
    approvers: list[ApproverPolicyConfig]
def approver_from_config(policy_config: str) -> Approver:
    policies = approval_policies_from_config(policy_config)
    return policy_approver(policies)
def approval_policies_from_config(
    policy_config: str | ApprovalPolicyConfig,
) -> list[ApprovalPolicy]:
    # create approver policy
    def create_approval_policy(
        name: str, tools: str | list[str], params: dict[str, Any] = {}
    ) -> ApprovalPolicy:
        approver = cast(Approver, registry_create("approver", name, **params))
        return ApprovalPolicy(approver, tools)
    # map config -> policy
    def policy_from_config(config: ApproverPolicyConfig) -> ApprovalPolicy:
        return create_approval_policy(config.name, config.tools, config.params)
    # resolve config if its a string
    if isinstance(policy_config, str):
        if Path(policy_config).exists():
            policy_config = read_policy_config(policy_config)
        elif registry_lookup("approver", policy_config):
            policy_config = ApprovalPolicyConfig(
                approvers=[ApproverPolicyConfig(name=policy_config, tools="*")]
            )
        else:
            raise ValueError(f"Invalid approval policy: {policy_config}")
    # resolve into approval policies
    return [policy_from_config(config) for config in policy_config.approvers]
def config_from_approval_policies(
    policies: list[ApprovalPolicy],
) -> ApprovalPolicyConfig:
    from inspect_ai._util.registry import (
        registry_log_name,
        registry_params,
    )
    approvers: list[ApproverPolicyConfig] = []
    for policy in policies:
        name = registry_log_name(policy.approver)
        params = registry_params(policy.approver)
        approvers.append(
            ApproverPolicyConfig(name=name, tools=policy.tools, params=params)
        )
    return ApprovalPolicyConfig(approvers=approvers)
def read_policy_config(policy_config: str) -> ApprovalPolicyConfig:
    # save specified policy for error message
    specified_policy_config = policy_config
    # read config file
    policy_config = resource(policy_config, type="file")
    # detect json vs. yaml
    policy_config_dict = read_config_object(policy_config)
    if not isinstance(policy_config_dict, dict):
        raise ValueError(f"Invalid approval policy: {specified_policy_config}")
    # parse and validate config
    return ApprovalPolicyConfig(**policy_config_dict)

================
File: src/inspect_ai/approval/_registry.py
================
import inspect
from typing import Any, Callable, TypeVar, cast
from typing_extensions import overload
from inspect_ai._util.registry import (
    RegistryInfo,
    registry_add,
    registry_name,
    registry_tag,
)
from ._approver import Approver
ApproverType = TypeVar("ApproverType", bound=Callable[..., Approver])
@overload
def approver(func: ApproverType) -> ApproverType: ...
@overload
def approver(
    *, name: str | None = ..., **attribs: Any
) -> Callable[[ApproverType], ApproverType]: ...
def approver(*args: Any, name: str | None = None, **attribs: Any) -> Any:
    r"""Decorator for registering approvers.
    Args:
      *args: Function returning `Approver` targeted by
        plain approver decorator without attributes (e.g. `@approver`)
      name:
        Optional name for approver. If the decorator has no name
        argument then the name of the function
        will be used to automatically assign a name.
      **attribs: Additional approver attributes.
    Returns:
        Approver with registry attributes.
    """
    def create_approver_wrapper(approver_type: ApproverType) -> ApproverType:
        # Get the name and parameters of the task
        approver_name = registry_name(
            approver_type, name or getattr(approver_type, "__name__")
        )
        params = list(inspect.signature(approver_type).parameters.keys())
        # Create and return the wrapper function
        def wrapper(*w_args: Any, **w_kwargs: Any) -> Approver:
            # Create the approver
            approver_instance = approver_type(*w_args, **w_kwargs)
            # Tag the approver with registry information
            registry_tag(
                approver_type,
                approver_instance,
                RegistryInfo(
                    type="approver",
                    name=approver_name,
                    metadata=dict(attribs=attribs, params=params),
                ),
                *w_args,
                **w_kwargs,
            )
            # Return the approver instance
            return approver_instance
        # Register the approver and return the wrapper
        return approver_register(
            approver=cast(ApproverType, wrapper),
            name=approver_name,
            attribs=attribs,
            params=params,
        )
    if args:
        # The decorator was used without arguments: @approver
        func = args[0]
        return create_approver_wrapper(func)
    else:
        # The decorator was used with arguments: @approver(name="foo")
        def decorator(func: ApproverType) -> ApproverType:
            return create_approver_wrapper(func)
        return decorator
def approver_register(
    approver: ApproverType, name: str, attribs: dict[str, Any], params: list[str]
) -> ApproverType:
    registry_add(
        approver,
        RegistryInfo(
            type="approver", name=name, metadata=dict(attribs=attribs, params=params)
        ),
    )
    return approver

================
File: src/inspect_ai/dataset/__init__.py
================
# ruff: noqa: F403 F405
from ._dataset import (
    Dataset,
    FieldSpec,
    MemoryDataset,
    RecordToSample,
    Sample,
)
from ._sources.csv import csv_dataset
from ._sources.example import example_dataset
from ._sources.file import file_dataset
from ._sources.hf import hf_dataset
from ._sources.json import json_dataset
__all__ = [
    "Dataset",
    "Sample",
    "FieldSpec",
    "RecordToSample",
    "MemoryDataset",
    "file_dataset",
    "csv_dataset",
    "hf_dataset",
    "json_dataset",
    "example_dataset",
]

================
File: src/inspect_ai/dataset/_dataset.py
================
import abc
import random
from dataclasses import dataclass, field
from typing import (
    TYPE_CHECKING,
    Any,
    Callable,
    Iterator,
    Sequence,
    Type,
    TypeVar,
    Union,
    overload,
)
from pydantic import BaseModel, Field, ValidationError
from typing_extensions import override
from inspect_ai.model import ChatMessage
from inspect_ai.util import SandboxEnvironmentSpec, SandboxEnvironmentType
from inspect_ai.util._sandbox.environment import resolve_sandbox_environment
if TYPE_CHECKING:
    from _typeshed import SupportsRichComparison
MT = TypeVar("MT", bound=BaseModel)
class Sample(BaseModel):
    r"""Sample for an evaluation task."""
    def __init__(
        self,
        input: str | list[ChatMessage],
        choices: list[str] | None = None,
        target: str | list[str] = "",
        id: int | str | None = None,
        metadata: dict[str, Any] | None = None,
        sandbox: SandboxEnvironmentType | None = None,
        files: dict[str, str] | None = None,
        setup: str | None = None,
    ) -> None:
        r"""Create a Sample.
        Args:
            input: The input to be submitted to the model.
            choices: Optional. List of available answer choices
                (used only for multiple-choice evals).
            target: Optional. Ideal target output. May be a literal value
                or narrative text to be used by a model grader.
            id: Optional. Unique identifier for sample.
            metadata: Optional. Arbitrary metadata associated with the sample.
                sandbox (SandboxEnvironmentType | None): Sandbox environment type (or optionally a str or tuple with a shorthand spec)
            sandbox: Optional. Sandbox specification for this sample.
            files: Optional. Files that go along with the sample (copied to
                SandboxEnvironment). Files can be paths, inline text, or inline binary (base64 encoded data URL).
            setup: Optional. Setup script to run for sample (run
                within default SandboxEnvironment).
        """
        super().__init__(
            input=input,
            choices=choices,
            target=target,
            id=id,
            metadata=metadata,
            sandbox=resolve_sandbox_environment(sandbox),
            files=files,
            setup=setup,
        )
    input: str | list[ChatMessage]
    """The input to be submitted to the model."""
    choices: list[str] | None = Field(default=None)
    """List of available answer choices (used only for multiple-choice evals)."""
    target: str | list[str] = Field(default_factory=str)
    """Ideal target output. May be a literal value or narrative text to be used by a model grader."""
    id: int | str | None = Field(default=None)
    """Unique identifier for sample."""
    metadata: dict[str, Any] | None = Field(default=None)
    """Arbitrary metadata associated with the sample."""
    def metadata_as(self, metadata_cls: Type[MT]) -> MT:
        """Metadata as a Pydantic model.
        Args:
           metadata_cls: BaseModel derived class.
        Returns:
           BaseModel: Instance of metadata_cls.
        """
        if self.metadata is None:
            raise ValueError("Sample does not have metadata")
        return metadata_as(self.metadata, metadata_cls)
    sandbox: SandboxEnvironmentSpec | None = Field(default=None)
    """Sandbox environment type and optional config file."""
    files: dict[str, str] | None = Field(default=None)
    """Files that go along with the sample (copied to SandboxEnvironment)"""
    setup: str | None = Field(default=None)
    """Setup script to run for sample (run within default SandboxEnvironment)."""
def sample_input_len(sample: Sample) -> int:
    """Measures the length of a samples `input` field.
    The default length function use in `Dataset.sort()`.
    Args:
        sample (Sample): A Sample to be used in an evaluation task.
    """
    return (
        len(sample.input)
        if isinstance(sample.input, str)
        else sum(len(inp.text) for inp in sample.input)
    )
DatasetRecord = dict[str, Any]
DatasetReader = Iterator[DatasetRecord]
class Dataset(Sequence[Sample], abc.ABC):
    r"""A sequence of Sample objects.
    Datasets provide sequential access (via conventional indexes or slicing)
    to a collection of Sample objects.
    """
    @property
    @abc.abstractmethod
    def name(self) -> str | None: ...
    @property
    @abc.abstractmethod
    def location(self) -> str | None: ...
    @property
    @abc.abstractmethod
    def shuffled(self) -> bool: ...
    @overload
    def __getitem__(self, index: int) -> Sample: ...
    @overload
    def __getitem__(self, index: slice) -> "Dataset": ...
    @abc.abstractmethod
    def __getitem__(self, index: Union[int, slice]) -> Union[Sample, "Dataset"]: ...
    @abc.abstractmethod
    def __len__(self) -> int: ...
    @abc.abstractmethod
    def sort(
        self,
        reverse: bool = False,
        key: Callable[[Sample], "SupportsRichComparison"] = sample_input_len,
    ) -> None:
        """Sort the dataset (in place) in ascending order and return None.
        If a key function is given, apply it once to each list item and sort them, ascending or descending, according to their function values.
        The key function defaults to measuring the length of the sample's input field.
        Args:
            reverse: If `Treu`, sort in descending order. Defaults to False.
            key: a callable mapping each item to a numeric value (optional, defaults to sample_input_len).
        """
    @abc.abstractmethod
    def filter(
        self, predicate: Callable[[Sample], bool], name: str | None = None
    ) -> "Dataset":
        """Filter the dataset using a predicate.
        Args:
          predicate: Filtering function.
          name: Name for filtered dataset (optional).
        Returns:
          Filtered dataset.
        """
    @abc.abstractmethod
    def shuffle(self, seed: int | None = None) -> None:
        """Shuffle the order of the dataset (in place).
        Args:
           seed: Random seed for shuffling (optional).
        """
    @abc.abstractmethod
    def shuffle_choices(self, seed: int | None = None) -> None:
        """Shuffle the order of the choices with each sample.
        Args:
           seed: Random seed for shuffling (optional).
        """
@dataclass
class FieldSpec:
    r"""Specification for mapping data source fields to sample fields."""
    input: str = field(default="input")
    """Name of the field containing the sample input."""
    target: str = field(default="target")
    """Name of the field containing the sample target."""
    choices: str = field(default="choices")
    """Name of field containing the list of answer choices."""
    id: str = field(default="id")
    """ Unique identifier for the sample."""
    metadata: list[str] | Type[BaseModel] | None = field(default=None)
    """List of additional field names that should be read as metadata."""
    sandbox: str = field(default="sandbox")
    """Sandbox type along with optional config file."""
    files: str = field(default="files")
    """Files that go along wtih the sample."""
    setup: str = field(default="setup")
    """Setup script to run for sample (run within default SandboxEnvironment)."""
RecordToSample = Callable[[DatasetRecord], Sample | list[Sample]]
r"""Callable that maps raw dictionary record to a Sample."""
class MemoryDataset(Dataset):
    r"""A Dataset stored in memory."""
    def __init__(
        self,
        samples: list[Sample],
        name: str | None = None,
        location: str | None = None,
        shuffled: bool = False,
    ) -> None:
        r"""A dataset of samples held in an in-memory list.
        Datasets provide sequential access (via conventional indexes or slicing)
        to a collection of Sample objects. The ListDataset is explicitly
        initialized with a list that is held in memory.
        Args:
            samples (list[Sample]): The list of sample objects.
            name (str | None): Optional name for dataset.
            location (str | None): Optional location for dataset.
            shuffled (bool): Was the dataset shuffled after reading.
        """
        self.samples = samples
        self._name = name
        self._location = location
        self._shuffled = shuffled
    @override
    @property
    def name(self) -> str | None:
        """Dataset name."""
        return self._name
    @override
    @property
    def location(self) -> str | None:
        """Dataset location."""
        return self._location
    @override
    @property
    def shuffled(self) -> bool:
        """Was the dataset shuffled."""
        return self._shuffled
    @overload
    def __getitem__(self, index: int) -> Sample: ...
    @overload
    def __getitem__(self, index: slice) -> Dataset: ...
    @override
    def __getitem__(self, index: Union[int, slice]) -> Union[Sample, Dataset]:
        if isinstance(index, int):
            return self.samples[index]
        else:
            return MemoryDataset(
                samples=self.samples[index],
                name=self.name,
                location=self.location,
                shuffled=self.shuffled,
            )
    @override
    def __len__(self) -> int:
        return len(self.samples)
    @override
    def shuffle(self, seed: int | None = None) -> None:
        if seed:
            random.Random(seed).shuffle(self.samples)
        else:
            random.shuffle(self.samples)
        self._shuffled = True
    @override
    def shuffle_choices(self, seed: int | None = None) -> None:
        rand = random.Random(seed)
        for sample in self.samples:
            if not sample.choices:
                continue
            # The original positions
            positions = list(range(len(sample.choices)))
            # Shuffle the choices
            rand.shuffle(positions)
            shuffled_choices = [sample.choices[i] for i in positions]
            # Map of original position / target letter
            position_map = {i: chr(65 + new_i) for new_i, i in enumerate(positions)}
            # Update to the shuffled choices and target
            sample.choices = shuffled_choices
            sample.target = self._remap_target(sample.target, position_map=position_map)
    def _remap_target(
        self, target: str | list[str], position_map: dict[int, str]
    ) -> str | list[str]:
        if isinstance(target, list):
            return [position_map[ord(t) - 65] for t in target]
        else:
            return position_map[ord(target) - 65]
    @override
    def sort(
        self,
        reverse: bool = False,
        key: Callable[[Sample], "SupportsRichComparison"] = sample_input_len,
    ) -> None:
        self.samples.sort(reverse=reverse, key=key)
    @override
    def filter(
        self, predicate: Callable[[Sample], bool], name: str | None = None
    ) -> "MemoryDataset":
        return MemoryDataset(
            name=name or self.name,
            location=self.location,
            samples=[sample for sample in self if predicate(sample)],
            shuffled=self.shuffled,
        )
def metadata_as(metadata: dict[str, Any], metadata_cls: Type[MT]) -> MT:
    # validate that metadata_cls is frozen
    if not metadata_cls.model_config.get("frozen", False):
        raise ValueError(
            f"Metadata model {metadata_cls.__name__} must have frozen=True"
        )
    # filter to only fields in the model
    model_fields = {
        k: v
        for k, v in metadata.items()
        if k in metadata_cls.__pydantic_fields__.keys()
    }
    # parse and return model instance
    try:
        return metadata_cls(**model_fields)
    except ValidationError as ex:
        raise ValueError(f"Could not parse metadata into {metadata_cls.__name__}: {ex}")

================
File: src/inspect_ai/dataset/_sources/csv.py
================
import csv
import os
from io import TextIOWrapper
from pathlib import Path
from typing import Any
from inspect_ai._util.file import file
from inspect_ai.dataset._sources.util import resolve_sample_files
from .._dataset import (
    Dataset,
    DatasetReader,
    FieldSpec,
    MemoryDataset,
    RecordToSample,
)
from .._util import data_to_samples, record_to_sample_fn
def csv_dataset(
    csv_file: str,
    sample_fields: FieldSpec | RecordToSample | None = None,
    auto_id: bool = False,
    shuffle: bool = False,
    seed: int | None = None,
    shuffle_choices: bool | int | None = None,
    limit: int | None = None,
    dialect: str = "unix",
    encoding: str = "utf-8",
    name: str | None = None,
    fs_options: dict[str, Any] = {},
    fieldnames: list[str] | None = None,
    delimiter: str = ",",
) -> Dataset:
    r"""Read dataset from CSV file.
    Args:
        csv_file: Path to CSV file. Can be a local filesystem path,
            a path to an S3 bucket (e.g. "s3://my-bucket"), or an HTTPS URL.
            Use `fs_options` to pass arguments through to the `S3FileSystem` constructor.
        sample_fields: Method of mapping underlying
            fields in the data source to Sample objects. Pass `None` if the data is already
            stored in `Sample` form (i.e. has "input" and "target" columns.); Pass a
            `FieldSpec` to specify mapping fields by name; Pass a `RecordToSample` to
            handle mapping with a custom function that returns one or more samples.
        auto_id: Assign an auto-incrementing ID for each sample.
        shuffle: Randomly shuffle the dataset order.
        seed: Seed used for random shuffle.
        shuffle_choices: Whether to shuffle the choices. If an int is passed, this will be used as the seed when shuffling.
        limit: Limit the number of records to read.
        dialect: CSV dialect ("unix", "excel" or"excel-tab"). Defaults to "unix". See https://docs.python.org/3/library/csv.html#dialects-and-formatting-parameters for more details
        encoding: Text encoding for file (defaults to "utf-8").
        name: Optional name for dataset (for logging). If not specified,
            defaults to the stem of the filename
        fs_options: Optional. Additional arguments to pass through
            to the filesystem provider (e.g. `S3FileSystem`). Use `{"anon": True }`
            if you are accessing a public S3 bucket with no credentials.
        fieldnames: Optional. A list of fieldnames to use for the CSV.
            If None, the values in the first row of the file will be used as the fieldnames.
            Useful for files without a header.
        delimiter: Optional. The delimiter to use when parsing the file. Defaults to ",".
    Returns:
        Dataset read from CSV file.
    """
    # resolve data_to_sample function
    data_to_sample = record_to_sample_fn(sample_fields)
    # read and convert samples
    with file(csv_file, "r", encoding=encoding, fs_options=fs_options) as f:
        # filter out rows with empty values
        valid_data = [
            data
            for data in csv_dataset_reader(f, dialect, fieldnames, delimiter)
            if data and any(value.strip() for value in data.values())
        ]
        name = name if name else Path(csv_file).stem
        dataset = MemoryDataset(
            samples=data_to_samples(valid_data, data_to_sample, auto_id),
            name=name,
            location=os.path.abspath(csv_file),
        )
        # resolve relative file paths
        resolve_sample_files(dataset)
        # shuffle if requested
        if shuffle:
            dataset.shuffle(seed=seed)
        # shuffle choices, if requested
        if isinstance(shuffle_choices, int):
            dataset.shuffle_choices(seed=shuffle_choices)
        elif shuffle_choices is True:
            dataset.shuffle_choices()
        # limit if requested
        if limit:
            return dataset[0:limit]
        return dataset
def csv_dataset_reader(
    file: TextIOWrapper,
    dialect: str = "unix",
    fieldnames: list[str] | None = None,
    delimiter: str = ",",
) -> DatasetReader:
    return csv.DictReader(
        file, dialect=dialect, fieldnames=fieldnames, delimiter=delimiter
    )

================
File: src/inspect_ai/dataset/_sources/example.py
================
from pathlib import Path
from typing import Callable
from .._dataset import Dataset, FieldSpec, MemoryDataset, RecordToSample
from .csv import csv_dataset
from .json import json_dataset
EXAMPLES_PATH = Path(__file__).parent.parent / "_examples"
def example_dataset(
    name: str,
    sample_fields: FieldSpec | RecordToSample | None = None,
) -> Dataset:
    """Read a dataset from inspect_ai package examples.
    This is primarily used for sharing runnable example
    snippets that don't need to read an external dataset.
    Args:
      name (str): Example dataset name. One of 'security_guide', 'theory_of_mind', 'popularity', or 'biology_qa'.
      sample_fields (FieldSpec | RecordToSample): Method of mapping underlying
        fields in the data source to `Sample` objects. Pass `None` if the data is already
        stored in `Sample` form (i.e. object with "input" and "target" fields); Pass a
        `FieldSpec` to specify mapping fields by name; Pass a `RecordToSample` to
        handle mapping with a custom function that returns one or more samples.
    Returns:
      Dataset read from example file.
    """
    def get_dataset(
        file_path: Path,
        dataset_func: Callable[[str, FieldSpec | RecordToSample | None], Dataset],
    ) -> Dataset | None:
        if file_path.exists():
            return dataset_func(str(file_path), sample_fields)
        return None
    json_file = EXAMPLES_PATH / f"{name}.jsonl"
    csv_file = EXAMPLES_PATH / f"{name}.csv"
    dataset = get_dataset(json_file, json_dataset) or get_dataset(csv_file, csv_dataset)
    if dataset is None:
        available_datasets = [
            file.stem for file in EXAMPLES_PATH.iterdir() if file.is_file()
        ]
        raise ValueError(
            f"Sample dataset {name} not found. Available datasets: {available_datasets}"
        )
    return MemoryDataset(samples=list(dataset), name=name, location=f"example://{name}")

================
File: src/inspect_ai/dataset/_sources/file.py
================
import os
from typing import Any
from .._dataset import (
    Dataset,
    FieldSpec,
    RecordToSample,
)
from .csv import csv_dataset
from .json import json_dataset
def file_dataset(
    file: str,
    sample_fields: FieldSpec | RecordToSample | None = None,
    auto_id: bool = False,
    shuffle: bool = False,
    seed: int | None = None,
    shuffle_choices: bool | int | None = None,
    limit: int | None = None,
    dialect: str = "unix",
    encoding: str = "utf-8",
    name: str | None = None,
    fs_options: dict[str, Any] = {},
    fieldnames: list[str] | None = None,
) -> Dataset:
    """Dataset read from a JSON or CSV file.
    The `file_dataset` function supports reading from CSV and JSON files
    (and automatically delegates to the appropriate function to do so)
    Args:
        file (str): Path to JSON or CSV file. Can be a local filesystem path or
            a path to an S3 bucket (e.g. "s3://my-bucket"). Use `fs_options`
            to pass arguments through to the `S3FileSystem` constructor.
        sample_fields (FieldSpec | RecordToSample): Method of mapping underlying
            fields in the data source to Sample objects. Pass `None` if the data is already
            stored in `Sample` form (i.e. has "input" and "target" columns.); Pass a
            `FieldSpec` to specify mapping fields by name; Pass a `RecordToSample` to
            handle mapping with a custom function that returns one or more samples.
        auto_id (bool): Assign an auto-incrementing ID for each sample.
        shuffle (bool): Randomly shuffle the dataset order.
        seed: (int | None): Seed used for random shuffle.
        shuffle_choices: (bool | int | None): Whether to shuffle the choices. If an int is passed, this will be used as the seed when shuffling.
        limit (int | None): Limit the number of records to read.
        dialect (str): CSV dialect ("unix" or "excel", defaults to "unix"). Only
            applies to reading CSV files.
        encoding (str): Text encoding for file (defaults to "utf-8").
        name (str): Optional name for dataset (for logging). If not specified,
            defaults to the stem of the filename
        fs_options (dict[str, Any]): Optional. Additional arguments to pass through
            to the filesystem provider (e.g. `S3FileSystem`). Use `{"anon": True }`
            if you are accessing a public S3 bucket with no credentials.
        fieldnames (list[str] | None): Optional. A list of fieldnames to use for the CSV.
            If None, the values in the first row of the file will be used as the fieldnames.
            Useful for files without a header. Only applies to reading CSV files.
    Returns:
        Dataset read from JSON or CSV file.
    """
    ext = os.path.splitext(file)[1].lower()
    match ext:
        case ".json" | ".jsonl":
            return json_dataset(
                json_file=file,
                sample_fields=sample_fields,
                auto_id=auto_id,
                shuffle=shuffle,
                seed=seed,
                shuffle_choices=shuffle_choices,
                limit=limit,
                encoding=encoding,
                name=name,
                fs_options=fs_options,
            )
        case ".csv":
            return csv_dataset(
                csv_file=file,
                sample_fields=sample_fields,
                auto_id=auto_id,
                shuffle=shuffle,
                seed=seed,
                shuffle_choices=shuffle_choices,
                limit=limit,
                dialect=dialect,
                encoding=encoding,
                name=name,
                fs_options=fs_options,
                fieldnames=fieldnames,
            )
        case _:
            raise ValueError(f"No dataset reader for file with extension {ext}")

================
File: src/inspect_ai/dataset/_sources/hf.py
================
# mypy: disable-error-code="unused-ignore"
import os
from pathlib import Path
from typing import Any
from inspect_ai._util.appdirs import inspect_cache_dir
from inspect_ai._util.error import pip_dependency_error
from inspect_ai._util.file import safe_filename
from inspect_ai._util.hash import mm3_hash
from inspect_ai._util.version import verify_required_version
from .._dataset import (
    Dataset,
    FieldSpec,
    MemoryDataset,
    RecordToSample,
)
from .._util import data_to_samples, record_to_sample_fn
def hf_dataset(
    path: str,
    split: str,
    name: str | None = None,
    data_dir: str | None = None,
    revision: str | None = None,
    sample_fields: FieldSpec | RecordToSample | None = None,
    auto_id: bool = False,
    shuffle: bool = False,
    seed: int | None = None,
    shuffle_choices: bool | int | None = None,
    limit: int | None = None,
    trust: bool = False,
    cached: bool = True,
    **kwargs: Any,
) -> Dataset:
    """Datasets read using the Hugging Face `datasets` package.
    The `hf_dataset` function supports reading datasets using the Hugging Face
    `datasets` package, including remote datasets on Hugging Face Hub.
    Args:
      path: Path or name of the dataset. Depending on path, the dataset
        builder that is used comes from a generic dataset script (JSON, CSV,
        Parquet, text etc.) or from the dataset script (a python file) inside
        the dataset directory.
      split: Which split of the data to load.
      name: Name of the dataset configuration.
      data_dir: data_dir of the dataset configuration
        to read data from.
      revision: Specific revision to load (e.g. "main", a branch
        name, or a specific commit SHA). When using `revision` the `cached` option
        is ignored and datasets are revalidated on Hugging Face before loading.
      sample_fields: Method of mapping underlying
        fields in the data source to Sample objects. Pass `None` if the data is already
        stored in `Sample` form (i.e. has "input" and "target" columns.); Pass a
        `FieldSpec` to specify mapping fields by name; Pass a `RecordToSample` to
          handle mapping with a custom function that returns one or more samples.
      auto_id: Assign an auto-incrementing ID for each sample.
      shuffle: Randomly shuffle the dataset order.
      seed: Seed used for random shuffle.
      shuffle_choices: Whether to shuffle the choices. If an int is passed, this will be used as the seed when shuffling.
      limit: Limit the number of records to read.
      trust: Whether or not to allow for datasets defined on the Hub
        using a dataset script. This option should only be set to True for
        repositories you trust and in which you have read the code, as it
        will execute code present on the Hub on your local machine.
      cached: By default, datasets are read once from HuggingFace
        Hub and then cached for future reads. Pass `cached=False` to force
        re-reading the dataset from Hugging Face. Ignored when the `revision`
        option is specified.
      **kwargs (dict[str, Any]): Additional arguments to pass through to the
          `load_dataset` function of the `datasets` package.
    Returns:
        Dataset read from Hugging Face
    """
    # ensure we have the datasets package (>= v2.16, which supports trust_remote_code)
    FEATURE = "Hugging Face Datasets"
    PACKAGE = "datasets"
    VERSION = "2.16.0"
    try:
        import datasets  # type: ignore
    except ImportError:
        raise pip_dependency_error(FEATURE, [PACKAGE])
    verify_required_version(FEATURE, PACKAGE, VERSION)
    # resolve data_to_sample function
    data_to_sample = record_to_sample_fn(sample_fields)
    # generate a unique cache dir for this dataset
    dataset_hash = mm3_hash(f"{path}{name}{data_dir}{split}{kwargs}")
    datasets_cache_dir = inspect_cache_dir("hf_datasets")
    dataset_cache_dir = os.path.join(
        datasets_cache_dir, f"{safe_filename(path)}-{dataset_hash}"
    )
    if os.path.exists(dataset_cache_dir) and cached and revision is None:
        dataset = datasets.load_from_disk(dataset_cache_dir)
    else:
        print(f"Loading dataset {path} from Hugging Face...")
        dataset = datasets.load_dataset(  # type: ignore
            path=path,
            name=name,
            data_dir=data_dir,
            split=split,
            revision=revision,
            trust_remote_code=trust,
            **kwargs,
        )
        dataset.save_to_disk(dataset_cache_dir)
    # shuffle if requested
    if shuffle:
        dataset = dataset.shuffle(seed=seed)
    # limit if requested
    if limit:
        dataset = dataset.select(range(limit))
    # return the dataset
    memory_dataset = MemoryDataset(
        samples=data_to_samples(dataset.to_list(), data_to_sample, auto_id),
        name=Path(path).stem if Path(path).exists() else path,
        location=path,
    )
    # maybe shuffle the choices
    if isinstance(shuffle_choices, int):
        memory_dataset.shuffle_choices(seed=shuffle_choices)
    elif shuffle_choices is True:
        memory_dataset.shuffle_choices()
    return memory_dataset

================
File: src/inspect_ai/dataset/_sources/json.py
================
import json
import os
from io import TextIOWrapper
from pathlib import Path
from typing import Any, cast
import jsonlines
from inspect_ai._util.file import file
from .._dataset import (
    Dataset,
    DatasetReader,
    FieldSpec,
    MemoryDataset,
    RecordToSample,
)
from .._util import data_to_samples, record_to_sample_fn
from .util import resolve_sample_files
def json_dataset(
    json_file: str,
    sample_fields: FieldSpec | RecordToSample | None = None,
    auto_id: bool = False,
    shuffle: bool = False,
    seed: int | None = None,
    shuffle_choices: bool | int | None = None,
    limit: int | None = None,
    encoding: str = "utf-8",
    name: str | None = None,
    fs_options: dict[str, Any] = {},
) -> Dataset:
    r"""Read dataset from a JSON file.
    Read a dataset from a JSON file containing an array of objects, or
    from a JSON Lines file containing one object per line. These objects may
    already be formatted as `Sample` instances, or may require some mapping using
    the `sample_fields` argument.
    Args:
      json_file: Path to JSON file. Can be a local filesystem path or
        a path to an S3 bucket (e.g. "s3://my-bucket"). Use `fs_options`
        to pass arguments through to the `S3FileSystem` constructor.
      sample_fields: Method of mapping underlying
        fields in the data source to `Sample` objects. Pass `None` if the data is already
        stored in `Sample` form (i.e. object with "input" and "target" fields); Pass a
        `FieldSpec` to specify mapping fields by name; Pass a `RecordToSample` to
        handle mapping with a custom function that returns one or more samples.
      auto_id: Assign an auto-incrementing ID for each sample.
      shuffle: Randomly shuffle the dataset order.
      seed: Seed used for random shuffle.
      shuffle_choices: Whether to shuffle the choices. If an int is passed, this will be used as the seed when shuffling.
      limit: Limit the number of records to read.
      encoding: Text encoding for file (defaults to "utf-8").
      name: Optional name for dataset (for logging). If not specified,
        defaults to the stem of the filename.
      fs_options: Optional. Additional arguments to pass through
        to the filesystem provider (e.g. `S3FileSystem`). Use `{"anon": True }`
        if you are accessing a public S3 bucket with no credentials.
    Returns:
        Dataset read from JSON file.
    """
    # resolve data_to_sample function
    data_to_sample = record_to_sample_fn(sample_fields)
    # pick the right reader for the file extension
    dataset_reader = (
        jsonlines_dataset_reader
        if json_file.lower().endswith(".jsonl")
        else json_dataset_reader
    )
    # read and convert samples
    with file(json_file, "r", encoding=encoding, fs_options=fs_options) as f:
        name = name if name else Path(json_file).stem
        dataset = MemoryDataset(
            samples=data_to_samples(dataset_reader(f), data_to_sample, auto_id),
            name=name,
            location=os.path.abspath(json_file),
        )
        # resolve relative file paths
        resolve_sample_files(dataset)
        # shuffle if requested
        if shuffle:
            dataset.shuffle(seed=seed)
        # shuffle choices, if requested
        if isinstance(shuffle_choices, int):
            dataset.shuffle_choices(seed=shuffle_choices)
        elif shuffle_choices is True:
            dataset.shuffle_choices()
        # limit if requested
        if limit:
            return dataset[0:limit]
    return dataset
def jsonlines_dataset_reader(file: TextIOWrapper) -> DatasetReader:
    jsonlines_reader = jsonlines.Reader(file)
    return jsonlines_reader.iter(type=dict)
def json_dataset_reader(file: TextIOWrapper) -> DatasetReader:
    data = cast(list[dict[str, Any]], json.load(file))
    return iter(data)

================
File: src/inspect_ai/dataset/_sources/util.py
================
from typing import Callable
from inspect_ai._util.content import Content, ContentAudio, ContentImage, ContentVideo
from inspect_ai._util.file import filesystem
from inspect_ai.model._chat_message import ChatMessage, ChatMessageUser
from inspect_ai.util._sandbox.environment import SandboxEnvironmentSpec
from .._dataset import Dataset
def resolve_sample_files(dataset: Dataset) -> None:
    """Resolve relative file paths to absolute (using the input file path)"""
    # bail if the dataset has no location
    if not dataset.location:
        return
    # filesystem and parent for resolving paths
    fs = filesystem(dataset.location)
    parent_dir = fs.sep.join(dataset.location.split(fs.sep)[:-1])
    # resolve file locations
    def resolve_file(file: str) -> str:
        # try/except (and ignore) to tolerate 'paths' that are actually
        # file contents (so will trip OS name too long constraints)
        try:
            target_file = f"{parent_dir}{fs.sep}{file}"
            if fs.exists(target_file):
                return target_file
            else:
                return file
        except OSError:
            return file
    # for each sample
    for sample in dataset:
        # check for sandbox config file
        if sample.sandbox and isinstance(sample.sandbox.config, str):
            sample.sandbox = SandboxEnvironmentSpec(
                sample.sandbox.type, resolve_file(sample.sandbox.config)
            )
        # check for files
        if sample.files is not None:
            for path in sample.files.keys():
                sample.files[path] = resolve_file(sample.files[path])
        # check for setup script
        if sample.setup is not None:
            sample.setup = resolve_file(sample.setup)
        # check for image paths
        if not isinstance(sample.input, str):
            sample.input = messages_with_resolved_content(sample.input, resolve_file)
def messages_with_resolved_content(
    messages: list[ChatMessage], resolver: Callable[[str], str]
) -> list[ChatMessage]:
    return [message_with_resolved_content(message, resolver) for message in messages]
def message_with_resolved_content(
    message: ChatMessage, resolver: Callable[[str], str]
) -> ChatMessage:
    if isinstance(message, ChatMessageUser) and not isinstance(message.content, str):
        return message.model_copy(
            update=dict(
                content=[
                    chat_content_with_resolved_content(content, resolver)
                    for content in message.content
                ],
            )
        )
    else:
        return message
def chat_content_with_resolved_content(
    content: Content, resolver: Callable[[str], str]
) -> Content:
    if isinstance(content, ContentImage):
        return ContentImage(
            image=resolver(content.image),
            detail=content.detail,
        )
    elif isinstance(content, ContentAudio):
        return ContentAudio(audio=resolver(content.audio), format=content.format)
    elif isinstance(content, ContentVideo):
        return ContentVideo(video=resolver(content.video), format=content.format)
    else:
        return content

================
File: src/inspect_ai/dataset/_util.py
================
import json
from typing import Any, Iterable, cast
from pydantic import ValidationError
from inspect_ai.model import (
    ChatMessage,
    ChatMessageAssistant,
    ChatMessageSystem,
    ChatMessageTool,
    ChatMessageUser,
)
from inspect_ai.util._sandbox.environment import SandboxEnvironmentSpec
from ._dataset import (
    DatasetRecord,
    FieldSpec,
    RecordToSample,
    Sample,
)
# determine how we will go from file records to samples. if there is
# no field spec, we assume the column names "input" and "target",
# otherwise use the provided field spec or custom converter function
def record_to_sample_fn(
    sample_fields: FieldSpec | RecordToSample | None,
) -> RecordToSample:
    if sample_fields is None:
        sample_fields = FieldSpec()
    if isinstance(sample_fields, FieldSpec):
        def record_to_sample(record: DatasetRecord) -> Sample:
            # collect metadata if specified
            metadata: dict[str, Any] | None = None
            if sample_fields.metadata:
                if isinstance(sample_fields.metadata, list):
                    metadata = {}
                    for name in sample_fields.metadata:
                        metadata[name] = record.get(name)
                else:
                    # must be frozen
                    if not sample_fields.metadata.model_config.get("frozen", False):
                        raise ValueError(
                            f"Metadata model {sample_fields.metadata.__name__} must have frozen=True"
                        )
                    # filter to only fields in the model
                    model_fields = record.get("metadata", None)
                    if isinstance(model_fields, str):
                        model_fields = json.loads(model_fields)
                    elif model_fields is None:
                        model_fields = {
                            k: v
                            for k, v in record.items()
                            if k in sample_fields.metadata.__pydantic_fields__.keys()
                        }
                    # parse and return metadata
                    try:
                        metadata = sample_fields.metadata(**model_fields).model_dump()
                    except ValidationError as ex:
                        raise ValueError(
                            f"Could not parse metadata into {sample_fields.metadata.__name__}: {ex}"
                        )
            elif "metadata" in record:
                metadata_field = record.get("metadata")
                if isinstance(metadata_field, str):
                    metadata = json.loads(metadata_field)
                elif isinstance(metadata_field, dict):
                    metadata = metadata_field
                else:
                    raise ValueError(
                        f"Unexpected type for 'metadata' field: {type(metadata_field)}"
                    )
            # return sample
            return Sample(
                input=read_input(record.get(sample_fields.input)),
                target=read_target(record.get(sample_fields.target)),
                choices=read_choices(record.get(sample_fields.choices)),
                id=record.get(sample_fields.id, None),
                metadata=metadata,
                sandbox=read_sandbox(record.get(sample_fields.sandbox)),
                files=read_files(record.get(sample_fields.files)),
                setup=read_setup(record.get(sample_fields.setup)),
            )
        return record_to_sample
    else:
        return sample_fields
def data_to_samples(
    data: Iterable[DatasetRecord], data_to_sample: RecordToSample, auto_id: bool
) -> list[Sample]:
    next_id = 1
    samples: list[Sample] = []
    for record in data:
        record_samples = as_sample_list(data_to_sample(record))
        if auto_id:
            for record_sample in record_samples:
                record_sample.id = next_id
                next_id += 1
        samples.extend(record_samples)
    return samples
def as_sample_list(samples: Sample | list[Sample]) -> list[Sample]:
    if isinstance(samples, list):
        return samples
    else:
        return [samples]
def read_input(input: Any | None) -> str | list[ChatMessage]:
    if not input:
        raise ValueError("No input in dataset")
    if not isinstance(input, str):
        return read_messages(input)
    else:
        return input
def read_messages(messages: list[dict[str, Any]]) -> list[ChatMessage]:
    chat_messages: list[ChatMessage] = []
    for message in messages:
        role = message.get("role", None)
        content = message.get("content", None)
        if content is None:
            raise ValueError("content not specified for chat input in dataset")
        match role:
            case "system":
                chat_messages.append(ChatMessageSystem(content=content, source="input"))
            case "user":
                chat_messages.append(ChatMessageUser(content=content, source="input"))
            case "assistant":
                chat_messages.append(
                    ChatMessageAssistant(
                        content=content,
                        source="input",
                        tool_calls=message.get("tool_calls", None),
                    )
                )
            case "tool":
                chat_messages.append(
                    ChatMessageTool(
                        content=content,
                        source="input",
                        tool_call_id=message.get("tool_call_id", None),
                        function=message.get("function", None),
                        error=message.get("error", None),
                    )
                )
            case _:
                raise ValueError("role not specified for chat input in dataset")
    return chat_messages
def read_target(obj: Any | None) -> str | list[str]:
    if obj is not None:
        return [str(item) for item in obj] if isinstance(obj, list) else str(obj)
    else:
        return ""
def read_choices(obj: Any | None) -> list[str] | None:
    if obj is not None:
        if isinstance(obj, list):
            return [str(choice) for choice in obj]
        elif isinstance(obj, str):
            choices = obj.split(",")
            if len(choices) == 1:
                choices = obj.split()
            return [choice.strip() for choice in choices]
        else:
            return [str(obj)]
    else:
        return None
def read_setup(setup: Any | None) -> str | None:
    if setup is not None:
        return str(setup)
    else:
        return None
def read_sandbox(sandbox: Any | None) -> SandboxEnvironmentSpec | None:
    if sandbox is not None:
        if isinstance(sandbox, str):
            if sandbox.strip().startswith("["):
                sandbox = json.loads(sandbox)
            else:
                return SandboxEnvironmentSpec(sandbox)
        if isinstance(sandbox, list):
            if len(sandbox) == 2:
                return SandboxEnvironmentSpec(str(sandbox[0]), str(sandbox[1]))
            else:
                raise ValueError(
                    f"Invalid 'sandbox' value: '{str(sandbox)}'. Sandbox must be string or 2-item list"
                )
        # didn't find the right type
        raise ValueError(f"Unexpected type for 'sandbox' field: {type(sandbox)}")
    else:
        return None
def read_files(files: Any | None) -> dict[str, str] | None:
    if files is not None:
        if isinstance(files, str):
            files = json.loads(files)
        if isinstance(files, dict):
            if all(isinstance(v, str) for v in files.values()):
                return cast(dict[str, str], files)
        # didn't find the right type
        raise ValueError(f"Unexpected type for 'files' field: {type(files)}")
    else:
        return None

================
File: src/inspect_ai/log/__init__.py
================
from inspect_ai._util.error import EvalError
from ._bundle import bundle_log_dir
from ._condense import condense_sample, resolve_sample_attachments
from ._convert import convert_eval_logs
from ._file import (
    EvalLogInfo,
    list_eval_logs,
    read_eval_log,
    read_eval_log_async,
    read_eval_log_sample,
    read_eval_log_samples,
    write_eval_log,
    write_eval_log_async,
    write_log_dir_manifest,
)
from ._log import (
    EvalConfig,
    EvalDataset,
    EvalLog,
    EvalMetric,
    EvalPlan,
    EvalPlanStep,
    EvalResults,
    EvalRevision,
    EvalSample,
    EvalSampleLimit,
    EvalSampleReductions,
    EvalSampleScore,
    EvalScore,
    EvalSpec,
    EvalStats,
)
from ._message import LoggingLevel, LoggingMessage
from ._retry import retryable_eval_logs
from ._transcript import (
    ApprovalEvent,
    ErrorEvent,
    Event,
    InfoEvent,
    InputEvent,
    LoggerEvent,
    ModelEvent,
    SampleInitEvent,
    SampleLimitEvent,
    SandboxEvent,
    ScoreEvent,
    StateEvent,
    StepEvent,
    StoreEvent,
    SubtaskEvent,
    ToolEvent,
    Transcript,
    transcript,
)
__all__ = [
    "EvalConfig",
    "EvalError",
    "EvalDataset",
    "EvalLog",
    "EvalMetric",
    "EvalPlan",
    "EvalPlanStep",
    "EvalResults",
    "EvalRevision",
    "EvalSample",
    "EvalSampleLimit",
    "EvalSampleScore",
    "EvalSampleReductions",
    "EvalScore",
    "EvalSpec",
    "EvalStats",
    "EvalLogInfo",
    "LoggingLevel",
    "LoggingMessage",
    "Transcript",
    "transcript",
    "Event",
    "ApprovalEvent",
    "ErrorEvent",
    "InfoEvent",
    "InputEvent",
    "LoggerEvent",
    "ModelEvent",
    "SampleInitEvent",
    "SampleLimitEvent",
    "SandboxEvent",
    "ScoreEvent",
    "StateEvent",
    "StepEvent",
    "StoreEvent",
    "SubtaskEvent",
    "ToolEvent",
    "convert_eval_logs",
    "list_eval_logs",
    "read_eval_log",
    "read_eval_log_async",
    "read_eval_log_sample",
    "read_eval_log_samples",
    "condense_sample",
    "resolve_sample_attachments",
    "write_eval_log",
    "write_eval_log_async",
    "write_log_dir_manifest",
    "retryable_eval_logs",
    "bundle_log_dir",
]

================
File: src/inspect_ai/log/_bundle.py
================
import logging
import math
import os
import shutil
import tempfile
from contextlib import contextmanager
from pathlib import Path
from typing import Any, Callable, Iterator
from inspect_ai._util.error import PrerequisiteError
from inspect_ai._util.file import filesystem
from ._file import log_files_from_ls, write_log_dir_manifest
# INSPECT_VIEW_BUNDLE_OUT_DIR
logger = logging.getLogger(__name__)
DIST_DIR = os.path.join(Path(__file__).parent, "..", "_view", "www", "dist")
def bundle_log_dir(
    log_dir: str | None = None,
    output_dir: str | None = None,
    overwrite: bool = False,
    fs_options: dict[str, Any] = {},
) -> None:
    r"""Bundle a log_dir into a statically deployable viewer
    Args:
        log_dir: (str | None): The log_dir to bundle
        output_dir: (str | None): The directory to place bundled output. If no directory
            is specified, the env variable `INSPECT_VIEW_BUNDLE_OUTPUT_DIR` will be used.
        overwrite: (bool): Optional. Whether to overwrite files in the output directory.
            Defaults to False.
        fs_options (dict[str, Any]): Optional. Additional arguments to pass through
            to the filesystem provider (e.g. `S3FileSystem`).
    """
    # resolve the log directory
    log_dir = log_dir if log_dir else os.getenv("INSPECT_LOG_DIR", "./logs")
    # resolve the output directory
    output_dir = (
        output_dir if output_dir else os.getenv("INSPECT_VIEW_BUNDLE_OUTPUT_DIR", "")
    )
    if output_dir == "":
        raise PrerequisiteError("You must provide an 'output_dir'")
    # ensure output_dir doesn't exist
    if filesystem(output_dir, fs_options).exists(output_dir) and not overwrite:
        raise PrerequisiteError(
            f"The output directory '{output_dir}' already exists. Choose another output directory or use 'overwrite' to overwrite the directory and contents"
        )
    from inspect_ai._display import display
    display().print(f"Creating view bundle in '{output_dir}'")
    with display().progress(total=500) as p:
        # Work in a temporary working directory
        with tempfile.TemporaryDirectory(ignore_cleanup_errors=True) as working_dir:
            # copy dist to output_dir
            copy_dir_contents(DIST_DIR, working_dir)
            p.update(25)
            # create a logs dir
            log_dir_name = "logs"
            view_logs_dir = os.path.join(working_dir, log_dir_name)
            os.makedirs(view_logs_dir)
            p.update(25)
            # Copy the logs to the log dir
            copy_log_files(log_dir, view_logs_dir, p.update, fs_options)
            # Always regenerate the manifest
            write_log_dir_manifest(view_logs_dir)
            p.update(25)
            # update the index html to embed the log_dir
            inject_configuration(
                os.path.join(working_dir, "index.html"), log_dir=log_dir_name
            )
            write_robots_txt(working_dir)
            p.update(25)
            # Now move the contents of the working directory to the output directory
            move_output(working_dir, output_dir, p.update, fs_options)
            p.complete()
    display().print(f"View bundle '{output_dir}' created")
def copy_dir_contents(source_dir: str, dest_dir: str) -> None:
    for root, _dirs, files in os.walk(source_dir):
        # Calculate the relative path from the source directory
        relative_path = os.path.relpath(root, source_dir)
        # Create the corresponding directory in the destination
        dest_path = os.path.join(dest_dir, relative_path)
        if not os.path.exists(dest_path):
            os.makedirs(dest_path)
        # Copy all files in the current directory
        for file in files:
            src_file_path = os.path.join(root, file)
            dest_file_path = os.path.join(dest_path, file)
            shutil.copy2(src_file_path, dest_file_path)
def inject_configuration(html_file: str, log_dir: str) -> None:
    # update the index html to embed the log_dir
    with open(html_file, "r") as file:
        index_contents = file.read()
    # inject the log dir information into the viewer html
    # so it will load directly
    content = index_contents.replace(
        "</head>",
        f'  <script id="log_dir_context" type="application/json">{{"log_dir": "{log_dir}"}}</script>\n  </head>',
    )
    # Open the file for writing to save the updated content
    with open(html_file, "w") as file:
        file.write(content)
def write_robots_txt(dir: str) -> None:
    # Full path to the robots.txt file
    file_path = os.path.join(dir, "robots.txt")
    # Content for the robots.txt file
    content = """User-agent: *
Disallow: /
"""
    # Write the content to the file
    with open(file_path, "w") as f:
        f.write(content)
def copy_log_files(
    log_dir: str,
    target_dir: str,
    p: Callable[[int], None],
    fs_options: dict[str, Any] = {},
) -> None:
    log_fs = filesystem(log_dir, fs_options)
    if log_fs.exists(log_dir):
        eval_logs = log_files_from_ls(
            log_fs.ls(log_dir, recursive=True), ["json", "eval"], True
        )
        if len(eval_logs) == 0:
            raise PrerequisiteError(
                f"The log directory {log_dir} doesn't contain any log files."
            )
        base_log_dir = log_fs.info(log_dir).name
        with progress_adapter(p, 200, len(eval_logs)) as tick:
            for eval_log in eval_logs:
                relative_path = os.path.relpath(eval_log.name, base_log_dir)
                output_path = os.path.join(target_dir, relative_path)
                # Make directories containing output_path if they don't exist.
                os.makedirs(os.path.dirname(output_path), exist_ok=True)
                # Copy log to output_path
                log_fs.get_file(eval_log.name, output_path)
                tick()
    else:
        raise PrerequisiteError(f"The log directory {log_dir} doesn't exist.")
def move_output(
    from_dir: str,
    to_dir: str,
    p: Callable[[int], None],
    fs_options: dict[str, Any] = {},
) -> None:
    output_fs = filesystem(to_dir, fs_options)
    # remove any existing target directory
    if output_fs.exists(to_dir):
        output_fs.rm(to_dir, recursive=True)
    # Now copy the files
    dir_contents = list(os.walk(from_dir))
    # count the title files to copy
    total_count = 0
    for root, dirs, files in dir_contents:
        total_count += len(dirs) + len(files)
    with progress_adapter(p, 200, total_count) as tick:
        for root, _dirs, files in dir_contents:
            # The relative path of the file to move
            relative_dir = os.path.relpath(root, from_dir)
            # make sure the directory exists
            dir_path = os.path.join(to_dir, relative_dir)
            if not output_fs.exists(dir_path):
                output_fs.mkdir(dir_path)
            tick()
            # Copy the files
            for working_file in files:
                target_path = (
                    os.path.join(relative_dir, working_file)
                    if relative_dir != "."
                    else working_file
                )
                src = os.path.join(root, working_file)
                dest = os.path.join(to_dir, target_path)
                output_fs.put_file(src, dest)
                tick()
@contextmanager
def progress_adapter(
    p: Callable[[int], None], units: int, total_ticks: int
) -> Iterator[Callable[[], None]]:
    # Allocate 'units' ticks to represent the progress in
    # in 'total_ticks', adjusting the size of the increments based
    # upon the total ticks
    ticks = 0.0
    increment = units / total_ticks
    def tick() -> None:
        nonlocal ticks
        ticks = ticks + increment
        tick_value = math.floor(ticks)
        if tick_value >= 1:
            # increment the count
            p(tick_value)
            # hang on to 'leftover' ticks to accumulate with the next increment
            ticks = ticks - tick_value
    yield tick

================
File: src/inspect_ai/log/_condense.py
================
from logging import getLogger
from typing import (
    Callable,
)
from pydantic import JsonValue
from inspect_ai._util.constants import BASE_64_DATA_REMOVED
from inspect_ai._util.content import (
    Content,
    ContentAudio,
    ContentImage,
    ContentReasoning,
    ContentText,
    ContentVideo,
)
from inspect_ai._util.hash import mm3_hash
from inspect_ai._util.json import JsonChange
from inspect_ai._util.url import is_data_uri
from inspect_ai.dataset._dataset import Sample
from inspect_ai.model._chat_message import ChatMessage
from inspect_ai.model._model_call import ModelCall
from inspect_ai.model._model_output import ModelOutput
from ._log import EvalSample
from ._transcript import (
    Event,
    ModelEvent,
    SampleInitEvent,
    StateEvent,
    StoreEvent,
    SubtaskEvent,
    ToolEvent,
)
logger = getLogger(__name__)
ATTACHMENT_PROTOCOL = "attachment://"
def condense_sample(sample: EvalSample, log_images: bool = True) -> EvalSample:
    """Reduce the storage size of the eval sample.
    Reduce size by:
    1. De-duplciating larger content fields (especially important for images
       but also for message repeated over and over in the event stream)
    2. Removing base64 encoded images if log_images is True
    The de-duplication of content fields can be reversed by calling
    `resolve_attachments()`. Removal of base64 encoded images is a
    one-way operation.
    Args:
       sample (EvalSample): Eval sample to condense.
       log_images (bool): Should base64 images be logged for this sample.
    Returns:
       EvalSample: Eval sample in condensed form.
    """
    # de-duplicate large content fields as 'attachments'
    attachments: dict[str, str] = {}
    def create_attachment(text: str) -> str:
        hash = mm3_hash(text)
        attachments[hash] = text
        return f"{ATTACHMENT_PROTOCOL}{hash}"
    # for events, we want to strip images when requested and
    # create attachments for text > 100
    def events_fn(text: str) -> str:
        if not log_images and is_data_uri(text):
            return BASE_64_DATA_REMOVED
        elif len(text) > 100:
            return create_attachment(text)
        else:
            return text
    # for messages, we only want to handle images (either stripping
    # them or turning them into attachments as required)
    def messages_fn(text: str) -> str:
        if is_data_uri(text):
            if log_images:
                return create_attachment(text)
            else:
                return BASE_64_DATA_REMOVED
        else:
            return text
    return sample.model_copy(
        update={
            "input": walk_input(sample.input, messages_fn),
            "messages": walk_chat_messages(sample.messages, messages_fn),
            "events": walk_events(sample.events, events_fn),
            "attachments": attachments,
        }
    )
def resolve_sample_attachments(sample: EvalSample) -> EvalSample:
    """Resolve content attachments (typically images) in sample.
    Take 'attachment://*` references and resolve them to their
    underlying content, then remove the 'attachments' field.
    Args:
       sample (EvalSample): Eval sample with attachments.
    Returns:
       EvalSample: Sample with attachment content resolved.
    """
    def content_fn(text: str) -> str:
        # migrate previous flavor of content reference
        CONTENT_PROTOCOL = "tc://"
        if text.startswith(CONTENT_PROTOCOL):
            text = text.replace(CONTENT_PROTOCOL, ATTACHMENT_PROTOCOL, 1)
        # resovle attachment
        if text.startswith(ATTACHMENT_PROTOCOL):
            return sample.attachments.get(
                text.replace(ATTACHMENT_PROTOCOL, "", 1), text
            )
        else:
            return text
    return sample.model_copy(
        update={
            "input": walk_input(sample.input, content_fn),
            "messages": walk_chat_messages(sample.messages, content_fn),
            "events": walk_events(sample.events, content_fn),
            "attachments": {},
        }
    )
def walk_events(events: list[Event], content_fn: Callable[[str], str]) -> list[Event]:
    return [walk_event(event, content_fn) for event in events]
def walk_event(event: Event, content_fn: Callable[[str], str]) -> Event:
    if isinstance(event, SampleInitEvent):
        return walk_sample_init_event(event, content_fn)
    elif isinstance(event, ModelEvent):
        return walk_model_event(event, content_fn)
    elif isinstance(event, StateEvent):
        return walk_state_event(event, content_fn)
    elif isinstance(event, StoreEvent):
        return walk_store_event(event, content_fn)
    elif isinstance(event, SubtaskEvent):
        return walk_subtask_event(event, content_fn)
    elif isinstance(event, ToolEvent):
        return walk_tool_event(event, content_fn)
    else:
        return event
def walk_subtask_event(
    event: SubtaskEvent, content_fn: Callable[[str], str]
) -> SubtaskEvent:
    return event.model_copy(update=dict(events=walk_events(event.events, content_fn)))
def walk_tool_event(event: ToolEvent, content_fn: Callable[[str], str]) -> ToolEvent:
    return event.model_copy(update=dict(events=walk_events(event.events, content_fn)))
def walk_sample_init_event(
    event: SampleInitEvent, content_fn: Callable[[str], str]
) -> SampleInitEvent:
    return event.model_copy(
        update=dict(
            sample=walk_sample(event.sample, content_fn),
            state=walk_json_value(event.state, content_fn),
        )
    )
def walk_sample(sample: Sample, content_fn: Callable[[str], str]) -> Sample:
    if isinstance(sample.input, str):
        return sample.model_copy(
            update=dict(input=walk_json_value(sample.input, content_fn))
        )
    else:
        return sample.model_copy(
            update=dict(input=walk_chat_messages(sample.input, content_fn))
        )
def walk_model_event(event: ModelEvent, content_fn: Callable[[str], str]) -> ModelEvent:
    return event.model_copy(
        update=dict(
            input=walk_chat_messages(event.input, content_fn),
            output=walk_model_output(event.output, content_fn),
            call=walk_model_call(event.call, content_fn),
        ),
    )
def walk_model_output(
    output: ModelOutput, content_fn: Callable[[str], str]
) -> ModelOutput:
    return output.model_copy(
        update=dict(
            choices=[
                choice.model_copy(
                    update=dict(message=walk_chat_message(choice.message, content_fn))
                )
                for choice in output.choices
            ]
        )
    )
def walk_model_call(
    call: ModelCall | None, content_fn: Callable[[str], str]
) -> ModelCall | None:
    if call:
        return ModelCall(
            request=walk_json_dict(call.request, content_fn),
            response=walk_json_dict(call.response, content_fn),
            time=call.time,
        )
    else:
        return None
def walk_state_event(event: StateEvent, content_fn: Callable[[str], str]) -> StateEvent:
    event = event.model_copy(
        update=dict(
            changes=[
                walk_state_json_change(change, content_fn) for change in event.changes
            ]
        )
    )
    return event
def walk_store_event(event: StoreEvent, content_fn: Callable[[str], str]) -> StoreEvent:
    event = event.model_copy(
        update=dict(
            changes=[
                walk_state_json_change(change, content_fn) for change in event.changes
            ]
        )
    )
    return event
def walk_state_json_change(
    change: JsonChange, content_fn: Callable[[str], str]
) -> JsonChange:
    return change.model_copy(
        update=dict(value=walk_json_value(change.value, content_fn))
    )
def walk_json_value(value: JsonValue, content_fn: Callable[[str], str]) -> JsonValue:
    if isinstance(value, str):
        return content_fn(value)
    elif isinstance(value, list):
        return [walk_json_value(v, content_fn) for v in value]
    elif isinstance(value, dict):
        return walk_json_dict(value, content_fn)
    else:
        return value
def walk_json_dict(
    value: dict[str, JsonValue], content_fn: Callable[[str], str]
) -> dict[str, JsonValue]:
    updates: dict[str, JsonValue] = {}
    for k, v in value.items():
        updates[k] = walk_json_value(v, content_fn)
    if updates:
        value = value.copy()
        value.update(updates)
    return value
def walk_input(
    input: str | list[ChatMessage], content_fn: Callable[[str], str]
) -> str | list[ChatMessage]:
    if isinstance(input, str):
        return input
    else:
        return walk_chat_messages(input, content_fn)
def walk_chat_messages(
    messages: list[ChatMessage], content_fn: Callable[[str], str]
) -> list[ChatMessage]:
    return [walk_chat_message(message, content_fn) for message in messages]
def walk_chat_message(
    message: ChatMessage, content_fn: Callable[[str], str]
) -> ChatMessage:
    if isinstance(message.content, str):
        return message.model_copy(update=dict(content=content_fn(message.content)))
    else:
        return message.model_copy(
            update=dict(
                content=[
                    walk_content(content, content_fn) for content in message.content
                ]
            )
        )
def walk_content(content: Content, content_fn: Callable[[str], str]) -> Content:
    if isinstance(content, ContentText):
        return content.model_copy(update=dict(text=content_fn(content.text)))
    elif isinstance(content, ContentImage):
        return content.model_copy(update=dict(image=content_fn(content.image)))
    elif isinstance(content, ContentAudio):
        return content.model_copy(update=dict(audio=content_fn(content.audio)))
    elif isinstance(content, ContentVideo):
        return content.model_copy(update=dict(video=content_fn(content.video)))
    elif isinstance(content, ContentReasoning):
        return content.model_copy(update=dict(reasoning=content_fn(content.reasoning)))

================
File: src/inspect_ai/log/_convert.py
================
import os
from typing import Literal
from inspect_ai._util.error import PrerequisiteError
from inspect_ai._util.file import copy_file, exists, filesystem
from inspect_ai.log._file import (
    log_files_from_ls,
    read_eval_log,
    write_eval_log,
)
def convert_eval_logs(
    path: str, to: Literal["eval", "json"], output_dir: str, overwrite: bool = False
) -> None:
    """Convert between log file formats.
    Convert log file(s) to a target format. If a file is already in the target
    format it will just be copied to the output dir.
    Args:
        path (str): Path to source log file(s). Should be either a single
            log file or a directory containing log files.
        to (Literal["eval", "json"]): Format to convert to. If a file is
            already in the target format it will just be copied to the output dir.
        output_dir (str): Output directory to write converted log file(s) to.
        overwrite (bool): Overwrite existing log files (defaults to `False`,
            raising an error if the output file path already exists).
    """
    from inspect_ai._display import display
    # confirm that path exists
    fs = filesystem(path)
    if not fs.exists(path):
        raise PrerequisiteError(f"Error: path '{path}' does not exist.")
    # normalise output dir and ensure it exists
    if output_dir.endswith(fs.sep):
        output_dir = output_dir[:-1]
    fs.mkdir(output_dir, exist_ok=True)
    # convert a single file (input file is relative to the 'path')
    def convert_file(input_file: str) -> None:
        # compute input and ensure output dir exists
        input_name, _ = os.path.splitext(input_file)
        input_dir = os.path.dirname(input_name.replace("\\", "/"))
        # Compute paths, handling directories being converted
        # and files being converted specially
        path_is_dir = fs.info(path).type == "directory"
        if path_is_dir:
            target_dir = f"{output_dir}{fs.sep}{input_dir}"
            input_file = f"{path}{fs.sep}{input_file}"
            output_file_basename = input_name
        else:
            target_dir = output_dir
            output_file_basename = os.path.basename(input_name)
        output_fs = filesystem(target_dir)
        output_fs.mkdir(target_dir, exist_ok=True)
        # compute full output file and enforce overwrite
        output_file = f"{output_dir}{fs.sep}{output_file_basename}.{to}"
        if exists(output_file) and not overwrite:
            raise FileExistsError(
                "Output file {output_file} already exists (use --overwrite to overwrite existing files)"
            )
        # if the input and output files have the same format just copy
        if input_file.endswith(f".{to}"):
            copy_file(input_file, output_file)
        # otherwise do a full read/write
        else:
            log = read_eval_log(input_file)
            write_eval_log(log, output_file)
    if fs.info(path).type == "file":
        convert_file(path)
    else:
        root_dir = fs.info(path).name
        eval_logs = log_files_from_ls(fs.ls(path, recursive=True), None, True)
        input_files = [
            eval_log.name.replace(f"{root_dir}/", "", 1) for eval_log in eval_logs
        ]
        display().print("Converting log files...")
        with display().progress(total=len(input_files)) as p:
            for input_file in input_files:
                convert_file(input_file)
                p.update()

================
File: src/inspect_ai/log/_file.py
================
import os
import re
from logging import getLogger
from typing import Any, Callable, Generator, Literal
from pydantic import BaseModel
from pydantic_core import to_json
from inspect_ai._util._async import current_async_backend, run_coroutine
from inspect_ai._util.constants import ALL_LOG_FORMATS, EVAL_LOG_FORMAT
from inspect_ai._util.file import (
    FileInfo,
    file,
    filesystem,
)
from inspect_ai._util.json import jsonable_python
from inspect_ai.log._condense import resolve_sample_attachments
from ._log import EvalLog, EvalSample
from ._recorders import recorder_type_for_format, recorder_type_for_location
logger = getLogger(__name__)
class EvalLogInfo(BaseModel):
    """File info and task identifiers for eval log."""
    name: str
    """Name of file."""
    type: str
    """Type of file (file or directory)"""
    size: int
    """File size in bytes."""
    mtime: float | None
    """File modification time (None if the file is a directory on S3)."""
    task: str
    """Task name."""
    task_id: str
    """Task id."""
    suffix: str | None
    """Log file suffix (e.g. "-scored")"""
def list_eval_logs(
    log_dir: str = os.environ.get("INSPECT_LOG_DIR", "./logs"),
    formats: list[Literal["eval", "json"]] | None = None,
    filter: Callable[[EvalLog], bool] | None = None,
    recursive: bool = True,
    descending: bool = True,
    fs_options: dict[str, Any] = {},
) -> list[EvalLogInfo]:
    """List all eval logs in a directory.
    Args:
      log_dir (str): Log directory (defaults to INSPECT_LOG_DIR)
      formats (Literal["eval", "json"]): Formats to list (default
        to listing all formats)
      filter (Callable[[EvalLog], bool]): Filter to limit logs returned.
         Note that the EvalLog instance passed to the filter has only
         the EvalLog header (i.e. does not have the samples or logging output).
      recursive (bool): List log files recursively (defaults to True).
      descending (bool): List in descending order.
      fs_options (dict[str, Any]): Optional. Additional arguments to pass through
          to the filesystem provider (e.g. `S3FileSystem`).
    Returns:
       List of EvalLog Info.
    """
    # get the eval logs
    fs = filesystem(log_dir, fs_options)
    if fs.exists(log_dir):
        logger.debug(f"Listing eval logs for {log_dir}")
        eval_logs = log_files_from_ls(
            fs.ls(log_dir, recursive=recursive), formats, descending
        )
        logger.debug(f"Listing eval logs for {log_dir} completed")
    else:
        return []
    # apply filter if requested
    if filter:
        return [
            log
            for log in eval_logs
            if filter(read_eval_log(log.name, header_only=True))
        ]
    else:
        return eval_logs
def write_eval_log(
    log: EvalLog,
    location: str | FileInfo | None = None,
    format: Literal["eval", "json", "auto"] = "auto",
) -> None:
    """Write an evaluation log.
    Args:
       log (EvalLog): Evaluation log to write.
       location (str | FileInfo): Location to write log to.
       format (Literal["eval", "json", "auto"]): Write to format
          (defaults to 'auto' based on `log_file` extension)
    """
    # don't mix trio and asyncio
    if current_async_backend() == "trio":
        raise RuntimeError(
            "write_eval_log cannot be called from a trio async context (please use write_eval_log_async instead)"
        )
    # will use s3fs and is not called from main inspect solver/scorer/tool/sandbox
    # flow, so force the use of asyncio
    run_coroutine(write_eval_log_async(log, location, format))
async def write_eval_log_async(
    log: EvalLog,
    location: str | FileInfo | None = None,
    format: Literal["eval", "json", "auto"] = "auto",
) -> None:
    """Write an evaluation log.
    Args:
       log (EvalLog): Evaluation log to write.
       location (str | FileInfo): Location to write log to.
       format (Literal["eval", "json", "auto"]): Write to format
          (defaults to 'auto' based on `log_file` extension)
    """
    # resolve location
    if location is None:
        if log.location:
            location = log.location
        else:
            raise ValueError(
                "EvalLog passe to write_eval_log does not have a location, so you must pass an explicit location"
            )
    location = location if isinstance(location, str) else location.name
    logger.debug(f"Writing eval log to {location}")
    # get recorder type
    if format == "auto":
        recorder_type = recorder_type_for_location(location)
    else:
        recorder_type = recorder_type_for_format(format)
    await recorder_type.write_log(location, log)
    logger.debug(f"Writing eval log to {location} completed")
def write_log_dir_manifest(
    log_dir: str,
    *,
    filename: str = "logs.json",
    output_dir: str | None = None,
    fs_options: dict[str, Any] = {},
) -> None:
    """Write a manifest for a log directory.
    A log directory manifest is a dictionary of EvalLog headers (EvalLog w/o samples)
    keyed by log file names (names are relative to the log directory)
    Args:
      log_dir (str): Log directory to write manifest for.
      filename (str): Manifest filename (defaults to "logs.json")
      output_dir (str | None): Output directory for manifest (defaults to log_dir)
      fs_options (dict[str,Any]): Optional. Additional arguments to pass through
        to the filesystem provider (e.g. `S3FileSystem`).
    """
    # resolve log dir to full path
    fs = filesystem(log_dir)
    log_dir = fs.info(log_dir).name
    # list eval logs
    logs = list_eval_logs(log_dir)
    # resolve to manifest (make filenames relative to the log dir)
    names = [manifest_eval_log_name(log, log_dir, fs.sep) for log in logs]
    headers = read_eval_log_headers(logs)
    manifest_logs = dict(zip(names, headers))
    # form target path and write
    output_dir = output_dir or log_dir
    fs = filesystem(output_dir)
    manifest = f"{output_dir}{fs.sep}{filename}"
    manifest_json = to_json(
        value=manifest_logs, indent=2, exclude_none=True, fallback=lambda _x: None
    )
    with file(manifest, mode="wb", fs_options=fs_options) as f:
        f.write(manifest_json)
def read_eval_log(
    log_file: str | EvalLogInfo,
    header_only: bool = False,
    resolve_attachments: bool = False,
    format: Literal["eval", "json", "auto"] = "auto",
) -> EvalLog:
    """Read an evaluation log.
    Args:
       log_file (str | FileInfo): Log file to read.
       header_only (bool): Read only the header (i.e. exclude
          the "samples" and "logging" fields). Defaults to False.
       resolve_attachments (bool): Resolve attachments (e.g. images)
          to their full content.
       format (Literal["eval", "json", "auto"]): Read from format
          (defaults to 'auto' based on `log_file` extension)
    Returns:
       EvalLog object read from file.
    """
    # don't mix trio and asyncio
    if current_async_backend() == "trio":
        raise RuntimeError(
            "read_eval_log cannot be called from a trio async context (please use read_eval_log_async instead)"
        )
    # will use s3fs and is not called from main inspect solver/scorer/tool/sandbox
    # flow, so force the use of asyncio
    return run_coroutine(
        read_eval_log_async(
            log_file,
            header_only,
            resolve_attachments,
            format,
        )
    )
async def read_eval_log_async(
    log_file: str | EvalLogInfo,
    header_only: bool = False,
    resolve_attachments: bool = False,
    format: Literal["eval", "json", "auto"] = "auto",
) -> EvalLog:
    """Read an evaluation log.
    Args:
       log_file (str | FileInfo): Log file to read.
       header_only (bool): Read only the header (i.e. exclude
          the "samples" and "logging" fields). Defaults to False.
       resolve_attachments (bool): Resolve attachments (e.g. images)
          to their full content.
       format (Literal["eval", "json", "auto"]): Read from format
          (defaults to 'auto' based on `log_file` extension)
    Returns:
       EvalLog object read from file.
    """
    # resolve to file path
    log_file = log_file if isinstance(log_file, str) else log_file.name
    logger.debug(f"Reading eval log from {log_file}")
    # get recorder type
    if format == "auto":
        recorder_type = recorder_type_for_location(log_file)
    else:
        recorder_type = recorder_type_for_format(format)
    log = await recorder_type.read_log(log_file, header_only)
    # resolve attachement if requested
    if resolve_attachments and log.samples:
        log.samples = [resolve_sample_attachments(sample) for sample in log.samples]
    # provide sample ids if they aren't there
    if log.eval.dataset.sample_ids is None and log.samples is not None:
        sample_ids: dict[str | int, None] = {}
        for sample in log.samples:
            if sample.id not in sample_ids:
                sample_ids[sample.id] = None
        log.eval.dataset.sample_ids = list(sample_ids.keys())
    logger.debug(f"Completed reading eval log from {log_file}")
    return log
def read_eval_log_headers(
    log_files: list[str] | list[EvalLogInfo],
) -> list[EvalLog]:
    # will use s3fs and is not called from main inspect solver/scorer/tool/sandbox
    # flow, so force the use of asyncio
    return run_coroutine(read_eval_log_headers_async(log_files))
async def read_eval_log_headers_async(
    log_files: list[str] | list[EvalLogInfo],
) -> list[EvalLog]:
    return [
        await read_eval_log_async(log_file, header_only=True) for log_file in log_files
    ]
def read_eval_log_sample(
    log_file: str | EvalLogInfo,
    id: int | str,
    epoch: int = 1,
    resolve_attachments: bool = False,
    format: Literal["eval", "json", "auto"] = "auto",
) -> EvalSample:
    """Read a sample from an evaluation log.
    Args:
       log_file (str | FileInfo): Log file to read.
       id (int | str): Sample id to read.
       epoch (int): Epoch for sample id (defaults to 1)
       resolve_attachments (bool): Resolve attachments (e.g. images)
          to their full content.
       format (Literal["eval", "json", "auto"]): Read from format
          (defaults to 'auto' based on `log_file` extension)
    Returns:
       EvalSample object read from file.
    Raises:
       IndexError: If the passed id and epoch are not found.
    """
    # don't mix trio and asyncio
    if current_async_backend() == "trio":
        raise RuntimeError(
            "read_eval_log_sample cannot be called from a trio async context (please use read_eval_log_sample_async instead)"
        )
    # will use s3fs and is not called from main inspect solver/scorer/tool/sandbox
    # flow, so force the use of asyncio
    return run_coroutine(
        read_eval_log_sample_async(log_file, id, epoch, resolve_attachments, format)
    )
async def read_eval_log_sample_async(
    log_file: str | EvalLogInfo,
    id: int | str,
    epoch: int = 1,
    resolve_attachments: bool = False,
    format: Literal["eval", "json", "auto"] = "auto",
) -> EvalSample:
    """Read a sample from an evaluation log.
    Args:
       log_file (str | FileInfo): Log file to read.
       id (int | str): Sample id to read.
       epoch (int): Epoch for sample id (defaults to 1)
       resolve_attachments (bool): Resolve attachments (e.g. images)
          to their full content.
       format (Literal["eval", "json", "auto"]): Read from format
          (defaults to 'auto' based on `log_file` extension)
    Returns:
       EvalSample object read from file.
    Raises:
       IndexError: If the passed id and epoch are not found.
    """
    # resolve to file path
    log_file = log_file if isinstance(log_file, str) else log_file.name
    if format == "auto":
        recorder_type = recorder_type_for_location(log_file)
    else:
        recorder_type = recorder_type_for_format(format)
    sample = await recorder_type.read_log_sample(log_file, id, epoch)
    if resolve_attachments:
        sample = resolve_sample_attachments(sample)
    return sample
def read_eval_log_samples(
    log_file: str | EvalLogInfo,
    all_samples_required: bool = True,
    resolve_attachments: bool = False,
    format: Literal["eval", "json", "auto"] = "auto",
) -> Generator[EvalSample, None, None]:
    """Read all samples from an evaluation log incrementally.
    Generator for samples in a log file. Only one sample at a time
    will be read into memory and yielded to the caller.
    Args:
       log_file (str | FileInfo): Log file to read.
       all_samples_required (bool): All samples must be included in
          the file or an IndexError is thrown.
       resolve_attachments (bool): Resolve attachments (e.g. images)
          to their full content.
       format (Literal["eval", "json", "auto"]): Read from format
          (defaults to 'auto' based on `log_file` extension)
    Returns:
       Generator of EvalSample objects in the log file.
    Raises:
       IndexError: If `all_samples_required` is `True` and one of the target
          samples does not exist in the log file.
    """
    # read header
    log_header = read_eval_log(log_file, header_only=True, format=format)
    # do we have the list of samples?
    if log_header.eval.dataset.sample_ids is None:
        raise RuntimeError(
            "This log file does not include sample_ids "
            + "(fully reading and re-writing the log will add sample_ids)"
        )
    # if the status is not success and all_samples_required, this is an error
    if log_header.status != "success" and all_samples_required:
        raise RuntimeError(
            f"This log does not have all samples (status={log_header.status}). "
            + "Specify all_samples_required=False to read the samples that exist."
        )
    # loop over samples and epochs
    for sample_id in log_header.eval.dataset.sample_ids:
        for epoch_id in range(1, (log_header.eval.config.epochs or 1) + 1):
            try:
                sample = read_eval_log_sample(
                    log_file=log_file,
                    id=sample_id,
                    epoch=epoch_id,
                    resolve_attachments=resolve_attachments,
                    format=format,
                )
                yield sample
            except IndexError:
                if all_samples_required:
                    raise
def manifest_eval_log_name(info: EvalLogInfo, log_dir: str, sep: str) -> str:
    # ensure that log dir has a trailing seperator
    if not log_dir.endswith(sep):
        log_dir = f"{log_dir}/"
    # slice off log_dir from the front
    log = info.name.replace(log_dir, "")
    # manifests are web artifacts so always use forward slash
    return log.replace("\\", "/")
def log_files_from_ls(
    ls: list[FileInfo],
    formats: list[Literal["eval", "json"]] | None,
    descending: bool = True,
) -> list[EvalLogInfo]:
    extensions = [f".{format}" for format in (formats or ALL_LOG_FORMATS)]
    return [
        log_file_info(file)
        for file in sorted(
            ls, key=lambda file: (file.mtime if file.mtime else 0), reverse=descending
        )
        if file.type == "file" and is_log_file(file.name, extensions)
    ]
log_file_pattern = r"^\d{4}-\d{2}-\d{2}T\d{2}[:-]\d{2}[:-]\d{2}.*$"
def is_log_file(file: str, extensions: list[str]) -> bool:
    parts = file.replace("\\", "/").split("/")
    name = parts[-1]
    if name.endswith(f".{EVAL_LOG_FORMAT}"):
        return True
    else:
        return re.match(log_file_pattern, name) is not None and any(
            [name.endswith(suffix) for suffix in extensions]
        )
def log_file_info(info: FileInfo) -> "EvalLogInfo":
    # extract the basename and split into parts
    # (deal with previous logs had the model in their name)
    basename = os.path.splitext(info.name)[0]
    parts = basename.split("/").pop().split("_")
    if len(parts) == 1:
        task = ""
        task_id = ""
        suffix = None
    elif len(parts) == 2:
        task = parts[1]
        task_id = ""
        suffix = None
    else:
        last_idx = 3 if len(parts) > 3 else 2
        task = parts[1]
        part3 = parts[last_idx].split("-")
        task_id = part3[0]
        suffix = task_id[2] if len(part3) > 1 else None
    return EvalLogInfo(
        name=info.name,
        type=info.type,
        size=info.size,
        mtime=info.mtime,
        task=task,
        task_id=task_id,
        suffix=suffix,
    )
def eval_log_json(log: EvalLog) -> bytes:
    # serialize to json (ignore values that are unserializable)
    # these values often result from solvers using metadata to
    # pass around 'live' objects -- this is fine to do and we
    # don't want to prevent it at the serialization level
    return to_json(
        value=jsonable_python(log),
        indent=2,
        exclude_none=True,
        fallback=lambda _x: None,
    )
def eval_log_json_str(log: EvalLog) -> str:
    return eval_log_json(log).decode()

================
File: src/inspect_ai/log/_log.py
================
import asyncio
import os
import sys
import traceback
from logging import getLogger
from types import TracebackType
from typing import Any, Literal, Tuple, Type, TypedDict
import click
import tenacity
from pydantic import (
    BaseModel,
    ConfigDict,
    Field,
    PrivateAttr,
    model_validator,
)
from rich.console import Console, RenderableType
from rich.traceback import Traceback
from inspect_ai._util.constants import CONSOLE_DISPLAY_WIDTH, PKG_NAME
from inspect_ai._util.error import EvalError, exception_message
from inspect_ai._util.logger import warn_once
from inspect_ai.approval._policy import ApprovalPolicyConfig
from inspect_ai.dataset._dataset import MT, metadata_as
from inspect_ai.model import ChatMessage, GenerateConfig, ModelOutput, ModelUsage
from inspect_ai.scorer import Score
from inspect_ai.util._sandbox.environment import SandboxEnvironmentSpec
from inspect_ai.util._store import Store
from inspect_ai.util._store_model import SMT
from ._transcript import Event
logger = getLogger(__name__)
SCORER_PLACEHOLDER = "88F74D2C"
class EvalConfigDefaults(TypedDict):
    epochs: int
    epochs_reducer: list[str]
    fail_on_error: bool
    sandbox_cleanup: bool
    log_samples: bool
    log_images: bool
    score_display: bool
def eval_config_defaults() -> EvalConfigDefaults:
    return {
        "epochs": 1,
        "epochs_reducer": ["mean"],
        "fail_on_error": True,
        "sandbox_cleanup": True,
        "log_samples": True,
        "log_images": True,
        "score_display": True,
    }
class EvalConfig(BaseModel):
    """Configuration used for evaluation."""
    limit: int | tuple[int, int] | None = Field(default=None)
    """Sample limit (number of samples or range of samples)."""
    sample_id: str | int | list[str | int] | None = Field(default=None)
    """Evaluate specific sample(s)."""
    epochs: int | None = Field(default=None)
    """Number of epochs to run samples over."""
    epochs_reducer: list[str] | None = Field(default=None)
    """Reducers for aggregating per-sample scores."""
    approval: ApprovalPolicyConfig | None = Field(default=None)
    """Approval policy for tool use."""
    fail_on_error: bool | float | None = Field(default=None)
    """Fail eval when sample errors occur.
    `True` to fail on first sample error (default); `False` to never
    fail on sample errors; Value between 0 and 1 to fail if a proportion
    of total samples fails. Value greater than 1 to fail eval if a count
    of samples fails.
    """
    message_limit: int | None = Field(default=None)
    """Maximum messages to allow per sample."""
    token_limit: int | None = Field(default=None)
    """Maximum tokens usage per sample."""
    time_limit: int | None = Field(default=None)
    """Maximum clock time per sample."""
    working_limit: int | None = Field(default=None)
    """Meximum working time per sample."""
    max_samples: int | None = Field(default=None)
    """Maximum number of samples to run in parallel."""
    max_tasks: int | None = Field(default=None)
    """Maximum number of tasks to run in parallel."""
    max_subprocesses: int | None = Field(default=None)
    """Maximum number of subprocesses to run concurrently."""
    max_sandboxes: int | None = Field(default=None)
    """Maximum number of sandboxes to run concurrently."""
    sandbox_cleanup: bool | None = Field(default=None)
    """Cleanup sandbox environments after task completes."""
    log_samples: bool | None = Field(default=None)
    """Log detailed information on each sample."""
    log_images: bool | None = Field(default=None)
    """Log base64 encoded versions of images."""
    log_buffer: int | None = Field(default=None)
    """Number of samples to buffer before writing log file."""
    score_display: bool | None = Field(default=None)
    """Display scoring metrics realtime."""
    @property
    def max_messages(self) -> int | None:
        """Deprecated max_messages property."""
        return self.message_limit
    @model_validator(mode="before")
    @classmethod
    def convert_max_messages_to_message_limit(
        cls: Type["EvalConfig"], values: dict[str, Any]
    ) -> dict[str, Any]:
        """Migrate deprecated max_messages property."""
        max_messages = values.get("max_messages", None)
        if max_messages:
            values["message_limit"] = max_messages
        return values
class EvalSampleLimit(BaseModel):
    """Limit encontered by sample."""
    type: Literal[
        "context", "time", "working", "message", "token", "operator", "custom"
    ]
    """The type of limit"""
    limit: int
    """The limit value"""
class EvalSample(BaseModel):
    """Sample from evaluation task."""
    id: int | str
    """Unique id for sample."""
    epoch: int
    """Epoch number for sample."""
    input: str | list[ChatMessage]
    """Sample input."""
    choices: list[str] | None = Field(default=None)
    """Sample choices."""
    target: str | list[str]
    """Sample target value(s)"""
    sandbox: SandboxEnvironmentSpec | None = Field(default=None)
    """Sandbox environment type and optional config file."""
    files: list[str] | None = Field(default=None)
    """Files that go along with the sample (copied to SandboxEnvironment)"""
    setup: str | None = Field(default=None)
    """Setup script to run for sample (run within default SandboxEnvironment)."""
    messages: list[ChatMessage]
    """Chat conversation history for sample."""
    output: ModelOutput
    """Model output from sample."""
    scores: dict[str, Score] | None = Field(default=None)
    """Scores for sample."""
    metadata: dict[str, Any]
    """Additional sample metadata."""
    def metadata_as(self, metadata_cls: Type[MT]) -> MT:
        """Pydantic model interface to metadata.
        Args:
          metadata_cls: Pydantic model type
        Returns:
          BaseModel: Instance of metadata_cls bound to sample metadata.
        """
        return metadata_as(self.metadata, metadata_cls)
    store: dict[str, Any] = Field(default_factory=dict)
    """State at end of sample execution."""
    def store_as(self, model_cls: Type[SMT]) -> SMT:
        """Pydantic model interface to the store.
        Args:
          model_cls: Pydantic model type (must derive from StoreModel)
        Returns:
          StoreModel: Instance of model_cls bound to sample store data.
        """
        # un-namespace names for creation
        data = {
            k.replace(f"{model_cls.__name__}:", "", 1): v for k, v in self.store.items()
        }
        # since we are reading from the log provide a fully detached store
        data["store"] = Store()
        # create the model
        return model_cls.model_validate(data)
    events: list[Event] = Field(default_factory=list)
    """Events that occurred during sample execution."""
    model_usage: dict[str, ModelUsage] = Field(default_factory=dict)
    """Model token usage for sample."""
    total_time: float | None = Field(default=None)
    """Total time that the sample was running."""
    working_time: float | None = Field(default=None)
    """Time spent working (model generation, sandbox calls, etc.)"""
    uuid: str | None = Field(default=None)
    """Globally unique identifier for sample run (exists for samples created in Inspect >= 0.3.70)"""
    error: EvalError | None = Field(default=None)
    """Error that halted sample."""
    attachments: dict[str, str] = Field(default_factory=dict)
    """Attachments referenced from messages and events.
    Resolve attachments for a sample (replacing attachment://* references with
    attachment content) by passing `resolve_attachments=True` to log reading functions.
    """
    limit: EvalSampleLimit | None = Field(default=None)
    """The limit that halted the sample"""
    # deprecated properties
    @property
    def score(self) -> Score | None:
        """Score for sample (deprecated)."""
        warn_once(
            logger,
            "The 'score' field is deprecated. Access sample scores through 'scores' instead.",
        )
        return list(self.scores.values())[0] if self.scores else None
    @property
    def transcript(self) -> "EvalEvents":
        """Transcript of sample events (deprecated)."""
        warn_once(
            logger,
            "EvalSample 'transcript' field is deprecated. Please use 'events' and 'attachments' fields instead.",
        )
        return EvalEvents(events=self.events, content=self.attachments)
    @model_validator(mode="before")
    @classmethod
    def migrate_deprecated(
        cls: Type["EvalSample"], values: dict[str, Any]
    ) -> dict[str, Any]:
        if "score" in values:
            # There cannot be a scorers property too
            if "scores" in values:
                raise TypeError(
                    "Unexpected value `scores` present when `score` has already been specified."
                )
            # Convert the scorer to the new schema
            score = values["score"]
            values["scores"] = {SCORER_PLACEHOLDER: score}
            # Get rid of the 'scorer' property
            del values["score"]
        if "transcript" in values:
            # promote 'transcript' up to 'events' and 'attachments'
            eval_events = EvalEvents(**values["transcript"])
            values["events"] = eval_events.events
            values["attachments"] = eval_events.content
            # get rid of transcript (property accessor w/ deprecation
            # warning will handle this)
            del values["transcript"]
        return migrate_sandbox_spec(values)
    # allow field model_usage
    model_config = ConfigDict(protected_namespaces=())
class EvalEvents(BaseModel):
    events: list[Event] = Field(default_factory=list)
    """List of events."""
    content: dict[str, str] = Field(default_factory=dict)
    """Content references."""
class EvalPlanStep(BaseModel):
    """Solver step."""
    solver: str
    """Name of solver."""
    params: dict[str, Any] = Field(default_factory=dict)
    """Parameters used to instantiate solver."""
class EvalPlan(BaseModel):
    """Plan (solvers) used in evaluation."""
    name: str = Field(default="plan")
    """Plan name."""
    steps: list[EvalPlanStep] = Field(default=[])
    """Steps in plan."""
    finish: EvalPlanStep | None = Field(default=None)
    """Step to always run at the end."""
    config: GenerateConfig = Field(default=GenerateConfig())
    """Generation config."""
class EvalMetric(BaseModel):
    """Metric for evaluation score."""
    name: str
    """Metric name."""
    value: int | float
    """Metric value."""
    params: dict[str, Any] = Field(default_factory=dict)
    """Params specified when creating metric."""
    metadata: dict[str, Any] | None = Field(default=None)
    """Additional metadata associated with metric."""
class EvalScore(BaseModel):
    """Score for evaluation task."""
    name: str
    """Score name."""
    scorer: str
    """Scorer name."""
    reducer: str | None = Field(default=None)
    """Reducer name."""
    params: dict[str, Any] = Field(default_factory=dict)
    """Parameters specified when creating scorer."""
    metrics: dict[str, EvalMetric] = Field(default_factory=dict)
    """Metrics computed for this scorer."""
    metadata: dict[str, Any] | None = Field(default=None)
    """Additional scorer metadata."""
class EvalSampleScore(Score):
    """Score and sample_id scored."""
    sample_id: str | int | None = Field(default=None)
    """Sample ID."""
class EvalSampleReductions(BaseModel):
    """Score reductions."""
    scorer: str
    """Name the of scorer"""
    reducer: str | None = Field(default=None)
    """Name the of reducer"""
    samples: list[EvalSampleScore]
    """List of reduced scores"""
class EvalResults(BaseModel):
    """Scoring results from evaluation."""
    total_samples: int = Field(default=0)
    """Total samples in eval (dataset samples * epochs)"""
    completed_samples: int = Field(default=0)
    """Samples completed without error.
    Will be equal to total_samples except when --fail-on-error is enabled.
    """
    @property
    def scorer(self) -> EvalScore | None:
        """Scorer used to compute results (deprecated)."""
        warn_once(
            logger,
            "The 'scorer' field is deprecated. Use 'scores' instead.",
        )
        return self.scores[0] if self.scores else None
    @property
    def metrics(self) -> dict[str, EvalMetric]:
        """Metrics computed (deprecated)."""
        warn_once(
            logger,
            "The 'metrics' field is deprecated. Access metrics through 'scores' instead.",
        )
        return self.scores[0].metrics if self.scores else {}
    scores: list[EvalScore] = Field(default=[])
    """Scorers used to compute results"""
    metadata: dict[str, Any] | None = Field(default=None)
    """Additional results metadata."""
    _sample_reductions: list[EvalSampleReductions] | None = PrivateAttr(default=None)
    """Private member to hold sample reductions"""
    @property
    def sample_reductions(self) -> list[EvalSampleReductions] | None:
        """List of per sample scores reduced across epochs"""
        warn_once(
            logger,
            "The 'sample_reductions' field is deprecated. Access reductions through the 'reductions' field on EvalLog instead.",
        )
        return self._sample_reductions
    @sample_reductions.setter
    def sample_reductions(self, value: list[EvalSampleReductions] | None) -> None:
        """Set list of per sample scores reduced across epochs"""
        self._sample_reductions = value
    @model_validator(mode="before")
    @classmethod
    def convert_scorer_to_scorers(
        cls: Type["EvalResults"], values: dict[str, Any]
    ) -> dict[str, Any]:
        if "scorer" in values:
            # There cannot be a scorers property too
            if "scores" in values:
                raise TypeError(
                    "Unexpected value `scores` present when `scorer` has already been specified."
                )
            # Gather metrics
            if "metrics" in values:
                metrics = values["metrics"]
                del values["metrics"]
            else:
                metrics = None
            # Convert the scorer to the new schema
            score = values["scorer"]
            if metrics:
                score["metrics"] = metrics
            score["scorer"] = score["name"]
            values["scores"] = [score]
            # Get rid of the 'scorer' property
            del values["scorer"]
        return values
class EvalDataset(BaseModel):
    """Dataset used for evaluation."""
    name: str | None = Field(default=None)
    """Dataset name."""
    location: str | None = Field(default=None)
    """Dataset location (file path or remote URL)"""
    samples: int | None = Field(default=None)
    """Number of samples in the dataset."""
    sample_ids: list[int | str] | None = Field(default=None)
    """IDs of samples in the dataset."""
    shuffled: bool | None = Field(default=None)
    """Was the dataset shuffled after reading."""
class EvalMetricDefinition(BaseModel):
    name: str
    """Metric name"""
    options: dict[str, Any] | None = Field(default=None)
class EvalScorer(BaseModel):
    name: str
    """Scorer name"""
    options: dict[str, Any] | None = Field(default=None)
    """Scorer arguments"""
    metrics: (
        list[EvalMetricDefinition | dict[str, list[EvalMetricDefinition]]]
        | dict[str, list[EvalMetricDefinition]]
        | None
    ) = Field(default=None)
    metadata: dict[str, Any] | None = Field(default=None)
    """Scorer metadata"""
class EvalRevision(BaseModel):
    """Git revision for evaluation."""
    type: Literal["git"]
    """Type of revision (currently only "git")"""
    origin: str
    """Revision origin server"""
    commit: str
    """Revision commit."""
class EvalSpec(BaseModel):
    """Eval target and configuration."""
    run_id: str = Field(default_factory=str)
    """Unique run id"""
    created: str
    """Time created."""
    task: str
    """Task name."""
    task_id: str = Field(default_factory=str)
    """Unique task id."""
    task_version: int = Field(default=0)
    """Task version."""
    task_file: str | None = Field(default=None)
    """Task source file."""
    task_attribs: dict[str, Any] = Field(default_factory=dict)
    """Attributes of the @task decorator."""
    task_args: dict[str, Any] = Field(default_factory=dict)
    """Arguments used for invoking the task."""
    solver: str | None = Field(default=None)
    """Solver name."""
    solver_args: dict[str, Any] | None = Field(default=None)
    """Arguments used for invoking the solver."""
    tags: list[str] | None = Field(default=None)
    """Tags associated with evaluation run."""
    dataset: EvalDataset
    """Dataset used for eval."""
    sandbox: SandboxEnvironmentSpec | None = Field(default=None)
    """Sandbox environment type and optional config file."""
    model: str
    """Model used for eval."""
    model_base_url: str | None = Field(default=None)
    """Optional override of model base url"""
    model_args: dict[str, Any] = Field(default_factory=dict)
    """Model specific arguments."""
    config: EvalConfig
    """Configuration values for eval."""
    revision: EvalRevision | None = Field(default=None)
    """Source revision of eval."""
    packages: dict[str, str] = Field(default_factory=dict)
    """Package versions for eval."""
    metadata: dict[str, Any] | None = Field(default=None)
    """Additional eval metadata."""
    scorers: list[EvalScorer] | None = Field(default=None)
    """Scorers and args for this eval"""
    metrics: (
        list[EvalMetricDefinition] | dict[str, list[EvalMetricDefinition]] | None
    ) = Field(default=None)
    """metrics and args for this eval"""
    # allow field model_args
    model_config = ConfigDict(protected_namespaces=())
    @model_validator(mode="before")
    @classmethod
    def read_sandbox_spec(
        cls: Type["EvalSpec"], values: dict[str, Any]
    ) -> dict[str, Any]:
        return migrate_sandbox_spec(values)
def migrate_sandbox_spec(values: dict[str, Any]) -> dict[str, Any]:
    if "sandbox" in values:
        sandbox = values.get("sandbox")
        if isinstance(sandbox, list):
            values["sandbox"] = SandboxEnvironmentSpec(
                type=sandbox[0], config=sandbox[1]
            )
    return values
def eval_error(
    exception: BaseException,
    exc_type: Type[Any],
    exc_value: BaseException,
    exc_traceback: TracebackType | None,
) -> EvalError:
    # get text traceback
    traceback_text, truncated = truncate_traceback(exc_type, exc_value, exc_traceback)
    if not truncated:
        with open(os.devnull, "w") as f:
            console = Console(record=True, file=f, legacy_windows=True)
            console.print(rich_traceback(exc_type, exc_value, exc_traceback))
            traceback_ansi = console.export_text(styles=True)
    else:
        traceback_ansi = traceback_text
    # return error
    return EvalError(
        message=exception_message(exception),
        traceback=traceback_text,
        traceback_ansi=traceback_ansi,
    )
def rich_traceback(
    exc_type: Type[Any], exc_value: BaseException, exc_traceback: TracebackType | None
) -> RenderableType:
    rich_tb = Traceback.from_exception(
        exc_type=exc_type,
        exc_value=exc_value,
        traceback=exc_traceback,
        suppress=[click, asyncio, tenacity, sys.modules[PKG_NAME]],
        show_locals=False,
        width=CONSOLE_DISPLAY_WIDTH,
    )
    return rich_tb
def truncate_traceback(
    exc_type: Type[Any],
    exc_value: BaseException,
    exc_traceback: TracebackType | None,
    max_length: int = 1048576,  # 1MB
) -> Tuple[str, bool]:
    tb_list = traceback.format_exception(exc_type, exc_value, exc_traceback)
    # Keep the front and back of the traceback
    header = tb_list[0]
    error_msg = tb_list[-1]
    # Join the middle parts (stack frames)
    frames = "".join(tb_list[1:-1])
    # It all fits, use it as is
    full_tb = header + frames + error_msg
    if len(full_tb) <= max_length:
        return full_tb, False
    ellipsis = "\n...\n"
    # Minimum header size
    header_size = min(len(header), 1024)
    # Minimum frames size
    frames_size = min(len(frames), 1024)
    # Remaining space for error message
    error_msg_size = max(0, max_length - header_size - frames_size)
    def truncate_middle(text: str, size: int) -> str:
        if len(text) <= size:
            return text
        half = (size - len(ellipsis)) // 2
        return f"{text[:half]}{ellipsis}{text[-half:]}"
    # Truncate each part as needed
    truncated_header = truncate_middle(header, header_size)
    truncated_frames = truncate_middle(frames, frames_size)
    truncated_error = truncate_middle(error_msg, error_msg_size)
    return truncated_header + truncated_frames + truncated_error, True
class EvalStats(BaseModel):
    """Timing and usage statistics."""
    started_at: str = Field(default_factory=str)
    """Evaluation start time."""
    completed_at: str = Field(default_factory=str)
    """Evaluation completion time."""
    model_usage: dict[str, ModelUsage] = Field(default_factory=dict)
    """Model token usage for evaluation."""
    # allow field model_usage
    model_config = ConfigDict(protected_namespaces=())
class EvalLog(BaseModel):
    """Evaluation log."""
    # WARNING: The order of these fields is important for the log file format.
    # Do not change the order of these fields without incrementing the version number,
    # updating the log file read/write functionality (such as read_eval_log),
    # and updating the tests.
    version: int = Field(default=2)
    """Eval log file format version."""
    status: Literal["started", "success", "cancelled", "error"] = Field(
        default="started"
    )
    """Status of evaluation (did it succeed or fail)."""
    eval: EvalSpec
    """Eval identity and configuration."""
    plan: EvalPlan = Field(default_factory=EvalPlan)
    """Eval plan (solvers and config)"""
    results: EvalResults | None = None
    """Eval results (scores and metrics)."""
    stats: EvalStats = Field(default_factory=EvalStats)
    """Eval stats (runtime, model usage)"""
    error: EvalError | None = Field(default=None)
    """Error that halted eval (if status=="error")"""
    samples: list[EvalSample] | None = Field(default=None)
    """Samples processed by eval."""
    reductions: list[EvalSampleReductions] | None = Field(default=None)
    """Reduced sample values"""
    location: str = Field(default_factory=str, exclude=True)
    """Location that the log file was read from."""
    @model_validator(mode="after")
    def populate_scorer_name_for_samples(self) -> "EvalLog":
        if self.samples and self.results and self.results.scores:
            scorer_name = self.results.scores[0].name
            for sample in self.samples:
                if sample.scores and SCORER_PLACEHOLDER in sample.scores:
                    sample.scores[scorer_name] = sample.scores[SCORER_PLACEHOLDER]
                    del sample.scores[SCORER_PLACEHOLDER]
        return self
    @model_validator(mode="before")
    @classmethod
    def resolve_sample_reductions(
        cls: Type["EvalLog"], values: dict[str, Any]
    ) -> dict[str, Any]:
        has_reductions = "reductions" in values
        has_results = values.get("results", None) is not None
        has_sample_reductions = has_results and (
            "sample_reductions" in values["results"]
        )
        if has_sample_reductions and not has_reductions:
            values["reductions"] = values["results"]["sample_reductions"]
        elif has_reductions and (has_results and not has_sample_reductions):
            values["results"]["sample_reductions"] = values["reductions"]
        return values
def sort_samples(samples: list[EvalSample]) -> None:
    # convert into string zfilled so order is preserved
    samples.sort(
        key=lambda sample: (
            sample.epoch,
            (sample.id if isinstance(sample.id, str) else str(sample.id).zfill(20)),
        )
    )

================
File: src/inspect_ai/log/_message.py
================
import re
from logging import LogRecord
from typing import Any, Literal, Type, cast
from pydantic import BaseModel, Field, model_validator
LoggingLevel = Literal[
    "debug", "trace", "http", "sandbox", "info", "warning", "error", "critical"
]
"""Logging level."""
class LoggingMessage(BaseModel):
    """Message written to Python log."""
    name: str | None = Field(default=None)
    """Logger name (e.g. 'httpx')"""
    level: LoggingLevel
    """Logging level."""
    message: str
    """Log message."""
    created: float
    """Message created time."""
    filename: str = Field(default="unknown")
    """Logged from filename."""
    module: str = Field(default="unknown")
    """Logged from module."""
    lineno: int = Field(default=0)
    """Logged from line number."""
    @staticmethod
    def _from_log_record(record: LogRecord) -> "LoggingMessage":
        """Create a LoggingMesssage from a LogRecord.
        Args:
          record (LogRecord): LogRecord to convert.
        Returns:
          LoggingMessage for LogRecord
        """
        return LoggingMessage(
            # don't include full file paths (as the filename is also below),
            # we just want to use this to capture e.g. 'httpx', 'openai', etc.
            name=record.name
            if re.match(r"^[\w_]+$", record.name) is not None
            else None,
            level=cast(LoggingLevel, record.levelname.lower()),
            message=record.getMessage(),
            created=record.created * 1000,
            filename=str(record.filename),
            module=str(record.module),
            lineno=record.lineno or 0,
        )
    @model_validator(mode="before")
    @classmethod
    def convert_log_levels(
        cls: Type["LoggingMessage"], values: dict[str, Any]
    ) -> dict[str, Any]:
        if "level" in values:
            level = values["level"]
            if level == "tools" or level == "sandbox":
                values["level"] = "trace"
        return values

================
File: src/inspect_ai/log/_recorders/__init__.py
================
from .create import (
    create_recorder_for_format,
    create_recorder_for_location,
    recorder_type_for_format,
    recorder_type_for_location,
)
from .recorder import Recorder
__all__ = [
    "Recorder",
    "create_recorder_for_format",
    "create_recorder_for_location",
    "recorder_type_for_format",
    "recorder_type_for_location",
]

================
File: src/inspect_ai/log/_recorders/create.py
================
from typing import Any, Callable, Literal, cast
from .eval import EvalRecorder
from .json import JSONRecorder
from .recorder import Recorder
_recorders: dict[str, type[Recorder]] = {"eval": EvalRecorder, "json": JSONRecorder}
def create_recorder_for_format(
    format: Literal["eval", "json"], *args: Any, **kwargs: Any
) -> Recorder:
    recorder = recorder_type_for_format(format)
    return recorder(*args, **kwargs)
def recorder_type_for_format(format: Literal["eval", "json"]) -> type[Recorder]:
    recorder = _recorders.get(format, None)
    if recorder:
        return recorder
    else:
        raise ValueError(f"No recorder for format: {format}")
def create_recorder_for_location(location: str, log_dir: str) -> Recorder:
    recorder = recorder_type_for_location(location)
    return cast(Callable[[str], Recorder], recorder)(log_dir)
def recorder_type_for_location(location: str) -> type[Recorder]:
    for recorder in _recorders.values():
        if recorder.handles_location(location):
            return recorder
    raise ValueError(f"No recorder for location: {location}")

================
File: src/inspect_ai/log/_recorders/eval.py
================
import json
import os
import tempfile
from logging import getLogger
from typing import Any, BinaryIO, Literal, cast
from zipfile import ZIP_DEFLATED, ZipFile
import anyio
from pydantic import BaseModel, Field
from pydantic_core import to_json
from typing_extensions import override
from inspect_ai._util.constants import DESERIALIZING_CONTEXT, LOG_SCHEMA_VERSION
from inspect_ai._util.content import (
    ContentAudio,
    ContentImage,
    ContentReasoning,
    ContentText,
    ContentVideo,
)
from inspect_ai._util.error import EvalError
from inspect_ai._util.file import FileSystem, dirname, file, filesystem
from inspect_ai._util.json import jsonable_python
from inspect_ai._util.trace import trace_action
from inspect_ai.model._chat_message import ChatMessage
from inspect_ai.scorer._metric import Score
from .._log import (
    EvalLog,
    EvalPlan,
    EvalResults,
    EvalSample,
    EvalSampleReductions,
    EvalSpec,
    EvalStats,
    sort_samples,
)
from .file import FileRecorder
logger = getLogger(__name__)
class SampleSummary(BaseModel):
    id: int | str
    epoch: int
    input: str | list[ChatMessage]
    target: str | list[str]
    scores: dict[str, Score] | None = Field(default=None)
    error: str | None = Field(default=None)
    limit: str | None = Field(default=None)
class LogStart(BaseModel):
    version: int
    eval: EvalSpec
    plan: EvalPlan
class LogResults(BaseModel):
    status: Literal["started", "success", "cancelled", "error"]
    stats: EvalStats
    results: EvalResults | None = Field(default=None)
    error: EvalError | None = Field(default=None)
JOURNAL_DIR = "_journal"
SUMMARY_DIR = "summaries"
SAMPLES_DIR = "samples"
START_JSON = "start.json"
RESULTS_JSON = "results.json"
REDUCTIONS_JSON = "reductions.json"
SUMMARIES_JSON = "summaries.json"
HEADER_JSON = "header.json"
class EvalRecorder(FileRecorder):
    @override
    @classmethod
    def handles_location(cls, location: str) -> bool:
        return location.endswith(".eval")
    @override
    def default_log_buffer(self) -> int:
        # .eval files are 5-8x smaller than .json files so we
        # are much less worried about flushing frequently
        return 10
    def __init__(self, log_dir: str, fs_options: dict[str, Any] = {}):
        super().__init__(log_dir, ".eval", fs_options)
        # each eval has a unique key (created from run_id and task name/version)
        # which we use to track the output path, accumulated data, and event counter
        self.data: dict[str, ZipLogFile] = {}
    @override
    async def log_init(
        self, eval: EvalSpec, location: str | None = None, *, clean: bool = False
    ) -> str:
        # if the file exists then read summaries
        if not clean and location is not None and self.fs.exists(location):
            with file(location, "rb") as f:
                with ZipFile(f, "r") as zip:
                    log_start = _read_start(zip)
                    summary_counter = _read_summary_counter(zip)
                    summaries = _read_all_summaries(zip, summary_counter)
        else:
            log_start = None
            summary_counter = 0
            summaries = []
        # create zip wrapper
        zip_file = location or self._log_file_path(eval)
        zip_log_file = ZipLogFile(file=zip_file)
        await zip_log_file.init(log_start, summary_counter, summaries)
        # track zip
        self.data[self._log_file_key(eval)] = zip_log_file
        # return file path
        return zip_file
    @override
    async def log_start(self, eval: EvalSpec, plan: EvalPlan) -> None:
        log = self.data[self._log_file_key(eval)]
        start = LogStart(version=LOG_SCHEMA_VERSION, eval=eval, plan=plan)
        await log.start(start)
    @override
    async def log_sample(self, eval: EvalSpec, sample: EvalSample) -> None:
        log = self.data[self._log_file_key(eval)]
        await log.buffer_sample(sample)
    @override
    async def flush(self, eval: EvalSpec) -> None:
        # get the zip log
        log = self.data[self._log_file_key(eval)]
        # write the buffered samples
        await log.write_buffered_samples()
        # flush to underlying stream
        await log.flush()
    @override
    async def log_finish(
        self,
        eval: EvalSpec,
        status: Literal["started", "success", "cancelled", "error"],
        stats: EvalStats,
        results: EvalResults | None,
        reductions: list[EvalSampleReductions] | None,
        error: EvalError | None = None,
    ) -> EvalLog:
        # get the key and log
        key = self._log_file_key(eval)
        log = self.data[key]
        # write the buffered samples
        await log.write_buffered_samples()
        # write consolidated summaries
        await log.write(SUMMARIES_JSON, log._summaries)
        # write reductions
        if reductions is not None:
            await log.write(REDUCTIONS_JSON, reductions)
        # Get the results
        log_results = LogResults(
            status=status, stats=stats, results=results, error=error
        )
        # add the results to the original eval log from start.json
        log_start = log.log_start
        if log_start is None:
            raise RuntimeError("Log not properly initialised")
        eval_header = EvalLog(
            version=log_start.version,
            eval=log_start.eval,
            plan=log_start.plan,
            results=log_results.results,
            stats=log_results.stats,
            status=log_results.status,
            error=log_results.error,
        )
        await log.write(HEADER_JSON, eval_header)
        # stop tracking this eval
        del self.data[key]
        # flush and write the results
        await log.flush()
        return await log.close()
    @classmethod
    @override
    async def read_log(cls, location: str, header_only: bool = False) -> EvalLog:
        # if the log is not stored in the local filesystem then download it first,
        # and then read it from a temp file (eliminates the possiblity of hundreds
        # of small fetches from the zip file streams)
        temp_log: str | None = None
        fs = filesystem(location)
        if not fs.is_local() and header_only is False:
            with tempfile.NamedTemporaryFile(delete=False) as temp:
                temp_log = temp.name
                fs.get_file(location, temp_log)
        # read log (use temp_log if we have it)
        try:
            with file(temp_log or location, "rb") as z:
                return _read_log(z, location, header_only)
        finally:
            if temp_log:
                os.unlink(temp_log)
    @override
    @classmethod
    async def read_log_sample(
        cls, location: str, id: str | int, epoch: int = 1
    ) -> EvalSample:
        with file(location, "rb") as z:
            with ZipFile(z, mode="r") as zip:
                try:
                    with zip.open(_sample_filename(id, epoch), "r") as f:
                        return EvalSample.model_validate(
                            json.load(f), context=DESERIALIZING_CONTEXT
                        )
                except KeyError:
                    raise IndexError(
                        f"Sample id {id} for epoch {epoch} not found in log {location}"
                    )
    @classmethod
    @override
    async def write_log(cls, location: str, log: EvalLog) -> None:
        # write using the recorder (so we get all of the extra streams)
        recorder = EvalRecorder(dirname(location))
        await recorder.log_init(log.eval, location, clean=True)
        await recorder.log_start(log.eval, log.plan)
        for sample in log.samples or []:
            await recorder.log_sample(log.eval, sample)
        await recorder.log_finish(
            log.eval, log.status, log.stats, log.results, log.reductions, log.error
        )
def text_inputs(inputs: str | list[ChatMessage]) -> str | list[ChatMessage]:
    # Clean the input of any images
    if isinstance(inputs, list):
        input: list[ChatMessage] = []
        for message in inputs:
            if not isinstance(message.content, str):
                filtered_content: list[
                    ContentText
                    | ContentReasoning
                    | ContentImage
                    | ContentAudio
                    | ContentVideo
                ] = []
                for content in message.content:
                    if content.type == "text":
                        filtered_content.append(content)
                    else:
                        filtered_content.append(
                            ContentText(text=f"({content.type.capitalize()})")
                        )
                message.content = filtered_content
                input.append(message)
            else:
                input.append(message)
        return input
    else:
        return inputs
class ZipLogFile:
    _zip: ZipFile | None
    _temp_file: BinaryIO
    _fs: FileSystem
    def __init__(self, file: str) -> None:
        self._file = file
        self._fs = filesystem(file)
        self._lock = anyio.Lock()
        self._temp_file = tempfile.TemporaryFile()
        self._samples: list[EvalSample] = []
        self._summary_counter = 0
        self._summaries: list[SampleSummary] = []
        self._log_start: LogStart | None = None
    async def init(
        self,
        log_start: LogStart | None,
        summary_counter: int,
        summaries: list[SampleSummary],
    ) -> None:
        async with self._lock:
            self._open()
            self._summary_counter = summary_counter
            self._summaries = summaries
            self._log_start = log_start
    @property
    def log_start(self) -> LogStart | None:
        return self._log_start
    async def start(self, start: LogStart) -> None:
        async with self._lock:
            self._log_start = start
            self._zip_writestr(_journal_path(START_JSON), start)
    async def buffer_sample(self, sample: EvalSample) -> None:
        async with self._lock:
            self._samples.append(sample)
    async def write_buffered_samples(self) -> None:
        async with self._lock:
            # Write the buffered samples
            summaries: list[SampleSummary] = []
            for sample in self._samples:
                # Write the sample
                self._zip_writestr(_sample_filename(sample.id, sample.epoch), sample)
                # Capture the summary
                summaries.append(
                    SampleSummary(
                        id=sample.id,
                        epoch=sample.epoch,
                        input=text_inputs(sample.input),
                        target=sample.target,
                        scores=sample.scores,
                        error=sample.error.message
                        if sample.error is not None
                        else None,
                        limit=f"{sample.limit.type}"
                        if sample.limit is not None
                        else None,
                    )
                )
            self._samples.clear()
            # write intermediary summaries and add to master list
            if len(summaries) > 0:
                self._summary_counter += 1
                summary_file = _journal_summary_file(self._summary_counter)
                summary_path = _journal_summary_path(summary_file)
                self._zip_writestr(summary_path, summaries)
                self._summaries.extend(summaries)
    async def write(self, filename: str, data: Any) -> None:
        async with self._lock:
            self._zip_writestr(filename, data)
    async def flush(self) -> None:
        async with self._lock:
            # close the zip file so it is flushed
            if self._zip:
                self._zip.close()
            # read the temp_file (leaves pointer at end for subsequent appends)
            self._temp_file.seek(0)
            log_bytes = self._temp_file.read()
            with trace_action(logger, "Log Write", self._file):
                try:
                    with file(self._file, "wb") as f:
                        f.write(log_bytes)
                finally:
                    # re-open zip file w/ self.temp_file pointer at end
                    self._open()
    async def close(self) -> EvalLog:
        async with self._lock:
            # read the log from the temp file then close it
            try:
                self._temp_file.seek(0)
                return _read_log(self._temp_file, self._file)
            finally:
                self._temp_file.close()
                if self._zip:
                    self._zip.close()
    # cleanup zip file if we didn't in normal course
    def __del__(self) -> None:
        if self._zip:
            self._zip.close()
    def _open(self) -> None:
        self._zip = ZipFile(
            self._temp_file,
            mode="a",
            compression=ZIP_DEFLATED,
            compresslevel=5,
        )
    # raw unsynchronized version of write
    def _zip_writestr(self, filename: str, data: Any) -> None:
        assert self._zip
        self._zip.writestr(
            filename,
            to_json(
                value=jsonable_python(data),
                indent=2,
                exclude_none=True,
                fallback=lambda _x: None,
            ),
        )
def _read_log(log: BinaryIO, location: str, header_only: bool = False) -> EvalLog:
    with ZipFile(log, mode="r") as zip:
        evalLog = _read_header(zip, location)
        if REDUCTIONS_JSON in zip.namelist():
            with zip.open(REDUCTIONS_JSON, "r") as f:
                reductions = [
                    EvalSampleReductions.model_validate(
                        reduction, context=DESERIALIZING_CONTEXT
                    )
                    for reduction in json.load(f)
                ]
                if evalLog.results is not None:
                    evalLog.reductions = reductions
        samples: list[EvalSample] | None = None
        if not header_only:
            samples = []
            for name in zip.namelist():
                if name.startswith(f"{SAMPLES_DIR}/") and name.endswith(".json"):
                    with zip.open(name, "r") as f:
                        samples.append(
                            EvalSample.model_validate(
                                json.load(f), context=DESERIALIZING_CONTEXT
                            ),
                        )
            sort_samples(samples)
            evalLog.samples = samples
        return evalLog
def _read_start(zip: ZipFile) -> LogStart | None:
    start_path = _journal_path(START_JSON)
    if start_path in zip.namelist():
        return cast(LogStart, _read_json(zip, start_path))
    else:
        return None
def _read_summary_counter(zip: ZipFile) -> int:
    current_count = 0
    for name in zip.namelist():
        if name.startswith(_journal_summary_path()) and name.endswith(".json"):
            this_count = int(name.split("/")[-1].split(".")[0])
            current_count = max(this_count, current_count)
    return current_count
def _read_all_summaries(zip: ZipFile, count: int) -> list[SampleSummary]:
    if SUMMARIES_JSON in zip.namelist():
        summaries_raw = _read_json(zip, SUMMARIES_JSON)
        if isinstance(summaries_raw, list):
            return [
                SampleSummary.model_validate(value, context=DESERIALIZING_CONTEXT)
                for value in summaries_raw
            ]
        else:
            raise ValueError(
                f"Expected a list of summaries when reading {SUMMARIES_JSON}"
            )
    else:
        summaries: list[SampleSummary] = []
        for i in range(1, count):
            summary_file = _journal_summary_file(i)
            summary_path = _journal_summary_path(summary_file)
            summary = _read_json(zip, summary_path)
            if isinstance(summary, list):
                summaries.extend(
                    [
                        SampleSummary.model_validate(
                            value, context=DESERIALIZING_CONTEXT
                        )
                        for value in summary
                    ]
                )
            else:
                raise ValueError(
                    f"Expected a list of summaries when reading {summary_file}"
                )
        return summaries
def _read_header(zip: ZipFile, location: str) -> EvalLog:
    # first see if the header is here
    if HEADER_JSON in zip.namelist():
        with zip.open(HEADER_JSON, "r") as f:
            log = EvalLog.model_validate(json.load(f), context=DESERIALIZING_CONTEXT)
            log.location = location
            return log
    else:
        with zip.open(_journal_path(START_JSON), "r") as f:
            start = LogStart.model_validate(json.load(f), context=DESERIALIZING_CONTEXT)
        return EvalLog(
            version=start.version, eval=start.eval, plan=start.plan, location=location
        )
def _sample_filename(id: str | int, epoch: int) -> str:
    return f"{SAMPLES_DIR}/{id}_epoch_{epoch}.json"
def _read_json(zip: ZipFile, filename: str) -> Any:
    with zip.open(filename) as f:
        return json.load(f)
def _journal_path(file: str) -> str:
    return JOURNAL_DIR + "/" + file
def _journal_summary_path(file: str | None = None) -> str:
    if file is None:
        return _journal_path(SUMMARY_DIR)
    else:
        return f"{_journal_path(SUMMARY_DIR)}/{file}"
def _journal_summary_file(index: int) -> str:
    return f"{index}.json"

================
File: src/inspect_ai/log/_recorders/file.py
================
from logging import getLogger
from typing import Any
from typing_extensions import override
from inspect_ai._util.file import filesystem
from inspect_ai._util.registry import registry_unqualified_name
from .._log import EvalLog, EvalSample, EvalSpec
from .recorder import Recorder
logger = getLogger(__name__)
class FileRecorder(Recorder):
    __last_read_sample_log: tuple[str, EvalLog] | None = None
    def __init__(
        self, log_dir: str, suffix: str, fs_options: dict[str, Any] = {}
    ) -> None:
        self.log_dir = log_dir.rstrip("/\\")
        self.suffix = suffix
        # initialise filesystem
        self.fs = filesystem(log_dir, fs_options)
        self.fs.mkdir(self.log_dir, exist_ok=True)
    def is_local(self) -> bool:
        return self.fs.is_local()
    @override
    def is_writeable(self) -> bool:
        return self.fs.is_writeable(self.log_dir)
    @override
    @classmethod
    async def read_log_sample(
        cls, location: str, id: str | int, epoch: int = 1
    ) -> EvalSample:
        # establish the log to read from (might be cached)
        if cls.__last_read_sample_log and (cls.__last_read_sample_log[0] == "location"):
            eval_log = cls.__last_read_sample_log[1]
        else:
            eval_log = await cls.read_log(location)
            cls.__last_read_sample_log = (location, eval_log)
        # throw if no samples
        if not eval_log.samples:
            raise IndexError(f"No samples found in log {location}")
        # find the sample
        eval_sample = next(
            (
                sample
                for sample in (eval_log.samples)
                if sample.id == id and sample.epoch == epoch
            ),
            None,
        )
        if eval_sample is None:
            raise IndexError(
                f"Sample id {id} for epoch {epoch} not found in log {location}"
            )
        else:
            return eval_sample
    def _log_file_key(self, eval: EvalSpec) -> str:
        # clean underscores, slashes, and : from the log file key (so we can reliably parse it
        # later without worrying about underscores)
        def clean(s: str) -> str:
            return s.replace("_", "-").replace("/", "-").replace(":", "-")
        # remove package from task name
        task = registry_unqualified_name(eval.task)
        return f"{clean(eval.created)}_{clean(task)}_{clean(eval.task_id)}"
    def _log_file_path(self, eval: EvalSpec) -> str:
        return f"{self.log_dir}{self.fs.sep}{self._log_file_key(eval)}{self.suffix}"

================
File: src/inspect_ai/log/_recorders/json.py
================
from logging import getLogger
from typing import Any, Literal, get_args
import ijson  # type: ignore
from ijson import IncompleteJSONError
from pydantic import BaseModel
from pydantic_core import from_json
from typing_extensions import override
from inspect_ai._util.constants import DESERIALIZING_CONTEXT, LOG_SCHEMA_VERSION
from inspect_ai._util.error import EvalError
from inspect_ai._util.file import absolute_file_path, file
from inspect_ai._util.trace import trace_action
from .._log import (
    EvalLog,
    EvalPlan,
    EvalResults,
    EvalSample,
    EvalSampleReductions,
    EvalSpec,
    EvalStats,
    sort_samples,
)
from .file import FileRecorder
logger = getLogger(__name__)
class JSONRecorder(FileRecorder):
    @override
    @classmethod
    def handles_location(cls, location: str) -> bool:
        return location.endswith(".json")
    @override
    def default_log_buffer(self) -> int:
        # we write the entire file in one shot and the files can
        # get fairly large (> 100MB) so we are a bit more sparing
        # for remote filesystem writes
        if self.is_local():
            return 10
        else:
            return 100
    class JSONLogFile(BaseModel):
        file: str
        data: EvalLog
    def __init__(
        self, log_dir: str, suffix: str = ".json", fs_options: dict[str, Any] = {}
    ):
        # call super
        super().__init__(log_dir, suffix, fs_options)
        # each eval has a unique key (created from run_id and task name/version)
        # which we use to track the output path, accumulated data, and event counter
        self.data: dict[str, JSONRecorder.JSONLogFile] = {}
    @override
    async def log_init(self, eval: EvalSpec, location: str | None = None) -> str:
        # initialize file log for this eval
        # compute an absolute path if it's a relative ref
        # (so that the writes go to the correct place even
        # if the working directory is switched for a task)
        file = location or absolute_file_path(self._log_file_path(eval))
        # compute an absolute path if it's a relative ref
        # (so that the writes go to the correct place even
        # if the working directory is switched for a task)
        self.data[self._log_file_key(eval)] = JSONRecorder.JSONLogFile(
            file=file, data=EvalLog(eval=eval)
        )
        # attempt to
        return file
    @override
    async def log_start(self, eval: EvalSpec, plan: EvalPlan) -> None:
        log = self.data[self._log_file_key(eval)]
        log.data.plan = plan
    @override
    async def log_sample(self, eval: EvalSpec, sample: EvalSample) -> None:
        log = self.data[self._log_file_key(eval)]
        if log.data.samples is None:
            log.data.samples = []
        log.data.samples.append(sample)
    @override
    async def log_finish(
        self,
        spec: EvalSpec,
        status: Literal["started", "success", "cancelled", "error"],
        stats: EvalStats,
        results: EvalResults | None,
        reductions: list[EvalSampleReductions] | None,
        error: EvalError | None = None,
    ) -> EvalLog:
        log = self.data[self._log_file_key(spec)]
        log.data.status = status
        log.data.stats = stats
        log.data.results = results
        if error:
            log.data.error = error
        if reductions:
            log.data.reductions = reductions
        await self.write_log(log.file, log.data)
        log.data.location = log.file
        # stop tracking this data
        del self.data[self._log_file_key(spec)]
        # return the log
        return log.data
    @override
    async def flush(self, eval: EvalSpec) -> None:
        log = self.data[self._log_file_key(eval)]
        await self.write_log(log.file, log.data)
    @override
    @classmethod
    async def read_log(cls, location: str, header_only: bool = False) -> EvalLog:
        if header_only:
            try:
                return _read_header_streaming(location)
            # The Python JSON serializer supports NaN and Inf, however
            # this isn't technically part of the JSON spec. The json-stream
            # library shares this limitation, so if we fail with an
            # invalid character then we move on and and parse w/ pydantic
            # (which does support NaN and Inf by default)
            except (ValueError, IncompleteJSONError) as ex:
                if (
                    str(ex).find("Invalid JSON character") != -1
                    or str(ex).find("invalid char in json text") != -1
                ):
                    pass
                else:
                    raise ValueError(f"Unable to read log file: {location}") from ex
        # full reads (and fallback to streaing reads if they encounter invalid json characters)
        with file(location, "r") as f:
            # parse w/ pydantic
            raw_data = from_json(f.read())
            log = EvalLog.model_validate(raw_data, context=DESERIALIZING_CONTEXT)
            log.location = location
            # fail for unknown version
            _validate_version(log.version)
            # set the version to the schema version we'll be returning
            log.version = LOG_SCHEMA_VERSION
            # prune if header_only
            if header_only:
                # exclude samples
                log.samples = None
                # prune sample reductions
                if log.results is not None:
                    log.results.sample_reductions = None
                    log.reductions = None
            # return log
            return log
    @override
    @classmethod
    async def write_log(cls, location: str, log: EvalLog) -> None:
        from inspect_ai.log._file import eval_log_json
        # sort samples before writing as they can come in out of order
        if log.samples:
            sort_samples(log.samples)
        # get log as bytes
        log_bytes = eval_log_json(log)
        with trace_action(logger, "Log Write", location):
            with file(location, "wb") as f:
                f.write(log_bytes)
def _validate_version(ver: int) -> None:
    if ver > LOG_SCHEMA_VERSION:
        raise ValueError(f"Unable to read version {ver} of log format.")
def _read_header_streaming(log_file: str) -> EvalLog:
    with file(log_file, "rb") as f:
        # Do low-level parsing to get the version number and also
        # detect the presence of results or error sections
        version: int | None = None
        has_results = False
        has_error = False
        for prefix, event, value in ijson.parse(f):
            if (prefix, event) == ("version", "number"):
                version = value
            elif (prefix, event) == ("results", "start_map"):
                has_results = True
            elif (prefix, event) == ("error", "start_map"):
                has_error = True
            elif prefix == "samples":
                # Break as soon as we hit samples as that can be very large
                break
        if version is None:
            raise ValueError("Unable to read version of log format.")
        _validate_version(version)
        version = LOG_SCHEMA_VERSION
        # Rewind the file to the beginning to re-parse the contents of fields
        f.seek(0)
        # Parse the log file, stopping before parsing samples
        status: Literal["started", "success", "cancelled", "error"] | None = None
        for k, v in ijson.kvitems(f, ""):
            if k == "status":
                assert v in get_args(
                    Literal["started", "success", "cancelled", "error"]
                )
                status = v
            if k == "eval":
                eval = EvalSpec(**v)
            elif k == "plan":
                plan = EvalPlan(**v)
            elif k == "results":
                results = EvalResults(**v)
            elif k == "stats":
                stats = EvalStats(**v)
                if not has_error:
                    # Exit before parsing samples
                    break
            elif k == "error":
                error = EvalError(**v)
                break
    assert status, "Must encounter a 'status'"
    return EvalLog(
        eval=eval,
        plan=plan,
        results=results if has_results else None,
        stats=stats,
        status=status,
        version=version,
        error=error if has_error else None,
        location=log_file,
    )

================
File: src/inspect_ai/log/_recorders/recorder.py
================
import abc
from typing import Literal
from inspect_ai._util.error import EvalError
from inspect_ai.log._log import (
    EvalLog,
    EvalPlan,
    EvalResults,
    EvalSample,
    EvalSampleReductions,
    EvalSpec,
    EvalStats,
)
class Recorder(abc.ABC):
    @classmethod
    @abc.abstractmethod
    def handles_location(cls, location: str) -> bool: ...
    @abc.abstractmethod
    def default_log_buffer(self) -> int: ...
    @abc.abstractmethod
    def is_writeable(self) -> bool: ...
    @abc.abstractmethod
    async def log_init(self, eval: EvalSpec, location: str | None = None) -> str: ...
    @abc.abstractmethod
    async def log_start(self, eval: EvalSpec, plan: EvalPlan) -> None: ...
    @abc.abstractmethod
    async def log_sample(self, eval: EvalSpec, sample: EvalSample) -> None: ...
    @abc.abstractmethod
    async def flush(self, eval: EvalSpec) -> None: ...
    @abc.abstractmethod
    async def log_finish(
        self,
        eval: EvalSpec,
        status: Literal["success", "cancelled", "error"],
        stats: EvalStats,
        results: EvalResults | None,
        reductions: list[EvalSampleReductions] | None,
        error: EvalError | None = None,
    ) -> EvalLog: ...
    @classmethod
    @abc.abstractmethod
    async def read_log(cls, location: str, header_only: bool = False) -> EvalLog: ...
    @classmethod
    @abc.abstractmethod
    async def read_log_sample(
        cls, location: str, id: str | int, epoch: int = 1
    ) -> EvalSample: ...
    @classmethod
    @abc.abstractmethod
    async def write_log(cls, location: str, log: EvalLog) -> None: ...

================
File: src/inspect_ai/log/_retry.py
================
from logging import getLogger
from inspect_ai._util.logger import warn_once
from ._file import EvalLogInfo, read_eval_log_headers
logger = getLogger(__name__)
def retryable_eval_logs(logs: list[EvalLogInfo]) -> list[EvalLogInfo]:
    """Extract the list of retryable logs from a list of logs.
    Retryable logs are logs with status "error" or "cancelled" that
    do not have a corresponding log with status "success" (indicating
    they were subsequently retried and completed)
    Args:
      logs (list[EvalLogInfo]): List of logs to examine.
    Returns:
      List of retryable eval logs found in the list of logs.
    """
    warn_once(
        logger,
        "The retryable_eval_logs function is deprecated. Please use the eval_set() function instead.",
    )
    # first collect up all of the headers (so we can look at status)
    log_headers = read_eval_log_headers(logs)
    # build a set of completed task ids
    completed_task_ids = set(
        [
            log_header.eval.task_id
            for log_header in log_headers
            if log_header.status == "success"
        ]
    )
    # find all logs for incomplete tasks ("started", "error", or "cancelled") that # # have not been subsequently completed (keep a map by task_id, and perserve only
    # the most recent one)
    retryable_logs: dict[str, EvalLogInfo] = {}
    for log, log_header in zip(logs, log_headers):
        if log_header.status == "cancelled" or log_header.status == "error":
            if log_header.eval.task_id not in completed_task_ids:
                existing_log = retryable_logs.get(log_header.eval.task_id, None)
                if existing_log:
                    if (
                        existing_log.mtime is not None
                        and log.mtime is not None
                        and log.mtime > existing_log.mtime
                    ):
                        retryable_logs[log_header.eval.task_id] = log
                else:
                    retryable_logs[log_header.eval.task_id] = log
    # return the retryable logs
    return list(retryable_logs.values())

================
File: src/inspect_ai/log/_samples.py
================
import contextlib
from contextvars import ContextVar
from datetime import datetime
from typing import AsyncGenerator, Iterator, Literal
from shortuuid import uuid
from inspect_ai._util.constants import SAMPLE_SUBTASK
from inspect_ai.dataset._dataset import Sample
from inspect_ai.util._sandbox import SandboxConnection
from inspect_ai.util._sandbox.context import sandbox_connections
from ._transcript import Transcript, transcript
class ActiveSample:
    def __init__(
        self,
        *,
        task: str,
        model: str,
        sample: Sample,
        epoch: int,
        message_limit: int | None,
        token_limit: int | None,
        time_limit: int | None,
        working_limit: int | None,
        fails_on_error: bool,
        transcript: Transcript,
        sandboxes: dict[str, SandboxConnection],
    ) -> None:
        self.id = uuid()
        self.started: float | None = None
        self.completed: float | None = None
        self.task = task
        self.model = model
        self.sample = sample
        self.epoch = epoch
        self.message_limit = message_limit
        self.token_limit = token_limit
        self.time_limit = time_limit
        self.working_limit = working_limit
        self.fails_on_error = fails_on_error
        self.total_messages = 0
        self.total_tokens = 0
        self.transcript = transcript
        self.sandboxes = sandboxes
        self.retry_count = 0
        self._interrupt_action: Literal["score", "error"] | None = None
    @property
    def running_time(self) -> float:
        if self.started is not None:
            completed = (
                self.completed
                if self.completed is not None
                else datetime.now().timestamp()
            )
            return completed - self.started
        else:
            return 0
    def interrupt(self, action: Literal["score", "error"]) -> None:
        self._interrupt_action = action
    @property
    def interrupt_action(self) -> Literal["score", "error"] | None:
        return self._interrupt_action
def init_active_samples() -> None:
    _active_samples.clear()
@contextlib.asynccontextmanager
async def active_sample(
    *,
    task: str,
    model: str,
    sample: Sample,
    epoch: int,
    message_limit: int | None,
    token_limit: int | None,
    time_limit: int | None,
    working_limit: int | None,
    fails_on_error: bool,
    transcript: Transcript,
) -> AsyncGenerator[ActiveSample, None]:
    # create the sample
    active = ActiveSample(
        task=task,
        model=model,
        sample=sample,
        epoch=epoch,
        message_limit=message_limit,
        token_limit=token_limit,
        time_limit=time_limit,
        working_limit=working_limit,
        sandboxes=await sandbox_connections(),
        fails_on_error=fails_on_error,
        transcript=transcript,
    )
    _active_samples.append(active)
    _sample_active.set(active)
    try:
        yield active
    finally:
        active.completed = datetime.now().timestamp()
        _active_samples.remove(active)
        _sample_active.set(None)
def sample_active() -> ActiveSample | None:
    return _sample_active.get(None)
def active_sample_token_limit() -> int | None:
    active = sample_active()
    if active:
        return active.token_limit
    else:
        return None
def set_active_sample_token_limit(token_limit: int | None) -> None:
    active = sample_active()
    if active:
        active.token_limit = token_limit
def set_active_sample_total_tokens(total_tokens: int) -> None:
    active = sample_active()
    if active:
        active.total_tokens = total_tokens
def active_sample_message_limit() -> int | None:
    active = sample_active()
    if active:
        return active.message_limit
    else:
        return None
def set_active_sample_message_limit(message_limit: int | None) -> None:
    active = sample_active()
    if active:
        active.message_limit = message_limit
def set_active_sample_total_messages(total_messages: int) -> None:
    active = sample_active()
    if active:
        active.total_messages = total_messages
@contextlib.contextmanager
def track_active_sample_retries() -> Iterator[None]:
    reset_active_sample_retries()
    try:
        yield
    finally:
        reset_active_sample_retries()
def reset_active_sample_retries() -> None:
    active = sample_active()
    if active:
        active.retry_count = 0
def report_active_sample_retry() -> None:
    active = sample_active()
    if active:
        # only do this for the top level subtask
        if transcript().name == SAMPLE_SUBTASK:
            active.retry_count = active.retry_count + 1
_sample_active: ContextVar[ActiveSample | None] = ContextVar(
    "_sample_active", default=None
)
def active_samples() -> list[ActiveSample]:
    return _active_samples
_active_samples: list[ActiveSample] = []

================
File: src/inspect_ai/log/_transcript.py
================
import contextlib
from contextvars import ContextVar
from datetime import datetime
from logging import getLogger
from typing import (
    Any,
    Callable,
    Iterator,
    Literal,
    Sequence,
    Type,
    TypeAlias,
    TypeVar,
    Union,
)
from pydantic import BaseModel, ConfigDict, Field, JsonValue, field_serializer
from inspect_ai._util.constants import SAMPLE_SUBTASK
from inspect_ai._util.error import EvalError
from inspect_ai._util.json import JsonChange, json_changes
from inspect_ai._util.working import sample_working_time
from inspect_ai.dataset._dataset import Sample
from inspect_ai.log._message import LoggingMessage
from inspect_ai.model._chat_message import ChatMessage
from inspect_ai.model._generate_config import GenerateConfig
from inspect_ai.model._model_call import ModelCall
from inspect_ai.model._model_output import ModelOutput
from inspect_ai.scorer._metric import Score
from inspect_ai.solver._task_state import state_jsonable
from inspect_ai.tool._tool import ToolResult
from inspect_ai.tool._tool_call import (
    ToolCall,
    ToolCallContent,
    ToolCallError,
    ToolCallView,
)
from inspect_ai.tool._tool_choice import ToolChoice
from inspect_ai.tool._tool_info import ToolInfo
from inspect_ai.util._store import store, store_changes, store_jsonable
logger = getLogger(__name__)
class BaseEvent(BaseModel):
    timestamp: datetime = Field(default_factory=datetime.now)
    """Clock time at which event occurred."""
    working_start: float = Field(default_factory=sample_working_time)
    """Working time (within sample) at which the event occurred."""
    pending: bool | None = Field(default=None)
    """Is this event pending?"""
    @field_serializer("timestamp")
    def serialize_timestamp(self, dt: datetime) -> str:
        return dt.astimezone().isoformat()
class SampleInitEvent(BaseEvent):
    """Beginning of processing a Sample."""
    event: Literal["sample_init"] = Field(default="sample_init")
    """Event type."""
    sample: Sample
    """Sample."""
    state: JsonValue
    """Initial state."""
class SampleLimitEvent(BaseEvent):
    """The sample was unable to finish processing due to a limit"""
    event: Literal["sample_limit"] = Field(default="sample_limit")
    """Event type."""
    type: Literal["message", "time", "working", "token", "operator", "custom"]
    """Type of limit that halted processing"""
    message: str
    """A message associated with this limit"""
    limit: int | None = Field(default=None)
    """The limit value (if any)"""
class StoreEvent(BaseEvent):
    """Change to data within the current `Store`."""
    event: Literal["store"] = Field(default="store")
    """Event type."""
    changes: list[JsonChange]
    """List of changes to the `Store`."""
class StateEvent(BaseEvent):
    """Change to the current `TaskState`"""
    event: Literal["state"] = Field(default="state")
    """Event type."""
    changes: list[JsonChange]
    """List of changes to the `TaskState`"""
class ModelEvent(BaseEvent):
    """Call to a language model."""
    event: Literal["model"] = Field(default="model")
    """Event type."""
    model: str
    """Model name."""
    input: list[ChatMessage]
    """Model input (list of messages)."""
    tools: list[ToolInfo]
    """Tools available to the model."""
    tool_choice: ToolChoice
    """Directive to the model which tools to prefer."""
    config: GenerateConfig
    """Generate config used for call to model."""
    output: ModelOutput
    """Output from model."""
    error: str | None = Field(default=None)
    """Error which occurred during model call."""
    cache: Literal["read", "write"] | None = Field(default=None)
    """Was this a cache read or write."""
    call: ModelCall | None = Field(default=None)
    """Raw call made to model API."""
    completed: datetime | None = Field(default=None)
    """Time that model call completed (see `timestamp` for started)"""
    working_time: float | None = Field(default=None)
    """working time for model call that succeeded (i.e. was not retried)."""
    @field_serializer("completed")
    def serialize_completed(self, dt: datetime) -> str:
        if dt is None:
            return None
        return dt.astimezone().isoformat()
class ToolEvent(BaseEvent):
    """Call to a tool."""
    event: Literal["tool"] = Field(default="tool")
    """Event type."""
    type: Literal["function"] = Field(default="function")
    """Type of tool call (currently only 'function')"""
    id: str
    """Unique identifier for tool call."""
    function: str
    """Function called."""
    arguments: dict[str, JsonValue]
    """Arguments to function."""
    view: ToolCallContent | None = Field(default=None)
    """Custom view of tool call input."""
    result: ToolResult = Field(default_factory=str)
    """Function return value."""
    truncated: tuple[int, int] | None = Field(default=None)
    """Bytes truncated (from,to) if truncation occurred"""
    error: ToolCallError | None = Field(default=None)
    """Error that occurred during tool call."""
    events: list["Event"] = Field(default_factory=list)
    """Transcript of events for tool."""
    completed: datetime | None = Field(default=None)
    """Time that tool call completed (see `timestamp` for started)"""
    working_time: float | None = Field(default=None)
    """Working time for tool call (i.e. time not spent waiting on semaphores)."""
    def _set_result(
        self,
        result: ToolResult,
        truncated: tuple[int, int] | None,
        error: ToolCallError | None,
        events: list["Event"],
        waiting_time: float,
    ) -> None:
        self.result = result
        self.truncated = truncated
        self.error = error
        self.events = events
        self.pending = None
        completed = datetime.now()
        self.completed = completed
        self.working_time = (completed - self.timestamp).total_seconds() - waiting_time
    # mechanism for operator to cancel the tool call
    def _set_cancel_fn(self, cancel_fn: Callable[[], None]) -> None:
        """Set the tool task (for possible cancellation)"""
        self._cancel_fn = cancel_fn
    def _cancel(self) -> None:
        """Cancel the tool task."""
        if self._cancel_fn and not self.cancelled:
            self._cancelled = True
            self._cancel_fn()
    @property
    def cancelled(self) -> bool:
        """Was the task cancelled?"""
        return self._cancelled is True
    _cancelled: bool | None = None
    """Was this tool call cancelled?"""
    _cancel_fn: Callable[[], None] | None = None
    """Function which can be used to cancel the tool call."""
    model_config = ConfigDict(arbitrary_types_allowed=True)
    """Required so that we can include '_cancel_fn' as a member."""
    @field_serializer("completed")
    def serialize_completed(self, dt: datetime) -> str:
        return dt.astimezone().isoformat()
class SandboxEvent(BaseEvent):
    """Sandbox execution or I/O"""
    event: Literal["sandbox"] = Field(default="sandbox")
    """Event type"""
    action: Literal["exec", "read_file", "write_file"]
    """Sandbox action"""
    cmd: str | None = Field(default=None)
    """Command (for exec)"""
    options: dict[str, JsonValue] | None = Field(default=None)
    """Options (for exec)"""
    file: str | None = Field(default=None)
    """File (for read_file and write_file)"""
    input: str | None = Field(default=None)
    """Input (for cmd and write_file). Truncated to 100 lines."""
    result: int | None = Field(default=None)
    """Result (for exec)"""
    output: str | None = Field(default=None)
    """Output (for exec and read_file). Truncated to 100 lines."""
    completed: datetime | None = Field(default=None)
    """Time that sandbox action completed (see `timestamp` for started)"""
    @field_serializer("completed")
    def serialize_completed(self, dt: datetime) -> str:
        return dt.astimezone().isoformat()
class ApprovalEvent(BaseEvent):
    """Tool approval."""
    event: Literal["approval"] = Field(default="approval")
    """Event type"""
    message: str
    """Message generated by model along with tool call."""
    call: ToolCall
    """Tool call being approved."""
    view: ToolCallView | None = Field(default=None)
    """View presented for approval."""
    approver: str
    """Aprover name."""
    decision: Literal["approve", "modify", "reject", "escalate", "terminate"]
    """Decision of approver."""
    modified: ToolCall | None = Field(default=None)
    """Modified tool call for decision 'modify'."""
    explanation: str | None = Field(default=None)
    """Explanation for decision."""
class InputEvent(BaseEvent):
    """Input screen interaction."""
    event: Literal["input"] = Field(default="input")
    """Event type."""
    input: str
    """Input interaction (plain text)."""
    input_ansi: str
    """Input interaction (ANSI)."""
class LoggerEvent(BaseEvent):
    """Log message recorded with Python logger."""
    event: Literal["logger"] = Field(default="logger")
    """Event type."""
    message: LoggingMessage
    """Logging message"""
class InfoEvent(BaseEvent):
    """Event with custom info/data."""
    event: Literal["info"] = Field(default="info")
    """Event type."""
    source: str | None = Field(default=None)
    """Optional source for info event."""
    data: JsonValue
    """Data provided with event."""
class ErrorEvent(BaseEvent):
    """Event with sample error."""
    event: Literal["error"] = Field(default="error")
    """Event type."""
    error: EvalError
    """Sample error"""
class ScoreEvent(BaseEvent):
    """Event with score.
    Can be the final score for a `Sample`, or can be an intermediate score
    resulting from a call to `score`.
    """
    event: Literal["score"] = Field(default="score")
    """Event type."""
    score: Score
    """Score value."""
    target: str | list[str] | None = Field(default=None)
    """"Sample target."""
    intermediate: bool = Field(default=False)
    """Was this an intermediate scoring?"""
class StepEvent(BaseEvent):
    """Step within current sample or subtask."""
    event: Literal["step"] = Field(default="step")
    """Event type."""
    action: Literal["begin", "end"]
    """Designates beginning or end of event."""
    type: str | None = Field(default=None)
    """Optional 'type' field for events"""
    name: str
    """Event name."""
class SubtaskEvent(BaseEvent):
    """Subtask spawned."""
    event: Literal["subtask"] = Field(default="subtask")
    """Event type."""
    name: str
    """Name of subtask function."""
    type: str | None = Field(default=None)
    """Type of subtask"""
    input: dict[str, Any]
    """Subtask function inputs."""
    result: Any = Field(default=None)
    """Subtask function result."""
    events: list["Event"] = Field(default_factory=list)
    """Transcript of events for subtask."""
    completed: datetime | None = Field(default=None)
    """Time that subtask completed (see `timestamp` for started)"""
    working_time: float | None = Field(default=None)
    """Working time for subtask (i.e. time not spent waiting on semaphores or model retries)."""
    @field_serializer("completed")
    def serialize_completed(self, dt: datetime) -> str:
        return dt.astimezone().isoformat()
Event: TypeAlias = Union[
    SampleInitEvent
    | SampleLimitEvent
    | SandboxEvent
    | StateEvent
    | StoreEvent
    | ModelEvent
    | ToolEvent
    | SandboxEvent
    | ApprovalEvent
    | InputEvent
    | ScoreEvent
    | ErrorEvent
    | LoggerEvent
    | InfoEvent
    | StepEvent
    | SubtaskEvent,
]
"""Event in a transcript."""
ET = TypeVar("ET", bound=BaseEvent)
class Transcript:
    """Transcript of events."""
    def __init__(self, name: str = "") -> None:
        self.name = name
        self._events: list[Event] = []
    def info(self, data: JsonValue, *, source: str | None = None) -> None:
        """Add an `InfoEvent` to the transcript.
        Args:
           data: Data associated with the event.
           source: Optional event source.
        """
        self._event(InfoEvent(source=source, data=data))
    @contextlib.contextmanager
    def step(self, name: str, type: str | None = None) -> Iterator[None]:
        """Context manager for recording StepEvent.
        Args:
            name (str): Step name.
            type (str | None): Optional step type.
        """
        # step event
        self._event(StepEvent(action="begin", name=name, type=type))
        # run the step (tracking state/store changes)
        with track_state_changes(type), track_store_changes():
            yield
        # end step event
        self._event(StepEvent(action="end", name=name, type=type))
    @property
    def events(self) -> Sequence[Event]:
        return self._events
    def find_last_event(self, event_cls: Type[ET]) -> ET | None:
        for event in reversed(self.events):
            if isinstance(event, event_cls):
                return event
        return None
    def _event(self, event: Event) -> None:
        self._events.append(event)
def transcript() -> Transcript:
    """Get the current `Transcript`."""
    return _transcript.get()
@contextlib.contextmanager
def track_store_changes() -> Iterator[None]:
    before = store_jsonable(store())
    yield
    after = store_jsonable(store())
    changes = store_changes(before, after)
    if changes:
        transcript()._event(StoreEvent(changes=changes))
@contextlib.contextmanager
def track_state_changes(type: str | None = None) -> Iterator[None]:
    # we only want to track for step() inside the the sample
    # (solver level tracking is handled already and there are
    # no state changes in subtasks)
    if transcript().name == SAMPLE_SUBTASK and type != "solver":
        before = state_jsonable()
        yield
        after = state_jsonable()
        changes = json_changes(before, after)
        if changes:
            transcript()._event(StateEvent(changes=changes))
    else:
        yield
def init_transcript(transcript: Transcript) -> None:
    _transcript.set(transcript)
_transcript: ContextVar[Transcript] = ContextVar(
    "subtask_transcript", default=Transcript()
)

================
File: src/inspect_ai/model/__init__.py
================
# ruff: noqa: F401 F403 F405
from inspect_ai._util.content import (
    Content,
    ContentAudio,
    ContentImage,
    ContentReasoning,
    ContentText,
    ContentVideo,
)
from inspect_ai._util.deprecation import relocated_module_attribute
from ._cache import (
    CachePolicy,
    cache_clear,
    cache_list_expired,
    cache_path,
    cache_prune,
    cache_size,
)
from ._call_tools import call_tools
from ._chat_message import (
    ChatMessage,
    ChatMessageAssistant,
    ChatMessageBase,
    ChatMessageSystem,
    ChatMessageTool,
    ChatMessageUser,
)
from ._generate_config import GenerateConfig, GenerateConfigArgs, ResponseSchema
from ._model import (
    Model,
    ModelAPI,
    ModelName,
    get_model,
)
from ._model_output import (
    ChatCompletionChoice,
    Logprob,
    Logprobs,
    ModelOutput,
    ModelUsage,
    StopReason,
    TopLogprob,
)
from ._providers.providers import *
from ._registry import modelapi
__all__ = [
    "GenerateConfig",
    "GenerateConfigArgs",
    "ResponseSchema",
    "CachePolicy",
    "ContentAudio",
    "ContentImage",
    "ContentReasoning",
    "ContentText",
    "ContentVideo",
    "Content",
    "ChatMessage",
    "ChatMessageBase",
    "ChatMessageSystem",
    "ChatMessageUser",
    "ChatMessageAssistant",
    "ChatMessageTool",
    "ChatCompletionChoice",
    "ModelOutput",
    "Logprobs",
    "Logprob",
    "TopLogprob",
    "Model",
    "ModelAPI",
    "ModelName",
    "ModelUsage",
    "StopReason",
    "call_tools",
    "cache_clear",
    "cache_list_expired",
    "cache_path",
    "cache_prune",
    "cache_size",
    "get_model",
    "modelapi",
]
_TOOL_MODULE_VERSION = "0.3.18"
_REMOVED_IN = "0.4"
relocated_module_attribute(
    "ToolCall", "inspect_ai.tool.ToolCall", _TOOL_MODULE_VERSION, _REMOVED_IN
)
relocated_module_attribute(
    "ToolChoice", "inspect_ai.tool.ToolChoice", _TOOL_MODULE_VERSION, _REMOVED_IN
)
relocated_module_attribute(
    "ToolFunction", "inspect_ai.tool.ToolFunction", _TOOL_MODULE_VERSION, _REMOVED_IN
)
relocated_module_attribute(
    "ToolParam", "inspect_ai.tool.ToolParam", _TOOL_MODULE_VERSION, _REMOVED_IN
)
relocated_module_attribute(
    "ToolInfo", "inspect_ai.tool.ToolInfo", _TOOL_MODULE_VERSION, _REMOVED_IN
)

================
File: src/inspect_ai/model/_cache.py
================
import logging
import os
import pickle
from contextvars import ContextVar
from datetime import datetime, timezone
from hashlib import md5
from pathlib import Path
from shutil import rmtree
from typing import Any
from dateutil.relativedelta import relativedelta
from inspect_ai._util.appdirs import inspect_cache_dir
from inspect_ai._util.trace import trace_message
from inspect_ai.tool import ToolChoice, ToolInfo
from ._chat_message import ChatMessage
from ._generate_config import GenerateConfig
from ._model_output import ModelOutput
logger = logging.getLogger(__name__)
def trace(msg: str, *args: Any) -> None:
    trace_message(logger, "Cache", msg, *args)
def _path_is_in_cache(path: Path | str) -> bool:
    """This ensures the path is in our cache directory, just in case the `model` is ../../../home/ubuntu/maliciousness"""
    if isinstance(path, str):
        path = Path(path)
    return cache_path() in Path(os.path.normpath(path)).parents
def _parse_expiry(period: str) -> int:
    """Returns the number of seconds in the period where period is a string of the format "12h" or "1W" etc."""
    factor = period[-1]
    match factor:
        case "s":
            return int(period[:-1])
        case "m":
            return int(period[:-1]) * 60
        case "h":
            return int(period[:-1]) * 60 * 60
        case "D":
            return int(period[:-1]) * 60 * 60 * 24
        case "W":
            return int(period[:-1]) * 60 * 60 * 24 * 7
        case "M":
            return int(period[:-1]) * 60 * 60 * 24 * 30
        case "Y":
            return int(period[:-1]) * 60 * 60 * 24 * 365
        case _:
            raise ValueError(f"Invalid expiry: {period}")
class CachePolicy:
    """The `CachePolicy` is used to define various criteria that impact how model calls are cached.
    `expiry`: Default "24h". The expiry time for the cache entry.
    This is a string of the format "12h" for 12 hours or "1W" for a week,
    etc. This is how long we will keep the cache entry, if we access it
    after this point we'll clear it. Setting to `None` will cache
    indefinitely.
    `per_epoch`: Default True. By default we cache responses separately
    for different epochs. The general use case is that if there are
    multiple epochs, we should cache each response separately because
    scorers will aggregate across epochs. However, sometimes a response
    can be cached regardless of epoch if the call being made isn't under
    test as part of the evaluation. If False, this option allows you to
    bypass that and cache independently of the epoch.
    `scopes`: A dictionary of additional metadata that should
    be included in the cache key. This allows for more fine-grained
    control over the cache key generation.
    """
    def __init__(
        self,
        expiry: str | None = "1W",
        per_epoch: bool = True,
        scopes: dict[str, str] = {},
    ) -> None:
        """Create a CachePolicy.
        Args:
           expiry: Expiry.
           per_epoch: Per epoch
           scopes: Scopes
        """
        self.per_epoch = per_epoch
        self.scopes = scopes
        if expiry is None:
            self.expiry = None
        else:
            self.expiry = _parse_expiry(expiry)
# The `epoch` is an essential part of the cache key for `generate` call. When
# calling with multiple epochs we are essentially making *exactly the same call*
# multiple times, so all other fields will be the same.
#
# Abstracting this to a ContextVar allows us to set it at the execution stage
# and access it safely where we're fetching/storing from the cache without
# adding more noise to the call stack.
epoch: ContextVar[int] = ContextVar("epoch")
class CacheEntry:
    """
    This is used to orchestrate the caching of ModelOutput from the model.
    All the attributes are used to generate a unique key for each cache entry.
    Attributes:
        base_url(str | None): The base URL of the model API, if any.
        config(GenerateConfig): The configuration used to generate the output.
        input(list[ChatMessage]): The messages exchanged with the model.
        model(str): The model name.
        policy(CachePolicy): The `CachePolicy` defining things like additional
          metadata for the cache key, expiry time, and other settings which
          affect all individual cached entries.
        tool_choice(ToolChoice | None): The tool choice, if any.
        tools(list[ToolInfo]): The tools provided to the model.
    """
    def __init__(
        self,
        base_url: str | None,
        config: GenerateConfig,
        input: list[ChatMessage],
        model: str,
        policy: CachePolicy,
        tool_choice: ToolChoice | None,
        tools: list[ToolInfo],
    ):
        self.base_url = base_url
        self.config = config
        self.input = input
        self.model = model
        self.tool_choice = tool_choice
        self.tools = tools
        self.policy = policy
def _cache_key(entry: CacheEntry) -> str:
    components = [
        (
            entry.config.model_dump(
                # Exclude fields that don't affect the model output
                exclude=set(["max_retries", "timeout", "max_connections"])
            )
        ),
        ",".join(
            [str(message.model_dump(exclude=set(["id"]))) for message in entry.input]
        ),
        entry.base_url,
        entry.tool_choice,
        entry.tools,
        entry.policy.expiry,
        entry.policy.scopes,
    ]
    if entry.policy.per_epoch:
        components.append(epoch.get(None))
    base_string = "|".join([str(component) for component in components])
    trace(_cache_key_debug_string([str(component) for component in components]))
    return md5(base_string.encode("utf-8")).hexdigest()
def _cache_key_debug_string(components: list[str]) -> str:
    components_str = "\n".join(f"  - {component}" for component in components)
    return f"Computed cache key from components:\n{components_str}"
def _cache_expiry(policy: CachePolicy) -> datetime | None:
    if policy.expiry is None:
        return None
    expiry_time: datetime = datetime.now(timezone.utc) + relativedelta(
        seconds=policy.expiry
    )
    return expiry_time
def _is_expired(expiry: datetime | None) -> bool:
    if expiry is None:
        return False
    return datetime.now(timezone.utc) > expiry
def cache_store(
    entry: CacheEntry,
    output: ModelOutput,
) -> bool:
    """Cache a value in the cache directory."""
    filename = cache_path(model=entry.model) / _cache_key(entry)
    try:
        filename.parent.mkdir(parents=True, exist_ok=True)
        with open(filename, "wb") as f:
            expiry = _cache_expiry(entry.policy)
            trace("Storing in cache: %s (expires: %s)", filename, expiry)
            pickle.dump((expiry, output), f)
        return True
    except Exception as e:
        trace(f"Failed to cache {filename}: {e}")
        return False
def cache_fetch(entry: CacheEntry) -> ModelOutput | None:
    """Fetch a value from the cache directory."""
    filename = cache_path(model=entry.model) / _cache_key(entry)
    try:
        trace("Fetching from cache: %s", filename)
        with open(filename, "rb") as f:
            expiry, output = pickle.load(f)
            if not isinstance(output, ModelOutput):
                trace(
                    "Unexpected cached type, can only fetch ModelOutput: %s (%s)",
                    type(output),
                    filename,
                )
                return None
            if _is_expired(expiry):
                trace("Cache expired for %s (%s)", filename, expiry)
                # If it's expired, no point keeping it as we'll never access it
                # successfully again.
                filename.unlink(missing_ok=True)
                return None
            return output
    except Exception as e:
        trace(f"Failed to fetch from cache {filename}: {e}")
        return None
def cache_clear(model: str = "") -> bool:
    """Clear the cache directory.
    Args:
       model: Model to clear cache for.
    """
    try:
        path = cache_path(model)
        if (model == "" or _path_is_in_cache(path)) and path.exists():
            trace("Clearing cache: %s", path)
            rmtree(path)
            return True
        return False
    except Exception as e:
        logger.error(f"Failed to clear cache: {e}")
        return False
def cache_path(model: str = "") -> Path:
    """Path to cache directory.
    Args:
       model: Path to cache directory for specific model.
    """
    env_cache_dir = os.environ.get("INSPECT_CACHE_DIR", None)
    if env_cache_dir:
        generate_cache = Path(env_cache_dir) / "generate"
        generate_cache.mkdir(parents=True, exist_ok=True)
    else:
        generate_cache = inspect_cache_dir("generate")
    if model:
        return generate_cache / model
    else:
        return generate_cache
def _cache_size_directories_only(filter_by: list[str]) -> list[tuple[str, int]]:
    root = cache_path()
    non_empty_directories = []
    for dirpath, _dirnames, filenames in os.walk(root):
        if not filenames:
            # Empty directory or just directories, carry on searching
            continue
        if not filter_by:
            # No filtering, so we want all directories
            non_empty_directories.append(dirpath)
            continue
        filtered_path = any(model in str(dirpath) for model in filter_by)
        if filtered_path:
            non_empty_directories.append(dirpath)
    models_with_sizes = []
    for directory in non_empty_directories:
        model_name = directory.replace(f"{root}/", "")
        size = sum(
            f.stat().st_size for f in Path(directory).glob("**/*") if f.is_file()
        )
        models_with_sizes.append((model_name, size))
    return models_with_sizes
def _cache_size_files_only(files: list[Path]) -> list[tuple[str, int]]:
    if not files:
        return []
    directories: dict[str, int] = {}
    root = str(cache_path())
    for file in files:
        if root in str(file) and file.exists():
            model = str(file.parent).replace(f"{root}/", "")
            if directories.get(model):
                directories[model] += file.stat().st_size
            else:
                directories[model] = file.stat().st_size
    return [(path, size) for path, size in directories.items()]
def cache_size(
    subdirs: list[str] = [], files: list[Path] = []
) -> list[tuple[str, int]]:
    """Calculate the size of various cached directories and files
    If neither  `subdirs` nor `files` are provided, the entire cache directory
    will be calculated.
    Args:
        subdirs: List of folders to filter by, which are generally
            model names. Empty directories will be ignored.
        files: List of files to filter by explicitly. Note that
            return value group these up by their parent directory
    Returns:
        list[tuple[str, int]]: List of tuples containing the model name and the
            size of the cache in bytes
    """
    if files and not subdirs:
        # This prevents us accidentally working out the cache for all paths when
        # we've just been given a list of files
        subdir_sizes = []
    else:
        subdir_sizes = _cache_size_directories_only(filter_by=subdirs)
    file_sizes = _cache_size_files_only(files=files)
    return sorted(subdir_sizes + file_sizes, key=lambda size: size[0])
def cache_list_expired(filter_by: list[str] = []) -> list[Path]:
    """Returns a list of all the cached files that have passed their expiry time.
    Args:
        filter_by: Default []. List of model names to filter by. If
            an empty list, this will search the entire cache.
    """
    expired_cache_entries = []
    filter_by_paths = [
        cache_path(model) for model in filter_by if _path_is_in_cache(cache_path(model))
    ]
    if filter_by and not filter_by_paths:
        # An edge case where all the paths we get are invalid ones (e.g.
        # "../../foo/bar") but we don't want to search the entire cache
        return []
    trace("Filtering by paths: %s", filter_by_paths)
    for dirpath, _dirnames, filenames in os.walk(cache_path()):
        if filter_by_paths and Path(dirpath) not in filter_by_paths:
            trace("Skipping path %s", dirpath)
            continue
        trace("Checking dirpath %s", dirpath)
        for filename in filenames:
            path = Path(dirpath) / filename
            trace("Checking path %s", path)
            try:
                with open(path, "rb") as f:
                    expiry, _cache_entry = pickle.load(f)
                    if _is_expired(expiry):
                        trace("Expired cache entry found: %s (%s)", path, expiry)
                        expired_cache_entries.append(path)
            except Exception as e:
                trace("Failed to load cached item %s: %s", path, e)
                continue
    return expired_cache_entries
def cache_prune(files: list[Path] = []) -> None:
    """Delete all expired cache entries.
    Args:
        files: List of files to prune. If empty, this
            will search the entire cache.
    """
    if not files:
        files = cache_list_expired()
    for file in files:
        try:
            with open(file, "rb") as f:
                expiry, _cache_entry = pickle.load(f)
                if _is_expired(expiry):
                    trace("Pruning expired cache: %s", file)
                    file.unlink(missing_ok=True)
        except Exception as e:
            trace("Failed to prune cache %s: %s", file, e)
            continue

================
File: src/inspect_ai/model/_call_tools.py
================
import inspect
import json
import sys
import types
from dataclasses import is_dataclass
from logging import getLogger
from textwrap import dedent
from types import UnionType
from typing import (
    Any,
    Callable,
    Dict,
    List,
    NamedTuple,
    Optional,
    Tuple,
    Type,
    Union,
    get_args,
    get_origin,
    get_type_hints,
    is_typeddict,
)
if sys.version_info < (3, 11):
    from exceptiongroup import ExceptionGroup
import anyio
import yaml
from anyio.streams.memory import MemoryObjectSendStream
from jsonschema import Draft7Validator
from pydantic import BaseModel
from inspect_ai._util.content import (
    Content,
    ContentAudio,
    ContentImage,
    ContentText,
    ContentVideo,
)
from inspect_ai._util.format import format_function_call
from inspect_ai._util.text import truncate_string_to_bytes
from inspect_ai._util.trace import trace_action
from inspect_ai._util.working import sample_waiting_time
from inspect_ai.model._conversation import conversation_tool_mesage
from inspect_ai.tool import Tool, ToolCall, ToolError, ToolInfo
from inspect_ai.tool._tool import ToolApprovalError, ToolParsingError
from inspect_ai.tool._tool_call import ToolCallContent, ToolCallError
from inspect_ai.tool._tool_def import ToolDef, tool_defs
from inspect_ai.tool._tool_info import parse_docstring
from inspect_ai.tool._tool_params import ToolParams
from inspect_ai.util import OutputLimitExceededError
from ._chat_message import ChatMessageAssistant, ChatMessageTool
from ._generate_config import active_generate_config
logger = getLogger(__name__)
async def call_tools(
    message: ChatMessageAssistant,
    tools: list[Tool] | list[ToolDef] | list[Tool | ToolDef],
    max_output: int | None = None,
) -> list[ChatMessageTool]:
    """Perform tool calls in assistant message.
    Args:
       message (ChatMessageAssistant): Assistant message
       tools (list[Tool]): Available tools
       max_output (int | None): Maximum output length (in bytes).
          Defaults to max_tool_output from active GenerateConfig
          (16 * 1024 by default).
    Returns:
       List of tool calls
    """
    if message.tool_calls:
        from inspect_ai.log._transcript import (
            ToolEvent,
            Transcript,
            init_transcript,
            track_store_changes,
            transcript,
        )
        tdefs = tool_defs(tools)
        async def call_tool_task(
            call: ToolCall,
            send_stream: MemoryObjectSendStream[tuple[ChatMessageTool, ToolEvent]],
        ) -> None:
            # create a transript for this call
            init_transcript(Transcript(name=call.function))
            result: Any = ""
            tool_error: ToolCallError | None = None
            try:
                with track_store_changes():
                    result = await call_tool(tdefs, message.text, call)
            except TimeoutError:
                tool_error = ToolCallError(
                    "timeout", "Command timed out before completing."
                )
            except UnicodeDecodeError as ex:
                tool_error = ToolCallError(
                    "unicode_decode",
                    f"Error decoding bytes to {ex.encoding}: {ex.reason}",
                )
            except PermissionError as ex:
                err = f"{ex.strerror}."
                if isinstance(ex.filename, str):
                    err = f"{err} Filename '{ex.filename}'."
                tool_error = ToolCallError("permission", err)
            except FileNotFoundError as ex:
                tool_error = ToolCallError(
                    "file_not_found",
                    f"File '{ex.filename}' was not found.",
                )
            except IsADirectoryError as ex:
                err = f"{ex.strerror}."
                if isinstance(ex.filename, str):
                    err = f"{err} Filename '{ex.filename}'."
                tool_error = ToolCallError("is_a_directory", err)
            except OutputLimitExceededError as ex:
                tool_error = ToolCallError(
                    "output_limit",
                    f"The tool output limit of {ex.limit_str} was exceeded.",
                )
                result = ex.truncated_output or ""
            except ToolParsingError as ex:
                tool_error = ToolCallError("parsing", ex.message)
            except ToolApprovalError as ex:
                tool_error = ToolCallError("approval", ex.message)
            except ToolError as ex:
                tool_error = ToolCallError("unknown", ex.message)
            # massage result, leave list[Content] alone, convert all other
            # types to string as that is what the model APIs accept
            truncated: tuple[int, int] | None = None
            if isinstance(
                result, ContentText | ContentImage | ContentAudio | ContentVideo
            ):
                content: str | list[Content] = [result]
            elif isinstance(result, list) and (
                len(result) == 0
                or isinstance(
                    result[0], ContentText | ContentImage | ContentAudio | ContentVideo
                )
            ):
                content = result
            else:
                content = str(result)
                # truncate if necessary
                truncated_output = truncate_tool_output(
                    call.function, content, max_output
                )
                if truncated_output:
                    content = truncated_output.output
                    truncated = (
                        truncated_output.raw_bytes,
                        truncated_output.truncated_bytes,
                    )
            # create event
            event = ToolEvent(
                id=call.id,
                function=call.function,
                arguments=call.arguments,
                result=content,
                truncated=truncated,
                view=call.view,
                error=tool_error,
                events=list(transcript().events),
            )
            # yield message and event
            async with send_stream:
                await send_stream.send(
                    (
                        ChatMessageTool(
                            content=content,
                            tool_call_id=call.id,
                            function=call.function,
                            error=tool_error,
                        ),
                        event,
                    )
                )
        # call tools
        tool_messages: list[ChatMessageTool] = []
        for call in message.tool_calls:
            # create pending tool event and add it to the transcript
            # (record the waiting time for the sample so we can compare
            # it at the end to deduce total waiting time inside the tool
            # call (in turn used to calculate working time)
            waiting_time_start = sample_waiting_time()
            event = ToolEvent(
                id=call.id,
                function=call.function,
                arguments=call.arguments,
                view=call.view,
                pending=True,
            )
            transcript()._event(event)
            # execute the tool call. if the operator cancels the
            # tool call then synthesize the appropriate message/event
            send_stream, receive_stream = anyio.create_memory_object_stream[
                tuple[ChatMessageTool, ToolEvent]
            ]()
            try:
                async with anyio.create_task_group() as tg:
                    tg.start_soon(call_tool_task, call, send_stream)
                    event._set_cancel_fn(tg.cancel_scope.cancel)
                    async with receive_stream:
                        async for result in receive_stream:
                            tool_message, result_event = result
                            break
            except ExceptionGroup as ex:
                raise ex.exceptions[0]
            if event.cancelled:
                tool_message = ChatMessageTool(
                    content="",
                    function=call.function,
                    tool_call_id=call.id,
                    error=ToolCallError(
                        "timeout", "Command timed out before completing."
                    ),
                )
                result_event = ToolEvent(
                    id=call.id,
                    function=call.function,
                    arguments=call.arguments,
                    result=tool_message.content,
                    truncated=None,
                    view=call.view,
                    error=tool_message.error,
                    events=[],
                )
                transcript().info(
                    f"Tool call '{call.function}' was cancelled by operator."
                )
            # update return messages
            tool_messages.append(tool_message)
            # print conversation if display is conversation
            conversation_tool_mesage(tool_message)
            # update the event with the results
            waiting_time_end = sample_waiting_time()
            event._set_result(
                result=result_event.result,
                truncated=result_event.truncated,
                error=result_event.error,
                events=result_event.events,
                waiting_time=waiting_time_end - waiting_time_start,
            )
        # return tool messages
        return tool_messages
    else:
        return []
async def call_tool(tools: list[ToolDef], message: str, call: ToolCall) -> Any:
    # if there was an error parsing the ToolCall, raise that
    if call.parse_error:
        raise ToolParsingError(call.parse_error)
    # find the tool
    tool_def = next((tool for tool in tools if tool.name == call.function), None)
    if tool_def is None:
        raise ToolParsingError(f"Tool {call.function} not found")
    # if we have a tool approver, apply it now
    from inspect_ai.approval._apply import apply_tool_approval
    approved, approval = await apply_tool_approval(message, call, tool_def.viewer)
    if not approved:
        raise ToolApprovalError(approval.explanation if approval else None)
    if approval and approval.modified:
        call = approval.modified
    # validate the schema of the passed object
    validation_errors = validate_tool_input(call.arguments, tool_def.parameters)
    if validation_errors:
        raise ToolParsingError(validation_errors)
    # get arguments (with creation of dataclasses, pydantic objects, etc.)
    arguments = tool_params(call.arguments, tool_def.tool)
    # call the tool
    with trace_action(
        logger, "Tool Call", format_function_call(tool_def.name, arguments, width=1000)
    ):
        result = await tool_def.tool(**arguments)
    # return result
    return result
def tools_info(
    tools: list[Tool]
    | list[ToolDef]
    | list[ToolInfo]
    | list[Tool | ToolDef | ToolInfo],
) -> list[ToolInfo]:
    tools_info: list[ToolInfo] = []
    for tool in tools:
        if isinstance(tool, ToolInfo):
            tools_info.append(tool)
        else:
            if isinstance(tool, Tool):
                tool = ToolDef(tool)
            tools_info.append(
                ToolInfo(
                    name=tool.name,
                    description=tool.description,
                    parameters=tool.parameters,
                )
            )
    return tools_info
def disable_parallel_tools(
    tools: list[Tool]
    | list[ToolDef]
    | list[ToolInfo]
    | list[Tool | ToolDef | ToolInfo],
) -> bool:
    for tool in tools:
        if isinstance(tool, Tool):
            tool = ToolDef(tool)
        if isinstance(tool, ToolDef) and not tool.parallel:
            return True
    return False
def type_hint_includes_none(type_hint: Type[Any] | None) -> bool:
    origin = get_origin(type_hint)
    if origin in {Union, UnionType}:
        return type(None) in get_args(type_hint)
    elif origin is Optional:
        return True
    return False
def tool_params(input: dict[str, Any], func: Callable[..., Any]) -> dict[str, Any]:
    # parse function typeinfo
    signature = inspect.signature(func)
    type_hints = get_type_hints(func)
    docstring = inspect.getdoc(func)
    # if the function takes **kwargs: Any then just pass the tool arguments through
    if "kwargs" in type_hints and type_hints["kwargs"] == Any:
        return input
    # build params
    params: dict[str, Any] = {}
    for param_name, param in signature.parameters.items():
        # Parse docstring
        docstring_info = parse_docstring(docstring, param_name)
        # get type hint (fallback to docstring as required)
        type_hint: Type[Any] | None = None
        if param_name in type_hints:
            type_hint = type_hints[param_name]
        # as a fallback try to parse it from the docstring
        elif "docstring_type" in docstring_info:
            docstring_type = docstring_info["docstring_type"]
            type_hint = getattr(__builtins__, docstring_type, None)
        # error if there is no type_hint
        if type_hint is None:
            raise ValueError(f"No type annotation available for parameter {param_name}")
        # yield parameter (fail if not passed and there is no default)
        if param_name in input:
            params[param_name] = tool_param(type_hint, input.get(param_name))
        elif param.default is not None or type_hint_includes_none(type_hint):
            params[param_name] = param.default
        else:
            raise ToolParsingError(
                f"Required parameter {param_name} not provided to tool call."
            )
    return params
def tool_param(type_hint: Type[Any], input: Any) -> Any:
    origin = get_origin(type_hint)
    args = get_args(type_hint)
    if origin is None:
        if type_hint in [int, str, float, bool]:
            try:
                return type_hint(input)
            except (ValueError, TypeError):
                raise ToolParsingError(
                    f"Unable to convert '{input}' to {type_hint.__name__}"
                )
        elif is_typeddict(type_hint):
            typeddict_data: dict[str, Any] = {}
            annotations = get_type_hints(type_hint)
            for name, hint in annotations.items():
                typeddict_data[name] = tool_param(hint, input.get(name))
            return typeddict_data
        elif is_dataclass(type_hint):
            dataclass_data: dict[str, Any] = {}
            fields = type_hint.__dataclass_fields__  # type: ignore
            for name, field in fields.items():
                dataclass_data[name] = tool_param(field.type, input.get(name))  # type: ignore
            return type_hint(**dataclass_data)
        elif issubclass(type_hint, BaseModel):
            return type_hint(**input)
        else:
            return input
    elif origin is list or origin is List:
        if args:
            return [tool_param(args[0], x) for x in input]
        else:
            return input
    elif origin is tuple or origin is Tuple:
        if args:
            return tuple([tool_param(args[0], x) for x in input])
        else:
            return tuple(input)
    elif origin is dict or origin is Dict:
        if args and len(args) > 1:
            return {k: tool_param(args[1], v) for k, v in input.items()}
        else:
            return input
    elif origin is Union or origin is types.UnionType:
        if args[1] is type(None):
            return tool_param(args[0], input)
        else:
            return input
    else:
        return input
def tool_call_view(call: ToolCall, tdefs: list[ToolDef]) -> ToolCallContent | None:
    tool_def = next((tool for tool in tdefs if tool.name == call.function), None)
    if tool_def and tool_def.viewer:
        return tool_def.viewer(call).call
    else:
        return None
def validate_tool_input(input: dict[str, Any], parameters: ToolParams) -> str | None:
    schema = parameters.model_dump(exclude_none=True)
    validator = Draft7Validator(schema)
    errors = list(validator.iter_errors(input))
    if errors:
        message = "\n".join(
            [f"Found {len(errors)} validation errors parsing tool input arguments:"]
            + [f"- {error.message}" for error in errors]
        )
        return message
    else:
        return None
class TruncatedToolOutput(NamedTuple):
    output: str
    raw_bytes: int
    truncated_bytes: int
def truncate_tool_output(
    tool_name: str, output: str, max_output: int | None
) -> TruncatedToolOutput | None:
    # determine active max output
    active_max_output = max_output
    if active_max_output is None:
        active_max_output = active_generate_config().max_tool_output
        if active_max_output is None:
            active_max_output = 16 * 1024
    # truncate if required
    truncated = truncate_string_to_bytes(output, active_max_output)
    if truncated:
        truncated_output = dedent("""
            The output of your call to {tool_name} was too long to be displayed.
            Here is a truncated version:
            <START_TOOL_OUTPUT>
            {truncated_output}
            <END_TOOL_OUTPUT>
            """).format(tool_name=tool_name, truncated_output=truncated.output)
        return TruncatedToolOutput(
            truncated_output, truncated.original_bytes, active_max_output
        )
    else:
        return None
def tool_parse_error_message(arguments: str, ex: Exception) -> str:
    return f"Error parsing the following tool call arguments:\n\n{arguments}\n\nError details: {ex}"
def parse_tool_call(
    id: str, function: str, arguments: str, tools: list[ToolInfo] | None = None
) -> ToolCall:
    error: str | None = None
    arguments_dict: dict[str, Any] = {}
    def report_parse_error(ex: Exception) -> None:
        nonlocal error
        error = tool_parse_error_message(arguments, ex)
        logger.info(error)
    # if the arguments is a dict, then handle it with a plain json.loads
    arguments = arguments.strip()
    if arguments.startswith("{"):
        try:
            arguments_dict = json.loads(arguments)
        except json.JSONDecodeError as ex:
            report_parse_error(ex)
    # otherwise parse it as yaml (which will pickup unquoted strings, numbers, and true/false)
    # and then create a dict that maps it to the first function argument
    elif function and tools:
        tool_info = next(
            (
                tool
                for tool in tools
                if tool.name == function and len(tool.parameters.properties) > 0
            ),
            None,
        )
        if tool_info:
            param_names = list(tool_info.parameters.properties.keys())
            try:
                value = yaml.safe_load(arguments)
                arguments_dict[param_names[0]] = value
            except yaml.error.YAMLError:
                # If the yaml parser fails, we treat it as a string argument.
                arguments_dict[param_names[0]] = arguments
    # return ToolCall with error payload
    return ToolCall(
        id=id,
        function=function,
        arguments=arguments_dict,
        type="function",
        parse_error=error,
    )

================
File: src/inspect_ai/model/_chat_message.py
================
from logging import getLogger
from typing import Any, Literal, Type, Union
from pydantic import BaseModel, Field, model_validator
from shortuuid import uuid
from inspect_ai._util.constants import DESERIALIZING
from inspect_ai._util.content import Content, ContentReasoning, ContentText
from inspect_ai.tool import ToolCall
from inspect_ai.tool._tool_call import ToolCallError
from ._reasoning import parse_content_with_reasoning
logger = getLogger(__name__)
class ChatMessageBase(BaseModel):
    """Base class for chat messages."""
    id: str | None = Field(default=None)
    """Unique identifer for message."""
    content: str | list[Content]
    """Content (simple string or list of content objects)"""
    source: Literal["input", "generate"] | None = Field(default=None)
    """Source of message."""
    def model_post_init(self, __context: Any) -> None:
        # check if deserializing
        is_deserializing = isinstance(__context, dict) and __context.get(
            DESERIALIZING, False
        )
        # Generate ID if needed and not deserializing
        if self.id is None and not is_deserializing:
            self.id = uuid()
    @property
    def text(self) -> str:
        """Get the text content of this message.
        ChatMessage content is very general and can contain either
        a simple text value or a list of content parts (each of which
        can either be text or an image). Solvers (e.g. for prompt
        engineering) often need to interact with chat messages with
        the assumption that they are a simple string. The text
        property returns either the plain str content, or if the
        content is a list of text and images, the text items
        concatenated together (separated by newline)
        """
        if isinstance(self.content, str):
            return self.content
        else:
            all_text = [
                content.text for content in self.content if content.type == "text"
            ]
            return "\n".join(all_text)
    @text.setter
    def text(self, text: str) -> None:
        """Set the primary text content for this message.
        ChatMessage content is very general and can contain either
        a simple text value or a list of content parts (each of which
        can either be text or an image). Solvers (e.g. for prompt
        engineering) often need to interact with chat messages with
        the assumption that they are a simple string. The text property
        sets text either to content directly (if it is a `str`) or to
        the first text content item in the message (inserting one at
        the beginning if necessary). If there are multiple text content
        items in the message then after the set there will be only
        one remaining (image content will remain).
        """
        if isinstance(self.content, str):
            self.content = text
        else:
            all_other = [content for content in self.content if content.type != "text"]
            self.content = all_other + [ContentText(text=text)]
class ChatMessageSystem(ChatMessageBase):
    """System chat message."""
    role: Literal["system"] = Field(default="system")
    """Conversation role."""
class ChatMessageUser(ChatMessageBase):
    """User chat message."""
    role: Literal["user"] = Field(default="user")
    """Conversation role."""
    tool_call_id: list[str] | None = Field(default=None)
    """ID(s) of tool call(s) this message has the content payload for."""
class ChatMessageAssistant(ChatMessageBase):
    """Assistant chat message."""
    role: Literal["assistant"] = Field(default="assistant")
    """Conversation role."""
    tool_calls: list[ToolCall] | None = Field(default=None)
    """Tool calls made by the model."""
    # Some OpenAI compatible REST endpoints include reasoning as a field alongside
    # content, however since this field doesn't exist in the OpenAI interface,
    # hosting providers (so far we've seen this with Together and Groq) may
    # include the reasoning in a <think></think> tag before the main response.
    # We expect this pattern to be repeated elsewhere, so include this hook to
    # automatically extract the reasoning content when the response is prefaced
    # with a <think> block. If this ends up being an overeach we can fall back
    # to each provider manually parsing out <think> using a helper function.
    # The implementation isn't important here, the critical thing to establish
    # is that Inspect makes reasoning content available separately.
    @model_validator(mode="before")
    @classmethod
    def extract_reasoning(cls, data: Any) -> Any:
        if isinstance(data, dict):
            # cleave apart <think> blocks
            content = data.get("content", None)
            if isinstance(content, str):
                parsed = parse_content_with_reasoning(content)
                if parsed:
                    data["content"] = [
                        ContentReasoning(reasoning=parsed.reasoning),
                        ContentText(text=parsed.content),
                    ]
            # migrate messages that has explicit 'reasoning' field
            # (which was our original representation of reasoning)
            reasoning = data.get("reasoning", None)
            if isinstance(reasoning, str):
                # ensure that content is a list
                content = data.get("content", None)
                if content is None:
                    data["content"] = []
                elif isinstance(content, str):
                    data["content"] = [ContentText(text=content)]
                elif not isinstance(content, list):
                    data["content"] = []
                data["content"].insert(0, ContentReasoning(reasoning=reasoning))
                del data["reasoning"]
        return data
class ChatMessageTool(ChatMessageBase):
    """Tool chat message."""
    role: Literal["tool"] = Field(default="tool")
    """Conversation role."""
    tool_call_id: str | None = Field(default=None)
    """ID of tool call."""
    function: str | None = Field(default=None)
    """Name of function called."""
    error: ToolCallError | None = Field(default=None)
    """Error which occurred during tool call."""
    @property
    def tool_error(self) -> str | None:
        """Tool error (deprecated)."""
        from inspect_ai._util.logger import warn_once
        warn_once(
            logger,
            "The 'tool_error' field is deprecated. Access error information via 'error' instead.",
        )
        if self.error:
            return self.error.message
        else:
            return None
    @model_validator(mode="before")
    @classmethod
    def convert_tool_error_to_error(
        cls: Type["ChatMessageTool"], values: dict[str, Any]
    ) -> dict[str, Any]:
        tool_error = values.get("tool_error", None)
        if tool_error:
            values["error"] = ToolCallError("unknown", tool_error)
        return values
ChatMessage = Union[
    ChatMessageSystem, ChatMessageUser, ChatMessageAssistant, ChatMessageTool
]
"""Message in a chat conversation"""

================
File: src/inspect_ai/model/_conversation.py
================
from rich.console import RenderableType
from rich.text import Text
from inspect_ai._util.content import ContentReasoning, ContentText
from inspect_ai._util.rich import lines_display
from inspect_ai._util.transcript import transcript_markdown, transcript_reasoning
from inspect_ai.util._conversation import conversation_panel
from inspect_ai.util._display import display_type
from ._chat_message import ChatMessage, ChatMessageAssistant, ChatMessageTool
from ._render import messages_preceding_assistant, render_tool_calls
MESSAGE_TITLE = "Message"
def conversation_tool_mesage(message: ChatMessageTool) -> None:
    if display_type() == "conversation":
        # truncate output to 100 lines
        output = (
            message.error.message.strip() if message.error else message.text.strip()
        )
        if output:
            content = lines_display(output, 50)
            conversation_panel(
                title=f"Tool Output: {message.function}",
                content=content,
            )
def conversation_assistant_message(
    input: list[ChatMessage], message: ChatMessageAssistant
) -> None:
    if display_type() == "conversation":
        # print precding messages that aren't tool or assistant
        for m in messages_preceding_assistant(input):
            conversation_panel(
                title=m.role.capitalize(),
                content=transcript_markdown(m.text, escape=True),
            )
        # build content
        content: list[RenderableType] = []
        # deal with plain text or with content blocks
        if isinstance(message.content, str):
            content.extend([transcript_markdown(message.text.strip(), escape=True)])
        else:
            for c in message.content:
                if isinstance(c, ContentReasoning):
                    content.extend(transcript_reasoning(c))
                elif isinstance(c, ContentText) and c.text:
                    content.extend([transcript_markdown(c.text.strip(), escape=True)])
        # print tool calls
        if message.tool_calls:
            if content:
                content.append(Text())
            content.extend(render_tool_calls(message.tool_calls))
        # print the assistant message
        conversation_panel(title="Assistant", content=content)
def conversation_assistant_error(error: Exception) -> None:
    if display_type() == "conversation":
        conversation_panel(title="Assistant", content=repr(error))

================
File: src/inspect_ai/model/_generate_config.py
================
from contextvars import ContextVar
from copy import deepcopy
from typing import Any, Literal, Union
from pydantic import BaseModel, Field, model_validator
from typing_extensions import TypedDict
from inspect_ai.util._json import JSONSchema
class ResponseSchema(BaseModel):
    """Schema for model response when using Structured Output."""
    name: str
    """The name of the response schema. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."""
    json_schema: JSONSchema
    """The schema for the response format, described as a JSON Schema object."""
    description: str | None = Field(default=None)
    """A description of what the response format is for, used by the model to determine how to respond in the format."""
    strict: bool | None = Field(default=None)
    """Whether to enable strict schema adherence when generating the output. If set to true, the model will always follow the exact schema defined in the schema field.
    OpenAI and Mistral only."""
class GenerateConfigArgs(TypedDict, total=False):
    """Type for kwargs that selectively override GenerateConfig."""
    max_retries: int | None
    """Maximum number of times to retry request (defaults to 5)."""
    timeout: int | None
    """Request timeout (in seconds)."""
    max_connections: int | None
    """Maximum number of concurrent connections to Model API (default is model specific)."""
    system_message: str | None
    """Override the default system message."""
    max_tokens: int | None
    """The maximum number of tokens that can be generated in the completion (default is model specific)."""
    top_p: float | None
    """An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass."""
    temperature: float | None
    """What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."""
    stop_seqs: list[str] | None
    """Sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."""
    best_of: int | None
    """Generates best_of completions server-side and returns the 'best' (the one with the highest log probability per token). vLLM only."""
    frequency_penalty: float | None
    """Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim. OpenAI, Google, Grok, Groq, and vLLM only."""
    presence_penalty: float | None
    """Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. OpenAI, Google, Grok, Groq, and vLLM only."""
    logit_bias: dict[int, float] | None
    """Map token Ids to an associated bias value from -100 to 100 (e.g. "42=10,43=-10"). OpenAI and Grok only."""
    seed: int | None
    """Random seed. OpenAI, Google, Mistral, Groq, HuggingFace, and vLLM only."""
    top_k: int | None
    """Randomly sample the next word from the top_k most likely next words. Anthropic, Google, and HuggingFace only."""
    num_choices: int | None
    """How many chat completion choices to generate for each input message. OpenAI, Grok, Google, and TogetherAI only."""
    logprobs: bool | None
    """Return log probabilities of the output tokens. OpenAI, Grok, TogetherAI, Huggingface, llama-cpp-python, and vLLM only."""
    top_logprobs: int | None
    """Number of most likely tokens (0-20) to return at each token position, each with an associated log probability. OpenAI, Grok, and Huggingface only."""
    parallel_tool_calls: bool | None
    """Whether to enable parallel function calling during tool use (defaults to True). OpenAI and Groq only."""
    internal_tools: bool | None
    """Whether to automatically map tools to model internal implementations (e.g. 'computer' for anthropic)."""
    max_tool_output: int | None
    """Maximum tool output (in bytes). Defaults to 16 * 1024."""
    cache_prompt: Literal["auto"] | bool | None
    """Whether to cache the prompt prefix. Defaults to "auto", which will enable caching for requests with tools. Anthropic only."""
    reasoning_effort: Literal["low", "medium", "high"] | None
    """Constrains effort on reasoning for reasoning models. Open AI o1 models only."""
    reasoning_tokens: int | None
    """Maximum number of tokens to use for reasoning. Anthropic Claude models only."""
    reasoning_history: Literal["none", "all", "last", "auto"] | None
    """Include reasoning in chat message history sent to generate."""
    response_schema: ResponseSchema | None
    """Request a response format as JSONSchema (output should still be validated). OpenAI, Google, and Mistral only."""
class GenerateConfig(BaseModel):
    """Model generation options."""
    max_retries: int | None = Field(default=None)
    """Maximum number of times to retry request (defaults to 5)."""
    timeout: int | None = Field(default=None)
    """Request timeout (in seconds)."""
    max_connections: int | None = Field(default=None)
    """Maximum number of concurrent connections to Model API (default is model specific)."""
    system_message: str | None = Field(default=None)
    """Override the default system message."""
    max_tokens: int | None = Field(default=None)
    """The maximum number of tokens that can be generated in the completion (default is model specific)."""
    top_p: float | None = Field(default=None)
    """An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass."""
    temperature: float | None = Field(default=None)
    """What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."""
    stop_seqs: list[str] | None = Field(default=None)
    """Sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence."""
    best_of: int | None = Field(default=None)
    """Generates best_of completions server-side and returns the 'best' (the one with the highest log probability per token). vLLM only."""
    frequency_penalty: float | None = Field(default=None)
    """Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim. OpenAI, Google, Grok, Groq, and vLLM only."""
    presence_penalty: float | None = Field(default=None)
    """Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. OpenAI, Google, Grok, Groq, and vLLM only."""
    logit_bias: dict[int, float] | None = Field(default=None)
    """Map token Ids to an associated bias value from -100 to 100 (e.g. "42=10,43=-10"). OpenAI, Grok, and Grok only."""
    seed: int | None = Field(default=None)
    """Random seed. OpenAI, Google, Mistral, Groq, HuggingFace, and vLLM only."""
    top_k: int | None = Field(default=None)
    """Randomly sample the next word from the top_k most likely next words. Anthropic, Google, HuggingFace, and vLLM only."""
    num_choices: int | None = Field(default=None)
    """How many chat completion choices to generate for each input message. OpenAI, Grok, Google, TogetherAI, and vLLM only."""
    logprobs: bool | None = Field(default=None)
    """Return log probabilities of the output tokens. OpenAI, Grok, TogetherAI, Huggingface, llama-cpp-python, and vLLM only."""
    top_logprobs: int | None = Field(default=None)
    """Number of most likely tokens (0-20) to return at each token position, each with an associated log probability. OpenAI, Grok, Huggingface, and vLLM only."""
    parallel_tool_calls: bool | None = Field(default=None)
    """Whether to enable parallel function calling during tool use (defaults to True). OpenAI and Groq only."""
    internal_tools: bool | None = Field(default=None)
    """Whether to automatically map tools to model internal implementations (e.g. 'computer' for anthropic)."""
    max_tool_output: int | None = Field(default=None)
    """Maximum tool output (in bytes). Defaults to 16 * 1024."""
    cache_prompt: Literal["auto"] | bool | None = Field(default=None)
    """Whether to cache the prompt prefix. Defaults to "auto", which will enable caching for requests with tools. Anthropic only."""
    reasoning_effort: Literal["low", "medium", "high"] | None = Field(default=None)
    """Constrains effort on reasoning for reasoning models. Open AI o1 models only."""
    reasoning_tokens: int | None = Field(default=None)
    """Maximum number of tokens to use for reasoning. Anthropic Claude models only."""
    reasoning_history: Literal["none", "all", "last", "auto"] | None = Field(
        default=None
    )
    """Include reasoning in chat message history sent to generate."""
    response_schema: ResponseSchema | None = Field(default=None)
    """Request a response format as JSONSchema (output should still be validated). OpenAI, Google, and Mistral only."""
    # migrate reasoning_history as a bool
    @model_validator(mode="before")
    @classmethod
    def migrate_reasoning(cls, data: Any) -> Any:
        if isinstance(data, dict):
            reasoning_history = data.get("reasoning_history", None)
            if reasoning_history is True:
                data["reasoning_history"] = "all"
            elif reasoning_history is False:
                data["reasoning_history"] = "none"
        return data
    def merge(
        self, other: Union["GenerateConfig", GenerateConfigArgs]
    ) -> "GenerateConfig":
        """Merge another model configuration into this one.
        Args:
           other (Union[GenerateConfig, GenerateConfigArgs]):
              Configuration to merge.
        Returns:
           Merged configuration.
        """
        if not isinstance(other, GenerateConfig):
            other = GenerateConfig(**other)
        config_keys = list(GenerateConfigArgs.__mutable_keys__)  # type: ignore
        config = deepcopy(self)
        for key in config_keys:
            value = getattr(other, key, None)
            if value is not None:
                setattr(config, key, value)
        return config
def active_generate_config() -> GenerateConfig:
    return active_generate_config_context_var.get()
def set_active_generate_config(config: GenerateConfig) -> None:
    active_generate_config_context_var.set(config)
active_generate_config_context_var: ContextVar[GenerateConfig] = ContextVar(
    "generate_config", default=GenerateConfig()
)

================
File: src/inspect_ai/model/_image.py
================
from copy import copy
from pydantic import JsonValue
from inspect_ai._util.constants import BASE_64_DATA_REMOVED
def image_url_filter(key: JsonValue | None, value: JsonValue) -> JsonValue:
    # remove images from raw api call
    if key == "image_url" and isinstance(value, dict) and "url" in value:
        url = str(value.get("url"))
        if url.startswith("data:"):
            value = copy(value)
            value.update(url=BASE_64_DATA_REMOVED)
    return value

================
File: src/inspect_ai/model/_model_call.py
================
from typing import Any, Callable
from pydantic import BaseModel, Field, JsonValue
from inspect_ai._util.json import jsonable_python
ModelCallFilter = Callable[[JsonValue | None, JsonValue], JsonValue]
"""Filter for transforming or removing some values (e.g. images).
The first parmaeter is the key if the value is a dictionary item.
The second parameter is the value. Return a modified value if appropriate.
"""
class ModelCall(BaseModel):
    """Model call (raw request/response data)."""
    request: dict[str, JsonValue]
    """Raw data posted to model."""
    response: dict[str, JsonValue]
    """Raw response data from model."""
    time: float | None = Field(default=None)
    """Time taken for underlying model call."""
    @staticmethod
    def create(
        request: Any,
        response: Any,
        filter: ModelCallFilter | None = None,
        time: float | None = None,
    ) -> "ModelCall":
        """Create a ModelCall object.
        Create a ModelCall from arbitrary request and response objects (they might
        be dataclasses, Pydandic objects, dicts, etc.). Converts all values to
        JSON serialiable (exluding those that can't be)
        Args:
           request (Any): Request object (dict, dataclass, BaseModel, etc.)
           response (Any): Response object (dict, dataclass, BaseModel, etc.)
           filter (ModelCallFilter): Function for filtering model call data.
           time: Time taken for underlying ModelCall
        """
        request_dict = jsonable_python(request)
        if filter:
            request_dict = _walk_json_value(None, request_dict, filter)
        response_dict = jsonable_python(response)
        if filter:
            response_dict = _walk_json_value(None, response_dict, filter)
        return ModelCall(request=request_dict, response=response_dict, time=time)
def _walk_json_value(
    key: JsonValue | None, value: JsonValue, filter: ModelCallFilter
) -> JsonValue:
    value = filter(key, value)
    if isinstance(value, list):
        return [_walk_json_value(None, v, filter) for v in value]
    elif isinstance(value, dict):
        return {k: _walk_json_value(k, v, filter) for k, v in value.items()}
    else:
        return value

================
File: src/inspect_ai/model/_model_output.py
================
import uuid
from typing import Any, Literal, Type
from pydantic import BaseModel, Field, model_validator
from inspect_ai.tool._tool_call import ToolCall
from ._chat_message import ChatMessageAssistant
class ModelUsage(BaseModel):
    """Token usage for completion."""
    input_tokens: int = Field(default=0)
    """Total input tokens used."""
    output_tokens: int = Field(default=0)
    """Total output tokens used."""
    total_tokens: int = Field(default=0)
    """Total tokens used."""
    input_tokens_cache_write: int | None = Field(default=None)
    """Number of tokens written to the cache."""
    input_tokens_cache_read: int | None = Field(default=None)
    """Number of tokens retrieved from the cache."""
    reasoning_tokens: int | None = Field(default=None)
    """Number of tokens used for reasoning."""
StopReason = Literal[
    "stop",
    "max_tokens",
    "model_length",
    "tool_calls",
    "content_filter",
    "unknown",
]
"""Reason that the model stopped or failed to generate."""
class TopLogprob(BaseModel):
    """List of the most likely tokens and their log probability, at this token position."""
    token: str
    """The top-kth token represented as a string."""
    logprob: float
    """The log probability value of the model for the top-kth token."""
    bytes: list[int] | None = Field(default=None)
    """The top-kth token represented as a byte array (a list of integers)."""
class Logprob(BaseModel):
    """Log probability for a token."""
    token: str
    """The predicted token represented as a string."""
    logprob: float
    """The log probability value of the model for the predicted token."""
    bytes: list[int] | None = Field(default=None)
    """The predicted token represented as a byte array (a list of integers)."""
    top_logprobs: list[TopLogprob] | None = Field(default=None)
    """If the `top_logprobs` argument is greater than 0, this will contain an ordered list of the top K most likely tokens and their log probabilities."""
class Logprobs(BaseModel):
    """Log probability information for a completion choice."""
    content: list[Logprob]
    """a (num_generated_tokens,) length list containing the individual log probabilities for each generated token."""
class ChatCompletionChoice(BaseModel):
    """Choice generated for completion."""
    message: ChatMessageAssistant
    """Assistant message."""
    stop_reason: StopReason = Field(default="unknown")
    """Reason that the model stopped generating."""
    logprobs: Logprobs | None = Field(default=None)
    """Logprobs."""
    @model_validator(mode="before")
    @classmethod
    def migrate_stop_reason(
        cls: Type["ChatCompletionChoice"], values: dict[str, Any]
    ) -> dict[str, Any]:
        if "stop_reason" in values:
            stop_reason = values["stop_reason"]
            if stop_reason == "length":
                values["stop_reason"] = "max_tokens"
        return values
class ModelOutput(BaseModel):
    """Output from model generation."""
    model: str = Field(default_factory=str)
    """Model used for generation."""
    choices: list[ChatCompletionChoice] = Field(default=[])
    """Completion choices."""
    usage: ModelUsage | None = Field(default=None)
    """Model token usage"""
    time: float | None = Field(default=None)
    """Time elapsed (in seconds) for call to generate."""
    metadata: dict[str, Any] | None = Field(default=None)
    """Additional metadata associated with model output."""
    error: str | None = Field(default=None)
    """Error message in the case of content moderation refusals."""
    @property
    def stop_reason(self) -> StopReason:
        """First message stop reason."""
        return self.choices[0].stop_reason
    @property
    def message(self) -> ChatMessageAssistant:
        """First message choice."""
        return self.choices[0].message
    @property
    def completion(self) -> str:
        """Text of first message choice text."""
        if len(self.choices) > 0:
            return self.choices[0].message.text
        else:
            return ""
    @completion.setter
    def completion(self, completion: str) -> None:
        """Set the text of the first message choice.
        Args:
          completion (str): Text for first message.
        """
        if len(self.choices) > 0:
            self.choices[0].message.text = completion
        else:
            self.choices.append(
                ChatCompletionChoice(
                    message=ChatMessageAssistant(content=completion), stop_reason="stop"
                )
            )
    @staticmethod
    def from_content(
        model: str,
        content: str,
        stop_reason: StopReason = "stop",
        error: str | None = None,
    ) -> "ModelOutput":
        """Create ModelOutput from simple text content.
        Args:
           model: Model name.
           content: Text content from generation.
           stop_reason: Stop reason for generation.
           error: Error message.
        """
        return ModelOutput(
            model=model,
            choices=[
                ChatCompletionChoice(
                    message=ChatMessageAssistant(content=content, source="generate"),
                    stop_reason=stop_reason,
                )
            ],
            error=error,
        )
    @staticmethod
    def for_tool_call(
        model: str,
        tool_name: str,
        tool_arguments: dict[str, Any],
        tool_call_id: str | None = None,
        content: str | None = None,
    ) -> "ModelOutput":
        """
        Returns a ModelOutput for requesting a tool call.
        Args:
            model: model name
            tool_name: The name of the tool.
            tool_arguments: The arguments passed to the tool.
            tool_call_id: Optional ID for the tool call. Defaults to a random UUID.
            content: Optional content to include in the message. Defaults to "tool call for tool {tool_name}".
        Returns:
            A ModelOutput corresponding to the tool call
        """
        if content is None:
            content = f"tool call for tool {tool_name}"
        if tool_call_id is None:
            tool_call_id = f"for_tool_call_{uuid.uuid4()}"
        return ModelOutput(
            model=model,
            choices=[
                ChatCompletionChoice(
                    message=ChatMessageAssistant(
                        content=content,
                        source="generate",
                        tool_calls=[
                            ToolCall(
                                id=tool_call_id,
                                function=tool_name,
                                arguments=tool_arguments,
                                type="function",
                            )
                        ],
                    ),
                    stop_reason="tool_calls",
                )
            ],
        )
def as_stop_reason(reason: str | None) -> StopReason:
    """Encode common reason strings into standard StopReason."""
    match reason:
        case "stop" | "eos":
            return "stop"
        case "length":
            return "max_tokens"
        case "tool_calls" | "function_call":
            return "tool_calls"
        case "content_filter" | "model_length" | "max_tokens":
            return reason
        case _:
            return "unknown"

================
File: src/inspect_ai/model/_model.py
================
import abc
import contextlib
import functools
import json
import logging
import os
import time
from contextvars import ContextVar
from copy import deepcopy
from datetime import datetime
from types import TracebackType
from typing import Any, AsyncIterator, Callable, Literal, Type, cast
from pydantic_core import to_jsonable_python
from tenacity import (
    RetryCallState,
    retry,
    retry_if_exception,
    stop_after_attempt,
    stop_after_delay,
    stop_never,
    wait_exponential_jitter,
)
from tenacity.stop import StopBaseT
from inspect_ai._util.constants import DEFAULT_MAX_CONNECTIONS, HTTP
from inspect_ai._util.content import (
    Content,
    ContentImage,
    ContentReasoning,
    ContentText,
)
from inspect_ai._util.hooks import init_hooks, override_api_key, send_telemetry
from inspect_ai._util.interrupt import check_sample_interrupt
from inspect_ai._util.logger import warn_once
from inspect_ai._util.notgiven import NOT_GIVEN, NotGiven
from inspect_ai._util.platform import platform_init
from inspect_ai._util.registry import (
    RegistryInfo,
    registry_find,
    registry_info,
    registry_unqualified_name,
)
from inspect_ai._util.retry import report_http_retry
from inspect_ai._util.trace import trace_action
from inspect_ai._util.working import report_sample_waiting_time, sample_working_time
from inspect_ai.tool import Tool, ToolChoice, ToolFunction, ToolInfo
from inspect_ai.tool._tool_def import ToolDef, tool_defs
from inspect_ai.util import concurrency
from ._cache import CacheEntry, CachePolicy, cache_fetch, cache_store
from ._call_tools import disable_parallel_tools, tool_call_view, tools_info
from ._chat_message import (
    ChatMessage,
    ChatMessageAssistant,
    ChatMessageSystem,
    ChatMessageTool,
    ChatMessageUser,
)
from ._conversation import conversation_assistant_error, conversation_assistant_message
from ._generate_config import (
    GenerateConfig,
    active_generate_config,
    set_active_generate_config,
)
from ._model_call import ModelCall
from ._model_output import ModelOutput, ModelUsage
logger = logging.getLogger(__name__)
class ModelAPI(abc.ABC):
    """Model API provider.
    If you are implementing a custom ModelAPI provider your `__init__()`
    method will also receive a `**model_args` parameter that will carry
    any custom `model_args` (or `-M` arguments from the CLI) specified
    by the user. You can then pass these on to the approriate place in
    your model initialisation code (for example, here is what many
    of the built-in providers do with the `model_args` passed to them:
    https://inspect.ai-safety-institute.org.uk/models.html#model-args)
    """
    def __init__(
        self,
        model_name: str,
        base_url: str | None = None,
        api_key: str | None = None,
        api_key_vars: list[str] = [],
        config: GenerateConfig = GenerateConfig(),
    ) -> None:
        """Create a model API provider.
        Args:
           model_name (str): Model name.
           base_url (str | None): Alternate base URL for model.
           api_key (str | None): API key for model.
           api_key_vars (list[str]): Environment variables that
              may contain keys for this provider (used for override)
           config (GenerateConfig): Model configuration.
        """
        self.model_name = model_name
        self.base_url = base_url
        self.config = config
        # apply api key override
        for key in api_key_vars:
            # if there is an explicit api_key passed then it
            # overrides anything in the environment so use it
            if api_key is not None:
                override = override_api_key(key, api_key)
                if override is not None:
                    api_key = override
            # otherwise look it up in the environment and
            # override it if it has a value
            else:
                value = os.environ.get(key, None)
                if value is not None:
                    override = override_api_key(key, value)
                    if override is not None:
                        os.environ[key] = override
        # set any explicitly specified api key
        self.api_key = api_key
    async def close(self) -> None:
        """Close method for closing any client allocated for the model."""
        pass
    @abc.abstractmethod
    async def generate(
        self,
        input: list[ChatMessage],
        tools: list[ToolInfo],
        tool_choice: ToolChoice,
        config: GenerateConfig,
    ) -> ModelOutput | tuple[ModelOutput | Exception, ModelCall]:
        """Generate output from the model.
        Args:
          input (str | list[ChatMessage]): Chat message
            input (if a `str` is passed it is converted
            to a `ChatUserMessage`).
          tools (list[ToolInfo]): Tools available for the
            model to call.
          tool_choice (ToolChoice): Directives to the model
            as to which tools to prefer.
          config (GenerateConfig): Model configuration.
        Returns:
           ModelOutput or tuple[ModelOutput,ModelCall], the latter being
           useful if you want the underlying model API call logged as
           part of the ModelEvent.
        """
        ...
    def max_tokens(self) -> int | None:
        """Default max_tokens."""
        return None
    def max_tokens_for_config(self, config: GenerateConfig) -> int | None:
        """Default max_tokens for a given config.
        Args:
           config: Generation config.
        Returns:
           Default maximum tokens for specified configuration.
        """
        return None
    def max_connections(self) -> int:
        """Default max_connections."""
        return DEFAULT_MAX_CONNECTIONS
    def connection_key(self) -> str:
        """Scope for enforcement of max_connections."""
        return "default"
    def should_retry(self, ex: Exception) -> bool:
        """Should this exception be retried?
        Args:
           ex: Exception to check for retry
        """
        return False
    def collapse_user_messages(self) -> bool:
        """Collapse consecutive user messages into a single message."""
        return False
    def collapse_assistant_messages(self) -> bool:
        """Collapse consecutive assistant messages into a single message."""
        return False
    def tools_required(self) -> bool:
        """Any tool use in a message stream means that tools must be passed."""
        return False
    def tool_result_images(self) -> bool:
        """Tool results can contain images"""
        return False
    def emulate_reasoning_history(self) -> bool:
        """Chat message assistant messages with reasoning should playback reasoning with emulation (.e.g. <think> tags)"""
        return True
    def force_reasoning_history(self) -> Literal["none", "all", "last"] | None:
        """Force a specific reasoning history behavior for this provider."""
        return None
    def auto_reasoning_history(self) -> Literal["none", "all", "last"]:
        """Behavior to use for reasoning_history='auto'"""
        return "all"
class Model:
    """Model interface.
    Use `get_model()` to get an instance of a model. Model provides an
    async context manager for closing the connection to it after use.
    For example:
    ```python
    async with get_model("openai/gpt-4o") as model:
        response = await model.generate("Say hello")
    ```
    """
    api: ModelAPI
    """Model API."""
    config: GenerateConfig
    """Generation config."""
    def __init__(
        self, api: ModelAPI, config: GenerateConfig, model_args: dict[str, Any] = {}
    ) -> None:
        """Create a model.
        Args:
           api: Model API provider.
           config: Model configuration.
           model_args: Optional model args
        """
        self.api = api
        self.config = config
        self.model_args = model_args
        # state indicating whether our lifetime is bound by a context manager
        self._context_bound = False
        self._closed = False
        # if using the Model API standalone in a notebook this will
        # get hit before score() or eval() so we activate nest_asyncio
        platform_init()
    async def __aenter__(self: "Model") -> "Model":
        self._context_bound = True
        return self
    async def __aexit__(
        self,
        exc_type: type[BaseException] | None,
        exc: BaseException | None,
        exc_tb: TracebackType | None,
    ) -> None:
        if not self._closed:
            await self.api.close()
            self._closed = True
    @property
    def name(self) -> str:
        """Model name."""
        return self.api.model_name
    def __str__(self) -> str:
        return f"{ModelName(self)}"
    async def generate(
        self,
        input: str | list[ChatMessage],
        tools: list[Tool]
        | list[ToolDef]
        | list[ToolInfo]
        | list[Tool | ToolDef | ToolInfo] = [],
        tool_choice: ToolChoice | None = None,
        config: GenerateConfig = GenerateConfig(),
        cache: bool | CachePolicy = False,
    ) -> ModelOutput:
        """Generate output from the model.
        Args:
          input: Chat message input (if a `str` is passed it is converted
            to a `ChatMessageUser`).
          tools: Tools available for the model to call.
          tool_choice: Directives to the model as to which tools to prefer.
          config: Model configuration.
          cache: Caching behavior for generate responses (defaults to no caching).
        Returns:
           ModelOutput
        """
        # if we are the default model then enforce message limit if it
        # exists (raise an exception if it is exceeded)
        is_active_model = self == active_model()
        if is_active_model:
            handle_sample_message_limit(input)
        # base config for this model
        base_config = self.config
        # if we are the active_model then merge active generate config
        if is_active_model:
            base_config = base_config.merge(active_generate_config())
        # merge passed config
        config = base_config.merge(config)
        # provide max_tokens from the model api if required
        if config.max_tokens is None:
            config.max_tokens = self.api.max_tokens_for_config(config)
            if config.max_tokens is None:
                config.max_tokens = self.api.max_tokens()
        # disable parallel tool calls if requested by any of our tools
        if disable_parallel_tools(tools):
            config.parallel_tool_calls = False
        # normalize input to chat
        if isinstance(input, str):
            input = [ChatMessageUser(content=input)]
        # insert any system message provided in config
        if config.system_message:
            input = [ChatMessageSystem(content=config.system_message)] + input
        # enforce concurrency limits
        start_time = datetime.now()
        working_start = sample_working_time()
        async with self._connection_concurrency(config):
            from inspect_ai.log._samples import track_active_sample_retries
            # generate
            with track_active_sample_retries():
                output = await self._generate(
                    input=input,
                    tools=tools,
                    tool_choice=tool_choice,
                    config=config,
                    cache=cache,
                )
            # update the most recent ModelEvent with the actual start/completed
            # times as well as a computation of working time (events are
            # created _after_ the call to _generate, potentially in response
            # to retries, so they need their timestamp updated so it accurately
            # reflects the full start/end time which we know here)
            from inspect_ai.log._transcript import ModelEvent, transcript
            last_model_event = transcript().find_last_event(ModelEvent)
            if last_model_event:
                last_model_event.timestamp = start_time
                last_model_event.working_start = working_start
                completed = datetime.now()
                last_model_event.completed = completed
                last_model_event.working_time = (
                    output.time
                    if output.time is not None
                    else (completed - start_time).total_seconds()
                )
            # return output
            return output
    async def _generate(
        self,
        input: list[ChatMessage],
        tools: list[Tool]
        | list[ToolDef]
        | list[ToolInfo]
        | list[Tool | ToolDef | ToolInfo],
        tool_choice: ToolChoice | None,
        config: GenerateConfig,
        cache: bool | CachePolicy = False,
    ) -> ModelOutput:
        # default to 'auto' for tool_choice (same as underlying model apis)
        tool_choice = tool_choice if tool_choice else "auto"
        # extract tool defs if we can
        tdefs = tool_defs([tool for tool in tools if not isinstance(tool, ToolInfo)])
        # resolve all tools into tool_info
        tools = tools_info(tools)
        # if we have a specific tool selected then filter out the others
        if isinstance(tool_choice, ToolFunction):
            tools = [tool for tool in tools if tool.name == tool_choice.name]
        # if tool_choice is "none" or if there are no tools then fully purge
        # the tools (as some models (e.g. openai and mistral) get confused
        # if you pass them tool definitions along with tool_choice == "none"
        # (they both 'semi' use the tool by placing the arguments in JSON
        # in their output!). on the other hand, anthropic actually errors if
        # there are tools anywhere in the message stream and no tools defined.
        if tool_choice == "none" or len(tools) == 0:
            # allow model providers to implement a tools_required() method to
            # force tools to be passed (we need this for anthropic)
            if not self.api.tools_required():
                tools = []
            tool_choice = "none"
        # handle reasoning history
        input = resolve_reasoning_history(input, config, self.api)
        # apply any tool model_input handlers
        input = resolve_tool_model_input(tdefs, input)
        # break tool image content out into user messages if the model doesn't
        # support tools returning images
        if not self.api.tool_result_images():
            input = tool_result_images_as_user_message(input)
        # optionally collapse *consecutive* messages into one -
        # (some apis e.g. anthropic require this)
        if self.api.collapse_user_messages():
            input = collapse_consecutive_user_messages(input)
        if self.api.collapse_assistant_messages():
            input = collapse_consecutive_assistant_messages(input)
        # retry for transient http errors:
        # - no default timeout or max_retries (try forever)
        # - exponential backoff starting at 3 seconds (will wait 25 minutes
        #   on the 10th retry,then will wait no longer than 30 minutes on
        #   subsequent retries)
        if config.max_retries is not None and config.timeout is not None:
            stop: StopBaseT = stop_after_attempt(config.max_retries) | stop_after_delay(
                config.timeout
            )
        elif config.max_retries is not None:
            stop = stop_after_attempt(config.max_retries)
        elif config.timeout is not None:
            stop = stop_after_delay(config.timeout)
        else:
            stop = stop_never
        @retry(
            wait=wait_exponential_jitter(initial=3, max=(30 * 60), jitter=3),
            retry=retry_if_exception(self.should_retry),
            stop=stop,
            before_sleep=functools.partial(log_model_retry, self.api.model_name),
        )
        async def generate() -> ModelOutput:
            check_sample_interrupt()
            if cache:
                if isinstance(cache, CachePolicy):
                    policy = cache
                else:
                    policy = CachePolicy()
                cache_entry = CacheEntry(
                    base_url=self.api.base_url,
                    config=deepcopy(config),
                    input=input,
                    model=str(self),
                    policy=policy,
                    tool_choice=tool_choice,
                    tools=tools,
                )
                existing = cache_fetch(cache_entry)
                if isinstance(existing, ModelOutput):
                    self._record_model_interaction(
                        input=input,
                        tools=tools,
                        tool_choice=tool_choice,
                        config=config,
                        cache="read",
                        output=existing,
                        call=None,
                    )
                    return existing
            # verify that model apis are allowed
            self.verify_model_apis()
            # record the interaction before the call to generate
            # (we'll update it with the results once we have them)
            complete = self._record_model_interaction(
                input=input,
                tools=tools,
                tool_choice=tool_choice,
                config=config,
                cache="write" if cache else None,
            )
            with trace_action(logger, "Model", f"generate ({str(self)})"):
                time_start = time.monotonic()
                try:
                    result = await self.api.generate(
                        input=input,
                        tools=tools,
                        tool_choice=tool_choice,
                        config=config,
                    )
                finally:
                    time_elapsed = time.monotonic() - time_start
            if isinstance(result, tuple):
                output, call = result
            else:
                output = result
                call = None
            # raise error
            if isinstance(output, Exception):
                complete(output, call)
                # Wrap the error in a runtime error which will show the
                # request which caused the error
                error = repr(output)
                request = json.dumps(call.request, indent=2) if call is not None else ""
                error_message = f"{error}\n\nRequest:\n{request}"
                raise RuntimeError(error_message)
            # update output with time (call.time captures time spent
            # on the actual request that succeeds w/ status 200)
            if call and call.time is not None:
                output.time = call.time
            else:
                output.time = time_elapsed
            # add views to tool calls
            for choice in output.choices:
                for tool_call in choice.message.tool_calls or []:
                    tool_call.view = tool_call_view(tool_call, tdefs)
            # complete the transcript event
            complete(output, call)
            # record usage
            if output.usage:
                # record usage
                record_model_usage(f"{self}", output.usage)
                # send telemetry if its hooked up
                await send_telemetry(
                    "model_usage",
                    json.dumps(dict(model=str(self), usage=output.usage.model_dump())),
                )
            if cache:
                cache_store(entry=cache_entry, output=output)
            return output
        # call the model (this will so retries, etc., so report waiting time
        # as elapsed time - actual time for successful model call)
        time_start = time.monotonic()
        model_output = await generate()
        total_time = time.monotonic() - time_start
        if model_output.time:
            report_sample_waiting_time(total_time - model_output.time)
        # return results
        return model_output
    def should_retry(self, ex: BaseException) -> bool:
        if isinstance(ex, Exception):
            # check standard should_retry() method
            retry = self.api.should_retry(ex)
            if retry:
                report_http_retry()
                return True
            # see if the API implements legacy is_rate_limit() method
            is_rate_limit = getattr(self.api, "is_rate_limit", None)
            if is_rate_limit:
                warn_once(
                    logger,
                    f"provider '{self.name}' implements deprecated is_rate_limit() method, "
                    + "please change to should_retry()",
                )
                retry = cast(bool, is_rate_limit(ex))
                if retry:
                    report_http_retry()
                    return True
        # no retry
        return False
    # function to verify that its okay to call model apis
    def verify_model_apis(self) -> None:
        if (
            os.getenv("INSPECT_DISABLE_MODEL_API", None) is not None
            and ModelName(self).api != "mockllm"
        ):
            raise RuntimeError("Model APIs disabled by INSPECT_DISABLE_MODEL_API")
    # semaphore for model generate requests. these can be shared across
    # instances of Model.  This is so that each distinct model endpoint/account
    # combination shares the semaphore -- i.e. if you had 3 instances
    # of a model class (e.g. attacker model, evaluated model, and grader
    # model) in an eval, they won't each get the full max_connections allocated
    # (which would likely cause the rate limit to be exceeded). conversely if
    # you are using distinct models/endpoints/accounts within an eval you should
    # be able get the full max_connections for each of them. subclasses can
    # override the _connection_key() argument to provide a scope within which
    # to enforce max_connections (e.g. by account/api_key, by endpoint, etc.)
    @contextlib.asynccontextmanager
    async def _connection_concurrency(
        self, config: GenerateConfig
    ) -> AsyncIterator[None]:
        """Get the appropriate connection semaphore for this model instance."""
        max_connections = (
            config.max_connections
            if config.max_connections
            else self.api.max_connections()
        )
        model_name = ModelName(self)
        async with concurrency(
            name=f"{model_name.api}",
            concurrency=max_connections,
            key=f"Model{self.api.connection_key()}",
        ):
            yield
    def _record_model_interaction(
        self,
        input: list[ChatMessage],
        tools: list[ToolInfo],
        tool_choice: ToolChoice,
        config: GenerateConfig,
        cache: Literal["read", "write"] | None,
        output: ModelOutput | None = None,
        call: ModelCall | None = None,
    ) -> Callable[[ModelOutput | Exception, ModelCall | None], None]:
        from inspect_ai.log._transcript import ModelEvent, transcript
        # create event and add it to the transcript
        model = str(self)
        event = ModelEvent(
            model=model,
            input=input,
            tools=tools,
            tool_choice=tool_choice,
            config=config,
            output=output if output else ModelOutput.from_content(model, ""),
            cache=cache,
            call=call,
            pending=output is None,
        )
        transcript()._event(event)
        # callable that can be used to update the interaction w/ output
        def complete(
            result: ModelOutput | Exception, updated_call: ModelCall | None
        ) -> None:
            # trace
            if isinstance(result, ModelOutput):
                if result.choices:
                    conversation_assistant_message(input, result.choices[0].message)
                event.output = result
            else:
                conversation_assistant_error(result)
                event.error = repr(result)
            event.call = updated_call
            event.pending = None
        # if we have output then complete it now
        if output:
            complete(output, call)
        return complete
class ModelName:
    r"""Model name (api and specific model served by the api).
    Can be used for structural pattern matching of models against
    various string specifications of models. Used primarily by
    tasks to allow them to condition their behavior on models or
    model families.
    String specifications can be fully specified (e.g. openai/gpt-4),
    partially specified by model name only (e.g. gpt-4) or even
    partially specified by a substring of model name (e.g. gpt).
    """
    def __init__(self, model: str | Model) -> None:
        """Create a ModelName.
        Args:
           model: Model to create name for.
        """
        if isinstance(model, str):
            (api, name) = self._parse_model(model)
            if api is None:
                raise ValueError("API not specified for model name")
            self.api = api
            self.name = name
        else:
            # registry names have a package prefix, strip it off
            name = registry_info(model.api).name
            parts = name.split("/")
            self.api = "/".join(parts[1:]) if len(parts) > 1 else name
            self.name = model.name
    def __eq__(self, pattern: object) -> bool:
        if isinstance(pattern, str):
            (api, name) = self._parse_model(pattern)
            if (api and api in self.api) and name in self.name:
                return True
            else:
                return name in self.name
        else:
            return False
    def __str__(self) -> str:
        return f"{self.api}/{self.name}"
    def _parse_model(self, model: str) -> tuple[str | None, str]:
        parts = model.split("/")
        if len(parts) > 1:
            return (parts[0], "/".join(parts[1:]))
        else:
            return (None, model)
def get_model(
    model: str | Model | None = None,
    config: GenerateConfig = GenerateConfig(),
    base_url: str | None = None,
    api_key: str | None = None,
    memoize: bool = True,
    **model_args: Any,
) -> Model:
    """Get an instance of a model.
    Calls to get_model() are memoized (i.e. a call with the same arguments
    will return an existing instance of the model rather than creating a
    new one). You can disable this with `memoize=False`.
    If you prefer to immediately close models after use (as well as
    prevent caching) you can employ the async context manager built in
    to the `Model` class. For example:
    ```python
    async with get_model("openai/gpt-4o") as model:
        response = await model.generate("Say hello")
    ```
    In this case, the model client will be closed at the end of the
    context manager and will not be available in the get_model() cache.
    Args:
       model: Model specification.
          If `Model` is passed it is returned unmodified,
          if `None` is passed then the model currently being
          evaluated is returned (or if there is no evaluation
          then the model referred to by `INSPECT_EVAL_MODEL`).
       config: Configuration for model.
       base_url: Optional. Alternate base URL for model.
       api_key: Optional. API key for model.
       memoize: Use/store a cached version of the model based on
          the parameters to `get_model()`
       **model_args: Additional args to
          pass to model constructor.
    Returns:
        Model instance.
    """
    # start with seeing if a model was passed
    if isinstance(model, Model):
        return model
    # next see if this is the special "none" model
    if model == "none":
        model = "none/none"
    # now try finding an 'ambient' model (active or env var)
    if model is None:
        # return active_model if there is one
        active = active_model()
        if active:
            return active
        # return based on env var if there is one
        # (handle lists by taking the first model)
        model = os.getenv("INSPECT_EVAL_MODEL", None)
        if model is not None:
            model = model.split(",")[0]
        else:
            raise ValueError("No model specified (and no INSPECT_EVAL_MODEL defined)")
    # see if we can return a memoized model instance
    # (exclude mockllm since custom_outputs is an infinite generator)
    model_cache_key: str = ""  # for mypy below
    if model.startswith("mockllm/"):
        memoize = False
    if memoize:
        model_cache_key = (
            model
            + config.model_dump_json(exclude_none=True)
            + str(base_url)
            + str(api_key)
            + str(to_jsonable_python(model_args, fallback=lambda _: None))
        )
        cached = cached_model(model_cache_key)
        if cached is not None:
            return cached
    # split model into api name and model name if necessary
    api_name = None
    parts = model.split("/")
    if len(parts) > 1:
        api_name = parts[0]
        model = "/".join(parts[1:])
    # check for api_name matching unqualified name (package prefix not
    # required as modelapi providers are registred globally for ease of
    # use from the command line and .env files)
    def match_modelapi_type(info: RegistryInfo) -> bool:
        if info.type == "modelapi" and registry_unqualified_name(info) == api_name:
            return True
        else:
            return False
    # find a matching model type
    modelapi_types = registry_find(match_modelapi_type)
    if len(modelapi_types) > 0:
        # create the model (init_hooks here in case the model api
        # is being used as a stadalone model interface outside of evals)
        init_hooks()
        modelapi_type = cast(type[ModelAPI], modelapi_types[0])
        modelapi_instance = modelapi_type(
            model_name=model,
            base_url=base_url,
            api_key=api_key,
            config=config,
            **model_args,
        )
        m = Model(modelapi_instance, config, model_args)
        if memoize:
            _models[model_cache_key] = m
        return m
    else:
        from_api = f" from {api_name}" if api_name else ""
        raise ValueError(f"Model name {model}{from_api} not recognized.")
# cache for memoization of get_model
_models: dict[str, Model] = {}
def cached_model(key: str) -> Model | None:
    # clean out context bound models before accessing the cache
    for k in list(_models.keys()):
        if _models[k]._context_bound:
            del _models[k]
    # read from the cache
    return _models.get(key, None)
def resolve_models(
    model: str | Model | list[str] | list[Model] | None | NotGiven = NOT_GIVEN,
    model_base_url: str | None = None,
    model_args: dict[str, Any] = dict(),
    config: GenerateConfig = GenerateConfig(),
) -> list[Model]:
    # resolve NotGiven to current INSPECT_EVAL_MODEL
    if isinstance(model, NotGiven):
        model = os.getenv("INSPECT_EVAL_MODEL", None)
    # resolve None to NoModel
    if model is None:
        return [get_model("none")]
    # reflect back a plain model
    if isinstance(model, Model):
        return [model]
    # helper to resolve model of various types
    def resolve_model(m: str | Model) -> Model:
        return get_model(
            model=m,
            base_url=model_base_url,
            config=config,
            **model_args,
        )
    # str to list
    if isinstance(model, str):
        model = [m.strip() for m in model.split(",")]
    # resolve models
    return [resolve_model(m) for m in model]
def simple_input_messages(
    input: list[ChatMessage],
    fold_system_message: Callable[[str, str], str] | None = None,
) -> list[ChatMessage]:
    """Transform input messages into a format compatible with more simplistic chat APIs.
    Collects up system messages and folds them into the first user message
    (according to a passed in folding function). Also collapses consecutive
    user messages (as many LLMs require an alternating structure)
    """
    # start by making a deep copy so our mutations don't propagate (e.g. end up in log)
    input = deepcopy(input)
    # aggregate system message from all system messages
    system_message = " ".join(
        [message.text for message in input if isinstance(message, ChatMessageSystem)]
    ).strip()
    # collect all non-system messages and collapse consecutive user messages
    messages: list[ChatMessage] = collapse_consecutive_user_messages(
        [message for message in input if not isinstance(message, ChatMessageSystem)]
    )
    # fold the system message into the first user message
    first_user_message = next(
        message for message in messages if isinstance(message, ChatMessageUser)
    )
    if fold_system_message:
        first_user_message.text = fold_system_message(
            first_user_message.text, system_message
        )
    else:
        first_user_message.text = f"{system_message}\n\n{first_user_message.text}"
    # all done!
    return messages
def resolve_reasoning_history(
    messages: list[ChatMessage],
    config: GenerateConfig,
    model_api: ModelAPI,
) -> list[ChatMessage]:
    # determine up front if we have any reasoning content
    have_reasoning = any(
        [
            isinstance(m, ChatMessageAssistant)
            and isinstance(m.content, list)
            and any([c for c in m.content if isinstance(c, ContentReasoning)])
            for m in messages
        ]
    )
    if not have_reasoning:
        return messages
    # determine reasoning history configuration
    reasoning_history = (
        config.reasoning_history if config.reasoning_history is not None else "auto"
    )
    # see if the provider is forcing a reasoning history
    force = model_api.force_reasoning_history()
    if force is not None:
        reasoning_history = force
    # if it's 'auto' then defer to the provider
    elif reasoning_history == "auto":
        reasoning_history = model_api.auto_reasoning_history()
    # generate a version of message history with the correct history
    if reasoning_history == "all":
        resolved_messages: list[ChatMessage] = messages
    else:
        found_last = False
        resolved_messages = []
        for message in reversed(messages):
            if isinstance(message, ChatMessageAssistant) and isinstance(
                message.content, list
            ):
                # is there reasoning in this message?
                has_reasoning = any(
                    isinstance(c, ContentReasoning) for c in message.content
                )
                # remove it unless we are in "last" mode and haven't yet found last
                if has_reasoning:
                    if reasoning_history == "none" or found_last:
                        message = message.model_copy(
                            update={
                                "content": [
                                    content
                                    for content in message.content
                                    if not isinstance(content, ContentReasoning)
                                ]
                            }
                        )
                    found_last = True
            resolved_messages.append(message)
        # reverse them back
        resolved_messages.reverse()
    # api can't represent reasoning natively so emulate it
    if model_api.emulate_reasoning_history():
        emulated_messages: list[ChatMessage] = []
        for message in resolved_messages:
            if isinstance(message, ChatMessageAssistant) and isinstance(
                message.content, list
            ):
                content: list[Content] = []
                for c in message.content:
                    if isinstance(c, ContentReasoning):
                        content.append(
                            ContentText(text=f"<think>\n{c.reasoning}\n</think>")
                        )
                    else:
                        content.append(c)
                message = message.model_copy(update={"content": content})
            emulated_messages.append(message)
        resolved_messages = emulated_messages
    # return messages
    return resolved_messages
def resolve_tool_model_input(
    tdefs: list[ToolDef], messages: list[ChatMessage]
) -> list[ChatMessage]:
    # filter on tooldefs that have a model input handler
    tdefs = [tdef for tdef in tdefs if tdef.model_input is not None]
    # bail if there are no handlers
    if len(tdefs) == 0:
        return messages
    # don't mutate the original messages
    messages = deepcopy(messages)
    # extract tool messages
    tool_messages = [
        message for message in messages if isinstance(message, ChatMessageTool)
    ]
    # run model_input handlers over all tool_messages with the same function name
    for tdef in tdefs:
        assert tdef.model_input
        # filter messages down to just this tool
        tdef_tool_messages = [
            message for message in tool_messages if message.function == tdef.name
        ]
        # call the function for each tool, passing the index, total, and content
        for index, message in enumerate(tdef_tool_messages):
            message.content = tdef.model_input(
                index, len(tool_messages), message.content
            )
    # return modified messages
    return messages
def tool_result_images_as_user_message(
    messages: list[ChatMessage],
) -> list[ChatMessage]:
    """
    To conform to models lacking support for images in tool responses, create an alternate message history that moves images into a fabricated user message.
    Tool responses will have images replaced with "Image content is included below.", and the new user message will contain the images.
    """
    init_accum: ImagesAccumulator = ([], [], [])
    chat_messages, user_message_content, tool_call_ids = functools.reduce(
        tool_result_images_reducer, messages, init_accum
    )
    # if the last message was a tool result, we may need to flush the pending stuff here
    return maybe_adding_user_message(chat_messages, user_message_content, tool_call_ids)
ImagesAccumulator = tuple[list[ChatMessage], list[Content], list[str]]
"""
ImagesAccumulator is a tuple containing three lists:
- The first list contains ChatMessages that are the result of processing.
- The second list contains ContentImages that need to be inserted into a fabricated user message.
- The third list contains the tool_call_id's associated with the tool responses.
"""
def tool_result_images_reducer(
    accum: ImagesAccumulator,
    message: ChatMessage,
) -> ImagesAccumulator:
    messages, pending_content, tool_call_ids = accum
    # if there are tool result images, pull them out into a ChatUserMessage
    if (
        isinstance(message, ChatMessageTool)
        and isinstance(message.content, list)
        and any([isinstance(c, ContentImage) for c in message.content])
    ):
        init_accum: ImageContentAccumulator = ([], [])
        new_user_message_content, edited_tool_message_content = functools.reduce(
            tool_result_image_content_reducer, message.content, init_accum
        )
        return (
            messages
            + [
                ChatMessageTool(
                    id=message.id,
                    content=edited_tool_message_content,
                    tool_call_id=message.tool_call_id,
                    function=message.function,
                )
            ],
            pending_content + new_user_message_content,
            tool_call_ids + ([message.tool_call_id] if message.tool_call_id else []),
        )
    else:
        return (
            maybe_adding_user_message(messages, pending_content, tool_call_ids)
            + [message],
            [],
            [],
        )
ImageContentAccumulator = tuple[list[Content], list[Content]]
"""
ImageContentAccumulator is a tuple containing two lists of Content objects:
- The first list contains ContentImages that will be included in a fabricated user message.
- The second list contains modified content for the tool message with images replaced with text.
"""
def tool_result_image_content_reducer(
    acc: ImageContentAccumulator, content: Content
) -> ImageContentAccumulator:
    """
    Reduces the messages Content into two separate lists: one for a fabricated user message that will contain the images and one for modified tool message with the images replaced with text.
    Returns:
      ImageContentReducer: A tuple containing two lists of Content objects.
        - The first list contains the images that will be included in a fabricated user message.
        - The second list contains modified content for the tool message with images replaced with text.
    """
    new_user_message_content, edited_tool_message_content = acc
    if isinstance(content, ContentImage):
        return new_user_message_content + [content], edited_tool_message_content + [
            ContentText(text="Image content is included below.")
        ]
    else:
        return new_user_message_content, edited_tool_message_content + [content]
def maybe_adding_user_message(
    messages: list[ChatMessage], content: list[Content], tool_call_ids: list[str]
) -> list[ChatMessage]:
    """If content is empty, return messages, otherwise, create a new ChatMessageUser with it and return a new messages list with that message added."""
    return (
        messages + [ChatMessageUser(content=content, tool_call_id=tool_call_ids)]
        if content
        else messages
    )
# Functions to reduce consecutive user messages to a single user message -> required for some models
def collapse_consecutive_user_messages(
    messages: list[ChatMessage],
) -> list[ChatMessage]:
    return functools.reduce(user_message_reducer, messages, [])
# Functions to reduce consecutive assistant messages to a single user message -> required for some models
def collapse_consecutive_assistant_messages(
    messages: list[ChatMessage],
) -> list[ChatMessage]:
    return functools.reduce(assistant_message_reducer, messages, [])
def user_message_reducer(
    messages: list[ChatMessage],
    message: ChatMessage,
) -> list[ChatMessage]:
    return consecutive_message_reducer(messages, message, ChatMessageUser)
def assistant_message_reducer(
    messages: list[ChatMessage],
    message: ChatMessage,
) -> list[ChatMessage]:
    return consecutive_message_reducer(messages, message, ChatMessageAssistant)
def consecutive_message_reducer(
    messages: list[ChatMessage],
    message: ChatMessage,
    message_type: Type[ChatMessage],
) -> list[ChatMessage]:
    if (
        isinstance(message, message_type)
        and len(messages) > 0
        and isinstance(messages[-1], message_type)
    ):
        messages[-1] = combine_messages(messages[-1], message, message_type)
    else:
        messages.append(message)
    return messages
def combine_messages(
    a: ChatMessage, b: ChatMessage, message_type: Type[ChatMessage]
) -> ChatMessage:
    if isinstance(a.content, str) and isinstance(b.content, str):
        return message_type(id=a.id, content=f"{a.content}\n{b.content}")
    elif isinstance(a.content, list) and isinstance(b.content, list):
        return message_type(id=a.id, content=a.content + b.content)
    elif isinstance(a.content, str) and isinstance(b.content, list):
        return message_type(id=a.id, content=[ContentText(text=a.content), *b.content])
    elif isinstance(a.content, list) and isinstance(b.content, str):
        return message_type(id=a.id, content=a.content + [ContentText(text=b.content)])
    else:
        raise TypeError(
            f"Cannot combine messages with invalid content types: {a.content!r}, {b.content!r}"
        )
def log_model_retry(model_name: str, retry_state: RetryCallState) -> None:
    logger.log(
        HTTP,
        f"-> {model_name} retry {retry_state.attempt_number} after waiting for {retry_state.idle_for}",
    )
def init_active_model(model: Model, config: GenerateConfig) -> None:
    active_model_context_var.set(model)
    set_active_generate_config(config)
def active_model() -> Model | None:
    """The model currently being evaluated.
    Returns:
       The model currently being evaluated.
    """
    return active_model_context_var.get(None)
# shared contexts for asyncio tasks
active_model_context_var: ContextVar[Model | None] = ContextVar("active_model")
def handle_sample_message_limit(input: str | list[ChatMessage]) -> None:
    from inspect_ai.log._samples import (
        active_sample_message_limit,
        set_active_sample_total_messages,
    )
    from inspect_ai.solver._limit import SampleLimitExceededError
    total_messages = 1 if isinstance(input, str) else len(input)
    message_limit = active_sample_message_limit()
    if message_limit is not None:
        if total_messages >= message_limit:
            raise SampleLimitExceededError(
                "message", value=total_messages, limit=message_limit
            )
    # set total messages
    set_active_sample_total_messages(total_messages)
def init_model_usage() -> None:
    model_usage_context_var.set({})
def init_sample_model_usage() -> None:
    sample_model_usage_context_var.set({})
def record_model_usage(model: str, usage: ModelUsage) -> None:
    from inspect_ai.log._samples import (
        active_sample_token_limit,
        set_active_sample_total_tokens,
    )
    from inspect_ai.solver._limit import SampleLimitExceededError
    # record usage
    set_model_usage(model, usage, sample_model_usage_context_var.get(None))
    set_model_usage(model, usage, model_usage_context_var.get(None))
    # compute total tokens
    total_tokens = sample_total_tokens()
    # update active sample
    set_active_sample_total_tokens(total_tokens)
    # check for token limit overflow and raise
    token_limit = active_sample_token_limit()
    if token_limit is not None:
        if total_tokens > token_limit:
            raise SampleLimitExceededError(
                "token", value=total_tokens, limit=token_limit
            )
def set_model_usage(
    model: str, usage: ModelUsage, model_usage: dict[str, ModelUsage] | None
) -> None:
    if model_usage is not None:
        total_usage: ModelUsage | None = model_usage.get(model, None)
        if not total_usage:
            total_usage = ModelUsage()
        total_usage.input_tokens += usage.input_tokens
        total_usage.output_tokens += usage.output_tokens
        total_usage.total_tokens += usage.total_tokens
        if usage.input_tokens_cache_write is not None:
            if total_usage.input_tokens_cache_write is None:
                total_usage.input_tokens_cache_write = 0
            total_usage.input_tokens_cache_write += usage.input_tokens_cache_write
        if usage.input_tokens_cache_read is not None:
            if total_usage.input_tokens_cache_read is None:
                total_usage.input_tokens_cache_read = 0
            total_usage.input_tokens_cache_read += usage.input_tokens_cache_read
        if usage.reasoning_tokens is not None:
            if total_usage.reasoning_tokens is None:
                total_usage.reasoning_tokens = 0
            total_usage.reasoning_tokens += usage.reasoning_tokens
        model_usage[model] = total_usage
def model_usage() -> dict[str, ModelUsage]:
    return model_usage_context_var.get()
model_usage_context_var: ContextVar[dict[str, ModelUsage]] = ContextVar(
    "model_usage", default={}
)
def sample_model_usage() -> dict[str, ModelUsage]:
    return sample_model_usage_context_var.get()
def sample_total_tokens() -> int:
    total_tokens = [usage.total_tokens for usage in iter(sample_model_usage().values())]
    return sum(total_tokens)
sample_model_usage_context_var: ContextVar[dict[str, ModelUsage]] = ContextVar(
    "sample_model_usage", default={}
)

================
File: src/inspect_ai/model/_openai.py
================
import json
import re
from typing import Literal
from openai.types.chat import (
    ChatCompletion,
    ChatCompletionAssistantMessageParam,
    ChatCompletionContentPartImageParam,
    ChatCompletionContentPartInputAudioParam,
    ChatCompletionContentPartParam,
    ChatCompletionContentPartRefusalParam,
    ChatCompletionContentPartTextParam,
    ChatCompletionDeveloperMessageParam,
    ChatCompletionMessage,
    ChatCompletionMessageParam,
    ChatCompletionMessageToolCall,
    ChatCompletionMessageToolCallParam,
    ChatCompletionNamedToolChoiceParam,
    ChatCompletionSystemMessageParam,
    ChatCompletionToolChoiceOptionParam,
    ChatCompletionToolMessageParam,
    ChatCompletionToolParam,
    ChatCompletionUserMessageParam,
)
from openai.types.chat.chat_completion import Choice, ChoiceLogprobs
from openai.types.chat.chat_completion_message_tool_call import Function
from openai.types.completion_usage import CompletionUsage
from openai.types.shared_params.function_definition import FunctionDefinition
from inspect_ai._util.content import (
    Content,
    ContentAudio,
    ContentImage,
    ContentReasoning,
    ContentText,
)
from inspect_ai._util.images import file_as_data_uri
from inspect_ai._util.url import is_http_url
from inspect_ai.model._call_tools import parse_tool_call
from inspect_ai.model._model_output import ChatCompletionChoice, Logprobs
from inspect_ai.model._reasoning import parse_content_with_reasoning
from inspect_ai.tool import ToolCall, ToolChoice, ToolFunction, ToolInfo
from ._chat_message import (
    ChatMessage,
    ChatMessageAssistant,
    ChatMessageSystem,
    ChatMessageTool,
    ChatMessageUser,
)
from ._model_output import ModelUsage, StopReason, as_stop_reason
def is_o_series(name: str) -> bool:
    return bool(re.match(r"(^|.*\/)o\d+", name))
def is_o1_mini(name: str) -> bool:
    return name.startswith("o1-mini")
def is_o1_preview(name: str) -> bool:
    return name.startswith("o1-preview")
def is_gpt(name: str) -> bool:
    return name.startswith("gpt")
def openai_chat_tool_call(tool_call: ToolCall) -> ChatCompletionMessageToolCall:
    return ChatCompletionMessageToolCall(
        type="function",
        id=tool_call.id,
        function=Function(
            name=tool_call.function, arguments=json.dumps(tool_call.arguments)
        ),
    )
def openai_chat_tool_call_param(
    tool_call: ToolCall,
) -> ChatCompletionMessageToolCallParam:
    return ChatCompletionMessageToolCallParam(
        id=tool_call.id,
        function=dict(
            name=tool_call.function, arguments=json.dumps(tool_call.arguments)
        ),
        type=tool_call.type,
    )
async def openai_chat_completion_part(
    content: Content,
) -> ChatCompletionContentPartParam:
    if content.type == "text":
        return ChatCompletionContentPartTextParam(type="text", text=content.text)
    elif content.type == "image":
        # API takes URL or base64 encoded file. If it's a remote file or
        # data URL leave it alone, otherwise encode it
        image_url = content.image
        detail = content.detail
        if not is_http_url(image_url):
            image_url = await file_as_data_uri(image_url)
        return ChatCompletionContentPartImageParam(
            type="image_url",
            image_url=dict(url=image_url, detail=detail),
        )
    elif content.type == "audio":
        audio_data = await file_as_data_uri(content.audio)
        return ChatCompletionContentPartInputAudioParam(
            type="input_audio", input_audio=dict(data=audio_data, format=content.format)
        )
    else:
        raise RuntimeError(
            "Video content is not currently supported by Open AI chat models."
        )
async def openai_chat_message(
    message: ChatMessage, model: str
) -> ChatCompletionMessageParam:
    if message.role == "system":
        # o1-mini does not support developer or system messages
        # (see Dec 17, 2024 changelog: https://platform.openai.com/docs/changelog)
        if is_o1_mini(model):
            return ChatCompletionUserMessageParam(role="user", content=message.text)
        # other o-series models use 'developer' rather than 'system' messages
        # https://platform.openai.com/docs/guides/reasoning#advice-on-prompting
        elif is_o_series(model):
            return ChatCompletionDeveloperMessageParam(
                role="developer", content=message.text
            )
        # gpt models use standard 'system' messages
        else:
            return ChatCompletionSystemMessageParam(
                role=message.role, content=message.text
            )
    elif message.role == "user":
        return ChatCompletionUserMessageParam(
            role=message.role,
            content=(
                message.content
                if isinstance(message.content, str)
                else [
                    await openai_chat_completion_part(content)
                    for content in message.content
                ]
            ),
        )
    elif message.role == "assistant":
        if message.tool_calls:
            return ChatCompletionAssistantMessageParam(
                role=message.role,
                content=openai_assistant_content(message),
                tool_calls=[
                    openai_chat_tool_call_param(call) for call in message.tool_calls
                ],
            )
        else:
            return ChatCompletionAssistantMessageParam(
                role=message.role, content=openai_assistant_content(message)
            )
    elif message.role == "tool":
        return ChatCompletionToolMessageParam(
            role=message.role,
            content=(
                f"Error: {message.error.message}" if message.error else message.text
            ),
            tool_call_id=str(message.tool_call_id),
        )
    else:
        raise ValueError(f"Unexpected message role {message.role}")
async def openai_chat_messages(
    messages: list[ChatMessage], model: str
) -> list[ChatCompletionMessageParam]:
    return [await openai_chat_message(message, model) for message in messages]
def openai_assistant_content(message: ChatMessageAssistant) -> str:
    if isinstance(message.content, str):
        content = message.content
    else:
        content = ""
        for c in message.content:
            if c.type == "reasoning":
                attribs = ""
                if c.signature is not None:
                    attribs = f'{attribs} signature="{c.signature}"'
                if c.redacted:
                    attribs = f'{attribs} redacted="true"'
                content = f"{content}\n<think{attribs}>\n{c.reasoning}\n</think>\n"
            elif c.type == "text":
                content = f"{content}\n{c.text}"
    return content
def openai_chat_choices(choices: list[ChatCompletionChoice]) -> list[Choice]:
    oai_choices: list[Choice] = []
    for index, choice in enumerate(choices):
        content = openai_assistant_content(choice.message)
        if choice.message.tool_calls:
            tool_calls = [openai_chat_tool_call(tc) for tc in choice.message.tool_calls]
        else:
            tool_calls = None
        message = ChatCompletionMessage(
            role="assistant", content=content, tool_calls=tool_calls
        )
        oai_choices.append(
            Choice(
                finish_reason=openai_finish_reason(choice.stop_reason),
                index=index,
                message=message,
                logprobs=ChoiceLogprobs(**choice.logprobs.model_dump())
                if choice.logprobs is not None
                else None,
            )
        )
    return oai_choices
def openai_completion_usage(usage: ModelUsage) -> CompletionUsage:
    return CompletionUsage(
        completion_tokens=usage.output_tokens,
        prompt_tokens=usage.input_tokens,
        total_tokens=usage.total_tokens,
    )
def openai_finish_reason(
    stop_reason: StopReason,
) -> Literal["stop", "length", "tool_calls", "content_filter", "function_call"]:
    match stop_reason:
        case "stop" | "tool_calls" | "content_filter":
            return stop_reason
        case "model_length":
            return "length"
        case _:
            return "stop"
def openai_chat_tool_param(tool: ToolInfo) -> ChatCompletionToolParam:
    function = FunctionDefinition(
        name=tool.name,
        description=tool.description,
        parameters=tool.parameters.model_dump(exclude_none=True),
    )
    return ChatCompletionToolParam(type="function", function=function)
def openai_chat_tools(tools: list[ToolInfo]) -> list[ChatCompletionToolParam]:
    return [openai_chat_tool_param(tool) for tool in tools]
def openai_chat_tool_choice(
    tool_choice: ToolChoice,
) -> ChatCompletionToolChoiceOptionParam:
    if isinstance(tool_choice, ToolFunction):
        return ChatCompletionNamedToolChoiceParam(
            type="function", function=dict(name=tool_choice.name)
        )
    # openai supports 'any' via the 'required' keyword
    elif tool_choice == "any":
        return "required"
    else:
        return tool_choice
def chat_tool_calls_from_openai(
    message: ChatCompletionMessage, tools: list[ToolInfo]
) -> list[ToolCall] | None:
    if message.tool_calls:
        return [
            parse_tool_call(call.id, call.function.name, call.function.arguments, tools)
            for call in message.tool_calls
        ]
    else:
        return None
def chat_messages_from_openai(
    messages: list[ChatCompletionMessageParam],
) -> list[ChatMessage]:
    # track tool names by id
    tool_names: dict[str, str] = {}
    chat_messages: list[ChatMessage] = []
    for message in messages:
        content: str | list[Content] = []
        if message["role"] == "system" or message["role"] == "developer":
            sys_content = message["content"]
            if isinstance(sys_content, str):
                chat_messages.append(ChatMessageSystem(content=sys_content))
            else:
                content = []
                for sc in sys_content:
                    content.extend(content_from_openai(sc))
                chat_messages.append(ChatMessageSystem(content=content))
        elif message["role"] == "user":
            user_content = message["content"]
            if isinstance(user_content, str):
                chat_messages.append(ChatMessageUser(content=user_content))
            else:
                content = []
                for uc in user_content:
                    content.extend(content_from_openai(uc))
                chat_messages.append(ChatMessageUser(content=content))
        elif message["role"] == "assistant":
            # resolve content
            asst_content = message.get("content", None)
            if isinstance(asst_content, str):
                result = parse_content_with_reasoning(asst_content)
                if result is not None:
                    content = [
                        ContentReasoning(
                            reasoning=result.reasoning,
                            signature=result.signature,
                            redacted=result.redacted,
                        ),
                        ContentText(text=result.content),
                    ]
                else:
                    content = asst_content
            elif asst_content is None:
                content = message.get("refusal", None) or ""
            else:
                content = []
                for ac in asst_content:
                    content.extend(content_from_openai(ac, parse_reasoning=True))
            # resolve reasoning (OpenAI doesn't suport this however OpenAI-compatible
            # interfaces e.g. DeepSeek do include this field so we pluck it out)
            reasoning = message.get("reasoning_content", None) or message.get(
                "reasoning", None
            )
            if reasoning is not None:
                if isinstance(content, str):
                    content = [ContentText(text=content)]
                else:
                    content.insert(0, ContentReasoning(reasoning=str(reasoning)))
            # return message
            if "tool_calls" in message:
                tool_calls: list[ToolCall] = []
                for call in message["tool_calls"]:
                    tool_calls.append(tool_call_from_openai(call))
                    tool_names[call["id"]] = call["function"]["name"]
            else:
                tool_calls = []
            chat_messages.append(
                ChatMessageAssistant(
                    content=content,
                    tool_calls=tool_calls or None,
                )
            )
        elif message["role"] == "tool":
            tool_content = message.get("content", None) or ""
            if isinstance(tool_content, str):
                content = tool_content
            else:
                content = []
                for tc in tool_content:
                    content.extend(content_from_openai(tc))
            chat_messages.append(
                ChatMessageTool(
                    content=content,
                    tool_call_id=message["tool_call_id"],
                    function=tool_names.get(message["tool_call_id"], ""),
                )
            )
        else:
            raise ValueError(f"Unexpected message param type: {type(message)}")
    return chat_messages
def tool_call_from_openai(tool_call: ChatCompletionMessageToolCallParam) -> ToolCall:
    return parse_tool_call(
        tool_call["id"],
        tool_call["function"]["name"],
        tool_call["function"]["arguments"],
    )
def content_from_openai(
    content: ChatCompletionContentPartParam | ChatCompletionContentPartRefusalParam,
    parse_reasoning: bool = False,
) -> list[Content]:
    # Some providers omit the type tag and use "object-with-a-single-field" encoding
    if "type" not in content and len(content) == 1:
        content["type"] = list(content.keys())[0]  # type: ignore[arg-type]
    if content["type"] == "text":
        text = content["text"]
        if parse_reasoning:
            result = parse_content_with_reasoning(text)
            if result:
                return [
                    ContentReasoning(
                        reasoning=result.reasoning,
                        signature=result.signature,
                        redacted=result.redacted,
                    ),
                    ContentText(text=result.content),
                ]
            else:
                return [ContentText(text=text)]
        else:
            return [ContentText(text=text)]
    elif content["type"] == "reasoning":  # type: ignore[comparison-overlap]
        return [ContentReasoning(reasoning=content["reasoning"])]
    elif content["type"] == "image_url":
        return [
            ContentImage(
                image=content["image_url"]["url"], detail=content["image_url"]["detail"]
            )
        ]
    elif content["type"] == "input_audio":
        return [
            ContentAudio(
                audio=content["input_audio"]["data"],
                format=content["input_audio"]["format"],
            )
        ]
    elif content["type"] == "refusal":
        return [ContentText(text=content["refusal"])]
    else:
        content_type = content["type"]
        raise ValueError(f"Unexpected content type '{content_type}' in message.")
def chat_message_assistant_from_openai(
    message: ChatCompletionMessage, tools: list[ToolInfo]
) -> ChatMessageAssistant:
    refusal = getattr(message, "refusal", None)
    reasoning = getattr(message, "reasoning_content", None) or getattr(
        message, "reasoning", None
    )
    msg_content = refusal or message.content or ""
    if reasoning is not None:
        content: str | list[Content] = [
            ContentReasoning(reasoning=str(reasoning)),
            ContentText(text=msg_content),
        ]
    else:
        content = msg_content
    return ChatMessageAssistant(
        content=content,
        source="generate",
        tool_calls=chat_tool_calls_from_openai(message, tools),
    )
def chat_choices_from_openai(
    response: ChatCompletion, tools: list[ToolInfo]
) -> list[ChatCompletionChoice]:
    choices = list(response.choices)
    choices.sort(key=lambda c: c.index)
    return [
        ChatCompletionChoice(
            message=chat_message_assistant_from_openai(choice.message, tools),
            stop_reason=as_stop_reason(choice.finish_reason),
            logprobs=(
                Logprobs(**choice.logprobs.model_dump())
                if choice.logprobs is not None
                else None
            ),
        )
        for choice in choices
    ]

================
File: src/inspect_ai/model/_providers/anthropic.py
================
import functools
import os
import re
import sys
from copy import copy
from logging import getLogger
from typing import Any, Literal, Optional, Tuple, TypedDict, cast
import httpcore
import httpx
from inspect_ai._util.http import is_retryable_http_status
from .util.hooks import HttpxHooks
if sys.version_info >= (3, 11):
    from typing import NotRequired
else:
    from typing_extensions import NotRequired
from anthropic import (
    APIConnectionError,
    APIStatusError,
    APITimeoutError,
    AsyncAnthropic,
    AsyncAnthropicBedrock,
    AsyncAnthropicVertex,
    BadRequestError,
    NotGiven,
)
from anthropic._types import Body
from anthropic.types import (
    ImageBlockParam,
    Message,
    MessageParam,
    RedactedThinkingBlock,
    RedactedThinkingBlockParam,
    TextBlock,
    TextBlockParam,
    ThinkingBlock,
    ThinkingBlockParam,
    ToolParam,
    ToolResultBlockParam,
    ToolUseBlock,
    ToolUseBlockParam,
    message_create_params,
)
from pydantic import JsonValue
from typing_extensions import override
from inspect_ai._util.constants import (
    BASE_64_DATA_REMOVED,
    NO_CONTENT,
)
from inspect_ai._util.content import (
    Content,
    ContentImage,
    ContentReasoning,
    ContentText,
)
from inspect_ai._util.error import exception_message
from inspect_ai._util.images import file_as_data_uri
from inspect_ai._util.logger import warn_once
from inspect_ai._util.url import data_uri_mime_type, data_uri_to_base64
from inspect_ai.tool import ToolCall, ToolChoice, ToolFunction, ToolInfo
from .._chat_message import ChatMessage, ChatMessageAssistant, ChatMessageSystem
from .._generate_config import GenerateConfig
from .._model import ModelAPI
from .._model_call import ModelCall
from .._model_output import ChatCompletionChoice, ModelOutput, ModelUsage, StopReason
from .util import environment_prerequisite_error, model_base_url
logger = getLogger(__name__)
ANTHROPIC_API_KEY = "ANTHROPIC_API_KEY"
class AnthropicAPI(ModelAPI):
    def __init__(
        self,
        model_name: str,
        base_url: str | None = None,
        api_key: str | None = None,
        config: GenerateConfig = GenerateConfig(),
        **model_args: Any,
    ):
        # extract any service prefix from model name
        parts = model_name.split("/")
        if len(parts) > 1:
            self.service: str | None = parts[0]
            model_name = "/".join(parts[1:])
        else:
            self.service = None
        # collect gemerate model_args (then delete them so we can pass the rest on)
        def collect_model_arg(name: str) -> Any | None:
            nonlocal model_args
            value = model_args.get(name, None)
            if value is not None:
                model_args.pop(name)
            return value
        self.extra_body: Body | None = collect_model_arg("extra_body")
        # call super
        super().__init__(
            model_name=model_name,
            base_url=base_url,
            api_key=api_key,
            api_key_vars=[ANTHROPIC_API_KEY],
            config=config,
        )
        # create client
        if self.is_bedrock():
            base_url = model_base_url(
                base_url, ["ANTHROPIC_BEDROCK_BASE_URL", "BEDROCK_ANTHROPIC_BASE_URL"]
            )
            # resolve the default region
            aws_region = None
            base_region = os.environ.get("AWS_REGION", None)
            if base_region is None:
                aws_region = os.environ.get("AWS_DEFAULT_REGION", None)
            self.client: (
                AsyncAnthropic | AsyncAnthropicBedrock | AsyncAnthropicVertex
            ) = AsyncAnthropicBedrock(
                base_url=base_url,
                aws_region=aws_region,
                **model_args,
            )
        elif self.is_vertex():
            base_url = model_base_url(
                base_url, ["ANTHROPIC_VERTEX_BASE_URL", "VERTEX_ANTHROPIC_BASE_URL"]
            )
            region = os.environ.get("ANTHROPIC_VERTEX_REGION", NotGiven())
            project_id = os.environ.get("ANTHROPIC_VERTEX_PROJECT_ID", NotGiven())
            self.client = AsyncAnthropicVertex(
                region=region,
                project_id=project_id,
                base_url=base_url,
                **model_args,
            )
        else:
            # resolve api_key
            if not self.api_key:
                self.api_key = os.environ.get(ANTHROPIC_API_KEY, None)
            if self.api_key is None:
                raise environment_prerequisite_error("Anthropic", ANTHROPIC_API_KEY)
            base_url = model_base_url(base_url, "ANTHROPIC_BASE_URL")
            self.client = AsyncAnthropic(
                base_url=base_url,
                api_key=self.api_key,
                **model_args,
            )
        # create time tracker
        self._http_hooks = HttpxHooks(self.client._client)
    @override
    async def close(self) -> None:
        await self.client.close()
    def is_bedrock(self) -> bool:
        return self.service == "bedrock"
    def is_vertex(self) -> bool:
        return self.service == "vertex"
    async def generate(
        self,
        input: list[ChatMessage],
        tools: list[ToolInfo],
        tool_choice: ToolChoice,
        config: GenerateConfig,
    ) -> ModelOutput | tuple[ModelOutput | Exception, ModelCall]:
        # allocate request_id (so we can see it from ModelCall)
        request_id = self._http_hooks.start_request()
        # setup request and response for ModelCall
        request: dict[str, Any] = {}
        response: dict[str, Any] = {}
        def model_call() -> ModelCall:
            return ModelCall.create(
                request=request,
                response=response,
                filter=model_call_filter,
                time=self._http_hooks.end_request(request_id),
            )
        # generate
        try:
            (
                system_param,
                tools_param,
                messages,
                computer_use,
            ) = await self.resolve_chat_input(input, tools, config)
            # prepare request params (assembed this way so we can log the raw model call)
            request = dict(messages=messages)
            # system messages and tools
            if system_param is not None:
                request["system"] = system_param
            request["tools"] = tools_param
            if len(tools) > 0:
                request["tool_choice"] = message_tool_choice(
                    tool_choice, self.is_using_thinking(config)
                )
            # additional options
            req, headers, betas = self.completion_config(config)
            request = request | req
            # extra headers (for time tracker and computer use)
            extra_headers = headers | {HttpxHooks.REQUEST_ID_HEADER: request_id}
            if computer_use:
                betas.append("computer-use-2025-01-24")
            if len(betas) > 0:
                extra_headers["anthropic-beta"] = ",".join(betas)
            request["extra_headers"] = extra_headers
            # extra_body
            if self.extra_body is not None:
                request["extra_body"] = self.extra_body
            # make request (stream if we are using reasoning)
            if self.is_using_thinking(config):
                async with self.client.messages.stream(**request) as stream:
                    message = await stream.get_final_message()
            else:
                message = await self.client.messages.create(**request, stream=False)
            # set response for ModelCall
            response = message.model_dump()
            # extract output
            output = await model_output_from_message(
                self.client, self.model_name, message, tools
            )
            # return output and call
            return output, model_call()
        except BadRequestError as ex:
            return self.handle_bad_request(ex), model_call()
        except APIStatusError as ex:
            if ex.status_code == 413:
                return ModelOutput.from_content(
                    model=self.model_name,
                    content=ex.message,
                    stop_reason="model_length",
                    error=ex.message,
                ), model_call()
            else:
                raise ex
    def completion_config(
        self, config: GenerateConfig
    ) -> tuple[dict[str, Any], dict[str, str], list[str]]:
        max_tokens = cast(int, config.max_tokens)
        params = dict(model=self.model_name, max_tokens=max_tokens)
        headers: dict[str, str] = {}
        betas: list[str] = []
        # some params not compatible with thinking models
        if not self.is_using_thinking(config):
            if config.temperature is not None:
                params["temperature"] = config.temperature
            if config.top_p is not None:
                params["top_p"] = config.top_p
            if config.top_k is not None:
                params["top_k"] = config.top_k
        # some thinking-only stuff
        if self.is_using_thinking(config):
            params["thinking"] = dict(
                type="enabled", budget_tokens=config.reasoning_tokens
            )
            headers["anthropic-version"] = "2023-06-01"
            if max_tokens > 8192:
                betas.append("output-128k-2025-02-19")
        # config that applies to all models
        if config.stop_seqs is not None:
            params["stop_sequences"] = config.stop_seqs
        # return config
        return params, headers, betas
    @override
    def max_tokens(self) -> int | None:
        # anthropic requires you to explicitly specify max_tokens (most others
        # set it to the maximum allowable output tokens for the model).
        # set to 4096 which is the highest possible for claude 3 (claude 3.5
        # allows up to 8192)
        return 4096
    @override
    def max_tokens_for_config(self, config: GenerateConfig) -> int | None:
        max_tokens = cast(int, self.max_tokens())
        if self.is_thinking_model() and config.reasoning_tokens is not None:
            max_tokens = max_tokens + config.reasoning_tokens
        return max_tokens
    def is_using_thinking(self, config: GenerateConfig) -> bool:
        return self.is_thinking_model() and config.reasoning_tokens is not None
    def is_thinking_model(self) -> bool:
        return not self.is_claude_3() and not self.is_claude_3_5()
    def is_claude_3(self) -> bool:
        return re.search(r"claude-3-[a-zA-Z]", self.model_name) is not None
    def is_claude_3_5(self) -> bool:
        return "claude-3-5-" in self.model_name
    def is_claude_3_7(self) -> bool:
        return "claude-3-7-" in self.model_name
    @override
    def connection_key(self) -> str:
        return str(self.api_key)
    @override
    def should_retry(self, ex: Exception) -> bool:
        if isinstance(ex, APIStatusError):
            return is_retryable_http_status(ex.status_code)
        elif isinstance(
            ex,
            APIConnectionError
            | APITimeoutError
            | httpx.RemoteProtocolError
            | httpcore.RemoteProtocolError,
        ):
            return True
        else:
            return False
    @override
    def collapse_user_messages(self) -> bool:
        return True
    @override
    def collapse_assistant_messages(self) -> bool:
        return True
    @override
    def tools_required(self) -> bool:
        return True
    @override
    def tool_result_images(self) -> bool:
        return True
    @override
    def emulate_reasoning_history(self) -> bool:
        return False
    @override
    def force_reasoning_history(self) -> Literal["none", "all", "last"] | None:
        return "all"
    # convert some common BadRequestError states into 'refusal' model output
    def handle_bad_request(self, ex: BadRequestError) -> ModelOutput | Exception:
        error = exception_message(ex).lower()
        content: str | None = None
        stop_reason: StopReason | None = None
        if "prompt is too long" in error:
            if (
                isinstance(ex.body, dict)
                and "error" in ex.body.keys()
                and isinstance(ex.body.get("error"), dict)
            ):
                error_dict = cast(dict[str, Any], ex.body.get("error"))
                if "message" in error_dict:
                    content = str(error_dict.get("message"))
                else:
                    content = str(error_dict)
            else:
                content = error
            stop_reason = "model_length"
        elif "content filtering" in error:
            content = "Sorry, but I am unable to help with that request."
            stop_reason = "content_filter"
        if content and stop_reason:
            return ModelOutput.from_content(
                model=self.model_name,
                content=content,
                stop_reason=stop_reason,
                error=error,
            )
        else:
            return ex
    async def resolve_chat_input(
        self,
        input: list[ChatMessage],
        tools: list[ToolInfo],
        config: GenerateConfig,
    ) -> Tuple[
        list[TextBlockParam] | None, list["ToolParamDef"], list[MessageParam], bool
    ]:
        # extract system message
        system_messages, messages = split_system_messages(input, config)
        # messages
        message_params = [(await message_param(message)) for message in messages]
        # collapse user messages (as Inspect 'tool' messages become Claude 'user' messages)
        message_params = functools.reduce(
            consecutive_user_message_reducer, message_params, []
        )
        # tools
        tools_params, computer_use = self.tool_params_for_tools(tools, config)
        # system messages
        if len(system_messages) > 0:
            system_param: list[TextBlockParam] | None = [
                TextBlockParam(type="text", text=message.text)
                for message in system_messages
            ]
        else:
            system_param = None
        # add caching directives if necessary
        cache_prompt = (
            config.cache_prompt
            if isinstance(config.cache_prompt, bool)
            else True
            if len(tools_params)
            else False
        )
        # only certain claude models qualify
        if cache_prompt:
            if (
                "claude-3-sonnet" in self.model_name
                or "claude-2" in self.model_name
                or "claude-instant" in self.model_name
            ):
                cache_prompt = False
        if cache_prompt:
            # system
            if system_param:
                add_cache_control(system_param[-1])
            # tools
            if tools_params:
                add_cache_control(tools_params[-1])
            # last 2 user messages
            user_message_params = list(
                filter(lambda m: m["role"] == "user", reversed(message_params))
            )
            for message in user_message_params[:2]:
                if isinstance(message["content"], str):
                    text_param = TextBlockParam(type="text", text=message["content"])
                    add_cache_control(text_param)
                    message["content"] = [text_param]
                else:
                    content = list(message["content"])
                    add_cache_control(cast(dict[str, Any], content[-1]))
        # return chat input
        return system_param, tools_params, message_params, computer_use
    def tool_params_for_tools(
        self, tools: list[ToolInfo], config: GenerateConfig
    ) -> tuple[list["ToolParamDef"], bool]:
        # tool params and computer_use bit to return
        tool_params: list["ToolParamDef"] = []
        computer_use = False
        # for each tool, check if it has a native computer use implementation and use that
        # when available (noting that we need to set the computer use request header)
        for tool in tools:
            computer_use_tool = (
                self.computer_use_tool_param(tool)
                if config.internal_tools is not False
                else None
            )
            if computer_use_tool:
                tool_params.append(computer_use_tool)
                computer_use = True
            else:
                tool_params.append(
                    ToolParam(
                        name=tool.name,
                        description=tool.description,
                        input_schema=tool.parameters.model_dump(exclude_none=True),
                    )
                )
        return tool_params, computer_use
    def computer_use_tool_param(
        self, tool: ToolInfo
    ) -> Optional["ComputerUseToolParam"]:
        # check for compatible 'computer' tool
        if tool.name == "computer" and (
            sorted(tool.parameters.properties.keys())
            == sorted(
                [
                    "action",
                    "coordinate",
                    "duration",
                    "scroll_amount",
                    "scroll_direction",
                    "start_coordinate",
                    "text",
                ]
            )
        ):
            if self.is_claude_3_5():
                warn_once(
                    logger,
                    "Use of Anthropic's native computer use support is not enabled in Claude 3.5. Please use 3.7 or later to leverage the native support.",
                )
                return None
            return ComputerUseToolParam(
                type="computer_20250124",
                name="computer",
                # Note: The dimensions passed here for display_width_px and display_height_px should
                # match the dimensions of screenshots returned by the tool.
                # Those dimensions will always be one of the values in MAX_SCALING_TARGETS
                # in _x11_client.py.
                # TODO: enhance this code to calculate the dimensions based on the scaled screen
                # size used by the container.
                display_width_px=1366,
                display_height_px=768,
                display_number=1,
            )
        # not a computer_use tool
        else:
            return None
# native anthropic tool definitions for computer use beta
# https://docs.anthropic.com/en/docs/build-with-claude/computer-use
class ComputerUseToolParam(TypedDict):
    type: str
    name: str
    display_width_px: NotRequired[int]
    display_height_px: NotRequired[int]
    display_number: NotRequired[int]
# tools can be either a stock tool param or a special computer use tool param
ToolParamDef = ToolParam | ComputerUseToolParam
def add_cache_control(
    param: TextBlockParam | ToolParam | ComputerUseToolParam | dict[str, Any],
) -> None:
    cast(dict[str, Any], param)["cache_control"] = {"type": "ephemeral"}
def consecutive_user_message_reducer(
    messages: list[MessageParam],
    message: MessageParam,
) -> list[MessageParam]:
    return consective_message_reducer(messages, message, "user")
def consective_message_reducer(
    messages: list[MessageParam],
    message: MessageParam,
    role: Literal["user", "assistant"],
) -> list[MessageParam]:
    if message["role"] == role and len(messages) > 0 and messages[-1]["role"] == role:
        messages[-1] = combine_messages(messages[-1], message)
    else:
        messages.append(message)
    return messages
def combine_messages(a: MessageParam, b: MessageParam) -> MessageParam:
    role = a["role"]
    a_content = a["content"]
    b_content = b["content"]
    if isinstance(a_content, str) and isinstance(b_content, str):
        return MessageParam(role=role, content=f"{a_content}\n{b_content}")
    elif isinstance(a_content, list) and isinstance(b_content, list):
        return MessageParam(role=role, content=a_content + b_content)
    elif isinstance(a_content, str) and isinstance(b_content, list):
        return MessageParam(
            role=role, content=[TextBlockParam(type="text", text=a_content)] + b_content
        )
    elif isinstance(a_content, list) and isinstance(b_content, str):
        return MessageParam(
            role=role, content=a_content + [TextBlockParam(type="text", text=b_content)]
        )
    else:
        raise ValueError(f"Unexpected content types for messages: {a}, {b}")
def message_tool_choice(
    tool_choice: ToolChoice, thinking_model: bool
) -> message_create_params.ToolChoice:
    if isinstance(tool_choice, ToolFunction):
        # forced tool use not compatible with thinking models
        if thinking_model:
            return {"type": "any"}
        else:
            return {"type": "tool", "name": tool_choice.name}
    elif tool_choice == "any":
        return {"type": "any"}
    elif tool_choice == "none":
        warn_once(
            logger,
            'The Anthropic API does not support tool_choice="none" (using "auto" instead)',
        )
        return {"type": "auto"}
    else:
        return {"type": "auto"}
async def message_param(message: ChatMessage) -> MessageParam:
    # if content is empty that is going to result in an error when we replay
    # this message to claude, so in that case insert a NO_CONTENT message
    if isinstance(message.content, list) and len(message.content) == 0:
        message = message.model_copy()
        message.content = [ContentText(text=NO_CONTENT)]
    # no system role for anthropic (this is more like an assertion,
    # as these should have already been filtered out)
    if message.role == "system":
        raise ValueError("Anthropic models do not support the system role")
    # "tool" means serving a tool call result back to claude
    elif message.role == "tool":
        if message.error is not None:
            content: (
                str
                | list[
                    TextBlockParam
                    | ImageBlockParam
                    | ThinkingBlockParam
                    | RedactedThinkingBlockParam
                ]
            ) = message.error.message
            # anthropic requires that content be populated when
            # is_error is true (throws bad_request_error when not)
            # so make sure this precondition is met
            if not content:
                content = message.text
            if not content:
                content = "error"
        elif isinstance(message.content, str):
            content = [TextBlockParam(type="text", text=message.content or NO_CONTENT)]
        else:
            content = [
                await message_param_content(content) for content in message.content
            ]
        return MessageParam(
            role="user",
            content=[
                ToolResultBlockParam(
                    tool_use_id=str(message.tool_call_id),
                    type="tool_result",
                    content=cast(list[TextBlockParam | ImageBlockParam], content),
                    is_error=message.error is not None,
                )
            ],
        )
    # tool_calls means claude is attempting to call our tools
    elif message.role == "assistant" and message.tool_calls:
        # first include content (claude <thinking>)
        tools_content: list[
            TextBlockParam
            | ThinkingBlockParam
            | RedactedThinkingBlockParam
            | ImageBlockParam
            | ToolUseBlockParam
        ] = (
            [TextBlockParam(type="text", text=message.content or NO_CONTENT)]
            if isinstance(message.content, str)
            else (
                [(await message_param_content(content)) for content in message.content]
            )
        )
        # filter out empty text content (sometimes claude passes empty text
        # context back with tool calls but won't let us play them back)
        tools_content = [
            c for c in tools_content if not c["type"] == "text" or len(c["text"]) > 0
        ]
        # now add tools
        for tool_call in message.tool_calls:
            tools_content.append(
                ToolUseBlockParam(
                    type="tool_use",
                    id=tool_call.id,
                    name=tool_call.function,
                    input=tool_call.arguments,
                )
            )
        return MessageParam(
            role=message.role,
            content=tools_content,
        )
    # normal text content
    elif isinstance(message.content, str):
        return MessageParam(role=message.role, content=message.content or NO_CONTENT)
    # mixed text/images
    else:
        return MessageParam(
            role=message.role,
            content=[
                await message_param_content(content) for content in message.content
            ],
        )
async def model_output_from_message(
    client: AsyncAnthropic | AsyncAnthropicBedrock | AsyncAnthropicVertex,
    model: str,
    message: Message,
    tools: list[ToolInfo],
) -> ModelOutput:
    # extract content and tool calls
    content: list[Content] = []
    reasoning_tokens = 0
    tool_calls: list[ToolCall] | None = None
    for content_block in message.content:
        if isinstance(content_block, TextBlock):
            # if this was a tool call then remove <result></result> tags that
            # claude sometimes likes to insert!
            content_text = content_block.text
            if len(tools) > 0:
                content_text = content_text.replace("<result>", "").replace(
                    "</result>", ""
                )
            content.append(ContentText(type="text", text=content_text))
        elif isinstance(content_block, ToolUseBlock):
            tool_calls = tool_calls or []
            tool_calls.append(
                ToolCall(
                    type="function",
                    id=content_block.id,
                    function=content_block.name,
                    arguments=content_block.model_dump().get("input", {}),
                )
            )
        elif isinstance(content_block, RedactedThinkingBlock):
            content.append(
                ContentReasoning(reasoning=content_block.data, redacted=True)
            )
        elif isinstance(content_block, ThinkingBlock):
            reasoning_tokens += await count_tokens(
                client, model, content_block.thinking
            )
            content.append(
                ContentReasoning(
                    reasoning=content_block.thinking, signature=content_block.signature
                )
            )
    # resolve choice
    choice = ChatCompletionChoice(
        message=ChatMessageAssistant(
            content=content, tool_calls=tool_calls, source="generate"
        ),
        stop_reason=message_stop_reason(message),
    )
    # return ModelOutput
    usage = message.usage.model_dump()
    input_tokens_cache_write = usage.get("cache_creation_input_tokens", None)
    input_tokens_cache_read = usage.get("cache_read_input_tokens", None)
    total_tokens = (
        message.usage.input_tokens
        + (input_tokens_cache_write or 0)
        + (input_tokens_cache_read or 0)
        + message.usage.output_tokens
    )
    return ModelOutput(
        model=message.model,
        choices=[choice],
        usage=ModelUsage(
            input_tokens=message.usage.input_tokens,
            output_tokens=message.usage.output_tokens,
            total_tokens=total_tokens,
            input_tokens_cache_write=input_tokens_cache_write,
            input_tokens_cache_read=input_tokens_cache_read,
            reasoning_tokens=reasoning_tokens if reasoning_tokens > 0 else None,
        ),
    )
def message_stop_reason(message: Message) -> StopReason:
    match message.stop_reason:
        case "end_turn" | "stop_sequence":
            return "stop"
        case "tool_use":
            return "tool_calls"
        case "max_tokens":
            return message.stop_reason
        case _:
            return "unknown"
def split_system_messages(
    input: list[ChatMessage], config: GenerateConfig
) -> Tuple[list[ChatMessageSystem], list[ChatMessage]]:
    # split messages
    system_messages = [m for m in input if isinstance(m, ChatMessageSystem)]
    messages = [m for m in input if not isinstance(m, ChatMessageSystem)]
    # return
    return system_messages, cast(list[ChatMessage], messages)
async def message_param_content(
    content: Content,
) -> TextBlockParam | ImageBlockParam | ThinkingBlockParam | RedactedThinkingBlockParam:
    if isinstance(content, ContentText):
        return TextBlockParam(type="text", text=content.text or NO_CONTENT)
    elif isinstance(content, ContentImage):
        # resolve to url
        image = await file_as_data_uri(content.image)
        # resolve mime type and base64 content
        media_type = data_uri_mime_type(image) or "image/png"
        image = data_uri_to_base64(image)
        if media_type not in ["image/jpeg", "image/png", "image/gif", "image/webp"]:
            raise ValueError(f"Unable to read image of type {media_type}")
        return ImageBlockParam(
            type="image",
            source=dict(type="base64", media_type=cast(Any, media_type), data=image),
        )
    elif isinstance(content, ContentReasoning):
        if content.redacted:
            return RedactedThinkingBlockParam(
                type="redacted_thinking",
                data=content.reasoning,
            )
        else:
            if content.signature is None:
                raise ValueError("Thinking content without signature.")
            return ThinkingBlockParam(
                type="thinking", thinking=content.reasoning, signature=content.signature
            )
    else:
        raise RuntimeError(
            "Anthropic models do not currently support audio or video inputs."
        )
async def count_tokens(
    client: AsyncAnthropic | AsyncAnthropicBedrock | AsyncAnthropicVertex,
    model: str,
    text: str,
) -> int:
    try:
        response = await client.messages.count_tokens(
            model=model,
            messages=[{"role": "user", "content": text}],
        )
        return response.input_tokens
    except Exception as e:
        logger.warning(
            f"Error counting tokens (falling back to estimated tokens): {str(e)}"
        )
        words = text.split()
        estimated_tokens = int(len(words) * 1.3)
        return estimated_tokens
def model_call_filter(key: JsonValue | None, value: JsonValue) -> JsonValue:
    # remove base64 encoded images
    if (
        key == "source"
        and isinstance(value, dict)
        and value.get("type", None) == "base64"
    ):
        value = copy(value)
        value.update(data=BASE_64_DATA_REMOVED)
    return value

================
File: src/inspect_ai/model/_providers/azureai.py
================
import json
import os
from copy import copy
from typing import Any
from azure.ai.inference.aio import ChatCompletionsClient
from azure.ai.inference.models import (
    AssistantMessage,
    ChatChoice,
    ChatCompletions,
    ChatCompletionsNamedToolChoice,
    ChatCompletionsNamedToolChoiceFunction,
    ChatCompletionsToolCall,
    ChatCompletionsToolChoicePreset,
    ChatCompletionsToolDefinition,
    ChatRequestMessage,
    ChatResponseMessage,
    CompletionsFinishReason,
    ContentItem,
    FunctionCall,
    FunctionDefinition,
    ImageContentItem,
    ImageUrl,
    SystemMessage,
    TextContentItem,
    ToolMessage,
    UserMessage,
)
from azure.core.credentials import AzureKeyCredential
from azure.core.exceptions import (
    AzureError,
    HttpResponseError,
    ServiceResponseError,
)
from typing_extensions import override
from inspect_ai._util.constants import DEFAULT_MAX_TOKENS
from inspect_ai._util.content import Content, ContentImage, ContentText
from inspect_ai._util.http import is_retryable_http_status
from inspect_ai._util.images import file_as_data_uri
from inspect_ai.tool import ToolChoice, ToolInfo
from inspect_ai.tool._tool_call import ToolCall
from inspect_ai.tool._tool_choice import ToolFunction
from .._call_tools import parse_tool_call
from .._chat_message import (
    ChatMessage,
    ChatMessageAssistant,
    ChatMessageSystem,
    ChatMessageTool,
    ChatMessageUser,
)
from .._generate_config import GenerateConfig
from .._image import image_url_filter
from .._model import ModelAPI
from .._model_call import ModelCall
from .._model_output import (
    ChatCompletionChoice,
    ModelOutput,
    ModelUsage,
    StopReason,
)
from .util import (
    environment_prerequisite_error,
    model_base_url,
)
from .util.chatapi import ChatAPIHandler
from .util.llama31 import Llama31Handler
AZUREAI_API_KEY = "AZUREAI_API_KEY"
AZUREAI_ENDPOINT_KEY = "AZUREAI_ENDPOINT_KEY"
AZUREAI_BASE_URL = "AZUREAI_BASE_URL"
AZUREAI_ENDPOINT_URL = "AZUREAI_ENDPOINT_URL"
# legacy vars for migration
AZURE_API_KEY = "AZURE_API_KEY"
AZURE_ENDPOINT_URL = "AZURE_ENDPOINT_URL"
class AzureAIAPI(ModelAPI):
    def __init__(
        self,
        model_name: str,
        base_url: str | None = None,
        api_key: str | None = None,
        config: GenerateConfig = GenerateConfig(),
        **model_args: Any,
    ):
        super().__init__(
            model_name=model_name,
            base_url=base_url,
            api_key=api_key,
            api_key_vars=[AZURE_API_KEY, AZUREAI_ENDPOINT_KEY],
            config=config,
        )
        # collect known model_args (then delete them so we can pass the rest on)
        def collect_model_arg(name: str) -> Any | None:
            nonlocal model_args
            value = model_args.get(name, None)
            if value is not None:
                model_args.pop(name)
            return value
        emulate_tools = collect_model_arg("emulate_tools")
        self.emulate_tools = (
            not not emulate_tools if emulate_tools is not None else None
        )
        # resolve api_key
        if not self.api_key:
            self.api_key = os.environ.get(
                AZURE_API_KEY, os.environ.get(AZUREAI_API_KEY, "")
            )
            if not self.api_key:
                raise environment_prerequisite_error("AzureAI", AZURE_API_KEY)
        # resolve base url
        endpoint_url = model_base_url(
            base_url,
            [
                AZURE_ENDPOINT_URL,
                AZUREAI_ENDPOINT_URL,
                AZUREAI_BASE_URL,
            ],
        )
        if not endpoint_url:
            raise environment_prerequisite_error("AzureAI", AZUREAI_BASE_URL)
        self.endpoint_url = endpoint_url
        self.model_args = model_args
    @override
    async def close(self) -> None:
        # client is created/destroyed each time in generate()
        pass
    async def generate(
        self,
        input: list[ChatMessage],
        tools: list[ToolInfo],
        tool_choice: ToolChoice,
        config: GenerateConfig,
    ) -> ModelOutput | tuple[ModelOutput | Exception, ModelCall]:
        # emulate tools (auto for llama, opt-in for others)
        if self.emulate_tools is None and self.is_llama():
            handler: ChatAPIHandler | None = Llama31Handler()
        elif self.emulate_tools:
            handler = Llama31Handler()
        else:
            handler = None
        # resolve input
        if handler:
            input = handler.input_with_tools(input, tools)
        # prepare request
        request = dict(
            messages=await chat_request_messages(input, handler),
            tools=chat_tools(tools) if len(tools) > 0 else None,
            tool_choice=chat_tool_choice(tool_choice) if len(tools) > 0 else None,
            **self.completion_params(config),
        )
        # create client (note the client needs to be created and closed
        # with each call so it can be cleaned up and not end up on another
        # event loop in a subsequent pass of eval)
        assert self.api_key
        client = ChatCompletionsClient(
            endpoint=self.endpoint_url,
            credential=AzureKeyCredential(self.api_key),
            model=self.model_name,
            model_extras=self.model_args,
        )
        def model_call(response: ChatCompletions | None = None) -> ModelCall:
            return ModelCall.create(
                request=request
                | dict(
                    messages=[message.as_dict() for message in request["messages"]],
                    tools=[tool.as_dict() for tool in request["tools"]]
                    if request.get("tools", None) is not None
                    else None,
                ),
                response=response.as_dict() if response else {},
                filter=image_url_filter,
            )
        # make call
        try:
            response: ChatCompletions = await client.complete(**request)
            return ModelOutput(
                model=response.model,
                choices=chat_completion_choices(response.choices, tools, handler),
                usage=ModelUsage(
                    input_tokens=response.usage.prompt_tokens,
                    output_tokens=response.usage.completion_tokens,
                    total_tokens=response.usage.total_tokens,
                ),
            ), model_call(response)
        except AzureError as ex:
            return self.handle_azure_error(ex), model_call()
        finally:
            await client.close()
    def completion_params(self, config: GenerateConfig) -> dict[str, Any]:
        params: dict[str, str | int | float | list[str]] = {}
        if config.frequency_penalty is not None:
            params["frequency_penalty"] = config.frequency_penalty
        if config.presence_penalty is not None:
            params["presence_penalty"] = config.presence_penalty
        if config.temperature is not None:
            params["temperature"] = config.temperature
        if config.top_p is not None:
            params["top_p"] = config.top_p
        if config.max_tokens is not None:
            params["max_tokens"] = config.max_tokens
        if config.stop_seqs is not None:
            params["stop"] = config.stop_seqs
        if config.seed is not None:
            params["seed"] = config.seed
        return params
    @override
    def max_tokens(self) -> int | None:
        if self.is_llama():
            return 2048  # llama2 and llama3 on azureai have context windows of 4096
        # Mistral uses a default of 8192 which is fine, so we don't mess with it
        # see: https://learn.microsoft.com/en-us/azure/ai-studio/how-to/deploy-models-mistral#request-schema
        elif self.is_mistral():
            return None
        # Not sure what do to about other model types... (there aren't currently any others)
        else:
            return DEFAULT_MAX_TOKENS
    @override
    def should_retry(self, ex: Exception) -> bool:
        if isinstance(ex, HttpResponseError) and ex.status_code is not None:
            return is_retryable_http_status(ex.status_code)
        elif isinstance(ex, ServiceResponseError):
            return True
        else:
            return False
    @override
    def collapse_user_messages(self) -> bool:
        return True
    @override
    def connection_key(self) -> str:
        return f"{self.api_key}{self.model_name}"
    def is_llama(self) -> bool:
        return "llama" in self.model_name.lower()
    def is_llama3(self) -> bool:
        return "llama-3" in self.model_name.lower()
    def is_mistral(self) -> bool:
        return "mistral" in self.model_name.lower()
    def handle_azure_error(self, ex: AzureError) -> ModelOutput | Exception:
        if isinstance(ex, HttpResponseError):
            response = str(ex.message)
            if "maximum context length" in response.lower():
                return ModelOutput.from_content(
                    model=self.model_name,
                    content=response,
                    stop_reason="model_length",
                )
            elif ex.status_code == 400:
                return ex
        raise ex
async def chat_request_messages(
    messages: list[ChatMessage], handler: ChatAPIHandler | None
) -> list[ChatRequestMessage]:
    return [await chat_request_message(message, handler) for message in messages]
async def chat_request_message(
    message: ChatMessage, handler: ChatAPIHandler | None
) -> ChatRequestMessage:
    if isinstance(message, ChatMessageSystem):
        return SystemMessage(content=message.text)
    elif isinstance(message, ChatMessageUser):
        return UserMessage(
            content=message.content
            if isinstance(message.content, str)
            else [await chat_content_item(item) for item in message.content]
        )
    elif isinstance(message, ChatMessageTool):
        return ToolMessage(
            content=(
                f"Error: {message.error.message}" if message.error else message.text
            ),
            tool_call_id=str(message.tool_call_id),
        )
    else:
        if message.tool_calls:
            if handler:
                return AssistantMessage(
                    content=handler.assistant_message(message)["content"]
                )
            else:
                return AssistantMessage(
                    content=message.text or None,
                    tool_calls=[chat_tool_call(call) for call in message.tool_calls],
                )
        else:
            return AssistantMessage(content=message.text)
async def chat_content_item(content: Content) -> ContentItem:
    if isinstance(content, ContentText):
        return TextContentItem(text=content.text)
    elif isinstance(content, ContentImage):
        return ImageContentItem(
            image_url=ImageUrl(
                url=await file_as_data_uri(content.image), detail=content.detail
            )
        )
    else:
        raise RuntimeError("Azure AI models do not support audio or video inputs.")
def chat_tool_call(tool_call: ToolCall) -> ChatCompletionsToolCall:
    return ChatCompletionsToolCall(
        id=tool_call.id,
        function=FunctionCall(
            name=tool_call.function, arguments=json.dumps(tool_call.arguments)
        ),
    )
def chat_tools(tools: list[ToolInfo]) -> list[ChatCompletionsToolDefinition]:
    return [chat_tool_definition(tool) for tool in tools]
def chat_tool_definition(tool: ToolInfo) -> ChatCompletionsToolDefinition:
    return ChatCompletionsToolDefinition(
        function=FunctionDefinition(
            name=tool.name,
            description=tool.description,
            parameters=tool.parameters.model_dump(exclude_none=True),
        )
    )
def chat_tool_choice(
    tool_choice: ToolChoice,
) -> str | ChatCompletionsToolChoicePreset | ChatCompletionsNamedToolChoice:
    if isinstance(tool_choice, ToolFunction):
        return ChatCompletionsNamedToolChoice(
            function=ChatCompletionsNamedToolChoiceFunction(name=tool_choice.name)
        )
    elif tool_choice == "auto":
        return ChatCompletionsToolChoicePreset.AUTO
    elif tool_choice == "none":
        return ChatCompletionsToolChoicePreset.NONE
    elif tool_choice == "any":
        return ChatCompletionsToolChoicePreset.REQUIRED
def chat_completion_choices(
    choices: list[ChatChoice], tools: list[ToolInfo], handler: ChatAPIHandler | None
) -> list[ChatCompletionChoice]:
    choices = copy(choices)
    choices.sort(key=lambda c: c.index)
    return [chat_complection_choice(choice, tools, handler) for choice in choices]
def chat_complection_choice(
    choice: ChatChoice, tools: list[ToolInfo], handler: ChatAPIHandler | None
) -> ChatCompletionChoice:
    return ChatCompletionChoice(
        message=chat_completion_assistant_message(choice.message, tools, handler),
        stop_reason=chat_completion_stop_reason(choice.finish_reason),
    )
def chat_completion_assistant_message(
    response: ChatResponseMessage, tools: list[ToolInfo], handler: ChatAPIHandler | None
) -> ChatMessageAssistant:
    if handler:
        return handler.parse_assistant_response(response.content, tools)
    else:
        return ChatMessageAssistant(
            content=response.content or "",
            tool_calls=[
                chat_completion_tool_call(call, tools) for call in response.tool_calls
            ]
            if response.tool_calls is not None
            else None,
        )
def chat_completion_tool_call(
    tool_call: ChatCompletionsToolCall, tools: list[ToolInfo]
) -> ToolCall:
    return parse_tool_call(
        tool_call.id, tool_call.function.name, tool_call.function.arguments, tools
    )
def chat_completion_stop_reason(reason: str) -> StopReason:
    match reason:
        case CompletionsFinishReason.STOPPED:
            return "stop"
        case CompletionsFinishReason.TOKEN_LIMIT_REACHED:
            return "max_tokens"
        case CompletionsFinishReason.CONTENT_FILTERED:
            return "content_filter"
        case CompletionsFinishReason.TOOL_CALLS:
            return "tool_calls"
        case _:
            return "unknown"

================
File: src/inspect_ai/model/_providers/bedrock.py
================
import base64
from logging import getLogger
from typing import Any, Literal, Tuple, Union, cast
from pydantic import BaseModel, Field
from typing_extensions import override
from inspect_ai._util._async import current_async_backend
from inspect_ai._util.constants import DEFAULT_MAX_TOKENS
from inspect_ai._util.content import Content, ContentImage, ContentText
from inspect_ai._util.error import PrerequisiteError, pip_dependency_error
from inspect_ai._util.images import file_as_data
from inspect_ai._util.version import verify_required_version
from inspect_ai.tool import ToolChoice, ToolInfo
from inspect_ai.tool._tool_call import ToolCall
from inspect_ai.tool._tool_choice import ToolFunction
from .._chat_message import (
    ChatMessage,
    ChatMessageAssistant,
    ChatMessageSystem,
    ChatMessageTool,
    ChatMessageUser,
)
from .._generate_config import GenerateConfig
from .._model import ModelAPI
from .._model_call import ModelCall
from .._model_output import ChatCompletionChoice, ModelOutput, ModelUsage
from .util import (
    model_base_url,
)
from .util.hooks import ConverseHooks
logger = getLogger(__name__)
# Model for Bedrock Converse API (Response)
# generated from: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-runtime/client/converse.html#converse
ConverseRole = Literal["user", "assistant"]
ConverseImageFormat = Literal["png", "jpeg", "gif", "webp"]
ConverseDocumentFormat = Literal[
    "pdf", "csv", "doc", "docx", "xls", "xlsx", "html", "txt", "md"
]
ConverseToolResultStatus = Literal["success", "error"]
ConverseStopReason = Literal[
    "end_turn",
    "tool_use",
    "max_tokens",
    "stop_sequence",
    "guardrail_intervened",
    "content_filtered",
]
ConverseGuardContentQualifier = Literal["grounding_source", "query", "guard_content"]
ConverseFilterType = Literal[
    "INSULTS", "HATE", "SEXUAL", "VIOLENCE", "MISCONDUCT", "PROMPT_ATTACK"
]
ConverseFilterConfidence = Literal["NONE", "LOW", "MEDIUM", "HIGH"]
ConverseFilterStrength = Literal["NONE", "LOW", "MEDIUM", "HIGH"]
class ConverseImageSource(BaseModel):
    bytes: bytes
class ConverseDocumentSource(BaseModel):
    bytes: bytes
class ConverseImage(BaseModel):
    format: ConverseImageFormat
    source: ConverseImageSource
class ConverseDocument(BaseModel):
    format: ConverseDocumentFormat
    name: str
    source: ConverseDocumentSource
class ConverseToolUse(BaseModel):
    toolUseId: str
    name: str
    input: Union[dict[str, Any], list[Any], int, float, str, bool, None]
class ConverseToolResultContent(BaseModel):
    json_content: Union[dict[str, Any], list[Any], int, float, str, bool, None] = Field(
        default=None, alias="json"
    )
    text: str | None = None
    image: ConverseImage | None = None
    document: ConverseDocument | None = None
class ConverseToolResult(BaseModel):
    toolUseId: str
    content: list[ConverseToolResultContent]
    status: ConverseToolResultStatus
class ConverseGuardContentText(BaseModel):
    text: str
    qualifiers: list[ConverseGuardContentQualifier]
class ConverseGuardContent(BaseModel):
    text: ConverseGuardContentText
class ConverseMessageContent(BaseModel):
    text: str | None = None
    image: ConverseImage | None = None
    document: ConverseDocument | None = None
    toolUse: ConverseToolUse | None = None
    toolResult: ConverseToolResult | None = None
    guardContent: ConverseGuardContent | None = None
class ConverseMessage(BaseModel):
    role: ConverseRole
    content: list[ConverseMessageContent]
class ConverseOutput(BaseModel):
    message: ConverseMessage
class ConverseUsage(BaseModel):
    inputTokens: int
    outputTokens: int
    totalTokens: int
class ConverseMetrics(BaseModel):
    latencyMs: int
class ConverseTraceGuardrailFilter(BaseModel):
    type: ConverseFilterType
    confidence: ConverseFilterConfidence
    filterStrength: ConverseFilterStrength
    action: str
class ConverseAdditionalModelResponseFields(BaseModel):
    value: Union[dict[str, Any], list[Any], int, float, str, bool, None]
class ConverseResponse(BaseModel):
    output: ConverseOutput
    stopReason: ConverseStopReason
    usage: ConverseUsage
    metrics: ConverseMetrics
    additionalModelResponseFields: ConverseAdditionalModelResponseFields | None = None
    trace: dict[str, Any] | None = None
# Model for Bedrock Converse API (Request)
# generated from: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-runtime/client/converse.html#converse
class ConverseSource(BaseModel):
    bytes: bytes
class ConverseContent(BaseModel):
    text: str | None = None
    image: ConverseImage | None = None
    document: ConverseDocument | None = None
    toolUse: ConverseToolUse | None = None
    toolResult: ConverseToolResult | None = None
    guardContent: ConverseGuardContent | None = None
class ConverseSystemContent(BaseModel):
    text: str | None = None
    guardContent: ConverseGuardContent | None = None
class ConverseInferenceConfig(BaseModel):
    maxTokens: int | None = None
    temperature: float | None = None
    topP: float | None = None
    stopSequences: list[str] | None = None
class ConverseToolSpec(BaseModel):
    name: str
    description: str | None = None
    inputSchema: (
        Union[dict[str, Any], list[Any], int, float, str, bool, None] | None
    ) = None
class ConverseTool(BaseModel):
    toolSpec: ConverseToolSpec
class ConverseToolChoice(BaseModel):
    auto: dict[str, Any] | None = None
    any: dict[str, Any] | None = None
    tool: dict[str, str] | None = None
class ConverseToolConfig(BaseModel):
    tools: list[ConverseTool] | None = None
    toolChoice: ConverseToolChoice | None = None
class ConverseGuardrailConfig(BaseModel):
    guardrailIdentifier: str | None = None
    guardrailVersion: str | None = None
    trace: str | None = None
class ConversePromptVariable(BaseModel):
    text: str
class ConverseClientConverseRequest(BaseModel):
    modelId: str
    messages: list[ConverseMessage]
    system: list[ConverseSystemContent] | None = None
    inferenceConfig: ConverseInferenceConfig | None = None
    toolConfig: ConverseToolConfig | None = None
    guardrailConfig: ConverseGuardrailConfig | None = None
    additionalModelRequestFields: (
        Union[dict[str, Any], list[Any], int, float, str, bool, None] | None
    ) = None
    additionalModelResponseFieldPaths: list[str] = []
class BedrockAPI(ModelAPI):
    def __init__(
        self,
        model_name: str,
        base_url: str | None,
        api_key: str | None = None,
        config: GenerateConfig = GenerateConfig(),
        **model_args: Any,
    ):
        super().__init__(
            model_name=model_name,
            base_url=model_base_url(base_url, "BEDROCK_BASE_URL"),
            api_key=api_key,
            api_key_vars=[],
            config=config,
        )
        # raise if we are using trio
        if current_async_backend() == "trio":
            raise PrerequisiteError(
                "ERROR: The bedrock provider does not work with the trio async backend."
            )
        # save model_args
        self.model_args = model_args
        # import aioboto3 on demand
        try:
            import aioboto3
            verify_required_version("Bedrock API", "aioboto3", "13.0.0")
            # Create a shared session to be used when generating
            self.session = aioboto3.Session()
            # create time tracker
            self._http_hooks = ConverseHooks(self.session)
        except ImportError:
            raise pip_dependency_error("Bedrock API", ["aioboto3"])
    @override
    async def close(self) -> None:
        # client is created/destroyed each time in generate()
        pass
    @override
    def connection_key(self) -> str:
        return self.model_name
    @override
    def max_tokens(self) -> int | None:
        if "llama3-70" in self.model_name or "llama3-8" in self.model_name:
            return 2048
        if "llama3" in self.model_name or "claude3" in self.model_name:
            return 4096
        elif "mistral-large" in self.model_name:
            return 8192
        # Other models will just the default
        else:
            return DEFAULT_MAX_TOKENS
    @override
    def should_retry(self, ex: Exception) -> bool:
        from botocore.exceptions import ClientError
        # Look for an explicit throttle exception
        if isinstance(ex, ClientError):
            error_code = ex.response.get("Error", {}).get("Code", "")
            return error_code in [
                "ThrottlingException",
                "RequestLimitExceeded",
                "Throttling",
                "RequestThrottled",
                "TooManyRequestsException",
                "ProvisionedThroughputExceededException",
                "TransactionInProgressException",
                "RequestTimeout",
                "ServiceUnavailable",
            ]
        else:
            return False
    @override
    def collapse_user_messages(self) -> bool:
        return True
    @override
    def collapse_assistant_messages(self) -> bool:
        return True
    async def generate(
        self,
        input: list[ChatMessage],
        tools: list[ToolInfo],
        tool_choice: ToolChoice,
        config: GenerateConfig,
    ) -> ModelOutput | tuple[ModelOutput | Exception, ModelCall]:
        from botocore.config import Config
        from botocore.exceptions import ClientError
        # The bedrock client
        request_id = self._http_hooks.start_request()
        async with self.session.client(  # type: ignore[call-overload]
            service_name="bedrock-runtime",
            endpoint_url=self.base_url,
            config=Config(
                retries=dict(mode="adaptive"),
                user_agent_extra=self._http_hooks.user_agent_extra(request_id),
            ),
            **self.model_args,
        ) as client:
            # Process the tools
            resolved_tools = converse_tools(tools)
            tool_config = None
            if resolved_tools is not None:
                choice = converse_tool_choice(tool_choice)
                tool_config = ConverseToolConfig(
                    tools=resolved_tools, toolChoice=choice
                )
            # Resolve the input messages into converse messages
            system, messages = await converse_messages(input)
            # Make the request
            request = ConverseClientConverseRequest(
                modelId=self.model_name,
                messages=messages,
                system=system,
                inferenceConfig=ConverseInferenceConfig(
                    maxTokens=config.max_tokens,
                    temperature=config.temperature,
                    topP=config.top_p,
                    stopSequences=config.stop_seqs,
                ),
                additionalModelRequestFields={
                    "top_k": config.top_k,
                    **config.model_config,
                },
                toolConfig=tool_config,
            )
            def model_call(response: dict[str, Any] | None = None) -> ModelCall:
                return ModelCall.create(
                    request=replace_bytes_with_placeholder(
                        request.model_dump(exclude_none=True)
                    ),
                    response=response,
                    time=self._http_hooks.end_request(request_id),
                )
            try:
                # Process the reponse
                response = await client.converse(
                    **request.model_dump(exclude_none=True)
                )
                converse_response = ConverseResponse(**response)
            except ClientError as ex:
                # Look for an explicit validation exception
                if ex.response["Error"]["Code"] == "ValidationException":
                    response = ex.response["Error"]["Message"]
                    if "Too many input tokens" in response:
                        return ModelOutput.from_content(
                            model=self.model_name,
                            content=response,
                            stop_reason="model_length",
                        )
                    else:
                        return ex, model_call(None)
                else:
                    raise ex
        # create a model output from the response
        output = model_output_from_response(self.model_name, converse_response, tools)
        # return
        return output, model_call(response)
async def converse_messages(
    messages: list[ChatMessage],
) -> Tuple[list[ConverseSystemContent] | None, list[ConverseMessage]]:
    # Split up system messages and input messages
    system_messages: list[ChatMessage] = []
    non_system_messages: list[ChatMessage] = []
    for message in messages:
        if message.role == "system":
            system_messages.append(message)
        else:
            non_system_messages.append(message)
    # input messages
    non_system: list[ConverseMessage] = await as_converse_chat_messages(
        non_system_messages
    )
    # system messages
    system: list[ConverseSystemContent] = as_converse_system_messages(system_messages)
    return system if len(system) > 0 else None, non_system
def model_output_from_response(
    model: str, response: ConverseResponse, tools: list[ToolInfo]
) -> ModelOutput:
    # extract content and tool calls
    content: list[Content] = []
    tool_calls: list[ToolCall] = []
    # process the content in the response message
    for c in response.output.message.content:
        if c.text is not None:
            content.append(ContentText(type="text", text=c.text))
        elif c.image is not None:
            base64_image = base64.b64encode(c.image.source.bytes).decode("utf-8")
            content.append(
                ContentImage(image=f"data:image/{c.image.format};base64,{base64_image}")
            )
        elif c.toolUse is not None:
            tool_calls.append(
                ToolCall(
                    id=c.toolUse.toolUseId,
                    type="function",
                    function=c.toolUse.name,
                    arguments=cast(dict[str, Any], c.toolUse.input or {}),
                )
            )
        else:
            raise ValueError("Unexpected message response in Bedrock provider")
    # resolve choice
    choice = ChatCompletionChoice(
        message=ChatMessageAssistant(
            content=content, tool_calls=tool_calls, source="generate"
        ),
        stop_reason=message_stop_reason(response.stopReason),
    )
    # Compute usage
    input_tokens = response.usage.inputTokens
    output_tokens = response.usage.outputTokens
    total_tokens = input_tokens + output_tokens
    # return ModelOutput
    return ModelOutput(
        model=model,
        choices=[choice],
        usage=ModelUsage(
            input_tokens=input_tokens,
            output_tokens=output_tokens,
            total_tokens=total_tokens,
        ),
    )
def message_stop_reason(
    reason: ConverseStopReason,
) -> Literal[
    "stop", "max_tokens", "model_length", "tool_calls", "content_filter", "unknown"
]:
    match reason:
        case "end_turn" | "stop_sequence":
            return "stop"
        case "tool_use":
            return "tool_calls"
        case "max_tokens":
            return reason
        case "content_filtered":
            return "content_filter"
        case "guardrail_intervened":
            return "content_filter"
        case _:
            return "unknown"
def as_converse_system_messages(
    messages: list[ChatMessage],
) -> list[ConverseSystemContent]:
    return [
        ConverseSystemContent(text=message.text) for message in messages if message.text
    ]
async def as_converse_chat_messages(
    messages: list[ChatMessage],
) -> list[ConverseMessage]:
    result: list[ConverseMessage] = []
    for message in messages:
        converse_message = await converse_chat_message(message)
        if converse_message is not None:
            result.extend(converse_message)
    return collapse_consecutive_messages(result)
async def converse_chat_message(
    message: ChatMessage,
) -> list[ConverseMessage] | None:
    if isinstance(message, ChatMessageSystem):
        raise ValueError("System messages should be processed separately for Converse")
    if isinstance(message, ChatMessageUser):
        # Simple user message
        return [
            ConverseMessage(
                role="user", content=await converse_contents(message.content)
            )
        ]
    elif isinstance(message, ChatMessageAssistant):
        if message.tool_calls:
            # The assistant is calling tools, process those
            results: list[ConverseMessage] = []
            for tool_call in message.tool_calls:
                tool_use = ConverseToolUse(
                    toolUseId=tool_call.id,
                    name=tool_call.function,
                    input=tool_call.arguments,
                )
                m = ConverseMessage(
                    role="assistant", content=[ConverseMessageContent(toolUse=tool_use)]
                )
                results.append(m)
            return results
        else:
            # Simple assistant message
            return [
                ConverseMessage(
                    role="assistant", content=await converse_contents(message.content)
                )
            ]
    elif isinstance(message, ChatMessageTool):
        # Process tool message
        if message.tool_call_id is None:
            raise ValueError(
                "Tool call is missing a tool call id, which is required for Converse API"
            )
        if message.function is None:
            print(message)
            raise ValueError(
                "Tool call is missing a function, which is required for Converse API"
            )
        status: Literal["success", "error"] = (
            "success" if message.error is None else "error"
        )
        # process the tool response content
        tool_result_content: list[ConverseToolResultContent] = []
        if isinstance(message.content, str):
            tool_result_content.append(ConverseToolResultContent(text=message.content))
        else:
            for c in message.content:
                if c.type == "text":
                    tool_result_content.append(ConverseToolResultContent(text=c.text))
                elif c.type == "image":
                    image_data, image_type = await file_as_data(c.image)
                    tool_result_content.append(
                        ConverseToolResultContent(
                            image=ConverseImage(
                                format=converse_image_type(image_type),
                                source=ConverseImageSource(bytes=image_data),
                            )
                        )
                    )
                else:
                    raise ValueError(
                        "Unsupported tool content type in Bedrock provider."
                    )
        # return the tool result
        tool_result = ConverseToolResult(
            toolUseId=message.tool_call_id,
            status=status,
            content=tool_result_content,
        )
        return [
            ConverseMessage(
                role="user",
                content=[ConverseMessageContent(toolResult=tool_result)],
            )
        ]
    else:
        raise ValueError(f"Unexpected message role {message.role}")
async def converse_contents(
    content: list[Content] | str,
) -> list[ConverseMessageContent]:
    if isinstance(content, str):
        return [ConverseMessageContent(text=content)]
    else:
        result: list[ConverseMessageContent] = []
        for c in content:
            if c.type == "image":
                image_data, image_type = await file_as_data(c.image)
                result.append(
                    ConverseMessageContent(
                        image=ConverseImage(
                            format=converse_image_type(image_type),
                            source=ConverseImageSource(bytes=image_data),
                        )
                    )
                )
            elif c.type == "text":
                result.append(ConverseMessageContent(text=c.text))
            else:
                raise RuntimeError(f"Unsupported content type {c.type}")
        return result
def collapse_consecutive_messages(
    messages: list[ConverseMessage],
) -> list[ConverseMessage]:
    if not messages:
        return []
    collapsed_messages = [messages[0]]
    for message in messages[1:]:
        last_message = collapsed_messages[-1]
        if message.role == last_message.role:
            last_content = last_message.content[-1]
            if last_content.toolResult is not None:
                # Special case tool results since conversation blocks and tool result
                # blocks cannot be provided in the same turn. If the last block was a
                # tool result, we'll need to merge the subsequent blocks into the content
                # itself
                for c in message.content:
                    if (
                        c.text is not None
                        or c.image is not None
                        or c.document is not None
                    ):
                        last_content.toolResult.content.append(
                            ConverseToolResultContent(
                                text=c.text, image=c.image, document=c.document
                            )
                        )
                    else:
                        last_message.content.extend(message.content)
            else:
                last_message.content.extend(message.content)
        else:
            collapsed_messages.append(message)
    return collapsed_messages
def converse_image_type(type: str) -> ConverseImageFormat:
    match type:
        case "image/png":
            return "png"
        case "image/gif":
            return "gif"
        case "image/png":
            return "png"
        case "image/webp":
            return "webp"
        case "image/jpeg":
            return "jpeg"
        case _:
            raise ValueError(
                f"Image mime type {type} is not supported for Bedrock Converse models."
            )
def converse_tools(tools: list[ToolInfo]) -> list[ConverseTool] | None:
    if len(tools) == 0:
        return None
    result = []
    for tool in tools:
        tool_spec = ConverseToolSpec(
            name=tool.name,
            description=tool.description,
            inputSchema={
                "json": tool.parameters.model_dump(
                    exclude_none=True, exclude={"additionalProperties"}
                )
            },
        )
        result.append(ConverseTool(toolSpec=tool_spec))
    return result
def converse_tool_choice(
    tool_choice: ToolChoice,
) -> ConverseToolChoice | None:
    match tool_choice:
        case "auto":
            return ConverseToolChoice(auto={})
        case "any":
            return ConverseToolChoice(any={})
        case "none":
            return ConverseToolChoice(auto={})
        case ToolFunction(name=name):
            return ConverseToolChoice(tool={"name": name})
        case _:
            raise ValueError(
                f"Tool choice {tool_choice} is not supported for Bedrock Converse models."
            )
def replace_bytes_with_placeholder(data: Any, placeholder: Any = "<bytes>") -> Any:
    if isinstance(data, bytes):
        return placeholder
    elif isinstance(data, dict):
        return {
            k: replace_bytes_with_placeholder(v, placeholder) for k, v in data.items()
        }
    elif isinstance(data, list):
        return [replace_bytes_with_placeholder(item, placeholder) for item in data]
    elif isinstance(data, tuple):
        return tuple(replace_bytes_with_placeholder(item, placeholder) for item in data)
    return data

================
File: src/inspect_ai/model/_providers/cloudflare.py
================
import os
from typing import Any
import httpx
from typing_extensions import override
from inspect_ai._util.constants import DEFAULT_MAX_TOKENS
from inspect_ai.tool import ToolChoice, ToolInfo
from ...model import ChatMessage, GenerateConfig, ModelAPI, ModelOutput
from .._model_call import ModelCall
from .._model_output import ChatCompletionChoice
from .util import (
    ChatAPIHandler,
    Llama31Handler,
    chat_api_input,
    chat_api_request,
    environment_prerequisite_error,
    model_base_url,
    should_retry_chat_api_error,
)
from .util.hooks import HttpxHooks
# https://developers.cloudflare.com/workers-ai/models/#text-generation
CLOUDFLARE_API_TOKEN = "CLOUDFLARE_API_TOKEN"
class CloudFlareAPI(ModelAPI):
    def __init__(
        self,
        model_name: str,
        base_url: str | None = None,
        api_key: str | None = None,
        config: GenerateConfig = GenerateConfig(),
        **model_args: Any,
    ):
        super().__init__(
            model_name=model_name,
            base_url=base_url,
            api_key=api_key,
            api_key_vars=[CLOUDFLARE_API_TOKEN],
            config=config,
        )
        self.account_id = os.getenv("CLOUDFLARE_ACCOUNT_ID")
        if not self.account_id:
            raise environment_prerequisite_error("CloudFlare", "CLOUDFLARE_ACCOUNT_ID")
        if not self.api_key:
            self.api_key = os.getenv(CLOUDFLARE_API_TOKEN)
            if not self.api_key:
                raise environment_prerequisite_error("CloudFlare", CLOUDFLARE_API_TOKEN)
        self.client = httpx.AsyncClient()
        self._http_hooks = HttpxHooks(self.client)
        base_url = model_base_url(base_url, "CLOUDFLARE_BASE_URL")
        self.base_url = (
            base_url if base_url else "https://api.cloudflare.com/client/v4/accounts"
        )
        self.model_args = model_args
    @override
    async def close(self) -> None:
        await self.client.aclose()
    async def generate(
        self,
        input: list[ChatMessage],
        tools: list[ToolInfo],
        tool_choice: ToolChoice,
        config: GenerateConfig,
    ) -> tuple[ModelOutput, ModelCall]:
        # chat url
        chat_url = f"{self.base_url}/{self.account_id}/ai/run/@cf"
        # chat api input
        json: dict[str, Any] = dict(**self.model_args)
        if config.max_tokens is not None:
            json["max_tokens"] = config.max_tokens
        json["messages"] = chat_api_input(input, tools, self.chat_api_handler())
        # request_id
        request_id = self._http_hooks.start_request()
        # setup response
        response: dict[str, Any] = {}
        def model_call() -> ModelCall:
            return ModelCall.create(
                request=json,
                response=response,
                time=self._http_hooks.end_request(request_id),
            )
        # make the call
        response = await chat_api_request(
            self.client,
            model_name=self.model_name,
            url=f"{chat_url}/{self.model_name}",
            headers={
                "Authorization": f"Bearer {self.api_key}",
                HttpxHooks.REQUEST_ID_HEADER: request_id,
            },
            json=json,
        )
        # handle response
        if response["success"]:
            # extract output
            content = response["result"]["response"]
            output = ModelOutput(
                model=self.model_name,
                choices=[
                    ChatCompletionChoice(
                        message=self.chat_api_handler().parse_assistant_response(
                            content, tools
                        ),
                        stop_reason="stop",
                    )
                ],
            )
            # return
            return output, model_call()
        else:
            error = str(response.get("errors", "Unknown"))
            raise RuntimeError(f"Error calling {self.model_name}: {error}")
    @override
    def should_retry(self, ex: Exception) -> bool:
        return should_retry_chat_api_error(ex)
    # cloudflare enforces rate limits by model for each account
    @override
    def connection_key(self) -> str:
        return f"{self.account_id}{self.model_name}"
    # cloudflare defaults to 256 max tokens, not enough for evals
    @override
    def max_tokens(self) -> int:
        return DEFAULT_MAX_TOKENS
    def chat_api_handler(self) -> ChatAPIHandler:
        if "llama" in self.model_name.lower():
            return Llama31Handler()
        else:
            return ChatAPIHandler()

================
File: src/inspect_ai/model/_providers/goodfire.py
================
import os
from typing import Any, List, Literal, get_args
from goodfire import AsyncClient
from goodfire.api.chat.interfaces import ChatMessage as GoodfireChatMessage
from goodfire.api.exceptions import (
    InvalidRequestException,
    RateLimitException,
    ServerErrorException,
)
from goodfire.variants.variants import SUPPORTED_MODELS, Variant
from typing_extensions import override
from inspect_ai.tool._tool_choice import ToolChoice
from inspect_ai.tool._tool_info import ToolInfo
from .._chat_message import (
    ChatMessage,
    ChatMessageAssistant,
    ChatMessageSystem,
    ChatMessageTool,
    ChatMessageUser,
)
from .._generate_config import GenerateConfig
from .._model import ModelAPI
from .._model_call import ModelCall
from .._model_output import (
    ChatCompletionChoice,
    ModelOutput,
    ModelUsage,
)
from .util import environment_prerequisite_error, model_base_url
# Constants
GOODFIRE_API_KEY = "GOODFIRE_API_KEY"
DEFAULT_BASE_URL = "https://api.goodfire.ai"
DEFAULT_MAX_TOKENS = 4096
DEFAULT_TEMPERATURE = 1.0  # Standard sampling temperature (baseline)
DEFAULT_TOP_P = 1.0  # No nucleus sampling truncation (baseline)
class GoodfireAPI(ModelAPI):
    """Goodfire API provider.
    This provider implements the Goodfire API for LLM inference. It supports:
    - Chat completions with standard message formats
    - Basic parameter controls (temperature, top_p, etc.)
    - Usage statistics tracking
    - Stop reason handling
    Does not currently support:
    - Tool calls
    - Feature analysis
    - Streaming responses
    Known limitations:
    - Limited role support (system/user/assistant only)
    - Tool messages converted to user messages
    """
    client: AsyncClient
    variant: Variant
    model_args: dict[str, Any]
    def __init__(
        self,
        model_name: str,
        base_url: str | None = None,
        api_key: str | None = None,
        config: GenerateConfig = GenerateConfig(),
        **model_args: Any,
    ) -> None:
        """Initialize the Goodfire API provider.
        Args:
            model_name: Name of the model to use
            base_url: Optional custom API base URL
            api_key: Optional API key (will check env vars if not provided)
            config: Generation config options
            **model_args: Additional arguments passed to the API
        """
        super().__init__(
            model_name=model_name,
            base_url=base_url,
            api_key=api_key,
            api_key_vars=[GOODFIRE_API_KEY],
            config=config,
        )
        # resolve api_key
        if not self.api_key:
            self.api_key = os.environ.get(GOODFIRE_API_KEY)
            if not self.api_key:
                raise environment_prerequisite_error("Goodfire", GOODFIRE_API_KEY)
        # Validate model name against supported models
        supported_models = list(get_args(SUPPORTED_MODELS))
        if self.model_name not in supported_models:
            raise ValueError(
                f"Model {self.model_name} not supported. Supported models: {supported_models}"
            )
        # Initialize client with minimal configuration
        base_url_val = model_base_url(base_url, "GOODFIRE_BASE_URL")
        assert isinstance(base_url_val, str) or base_url_val is None
        # Store model args for use in generate
        self.model_args = model_args
        self.client = AsyncClient(
            api_key=self.api_key,
            base_url=base_url_val or DEFAULT_BASE_URL,
        )
        # Initialize variant directly with model name
        self.variant = Variant(self.model_name)  # type: ignore
    @override
    async def close(self) -> None:
        # httpx.AsyncClient is created on each generate()
        pass
    def _to_goodfire_message(self, message: ChatMessage) -> GoodfireChatMessage:
        """Convert an Inspect message to a Goodfire message format.
        Args:
            message: The message to convert
        Returns:
            The converted message in Goodfire format
        Raises:
            ValueError: If the message type is unknown
        """
        role: Literal["system", "user", "assistant"] = "user"
        if isinstance(message, ChatMessageSystem):
            role = "system"
        elif isinstance(message, ChatMessageUser):
            role = "user"
        elif isinstance(message, ChatMessageAssistant):
            role = "assistant"
        elif isinstance(message, ChatMessageTool):
            role = "user"  # Convert tool messages to user messages
        else:
            raise ValueError(f"Unknown message type: {type(message)}")
        content = str(message.content)
        if isinstance(message, ChatMessageTool):
            content = f"Tool {message.function}: {content}"
        return GoodfireChatMessage(role=role, content=content)
    def handle_error(self, ex: Exception) -> ModelOutput | Exception:
        """Handle only errors that need special treatment for retry logic or model limits."""
        # Handle token/context length errors
        if isinstance(ex, InvalidRequestException):
            error_msg = str(ex).lower()
            if "context length" in error_msg or "max tokens" in error_msg:
                return ModelOutput.from_content(
                    model=self.model_name,
                    content=str(ex),
                    stop_reason="model_length",
                    error=error_msg,
                )
        # Let all other errors propagate
        return ex
    @override
    def should_retry(self, ex: Exception) -> bool:
        """Check if exception is due to rate limiting."""
        return isinstance(ex, RateLimitException | ServerErrorException)
    @override
    def connection_key(self) -> str:
        """Return key for connection pooling."""
        return f"goodfire:{self.api_key}"
    @override
    def max_tokens(self) -> int | None:
        """Return maximum tokens supported by model."""
        return DEFAULT_MAX_TOKENS  # Let Goodfire's Variant handle model-specific limits
    async def generate(
        self,
        input: List[ChatMessage],
        tools: List[ToolInfo],
        tool_choice: ToolChoice,
        config: GenerateConfig,
        *,
        cache: bool = True,
    ) -> tuple[ModelOutput | Exception, ModelCall]:
        """Generate output from the model."""
        # Convert messages and prepare request params
        messages = [self._to_goodfire_message(msg) for msg in input]
        # Build request parameters with type hints
        params: dict[str, Any] = {
            "model": self.variant.base_model,  # Use base_model instead of stringifying the Variant
            "messages": messages,
            "max_completion_tokens": int(config.max_tokens)
            if config.max_tokens
            else DEFAULT_MAX_TOKENS,
            "stream": False,
        }
        # Add generation parameters from config if not in model_args
        if "temperature" not in self.model_args and config.temperature is not None:
            params["temperature"] = float(config.temperature)
        elif "temperature" not in self.model_args:
            params["temperature"] = DEFAULT_TEMPERATURE
        if "top_p" not in self.model_args and config.top_p is not None:
            params["top_p"] = float(config.top_p)
        elif "top_p" not in self.model_args:
            params["top_p"] = DEFAULT_TOP_P
        # Add any additional model args (highest priority)
        api_params = {
            k: v
            for k, v in self.model_args.items()
            if k not in ["api_key", "base_url", "model_args"]
        }
        params.update(api_params)
        try:
            # Use native async client
            response = await self.client.chat.completions.create(**params)
            response_dict = response.model_dump()
            output = ModelOutput(
                model=self.model_name,
                choices=[
                    ChatCompletionChoice(
                        message=ChatMessageAssistant(
                            content=response_dict["choices"][0]["message"]["content"]
                        ),
                        stop_reason="stop",
                    )
                ],
                usage=ModelUsage(**response_dict["usage"])
                if "usage" in response_dict
                else None,
            )
            model_call = ModelCall.create(request=params, response=response_dict)
            return (output, model_call)
        except Exception as ex:
            result = self.handle_error(ex)
            model_call = ModelCall.create(
                request=params,
                response={},  # Empty response for error case
            )
            return (result, model_call)
    @property
    def name(self) -> str:
        """Get provider name."""
        return "goodfire"

================
File: src/inspect_ai/model/_providers/google.py
================
import functools
import hashlib
import json
import os
from copy import copy
from io import BytesIO
from logging import getLogger
from typing import Any
# SDK Docs: https://googleapis.github.io/python-genai/
import anyio
from google.genai import Client  # type: ignore
from google.genai.errors import APIError, ClientError  # type: ignore
from google.genai.types import (  # type: ignore
    Candidate,
    Content,
    File,
    FinishReason,
    FunctionCallingConfig,
    FunctionDeclaration,
    FunctionResponse,
    GenerateContentConfig,
    GenerateContentResponse,
    GenerateContentResponsePromptFeedback,
    GenerateContentResponseUsageMetadata,
    GenerationConfig,
    HarmBlockThreshold,
    HarmCategory,
    HttpOptions,
    Part,
    SafetySetting,
    SafetySettingDict,
    Schema,
    Tool,
    ToolConfig,
    Type,
)
from pydantic import JsonValue
from typing_extensions import override
from inspect_ai._util.constants import BASE_64_DATA_REMOVED, NO_CONTENT
from inspect_ai._util.content import (
    Content as InspectContent,
)
from inspect_ai._util.content import (
    ContentAudio,
    ContentImage,
    ContentReasoning,
    ContentText,
    ContentVideo,
)
from inspect_ai._util.error import PrerequisiteError
from inspect_ai._util.http import is_retryable_http_status
from inspect_ai._util.images import file_as_data
from inspect_ai._util.kvstore import inspect_kvstore
from inspect_ai._util.trace import trace_message
from inspect_ai.model import (
    ChatCompletionChoice,
    ChatMessage,
    ChatMessageAssistant,
    ChatMessageTool,
    ChatMessageUser,
    GenerateConfig,
    Logprob,
    Logprobs,
    ModelAPI,
    ModelOutput,
    ModelUsage,
    StopReason,
    TopLogprob,
)
from inspect_ai.model._model_call import ModelCall
from inspect_ai.model._providers.util import model_base_url
from inspect_ai.model._providers.util.hooks import HttpHooks, urllib3_hooks
from inspect_ai.tool import (
    ToolCall,
    ToolChoice,
    ToolFunction,
    ToolInfo,
    ToolParam,
    ToolParams,
)
logger = getLogger(__name__)
GOOGLE_API_KEY = "GOOGLE_API_KEY"
VERTEX_API_KEY = "VERTEX_API_KEY"
SAFETY_SETTINGS = "safety_settings"
DEFAULT_SAFETY_SETTINGS = {
    HarmCategory.HARM_CATEGORY_CIVIC_INTEGRITY: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
}
class GoogleGenAIAPI(ModelAPI):
    def __init__(
        self,
        model_name: str,
        base_url: str | None,
        api_key: str | None,
        config: GenerateConfig = GenerateConfig(),
        **model_args: Any,
    ) -> None:
        super().__init__(
            model_name=model_name,
            base_url=base_url,
            api_key=api_key,
            api_key_vars=[GOOGLE_API_KEY, VERTEX_API_KEY],
            config=config,
        )
        # pick out user-provided safety settings and merge against default
        self.safety_settings = DEFAULT_SAFETY_SETTINGS.copy()
        if SAFETY_SETTINGS in model_args:
            self.safety_settings.update(
                parse_safety_settings(model_args.get(SAFETY_SETTINGS))
            )
            del model_args[SAFETY_SETTINGS]
        # extract any service prefix from model name
        parts = model_name.split("/")
        if len(parts) > 1:
            self.service: str | None = parts[0]
            model_name = "/".join(parts[1:])
        else:
            self.service = None
        # vertex can also be forced by the GOOGLE_GENAI_USE_VERTEX_AI flag
        if self.service is None:
            if os.environ.get("GOOGLE_GENAI_USE_VERTEXAI", "").lower() == "true":
                self.service = "vertex"
        # ensure we haven't specified an invalid service
        if self.service is not None and self.service != "vertex":
            raise RuntimeError(
                f"Invalid service name for google: {self.service}. "
                + "Currently 'vertex' is the only supported service."
            )
        # handle auth (vertex or standard google api key)
        if self.is_vertex():
            # see if we are running in express mode (propagate api key if we are)
            # https://cloud.google.com/vertex-ai/generative-ai/docs/start/express-mode/overview
            vertex_api_key = os.environ.get(VERTEX_API_KEY, None)
            if vertex_api_key and not self.api_key:
                self.api_key = vertex_api_key
            # When not using express mode the GOOGLE_CLOUD_PROJECT and GOOGLE_CLOUD_LOCATION
            # environment variables should be set, OR the 'project' and 'location' should be
            # passed within the model_args.
            # https://cloud.google.com/vertex-ai/generative-ai/docs/gemini-v2
            if not vertex_api_key:
                if not os.environ.get(
                    "GOOGLE_CLOUD_PROJECT", None
                ) and not model_args.get("project", None):
                    raise PrerequisiteError(
                        "Google provider requires either the GOOGLE_CLOUD_PROJECT environment variable "
                        + "or the 'project' custom model arg (-M) when running against vertex."
                    )
                if not os.environ.get(
                    "GOOGLE_CLOUD_LOCATION", None
                ) and not model_args.get("location", None):
                    raise PrerequisiteError(
                        "Google provider requires either the GOOGLE_CLOUD_LOCATION environment variable "
                        + "or the 'location' custom model arg (-M) when running against vertex."
                    )
        # normal google endpoint
        else:
            # read api key from env
            if not self.api_key:
                self.api_key = os.environ.get(GOOGLE_API_KEY, None)
            # custom base_url
            base_url = model_base_url(base_url, "GOOGLE_BASE_URL")
        # create client
        self.client = Client(
            vertexai=self.is_vertex(),
            api_key=self.api_key,
            http_options={"base_url": base_url},
            **model_args,
        )
    @override
    async def close(self) -> None:
        # GenerativeModel uses a cached/shared client so there is no 'close'
        pass
    def is_vertex(self) -> bool:
        return self.service == "vertex"
    async def generate(
        self,
        input: list[ChatMessage],
        tools: list[ToolInfo],
        tool_choice: ToolChoice,
        config: GenerateConfig,
    ) -> ModelOutput | tuple[ModelOutput | Exception, ModelCall]:
        # generate request_id
        request_id = urllib3_hooks().start_request()
        # Create google-genai types.
        gemini_contents = await as_chat_messages(self.client, input)
        gemini_tools = chat_tools(tools) if len(tools) > 0 else None
        gemini_tool_config = chat_tool_config(tool_choice) if len(tools) > 0 else None
        parameters = GenerateContentConfig(
            http_options=HttpOptions(headers={HttpHooks.REQUEST_ID_HEADER: request_id}),
            temperature=config.temperature,
            top_p=config.top_p,
            top_k=config.top_k,
            max_output_tokens=config.max_tokens,
            stop_sequences=config.stop_seqs,
            candidate_count=config.num_choices,
            presence_penalty=config.presence_penalty,
            frequency_penalty=config.frequency_penalty,
            safety_settings=safety_settings_to_list(self.safety_settings),
            tools=gemini_tools,
            tool_config=gemini_tool_config,
            system_instruction=await extract_system_message_as_parts(
                self.client, input
            ),
        )
        if config.response_schema is not None:
            parameters.response_mime_type = "application/json"
            parameters.response_schema = schema_from_param(
                config.response_schema.json_schema, nullable=None
            )
        response: GenerateContentResponse | None = None
        def model_call() -> ModelCall:
            return build_model_call(
                contents=gemini_contents,
                safety_settings=self.safety_settings,
                generation_config=parameters,
                tools=gemini_tools,
                tool_config=gemini_tool_config,
                response=response,
                time=urllib3_hooks().end_request(request_id),
            )
        try:
            response = await self.client.aio.models.generate_content(
                model=self.model_name,
                contents=gemini_contents,
                config=parameters,
            )
        except ClientError as ex:
            return self.handle_client_error(ex), model_call()
        output = ModelOutput(
            model=self.model_name,
            choices=completion_choices_from_candidates(response),
            usage=usage_metadata_to_model_usage(response.usage_metadata),
        )
        return output, model_call()
    @override
    def should_retry(self, ex: Exception) -> bool:
        import requests  # type: ignore
        # standard http errors
        if (
            isinstance(ex, APIError)
            and isinstance(ex.status, str)
            and ex.status.isdigit()
        ):
            return is_retryable_http_status(int(ex.status))
        # low-level requests exceptions
        elif isinstance(ex, requests.exceptions.RequestException):
            return isinstance(
                ex,
                (
                    requests.exceptions.ConnectionError
                    | requests.exceptions.ConnectTimeout
                    | requests.exceptions.ChunkedEncodingError
                ),
            )
        else:
            return False
    @override
    def connection_key(self) -> str:
        """Scope for enforcing max_connections (could also use endpoint)."""
        return self.model_name
    def handle_client_error(self, ex: ClientError) -> ModelOutput | Exception:
        if (
            ex.code == 400
            and ex.message
            and (
                "maximum number of tokens" in ex.message
                or "size exceeds the limit" in ex.message
            )
        ):
            return ModelOutput.from_content(
                self.model_name, content=ex.message, stop_reason="model_length"
            )
        else:
            raise ex
def safety_settings_to_list(safety_settings: SafetySettingDict) -> list[SafetySetting]:
    return [
        SafetySetting(
            category=category,
            threshold=threshold,
        )
        for category, threshold in safety_settings.items()
    ]
def build_model_call(
    contents: list[Content],
    generation_config: GenerationConfig,
    safety_settings: SafetySettingDict,
    tools: list[Tool] | None,
    tool_config: ToolConfig | None,
    response: GenerateContentResponse | None,
    time: float | None,
) -> ModelCall:
    return ModelCall.create(
        request=dict(
            contents=contents,
            generation_config=generation_config,
            safety_settings=safety_settings,
            tools=tools if tools is not None else None,
            tool_config=tool_config if tool_config is not None else None,
        ),
        response=response if response is not None else {},
        filter=model_call_filter,
        time=time,
    )
def model_call_filter(key: JsonValue | None, value: JsonValue) -> JsonValue:
    if key == "inline_data" and isinstance(value, dict) and "data" in value:
        value = copy(value)
        value.update(data=BASE_64_DATA_REMOVED)
    return value
async def as_chat_messages(
    client: Client, messages: list[ChatMessage]
) -> list[Content]:
    # There is no "system" role in the `google-genai` package. Instead, system messages
    # are included in the `GenerateContentConfig` as a `system_instruction`. Strip any
    # system messages out.
    supported_messages = [message for message in messages if message.role != "system"]
    # build google chat messages
    chat_messages = [await content(client, message) for message in supported_messages]
    # combine consecutive tool messages
    chat_messages = functools.reduce(
        consecutive_tool_message_reducer, chat_messages, []
    )
    # return messages
    return chat_messages
def consecutive_tool_message_reducer(
    messages: list[Content],
    message: Content,
) -> list[Content]:
    if (
        message.role == "function"
        and len(messages) > 0
        and messages[-1].role == "function"
    ):
        messages[-1] = Content(
            role="function", parts=messages[-1].parts + message.parts
        )
    else:
        messages.append(message)
    return messages
async def content(
    client: Client,
    message: ChatMessageUser | ChatMessageAssistant | ChatMessageTool,
) -> Content:
    if isinstance(message, ChatMessageUser):
        if isinstance(message.content, str):
            return Content(
                role="user", parts=[await content_part(client, message.content)]
            )
        return Content(
            role="user",
            parts=(
                [await content_part(client, content) for content in message.content]
            ),
        )
    elif isinstance(message, ChatMessageAssistant):
        content_parts: list[Part] = []
        # tool call parts
        if message.tool_calls is not None:
            content_parts.extend(
                [
                    Part.from_function_call(
                        name=tool_call.function,
                        args=tool_call.arguments,
                    )
                    for tool_call in message.tool_calls
                ]
            )
        # content parts
        if isinstance(message.content, str):
            content_parts.append(Part(text=message.content or NO_CONTENT))
        else:
            content_parts.extend(
                [await content_part(client, content) for content in message.content]
            )
        # return parts
        return Content(role="model", parts=content_parts)
    elif isinstance(message, ChatMessageTool):
        response = FunctionResponse(
            name=message.tool_call_id,
            response={
                "content": (
                    message.error.message if message.error is not None else message.text
                )
            },
        )
        return Content(role="function", parts=[Part(function_response=response)])
async def content_part(client: Client, content: InspectContent | str) -> Part:
    if isinstance(content, str):
        return Part.from_text(text=content or NO_CONTENT)
    elif isinstance(content, ContentText):
        return Part.from_text(text=content.text or NO_CONTENT)
    elif isinstance(content, ContentReasoning):
        return Part.from_text(text=content.reasoning or NO_CONTENT)
    else:
        return await chat_content_to_part(client, content)
async def chat_content_to_part(
    client: Client,
    content: ContentImage | ContentAudio | ContentVideo,
) -> Part:
    if isinstance(content, ContentImage):
        content_bytes, mime_type = await file_as_data(content.image)
        return Part.from_bytes(mime_type=mime_type, data=content_bytes)
    else:
        file = await file_for_content(client, content)
        return Part.from_uri(file_uri=file.uri, mime_type=file.mime_type)
async def extract_system_message_as_parts(
    client: Client,
    messages: list[ChatMessage],
) -> list[Part] | None:
    system_parts: list[Part] = []
    for message in messages:
        if message.role == "system":
            content = message.content
            if isinstance(content, str):
                system_parts.append(Part.from_text(text=content))
            elif isinstance(content, list):  # list[InspectContent]
                system_parts.extend(
                    [await content_part(client, content) for content in content]
                )
            else:
                raise ValueError(f"Unsupported system message content: {content}")
    # google-genai raises "ValueError: content is required." if the list is empty.
    return system_parts or None
def chat_tools(tools: list[ToolInfo]) -> list[Tool]:
    declarations = [
        FunctionDeclaration(
            name=tool.name,
            description=tool.description,
            parameters=schema_from_param(tool.parameters)
            if len(tool.parameters.properties) > 0
            else None,
        )
        for tool in tools
    ]
    return [Tool(function_declarations=declarations)]
# https://ai.google.dev/gemini-api/tutorials/extract_structured_data#define_the_schema
def schema_from_param(
    param: ToolParam | ToolParams, nullable: bool | None = False
) -> Schema:
    if isinstance(param, ToolParams):
        param = ToolParam(
            type=param.type, properties=param.properties, required=param.required
        )
    if param.type == "number":
        return Schema(
            type=Type.NUMBER, description=param.description, nullable=nullable
        )
    elif param.type == "integer":
        return Schema(
            type=Type.INTEGER, description=param.description, nullable=nullable
        )
    elif param.type == "boolean":
        return Schema(
            type=Type.BOOLEAN, description=param.description, nullable=nullable
        )
    elif param.type == "string":
        return Schema(
            type=Type.STRING, description=param.description, nullable=nullable
        )
    elif param.type == "array":
        return Schema(
            type=Type.ARRAY,
            description=param.description,
            items=schema_from_param(param.items) if param.items else None,
            nullable=nullable,
        )
    elif param.type == "object":
        return Schema(
            type=Type.OBJECT,
            description=param.description,
            properties={k: schema_from_param(v) for k, v in param.properties.items()}
            if param.properties is not None
            else {},
            required=param.required,
            nullable=nullable,
        )
    # convert unions to optional params if the second type is 'null'
    elif param.anyOf:
        if len(param.anyOf) == 2 and param.anyOf[1].type == "null":
            return schema_from_param(param.anyOf[0], nullable=True)
        else:
            return Schema(type=Type.TYPE_UNSPECIFIED)
    elif param.enum:
        return Schema(type=Type.STRING, format="enum", enum=param.enum)
    else:
        return Schema(type=Type.TYPE_UNSPECIFIED)
def chat_tool_config(tool_choice: ToolChoice) -> ToolConfig:
    if isinstance(tool_choice, ToolFunction):
        return ToolConfig(
            function_calling_config=FunctionCallingConfig(
                mode="ANY", allowed_function_names=[tool_choice.name]
            )
        )
    else:
        return ToolConfig(
            function_calling_config=FunctionCallingConfig(mode=tool_choice.upper())
        )
def completion_choice_from_candidate(candidate: Candidate) -> ChatCompletionChoice:
    # content can be None when the finish_reason is SAFETY
    if candidate.content is None:
        content = ""
    # content.parts can be None when the finish_reason is MALFORMED_FUNCTION_CALL
    elif candidate.content.parts is None:
        content = ""
    else:
        content = " ".join(
            [
                part.text
                for part in candidate.content.parts
                if part.text is not None and candidate.content is not None
            ]
        )
    # split reasoning
    reasoning, content = split_reasoning(content)
    # now tool calls
    tool_calls: list[ToolCall] = []
    if candidate.content is not None and candidate.content.parts is not None:
        for part in candidate.content.parts:
            if part.function_call:
                tool_calls.append(
                    ToolCall(
                        type="function",
                        id=part.function_call.name,
                        function=part.function_call.name,
                        arguments=part.function_call.args,
                    )
                )
    # stop reason
    stop_reason = finish_reason_to_stop_reason(candidate.finish_reason)
    # choice content may include reasoning
    if reasoning:
        choice_content: str | list[Content] = [
            ContentReasoning(reasoning=reasoning),
            ContentText(text=content),
        ]
    else:
        choice_content = content
    # build choice
    choice = ChatCompletionChoice(
        message=ChatMessageAssistant(
            content=choice_content,
            tool_calls=tool_calls if len(tool_calls) > 0 else None,
            source="generate",
        ),
        stop_reason=stop_reason,
    )
    # add logprobs if provided
    if candidate.logprobs_result:
        logprobs: list[Logprob] = []
        for chosen, top in zip(
            candidate.logprobs_result.chosen_candidates,
            candidate.logprobs_result.top_candidates,
        ):
            logprobs.append(
                Logprob(
                    token=chosen.token,
                    logprob=chosen.log_probability,
                    top_logprobs=[
                        TopLogprob(token=c.token, logprob=c.log_probability)
                        for c in top.candidates
                    ],
                )
            )
        choice.logprobs = Logprobs(content=logprobs)
    return choice
def completion_choices_from_candidates(
    response: GenerateContentResponse,
) -> list[ChatCompletionChoice]:
    candidates = response.candidates
    if candidates:
        candidates_list = sorted(candidates, key=lambda c: c.index)
        return [
            completion_choice_from_candidate(candidate) for candidate in candidates_list
        ]
    elif response.prompt_feedback:
        return [
            ChatCompletionChoice(
                message=ChatMessageAssistant(
                    content=prompt_feedback_to_content(response.prompt_feedback),
                    source="generate",
                ),
                stop_reason="content_filter",
            )
        ]
    else:
        raise RuntimeError(
            "Google response includes no completion candidates and no block reason: "
            + f"{response.model_dump_json(indent=2)}"
        )
def split_reasoning(content: str) -> tuple[str | None, str]:
    separator = "\nFinal Answer: "
    if separator in content:
        parts = content.split(separator, 1)  # dplit only on first occurrence
        return parts[0].strip(), separator.lstrip() + parts[1].strip()
    else:
        return None, content.strip()
def prompt_feedback_to_content(
    feedback: GenerateContentResponsePromptFeedback,
) -> str:
    content: list[str] = []
    block_reason = str(feedback.block_reason) if feedback.block_reason else "UNKNOWN"
    content.append(f"BLOCKED: {block_reason}")
    if feedback.block_reason_message is not None:
        content.append(feedback.block_reason_message)
    if feedback.safety_ratings is not None:
        content.extend(
            [rating.model_dump_json(indent=2) for rating in feedback.safety_ratings]
        )
    return "\n".join(content)
def usage_metadata_to_model_usage(
    metadata: GenerateContentResponseUsageMetadata,
) -> ModelUsage | None:
    if metadata is None:
        return None
    return ModelUsage(
        input_tokens=metadata.prompt_token_count or 0,
        output_tokens=metadata.candidates_token_count or 0,
        total_tokens=metadata.total_token_count or 0,
    )
def finish_reason_to_stop_reason(finish_reason: FinishReason) -> StopReason:
    match finish_reason:
        case FinishReason.STOP:
            return "stop"
        case FinishReason.MAX_TOKENS:
            return "max_tokens"
        case (
            FinishReason.SAFETY
            | FinishReason.RECITATION
            | FinishReason.BLOCKLIST
            | FinishReason.PROHIBITED_CONTENT
            | FinishReason.SPII
        ):
            return "content_filter"
        case _:
            # Note: to avoid adding another option to StopReason,
            # this includes FinishReason.MALFORMED_FUNCTION_CALL
            return "unknown"
def parse_safety_settings(
    safety_settings: Any,
) -> dict[HarmCategory, HarmBlockThreshold]:
    # ensure we have a dict
    if isinstance(safety_settings, str):
        safety_settings = json.loads(safety_settings)
    if not isinstance(safety_settings, dict):
        raise ValueError(f"{SAFETY_SETTINGS} must be dictionary.")
    parsed_settings: dict[HarmCategory, HarmBlockThreshold] = {}
    for key, value in safety_settings.items():
        if not isinstance(key, str):
            raise ValueError(f"Unexpected type for harm category: {key}")
        if not isinstance(value, str):
            raise ValueError(f"Unexpected type for harm block threshold: {value}")
        key = str_to_harm_category(key)
        value = str_to_harm_block_threshold(value)
        parsed_settings[key] = value
    return parsed_settings
def str_to_harm_category(category: str) -> HarmCategory:
    category = category.upper()
    # `in` instead of `==` to allow users to pass in short version e.g. "HARASSMENT" or
    # long version e.g. "HARM_CATEGORY_HARASSMENT" strings.
    if "CIVIC_INTEGRITY" in category:
        return HarmCategory.HARM_CATEGORY_CIVIC_INTEGRITY
    if "DANGEROUS_CONTENT" in category:
        return HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT
    if "HATE_SPEECH" in category:
        return HarmCategory.HARM_CATEGORY_HATE_SPEECH
    if "HARASSMENT" in category:
        return HarmCategory.HARM_CATEGORY_HARASSMENT
    if "SEXUALLY_EXPLICIT" in category:
        return HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT
    if "UNSPECIFIED" in category:
        return HarmCategory.HARM_CATEGORY_UNSPECIFIED
    raise ValueError(f"Unknown HarmCategory: {category}")
def str_to_harm_block_threshold(threshold: str) -> HarmBlockThreshold:
    threshold = threshold.upper()
    if "LOW" in threshold:
        return HarmBlockThreshold.BLOCK_LOW_AND_ABOVE
    if "MEDIUM" in threshold:
        return HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE
    if "HIGH" in threshold:
        return HarmBlockThreshold.BLOCK_ONLY_HIGH
    if "NONE" in threshold:
        return HarmBlockThreshold.BLOCK_NONE
    if "OFF" in threshold:
        return HarmBlockThreshold.OFF
    raise ValueError(f"Unknown HarmBlockThreshold: {threshold}")
async def file_for_content(
    client: Client, content: ContentAudio | ContentVideo
) -> File:
    # helper to write trace messages
    def trace(message: str) -> None:
        trace_message(logger, "Google Files", message)
    # get the file bytes and compute sha256 hash
    if isinstance(content, ContentAudio):
        file = content.audio
    else:
        file = content.video
    content_bytes, mime_type = await file_as_data(file)
    content_sha256 = hashlib.sha256(content_bytes).hexdigest()
    # we cache uploads for re-use, open the db where we track that
    # (track up to 1 million previous uploads)
    with inspect_kvstore("google_files", 1000000) as files_db:
        # can we serve from existing uploads?
        uploaded_file = files_db.get(content_sha256)
        if uploaded_file:
            try:
                upload: File = client.files.get(name=uploaded_file)
                if upload.state.name == "ACTIVE":
                    trace(f"Using uploaded file: {uploaded_file}")
                    return upload
                else:
                    trace(
                        f"Not using uploaded file '{uploaded_file} (state was {upload.state})"
                    )
            except Exception as ex:
                trace(f"Error attempting to access uploaded file: {ex}")
                files_db.delete(content_sha256)
        # do the upload (and record it)
        upload = client.files.upload(
            file=BytesIO(content_bytes), config=dict(mime_type=mime_type)
        )
        while upload.state.name == "PROCESSING":
            await anyio.sleep(3)
            upload = client.files.get(name=upload.name)
        if upload.state.name == "FAILED":
            trace(f"Failed to upload file '{upload.name}: {upload.error}")
            raise ValueError(f"Google file upload failed: {upload.error}")
        # trace and record it
        trace(f"Uploaded file: {upload.name}")
        files_db.put(content_sha256, upload.name)
        # return the file
        return upload

================
File: src/inspect_ai/model/_providers/grok.py
================
import os
from inspect_ai.model._providers.util import model_base_url
from inspect_ai.model._providers.util.util import environment_prerequisite_error
from .._generate_config import GenerateConfig
from .openai import OpenAIAPI
GROK_API_KEY = "GROK_API_KEY"
class GrokAPI(OpenAIAPI):
    def __init__(
        self,
        model_name: str,
        base_url: str | None = None,
        api_key: str | None = None,
        config: GenerateConfig = GenerateConfig(),
    ) -> None:
        # resolve base url
        base_url = model_base_url(base_url, "GROK_BASE_URL")
        base_url = base_url or "https://api.x.ai/v1"
        # resolve api key
        api_key = api_key or os.environ.get(GROK_API_KEY, None)
        if api_key is None:
            raise environment_prerequisite_error("Grok", GROK_API_KEY)
        # call super
        super().__init__(
            model_name=model_name,
            base_url=base_url,
            api_key=api_key,
            config=config,
        )

================
File: src/inspect_ai/model/_providers/groq.py
================
import json
import os
from copy import copy
from typing import Any, Dict, Iterable, List, Optional
import httpx
from groq import (
    APIStatusError,
    APITimeoutError,
    AsyncGroq,
)
from groq.types.chat import (
    ChatCompletion,
    ChatCompletionAssistantMessageParam,
    ChatCompletionContentPartImageParam,
    ChatCompletionContentPartParam,
    ChatCompletionContentPartTextParam,
    ChatCompletionMessageParam,
    ChatCompletionMessageToolCallParam,
    ChatCompletionSystemMessageParam,
    ChatCompletionToolMessageParam,
    ChatCompletionUserMessageParam,
)
from pydantic import JsonValue
from typing_extensions import override
from inspect_ai._util.constants import (
    BASE_64_DATA_REMOVED,
    DEFAULT_MAX_TOKENS,
)
from inspect_ai._util.content import Content, ContentReasoning, ContentText
from inspect_ai._util.http import is_retryable_http_status
from inspect_ai._util.images import file_as_data_uri
from inspect_ai._util.url import is_http_url
from inspect_ai.tool import ToolCall, ToolChoice, ToolFunction, ToolInfo
from .._call_tools import parse_tool_call
from .._chat_message import (
    ChatMessage,
    ChatMessageAssistant,
    ChatMessageSystem,
    ChatMessageTool,
    ChatMessageUser,
)
from .._generate_config import GenerateConfig
from .._model import ModelAPI
from .._model_call import ModelCall
from .._model_output import (
    ChatCompletionChoice,
    ModelOutput,
    ModelUsage,
    as_stop_reason,
)
from .util import (
    environment_prerequisite_error,
    model_base_url,
)
from .util.hooks import HttpxHooks
GROQ_API_KEY = "GROQ_API_KEY"
class GroqAPI(ModelAPI):
    def __init__(
        self,
        model_name: str,
        base_url: Optional[str] = None,
        api_key: Optional[str] = None,
        config: GenerateConfig = GenerateConfig(),
        **model_args: Any,
    ):
        super().__init__(
            model_name=model_name,
            base_url=base_url,
            api_key=api_key,
            api_key_vars=[GROQ_API_KEY],
            config=config,
        )
        if not self.api_key:
            self.api_key = os.environ.get(GROQ_API_KEY)
        if not self.api_key:
            raise environment_prerequisite_error("Groq", GROQ_API_KEY)
        self.client = AsyncGroq(
            api_key=self.api_key,
            base_url=model_base_url(base_url, "GROQ_BASE_URL"),
            **model_args,
            http_client=httpx.AsyncClient(limits=httpx.Limits(max_connections=None)),
        )
        # create time tracker
        self._http_hooks = HttpxHooks(self.client._client)
    @override
    async def close(self) -> None:
        await self.client.close()
    async def generate(
        self,
        input: list[ChatMessage],
        tools: list[ToolInfo],
        tool_choice: ToolChoice,
        config: GenerateConfig,
    ) -> tuple[ModelOutput, ModelCall]:
        # allocate request_id (so we can see it from ModelCall)
        request_id = self._http_hooks.start_request()
        # setup request and response for ModelCall
        request: dict[str, Any] = {}
        response: dict[str, Any] = {}
        def model_call() -> ModelCall:
            return ModelCall.create(
                request=request,
                response=response,
                filter=model_call_filter,
                time=self._http_hooks.end_request(request_id),
            )
        messages = await as_groq_chat_messages(input)
        params = self.completion_params(config)
        if tools:
            params["tools"] = chat_tools(tools)
            params["tool_choice"] = (
                chat_tool_choice(tool_choice) if tool_choice else "auto"
            )
            if config.parallel_tool_calls is not None:
                params["parallel_tool_calls"] = config.parallel_tool_calls
        request = dict(
            messages=messages,
            model=self.model_name,
            extra_headers={HttpxHooks.REQUEST_ID_HEADER: request_id},
            **params,
        )
        completion: ChatCompletion = await self.client.chat.completions.create(
            **request,
        )
        response = completion.model_dump()
        # extract metadata
        metadata: dict[str, Any] = {
            "id": completion.id,
            "system_fingerprint": completion.system_fingerprint,
            "created": completion.created,
        }
        if completion.usage:
            metadata = metadata | {
                "queue_time": completion.usage.queue_time,
                "prompt_time": completion.usage.prompt_time,
                "completion_time": completion.usage.completion_time,
                "total_time": completion.usage.total_time,
            }
        # extract output
        choices = self._chat_choices_from_response(completion, tools)
        output = ModelOutput(
            model=completion.model,
            choices=choices,
            usage=(
                ModelUsage(
                    input_tokens=completion.usage.prompt_tokens,
                    output_tokens=completion.usage.completion_tokens,
                    total_tokens=completion.usage.total_tokens,
                )
                if completion.usage
                else None
            ),
            metadata=metadata,
        )
        # return
        return output, model_call()
    def completion_params(self, config: GenerateConfig) -> Dict[str, Any]:
        params: dict[str, Any] = {}
        if config.temperature is not None:
            params["temperature"] = config.temperature
        if config.max_tokens is not None:
            params["max_tokens"] = config.max_tokens
        if config.top_p is not None:
            params["top_p"] = config.top_p
        if config.stop_seqs:
            params["stop"] = config.stop_seqs
        if config.presence_penalty is not None:
            params["presence_penalty"] = config.presence_penalty
        if config.frequency_penalty is not None:
            params["frequency_penalty"] = config.frequency_penalty
        if config.seed is not None:
            params["seed"] = config.seed
        if config.num_choices is not None:
            params["n"] = config.num_choices
        return params
    def _chat_choices_from_response(
        self, response: Any, tools: list[ToolInfo]
    ) -> List[ChatCompletionChoice]:
        choices = list(response.choices)
        choices.sort(key=lambda c: c.index)
        return [
            ChatCompletionChoice(
                message=chat_message_assistant(choice.message, tools),
                stop_reason=as_stop_reason(choice.finish_reason),
            )
            for choice in choices
        ]
    @override
    def should_retry(self, ex: Exception) -> bool:
        if isinstance(ex, APIStatusError):
            return is_retryable_http_status(ex.status_code)
        elif isinstance(ex, APITimeoutError):
            return True
        else:
            return False
    @override
    def connection_key(self) -> str:
        return str(self.api_key)
    @override
    def collapse_user_messages(self) -> bool:
        return False
    @override
    def collapse_assistant_messages(self) -> bool:
        return False
    @override
    def max_tokens(self) -> Optional[int]:
        return DEFAULT_MAX_TOKENS
async def as_groq_chat_messages(
    messages: list[ChatMessage],
) -> list[ChatCompletionMessageParam]:
    return [await groq_chat_message(message) for message in messages]
async def groq_chat_message(message: ChatMessage) -> ChatCompletionMessageParam:
    if isinstance(message, ChatMessageSystem):
        return ChatCompletionSystemMessageParam(role="system", content=message.text)
    elif isinstance(message, ChatMessageUser):
        content: str | Iterable[ChatCompletionContentPartParam] = (
            message.content
            if isinstance(message.content, str)
            else [await as_chat_completion_part(content) for content in message.content]
        )
        return ChatCompletionUserMessageParam(role="user", content=content)
    elif isinstance(message, ChatMessageAssistant):
        return ChatCompletionAssistantMessageParam(
            role="assistant",
            content=message.text,
            tool_calls=[
                ChatCompletionMessageToolCallParam(
                    id=call.id,
                    type="function",
                    function={
                        "name": call.function,
                        "arguments": json.dumps(call.arguments),
                    },
                )
                for call in (message.tool_calls or [])
            ],
        )
    elif isinstance(message, ChatMessageTool):
        return ChatCompletionToolMessageParam(
            role="tool",
            content=message.text,
            tool_call_id=str(message.tool_call_id),
        )
async def as_chat_completion_part(
    content: Content,
) -> ChatCompletionContentPartParam:
    if content.type == "text":
        return ChatCompletionContentPartTextParam(type="text", text=content.text)
    elif content.type == "image":
        # API takes URL or base64 encoded file. If it's a remote file or data URL leave it alone, otherwise encode it
        image_url = content.image
        detail = content.detail
        if not is_http_url(image_url):
            image_url = await file_as_data_uri(image_url)
        return ChatCompletionContentPartImageParam(
            type="image_url",
            image_url=dict(url=image_url, detail=detail),
        )
    else:
        raise RuntimeError("Groq models do not support audio or video inputs.")
def chat_tools(tools: List[ToolInfo]) -> List[Dict[str, Any]]:
    return [
        {"type": "function", "function": tool.model_dump(exclude_none=True)}
        for tool in tools
    ]
def chat_tool_choice(tool_choice: ToolChoice) -> str | Dict[str, Any]:
    if isinstance(tool_choice, ToolFunction):
        return {"type": "function", "function": {"name": tool_choice.name}}
    elif tool_choice == "any":
        return "auto"
    else:
        return tool_choice
def chat_tool_calls(message: Any, tools: list[ToolInfo]) -> Optional[List[ToolCall]]:
    if hasattr(message, "tool_calls") and message.tool_calls:
        return [
            parse_tool_call(call.id, call.function.name, call.function.arguments, tools)
            for call in message.tool_calls
        ]
    return None
def chat_message_assistant(message: Any, tools: list[ToolInfo]) -> ChatMessageAssistant:
    reasoning = getattr(message, "reasoning", None)
    if reasoning is not None:
        content: str | list[Content] = [
            ContentReasoning(reasoning=str(reasoning)),
            ContentText(text=message.content or ""),
        ]
    else:
        content = message.content or ""
    return ChatMessageAssistant(
        content=content,
        source="generate",
        tool_calls=chat_tool_calls(message, tools),
    )
def model_call_filter(key: JsonValue | None, value: JsonValue) -> JsonValue:
    # remove base64 encoded images
    if key == "image_url" and isinstance(value, dict):
        value = copy(value)
        value.update(url=BASE_64_DATA_REMOVED)
    return value

================
File: src/inspect_ai/model/_providers/hf.py
================
import concurrent
import concurrent.futures
import copy
import functools
import gc
import json
import os
import time
from concurrent.futures import Future
from dataclasses import dataclass
from logging import getLogger
from queue import Empty, Queue
from threading import Thread
from typing import Any, Literal, Protocol, cast
import anyio
import numpy as np
import torch  # type: ignore
from torch import Tensor  # type: ignore
from transformers import (  # type: ignore
    AutoModelForCausalLM,
    AutoTokenizer,
    PreTrainedTokenizerBase,
    set_seed,
)
from typing_extensions import override
from inspect_ai._util.constants import DEFAULT_MAX_TOKENS
from inspect_ai._util.content import ContentText
from inspect_ai._util.trace import trace_action
from inspect_ai.tool import ToolChoice, ToolInfo
from .._chat_message import ChatMessage, ChatMessageAssistant
from .._generate_config import GenerateConfig
from .._model import ModelAPI
from .._model_output import (
    ChatCompletionChoice,
    Logprob,
    Logprobs,
    ModelOutput,
    ModelUsage,
    TopLogprob,
)
from .util import ChatAPIHandler, HFHandler
logger = getLogger(__name__)
HF_TOKEN = "HF_TOKEN"
class HuggingFaceAPI(ModelAPI):
    def __init__(
        self,
        model_name: str,
        base_url: str | None = None,
        api_key: str | None = None,
        config: GenerateConfig = GenerateConfig(),
        **model_args: Any,
    ):
        super().__init__(
            model_name=model_name,
            base_url=base_url,
            api_key=api_key,
            api_key_vars=[HF_TOKEN],
            config=config,
        )
        # set random seeds
        if config.seed is not None:
            set_random_seeds(config.seed)
        # collect known model_args (then delete them so we can pass the rest on)
        def collect_model_arg(name: str) -> Any | None:
            nonlocal model_args
            value = model_args.get(name, None)
            if value is not None:
                model_args.pop(name)
            return value
        device = collect_model_arg("device")
        tokenizer = collect_model_arg("tokenizer")
        model_path = collect_model_arg("model_path")
        tokenizer_path = collect_model_arg("tokenizer_path")
        self.batch_size = collect_model_arg("batch_size")
        self.chat_template = collect_model_arg("chat_template")
        self.tokenizer_call_args = collect_model_arg("tokenizer_call_args")
        if self.tokenizer_call_args is None:
            self.tokenizer_call_args = {}
        # device
        if device:
            self.device = device
        elif torch.backends.mps.is_available():
            self.device = "mps"
        elif torch.cuda.is_available():
            self.device = "cuda:0"
        else:
            self.device = "cpu"
        # model
        if model_path:
            self.model = AutoModelForCausalLM.from_pretrained(
                model_path, device_map=self.device, token=self.api_key, **model_args
            )
        else:
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name, device_map=self.device, token=self.api_key, **model_args
            )
        # tokenizer
        if tokenizer:
            self.tokenizer = AutoTokenizer.from_pretrained(tokenizer)
        elif model_path:
            if tokenizer_path:
                self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
            else:
                self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        else:
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        # LLMs generally don't have a pad token and we need one for batching
        self.tokenizer.pad_token = self.tokenizer.eos_token
        self.tokenizer.padding_side = "left"
    @override
    async def close(self) -> None:
        self.model = None
        self.tokenizer = None
        gc.collect()
    async def generate(
        self,
        input: list[ChatMessage],
        tools: list[ToolInfo],
        tool_choice: ToolChoice,
        config: GenerateConfig,
    ) -> ModelOutput:
        # create handler
        handler: ChatAPIHandler | None = (
            HFHandler(self.model_name) if len(tools) > 0 else None
        )
        # create chat
        chat = self.hf_chat(input, tools)
        assert isinstance(self.tokenizer_call_args, dict)
        # prepare tokenizer
        tokenizer = functools.partial(
            self.tokenizer,
            return_tensors="pt",
            padding=True,
            **self.tokenizer_call_args,
        )
        # prepare generator
        kwargs: dict[str, Any] = dict(do_sample=True)
        if config.max_tokens is not None:
            kwargs["max_new_tokens"] = config.max_tokens
        if config.temperature is not None:
            kwargs["temperature"] = config.temperature
        if config.top_p is not None:
            kwargs["top_p"] = config.top_p
        if config.top_k is not None:
            kwargs["top_k"] = config.top_k
        if config.logprobs is not None:
            kwargs["output_logits"] = config.logprobs
        if "return_dict_in_generate" in kwargs:
            assert kwargs["return_dict_in_generate"]
        if config.stop_seqs is not None:
            from transformers.generation import StopStringCriteria  # type: ignore
            stopping_criteria = [StopStringCriteria(self.tokenizer, config.stop_seqs)]
            kwargs["stopping_criteria"] = stopping_criteria
        kwargs["return_dict_in_generate"] = True
        generator = functools.partial(self.model.generate, **kwargs)
        # prepare decoder
        decoder = functools.partial(
            self.tokenizer.batch_decode,
            skip_special_tokens=True,
            clean_up_tokenization_spaces=False,
        )
        # generate (uses a queue to batch so we await)
        response = await batched_generate(
            GenerateInput(
                input=chat,
                device=self.model.device,
                tokenizer=tokenizer,
                generator=generator,
                decoder=decoder,
                batch_size=config.max_connections or self.max_connections(),
            )
        )
        # gather logprobs
        final_logprobs = None
        if config.logprobs is not None:
            final_logprobs = extract_logprobs(
                response=response,
                top=config.top_logprobs,
                tokenizer=self.tokenizer,
            )
        # construct choice
        choice = ChatCompletionChoice(
            message=ChatMessageAssistant(content=response.output, source="generate"),
            logprobs=(
                Logprobs(content=final_logprobs) if final_logprobs is not None else None
            ),
        )
        choice = ChatCompletionChoice(
            message=chat_completion_assistant_message(
                response, tools, handler, self.model_name
            ),
            logprobs=(
                Logprobs(content=final_logprobs) if final_logprobs is not None else None
            ),
        )
        # return output
        return ModelOutput(
            model=self.model_name,
            choices=[choice],
            usage=ModelUsage(
                input_tokens=response.input_tokens,
                output_tokens=response.output_tokens,
                total_tokens=response.total_tokens,
            ),
            time=response.time,
        )
    @override
    def max_tokens(self) -> int | None:
        """Default is 16, bump it up to a value suitable for evals."""
        return DEFAULT_MAX_TOKENS
    @override
    def max_connections(self) -> int:
        """Effectively the batch size."""
        return 32
    @override
    def collapse_user_messages(self) -> bool:
        return True
    def hf_chat(self, messages: list[ChatMessage], tools: list[ToolInfo]) -> str:
        # convert to hf format
        tools_list = []
        hf_messages = copy.deepcopy(messages)
        if len(tools) > 0:
            tools_list = [
                json.loads(tool.model_dump_json(exclude_none=True, indent=2))
                for tool in tools
            ]
            if "mistral" in self.model_name.lower():
                hf_messages = shorten_tool_id(hf_messages)
                tools_list = tools_to_mistral_format(tools_list)
            elif "qwen" in self.model_name.lower():
                hf_messages = inspect_tools_to_string(hf_messages)
        # apply chat template
        if self.tokenizer.chat_template is not None:
            chat = self.tokenizer.apply_chat_template(
                hf_messages,
                add_generation_prompt=True,
                tokenize=False,
                tools=tools_list if len(tools_list) > 0 else None,
            )
        else:
            chat = ""
            for message in hf_messages:
                chat += f"{message.role}: {message.content}\n"
        # return
        return cast(str, chat)
def shorten_tool_id(messages: list[ChatMessage]) -> list[ChatMessage]:
    """Shorten the tool_call_id in the messages to the last 9 characters for Mistral."""
    for i, message in enumerate(messages):
        if message.role == "tool":
            # Trim tool_call_id in tool messages
            if message.tool_call_id is not None:
                message.tool_call_id = message.tool_call_id[-9:]
        elif message.role == "assistant" and hasattr(message, "tool_calls"):
            # Trim tool_call IDs inside tool_calls for assistant messages
            for tool_call in message.tool_calls or []:
                tool_call.id = tool_call.id[-9:]
    return messages
def tools_to_mistral_format(tools: list[dict[str, Any]]) -> list[dict[str, Any]]:
    """Convert tools to the format required for Mistral."""
    mistral_tools = []
    for tool in tools:
        mistral_tools.append(
            {
                "function": {
                    "name": tool["name"],
                    "description": tool["description"],
                    "parameters": {
                        "type": tool["parameters"]["type"],
                        "properties": tool["parameters"]["properties"],
                        "required": tool["parameters"]["required"],
                    },
                }
            }
        )
    return mistral_tools
def inspect_tools_to_string(messages: list[ChatMessage]) -> list[ChatMessage]:
    """Convert tools to a string for Qwen."""
    for message in messages:
        if message.role == "assistant":
            # check if the message contains a tool call
            tool_content = ""
            if message.tool_calls:
                for tool_call in message.tool_calls:
                    tool_content += f'\n```json\n{{"name": "{tool_call.function}", "arguments": {json.dumps(tool_call.arguments)}}}\n```'
            # remove the tool call from the message
            message.tool_calls = None
            if isinstance(message.content, str):
                message.content += tool_content
            else:
                message.content.append(ContentText(text=tool_content))
    return messages
def chat_completion_assistant_message(
    response: Any,
    tools: list[ToolInfo],
    handler: ChatAPIHandler | None,
    model_name: str,
) -> ChatMessageAssistant:
    if handler:
        return handler.parse_assistant_response(response.output, tools)
    else:
        return ChatMessageAssistant(content=response.output, source="generate")
def set_random_seeds(seed: int | None = None) -> None:
    if seed is None:
        seed = np.random.default_rng().integers(2**32 - 1)
    # python hash seed
    os.environ["PYTHONHASHSEED"] = str(seed)
    # transformers seed
    set_seed(seed)
# return value from generate as a result of specifying return_dict_in_generate
class ModelGenerateOutput:
    sequences: Tensor
    logits: tuple[Tensor]
class Tokenizer(Protocol):
    def __call__(
        self, input: list[str]
    ) -> dict[Literal["input_ids", "attention_mask"], Tensor]: ...
class Generator(Protocol):
    def __call__(self, input_ids: Tensor, attention_mask: Tensor) -> Tensor: ...
class Decoder(Protocol):
    def __call__(self, sequences: Tensor) -> list[str]: ...
@dataclass
class GenerateInput:
    input: str
    device: str
    tokenizer: Tokenizer
    generator: Generator
    decoder: Decoder
    batch_size: int
@dataclass
class GenerateOutput:
    output: str
    input_tokens: int
    output_tokens: int
    total_tokens: int
    logprobs: torch.Tensor | None
    time: float
@dataclass
class _QueueItem:
    input: GenerateInput
    future: Future[GenerateOutput]
batch_thread: Thread | None = None
batch_queue: "Queue[_QueueItem]" = Queue()
async def batched_generate(input: GenerateInput) -> GenerateOutput:
    # start the background thread if necessary
    global batch_thread
    if batch_thread is None:
        batch_thread = Thread(target=process_batches, daemon=True)
        batch_thread.start()
    # enqueue the job
    future = Future[GenerateOutput]()
    batch_queue.put(_QueueItem(input=input, future=future))
    # await the future
    with trace_action(logger, "HF Batched Generate", "HF Batched Generate"):
        while True:
            try:
                return future.result(timeout=0.01)
            except concurrent.futures.TimeoutError:
                pass
            await anyio.sleep(1)
def process_batches() -> None:
    while True:
        # drain the queue (wait until no new messages have shown up for 2 seconds)
        inputs: list[tuple[GenerateInput, Future[GenerateOutput]]] = []
        while True:
            try:
                input = batch_queue.get(timeout=2)
                inputs.append((input.input, input.future))
                if len(inputs) == input.input.batch_size:
                    # max batch size reached
                    break
            except Empty:
                # we have exhausted the queue
                break
        # see if we have any work to do
        if len(inputs) == 0:
            continue
        try:
            # capture the generator and decoder functions
            start_time = time.monotonic()
            first_input = inputs[0][0]
            device = first_input.device
            tokenizer = first_input.tokenizer
            generator = first_input.generator
            decoder = first_input.decoder
            # tokenize and move to device
            tokenized_inputs = tokenizer([item[0].input for item in inputs])
            input_ids = tokenized_inputs["input_ids"]
            attention_mask = tokenized_inputs["attention_mask"]
            input_ids = input_ids.to(device)
            attention_mask = attention_mask.to(device)
            # generate
            with torch.inference_mode():
                generation_outputs = cast(
                    ModelGenerateOutput,
                    generator(input_ids=input_ids, attention_mask=attention_mask),
                )
                generate_ids = generation_outputs.sequences
                logits = generation_outputs.logits
            # get logprobs from logits
            logprobs = None
            if logits is not None:
                stacked_logits = torch.stack(logits).transpose(0, 1)
                logprobs = torch.nn.functional.log_softmax(stacked_logits, dim=-1)
            # decode
            generated_tokens = generate_ids[:, input_ids.size(dim=1) :]
            if logprobs is not None:
                assert logprobs.shape[1] == generated_tokens.shape[1]
            outputs = decoder(sequences=generated_tokens)
            # call back futures
            total_time = time.monotonic() - start_time
            for i, output in enumerate(outputs):
                future = inputs[i][1]
                input_tokens = input_ids.size(dim=1)
                output_tokens = generate_ids.size(dim=1) - input_ids.size(dim=1)
                # asyncio futures are not thread safe, so we need to pass the event loop
                # down to this point, so we can mark the future as done in a thread safe manner.
                # see: https://docs.python.org/3/library/asyncio-dev.html#concurrency-and-multithreading
                future.set_result(
                    GenerateOutput(
                        output=output,
                        input_tokens=input_tokens,
                        output_tokens=output_tokens,
                        total_tokens=input_tokens + output_tokens,
                        logprobs=logprobs[i] if logprobs is not None else None,
                        time=total_time,
                    )
                )
        except Exception as ex:
            for inp in inputs:
                future = inp[1]
                future.set_exception(ex)
def extract_logprobs(
    response: GenerateOutput,
    top: int | None,
    tokenizer: PreTrainedTokenizerBase,
) -> list[Logprob]:
    assert response.logprobs is not None
    k = top or 1
    topk_values, topk_inds = response.logprobs.topk(k=k, dim=-1)
    final_logprobs = []
    for toks, vals in zip(topk_inds, topk_values):
        top_logprobs: list[TopLogprob] = []
        for tok, val in zip(toks, vals):
            # TODO: you get byte artifacts converting single ids to tokens like this...
            # but `tokenizer.decode` strips spaces. There must be a better way to do this.
            token_str = tokenizer.convert_ids_to_tokens(tok.item())
            top_logprobs.append(
                TopLogprob(
                    token=token_str,
                    logprob=val,
                    bytes=list(map(ord, token_str)),
                )
            )
        final_logprobs.append(
            Logprob(
                token=top_logprobs[0].token,
                logprob=top_logprobs[0].logprob,
                bytes=top_logprobs[0].bytes,
                top_logprobs=top_logprobs,
            )
        )
    return final_logprobs

================
File: src/inspect_ai/model/_providers/llama_cpp_python.py
================
from inspect_ai.model._providers.util import model_base_url
from .._generate_config import GenerateConfig
from .openai import OpenAIAPI
class LlamaCppPythonAPI(OpenAIAPI):
    def __init__(
        self,
        model_name: str,
        base_url: str | None = None,
        api_key: str | None = None,
        config: GenerateConfig = GenerateConfig(),
    ) -> None:
        base_url = model_base_url(base_url, "LLAMA_CPP_PYTHON_BASE_URL")
        base_url = base_url if base_url else "http://localhost:8000/v1"
        if not api_key:
            api_key = "llama-cpp-python"
        super().__init__(
            model_name=model_name, base_url=base_url, api_key=api_key, config=config
        )

================
File: src/inspect_ai/model/_providers/mistral.py
================
import functools
import json
import os
from typing import Any, Literal
from httpcore import ReadTimeout
from httpx import ReadTimeout as AsyncReadTimeout
from mistralai import (
    ContentChunk,
    DocumentURLChunk,
    FunctionCall,
    FunctionName,
    ImageURL,
    ImageURLChunk,
    Mistral,
    ReferenceChunk,
    TextChunk,
)
from mistralai.models import (
    AssistantMessage as MistralAssistantMessage,
)
from mistralai.models import (
    ChatCompletionChoice as MistralChatCompletionChoice,
)
from mistralai.models import Function as MistralFunction
from mistralai.models import (
    JSONSchema as MistralJSONSchema,
)
from mistralai.models import (
    ResponseFormat as MistralResponseFormat,
)
from mistralai.models import SDKError
from mistralai.models import SystemMessage as MistralSystemMessage
from mistralai.models import Tool as MistralTool
from mistralai.models import ToolCall as MistralToolCall
from mistralai.models import (
    ToolChoice as MistralToolChoice,
)
from mistralai.models import ToolMessage as MistralToolMessage
from mistralai.models import UserMessage as MistralUserMessage
from mistralai.models.chatcompletionresponse import (
    ChatCompletionResponse as MistralChatCompletionResponse,
)
from typing_extensions import override
# TODO: Migration guide:
# https://github.com/mistralai/client-python/blob/main/MIGRATION.md
from inspect_ai._util.constants import NO_CONTENT
from inspect_ai._util.content import Content, ContentImage, ContentText
from inspect_ai._util.http import is_retryable_http_status
from inspect_ai._util.images import file_as_data_uri
from inspect_ai.tool import ToolCall, ToolChoice, ToolFunction, ToolInfo
from .._call_tools import parse_tool_call
from .._chat_message import (
    ChatMessage,
    ChatMessageAssistant,
)
from .._generate_config import GenerateConfig
from .._model import ModelAPI
from .._model_call import ModelCall
from .._model_output import (
    ChatCompletionChoice,
    ModelOutput,
    ModelUsage,
    StopReason,
)
from .util import environment_prerequisite_error, model_base_url
from .util.hooks import HttpxHooks
AZURE_MISTRAL_API_KEY = "AZURE_MISTRAL_API_KEY"
AZUREAI_MISTRAL_API_KEY = "AZUREAI_MISTRAL_API_KEY"
MISTRAL_API_KEY = "MISTRAL_API_KEY"
class MistralAPI(ModelAPI):
    def __init__(
        self,
        model_name: str,
        base_url: str | None = None,
        api_key: str | None = None,
        config: GenerateConfig = GenerateConfig(),
        **model_args: Any,
    ):
        super().__init__(
            model_name=model_name,
            base_url=base_url,
            api_key=api_key,
            api_key_vars=[
                MISTRAL_API_KEY,
                AZURE_MISTRAL_API_KEY,
                AZUREAI_MISTRAL_API_KEY,
            ],
            config=config,
        )
        # resolve api_key -- look for mistral then azure
        if not self.api_key:
            self.api_key = os.environ.get(MISTRAL_API_KEY, None)
            if self.api_key:
                base_url = model_base_url(base_url, "MISTRAL_BASE_URL")
            else:
                self.api_key = os.environ.get(
                    AZUREAI_MISTRAL_API_KEY, os.environ.get(AZURE_MISTRAL_API_KEY, None)
                )
                if not self.api_key:
                    raise environment_prerequisite_error(
                        "Mistral", [MISTRAL_API_KEY, AZUREAI_MISTRAL_API_KEY]
                    )
                base_url = model_base_url(base_url, "AZUREAI_MISTRAL_BASE_URL")
                if not base_url:
                    raise ValueError(
                        "You must provide a base URL when using Mistral on Azure. Use the AZUREAI_MISTRAL_BASE_URL "
                        + " environment variable or the --model-base-url CLI flag to set the base URL."
                    )
        if base_url:
            model_args["server_url"] = base_url
        self.model_args = model_args
    @override
    async def close(self) -> None:
        # client is created and destroyed in generate
        pass
    async def generate(
        self,
        input: list[ChatMessage],
        tools: list[ToolInfo],
        tool_choice: ToolChoice,
        config: GenerateConfig,
    ) -> ModelOutput | tuple[ModelOutput | Exception, ModelCall]:
        # create client
        with Mistral(api_key=self.api_key, **self.model_args) as client:
            # create time tracker
            http_hooks = HttpxHooks(client.sdk_configuration.async_client)
            # build request
            request_id = http_hooks.start_request()
            request: dict[str, Any] = dict(
                model=self.model_name,
                messages=await mistral_chat_messages(input),
                tools=mistral_chat_tools(tools) if len(tools) > 0 else None,
                tool_choice=(
                    mistral_chat_tool_choice(tool_choice) if len(tools) > 0 else None
                ),
                http_headers={HttpxHooks.REQUEST_ID_HEADER: request_id},
            )
            if config.temperature is not None:
                request["temperature"] = config.temperature
            if config.top_p is not None:
                request["top_p"] = config.top_p
            if config.max_tokens is not None:
                request["max_tokens"] = config.max_tokens
            if config.seed is not None:
                request["random_seed"] = config.seed
            if config.response_schema is not None:
                request["response_format"] = MistralResponseFormat(
                    type="json_schema",
                    json_schema=MistralJSONSchema(
                        name=config.response_schema.name,
                        description=config.response_schema.description,
                        schema_definition=config.response_schema.json_schema.model_dump(
                            exclude_none=True
                        ),
                        strict=config.response_schema.strict,
                    ),
                )
            # prepare response for inclusion in model call
            response: dict[str, Any] = {}
            def model_call() -> ModelCall:
                req = request.copy()
                req.update(
                    messages=[message.model_dump() for message in req["messages"]]
                )
                if req.get("tools", None) is not None:
                    req["tools"] = [tool.model_dump() for tool in req["tools"]]
                return ModelCall.create(
                    request=req,
                    response=response,
                    time=http_hooks.end_request(request_id),
                )
            # send request
            try:
                completion = await client.chat.complete_async(**request)
                response = completion.model_dump()
            except SDKError as ex:
                if ex.status_code == 400:
                    return self.handle_bad_request(ex), model_call()
                else:
                    raise ex
            if completion is None:
                raise RuntimeError(
                    "Mistral model did not return a response from generate."
                )
            # return model output (w/ tool calls if they exist)
            choices = completion_choices_from_response(completion, tools)
            return ModelOutput(
                model=completion.model,
                choices=choices,
                usage=ModelUsage(
                    input_tokens=completion.usage.prompt_tokens,
                    output_tokens=(
                        completion.usage.completion_tokens
                        if completion.usage.completion_tokens
                        else completion.usage.total_tokens
                        - completion.usage.prompt_tokens
                    ),
                    total_tokens=completion.usage.total_tokens,
                ),
            ), model_call()
    @override
    def should_retry(self, ex: Exception) -> bool:
        if isinstance(ex, SDKError):
            return is_retryable_http_status(ex.status_code)
        elif isinstance(ex, ReadTimeout | AsyncReadTimeout):
            return True
        else:
            return False
    @override
    def connection_key(self) -> str:
        return str(self.api_key)
    def handle_bad_request(self, ex: SDKError) -> ModelOutput | Exception:
        body = json.loads(ex.body)
        content = body.get("message", ex.body)
        if "maximum context length" in ex.body:
            return ModelOutput.from_content(
                model=self.model_name, content=content, stop_reason="model_length"
            )
        else:
            return ex
def mistral_model_call(
    request: dict[str, Any], response: MistralChatCompletionResponse | None
) -> ModelCall:
    request = request.copy()
    request.update(messages=[message.model_dump() for message in request["messages"]])
    if request.get("tools", None) is not None:
        request["tools"] = [tool.model_dump() for tool in request["tools"]]
    return ModelCall.create(
        request=request, response=response.model_dump() if response else {}
    )
def mistral_chat_tools(tools: list[ToolInfo]) -> list[MistralTool]:
    return [
        MistralTool(
            type="function",
            function=mistral_function(tool),
        )
        for tool in tools
    ]
def mistral_function(tool: ToolInfo) -> MistralFunction:
    return MistralFunction(
        name=tool.name,
        description=tool.description,
        parameters={
            k: v.model_dump(exclude={"additionalProperties"}, exclude_none=True)
            for k, v in tool.parameters.properties.items()
        },
    )
def mistral_chat_tool_choice(
    tool_choice: ToolChoice,
) -> str | dict[str, Any]:
    if isinstance(tool_choice, ToolFunction):
        return MistralToolChoice(
            type="function", function=FunctionName(name=tool_choice.name)
        ).model_dump()
    elif tool_choice == "any":
        return "any"
    elif tool_choice == "auto":
        return "auto"
    elif tool_choice == "none":
        return "none"
MistralMessage = (
    MistralSystemMessage
    | MistralUserMessage
    | MistralAssistantMessage
    | MistralToolMessage
)
async def mistral_chat_messages(messages: list[ChatMessage]) -> list[MistralMessage]:
    mistral_messages = [await mistral_chat_message(message) for message in messages]
    mistral_messages = functools.reduce(mistral_message_reducer, mistral_messages, [])
    return mistral_messages
def mistral_message_reducer(
    messages: list[MistralMessage],
    message: MistralMessage,
) -> list[MistralMessage]:
    if (
        len(messages) > 0
        and isinstance(messages[-1], MistralToolMessage)
        and isinstance(message, MistralUserMessage)
    ):
        messages[-1] = fold_user_message_into_tool_message(messages[-1], message)
    else:
        messages.append(message)
    return messages
def fold_user_message_into_tool_message(
    tool: MistralToolMessage, user: MistralUserMessage
) -> MistralToolMessage:
    def normalise_content(
        content: str | list[ContentChunk] | None,
    ) -> list[ContentChunk]:
        return (
            []
            if content is None
            else [TextChunk(text=content)]
            if isinstance(content, str)
            else content
        )
    # normalise tool and user content
    tool_content = normalise_content(tool.content)
    user_content = normalise_content(user.content)
    # return tool message w/ tool and user content combined
    return MistralToolMessage(
        content=tool_content + user_content,
        tool_call_id=tool.tool_call_id,
        name=tool.name,
        role=tool.role,
    )
async def mistral_chat_message(
    message: ChatMessage,
) -> MistralMessage:
    if message.role == "assistant" and message.tool_calls:
        return MistralAssistantMessage(
            role=message.role,
            content=await mistral_message_content(message.content),
            tool_calls=[mistral_tool_call(call) for call in message.tool_calls],
        )
    elif message.role == "tool":
        return MistralToolMessage(
            role=message.role,
            tool_call_id=message.tool_call_id,
            name=message.function,
            content=(
                f"Error: {message.error.message}" if message.error else message.text
            ),
        )
    elif message.role == "user":
        return MistralUserMessage(
            content=await mistral_message_content(message.content)
        )
    elif message.role == "system":
        return MistralSystemMessage(
            content=mistral_system_message_content(message.content)
        )
    elif message.role == "assistant":
        return MistralAssistantMessage(
            content=await mistral_message_content(message.content)
        )
async def mistral_message_content(
    content: str | list[Content],
) -> str | list[ContentChunk]:
    if isinstance(content, str):
        return content or NO_CONTENT
    else:
        return [await mistral_content_chunk(c) for c in content]
def mistral_system_message_content(
    content: str | list[Content],
) -> str | list[TextChunk]:
    if isinstance(content, str):
        return content or NO_CONTENT
    else:
        return [TextChunk(text=c.text) for c in content if isinstance(c, ContentText)]
async def mistral_content_chunk(content: Content) -> ContentChunk:
    if isinstance(content, ContentText):
        return TextChunk(text=content.text or NO_CONTENT)
    elif isinstance(content, ContentImage):
        # resolve image to url
        image_url = await file_as_data_uri(content.image)
        # return chunk
        return ImageURLChunk(image_url=ImageURL(url=image_url, detail=content.detail))
    else:
        raise RuntimeError("Mistral models do not support audio or video inputs.")
def mistral_tool_call(tool_call: ToolCall) -> MistralToolCall:
    return MistralToolCall(
        id=tool_call.id, function=mistral_function_call(tool_call), type="function"
    )
def mistral_function_call(tool_call: ToolCall) -> FunctionCall:
    return FunctionCall(
        name=tool_call.function, arguments=json.dumps(tool_call.arguments)
    )
def chat_tool_calls(
    tool_calls: list[MistralToolCall], tools: list[ToolInfo]
) -> list[ToolCall]:
    return [chat_tool_call(tool, tools) for tool in tool_calls]
def chat_tool_call(tool_call: MistralToolCall, tools: list[ToolInfo]) -> ToolCall:
    id = tool_call.id or tool_call.function.name
    if isinstance(tool_call.function.arguments, str):
        return parse_tool_call(
            id, tool_call.function.name, tool_call.function.arguments, tools
        )
    else:
        return ToolCall(
            id, tool_call.function.name, tool_call.function.arguments, type="function"
        )
def completion_choice(
    choice: MistralChatCompletionChoice, tools: list[ToolInfo]
) -> ChatCompletionChoice:
    message = choice.message
    if message:
        completion = completion_content(message.content or "")
        return ChatCompletionChoice(
            message=ChatMessageAssistant(
                content=completion,
                tool_calls=chat_tool_calls(message.tool_calls, tools)
                if message.tool_calls
                else None,
                source="generate",
            ),
            stop_reason=(
                choice_stop_reason(choice)
                if choice.finish_reason is not None
                else "unknown"
            ),
        )
    else:
        raise ValueError(
            f"Mistral did not return a message in Completion Choice: {choice.model_dump_json(indent=2, exclude_none=True)}"
        )
def completion_content(content: str | list[ContentChunk]) -> str | list[Content]:
    if isinstance(content, str):
        return content
    else:
        return [completion_content_chunk(c) for c in content]
def completion_content_chunk(content: ContentChunk) -> Content:
    if isinstance(content, ReferenceChunk):
        raise TypeError("ReferenceChunk content is not supported by Inspect.")
    elif isinstance(content, TextChunk):
        return ContentText(text=content.text)
    elif isinstance(content, DocumentURLChunk):
        return ContentText(text=content.document_url)
    else:
        if isinstance(content.image_url, str):
            return ContentImage(image=content.image_url)
        else:
            match content.image_url.detail:
                case "low" | "high":
                    detail: Literal["auto", "low", "high"] = content.image_url.detail
                case _:
                    detail = "auto"
            return ContentImage(image=content.image_url.url, detail=detail)
def completion_choices_from_response(
    response: MistralChatCompletionResponse, tools: list[ToolInfo]
) -> list[ChatCompletionChoice]:
    if response.choices is None:
        return []
    else:
        return [completion_choice(choice, tools) for choice in response.choices]
def choice_stop_reason(choice: MistralChatCompletionChoice) -> StopReason:
    match choice.finish_reason:
        case "stop":
            return "stop"
        case "length":
            return "max_tokens"
        case "model_length" | "tool_calls":
            return choice.finish_reason
        case _:
            return "unknown"

================
File: src/inspect_ai/model/_providers/mockllm.py
================
from typing import Any, Generator, Iterable, Iterator
from inspect_ai.tool import ToolChoice, ToolInfo
from .._chat_message import ChatMessage
from .._generate_config import GenerateConfig
from .._model import ModelAPI
from .._model_output import (
    ModelOutput,
)
class MockLLM(ModelAPI):
    """A mock implementation of the ModelAPI class for testing purposes.
    Always returns default_output, unless you pass in a model_args
    key "custom_outputs" with a value of an Iterable[ModelOutput]
    """
    default_output = "Default output from mockllm/model"
    outputs: Iterator[ModelOutput]
    def __init__(
        self,
        model_name: str,
        base_url: str | None = None,
        api_key: str | None = None,
        config: GenerateConfig = GenerateConfig(),
        custom_outputs: Iterable[ModelOutput]
        | Generator[ModelOutput, None, None]
        | None = None,
        **model_args: dict[str, Any],
    ) -> None:
        super().__init__(model_name, base_url, api_key, [], config)
        self.model_args = model_args
        if custom_outputs is not None:
            # We cannot rely on the user of this model giving custom_outputs the correct type since they do not call this constructor
            # Hence this type check and the one in generate.
            if not isinstance(custom_outputs, Iterable | Generator):
                raise ValueError(
                    f"model_args['custom_outputs'] must be an Iterable or a Generator, got {custom_outputs}"
                )
            self.outputs = iter(custom_outputs)
        else:
            self.outputs = iter(
                (
                    ModelOutput.from_content(
                        model="mockllm", content=self.default_output
                    )
                    for _ in iter(int, 1)  # produce an infinite iterator
                )
            )
    async def generate(
        self,
        input: list[ChatMessage],
        tools: list[ToolInfo],
        tool_choice: ToolChoice,
        config: GenerateConfig,
    ) -> ModelOutput:
        try:
            output = next(self.outputs)
        except StopIteration:
            raise ValueError("custom_outputs ran out of values")
        if not isinstance(output, ModelOutput):
            raise ValueError(
                f"output must be an instance of ModelOutput; got {type(output)}; content: {repr(output)}"
            )
        return output

================
File: src/inspect_ai/model/_providers/none.py
================
from inspect_ai._util.error import PrerequisiteError
from inspect_ai.tool import ToolChoice, ToolInfo
from .._chat_message import ChatMessage
from .._generate_config import GenerateConfig
from .._model import ModelAPI
from .._model_output import ModelOutput
class NoModel(ModelAPI):
    """A sentinel model type indicating there is no model specified."""
    def __init__(
        self,
        model_name: str = "none",
        base_url: str | None = None,
        api_key: str | None = None,
        config: GenerateConfig = GenerateConfig(),
    ) -> None:
        super().__init__(model_name, base_url, api_key, [], config)
    async def generate(
        self,
        input: list[ChatMessage],
        tools: list[ToolInfo],
        tool_choice: ToolChoice,
        config: GenerateConfig,
    ) -> ModelOutput:
        raise PrerequisiteError(
            "No model specified (and no INSPECT_EVAL_MODEL defined)"
        )

================
File: src/inspect_ai/model/_providers/ollama.py
================
from inspect_ai.model._providers.util import model_base_url
from .._generate_config import GenerateConfig
from .openai import OpenAIAPI
class OllamaAPI(OpenAIAPI):
    def __init__(
        self,
        model_name: str,
        base_url: str | None = None,
        api_key: str | None = None,
        config: GenerateConfig = GenerateConfig(),
    ) -> None:
        base_url = model_base_url(base_url, "OLLAMA_BASE_URL")
        base_url = base_url if base_url else "http://localhost:11434/v1"
        if not api_key:
            api_key = "ollama"
        super().__init__(
            model_name=model_name, base_url=base_url, api_key=api_key, config=config
        )

================
File: src/inspect_ai/model/_providers/openai_o1.py
================
import json
import re
import textwrap
from logging import getLogger
from typing import Any
from openai import AsyncOpenAI, BadRequestError
from openai.types.chat import (
    ChatCompletion,
    ChatCompletionAssistantMessageParam,
    ChatCompletionMessageParam,
    ChatCompletionUserMessageParam,
)
from shortuuid import uuid
from typing_extensions import override
from inspect_ai.model import (
    ChatCompletionChoice,
    ChatMessage,
    ChatMessageAssistant,
    ChatMessageTool,
    ChatMessageUser,
    ModelOutput,
)
from inspect_ai.tool import ToolCall, ToolInfo
from .._call_tools import parse_tool_call, tool_parse_error_message
from .._model_call import ModelCall
from .._model_output import ModelUsage, StopReason, as_stop_reason
from .._providers.util import ChatAPIHandler, ChatAPIMessage, chat_api_input
logger = getLogger(__name__)
async def generate_o1(
    client: AsyncOpenAI,
    model: str,
    input: list[ChatMessage],
    tools: list[ToolInfo],
    **params: Any,
) -> ModelOutput | tuple[ModelOutput | Exception, ModelCall]:
    # create chatapi handler
    handler = O1PreviewChatAPIHandler()
    # call model
    request = dict(
        model=model,
        messages=chat_messages(input, tools, handler),
        **params,
    )
    response: dict[str, Any] = {}
    def model_call() -> ModelCall:
        return ModelCall.create(
            request=request,
            response=response,
        )
    try:
        completion: ChatCompletion = await client.chat.completions.create(**request)
        response = completion.model_dump()
    except BadRequestError as ex:
        return handle_bad_request(model, ex), model_call()
    # return model output
    return ModelOutput(
        model=completion.model,
        choices=chat_choices_from_response(completion, tools, handler),
        usage=ModelUsage(
            input_tokens=completion.usage.prompt_tokens,
            output_tokens=completion.usage.completion_tokens,
            input_tokens_cache_read=(
                completion.usage.prompt_tokens_details.cached_tokens
                if completion.usage.prompt_tokens_details is not None
                else None  # openai only have cache read stats/pricing.
            ),
            reasoning_tokens=(
                completion.usage.completion_tokens_details.reasoning_tokens
                if completion.usage.completion_tokens_details is not None
                else None
            ),
            total_tokens=completion.usage.total_tokens,
        )
        if completion.usage
        else None,
    ), model_call()
def handle_bad_request(model: str, ex: BadRequestError) -> ModelOutput | Exception:
    if ex.code == "context_length_exceeded":
        stop_reason: StopReason | None = "model_length"
    elif ex.code == "invalid_prompt":
        stop_reason = "content_filter"
    else:
        stop_reason = None
    if stop_reason:
        return ModelOutput.from_content(
            model=model, content=str(ex), stop_reason=stop_reason
        )
    else:
        return ex
def chat_messages(
    input: list[ChatMessage], tools: list[ToolInfo], handler: ChatAPIHandler
) -> list[ChatCompletionMessageParam]:
    # o1 does not allow system messages so convert system -> user
    messages: list[ChatMessage] = [
        ChatMessageUser(id=message.id, content=message.content)
        if message.role == "system"
        else message
        for message in input
    ]
    # prepare input for REST style chat API (including presenting tools to the
    # model in the fashion designated by the ChatAPIHandler)
    chat_messages = chat_api_input(messages, tools, handler)
    # convert these REST style chat messages to OpenAI message dicts
    return [chat_message(message) for message in chat_messages]
def chat_message(message: ChatAPIMessage) -> ChatCompletionMessageParam:
    if message["role"] == "user":
        return ChatCompletionUserMessageParam(role="user", content=message["content"])
    elif message["role"] == "assistant":
        return ChatCompletionAssistantMessageParam(
            role="assistant", content=message["content"]
        )
    # the handler should have already ensured that there are no system or tool messages
    else:
        raise ValueError(f"Unexpected message role: {message.get('role')}")
def chat_choices_from_response(
    response: ChatCompletion, tools: list[ToolInfo], handler: ChatAPIHandler
) -> list[ChatCompletionChoice]:
    choices = list(response.choices)
    choices.sort(key=lambda c: c.index)
    return [
        # the assistant message might include a tool call so we call the
        # ChatAPIHandler to parse it and sort this out
        ChatCompletionChoice(
            message=handler.parse_assistant_response(
                choice.message.content or "", tools
            ),
            stop_reason=as_stop_reason(choice.finish_reason),
        )
        for choice in choices
    ]
TOOL_CALL = "tool_call"
class O1PreviewChatAPIHandler(ChatAPIHandler):
    @override
    def input_with_tools(
        self, input: list[ChatMessage], tools: list[ToolInfo]
    ) -> list[ChatMessage]:
        """Prepare model input with tool definitions.
        In this implementation we borrow the technique used by Ollama to implement OpenAI
        compatible tool calling for Llama3.1 models. Tool definitions are prepended as a
        user message to the `input` messages.
        """
        # JSON schema for available tools
        available_tools = "\n\n".join(
            [tool.model_dump_json(exclude_none=True, indent=2) for tool in tools]
        )
        # tool prompt
        tool_prompt = textwrap.dedent(
            f"""
            You are a knowledgable assistant. You can answer questions and perform tasks. You are provided with function signatures within <tools></tools> XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions. For each function call return a json object with function name and arguments within <{TOOL_CALL}></{TOOL_CALL}> XML tags as follows:
            <{TOOL_CALL}>
            {{"name": <function-name>,"arguments": <args-dict>}}
            </{TOOL_CALL}>
            Here are the available tools defined in JSON Schema:
            <tools>
            {available_tools}
            </tools>
            Reminder:
            - Function calls MUST follow the specified format, start with <{TOOL_CALL}> and end with </{TOOL_CALL}>.
            - Please call only one function at a time.
            - It's fine to include some reasoning about which function to call and why.
            - Please ensure that </{TOOL_CALL}> is the last content in the message (there should be no text after it).
            - Please be absolutely sure that the function name you have specified matches one of the functions described in <tools>.
            - All function parameters MUST be specified.
            - If there is no function call available, answer the question like normal with your current knowledge and do not tell the user about function calls
            """
        )
        # return the tool prompt prepended to the input messages
        return [ChatMessageUser(content=tool_prompt)] + input
    @override
    def parse_assistant_response(
        self, response: str, tools: list[ToolInfo]
    ) -> ChatMessageAssistant:
        """Parse content and tool calls from a model response.
        This method has an interdependency with `input_with_tools()` (as that is the
        prompt that asks the model to use the <tool_call>...</tool_call> syntax)
        """
        # extract tool calls
        tool_call_regex = rf"<{TOOL_CALL}>((?:.|\n)*?)</{TOOL_CALL}>"
        tool_calls_content: list[str] = re.findall(tool_call_regex, response)
        # if there are tool calls proceed with parsing
        if len(tool_calls_content) > 0:
            # parse each tool call (if there are parsing error that occur
            # this will be reported in the `parse_error` field of the ToolCall
            # and ultimately reported back to the model)
            tool_calls = [
                parse_tool_call_content(content, tools)
                for content in tool_calls_content
            ]
            # find other content that exists outside tool calls
            tool_call_content_regex = rf"<{TOOL_CALL}>(?:.|\n)*?</{TOOL_CALL}>"
            other_content = re.split(tool_call_content_regex, response, flags=re.DOTALL)
            other_content = [
                str(content).strip()
                for content in other_content
                if str(content).strip()
            ]
            content = "\n\n".join(other_content)
            # return the message
            return ChatMessageAssistant(
                content=content, tool_calls=tool_calls, source="generate"
            )
        # otherwise this is just an ordinary assistant message
        else:
            return ChatMessageAssistant(content=response, source="generate")
    @override
    def assistant_message(self, message: ChatMessageAssistant) -> ChatAPIMessage:
        """Construct a chat REST API message from an assistant message.
        This method is called by `chat_api_input()` to convert assistant messages to
        something the model will recognize. Therefore, iff the message has tool calls,
        we need to reconstruct the native assistant tool calling syntax (as such this
        method also has an interdependency with `input_with_tools()`).
        """
        # for tool calls, join any message text with the calls in <tool_call> form
        if message.tool_calls:
            content = "\n\n".join(
                [message.text]
                + [
                    f'<{TOOL_CALL}>{{"name": "{tool.function}", "arguments": {json.dumps(tool.arguments)} }}</{TOOL_CALL}>'
                    for tool in message.tool_calls
                ]
            )
        # normal message, just get the text
        else:
            content = message.text
        # return assistant message
        return {"role": "assistant", "content": content}
    @override
    def tool_message(self, message: ChatMessageTool) -> ChatAPIMessage:
        """Construct a chat REST API message from a tool message.
        o1 models do not support role="tool", so we need to present
        these as user messages that report the results of calls.
        """
        # might be an error in which case we prepend 'Error'
        results = f"Error: {message.error.message}" if message.error else message.text
        # try to clearly spell out that this 'user' message is the response to a function call
        content = f"The '{message.function}' function was called. The results are:\n\n{results}"
        # return user message
        return {"role": "user", "content": content}
def parse_tool_call_content(content: str, tools: list[ToolInfo]) -> ToolCall:
    """Attempt to parse content from inside <tool_call> tags.
    Content inside a <tool_call> should be a JSON dictionary with `name` and
    `arguments` (which in turn should be a `dict[str,Any]` but in some cases
    we've seen models pass `str`). This function attempts to extract this from
    the passed tcontentext. A `ToolCall` is returned for all cases (if the
    parsing fails then it will have a `parse_error`, which will be subsequently
    reported to the model.
    """
    try:
        # parse raw JSON
        tool_call_data = json.loads(content)
        # if its not a dict then report error
        if not isinstance(tool_call_data, dict):
            raise ValueError("The provided arguments are not a JSON dictionary.")
        # see if we can get the fields (if not report error)
        name = tool_call_data.get("name", None)
        arguments = tool_call_data.get("arguments", None)
        if (not name) or (arguments is None):
            raise ValueError(
                "Required 'name' and/or 'arguments' not provided in JSON dictionary."
            )
        # now perform the parse (we need to call thi function because it includes
        # the special handling to for mapping arguments that are a plain `str`
        # to the first parameter of the function)
        unique_id = f"{name}_{uuid()}"
        return parse_tool_call(unique_id, name, json.dumps(arguments), tools)
    except Exception as ex:
        # buld error message
        parse_error = tool_parse_error_message(content, ex)
        # log it to 'info'
        logger.info(parse_error)
        # notify model
        return ToolCall(
            id="unknown",
            function="unknown",
            arguments={},
            type="function",
            parse_error=parse_error,
        )

================
File: src/inspect_ai/model/_providers/openai.py
================
import os
import socket
from logging import getLogger
from typing import Any
import httpx
from openai import (
    DEFAULT_CONNECTION_LIMITS,
    DEFAULT_TIMEOUT,
    APIStatusError,
    APITimeoutError,
    AsyncAzureOpenAI,
    AsyncOpenAI,
    BadRequestError,
    RateLimitError,
)
from openai._types import NOT_GIVEN
from openai.types.chat import ChatCompletion
from typing_extensions import override
from inspect_ai._util.error import PrerequisiteError
from inspect_ai._util.http import is_retryable_http_status
from inspect_ai._util.logger import warn_once
from inspect_ai.model._openai import chat_choices_from_openai
from inspect_ai.model._providers.util.hooks import HttpxHooks
from inspect_ai.tool import ToolChoice, ToolInfo
from .._chat_message import ChatMessage
from .._generate_config import GenerateConfig
from .._image import image_url_filter
from .._model import ModelAPI
from .._model_call import ModelCall
from .._model_output import (
    ChatCompletionChoice,
    ModelOutput,
    ModelUsage,
    StopReason,
)
from .._openai import (
    is_gpt,
    is_o1_mini,
    is_o1_preview,
    is_o_series,
    openai_chat_messages,
    openai_chat_tool_choice,
    openai_chat_tools,
)
from .openai_o1 import generate_o1
from .util import (
    environment_prerequisite_error,
    model_base_url,
)
logger = getLogger(__name__)
OPENAI_API_KEY = "OPENAI_API_KEY"
AZURE_OPENAI_API_KEY = "AZURE_OPENAI_API_KEY"
AZUREAI_OPENAI_API_KEY = "AZUREAI_OPENAI_API_KEY"
class OpenAIAPI(ModelAPI):
    def __init__(
        self,
        model_name: str,
        base_url: str | None = None,
        api_key: str | None = None,
        config: GenerateConfig = GenerateConfig(),
        **model_args: Any,
    ) -> None:
        # call super
        super().__init__(
            model_name=model_name,
            base_url=base_url,
            api_key=api_key,
            api_key_vars=[OPENAI_API_KEY, AZURE_OPENAI_API_KEY, AZUREAI_OPENAI_API_KEY],
            config=config,
        )
        # extract any service prefix from model name
        parts = model_name.split("/")
        if len(parts) > 1:
            self.service: str | None = parts[0]
            model_name = "/".join(parts[1:])
        else:
            self.service = None
        # resolve api_key
        if not self.api_key:
            self.api_key = os.environ.get(
                AZUREAI_OPENAI_API_KEY, os.environ.get(AZURE_OPENAI_API_KEY, None)
            )
            # backward compatibility for when env vars determined service
            if self.api_key and (os.environ.get(OPENAI_API_KEY, None) is None):
                self.service = "azure"
            else:
                self.api_key = os.environ.get(OPENAI_API_KEY, None)
                if not self.api_key:
                    raise environment_prerequisite_error(
                        "OpenAI",
                        [
                            OPENAI_API_KEY,
                            AZUREAI_OPENAI_API_KEY,
                        ],
                    )
        # create async http client
        http_client = OpenAIAsyncHttpxClient()
        # azure client
        if self.is_azure():
            # resolve base_url
            base_url = model_base_url(
                base_url,
                [
                    "AZUREAI_OPENAI_BASE_URL",
                    "AZURE_OPENAI_BASE_URL",
                    "AZURE_OPENAI_ENDPOINT",
                ],
            )
            if not base_url:
                raise PrerequisiteError(
                    "ERROR: You must provide a base URL when using OpenAI on Azure. Use the AZUREAI_OPENAI_BASE_URL "
                    + "environment variable or the --model-base-url CLI flag to set the base URL."
                )
            self.client: AsyncAzureOpenAI | AsyncOpenAI = AsyncAzureOpenAI(
                api_key=self.api_key,
                azure_endpoint=base_url,
                azure_deployment=model_name,
                http_client=http_client,
                **model_args,
            )
        else:
            self.client = AsyncOpenAI(
                api_key=self.api_key,
                base_url=model_base_url(base_url, "OPENAI_BASE_URL"),
                http_client=http_client,
                **model_args,
            )
        # create time tracker
        self._http_hooks = HttpxHooks(self.client._client)
    def is_azure(self) -> bool:
        return self.service == "azure"
    def is_o_series(self) -> bool:
        return is_o_series(self.model_name)
    def is_o1_mini(self) -> bool:
        return is_o1_mini(self.model_name)
    def is_o1_preview(self) -> bool:
        return is_o1_preview(self.model_name)
    def is_gpt(self) -> bool:
        return is_gpt(self.model_name)
    @override
    async def close(self) -> None:
        await self.client.close()
    async def generate(
        self,
        input: list[ChatMessage],
        tools: list[ToolInfo],
        tool_choice: ToolChoice,
        config: GenerateConfig,
    ) -> ModelOutput | tuple[ModelOutput | Exception, ModelCall]:
        # short-circuit to call o1- models that are text only
        if self.is_o1_preview() or self.is_o1_mini():
            return await generate_o1(
                client=self.client,
                input=input,
                tools=tools,
                **self.completion_params(config, False),
            )
        # allocate request_id (so we can see it from ModelCall)
        request_id = self._http_hooks.start_request()
        # setup request and response for ModelCall
        request: dict[str, Any] = {}
        response: dict[str, Any] = {}
        def model_call() -> ModelCall:
            return ModelCall.create(
                request=request,
                response=response,
                filter=image_url_filter,
                time=self._http_hooks.end_request(request_id),
            )
        # unlike text models, vision models require a max_tokens (and set it to a very low
        # default, see https://community.openai.com/t/gpt-4-vision-preview-finish-details/475911/10)
        OPENAI_IMAGE_DEFAULT_TOKENS = 4096
        if "vision" in self.model_name:
            if isinstance(config.max_tokens, int):
                config.max_tokens = max(config.max_tokens, OPENAI_IMAGE_DEFAULT_TOKENS)
            else:
                config.max_tokens = OPENAI_IMAGE_DEFAULT_TOKENS
        # prepare request (we do this so we can log the ModelCall)
        request = dict(
            messages=await openai_chat_messages(input, self.model_name),
            tools=openai_chat_tools(tools) if len(tools) > 0 else NOT_GIVEN,
            tool_choice=openai_chat_tool_choice(tool_choice)
            if len(tools) > 0
            else NOT_GIVEN,
            extra_headers={HttpxHooks.REQUEST_ID_HEADER: request_id},
            **self.completion_params(config, len(tools) > 0),
        )
        try:
            # generate completion
            completion: ChatCompletion = await self.client.chat.completions.create(
                **request
            )
            # save response for model_call
            response = completion.model_dump()
            # parse out choices
            choices = self._chat_choices_from_response(completion, tools)
            # return output and call
            return ModelOutput(
                model=completion.model,
                choices=choices,
                usage=(
                    ModelUsage(
                        input_tokens=completion.usage.prompt_tokens,
                        output_tokens=completion.usage.completion_tokens,
                        input_tokens_cache_read=(
                            completion.usage.prompt_tokens_details.cached_tokens
                            if completion.usage.prompt_tokens_details is not None
                            else None  # openai only have cache read stats/pricing.
                        ),
                        reasoning_tokens=(
                            completion.usage.completion_tokens_details.reasoning_tokens
                            if completion.usage.completion_tokens_details is not None
                            else None
                        ),
                        total_tokens=completion.usage.total_tokens,
                    )
                    if completion.usage
                    else None
                ),
            ), model_call()
        except BadRequestError as e:
            return self.handle_bad_request(e), model_call()
    def _chat_choices_from_response(
        self, response: ChatCompletion, tools: list[ToolInfo]
    ) -> list[ChatCompletionChoice]:
        # adding this as a method so we can override from other classes (e.g together)
        return chat_choices_from_openai(response, tools)
    @override
    def should_retry(self, ex: Exception) -> bool:
        if isinstance(ex, RateLimitError):
            # Do not retry on these rate limit errors
            # The quota exceeded one is related to monthly account quotas.
            if "You exceeded your current quota" in ex.message:
                warn_once(logger, f"OpenAI quota exceeded, not retrying: {ex.message}")
                return False
            else:
                return True
        elif isinstance(ex, APIStatusError):
            return is_retryable_http_status(ex.status_code)
        elif isinstance(ex, APITimeoutError):
            return True
        else:
            return False
    @override
    def connection_key(self) -> str:
        """Scope for enforcing max_connections (could also use endpoint)."""
        return str(self.api_key)
    def completion_params(self, config: GenerateConfig, tools: bool) -> dict[str, Any]:
        params: dict[str, Any] = dict(
            model=self.model_name,
        )
        if config.max_tokens is not None:
            if self.is_o_series():
                params["max_completion_tokens"] = config.max_tokens
            else:
                params["max_tokens"] = config.max_tokens
        if config.frequency_penalty is not None:
            params["frequency_penalty"] = config.frequency_penalty
        if config.stop_seqs is not None:
            params["stop"] = config.stop_seqs
        if config.presence_penalty is not None:
            params["presence_penalty"] = config.presence_penalty
        if config.logit_bias is not None:
            params["logit_bias"] = config.logit_bias
        if config.seed is not None:
            params["seed"] = config.seed
        if config.temperature is not None:
            if self.is_o_series():
                warn_once(
                    logger,
                    "o series models do not support the 'temperature' parameter (temperature is always 1).",
                )
            else:
                params["temperature"] = config.temperature
        # TogetherAPI requires temperature w/ num_choices
        elif config.num_choices is not None:
            params["temperature"] = 1
        if config.top_p is not None:
            params["top_p"] = config.top_p
        if config.num_choices is not None:
            params["n"] = config.num_choices
        if config.logprobs is not None:
            params["logprobs"] = config.logprobs
        if config.top_logprobs is not None:
            params["top_logprobs"] = config.top_logprobs
        if tools and config.parallel_tool_calls is not None and not self.is_o_series():
            params["parallel_tool_calls"] = config.parallel_tool_calls
        if (
            config.reasoning_effort is not None
            and not self.is_gpt()
            and not self.is_o1_mini()
        ):
            params["reasoning_effort"] = config.reasoning_effort
        if config.response_schema is not None:
            params["response_format"] = dict(
                type="json_schema",
                json_schema=dict(
                    name=config.response_schema.name,
                    schema=config.response_schema.json_schema.model_dump(
                        exclude_none=True
                    ),
                    description=config.response_schema.description,
                    strict=config.response_schema.strict,
                ),
            )
        return params
    # convert some well known bad request errors into ModelOutput
    def handle_bad_request(self, e: BadRequestError) -> ModelOutput | Exception:
        # extract message
        if isinstance(e.body, dict) and "message" in e.body.keys():
            content = str(e.body.get("message"))
        else:
            content = e.message
        # narrow stop_reason
        stop_reason: StopReason | None = None
        if e.code == "context_length_exceeded":
            stop_reason = "model_length"
        elif (
            e.code == "invalid_prompt"  # seems to happen for o1/o3
            or e.code == "content_policy_violation"  # seems to happen for vision
            or e.code == "content_filter"  # seems to happen on azure
        ):
            stop_reason = "content_filter"
        if stop_reason:
            return ModelOutput.from_content(
                model=self.model_name, content=content, stop_reason=stop_reason
            )
        else:
            return e
class OpenAIAsyncHttpxClient(httpx.AsyncClient):
    """Custom async client that deals better with long running Async requests.
    Based on Anthropic DefaultAsyncHttpClient implementation that they
    released along with Claude 3.7 as well as the OpenAI DefaultAsyncHttpxClient
    """
    def __init__(self, **kwargs: Any) -> None:
        # This is based on the openai DefaultAsyncHttpxClient:
        # https://github.com/openai/openai-python/commit/347363ed67a6a1611346427bb9ebe4becce53f7e
        kwargs.setdefault("timeout", DEFAULT_TIMEOUT)
        kwargs.setdefault("limits", DEFAULT_CONNECTION_LIMITS)
        kwargs.setdefault("follow_redirects", True)
        # This is based on the anthrpopic changes for claude 3.7:
        # https://github.com/anthropics/anthropic-sdk-python/commit/c5387e69e799f14e44006ea4e54fdf32f2f74393#diff-3acba71f89118b06b03f2ba9f782c49ceed5bb9f68d62727d929f1841b61d12bR1387-R1403
        # set socket options to deal with long running reasoning requests
        socket_options = [
            (socket.SOL_SOCKET, socket.SO_KEEPALIVE, True),
            (socket.IPPROTO_TCP, socket.TCP_KEEPINTVL, 60),
            (socket.IPPROTO_TCP, socket.TCP_KEEPCNT, 5),
        ]
        TCP_KEEPIDLE = getattr(socket, "TCP_KEEPIDLE", None)
        if TCP_KEEPIDLE is not None:
            socket_options.append((socket.IPPROTO_TCP, TCP_KEEPIDLE, 60))
        kwargs["transport"] = httpx.AsyncHTTPTransport(
            limits=DEFAULT_CONNECTION_LIMITS,
            socket_options=socket_options,
        )
        super().__init__(**kwargs)

================
File: src/inspect_ai/model/_providers/openrouter.py
================
import os
from typing import Any
from typing_extensions import override
from inspect_ai._util.error import PrerequisiteError
from inspect_ai.model._providers.util import model_base_url
from inspect_ai.model._providers.util.util import environment_prerequisite_error
from .._generate_config import GenerateConfig
from .openai import OpenAIAPI
OPENROUTER_API_KEY = "OPENROUTER_API_KEY"
class OpenRouterAPI(OpenAIAPI):
    def __init__(
        self,
        model_name: str,
        base_url: str | None = None,
        api_key: str | None = None,
        config: GenerateConfig = GenerateConfig(),
        **model_args: Any,
    ) -> None:
        # api_key
        if not api_key:
            api_key = os.environ.get(OPENROUTER_API_KEY, None)
            if not api_key:
                raise environment_prerequisite_error("OpenRouter", OPENROUTER_API_KEY)
        # base_url
        base_url = model_base_url(base_url, "OPENROUTER_BASE_URL")
        base_url = base_url if base_url else "https://openrouter.ai/api/v1"
        # collect known model args that we forward to generate
        def collect_model_arg(name: str) -> Any | None:
            nonlocal model_args
            value = model_args.get(name, None)
            if value is not None:
                model_args.pop(name)
            return value
        # models arg
        self.models = collect_model_arg("models")
        if self.models is not None:
            if not isinstance(self.models, list):
                raise PrerequisiteError("models must be a list of strings")
        # providers arg
        self.provider = collect_model_arg("provider")
        if self.provider is not None:
            if not isinstance(self.provider, dict):
                raise PrerequisiteError("provider must be a dict")
        # transforms arg
        self.transforms = collect_model_arg("transforms")
        if self.transforms is not None:
            if not isinstance(self.transforms, list):
                raise PrerequisiteError("transforms must be a list of strings")
        # call super
        super().__init__(
            model_name=model_name,
            base_url=base_url,
            api_key=api_key,
            config=config,
            **model_args,
        )
    @override
    def completion_params(self, config: GenerateConfig, tools: bool) -> dict[str, Any]:
        # default params
        params = super().completion_params(config, tools)
        # pass args if specifed
        EXTRA_BODY = "extra_body"
        if self.models or self.provider or self.transforms:
            params[EXTRA_BODY] = params.get(EXTRA_BODY, {})
            if self.models:
                params[EXTRA_BODY]["models"] = self.models
            if self.provider:
                params[EXTRA_BODY]["provider"] = self.provider
            if self.transforms:
                params[EXTRA_BODY]["transforms"] = self.transforms
        return params

================
File: src/inspect_ai/model/_providers/providers.py
================
import os
from inspect_ai._util.error import pip_dependency_error
from inspect_ai._util.version import verify_required_version
from .._model import ModelAPI
from .._registry import modelapi
# Defer importing model api classes until they are actually used
# (this allows the package to load without the optional deps)
# Note that some api providers (e.g. Cloudflare, AzureAI) don't
# strictly require this treatment but we do it anyway for uniformity,
@modelapi(name="groq")
def groq() -> type[ModelAPI]:
    FEATURE = "Groq API"
    PACKAGE = "groq"
    MIN_VERSION = "0.16.0"
    # verify we have the package
    try:
        import groq  # noqa: F401
    except ImportError:
        raise pip_dependency_error(FEATURE, [PACKAGE])
    # verify version
    verify_required_version(FEATURE, PACKAGE, MIN_VERSION)
    # in the clear
    from .groq import GroqAPI
    return GroqAPI
@modelapi(name="openai")
def openai() -> type[ModelAPI]:
    # validate
    validate_openai_client("OpenAI API")
    # in the clear
    from .openai import OpenAIAPI
    return OpenAIAPI
@modelapi(name="anthropic")
def anthropic() -> type[ModelAPI]:
    FEATURE = "Anthropic API"
    PACKAGE = "anthropic"
    MIN_VERSION = "0.47.1"
    # verify we have the package
    try:
        import anthropic  # noqa: F401
    except ImportError:
        raise pip_dependency_error(FEATURE, [PACKAGE])
    # verify version
    verify_required_version(FEATURE, PACKAGE, MIN_VERSION)
    # in the clear
    from .anthropic import AnthropicAPI
    return AnthropicAPI
@modelapi(name="vertex")
def vertex() -> type[ModelAPI]:
    FEATURE = "Google Vertex API"
    PACKAGE = "google-cloud-aiplatform"
    MIN_VERSION = "1.73.0"
    # workaround log spam
    # https://github.com/ray-project/ray/issues/24917
    os.environ["GRPC_ENABLE_FORK_SUPPORT"] = "0"
    # verify we have the package
    try:
        import vertexai  # type: ignore  # noqa: F401
    except ImportError:
        raise pip_dependency_error(FEATURE, [PACKAGE])
    # verify version
    verify_required_version(FEATURE, PACKAGE, MIN_VERSION)
    # in the clear
    from .vertex import VertexAPI
    return VertexAPI
@modelapi(name="google")
def google() -> type[ModelAPI]:
    FEATURE = "Google API"
    PACKAGE = "google-genai"
    MIN_VERSION = "1.2.0"
    # workaround log spam
    # https://github.com/ray-project/ray/issues/24917
    os.environ["GRPC_ENABLE_FORK_SUPPORT"] = "0"
    # verify we have the package
    try:
        import google.genai  # type: ignore  # noqa: F401
    except ImportError:
        raise pip_dependency_error(FEATURE, [PACKAGE])
    # verify version
    verify_required_version(FEATURE, PACKAGE, MIN_VERSION)
    # in the clear
    from .google import GoogleGenAIAPI
    return GoogleGenAIAPI
@modelapi(name="hf")
def hf() -> type[ModelAPI]:
    try:
        from .hf import HuggingFaceAPI
    except ImportError:
        raise pip_dependency_error(
            "Hugging Face Models", ["torch", "transformers", "accelerate"]
        )
    return HuggingFaceAPI
@modelapi(name="vllm")
def vllm() -> type[ModelAPI]:
    try:
        from .vllm import VLLMAPI
    except ImportError:
        raise pip_dependency_error("vLLM Models", ["vllm"])
    return VLLMAPI
@modelapi(name="cf")
def cf() -> type[ModelAPI]:
    from .cloudflare import CloudFlareAPI
    return CloudFlareAPI
@modelapi(name="mistral")
def mistral() -> type[ModelAPI]:
    FEATURE = "Mistral API"
    PACKAGE = "mistralai"
    MIN_VERSION = "1.5.1"
    # verify we have the package
    try:
        import mistralai  # noqa: F401
    except ImportError:
        raise pip_dependency_error(FEATURE, [PACKAGE])
    # verify version
    verify_required_version(FEATURE, PACKAGE, MIN_VERSION)
    # in the clear
    from .mistral import MistralAPI
    return MistralAPI
@modelapi(name="grok")
def grok() -> type[ModelAPI]:
    # validate
    validate_openai_client("Grok API")
    # in the clear
    from .grok import GrokAPI
    return GrokAPI
@modelapi(name="together")
def together() -> type[ModelAPI]:
    # validate
    validate_openai_client("TogetherAI API")
    # in the clear
    from .together import TogetherAIAPI
    return TogetherAIAPI
@modelapi(name="ollama")
def ollama() -> type[ModelAPI]:
    # validate
    validate_openai_client("Ollama API")
    # in the clear
    from .ollama import OllamaAPI
    return OllamaAPI
@modelapi(name="openrouter")
def openrouter() -> type[ModelAPI]:
    # validate
    validate_openai_client("OpenRouter API")
    # in the clear
    from .openrouter import OpenRouterAPI
    return OpenRouterAPI
@modelapi(name="llama-cpp-python")
def llama_cpp_python() -> type[ModelAPI]:
    # validate
    validate_openai_client("llama-cpp-python API")
    # in the clear
    from .llama_cpp_python import LlamaCppPythonAPI
    return LlamaCppPythonAPI
@modelapi(name="azureai")
def azureai() -> type[ModelAPI]:
    FEATURE = "AzureAI API"
    PACKAGE = "azure-ai-inference"
    # verify we have the package
    try:
        import azure.ai.inference  # noqa: F401
    except ImportError:
        raise pip_dependency_error(FEATURE, [PACKAGE])
    from .azureai import AzureAIAPI
    return AzureAIAPI
@modelapi(name="bedrock")
def bedrock() -> type[ModelAPI]:
    from .bedrock import BedrockAPI
    return BedrockAPI
@modelapi(name="mockllm")
def mockllm() -> type[ModelAPI]:
    from .mockllm import MockLLM
    return MockLLM
@modelapi(name="none")
def none() -> type[ModelAPI]:
    from .none import NoModel
    return NoModel
@modelapi("goodfire")
def goodfire() -> type[ModelAPI]:
    """Get the Goodfire API provider."""
    FEATURE = "Goodfire API"
    PACKAGE = "goodfire"
    MIN_VERSION = "0.3.4"  # Support for newer Llama models and OpenAI compatibility
    # verify we have the package
    try:
        import goodfire  # noqa: F401
    except ImportError:
        raise pip_dependency_error(FEATURE, [PACKAGE])
    # verify version
    verify_required_version(FEATURE, PACKAGE, MIN_VERSION)
    # in the clear
    from .goodfire import GoodfireAPI
    return GoodfireAPI
def validate_openai_client(feature: str) -> None:
    FEATURE = feature
    PACKAGE = "openai"
    MIN_VERSION = "1.58.1"
    # verify we have the package
    try:
        import openai  # noqa: F401
    except ImportError:
        raise pip_dependency_error(FEATURE, [PACKAGE])
    # verify version
    verify_required_version(FEATURE, PACKAGE, MIN_VERSION)

================
File: src/inspect_ai/model/_providers/together.py
================
import os
from json import dumps
from typing import Any
import httpx
from openai import BadRequestError
from openai.types.chat import (
    ChatCompletion,
)
from typing_extensions import override
from inspect_ai._util.constants import DEFAULT_MAX_TOKENS
from inspect_ai.model._providers.util.chatapi import ChatAPIHandler
from inspect_ai.tool._tool_choice import ToolChoice
from inspect_ai.tool._tool_info import ToolInfo
from .._chat_message import ChatMessage, ChatMessageAssistant
from .._generate_config import GenerateConfig
from .._model import ModelAPI
from .._model_output import (
    ChatCompletionChoice,
    Logprob,
    Logprobs,
    ModelOutput,
    ModelUsage,
    StopReason,
    as_stop_reason,
)
from .._openai import chat_message_assistant_from_openai
from .openai import (
    OpenAIAPI,
)
from .util import (
    chat_api_input,
    chat_api_request,
    environment_prerequisite_error,
    model_base_url,
    should_retry_chat_api_error,
)
def chat_choices_from_response_together(
    response: ChatCompletion, tools: list[ToolInfo]
) -> list[ChatCompletionChoice]:
    choices = list(response.choices)
    choices.sort(key=lambda c: c.index)
    logprobs_dicts = [
        choice.logprobs.model_dump() if choice.logprobs is not None else None
        for choice in choices
    ]
    logprobs_models: list[Logprobs | None] = []
    for logprob_dict in logprobs_dicts:
        if logprob_dict is None:
            logprobs_models.append(logprob_dict)
            continue
        tokens = logprob_dict["tokens"]
        token_logprobs = logprob_dict["token_logprobs"]
        logprobs_sequence = []
        for token, logprob in zip(tokens, token_logprobs):
            logprobs_sequence.append(
                Logprob(
                    token=token,
                    logprob=logprob,
                    bytes=list(map(ord, token)),
                    top_logprobs=None,
                )
            )
        logprobs_models.append(Logprobs(content=logprobs_sequence))
    return [
        ChatCompletionChoice(
            message=chat_message_assistant_from_openai(choice.message, tools),
            stop_reason=as_stop_reason(choice.finish_reason),
            logprobs=logprobs,
        )
        for choice, logprobs in zip(choices, logprobs_models)
    ]
TOGETHER_API_KEY = "TOGETHER_API_KEY"
class TogetherAIAPI(OpenAIAPI):
    def __init__(
        self,
        model_name: str,
        base_url: str | None = None,
        api_key: str | None = None,
        config: GenerateConfig = GenerateConfig(),
    ) -> None:
        if not api_key:
            api_key = os.environ.get(TOGETHER_API_KEY, None)
            if not api_key:
                raise environment_prerequisite_error("TogetherAI", TOGETHER_API_KEY)
        base_url = model_base_url(base_url, "TOGETHER_BASE_URL")
        base_url = base_url if base_url else "https://api.together.xyz/v1"
        super().__init__(
            model_name=model_name, base_url=base_url, api_key=api_key, config=config
        )
    # Together uses a default of 512 so we bump it up
    @override
    def max_tokens(self) -> int | None:
        return DEFAULT_MAX_TOKENS
    @override
    def handle_bad_request(self, ex: BadRequestError) -> ModelOutput | Exception:
        response = ex.response.json()
        if "error" in response and "message" in response.get("error"):
            content = response.get("error").get("message")
        else:
            content = str(response)
        if "max_new_tokens" in ex.message:
            return ModelOutput.from_content(
                model=self.model_name, content=content, stop_reason="model_length"
            )
        else:
            return ex
    # Together has a slightly different logprobs structure to OpenAI, so we need to remap it.
    def _chat_choices_from_response(
        self, response: ChatCompletion, tools: list[ToolInfo]
    ) -> list[ChatCompletionChoice]:
        return chat_choices_from_response_together(response, tools)
# Implementation of REST client for Together (currently not used)
class TogetherRESTAPI(ModelAPI):
    def __init__(
        self,
        model_name: str,
        base_url: str | None = None,
        api_key: str | None = None,
        config: GenerateConfig = GenerateConfig(),
        **model_args: Any,
    ):
        if not api_key:
            api_key = os.environ.get(TOGETHER_API_KEY, None)
            if not api_key:
                raise RuntimeError(f"{TOGETHER_API_KEY} environment variable not set")
        base_url = model_base_url(base_url, "TOGETHER_BASE_URL")
        base_url = base_url if base_url else "https://api.together.xyz/v1"
        super().__init__(
            model_name=model_name,
            base_url=base_url,
            api_key=api_key,
            api_key_vars=[TOGETHER_API_KEY],
            config=config,
        )
        self.client = httpx.AsyncClient()
        self.model_args = model_args
    async def generate(
        self,
        input: list[ChatMessage],
        tools: list[ToolInfo],
        tool_choice: ToolChoice,
        config: GenerateConfig,
    ) -> ModelOutput:
        # chat url
        chat_url = f"{self.base_url}/chat/completions"
        # chat api input
        json: dict[str, Any] = dict(**self.model_args)
        json["model"] = self.model_name
        if config.max_tokens is not None:
            json["max_tokens"] = config.max_tokens
        if config.temperature is not None:
            json["temperature"] = config.temperature
        if config.top_p is not None:
            json["top_p"] = config.top_p
        if config.top_k is not None:
            json["top_k"] = config.top_k
        if config.num_choices is not None:
            json["n"] = config.num_choices
        if config.logprobs:
            json["logprobs"] = config.logprobs
        json["messages"] = chat_api_input(input, tools, self.chat_api_handler())
        # make the call
        response = await chat_api_request(
            self.client,
            model_name=self.model_name,
            url=f"{chat_url}",
            headers={"Authorization": f"Bearer {self.api_key}"},
            json=json,
        )
        if "error" in response:
            error = response.get("error")
            raise RuntimeError(f"Error calling TogetherAI model: {dumps(error)}")
        else:
            # model name used by back end
            model = response.get("model", self.model_name)
            # generated choices
            choices = together_choices(
                response.get("choices"), tools, self.chat_api_handler()
            )
            # model usage
            if "usage" in response:
                usage_response = response.get("usage")
                usage = ModelUsage(
                    input_tokens=usage_response.get("prompt_tokens", 0),
                    output_tokens=usage_response.get("completion_tokens", 0),
                    total_tokens=usage_response.get("total_tokens", 0),
                )
            else:
                usage = ModelUsage()
            return ModelOutput(model=model, choices=choices, usage=usage)
    @override
    def should_retry(self, ex: Exception) -> bool:
        return should_retry_chat_api_error(ex)
    # cloudflare enforces rate limits by model for each account
    @override
    def connection_key(self) -> str:
        return f"{self.api_key}"
    # Together uses a default of 512 so we bump it up
    @override
    def max_tokens(self) -> int:
        return DEFAULT_MAX_TOKENS
    def chat_api_handler(self) -> ChatAPIHandler:
        return ChatAPIHandler()
def together_choices(
    choices: list[dict[str, Any]], tools: list[ToolInfo], handler: ChatAPIHandler
) -> list[ChatCompletionChoice]:
    choices.sort(key=lambda c: c.get("index", 0))
    return [together_choice(choice, tools, handler) for choice in choices]
def together_choice(
    choice: dict[str, Any], tools: list[ToolInfo], handler: ChatAPIHandler
) -> ChatCompletionChoice:
    return ChatCompletionChoice(
        message=together_chat_message(choice.get("message", {}), tools, handler),
        stop_reason=together_stop_reason(choice.get("finish_reason", "")),
        logprobs=together_logprobs(choice),
    )
def together_chat_message(
    message: dict[str, str], tools: list[ToolInfo], handler: ChatAPIHandler
) -> ChatMessageAssistant:
    content: str = message.get("content", "")
    return handler.parse_assistant_response(content, tools)
def together_stop_reason(reason: str) -> StopReason:
    match reason:
        case "stop" | "eos":
            return "stop"
        case "length":
            return "max_tokens"
        case "tool_calls" | "max_tokens":
            return reason
        case _:
            return "unknown"
def together_logprobs(choice: dict[str, Any]) -> Logprobs | None:
    logprobs = choice.get("logprobs", None)
    if logprobs:
        logprobs_sequence = []
        for token, logprob in zip(
            logprobs.get("tokens", []), logprobs.get("token_logprobs", [])
        ):
            logprobs_sequence.append(
                Logprob(
                    token=token,
                    logprob=logprob,
                    bytes=list(map(ord, token)),
                    top_logprobs=None,
                )
            )
        tlp = Logprobs(content=logprobs_sequence)
        print(tlp.model_dump_json(indent=2))
        return tlp
    else:
        return None

================
File: src/inspect_ai/model/_providers/util/__init__.py
================
from ..._call_tools import parse_tool_call, tool_parse_error_message
from ..._model_output import as_stop_reason
from .chatapi import (
    ChatAPIHandler,
    ChatAPIMessage,
    chat_api_input,
    chat_api_request,
    should_retry_chat_api_error,
)
from .hf_handler import HFHandler
from .llama31 import Llama31Handler
from .util import (
    environment_prerequisite_error,
    model_base_url,
)
__all__ = [
    "environment_prerequisite_error",
    "as_stop_reason",
    "chat_api_request",
    "chat_api_input",
    "should_retry_chat_api_error",
    "model_base_url",
    "parse_tool_call",
    "tool_parse_error_message",
    "ChatAPIHandler",
    "ChatAPIMessage",
    "Llama31Handler",
    "HFHandler",
]

================
File: src/inspect_ai/model/_providers/util/chatapi.py
================
from logging import getLogger
from typing import Any, Literal
import httpx
from tenacity import (
    RetryError,
    retry,
    retry_if_exception,
    stop_after_attempt,
    wait_exponential_jitter,
)
from inspect_ai._util.http import is_retryable_http_status
from inspect_ai._util.httpx import httpx_should_retry, log_httpx_retry_attempt
from inspect_ai.model._chat_message import ChatMessageAssistant, ChatMessageTool
from inspect_ai.tool._tool_info import ToolInfo
from ..._chat_message import ChatMessage
logger = getLogger(__name__)
ChatAPIMessage = dict[Literal["role", "content"], str]
class ChatAPIHandler:
    def input_with_tools(
        self, input: list[ChatMessage], tools: list[ToolInfo]
    ) -> list[ChatMessage]:
        return input
    def parse_assistant_response(
        self, response: str, tools: list[ToolInfo]
    ) -> ChatMessageAssistant:
        return ChatMessageAssistant(content=response)
    def assistant_message(self, message: ChatMessageAssistant) -> ChatAPIMessage:
        return {"role": "assistant", "content": message.text}
    def tool_message(self, message: ChatMessageTool) -> ChatAPIMessage:
        return {
            "role": "tool",
            "content": f"Error: {message.error.message}"
            if message.error
            else message.text,
        }
def chat_api_input(
    input: list[ChatMessage],
    tools: list[ToolInfo],
    handler: ChatAPIHandler = ChatAPIHandler(),
) -> list[ChatAPIMessage]:
    # add tools to input
    if len(tools) > 0:
        input = handler.input_with_tools(input, tools)
    # resolve other messages
    return [chat_api_message(message, handler) for message in input]
def chat_api_message(message: ChatMessage, handler: ChatAPIHandler) -> ChatAPIMessage:
    if message.role == "assistant":
        return handler.assistant_message(message)
    elif message.role == "tool":
        return handler.tool_message(message)
    else:
        return dict(role=message.role, content=message.text)
async def chat_api_request(
    client: httpx.AsyncClient,
    model_name: str,
    url: str,
    headers: dict[str, Any],
    json: Any,
) -> Any:
    # define call w/ retry policy
    @retry(
        wait=wait_exponential_jitter(),
        stop=(stop_after_attempt(2)),
        retry=retry_if_exception(httpx_should_retry),
        before_sleep=log_httpx_retry_attempt(model_name),
    )
    async def call_api() -> Any:
        response = await client.post(url=url, headers=headers, json=json)
        response.raise_for_status()
        return response.json()
    # make the call
    return await call_api()
# When calling chat_api_request() we use tenacity as the retry wrapper, so
# checking for rate limit errors needs to punch through the RetryError and
# look at its `__cause__`. we've observed Cloudflare giving transient 500
# status as well as a ReadTimeout, so we count these as rate limit errors
def should_retry_chat_api_error(ex: BaseException) -> bool:
    return isinstance(ex, RetryError) and (
        (
            isinstance(ex.__cause__, httpx.HTTPStatusError)
            and is_retryable_http_status(ex.__cause__.response.status_code)
        )
        or isinstance(ex.__cause__, httpx.ReadTimeout)
    )

================
File: src/inspect_ai/model/_providers/util/hf_handler.py
================
import json
import re
from logging import getLogger
from shortuuid import uuid
from typing_extensions import override
from inspect_ai.tool._tool_call import ToolCall
from inspect_ai.tool._tool_info import ToolInfo
from ..._call_tools import parse_tool_call, tool_parse_error_message
from ..._chat_message import ChatMessageAssistant
from .chatapi import ChatAPIHandler
logger = getLogger(__name__)
# Hugging Face handler currently supports LLama, Mistral and Qwen models, but will
# work with any model that uses the same tool calling conventions
class HFHandler(ChatAPIHandler):
    def __init__(self, model_name: str) -> None:
        self.model_name = model_name
    @override
    def parse_assistant_response(
        self, response: str, tools: list[ToolInfo]
    ) -> ChatMessageAssistant:
        """Parse content and tool calls from a model response.
        This method has an interdependency with `input_with_tools()` (as that is the
        prompt that asks the model to use the <tool_call>...</tool_call> syntax)
        """
        # extract tool calls
        content, tool_calls_content = model_specific_tool_parse(
            response, self.model_name
        )
        # if there are tool calls proceed with parsing
        if len(tool_calls_content) > 0:
            # parse each tool call (if there are parsing error that occur
            # this will be reported in the `parse_error` field of the ToolCall
            # and ultimately reported back to the model)
            tool_calls = [
                parse_tool_call_content(content, tools)
                for content in tool_calls_content
            ]
            # return the message
            return ChatMessageAssistant(
                content=content,
                tool_calls=tool_calls,
                source="generate",
            )
        # otherwise this is just an ordinary assistant message
        else:
            return ChatMessageAssistant(
                content=filter_assistant_header(response), source="generate"
            )
def parse_tool_call_content(content: str, tools: list[ToolInfo]) -> ToolCall:
    """Attempt to parse content from inside <tool_call> tags.
    Content inside a <tool_call> should be a JSON dictionary with `name` and
    `arguments` (which in turn should be a `dict[str,Any]` but in some cases
    we've seen models pass `str`). This function attempts to extract this from
    the passed tcontentext. A `ToolCall` is returned for all cases (if the
    parsing fails then it will have a `parse_error`, which will be subsequently
    reported to the model.
    """
    try:
        # parse raw JSON
        tool_call_data = json.loads(content)
        if "parameters" in tool_call_data:
            tool_call_data["arguments"] = tool_call_data.pop("parameters")
        # if its not a dict then report error
        if not isinstance(tool_call_data, dict):
            raise ValueError("The provided arguments are not a JSON dictionary.")
        # see if we can get the fields (if not report error)
        name = tool_call_data.get("name", None)
        arguments = tool_call_data.get("arguments", None)
        if not name or not arguments:
            raise ValueError(
                "Required 'name' and 'arguments' not provided in JSON dictionary."
            )
        # now perform the parse (we need to call thi function because it includes
        # the special handling to for mapping arguments that are a plain `str`
        # to the first parameter of the function)
        unique_id = f"{name}_{uuid()}"
        return parse_tool_call(unique_id, name, json.dumps(arguments), tools)
    except Exception as ex:
        # buld error message
        parse_error = tool_parse_error_message(content, ex)
        # log it to 'info'
        logger.info(parse_error)
        # notify model
        return ToolCall(
            id="unknown",
            function="unknown",
            arguments={},
            type="function",
            parse_error=parse_error,
        )
def model_specific_tool_parse(response: str, model_name: str) -> tuple[str, list[str]]:
    model_name = model_name.lower()
    if "llama" in model_name:
        if "name" in response and ("parameters" in response or "arguments" in response):
            function_calls, content = json_extract_raw(response)
        else:
            content = response
            function_calls = []
    elif "mistral" in model_name:
        if "name" in response and "arguments" in response:
            content = ""
            function_calls = [json.dumps(tool) for tool in json.loads(response)]
        else:
            content = response
            function_calls = []
    elif "qwen" in model_name and "coder" in model_name:
        if "name" in response and "arguments" in response:
            function_calls, content = json_extract(response)
        else:
            content = response
            function_calls = []
    elif "qwen" in model_name and "instruct" in model_name:
        if "name" in response and "arguments" in response:
            function_calls, content = xml_extract(response, "tool_call")
        else:
            content = response
            function_calls = []
    else:
        try:
            function_calls, content = parse_unknown_tool_calls(response)
        except Exception:
            raise ValueError(
                f"Unsupported model: {model_name}. No tool parsing implemented. Check if any of the current parsings work with your tool calling conventions and add the model name to the correct elif block."
            )
    return content, function_calls
def json_extract(raw_string: str) -> tuple[list[str], str]:
    """Extract tools in form ```json{...}``` and return the remaining content."""
    function_calls = re.findall(r"```json\s*(\{.*?\})\s*```", raw_string, re.DOTALL)
    remaining_content = re.sub(
        r"```json\s*\{.*?\}\s*```", "", raw_string, flags=re.DOTALL
    ).strip()
    return function_calls, remaining_content
def json_extract_raw(raw_string: str) -> tuple[list[str], str]:
    """Extract tools in form `{...}` and return the remaining content."""
    # Regex to extract sequences starting with '{' and ending with '}}'
    json_like_regex = r"\{.*?\}\}"
    function_calls = re.findall(json_like_regex, raw_string)
    remaining_content = re.sub(json_like_regex, "", raw_string).strip()
    return function_calls, remaining_content
def xml_extract(raw_string: str, tag: str) -> tuple[list[str], str]:
    """Extract tools in form <tag>{...}</tag> and return the remaining content."""
    tool_call_regex = rf"<{tag}>((?:.|\n)*?)</{tag}>"
    function_calls = re.findall(tool_call_regex, raw_string)
    tool_call_content_regex = rf"<{tag}>(?:.|\n)*?</{tag}>"
    other_content = re.split(tool_call_content_regex, raw_string, flags=re.DOTALL)
    other_content = [
        str(content).strip() for content in other_content if str(content).strip()
    ]
    content = "\n\n".join(other_content)
    return function_calls, content
def parse_unknown_tool_calls(response: str) -> tuple[list[str], str]:
    if "```json" in response:
        return json_extract(response)
    elif "<tool_call>" in response:
        return xml_extract(response, "tool_call")
    elif "<function>" in response:
        return xml_extract(response, "function")
    elif "{" in response and "}}" in response:
        return json_extract_raw(response)
    else:
        return [], response
def filter_assistant_header(message: str) -> str:
    return re.sub(r"<\|start_header_id\|>assistant<\|end_header_id\|>", "", message)

================
File: src/inspect_ai/model/_providers/util/hooks.py
================
import re
import time
from logging import getLogger
from typing import Any, Mapping, NamedTuple, cast
import httpx
from shortuuid import uuid
from inspect_ai._util.constants import HTTP
from inspect_ai._util.retry import report_http_retry
logger = getLogger(__name__)
class RequestInfo(NamedTuple):
    attempts: int
    last_request: float
class HttpHooks:
    """Class which hooks various HTTP clients for improved tracking/logging.
    A special header is injected into requests which is then read from
    a request event hook -- this creates a record of when the request
    started. Note that with retries a single request_id could be started
    several times; our request hook makes sure we always track the time of
    the last request.
    There is an 'end_request()' method which gets the total request time
    for a request_id and then purges the request_id from our tracking (so
    the dict doesn't grow unbounded)
    Additionally, an http response hook is installed and used for logging
    requests for the 'http' log-level
    """
    REQUEST_ID_HEADER = "x-irid"
    def __init__(self) -> None:
        # track request start times
        self._requests: dict[str, RequestInfo] = {}
    def start_request(self) -> str:
        request_id = uuid()
        self._requests[request_id] = RequestInfo(0, time.monotonic())
        return request_id
    def end_request(self, request_id: str) -> float:
        # read the request info (if available) and purge from dict
        request_info = self._requests.pop(request_id, None)
        if request_info is None:
            raise RuntimeError(f"request_id not registered: {request_id}")
        # return elapsed time
        return time.monotonic() - request_info.last_request
    def update_request_time(self, request_id: str) -> None:
        request_info = self._requests.get(request_id, None)
        if not request_info:
            raise RuntimeError(f"No request registered for request_id: {request_id}")
        # update the attempts and last request time
        request_info = RequestInfo(request_info.attempts + 1, time.monotonic())
        self._requests[request_id] = request_info
        # trace a retry if this is attempt > 1
        if request_info.attempts > 1:
            report_http_retry()
class ConverseHooks(HttpHooks):
    def __init__(self, session: Any) -> None:
        from aiobotocore.session import AioSession
        super().__init__()
        # register hooks
        session = cast(AioSession, session._session)
        session.register(
            "before-send.bedrock-runtime.Converse", self.converse_before_send
        )
        session.register(
            "after-call.bedrock-runtime.Converse", self.converse_after_call
        )
    def converse_before_send(self, **kwargs: Any) -> None:
        user_agent = kwargs["request"].headers["User-Agent"].decode()
        match = re.search(rf"{self.USER_AGENT_PREFIX}(\w+)", user_agent)
        if match:
            request_id = match.group(1)
            self.update_request_time(request_id)
    def converse_after_call(self, http_response: Any, **kwargs: Any) -> None:
        from botocore.awsrequest import AWSResponse
        response = cast(AWSResponse, http_response)
        logger.log(HTTP, f"POST {response.url} - {response.status_code}")
    def user_agent_extra(self, request_id: str) -> str:
        return f"{self.USER_AGENT_PREFIX}{request_id}"
    USER_AGENT_PREFIX = "ins/rid#"
class HttpxHooks(HttpHooks):
    def __init__(self, client: httpx.AsyncClient):
        super().__init__()
        # install hooks
        client.event_hooks["request"].append(self.request_hook)
        client.event_hooks["response"].append(self.response_hook)
    async def request_hook(self, request: httpx.Request) -> None:
        # update the last request time for this request id (as there could be retries)
        request_id = request.headers.get(self.REQUEST_ID_HEADER, None)
        if request_id:
            self.update_request_time(request_id)
    async def response_hook(self, response: httpx.Response) -> None:
        message = f'{response.request.method} {response.request.url} "{response.http_version} {response.status_code} {response.reason_phrase}" '
        logger.log(HTTP, message)
def urllib3_hooks() -> HttpHooks:
    import urllib3
    from urllib3.connectionpool import HTTPConnectionPool
    from urllib3.response import BaseHTTPResponse
    class Urllib3Hooks(HttpHooks):
        def request_hook(self, headers: Mapping[str, str]) -> None:
            # update the last request time for this request id (as there could be retries)
            request_id = headers.get(self.REQUEST_ID_HEADER, None)
            if request_id:
                self.update_request_time(request_id)
        def response_hook(
            self, method: str, url: str, response: BaseHTTPResponse
        ) -> None:
            message = f'{method} {url} "{response.version_string} {response.status} {response.reason}" '
            logger.log(HTTP, message)
    global _urlilb3_hooks
    if _urlilb3_hooks is None:
        # one time patch of urlopen
        urlilb3_hooks = Urllib3Hooks()
        original_urlopen = urllib3.connectionpool.HTTPConnectionPool.urlopen
        def patched_urlopen(
            self: HTTPConnectionPool, method: str, url: str, **kwargs: Any
        ) -> BaseHTTPResponse:
            headers = kwargs.get("headers", {})
            urlilb3_hooks.request_hook(headers)
            response = original_urlopen(self, method, url, **kwargs)
            urlilb3_hooks.response_hook(method, f"{self.host}{url}", response)
            return response
        urllib3.connectionpool.HTTPConnectionPool.urlopen = patched_urlopen  # type: ignore[assignment,method-assign]
        # assign to global hooks instance
        _urlilb3_hooks = urlilb3_hooks
    return _urlilb3_hooks
_urlilb3_hooks: HttpHooks | None = None

================
File: src/inspect_ai/model/_providers/util/llama31.py
================
import json
import re
import textwrap
from logging import getLogger
from shortuuid import uuid
from typing_extensions import override
from inspect_ai.tool._tool_call import ToolCall
from inspect_ai.tool._tool_info import ToolInfo
from ..._call_tools import parse_tool_call, tool_parse_error_message
from ..._chat_message import (
    ChatMessage,
    ChatMessageAssistant,
    ChatMessageSystem,
    ChatMessageTool,
)
from .chatapi import ChatAPIHandler, ChatAPIMessage
logger = getLogger(__name__)
# Llama 3.1 handler is based primarily on the tool calling conventions used by Ollama:
# (https://github.com/ollama/ollama/blob/main/server/testdata/tools/llama3-groq-tool-use.out)
#
# We initially tried the conventions promoted in the Llama 3.1 model card but
# this had severe problems with not formatting the function calls correctly
# (https://llama.meta.com/docs/model-cards-and-prompt-formats/llama3_1/#user-defined-custom-tool-calling)
TOOL_CALL = "tool_call"
class Llama31Handler(ChatAPIHandler):
    @override
    def input_with_tools(
        self, input: list[ChatMessage], tools: list[ToolInfo]
    ) -> list[ChatMessage]:
        # JSON schema for available tools
        available_tools = "\n\n".join(
            [tool.model_dump_json(exclude_none=True, indent=2) for tool in tools]
        )
        # tool prompt
        tool_prompt = textwrap.dedent(
            f"""
            You are a knowledgable assistant. You can answer questions and perform tasks. You are provided with function signatures within <tools></tools> XML tags. You may call one or more functions to assist with the user query. Don't make assumptions about what values to plug into functions. For each function call return a json object with function name and arguments within <{TOOL_CALL}></{TOOL_CALL}> XML tags as follows:
            <{TOOL_CALL}>
            {{"name": <function-name>,"arguments": <args-dict>}}
            </{TOOL_CALL}>
            Here are the available tools defined in JSON Schema:
            <tools>
            {available_tools}
            </tools>
            Reminder:
            - Function calls MUST follow the specified format, start with <{TOOL_CALL}> and end with </{TOOL_CALL}>.
            - Please call only one function at a time.
            - It's fine to include some reasoning about which function to call and why.
            - Please ensure that </{TOOL_CALL}> is the last content in the message (there should be no text after it).
            - Please be absolutely sure that the function name you have specified matches one of the functions described in <tools>.
            - All function parameters MUST be specified.
            - If there is no function call available, answer the question like normal with your current knowledge and do not tell the user about function calls
            """
        )
        return [ChatMessageSystem(content=tool_prompt)] + input
    @override
    def parse_assistant_response(
        self, response: str, tools: list[ToolInfo]
    ) -> ChatMessageAssistant:
        """Parse content and tool calls from a model response.
        This method has an interdependency with `input_with_tools()` (as that is the
        prompt that asks the model to use the <tool_call>...</tool_call> syntax)
        """
        # extract tool calls
        tool_call_regex = rf"<{TOOL_CALL}>((?:.|\n)*?)</{TOOL_CALL}>"
        tool_calls_content: list[str] = re.findall(tool_call_regex, response)
        # if there are tool calls proceed with parsing
        if len(tool_calls_content) > 0:
            # parse each tool call (if there are parsing error that occur
            # this will be reported in the `parse_error` field of the ToolCall
            # and ultimately reported back to the model)
            tool_calls = [
                parse_tool_call_content(content, tools)
                for content in tool_calls_content
            ]
            # find other content that exists outside tool calls
            tool_call_content_regex = rf"<{TOOL_CALL}>(?:.|\n)*?</{TOOL_CALL}>"
            other_content = re.split(tool_call_content_regex, response, flags=re.DOTALL)
            other_content = [
                str(content).strip()
                for content in other_content
                if str(content).strip()
            ]
            content = "\n\n".join(other_content)
            # return the message
            return ChatMessageAssistant(
                content=filter_assistant_header(content),
                tool_calls=tool_calls,
                source="generate",
            )
        # otherwise this is just an ordinary assistant message
        else:
            return ChatMessageAssistant(
                content=filter_assistant_header(response), source="generate"
            )
    @override
    def assistant_message(self, message: ChatMessageAssistant) -> ChatAPIMessage:
        if message.tool_calls:
            content = "\n\n".join(
                [message.text]
                + [
                    f'<tool_call>{{"name": "{tool.function}", "arguments": {json.dumps(tool.arguments)} }}</tool_call>'
                    for tool in message.tool_calls
                ]
            ).strip()
        else:
            content = message.text
        return {"role": "assistant", "content": content}
    @override
    def tool_message(self, message: ChatMessageTool) -> ChatAPIMessage:
        return {
            "role": "tool",
            "content": f"Error: {message.error.message}"
            if message.error
            else message.text,
        }
def parse_tool_call_content(content: str, tools: list[ToolInfo]) -> ToolCall:
    """Attempt to parse content from inside <tool_call> tags.
    Content inside a <tool_call> should be a JSON dictionary with `name` and
    `arguments` (which in turn should be a `dict[str,Any]` but in some cases
    we've seen models pass `str`). This function attempts to extract this from
    the passed tcontentext. A `ToolCall` is returned for all cases (if the
    parsing fails then it will have a `parse_error`, which will be subsequently
    reported to the model.
    """
    try:
        # parse raw JSON
        tool_call_data = json.loads(content)
        # if its not a dict then report error
        if not isinstance(tool_call_data, dict):
            raise ValueError("The provided arguments are not a JSON dictionary.")
        # see if we can get the fields (if not report error)
        name = tool_call_data.get("name", None)
        arguments = tool_call_data.get("arguments", None)
        if not name or not arguments:
            raise ValueError(
                "Required 'name' and 'arguments' not provided in JSON dictionary."
            )
        # now perform the parse (we need to call thi function because it includes
        # the special handling to for mapping arguments that are a plain `str`
        # to the first parameter of the function)
        unique_id = f"{name}_{uuid()}"
        return parse_tool_call(unique_id, name, json.dumps(arguments), tools)
    except Exception as ex:
        # buld error message
        parse_error = tool_parse_error_message(content, ex)
        # log it to 'info'
        logger.info(parse_error)
        # notify model
        return ToolCall(
            id="unknown",
            function="unknown",
            arguments={},
            type="function",
            parse_error=parse_error,
        )
def filter_assistant_header(message: str) -> str:
    return re.sub(r"<\|start_header_id\|>assistant<\|end_header_id\|>", "", message)

================
File: src/inspect_ai/model/_providers/util/util.py
================
import os
from logging import getLogger
from inspect_ai._util.error import PrerequisiteError
logger = getLogger(__name__)
def model_base_url(base_url: str | None, env_vars: str | list[str]) -> str | None:
    if base_url:
        return base_url
    if isinstance(env_vars, str):
        env_vars = [env_vars]
    for env_var in env_vars:
        base_url = os.getenv(env_var, None)
        if base_url:
            return base_url
    return os.getenv("INSPECT_EVAL_MODEL_BASE_URL", None)
def environment_prerequisite_error(
    client: str, env_vars: str | list[str]
) -> PrerequisiteError:
    def fmt(key: str) -> str:
        return f"[bold][blue]{key}[/blue][/bold]"
    env_vars = [env_vars] if isinstance(env_vars, str) else env_vars
    if len(env_vars) == 1:
        env_vars_list = fmt(env_vars[0])
    else:
        env_vars_list = (
            ", ".join([fmt(env_bar) for env_bar in env_vars[:-1]])
            + ("," if len(env_vars) > 2 else "")
            + " or "
            + fmt(env_vars[-1])
        )
    return PrerequisiteError(
        f"ERROR: Unable to initialise {client} client\n\nNo {env_vars_list} defined in the environment."
    )

================
File: src/inspect_ai/model/_providers/vertex.py
================
import functools
import json
from copy import copy
from typing import Any, cast
import vertexai  # type: ignore
from google.api_core.exceptions import (
    Aborted,
    ClientError,
    DeadlineExceeded,
    ServiceUnavailable,
)
from google.api_core.retry import if_transient_error
from google.protobuf.json_format import MessageToDict
from pydantic import JsonValue
from typing_extensions import override
from vertexai.generative_models import (  # type: ignore
    Candidate,
    FinishReason,
    FunctionDeclaration,
    GenerationConfig,
    GenerationResponse,
    GenerativeModel,
    HarmBlockThreshold,
    HarmCategory,
    Image,
    Part,
    Tool,
)
from vertexai.generative_models import Content as VertexContent
from inspect_ai._util.constants import BASE_64_DATA_REMOVED, NO_CONTENT
from inspect_ai._util.content import (
    Content,
    ContentAudio,
    ContentImage,
    ContentText,
    ContentVideo,
)
from inspect_ai._util.http import is_retryable_http_status
from inspect_ai._util.images import file_as_data
from inspect_ai.tool import ToolCall, ToolChoice, ToolInfo
from .._chat_message import (
    ChatMessage,
    ChatMessageAssistant,
    ChatMessageSystem,
    ChatMessageTool,
    ChatMessageUser,
)
from .._generate_config import GenerateConfig
from .._model import ModelAPI
from .._model_call import ModelCall
from .._model_output import (
    ChatCompletionChoice,
    Logprob,
    Logprobs,
    ModelOutput,
    ModelUsage,
    StopReason,
    TopLogprob,
)
SAFETY_SETTINGS = "safety_settings"
VERTEX_INIT_ARGS = "vertex_init_args"
DEFAULT_SAFETY_SETTINGS = {
    HarmCategory.HARM_CATEGORY_UNSPECIFIED: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
}
class VertexAPI(ModelAPI):
    """
    Provider for using the Google Vertex AI model endpoint.
    Note that this is an alternative to the provider implemented in `google.py`
    which deals with endpoints for Google AI Studio. If in doubt which one to
    use, you probably want `google.py`, you can see a comparison matrix here:
    https://cloud.google.com/vertex-ai/generative-ai/docs/migrate/migrate-google-ai
    By default we assume the environment we run in is authenticated
    appropriately, but you can pass in `vertex_init_args` to configure this
    directly, this is passed directly to `vertex.init` which is documented here:
    https://cloud.google.com/vertex-ai/generative-ai/docs/reference/python/latest/vertexai#vertexai_init
    Additional `model_args`:
        vertex_init_args (dict[str, Any]): Additional arguments to pass to `vertexai.init`
        safety_settings (dict[str, str]): Mapping for adjusting Gemini safety settings
    """
    def __init__(
        self,
        model_name: str,
        config: GenerateConfig = GenerateConfig(),
        **model_args: Any,
    ) -> None:
        super().__init__(model_name=model_name, config=config)
        if VERTEX_INIT_ARGS in model_args:
            vertexai.init(**model_args[VERTEX_INIT_ARGS])
            del model_args[VERTEX_INIT_ARGS]
        else:
            vertexai.init()
        # pick out vertex safety settings and merge against default
        self.safety_settings = DEFAULT_SAFETY_SETTINGS.copy()
        if SAFETY_SETTINGS in model_args:
            self.safety_settings.update(
                parse_safety_settings(model_args.get(SAFETY_SETTINGS))
            )
            del model_args[SAFETY_SETTINGS]
        self.model = GenerativeModel(model_name)
    @override
    async def close(self) -> None:
        # GenerativeModel uses a cached/shared client so there is no 'close'
        pass
    async def generate(
        self,
        input: list[ChatMessage],
        tools: list[ToolInfo],
        tool_choice: ToolChoice,
        config: GenerateConfig,
    ) -> tuple[ModelOutput, ModelCall]:
        parameters = GenerationConfig(
            temperature=config.temperature,
            top_p=config.top_p,
            top_k=config.top_k,
            max_output_tokens=config.max_tokens,
            stop_sequences=config.stop_seqs,
            candidate_count=config.num_choices,
            seed=config.seed,
            presence_penalty=config.presence_penalty,
            frequency_penalty=config.frequency_penalty,
            response_logprobs=config.logprobs,
            logprobs=config.top_logprobs,
        )
        messages = await as_chat_messages(input)
        vertex_tools = chat_tools(tools) if len(tools) > 0 else None
        response = await self.model.generate_content_async(
            contents=messages,
            safety_settings=self.safety_settings,
            generation_config=parameters,
            tools=vertex_tools,
        )
        # capture output
        output = ModelOutput(
            model=self.model_name,
            choices=completion_choices_from_candidates(response.candidates),
            usage=ModelUsage(
                input_tokens=response.usage_metadata.prompt_token_count,
                output_tokens=response.usage_metadata.candidates_token_count,
                total_tokens=response.usage_metadata.total_token_count,
            ),
        )
        # build call
        call = model_call(
            contents=messages,
            safety_settings=self.safety_settings,
            generation_config=parameters,
            tools=vertex_tools,
            response=response,
        )
        # return
        return output, call
    @override
    def should_retry(self, ex: Exception) -> bool:
        # google API-specific errors
        if isinstance(ex, Aborted | DeadlineExceeded | ServiceUnavailable):
            return True
        # standard HTTP errors
        elif isinstance(ex, ClientError) and ex.code is not None:
            return is_retryable_http_status(ex.code)
        # additional errors flagged by google as transient
        elif isinstance(ex, Exception):
            return if_transient_error(ex)
        else:
            return False
    @override
    def connection_key(self) -> str:
        """Scope for enforcing max_connections (could also use endpoint)."""
        return self.model_name
    @override
    def collapse_user_messages(self) -> bool:
        return True
    @override
    def collapse_assistant_messages(self) -> bool:
        return True
def model_call(
    contents: list[Content],
    generation_config: GenerationConfig,
    safety_settings: dict[HarmCategory, HarmBlockThreshold],
    tools: list[Tool] | None,
    response: GenerationResponse,
) -> ModelCall:
    return ModelCall.create(
        request=dict(
            contents=[model_call_content(content) for content in contents],
            generation_config=generation_config.to_dict(),
            safety_settings=safety_settings,
            tools=[tool.to_dict() for tool in tools] if tools is not None else None,
        ),
        response=response.to_dict(),
        filter=model_call_filter,
    )
def model_call_filter(key: JsonValue | None, value: JsonValue) -> JsonValue:
    # remove images from raw api call
    if key == "inline_data" and isinstance(value, dict) and "data" in value:
        value = copy(value)
        value.update(data=BASE_64_DATA_REMOVED)
    return value
def model_call_content(content: VertexContent) -> dict[str, Any]:
    return cast(dict[str, Any], content.to_dict())
async def as_chat_messages(messages: list[ChatMessage]) -> list[VertexContent]:
    # google does not support system messages so filter them out to start with
    system_messages = [message for message in messages if message.role == "system"]
    supported_messages = [message for message in messages if message.role != "system"]
    # build google chat messages
    chat_messages: list[VertexContent] = [
        await content_dict(message) for message in supported_messages
    ]
    # we want the system messages to be prepended to the first user message
    # (if there is no first user message then prepend one)
    prepend_system_messages(chat_messages, system_messages)
    # combine consecutive tool messages
    chat_messages = functools.reduce(consective_tool_message_reducer, chat_messages, [])
    # return messages
    return chat_messages
def consective_tool_message_reducer(
    messages: list[VertexContent],
    message: VertexContent,
) -> list[VertexContent]:
    if (
        message.role == "function"
        and len(messages) > 0
        and messages[-1].role == "function"
    ):
        messages[-1] = VertexContent(
            role="function", parts=messages[-1].parts + message.parts
        )
    else:
        messages.append(message)
    return messages
async def content_dict(
    message: ChatMessageUser | ChatMessageAssistant | ChatMessageTool,
) -> VertexContent:
    if isinstance(message, ChatMessageUser):
        if isinstance(message.content, str):
            parts = [Part.from_text(message.content or NO_CONTENT)]
        else:
            parts = [await content_part(content) for content in message.content]
        return VertexContent(role="user", parts=parts)
    elif isinstance(message, ChatMessageAssistant):
        content_parts: list[Part] = []
        if message.tool_calls is not None:
            content_parts.extend(
                [
                    # For some reason there's no `Parts.from_function_call`
                    # function, but there's a generic `from_dict` instead
                    Part.from_dict(
                        {
                            "function_call": {
                                "name": tool_call.function,
                                "args": tool_call.arguments,
                            }
                        }
                    )
                    for tool_call in message.tool_calls
                ]
            )
        if isinstance(message.content, str):
            content_parts.append(Part.from_text(message.content or NO_CONTENT))
        else:
            content_parts.extend(
                [await content_part(content) for content in message.content]
            )
        return VertexContent(role="model", parts=content_parts)
    elif isinstance(message, ChatMessageTool):
        return VertexContent(
            role="function",
            parts=[
                Part.from_function_response(
                    name=message.tool_call_id,
                    response={
                        "content": (
                            message.error.message
                            if message.error is not None
                            else message.text
                        )
                    },
                )
            ],
        )
async def content_part(content: Content | str) -> Part:
    if isinstance(content, str):
        return Part.from_text(content or NO_CONTENT)
    elif isinstance(content, ContentText):
        return Part.from_text(content.text or NO_CONTENT)
    elif isinstance(content, ContentImage):
        image_bytes, mime_type = await file_as_data(content.image)
        return Part.from_image(image=Image.from_bytes(data=image_bytes))
    else:
        if isinstance(content, ContentAudio):
            file = content.audio
        elif isinstance(content, ContentVideo):
            file = content.video
        file_bytes, mime_type = await file_as_data(file)
        return Part.from_data(file_bytes, mime_type)
def prepend_system_messages(
    messages: list[VertexContent], system_messages: list[ChatMessageSystem]
) -> None:
    # create system_parts
    system_parts = [Part.from_text(message.content) for message in system_messages]
    # we want the system messages to be prepended to the first user message
    # (if there is no first user message then prepend one)
    if messages[0].role == "user":
        parts = messages[0].parts
        messages[0] = VertexContent(role="user", parts=system_parts + parts)
    else:
        messages.insert(0, VertexContent(role="user", parts=system_parts))
def chat_tools(tools: list[ToolInfo]) -> list[Tool]:
    declarations = [
        FunctionDeclaration(
            name=tool.name,
            description=tool.description,
            parameters=tool.parameters.model_dump(
                exclude_none=True, exclude={"additionalProperties"}
            ),
        )
        for tool in tools
    ]
    return [Tool(function_declarations=declarations)]
def completion_choice_from_candidate(candidate: Candidate) -> ChatCompletionChoice:
    # check for completion text
    content = " ".join(
        [
            part.text
            for part in candidate.content.parts
            if part.to_dict().get("text") is not None
        ]
    )
    # now tool calls
    tool_calls: list[ToolCall] = []
    for part in candidate.content.parts:
        if part.function_call:
            function_call = MessageToDict(getattr(part.function_call, "_pb"))
            tool_calls.append(
                ToolCall(
                    type="function",
                    id=function_call["name"],
                    function=function_call["name"],
                    arguments=function_call["args"],
                )
            )
    # stop reason
    stop_reason = candidate_stop_reason(candidate.finish_reason)
    choice = ChatCompletionChoice(
        message=ChatMessageAssistant(
            content=content,
            tool_calls=tool_calls if len(tool_calls) > 0 else None,
            source="generate",
        ),
        stop_reason=stop_reason,
    )
    if candidate.logprobs_result:
        logprobs: list[Logprob] = []
        for chosen, top in zip(
            candidate.logprobs_result.chosen_candidates,
            candidate.logprobs_result.top_candidates,
        ):
            logprobs.append(
                Logprob(
                    token=chosen.token,
                    logprob=chosen.log_probability,
                    top_logprobs=[
                        TopLogprob(token=c.token, logprob=c.log_probability)
                        for c in top.candidates
                    ],
                )
            )
        choice.logprobs = Logprobs(content=logprobs)
    return choice
def completion_choices_from_candidates(
    candidates: list[Candidate],
) -> list[ChatCompletionChoice]:
    candidates = copy(candidates)
    candidates.sort(key=lambda c: c.index)
    return [completion_choice_from_candidate(candidate) for candidate in candidates]
def candidate_stop_reason(finish_reason: FinishReason) -> StopReason:
    match finish_reason:
        case FinishReason.STOP:
            return "stop"
        case FinishReason.MAX_TOKENS:
            return "max_tokens"
        case FinishReason.SAFETY | FinishReason.RECITATION:
            return "content_filter"
        case _:
            return "unknown"
def parse_safety_settings(
    safety_settings: Any,
) -> dict[HarmCategory, HarmBlockThreshold]:
    # ensure we have a dict
    if isinstance(safety_settings, str):
        safety_settings = json.loads(safety_settings)
    if not isinstance(safety_settings, dict):
        raise ValueError(f"{SAFETY_SETTINGS} must be dictionary.")
    parsed_settings: dict[HarmCategory, HarmBlockThreshold] = {}
    for key, value in safety_settings.items():
        if isinstance(key, str):
            key = str_to_harm_category(key)
        if not isinstance(key, HarmCategory):
            raise ValueError(f"Unexpected type for harm category: {key}")
        if isinstance(value, str):
            value = str_to_harm_block_threshold(value)
        if not isinstance(value, HarmBlockThreshold):
            raise ValueError(f"Unexpected type for harm block threshold: {value}")
        parsed_settings[key] = value
    return parsed_settings
def str_to_harm_category(category: str) -> HarmCategory:
    category = category.upper()
    if "HARASSMENT" in category:
        return HarmCategory.HARM_CATEGORY_HARASSMENT
    elif "HATE_SPEECH" in category:
        return HarmCategory.HARM_CATEGORY_HATE_SPEECH
    elif "SEXUALLY_EXPLICIT" in category:
        return HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT
    elif "DANGEROUS_CONTENT" in category:
        return HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT
    elif "UNSPECIFIED" in category:
        return HarmCategory.HARM_CATEGORY_UNSPECIFIED
    else:
        raise ValueError(f"Unknown HarmCategory: {category}")
def str_to_harm_block_threshold(threshold: str) -> HarmBlockThreshold:
    threshold = threshold.upper()
    if "LOW" in threshold:
        return HarmBlockThreshold.BLOCK_LOW_AND_ABOVE
    elif "MEDIUM" in threshold:
        return HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE
    elif "HIGH" in threshold:
        return HarmBlockThreshold.BLOCK_ONLY_HIGH
    elif "NONE" in threshold:
        return HarmBlockThreshold.BLOCK_NONE
    else:
        raise ValueError(f"Unknown HarmBlockThreshold: {threshold}")

================
File: src/inspect_ai/model/_providers/vllm.py
================
import concurrent.futures
import functools
import gc
import os
import time
from concurrent.futures import Future
from dataclasses import dataclass
from queue import Empty, Queue
from threading import Thread
from typing import Any, cast
import anyio
from typing_extensions import override
from vllm import LLM, CompletionOutput, RequestOutput, SamplingParams  # type: ignore
from inspect_ai._util.constants import DEFAULT_MAX_TOKENS
from inspect_ai.tool import ToolChoice, ToolInfo
from .._chat_message import ChatMessage, ChatMessageAssistant
from .._generate_config import GenerateConfig
from .._model import ModelAPI, simple_input_messages
from .._model_output import (
    ChatCompletionChoice,
    Logprob,
    Logprobs,
    ModelOutput,
    ModelUsage,
    StopReason,
    TopLogprob,
)
from .util import chat_api_input
DEFAULT_START_TOKEN = "<|im_start|>"
DEFAULT_END_TOKEN = "<|im_end|>"
HF_TOKEN = "HF_TOKEN"
@dataclass
class GenerateInput:
    input: str
    generator: Any
    batch_size: int
    num_top_logprobs: int | None = None
@dataclass
class GenerateOutput:
    output: str
    input_tokens: int
    output_tokens: int
    total_tokens: int
    stop_reason: StopReason
    logprobs: Logprobs | None
    time: float
class VLLMAPI(ModelAPI):
    def __init__(
        self,
        model_name: str,
        base_url: str | None = None,
        api_key: str | None = None,
        config: GenerateConfig = GenerateConfig(),
        **model_args: Any,
    ) -> None:
        super().__init__(
            model_name=model_name,
            base_url=base_url,
            api_key=api_key,
            api_key_vars=[HF_TOKEN],
            config=config,
        )
        self.seed = None
        if config.seed is not None:
            self.seed = config.seed
        # collect known model_args (then delete them so we can pass the rest on)
        def collect_model_arg(name: str) -> Any | None:
            nonlocal model_args
            value = model_args.get(name, None)
            if value is not None:
                model_args.pop(name)
            return value
        device = collect_model_arg("device")
        tokenizer = collect_model_arg("tokenizer")
        model_path = collect_model_arg("model_path")
        tokenizer_path = collect_model_arg("tokenizer_path")
        self.batch_size = collect_model_arg("batch_size")
        self.chat_template = collect_model_arg("chat_template")
        # if user provides model_path, use that instead of model_name
        if model_path:
            model_name = model_path
        # load tokenizer
        if not tokenizer:
            if tokenizer_path:
                tokenizer = tokenizer_path
            else:
                tokenizer = model_name
        # set which GPUs are available to use
        if device is not None:
            os.environ["CUDA_VISIBLE_DEVICES"] = str(device)
        # tell vllm how many GPUs to use
        if "tensor_parallel_size" not in model_args:
            devices = os.environ.get("CUDA_VISIBLE_DEVICES", "").split(",")
            num_devices = len(devices)
            if num_devices > 1:
                model_args["tensor_parallel_size"] = num_devices
            else:
                model_args["tensor_parallel_size"] = 1
        # https://github.com/vllm-project/vllm/pull/6051
        # Gemma 2 models require FlashInfer backend for softcap logits
        if "google/gemma-2" in model_name:
            os.environ["VLLM_ATTENTION_BACKEND"] = "FLASHINFER"
            try:
                import importlib
                # check if flashinfer is installed
                importlib.import_module("flashinfer")
            except ImportError:
                raise ImportError(
                    "To use the 'google/gemma-2' model, you must install the 'flashinfer' package. "
                    "See https://docs.flashinfer.ai/installation.html"
                )
        # load model
        self.model = LLM(model_name, tokenizer=tokenizer, **model_args)
        # we get the tokenizer so we can use it to apply the model's chat template later
        self.tokenizer = self.model.get_tokenizer()
    @override
    async def close(self) -> None:
        self.tokenizer = None
        self.model = None
        gc.collect()
    def apply_chat_template(
        self, messages: list[ChatMessage], tools: list[ToolInfo]
    ) -> str:
        # handle system message and consecutive user messages
        messages = simple_input_messages(messages)
        # convert to chat template input format
        chat_messages = chat_api_input(messages, tools)
        # apply chat template
        chat = self.tokenizer.apply_chat_template(
            chat_messages,
            add_generation_prompt=True,
            tokenize=False,
            chat_template=self.chat_template,
        )
        return cast(str, chat)
    @override
    def max_connections(self) -> int:
        """Effectively the batch size."""
        return 32
    def get_sampling_params(self, config: GenerateConfig, chat: str) -> SamplingParams:
        kwargs: dict[str, Any] = dict()
        if config.max_tokens is not None:
            kwargs["max_tokens"] = config.max_tokens
        else:
            kwargs["max_tokens"] = DEFAULT_MAX_TOKENS
        if config.temperature is not None:
            # for some reason vllm doesn't generate anything for 0 < temperature < 0.02
            if 0 < config.temperature < 0.02:
                config.temperature = 0.02
            kwargs["temperature"] = config.temperature
        if config.top_p is not None:
            kwargs["top_p"] = config.top_p
        if config.top_k is not None:
            kwargs["top_k"] = config.top_k
        # if config.min_p is not None:
        #     kwargs["min_p"] = config.min_p
        if config.seed is not None:
            kwargs["seed"] = config.seed
        elif self.seed is not None:
            kwargs["seed"] = self.seed
        if config.frequency_penalty is not None:
            kwargs["frequency_penalty"] = config.frequency_penalty
        if config.presence_penalty is not None:
            kwargs["presence_penalty"] = config.presence_penalty
        if config.num_choices is not None:
            kwargs["n"] = config.num_choices
        if config.best_of is not None:
            kwargs["best_of"] = config.best_of
        if config.logprobs is not None:
            kwargs["logprobs"] = 0
        if config.top_logprobs is not None:
            kwargs["logprobs"] = config.top_logprobs
        if config.stop_seqs is not None:
            kwargs["stop"] = config.stop_seqs
        # some models don't stop at <|im_end|> token
        # perhaps there is a better solution than this (modify tokenizer?)
        # TODO: what model needs this?
        if chat.startswith(DEFAULT_START_TOKEN):
            if "stop" not in kwargs:
                kwargs["stop"] = [DEFAULT_END_TOKEN]
            else:
                kwargs["stop"].append(DEFAULT_END_TOKEN)
        sampling_params = SamplingParams(
            **kwargs,
            stop_token_ids=self.tokenizer.all_special_ids,  # We default to stopping at all special tokens
            include_stop_str_in_output=False,
        )
        return sampling_params
    async def generate(
        self,
        input: list[ChatMessage],
        tools: list[ToolInfo],
        tool_choice: ToolChoice,
        config: GenerateConfig,
    ) -> ModelOutput:
        chat = self.apply_chat_template(input, tools)
        # prepare generator
        sampling_params = self.get_sampling_params(config, chat)
        generator = functools.partial(
            self.model.generate, sampling_params=sampling_params, use_tqdm=False
        )
        # generate
        responses = await batched_generate(
            GenerateInput(
                input=chat,
                generator=generator,
                batch_size=config.max_connections or self.max_connections(),
                num_top_logprobs=config.top_logprobs,
            )
        )
        return self.process_responses(responses, tools)
    def process_responses(
        self, responses: list[GenerateOutput], tools: list[ToolInfo]
    ) -> ModelOutput:
        choices = [
            ChatCompletionChoice(
                message=ChatMessageAssistant(
                    content=response.output, source="generate"
                ),
                stop_reason=response.stop_reason,
                logprobs=response.logprobs,
            )
            for response in responses
        ]
        # TODO: what's the best way to calculate token usage for num_choices > 1
        total_time = responses[0].time
        input_tokens = responses[0].input_tokens
        output_tokens = sum(response.output_tokens for response in responses)
        total_tokens = input_tokens + output_tokens
        return ModelOutput(
            model=self.model_name,
            choices=choices,
            usage=ModelUsage(
                input_tokens=input_tokens,
                output_tokens=output_tokens,
                total_tokens=total_tokens,
            ),
            time=total_time,
        )
@dataclass
class _QueueItem:
    input: GenerateInput
    future: Future[list[GenerateOutput]]
batch_thread: Thread | None = None
batch_queue: "Queue[_QueueItem]" = Queue()
async def batched_generate(input: GenerateInput) -> list[GenerateOutput]:
    # start the background thread if necessary
    global batch_thread
    if batch_thread is None:
        batch_thread = Thread(target=process_batches, daemon=True)
        batch_thread.start()
    # enqueue the job
    future = Future[list[GenerateOutput]]()
    batch_queue.put(_QueueItem(input=input, future=future))
    # await the future
    while True:
        try:
            return future.result(timeout=0.01)
        except concurrent.futures.TimeoutError:
            pass
        await anyio.sleep(1)
def string_to_bytes(string: str) -> list[int]:
    return list(map(ord, string))
def extract_logprobs(
    completion: CompletionOutput, num_top_logprobs: int | None
) -> Logprobs | None:
    if completion.logprobs is None or not completion.logprobs:
        return None
    # if config.logprobs = True, we want to get the selected tokens logprob
    # but if config.top_logprobs is not set, we don't want to return the top logprobs
    if num_top_logprobs is None:
        num_top_logprobs = 0
    logprobs = []
    for token_id, logprob in zip(completion.token_ids, completion.logprobs):
        top_logprobs = [
            TopLogprob(
                token=cast(str, token.decoded_token),
                logprob=token.logprob,
                bytes=string_to_bytes(cast(str, token.decoded_token)),
            )
            # exclude the chosen token if it's not in the top logprobs
            for token in logprob.values()
            if cast(int, token.rank) - 1 < num_top_logprobs
        ]
        selected_token = logprob[token_id]
        logprobs.append(
            Logprob(
                token=cast(str, selected_token.decoded_token),
                logprob=selected_token.logprob,
                bytes=string_to_bytes(cast(str, selected_token.decoded_token)),
                top_logprobs=top_logprobs,
            )
        )
    return Logprobs(content=logprobs)
def get_stop_reason(finish_reason: str | None) -> StopReason:
    if finish_reason == "stop":
        return "stop"
    elif finish_reason == "length":
        return "max_tokens"
    elif finish_reason == "abort":
        return "unknown"
    else:
        return "unknown"
def post_process_output(
    output: RequestOutput, i: int, num_top_logprobs: int | None, total_time: float
) -> GenerateOutput:
    completion = output.outputs[i]
    output_text: str = completion.text
    # # remove end token if it's there (byproduct of default chat template)
    # TODO: Remove
    # if output_text.endswith(DEFAULT_END_TOKEN):
    #     output_text = output_text[:len(DEFAULT_END_TOKEN)]
    input_tokens = len(output.prompt_token_ids)
    output_tokens = len(completion.token_ids)
    total_tokens = input_tokens + output_tokens
    return GenerateOutput(
        output=output_text,
        input_tokens=input_tokens,
        output_tokens=output_tokens,
        total_tokens=total_tokens,
        stop_reason=get_stop_reason(completion.finish_reason),
        logprobs=extract_logprobs(completion, num_top_logprobs),
        time=total_time,
    )
def post_process_outputs(
    output: RequestOutput, num_top_logprobs: int | None, total_time: float
) -> list[GenerateOutput]:
    return [
        post_process_output(output, i, num_top_logprobs, total_time)
        for i in range(len(output.outputs))
    ]
def process_batches() -> None:
    while True:
        # drain the queue (wait until no new messages have shown up for 2 seconds)
        inputs: list[tuple[GenerateInput, Future[list[GenerateOutput]]]] = []
        while True:
            try:
                input = batch_queue.get(
                    timeout=2
                )  # wait 2 seconds max TODO: what's optimal wait time?
                inputs.append((input.input, input.future))
                if len(inputs) >= input.input.batch_size:
                    # max batch size reached
                    break
            except Empty:
                # we have exhausted the queue
                break
        # see if we have any work to do
        if len(inputs) == 0:
            continue
        try:
            start_time = time.monotonic()
            first_input = inputs[0][0]
            generator = first_input.generator
            num_top_logprobs = first_input.num_top_logprobs
            # generate
            outputs = generator([input[0].input for input in inputs])
            total_time = time.monotonic() - start_time
            for i, output in enumerate(outputs):
                future = inputs[i][1]
                future.set_result(
                    post_process_outputs(output, num_top_logprobs, total_time),
                )
        except Exception as e:
            for _, future in inputs:
                future.set_exception(e)

================
File: src/inspect_ai/model/_reasoning.py
================
import re
from typing import NamedTuple
class ContentWithReasoning(NamedTuple):
    content: str
    reasoning: str
    signature: str | None = None
    redacted: bool = False
def parse_content_with_reasoning(content: str) -> ContentWithReasoning | None:
    # Match <think> tag with optional attributes
    pattern = r'\s*<think(?:\s+signature="([^"]*)")?(?:\s+redacted="(true)")?\s*>(.*?)</think>(.*)'
    match = re.match(pattern, content, re.DOTALL)
    if match:
        signature = match.group(1)  # This will be None if not present
        redacted_value = match.group(2)  # This will be "true" or None
        reasoning = match.group(3).strip()
        content_text = match.group(4).strip()
        return ContentWithReasoning(
            content=content_text,
            reasoning=reasoning,
            signature=signature,
            redacted=redacted_value == "true",
        )
    else:
        return None

================
File: src/inspect_ai/model/_registry.py
================
from typing import Any, Callable, cast
from inspect_ai._util.registry import (
    RegistryInfo,
    registry_add,
    registry_name,
    registry_tag,
)
from ._model import ModelAPI
def modelapi_register(model_type: type[ModelAPI], name: str) -> type[ModelAPI]:
    r"""Register a model api.
    Args:
        model_type (type[Model]): Class deriving from Model
        name (str): API serving this model
    Returns:
        Model API with registry attributes.
    """
    registry_add(
        model_type,
        RegistryInfo(type="modelapi", name=name),
    )
    return model_type
def modelapi(name: str) -> Callable[..., type[ModelAPI]]:
    r"""Decorator for registering model APIs.
    Args:
        name (str): Name of API
    Returns:
        Model API with registry attributes.
    """
    # create_model_wrapper:
    #  (a) Add the type[Model] to the registry using the appropriately
    #      package-namespaced name
    #  (b) Ensure that instances of Model created by type[Model] also
    #      carry registry info.
    def create_model_wrapper(
        wrapped: type[ModelAPI] | Callable[..., type[ModelAPI]], api: str
    ) -> type[ModelAPI]:
        model_api = registry_name(wrapped, api)
        def model_wrapper(*args: Any, **kwargs: Any) -> ModelAPI:
            if not isinstance(wrapped, type):
                model_type = wrapped()
            else:
                model_type = wrapped
            model = model_type(*args, **kwargs)
            registry_tag(
                model_type,
                model,
                RegistryInfo(
                    type="modelapi",
                    name=model_api,
                ),
                *args,
                **kwargs,
            )
            return model
        return modelapi_register(cast(type[ModelAPI], model_wrapper), model_api)
    def wrapper(
        model_type: type[ModelAPI] | Callable[..., type[ModelAPI]],
    ) -> type[ModelAPI]:
        return create_model_wrapper(model_type, name)
    return wrapper

================
File: src/inspect_ai/model/_render.py
================
from rich.console import RenderableType
from inspect_ai.tool._tool_call import ToolCall
from inspect_ai.tool._tool_transcript import transcript_tool_call
from ._chat_message import (
    ChatMessage,
    ChatMessageAssistant,
    ChatMessageTool,
    ChatMessageUser,
)
def messages_preceding_assistant(messages: list[ChatMessage]) -> list[ChatMessage]:
    preceding: list[ChatMessage] = []
    for m in reversed(messages):
        if not isinstance(m, ChatMessageTool | ChatMessageAssistant) and not (
            isinstance(m, ChatMessageUser) and m.tool_call_id
        ):
            preceding.append(m)
        else:
            break
    return list(reversed(preceding))
def render_tool_calls(tool_calls: list[ToolCall]) -> list[RenderableType]:
    formatted_calls: list[RenderableType] = []
    for call in tool_calls:
        formatted_calls.extend(transcript_tool_call(call))
    return formatted_calls

================
File: src/inspect_ai/scorer/__init__.py
================
from inspect_ai._util.deprecation import relocated_module_attribute
from ._answer import AnswerPattern, answer
from ._choice import choice
from ._classification import exact, f1
from ._match import includes, match
from ._metric import (
    CORRECT,
    INCORRECT,
    NOANSWER,
    PARTIAL,
    Metric,
    MetricProtocol,
    SampleScore,
    Score,
    Value,
    ValueToFloat,
    metric,
    value_to_float,
)
from ._metrics.accuracy import accuracy
from ._metrics.mean import mean
from ._metrics.std import bootstrap_stderr, std, stderr, var
from ._model import model_graded_fact, model_graded_qa
from ._multi import multi_scorer
from ._pattern import pattern
from ._reducer import (
    ScoreReducer,
    ScoreReducers,
    at_least,
    max_score,
    mean_score,
    median_score,
    mode_score,
    pass_at,
    score_reducer,
)
from ._score import score
from ._scorer import Scorer, scorer
from ._target import Target
__all__ = [
    "includes",
    "match",
    "model_graded_qa",
    "model_graded_fact",
    "answer",
    "choice",
    "pattern",
    "f1",
    "exact",
    "AnswerPattern",
    "Scorer",
    "Target",
    "scorer",
    "accuracy",
    "bootstrap_stderr",
    "std",
    "stderr",
    "mean",
    "var",
    "Metric",
    "MetricProtocol",
    "metric",
    "Score",
    "SampleScore",
    "score",
    "Value",
    "ValueToFloat",
    "value_to_float",
    "CORRECT",
    "INCORRECT",
    "PARTIAL",
    "NOANSWER",
    "multi_scorer",
    "ScoreReducer",
    "ScoreReducers",
    "score_reducer",
    "mode_score",
    "mean_score",
    "median_score",
    "max_score",
    "at_least",
    "pass_at",
]
_BOOTSTRAP_RENAME_VERSION = "0.3.58"
_REMOVED_IN = "0.4"
relocated_module_attribute(
    "bootstrap_std",
    "inspect_ai.scorer.bootstrap_stderr",
    _BOOTSTRAP_RENAME_VERSION,
    _REMOVED_IN,
)

================
File: src/inspect_ai/scorer/_answer.py
================
from enum import Enum
from typing import Literal
from inspect_ai._util.pattern import (
    ANSWER_PATTERN_LETTER,
    ANSWER_PATTERN_LINE,
    ANSWER_PATTERN_WORD,
)
from ._metrics import accuracy, stderr
from ._pattern import pattern as make_pattern
from ._scorer import Scorer, scorer
class AnswerPattern(str, Enum):
    """Regular expressions for extracting answers from output.
    These expressions act on output prefixed with "ANSWER: ".
    """
    LETTER = ANSWER_PATTERN_LETTER
    """Extracts a single letter (used with multiple choice)."""
    WORD = ANSWER_PATTERN_WORD
    """Extracts one or more word characters (used for yes/no output)."""
    LINE = ANSWER_PATTERN_LINE
    """Extracts the rest of the line after ANSWER: (used for more complex output).
    Note that when using a LINE pattern your prompt should instruct the
    model to answer with a separate line at the end.
    """
@scorer(metrics=[accuracy(), stderr()])
def answer(pattern: Literal["letter", "word", "line"]) -> Scorer:
    """Scorer for model output that preceded answers with ANSWER:.
    Some solvers including multiple_choice solicit answers from
    the model prefaced with "ANSWER:". This scorer extracts
    answers of this form for comparison with the target.
    Note that you must specify a `type` for the answer scorer.
    Args:
      pattern: Type of answer
        to extract. "letter" is used with multiple choice and
        extracts a single letter; "word" will extract the next
        word (often used for yes/no answers); "line" will take
        the rest of the line (used for more more complex answers
        that may have embedded spaces). Note that when using
        "line" your prompt should instruct the model to answer
        with a separate line at the end.
    """
    match pattern:
        case "letter":
            return make_pattern(AnswerPattern.LETTER)
        case "word":
            return make_pattern(AnswerPattern.WORD)
        case "line":
            return make_pattern(AnswerPattern.LINE)

================
File: src/inspect_ai/scorer/_choice.py
================
from inspect_ai.solver._multiple_choice import (
    answer_character,
    answer_index,
    answer_options,
    unshuffle_choices,
)
from inspect_ai.solver._task_state import Choices, TaskState
from ._metric import CORRECT, INCORRECT, Score
from ._metrics import accuracy, stderr
from ._scorer import Scorer, scorer
from ._target import Target
def _choices_are_shuffled(choices: Choices) -> bool:
    return any(i != choice.original_position for i, choice in enumerate(choices))
def _score_target(target: Target, choices: Choices) -> tuple[list[int], list[str]]:
    target_positions = [
        answer_index(target_character) for target_character in target.text
    ]
    choice_positions = [i for i, choice in enumerate(choices) if choice.correct is True]
    answers = [answer_character(choice) for choice in choice_positions]
    return target_positions, answers
def _shuffled_explanation(choices: Choices) -> str:
    generated_answers = [
        answer_character(i)
        for i, choice in enumerate(choices)
        if choice.correct is True
    ]
    return f"Choices were shuffled before generating a response, the following was sent to the model:\n\n{answer_options(choices)}\nShuffled answer:\nANSWER: {', '.join(generated_answers)}"
@scorer(metrics=[accuracy(), stderr()])
def choice() -> Scorer:
    """
    Scorer for multiple choice answers, required by the `multiple_choice` solver.
    This assumes that the model was called using a template ordered with letters
    corresponding to the answers, so something like:
        What is the capital of France?
        A) Paris
        B) Berlin
        C) London
    The target for the dataset will then have a letter corresponding to the
    correct answer, e.g. the `Target` would be `"A"` for the above question. If
    multiple choices are correct, the `Target` can be an array of these letters.
    """
    async def score(state: TaskState, target: Target) -> Score:
        choices = state.choices
        if _choices_are_shuffled(choices):
            explanation = _shuffled_explanation(choices)
            # Unshuffle the choices so that we can score them correctly against
            # the target
            choices = unshuffle_choices(choices)
        else:
            explanation = state.output.completion
        target_positions, answers = _score_target(target, choices)
        generated_selected_choices = [
            i for i, choice in enumerate(choices) if choice.correct is True
        ]
        target_matches_choices = generated_selected_choices == sorted(target_positions)
        return Score(
            value=CORRECT if target_matches_choices else INCORRECT,
            answer=", ".join(answers),
            explanation=explanation,
        )
    return score

================
File: src/inspect_ai/scorer/_classification.py
================
import re
import string
from typing import Callable, List
from inspect_ai.solver._task_state import TaskState
from ._metric import CORRECT, INCORRECT, Score
from ._metrics import mean, stderr
from ._scorer import Scorer, scorer
from ._target import Target
@scorer(metrics=[mean(), stderr()])
def f1(
    answer_fn: Callable[[str], str] | None = None, stop_words: list[str] | None = None
) -> Scorer:
    """Scorer which produces an F1 score
    Computes the `F1` score for the answer (which balances recall precision by taking the harmonic mean between recall and precision).
    Args:
       answer_fn: Custom function to extract the answer from the completion (defaults to using the completion).
       stop_words: Stop words to include in answer tokenization.
    """
    async def score(state: TaskState, target: Target) -> Score:
        # Get generated answer and extract relevant answer text
        answer = (
            answer_fn(state.output.completion) if answer_fn else state.output.completion
        )
        targets = target.target
        f1_score = max_f1_score(answer, targets, stop_words=stop_words)
        return Score(
            value=f1_score,
            answer=answer,
        )
    return score
@scorer(metrics=[mean(), stderr()])
def exact() -> Scorer:
    """Scorer which produces an exact match score
    Normalizes the text of the answer and target(s) and performs an exact matching comparison of the text. This scorer will return `CORRECT` when the answer is an exact match to one or more targets.
    """
    async def score(state: TaskState, target: Target) -> Score:
        # Get generated answer and extract relevant answer text
        answer = state.output.completion
        targets = target.target
        exact_score = max_exact_score(answer, targets)
        return Score(value=CORRECT if exact_score == 1.0 else INCORRECT, answer=answer)
    return score
def max_f1_score(
    answer: str, targets: List[str], stop_words: list[str] | None = None
) -> float:
    # Find the maximum F1 score for this answer
    max_f1 = 0.0
    for target in targets:
        if target[0].strip():
            f1_score = compute_f1(answer, target, stop_words)
            max_f1 = max(max_f1, f1_score)
    return round(max_f1, 2)
def max_exact_score(answer: str, targets: List[str]) -> float:
    # Find the maximum exact score for this answer
    max_exact = 0.0
    answer_words = _to_words(answer)
    for target in targets:
        if target[0].strip():
            target_words = _to_words(target)
            exact_score = 1.0 if target_words == answer_words else 0.0
            max_exact = max(max_exact, exact_score)
    return max_exact
def compute_f1(answer: str, target: str, stop_words: list[str] | None = None) -> float:
    """Takes a predicted answer and a gold answer (that are both either a string or a list of strings), and returns exact match and the SQuAD F1 metric for the prediction."""
    answer_words = _to_words(answer, stop_words)
    target_words = _to_words(target, stop_words)
    return _f1(answer_words=answer_words, target_words=target_words)
def _to_words(answer: str, stop_words: list[str] | None = None) -> set[str]:
    normalized = _normalize(answer, stop_words)
    token_bag = set(normalized.split())
    return token_bag
def _f1(answer_words: set[str], target_words: set[str]) -> float:
    intersection = len(answer_words.intersection(target_words))
    if not answer_words:
        precision = 1.0
    else:
        precision = intersection / float(len(answer_words))
    if not target_words:
        recall = 1.0
    else:
        recall = intersection / float(len(target_words))
    f1 = (
        (2 * precision * recall) / (precision + recall)
        if not (precision == 0.0 and recall == 0.0)
        else 0.0
    )
    return f1
def _is_number(text: str) -> bool:
    try:
        float(text)
        return True
    except ValueError:
        return False
def _remove_articles(text: str) -> str:
    _ARTICLES = re.compile(r"\b(a|an|the)\b", re.UNICODE)
    return _ARTICLES.sub(" ", text)
def _remove_punc(text: str) -> str:
    exclude = set(string.punctuation)
    is_number = _is_number(text)
    if not is_number:
        return "".join(ch for ch in text if ch not in exclude)
    else:
        return text
def _normalize_whitespace(text: str) -> str:
    return " ".join(text.split())
def _normalize_number(text: str) -> str:
    is_number = _is_number(text)
    if is_number:
        return str(float(text))
    else:
        return text
def _tokenize(text: str) -> List[str]:
    return re.split(" |-", text)
def _normalize(text: str, stop_words: list[str] | None = None) -> str:
    """Normalize text to remove extraneous characters and words."""
    tokens = []
    tokenized_answer = _tokenize(text)
    # Process stop words, if present
    if stop_words is not None:
        folded_stop_words = [_normalize_token(word) for word in stop_words]
    else:
        folded_stop_words = []
    # Now process the text
    for token in tokenized_answer:
        token = _normalize_token(token)
        if folded_stop_words is None or token not in folded_stop_words:
            tokens.append(token)
    # re-join the tokens into a normalized string
    tokens = [token for token in tokens if token.strip()]
    normalized = " ".join(tokens).strip()
    return normalized
def _normalize_token(token: str) -> str:
    token = _remove_punc(token.casefold())
    token = _normalize_number(token)
    token = _remove_articles(token)
    token = _normalize_whitespace(token)
    return token

================
File: src/inspect_ai/scorer/_common.py
================
import re
from typing import Callable, Literal
from inspect_ai._util.text import (
    str_to_float,
    strip_numeric_punctuation,
    strip_punctuation,
)
from inspect_ai.solver._task_state import TaskState
from ._metric import CORRECT, INCORRECT, Score
from ._scorer import Scorer
from ._target import Target
def str_match_scorer(match: Callable[[str, str], tuple[str, bool]]) -> Scorer:
    """Scorer that uses a matching function.
    The matching function returns tuple[str,bool], where str is the answer
    extracted from the model output and bool is whether it matched the target
    """
    async def score(state: TaskState, target: Target) -> Score:
        answer: str | None = None
        for value in target:
            answer, matched = match(state.output.completion, value)
            if matched:
                return Score(
                    value=CORRECT, answer=answer, explanation=state.output.completion
                )
        return Score(
            value=INCORRECT, answer=answer, explanation=state.output.completion
        )
    return score
def match_str(
    value: str,
    target: str,
    location: Literal["begin", "end", "any", "exact"] = "end",
    ignore_case: bool = True,
    ignore_punctuation: bool = True,
    numeric: bool = False,
) -> tuple[str, bool]:
    # strip ws
    v = value.strip()
    t = target.strip()
    # baseline answer (will only change for numeric)
    answer = v
    # further cleanup
    if ignore_case:
        v = v.casefold()
        t = t.casefold()
    if numeric and t.isnumeric():
        # remove punctuation
        v = strip_numeric_punctuation(v)
        t = strip_numeric_punctuation(t)
        # normalize as required
        t = normalize_number(t)
        if location == "begin":
            words = re.split(r"\s+", v)
            v = first_number_normalized(words)
        elif location == "end":
            words = re.split(r"\s+", v)
            words.reverse()
            v = first_number_normalized(words)
        elif location == "exact":
            v = normalize_number(v)
        answer = v
    elif ignore_punctuation:
        v = strip_punctuation(v)
        t = strip_punctuation(t)
    # comparisons
    if location == "begin":
        return answer, v.startswith(t)
    elif location == "end":
        return answer, v.endswith(t)
    elif location == "exact":
        return answer, v == t
    else:
        return answer, t in v
def first_number_normalized(words: list[str]) -> str:
    number = next(
        (word for word in words if word.replace(".", "").isnumeric()), words[0]
    )
    return normalize_number(number)
def normalize_number(number: str, precision: int = 5) -> str:
    if number.replace(".", "").isnumeric():
        num = str_to_float(number)
        return format(num, f".{precision}g")
    else:
        return number

================
File: src/inspect_ai/scorer/_match.py
================
from typing import Literal
from ._common import match_str, str_match_scorer
from ._metrics import accuracy, stderr
from ._scorer import Scorer, scorer
@scorer(metrics=[accuracy(), stderr()])
def match(
    location: Literal["begin", "end", "any", "exact"] = "end",
    *,
    ignore_case: bool = True,
    numeric: bool = False,
) -> Scorer:
    """Scorer which matches text or a number.
    Args:
       location: Location to match at. "any" matches anywhere in the
          output; "exact" requires the output be exactly
          equal to the target (module whitespace, etc.)
       ignore_case: Do case insensitive comparison.
       numeric: Is this a numeric match? (in this
          case different punctuation removal rules are
          used and numbers are normalized before comparison).
    """
    def check(value: str, target: str) -> tuple[str, bool]:
        return match_str(
            value=value,
            target=target,
            location=location,
            ignore_case=ignore_case,
            numeric=numeric,
        )
    return str_match_scorer(check)
@scorer(metrics=[accuracy(), stderr()])
def includes(ignore_case: bool = True) -> Scorer:
    """Check whether the specified text is included in the model output.
    Args:
       ignore_case: Use a case insensitive comparison.
    """
    def check(value: str, target: str) -> tuple[str, bool]:
        if ignore_case:
            value = value.casefold()
            target = target.casefold()
        return value, target in value
    return str_match_scorer(check)

================
File: src/inspect_ai/scorer/_metric.py
================
from dataclasses import dataclass, field
from logging import getLogger
from typing import (
    Any,
    Callable,
    ParamSpec,
    Protocol,
    Union,
    cast,
    overload,
    runtime_checkable,
)
from pydantic import BaseModel, Field
from inspect_ai._util.error import PrerequisiteError
from inspect_ai._util.registry import (
    RegistryInfo,
    is_registry_object,
    registry_add,
    registry_create,
    registry_info,
    registry_name,
    registry_params,
    registry_tag,
)
logger = getLogger(__name__)
CORRECT = "C"
"""Value to assign for correct answers."""
INCORRECT = "I"
"""Value to assign for incorrect answers."""
PARTIAL = "P"
"""Value to assign for partial credit."""
NOANSWER = "N"
"""Value to assign for no answer or refusal to answer."""
Value = Union[
    str | int | float | bool,
    list[str | int | float | bool],
    dict[str, str | int | float | bool | None],
]
"""Value provided by a score.
Use the methods of `Score` to easily treat
the `Value` as a simple scalar of various types.
"""
class Score(BaseModel):
    """Score generated by a scorer."""
    value: Value
    """Score value."""
    answer: str | None = Field(default=None)
    """Answer extracted from model output (optional)"""
    explanation: str | None = Field(default=None)
    """Explanation of score (optional)."""
    metadata: dict[str, Any] | None = Field(default=None)
    """Additional metadata related to the score"""
    @property
    def text(self) -> str:
        """Read the score as text."""
        return self.as_str()
    def as_str(self) -> str:
        """Read the score as a string."""
        return str(self._as_scalar())
    def as_int(self) -> int:
        """Read the score as an integer."""
        return int(self._as_scalar())
    def as_float(self) -> float:
        """Read the score as a float."""
        return float(self._as_scalar())
    def as_bool(self) -> bool:
        """Read the score as a boolean."""
        return bool(self._as_scalar())
    def as_list(self) -> list[str | int | float | bool]:
        """Read the score as a list."""
        if isinstance(self.value, list):
            return self.value
        else:
            raise ValueError("This score is not a list")
    def as_dict(self) -> dict[str, str | int | float | bool | None]:
        """Read the score as a dictionary."""
        if isinstance(self.value, dict):
            return self.value
        else:
            raise ValueError("This score is not a dictionary")
    def _as_scalar(self) -> str | int | float | bool:
        if isinstance(self.value, str | int | float | bool):
            return self.value
        else:
            raise ValueError("This score is not a scalar")
class SampleScore(BaseModel):
    """Score for a Sample."""
    score: Score
    """A score"""
    sample_id: str | int | None = Field(default=None)
    """A sample id"""
    sample_metadata: dict[str, Any] | None = Field(default=None)
    """Metadata from the sample"""
    scorer: str | None = Field(default=None)
    """Registry name of scorer that created this score."""
ValueToFloat = Callable[[Value], float]
"""Function used by metrics to translate from a Score value to a float value."""
def value_to_float(
    correct: Value = CORRECT,
    incorrect: Value = INCORRECT,
    partial: Value = PARTIAL,
    noanswer: Value = NOANSWER,
) -> ValueToFloat:
    """Create a ValueToFloat function.
    Create a ValueToFloat function that maps scalar values of
    different types into floats. For strings, common boolean
    representations (e.g. 'yes', 'no', 'true', 'false') are
    mapped to 1 and 0. In addition, the specified correct,
    incorrect, partial, and noanswer values (by default "C"
    "I", "P", and "N") are mapped to 1, 0, 0.5, and 0. Note that
    those are the default literal values, but they can be
    customized. Strings with only numbers are converted, and
    numeric values are cast to float. Arrays and dictionaries
    give a warning and return 0.
    Args:
       correct (Value): Value that represents a correct answer (1)
       incorrect (Value): Value that represents an incorrect answer (0)
       partial (Value): Value to assign partial credit for (0.5)
       noanswer (Value): Value for refusals to answer (0)
    Returns:
        ValueToFloat function.
    """
    def to_float(value: Value) -> float:
        if isinstance(value, int | float | bool):
            return float(value)
        elif value == correct:
            return 1.0
        elif value == partial:
            return 0.5
        elif value == incorrect or value == noanswer:
            return 0
        elif isinstance(value, str):
            value = value.lower()
            if value in ["yes", "true"]:
                return 1.0
            elif value in ["no", "false"]:
                return 0.0
            elif value.replace(".", "").isnumeric():
                return float(value)
        # couldn't extract a value
        logger.warning(f"Unable to convert value to float: {value}")
        return 0.0
    return to_float
@runtime_checkable
class MetricDeprecated(Protocol):
    def __call__(self, scores: list[Score]) -> Value: ...
@runtime_checkable
class MetricProtocol(Protocol):
    def __call__(self, scores: list[SampleScore]) -> Value:
        r"""Compute a metric on a list of scores.
        Args:
          scores: List of scores.
        Returns:
          Metric value
        Examples:
          ```python
          @metric
          def mean() -> Metric:
              def metric(scores: list[SampleScore]) -> Value:
                  return np.mean([score.score.as_float() for score in scores]).item()
              return metric
          ```
        """
        ...
Metric = MetricProtocol | MetricDeprecated
"""Metric protocol.
The Metric signature changed in release v0.3.64. Both
the previous and new signatures are supported -- you
should use `MetricProtocol` for new code as the
depreacated signature will eventually be removed.
"""
P = ParamSpec("P")
@dataclass(frozen=True)
class MetricSpec:
    """Scorer specification used to (re-)create scorers."""
    metric: str
    """Metric name"""
    args: dict[str, Any] = field(default_factory=dict)
    """Metric arguments."""
def metric_register(metric: Callable[P, Metric], name: str = "") -> Callable[P, Metric]:
    r"""Register a function or class as a metric.
    Args:
        metric (MetricType):
            Function that returns a Metric or class
            deriving fromMetric
        name (str): Name of metric (Optional, defaults to object name)
    Returns:
        Metric type with registry attributes.
    """
    metric_name = name if name else getattr(metric, "__name__")
    registry_add(metric, RegistryInfo(type="metric", name=metric_name))
    return metric
def metric_create(name: str, **kwargs: Any) -> Metric:
    r"""Create a Metric based on its registered name.
    Metrics can be functions that return a Metric or classes
    deriving from Metric
    Args:
        name (str): Name of metric (Optional, defaults to object name)
        **kwargs (dict): Optional creation arguments for the metric
    Returns:
        Metric with registry info attribute
    """
    return cast(Metric, registry_create("metric", name, **kwargs))
def to_metric_specs(
    metrics: list[Metric] | dict[str, list[Metric]],
) -> list[MetricSpec] | dict[str, list[MetricSpec]]:
    if isinstance(metrics, list):
        return [as_metric_spec(m) for m in metrics]
    else:
        return {
            k: [as_metric_spec(v) for v in metric_list]
            for k, metric_list in metrics.items()
        }
def as_metric_spec(metric: Metric) -> MetricSpec:
    if not is_registry_object(metric):
        raise PrerequisiteError(
            f"The metric {getattr(metric, '__name__', '<unknown>')} was not created by a function decorated with @metric so cannot be recorded."
        )
    return MetricSpec(metric=registry_info(metric).name, args=registry_params(metric))
@overload
def metric(name: str) -> Callable[[Callable[P, Metric]], Callable[P, Metric]]: ...
@overload
# type: ignore
def metric(name: Callable[P, Metric]) -> Callable[P, Metric]: ...
def metric(
    name: str | Callable[P, Metric],
) -> Callable[[Callable[P, Metric]], Callable[P, Metric]] | Callable[P, Metric]:
    r"""Decorator for registering metrics.
    Args:
      name: Optional name for metric. If the decorator has no name
        argument then the name of the underlying MetricType
        will be used to automatically assign a name.
    Examples:
      ```python
      @metric
      def mean() -> Metric:
          def metric(scores: list[SampleScore]) -> Value:
              return np.mean([score.score.as_float() for score in scores]).item()
          return metric
    ```
    """
    # create_metric_wrapper:
    #  (a) Add the MetricType to the registry using the appropriately
    #      package-namespaced name
    #  (b) Ensure that instances of Metric created by MetricType also
    #      carry registry info.
    def create_metric_wrapper(
        metric_type: Callable[P, Metric], name: str | None = None
    ) -> Callable[P, Metric]:
        metric_name = registry_name(
            metric_type, name if name else getattr(metric_type, "__name__")
        )
        def metric_wrapper(*args: P.args, **kwargs: P.kwargs) -> Metric:
            metric = metric_type(*args, **kwargs)
            registry_tag(
                metric_type,
                metric,
                RegistryInfo(type="metric", name=metric_name),
                *args,
                **kwargs,
            )
            return metric
        return metric_register(cast(Callable[P, Metric], metric_wrapper), metric_name)
    # for decorators with an explicit name, one more wrapper for the name
    if isinstance(name, str):
        def wrapper(metric_type: Callable[P, Metric]) -> Callable[P, Metric]:
            return create_metric_wrapper(metric_type, name)
        return wrapper
    # create a metric wrapper for the passed metric_type
    else:
        metric_type = name
        return create_metric_wrapper(metric_type)

================
File: src/inspect_ai/scorer/_metrics/__init__.py
================
from .accuracy import accuracy
from .mean import mean
from .std import bootstrap_stderr, std, stderr, var
__all__ = [
    "accuracy",
    "mean",
    "bootstrap_stderr",
    "std",
    "stderr",
    "var",
]

================
File: src/inspect_ai/scorer/_metrics/accuracy.py
================
from logging import getLogger
from .._metric import (
    Metric,
    SampleScore,
    ValueToFloat,
    metric,
    value_to_float,
)
logger = getLogger(__name__)
@metric
def accuracy(to_float: ValueToFloat = value_to_float()) -> Metric:
    r"""Compute proportion of total answers which are correct.
    Args:
       to_float: Function for mapping `Value` to float for computing
          metrics. The default `value_to_float()` maps CORRECT ("C") to 1.0,
          INCORRECT ("I") to 0, PARTIAL ("P") to 0.5, and NOANSWER ("N") to 0,
          casts numeric values to float directly, and prints a warning and returns
          0 if the Value is a complex object (list or dict).
    Returns:
       Accuracy metric
    """
    def metric(scores: list[SampleScore]) -> float:
        total = 0.0
        for item in scores:
            total += to_float(item.score.value)
        return total / float(len(scores))
    return metric

================
File: src/inspect_ai/scorer/_metrics/mean.py
================
import numpy as np
from .._metric import Metric, SampleScore, metric
@metric
def mean() -> Metric:
    """Compute mean of all scores.
    Returns:
       mean metric
    """
    def metric(scores: list[SampleScore]) -> float:
        return np.mean([score.score.as_float() for score in scores]).item()
    return metric

================
File: src/inspect_ai/scorer/_metrics/std.py
================
from logging import getLogger
from typing import cast
import numpy as np
from .._metric import (
    Metric,
    SampleScore,
    ValueToFloat,
    metric,
    value_to_float,
)
logger = getLogger(__name__)
@metric
def bootstrap_stderr(
    num_samples: int = 1000, to_float: ValueToFloat = value_to_float()
) -> Metric:
    """Standard error of the mean using bootstrap.
    Args:
       num_samples: Number of bootstrap samples to take.
       to_float: Function for mapping
          Value to float for computing metrics. The default
          `value_to_float()` maps CORRECT ("C") to 1.0,
          INCORRECT ("I") to 0, PARTIAL ("P") to 0.5, and
          NOANSWER ("N") to 0, casts numeric values to
          float directly, and prints a warning and returns
          0 if the Value is a complex object (list or dict).
    Returns:
       bootstrap_stderr metric
    """
    def metric(scores: list[SampleScore]) -> float:
        values = [to_float(score.score.value) for score in scores]
        std = np.std(
            [
                np.mean(np.random.choice(values, len(values), replace=True))
                for _ in range(num_samples)
            ]
        )
        return cast(float, std.item())
    return metric
@metric
def stderr(
    to_float: ValueToFloat = value_to_float(), cluster: str | None = None
) -> Metric:
    """Standard error of the mean using Central Limit Theorem.
    Args:
       to_float: Function for mapping `Value` to float for computing
          metrics. The default `value_to_float()` maps CORRECT ("C") to 1.0,
          INCORRECT ("I") to 0, PARTIAL ("P") to 0.5, and NOANSWER ("N") to 0,
          casts numeric values to float directly, and prints a warning and returns
          0 if the Value is a complex object (list or dict).
       cluster (str | None): The key from the Sample metadata
          corresponding to a cluster identifier for computing
          [clustered standard errors](https://en.wikipedia.org/wiki/Clustered_standard_errors).
    Returns:
       stderr metric
    """
    def clustered_metric(scores: list[SampleScore]) -> float:
        """Computes a clustered standard error.
        For details, see Appendix A of https://arxiv.org/pdf/2411.00640.
        The version here uses a finite cluster correction (unlike the paper)
        """
        assert cluster is not None
        cluster_list = []
        value_list = []
        for sample_score in scores:
            if (
                sample_score.sample_metadata is None
                or cluster not in sample_score.sample_metadata
            ):
                raise ValueError(
                    f"Sample {sample_score.sample_id} has no cluster metadata. To compute `stderr` with clustering, each sample metadata must have a value for '{cluster}'"
                )
            cluster_list.append(sample_score.sample_metadata[cluster])
            value_list.append(to_float(sample_score.score.value))
        clusters = np.array(cluster_list)
        values = np.array(value_list)
        mean = float(np.mean(values))
        # Convert to numpy arrays and get unique clusters
        unique_clusters = np.unique(clusters)
        cluster_count = len(unique_clusters)
        # Compute clustered variance using NumPy operations
        clustered_variance = 0.0
        for cluster_id in unique_clusters:
            # get a data vector for this cluster
            cluster_data = values[clusters == cluster_id]
            # this computes X' \Omega X = \sum_i \sum_j (s_{i,c} - mean) * (s_{j,c} - mean)
            clustered_variance += np.outer(
                cluster_data - mean, cluster_data - mean
            ).sum()
        # Multiply by C / (C - 1) to unbias the variance estimate
        standard_error = np.sqrt(
            clustered_variance * cluster_count / (cluster_count - 1)
        ) / len(scores)
        return cast(float, standard_error)
    def metric(scores: list[SampleScore]) -> float:
        values = [to_float(score.score.value) for score in scores]
        n = len(values)
        # standard deviation is calculated by dividing by n-ddof so ensure
        # that we won't divide by zero
        if (n - 1) < 1:
            return 0
        # Calculate the sample standard deviation
        sample_std = np.std(values, ddof=1)
        # Calculate the standard error of the mean
        standard_error = sample_std / np.sqrt(n)
        return cast(float, standard_error)
    if cluster is not None:
        return clustered_metric
    return metric
@metric
def std(to_float: ValueToFloat = value_to_float()) -> Metric:
    """Calculates the sample standard deviation of a list of scores.
    Args:
       to_float: Function for mapping `Value` to float for computing
          metrics. The default `value_to_float()` maps CORRECT ("C") to 1.0,
          INCORRECT ("I") to 0, PARTIAL ("P") to 0.5, and NOANSWER ("N") to 0,
          casts numeric values to float directly, and prints a warning and returns
          0 if the Value is a complex object (list or dict).
    Returns:
        std metric
    """
    def metric(scores: list[SampleScore]) -> float:
        values = [to_float(score.score.value) for score in scores]
        n = len(values)
        # standard deviation is calculated by dividing by n-ddof so ensure
        # that we won't divide by zero
        if (n - 1) < 1:
            return 0
        # Calculate the sample standard deviation
        sample_std = np.std(values, ddof=1)
        return cast(float, sample_std)
    return metric
@metric
def var(to_float: ValueToFloat = value_to_float()) -> Metric:
    """Compute the sample variance of a list of scores.
    Args:
        to_float (ValueToFloat): Function for mapping
            Value to float for computing metrics. The default
            `value_to_float()` maps CORRECT ("C") to 1.0,
            INCORRECT ("I") to 0, PARTIAL ("P") to 0.5, and
            NOANSWER ("N") to 0, casts numeric values to
            float directly, and prints a warning and returns
            0 if the Value is a complex object (list or dict).
    Returns:
       var metric
    """
    def metric(scores: list[SampleScore]) -> float:
        values = [to_float(score.score.value) for score in scores]
        n = len(values)
        # variance is calculated by dividing by n-ddof so ensure
        # that we won't divide by zero
        if (n - 1) < 1:
            return 0
        variance = np.var(values, ddof=1)
        return cast(float, variance)
    return metric

================
File: src/inspect_ai/scorer/_model.py
================
import re
from functools import partial
from typing import Callable
from inspect_ai._util.dict import omit
from inspect_ai._util.format import format_function_call
from inspect_ai._util.list import remove_last_match_and_after
from inspect_ai.model._chat_message import (
    ChatMessage,
    ChatMessageAssistant,
    ChatMessageSystem,
    ChatMessageTool,
    ChatMessageUser,
)
from inspect_ai.model._model import Model, get_model
from inspect_ai.solver._task_state import TaskState
from inspect_ai.util import resource
from ._metric import INCORRECT, Score
from ._metrics import accuracy, stderr
from ._multi import multi_scorer
from ._scorer import Scorer, scorer
from ._target import Target
@scorer(metrics=[accuracy(), stderr()])
def model_graded_fact(
    template: str | None = None,
    instructions: str | None = None,
    grade_pattern: str | None = None,
    include_history: bool | Callable[[TaskState], str] = False,
    partial_credit: bool = False,
    model: list[str | Model] | str | Model | None = None,
) -> Scorer:
    """Score a question/answer task with a fact response using a model.
    Args:
      template: Template for grading prompt. This template uses
        four variables: `question`, `criterion`, `answer`, and
        `instructions` (which is fed from the `instructions` parameter).
        Variables from sample `metadata` are also available in the template.
      instructions: Grading instructions. This should
        include a prompt for the model to answer (e.g. with
        with chain of thought reasoning) in a way that matches
        the specified `grade_pattern`, for example, the default
        `grade_pattern` looks for one of GRADE: C, GRADE: P, or
        GRADE: I).
      grade_pattern: Regex to extract the grade from the
        model response. Defaults to looking for e.g. GRADE: C
        The regex should have a single capture group that
        extracts exactly the letter C, P, or I.
      include_history:
        Whether to include the full chat history in the presented
        question. Defaults to `False`, which presents only the
        original sample input. Optionally provide a function to
        customise how the chat history is presented.
      partial_credit: Whether to allow for "partial" credit for
         answers (by default assigned a score of 0.5). Defaults
         to `False`. Note that this parameter is only used
         with the default `instructions` (as custom instructions
         provide their own prompts for grades).
      model: Model or Models to use for grading. If multiple models are passed, a majority vote of their grade will be returned. By default the model being evaluated is used.
    """
    return model_graded_qa(
        template=template if template else DEFAULT_MODEL_GRADED_FACT_TEMPLATE,
        instructions=instructions,
        grade_pattern=grade_pattern,
        include_history=include_history,
        partial_credit=partial_credit,
        model=model,
    )
@scorer(metrics=[accuracy(), stderr()])
def model_graded_qa(
    template: str | None = None,
    instructions: str | None = None,
    grade_pattern: str | None = None,
    include_history: bool | Callable[[TaskState], str] = False,
    partial_credit: bool = False,
    model: list[str | Model] | str | Model | None = None,
) -> Scorer:
    """Score a question/answer task using a model.
    Args:
      template: Template for grading prompt. This template has
        four variables:
           - `question`, `criterion`, `answer`, and
        `instructions` (which is fed from the `instructions` parameter).
        Variables from sample `metadata` are also available in the template.
      instructions: Grading instructions. This should
        include a prompt for the model to answer (e.g. with
        with chain of thought reasoning) in a way that matches
        the specified `grade_pattern`, for example, the default
        `grade_pattern` looks for one of GRADE: C, GRADE: P, or
        GRADE: I.
      grade_pattern: Regex to extract the grade from the
        model response. Defaults to looking for e.g. GRADE: C
        The regex should have a single capture group that
        extracts exactly the letter C, P, I.
      include_history:
        Whether to include the full chat history in the presented
        question. Defaults to `False`, which presents only the
        original sample input. Optionally provide a function to
        customise how the chat history is presented.
      partial_credit: Whether to allow for "partial" credit for
        answers (by default assigned a score of 0.5). Defaults
        to `False`. Note that this parameter is only used
        with the default `instructions` (as custom instructions
        provide their own prompts for grades).
      model: Model or Models to use for grading. If     multiple models are passed, a majority vote of their grade will be returned. By default the model being evaluated is used.
    """
    # bind variables
    get_scorer = partial(
        _model_graded_qa_single,
        template,
        instructions,
        grade_pattern,
        include_history,
        partial_credit,
    )
    # if only a single model is passed, return a single scorer
    if model is None or not isinstance(model, list):
        return get_scorer(model)
    # otherwise, use multi scorer
    assert isinstance(model, list)
    scorers = [get_scorer(model) for model in model]
    return multi_scorer(scorers, "mode")
@scorer(metrics=[accuracy(), stderr()])
def _model_graded_qa_single(
    template: str | None = None,
    instructions: str | None = None,
    grade_pattern: str | None = None,
    include_history: bool | Callable[[TaskState], str] = False,
    partial_credit: bool = False,
    model: str | Model | None = None,
) -> Scorer:
    # returns a scorer that does model graded qa for a single model
    # resolve grading template, instructions, and grade_pattern
    template = template if template else DEFAULT_MODEL_GRADED_QA_TEMPLATE
    grading_template = resource(template)
    instructions = (
        instructions if instructions else default_instructions(partial_credit)
    )
    async def score(state: TaskState, target: Target) -> Score:
        # resolve model
        nonlocal model
        model = model if isinstance(model, Model) else get_model(model)
        # metadata without grading template variables
        metadata = omit(
            state.metadata, ["question", "answer", "criterion", "instructions"]
        )
        # present the question
        if include_history is True:
            question = chat_history(state)
        elif callable(include_history):
            question = include_history(state)
        else:
            question = state.input_text
        # format the scoring template
        score_prompt = grading_template.format(
            question=question,
            answer=state.output.completion,
            criterion=target.text,
            instructions=instructions,
            **metadata,
        )
        # query the model for the score
        result = await model.generate(score_prompt)
        # extract the grade
        match = re.search(grade_pattern or DEFAULT_GRADE_PATTERN, result.completion)
        if match:
            return Score(
                value=match.group(1),
                answer=state.output.completion,
                explanation=result.completion,
                metadata=dict(
                    grading=[
                        ChatMessageUser(content=score_prompt),
                        result.message,
                    ]
                ),
            )
        else:
            return Score(
                value=INCORRECT,
                explanation="Grade not found in model output: "
                + f"{result.completion}",
            )
    return score
# these templates are based on the openai closedqa templates here:
# https://github.com/openai/evals/blob/main/evals/registry/modelgraded/closedqa.yaml
DEFAULT_MODEL_GRADED_QA_TEMPLATE = """
You are assessing a submitted answer on a given task based on a criterion. Here is the data:
[BEGIN DATA]
***
[Task]: {question}
***
[Submission]: {answer}
***
[Criterion]: {criterion}
***
[END DATA]
Does the submission meet the criterion?
{instructions}
"""
DEFAULT_MODEL_GRADED_FACT_TEMPLATE = """
You are comparing a submitted answer to an expert answer on a given question. Here is the data:
[BEGIN DATA]
************
[Question]: {question}
************
[Expert]: {criterion}
************
[Submission]: {answer}
************
[END DATA]
Compare the factual content of the submitted answer with the expert answer. Ignore any differences in style, grammar, or punctuation.
Does the submission contain the content in the expert answer?
{instructions}
"""
def default_instructions(partial_credit: bool) -> str:
    partial_letter = "P" if partial_credit else ""
    partial_prompt = '"P" for partially correct answers,' if partial_credit else ""
    return f"""
After assessing the submitted answer, reply with 'GRADE: $LETTER' (without quotes) where LETTER is one of C{partial_letter}I.  Please choose ONE option for the grade: either "C" for correct answers, {partial_prompt}or "I" for incorrect answers.
For example, after reviewing a correct answer you might write 'GRADE: C' or after reviewing an incorrect answer you might write 'GRADE: I'.
First, write out in a step by step manner your reasoning about the criterion to be sure that your conclusion is correct. Avoid simply stating the correct answers at the outset. Then, end with your answer formatted as 'GRADE: $LETTER' (without quotes) where LETTER is one of C{partial_letter}I.
"""
DEFAULT_GRADE_PATTERN = r"(?i)GRADE\s*:\s*([CPI])(.*)$"
"""Regex to extract the grade from the COT above."""
def chat_history(state: TaskState) -> str:
    # filter out system messages
    messages: list[ChatMessage] = [
        message
        for message in state.messages
        if not isinstance(message, ChatMessageSystem)
    ]
    # present message history (removing the final assistant message
    # and after as it will be contained in the 'Answer:'):
    messages = remove_last_match_and_after(
        messages, lambda message: isinstance(message, ChatMessageAssistant)
    )
    # begin history with text of first message (it will come right after
    # 'Task' or 'Question' in the template)
    history: list[str] = []
    if len(messages) > 0:
        history.append(messages[0].text)
        # for subsequent messages present with e.g. Assistant: {message.text}
        for message in messages[1:]:
            if isinstance(message, ChatMessageUser):
                history.append(f"User: {message.text}")
            elif isinstance(message, ChatMessageAssistant):
                assistant_message = [message.text] if message.text else []
                if message.tool_calls:
                    assistant_message.extend(
                        [
                            format_function_call(
                                tool_call.function, tool_call.arguments
                            )
                            for tool_call in message.tool_calls
                        ]
                    )
                history.append("Assistant: " + "\n\n".join(assistant_message))
            elif isinstance(message, ChatMessageTool):
                history.append(
                    f"Tool ({message.function}): {message.tool_error or ''}{message.text}"
                )
    return "\n\n".join(history)

================
File: src/inspect_ai/scorer/_multi.py
================
import functools
from inspect_ai._util._async import tg_collect
from inspect_ai.scorer._reducer.registry import create_reducers
from inspect_ai.solver._task_state import TaskState
from ._metric import Score
from ._reducer.types import ScoreReducer
from ._scorer import Scorer
from ._target import Target
def multi_scorer(scorers: list[Scorer], reducer: str | ScoreReducer) -> Scorer:
    r"""Returns a Scorer that runs multiple Scorers in parallel and aggregates their results into a single Score using the provided reducer function.
    Args:
        scorers: a list of Scorers.
        reducer: a function which takes in a list of Scores and returns a single Score.
    """
    reducer = create_reducers(reducer)[0]
    async def score(state: TaskState, target: Target) -> Score:
        scores = await tg_collect(
            [functools.partial(_scorer, state, target) for _scorer in scorers]
        )
        return reducer(scores)
    return score

================
File: src/inspect_ai/scorer/_pattern.py
================
import re
from typing import Any
from inspect_ai.solver._task_state import TaskState
from ._metric import CORRECT, INCORRECT, Score
from ._metrics import accuracy, stderr
from ._scorer import Scorer, scorer
from ._target import Target
def match_target(match: str, target: Target, ignore_case: bool) -> bool:
    if ignore_case:
        match = match.lower()
        target = Target([t.lower() for t in target])
    return match in target
def match_first(
    matches: tuple[str | Any, ...], target: Target, ignore_case: bool
) -> str | None:
    for match in matches:
        if not isinstance(match, str):
            continue
        if match_target(match, target, ignore_case):
            return match
    return None
def match_all_groups(
    matches: tuple[str | Any, ...], target: Target, ignore_case: bool
) -> str | None:
    for match in matches:
        if not isinstance(match, str):
            continue
        if not match_target(match, target, ignore_case):
            return None
    return target.text
@scorer(metrics=[accuracy(), stderr()])
def pattern(pattern: str, ignore_case: bool = True, match_all: bool = False) -> Scorer:
    """Scorer which extracts the model answer using a regex.
    Note that at least one regex group is required to match
    against the target.
    The regex can have a single capture group or multiple groups.
    In the case of multiple groups, the scorer can be configured
    to match either one or all of the extracted groups
    Args:
       pattern: Regular expression for extracting the
          answer from model output.
       ignore_case: Ignore case when comparing
          the extract answer to the targets. (Default: True)
       match_all: With multiple captures, do all captured
          values need to match the target? (Default: False)
    """
    async def score(state: TaskState, target: Target) -> Score:
        # extract the answer
        match = re.search(
            pattern, state.output.completion, re.IGNORECASE if ignore_case else 0
        )
        if match:
            groups = match.groups()
            if match_all:
                found_match = match_all_groups(
                    matches=groups, target=target, ignore_case=ignore_case
                )
                answer = found_match
            else:
                found_match = match_first(
                    matches=groups, target=target, ignore_case=ignore_case
                )
                if found_match is None and len(groups) == 1:
                    # A common use of a pattern is to extract a single answer
                    # from some templated text. If we fail to match in that
                    # scenario, it's worth returning the failed match because
                    # this is useful information for the user.
                    answer = groups[0]
                else:
                    answer = found_match
            return Score(
                value=CORRECT if found_match else INCORRECT,
                answer=answer,
                explanation=state.output.completion,
            )
        else:
            # didn't find the scoring pattern
            return Score(
                value=INCORRECT,
                explanation="Scoring pattern not matched in output: "
                + f"{state.output.completion}",
            )
    return score

================
File: src/inspect_ai/scorer/_reducer/__init__.py
================
from .reducer import at_least, max_score, mean_score, median_score, mode_score, pass_at
from .registry import (
    create_reducers,
    reducer_log_name,
    reducer_log_names,
    score_reducer,
    validate_reducer,
)
from .types import ScoreReducer, ScoreReducers
__all__ = [
    "ScoreReducer",
    "ScoreReducers",
    "score_reducer",
    "create_reducers",
    "reducer_log_name",
    "reducer_log_names",
    "mean_score",
    "median_score",
    "mode_score",
    "max_score",
    "at_least",
    "pass_at",
    "validate_reducer",
]

================
File: src/inspect_ai/scorer/_reducer/reducer.py
================
import statistics
from collections import Counter
from typing import Callable, cast
import numpy as np
from inspect_ai.scorer._metric import Score, Value, ValueToFloat, value_to_float
from .registry import REDUCER_NAME, score_reducer
from .types import ScoreReducer
@score_reducer(name="mode")
def mode_score() -> ScoreReducer:
    r"""Take the mode from a list of scores."""
    def reduce(scores: list[Score]) -> Score:
        r"""A utility function for the most common score in a list of scores.
        Args:
            scores: a list of Scores.
        """
        def most_common(
            counts: Counter[str | int | float | bool],
        ) -> str | int | float | bool:
            return counts.most_common(1)[0][0]
        if isinstance(scores[0].value, dict):
            return _count_dict(scores, most_common)
        elif isinstance(scores[0].value, list):
            return _count_list(scores, most_common)
        else:
            return _count_scalar(scores, most_common)
    return reduce
@score_reducer(name="mean")
def mean_score(value_to_float: ValueToFloat = value_to_float()) -> ScoreReducer:
    r"""Take the mean of a list of scores.
    Args:
       value_to_float: Function to convert the value to a float
    """
    def reduce(scores: list[Score]) -> Score:
        if isinstance(scores[0].value, dict):
            return _compute_dict_stat(scores, value_to_float, statistics.mean)
        elif isinstance(scores[0].value, list):
            return _compute_list_stat(scores, value_to_float, statistics.mean)
        else:
            return _compute_scalar_stat(scores, value_to_float, statistics.mean)
    return reduce
@score_reducer(name="median")
def median_score(value_to_float: ValueToFloat = value_to_float()) -> ScoreReducer:
    r"""Take the median value from a list of scores.
    Args:
       value_to_float: Function to convert the value to a float
    """
    def reduce(scores: list[Score]) -> Score:
        if isinstance(scores[0].value, dict):
            return _compute_dict_stat(scores, value_to_float, statistics.median)
        elif isinstance(scores[0].value, list):
            return _compute_list_stat(scores, value_to_float, statistics.median)
        else:
            return _compute_scalar_stat(scores, value_to_float, statistics.median)
    return reduce
@score_reducer
def at_least(
    k: int, value: float = 1.0, value_to_float: ValueToFloat = value_to_float()
) -> ScoreReducer:
    r"""Score correct if there are at least k score values greater than or equal to the value.
    Args:
       k: Number of score values that must exceed `value`.
       value: Score value threshold.
       value_to_float: Function to convert score values to float.
    """
    def reduce(scores: list[Score]) -> Score:
        def gte_n(
            counter: Counter[str | int | float | bool],
        ) -> str | int | float | bool:
            count_gte_n = sum(
                count for key, count in counter.items() if value_to_float(key) >= value
            )
            return 1 if count_gte_n >= k else 0
        if isinstance(scores[0].value, dict):
            return _count_dict(scores, gte_n)
        elif isinstance(scores[0].value, list):
            return _count_list(scores, gte_n)
        else:
            return _count_scalar(scores, gte_n)
    setattr(at_least, REDUCER_NAME, f"at_least_{k}")
    return reduce
@score_reducer
def pass_at(
    k: int, value: float = 1.0, value_to_float: ValueToFloat = value_to_float()
) -> ScoreReducer:
    r"""Probability of at least 1 correct sample given `k` epochs (<https://arxiv.org/pdf/2107.03374>).
    Args:
       k: Epochs to compute probability for.
       value: Score value threshold.
       value_to_float: Function to convert score values to float.
    """
    def reduce(scores: list[Score]) -> Score:
        def pass_at_k(values: list[float]) -> float:
            total = len(scores)
            correct = sum(1 for v in values if v == value)
            if total - correct < k:
                return 1.0
            else:
                return 1.0 - cast(  # type: ignore[redundant-cast]
                    float,
                    np.prod(1.0 - k / np.arange(total - correct + 1, total + 1)).item(),
                )
        if isinstance(scores[0].value, dict):
            return _compute_dict_stat(scores, value_to_float, pass_at_k)
        elif isinstance(scores[0].value, list):
            return _compute_list_stat(scores, value_to_float, pass_at_k)
        else:
            return _compute_scalar_stat(scores, value_to_float, pass_at_k)
    setattr(pass_at, REDUCER_NAME, f"pass_at_{k}")
    return reduce
@score_reducer(name="max")
def max_score(value_to_float: ValueToFloat = value_to_float()) -> ScoreReducer:
    r"""Take the maximum value from a list of scores.
    Args:
       value_to_float: Function to convert the value to a float
    """
    def reduce(scores: list[Score]) -> Score:
        if isinstance(scores[0].value, dict):
            dict_result: dict[str, str | int | float | bool | None] = {}
            keys = scores[0].value.keys()  # type: ignore
            for key in keys:
                max_value = max(
                    [score.value[key] for score in scores],  # type: ignore
                    key=value_to_float,  # type: ignore
                )
                dict_result[key] = max_value
            return _reduced_score(dict_result, scores)
        elif isinstance(scores[0].value, list):
            list_result: list[str | int | float | bool] = []
            list_size = len(scores[0].value)  # type: ignore
            for i in range(list_size):
                max_value = max(
                    [score.value[i] for score in scores],  # type:ignore
                    key=value_to_float,  # type: ignore
                )
                if max_value is None:
                    raise ValueError(
                        "List of scores values unexpectedly had a `None` max score"
                    )
                else:
                    list_result.append(max_value)
            return _reduced_score(list_result, scores)
        else:
            max_score = max(scores, key=lambda score: value_to_float(score.value))
            return _reduced_score(max_score.value, scores)
    return reduce
def _count_scalar(
    scores: list[Score],
    counter_fn: Callable[[Counter[str | int | float | bool]], str | int | float | bool],
) -> Score:
    r"""Counts scores and provides Counter to a counter_fn
    Args:
        scores: a list of Scores.
        counter_fn: a function which returns a scalar value based upon the counter
    """
    counts = Counter([score._as_scalar() for score in scores])
    return _reduced_score(counter_fn(counts), scores)
def _count_dict(
    scores: list[Score],
    counter_fn: Callable[[Counter[str | int | float | bool]], str | int | float | bool],
) -> Score:
    r"""Counts scores within a dictionary and provides Counter (for each key) to a counter_fn
    Args:
        scores: a list of Scores.
        counter_fn: a function which returns a scalar value based upon the counter
    """
    # Make sure these are all dictionaries be we proceed
    _check_value_dict(scores)
    dict_result: dict[str, str | int | float | bool] = {}
    keys = scores[0].value.keys()  # type: ignore
    for key in keys:
        counts: Counter[str | int | float | bool] = Counter(
            [score.value[key] for score in scores]  # type: ignore
        )
        dict_result[key] = counter_fn(counts)
    return _reduced_score(
        cast(dict[str, str | int | float | bool | None], dict_result), scores
    )
def _count_list(
    scores: list[Score],
    counter_fn: Callable[[Counter[str | int | float | bool]], str | int | float | bool],
) -> Score:
    r"""Counts scores within a list and provides Counter (for each index) to a counter_fn
    Args:
        scores: a list of Scores.
        counter_fn: a function which returns a scalar value based upon the counter
    """
    # Make sure these are all lists before we continue
    _check_value_list(scores)
    list_result: list[str | int | float | bool] = []
    list_size = len(scores[0].value)  # type: ignore
    for i in range(list_size):
        counts: Counter[str | int | float | bool] = Counter(
            [score.value[i] for score in scores]  # type:ignore
        )
        list_result.append(counter_fn(counts))
    return _reduced_score(list_result, scores)
def _compute_dict_stat(
    scores: list[Score],
    value_to_float: ValueToFloat,
    statistic: Callable[[list[float]], float],
) -> Score:
    r"""Applies a statistic function to reduce key by key a dictionary
    Args:
        scores: a list of Scores.
        value_to_float: Function to convert the value to a float
        statistic: the statistic to apply
    """
    # Make sure these are all dictionaries be we proceed
    _check_value_dict(scores)
    dict_result: dict[str, str | int | float | bool | None] = {}
    for key in scores[0].value.keys():  # type: ignore
        values = [value_to_float(score.value[key]) for score in scores]  # type: ignore
        dict_result[key] = statistic(values)
    return _reduced_score(dict_result, scores)
def _compute_list_stat(
    scores: list[Score],
    value_to_float: ValueToFloat,
    statistic: Callable[[list[float]], float],
) -> Score:
    r"""Applies a statistic function to reduce index by index a list
    Args:
        scores: a list of Scores.
        value_to_float: function to convert the value to a float
        statistic: the statistic to apply
    """
    # Make sure these are all lists before we continue
    _check_value_list(scores)
    list_result: list[str | int | float | bool] = []
    list_size = len(scores[0].value)  # type: ignore
    for i in range(list_size):
        values = [value_to_float(score.value[i]) for score in scores]  # type: ignore
        list_result.append(statistic(values))
    return _reduced_score(list_result, scores)
def _compute_scalar_stat(
    scores: list[Score],
    value_to_float: ValueToFloat,
    statistic: Callable[[list[float]], float],
) -> Score:
    r"""Applies a statistic function to reduce scalar scores
    Args:
        scores: a list of Scores.
        value_to_float: function to convert the value to a float
        statistic: the statistic to apply
    """
    values = [value_to_float(score.value) for score in scores]
    result = statistic(values)
    return _reduced_score(result, scores)
def _check_value_dict(scores: list[Score]) -> None:
    r"""Ensure that all score values are dictionaries
    Args:
        scores: a list of Scores.
    """
    for score in scores:
        if not isinstance(score.value, dict):
            raise ValueError(
                "Attempting to reduce a dictionary score for a non-dictionary value"
            )
def _check_value_list(scores: list[Score]) -> None:
    r"""Ensure that all score values are lists
    Args:
        scores: a list of Scores.
    """
    for score in scores:
        if not isinstance(score.value, list):
            raise ValueError("Attempting to reduce a list score for a non-list value")
def _reduced_score(value: Value, scores: list[Score]) -> Score:
    r"""Create a Score based upon a single Value and list of Scores that produced it
    Args:
        value: the reduced Value
        scores: ths list of scores being reduced
    """
    return Score(
        value=value,
        # retain remaining fields only if equal across all Scores
        answer=scores[0].answer
        if len(set(score.answer for score in scores)) == 1
        else None,
        explanation=scores[0].explanation
        if len(set(score.explanation for score in scores)) == 1
        else None,
        metadata=scores[0].metadata,
    )

================
File: src/inspect_ai/scorer/_reducer/registry.py
================
import re
from typing import Any, Callable, TypeVar, cast, overload
from inspect_ai._util.error import PrerequisiteError
from inspect_ai._util.registry import (
    RegistryInfo,
    registry_add,
    registry_create,
    registry_info,
    registry_log_name,
    registry_name,
    registry_params,
    registry_tag,
    set_registry_info,
)
from .types import ScoreReducer, ScoreReducers
REDUCER_NAME = "__REDUCER_NAME__"
ScoreReducerType = TypeVar("ScoreReducerType", bound=Callable[..., ScoreReducer])
@overload
def score_reducer(func: ScoreReducerType) -> ScoreReducerType: ...
@overload
def score_reducer() -> Callable[[ScoreReducerType], ScoreReducerType]: ...
@overload
def score_reducer(*, name: str) -> Callable[[ScoreReducerType], ScoreReducerType]: ...
def score_reducer(
    func: ScoreReducerType | None = None, *, name: str | None = None
) -> Callable[[ScoreReducerType], ScoreReducerType] | ScoreReducerType:
    """Decorator for registering Score Reducers.
    Args:
        func: Function returning `ScoreReducer` targeted by
            plain task decorator without attributes (e.g. `@score_reducer`)
        name: Optional name for reducer. If the decorator has no name
            argument then the name of the function will be used to automatically assign a name.
    Returns:
        ScoreReducer with registry attributes or a decorator function.
    """
    def create_reducer_wrapper(reducer_type: ScoreReducerType) -> ScoreReducerType:
        # get the name and params
        nonlocal name
        reducer_name = name or registry_name(
            reducer_type, getattr(reducer_type, "__name__")
        )
        # create and return the wrapper
        def wrapper(*w_args: Any, **w_kwargs: Any) -> ScoreReducer:
            # create the task
            score_reducer = reducer_type(*w_args, **w_kwargs)
            # If a name has been explicitly set, use that
            reducer_nm = getattr(score_reducer, REDUCER_NAME, reducer_name)
            # tag it
            registry_tag(
                reducer_type,
                score_reducer,
                RegistryInfo(
                    type="score_reducer",
                    name=reducer_nm,
                ),
                *w_args,
                **w_kwargs,
            )
            # return it
            return score_reducer
        return reducer_register(
            score_reducer=cast(ScoreReducerType, wrapper), name=reducer_name
        )
    if func is not None:
        return create_reducer_wrapper(func)
    else:
        return create_reducer_wrapper
def reducer_register(score_reducer: ScoreReducerType, name: str) -> ScoreReducerType:
    r"""Register a task.
    Args:
        score_reducer (ReducerType):
            function that returns a Task or class
            deriving from Task
        name (str): Name of task
    Returns:
        ScoreReducer with registry attributes.
    """
    registry_add(
        score_reducer,
        RegistryInfo(type="score_reducer", name=name),
    )
    return score_reducer
def reducer_log_names(
    reducer: ScoreReducer | list[ScoreReducer] | None,
) -> list[str] | None:
    reducer = [reducer] if isinstance(reducer, ScoreReducer) else reducer
    if reducer is not None:
        names = [reducer_log_name(r) for r in reducer]
        return names
    else:
        return None
def reducer_log_name(reducer: ScoreReducer) -> str:
    name = registry_log_name(reducer)
    params = registry_params(reducer)
    if "k" in params:
        name = f"{name}_{params.get('k')}"
    return name
@overload
def create_reducers(reducers: ScoreReducers) -> list[ScoreReducer]: ...
@overload
def create_reducers(reducers: None) -> None: ...
def create_reducers(reducers: ScoreReducers | None) -> list[ScoreReducer] | None:
    if reducers is None:
        return None
    def create_reducer(name: str) -> ScoreReducer:
        # special case to get digit parameters
        params: dict[str, Any] = {}
        match = re.match(r"^(.*?)_(\d+)$", name)
        if match:
            name = match.group(1)
            params["k"] = int(match.group(2))
        return cast(
            Callable[..., ScoreReducer], registry_create("score_reducer", name)
        )(**params)
    if isinstance(reducers, ScoreReducer):
        return [reducers]
    elif isinstance(reducers, str):
        return [create_reducer(reducers)]
    else:
        return [
            r if isinstance(r, ScoreReducer) else create_reducer(r) for r in reducers
        ]
def set_reducer_name(reducer: ScoreReducer, name: str) -> None:
    info = registry_info(reducer)
    set_registry_info(
        reducer,
        RegistryInfo(
            type="score_reducer",
            name=name,
            metadata=info.metadata,
        ),
    )
def validate_reducer(epochs: int, reducer: ScoreReducer) -> None:
    params = registry_params(reducer)
    if "k" in params:
        k = int(params["k"])
        if k > epochs:
            name = registry_log_name(reducer)
            # don't interfere w/ unknown uses of 'k' (i.e. only validate built in)
            if name.startswith("pass_at") or name.startswith("at_least"):
                raise PrerequisiteError(
                    f"Reducer '{name}_{k}' requires {k} epochs however evaluation has only {epochs} epochs."
                )

================
File: src/inspect_ai/scorer/_reducer/types.py
================
from typing import Protocol, runtime_checkable
from .._metric import Score
@runtime_checkable
class ScoreReducer(Protocol):
    def __call__(self, scores: list[Score]) -> Score:
        """Reduce a set of scores to a single score.
        Args:
          scores: List of scores.
        """
        ...
    @property
    def __name__(self) -> str: ...
ScoreReducers = str | ScoreReducer | list[str] | list[ScoreReducer]
r"""One or more score reducers."""

================
File: src/inspect_ai/scorer/_score.py
================
from contextvars import ContextVar
from inspect_ai.solver._task_state import TaskState
from ._metric import Score
from ._scorer import Scorer
from ._target import Target
async def score(state: TaskState) -> list[Score]:
    """Score a TaskState.
    Score a task state from within a solver.
    Args:
      state (TaskState): `TaskState` to submit for scoring
    Returns:
      List of scores (one for each task scorer)
    Raises:
      RuntimerError: If called from outside a task or within
        a task that does not have a scorer.
    """
    from inspect_ai.log._transcript import ScoreEvent, transcript
    scorers = _scorers.get(None)
    target = _target.get(None)
    if scorers is None or target is None:
        raise RuntimeError(
            "The score() function can only be called while executing a task with a scorer."
        )
    scores: list[Score] = []
    for scorer in scorers:
        score = await scorer(state, target)
        scores.append(score)
        transcript()._event(
            ScoreEvent(score=score, target=target.target, intermediate=True)
        )
    return scores
def init_scoring_context(scorers: list[Scorer], target: Target) -> None:
    _scorers.set(scorers)
    _target.set(target)
_scorers: ContextVar[list[Scorer]] = ContextVar("scorers")
_target: ContextVar[Target] = ContextVar("target")

================
File: src/inspect_ai/scorer/_scorer.py
================
from copy import deepcopy
from dataclasses import dataclass, field
from functools import wraps
from typing import (
    Any,
    Callable,
    ParamSpec,
    Protocol,
    cast,
    runtime_checkable,
)
from inspect_ai._util._async import is_callable_coroutine
from inspect_ai._util.error import PrerequisiteError
from inspect_ai._util.registry import (
    RegistryInfo,
    is_registry_object,
    registry_add,
    registry_create,
    registry_info,
    registry_name,
    registry_params,
    registry_tag,
    registry_unqualified_name,
)
from inspect_ai.solver._task_state import TaskState
from ._metric import Metric, MetricSpec, Score, as_metric_spec
from ._target import Target
@runtime_checkable
class Scorer(Protocol):
    async def __call__(
        self,
        state: TaskState,
        target: Target,
    ) -> Score:
        r"""Score model outputs.
        Evaluate the passed outputs and targets and return a
        dictionary with scoring outcomes and context.
        Args:
            state: Task state
            target: Ideal target for the output.
        Examples:
          ```python
          @scorer
          def custom_scorer() -> Scorer:
              async def score(state: TaskState, target: Target) -> Score:
                  # Compare state / model output with target
                  # to yield a score
                  return Score(value=...)
              return score
          ````
        """
        ...
@dataclass(frozen=True)
class ScorerSpec:
    """Scorer specification used to (re-)create scorers."""
    scorer: str
    """Scorer name"""
    args: dict[str, Any] = field(default_factory=dict)
    """Scorer arguments."""
    metadata: dict[str, Any] | None = field(default=None)
    """Scorer metadata"""
    metrics: (
        list[MetricSpec | dict[str, list[MetricSpec]]]
        | dict[str, list[MetricSpec]]
        | None
    ) = field(default=None)
    """Scorer metrics"""
P = ParamSpec("P")
def scorer_register(
    scorer: Callable[P, Scorer], name: str = "", metadata: dict[str, Any] = {}
) -> Callable[P, Scorer]:
    r"""Register a function or class as a scorer.
    Args:
        scorer (ScorerType):
            Scorer, function that returns a Scorer, or class
            deriving from the Scorer protocol.
        name (str): Name of scorer (Optional, defaults to object name)
        metadata (dict[str,Any]): Additional values to serialize
            in metadata.
    Returns:
        Scorer with registry attributes.
    """
    scorer_name = name if name else getattr(scorer, "__name__")
    registry_add(
        scorer, RegistryInfo(type="scorer", name=scorer_name, metadata=metadata)
    )
    return scorer
def scorer_create(name: str, **kwargs: Any) -> Scorer:
    r"""Create a Scorer based on its registered name.
    Args:
        name (str): Name of scorer (Optional, defaults to object name)
        **kwargs (dict): Optional creation arguments for the scorer
    Returns:
        Scorer with registry info attribute
    """
    return cast(Scorer, registry_create("scorer", name, **kwargs))
def scorer(
    metrics: list[Metric | dict[str, list[Metric]]] | dict[str, list[Metric]],
    name: str | None = None,
    **metadata: Any,
) -> Callable[[Callable[P, Scorer]], Callable[P, Scorer]]:
    r"""Decorator for registering scorers.
    Args:
        metrics: One or more metrics to calculate
            over the scores.
        name: Optional name for scorer. If the decorator has no name
            argument then the name of the underlying ScorerType
            object will be used to automatically assign a name.
        **metadata: Additional values to serialize
            in metadata.
    Returns:
        Scorer with registry attributes.
    Examples:
      ```python
      @scorer
      def custom_scorer() -> Scorer:
          async def score(state: TaskState, target: Target) -> Score:
              # Compare state / model output with target
              # to yield a score
              return Score(value=...)
          return score
      ````
    """
    def wrapper(scorer_type: Callable[P, Scorer]) -> Callable[P, Scorer]:
        # determine the name (explicit or implicit from object)
        scorer_name = registry_name(
            scorer_type, name if name else getattr(scorer_type, "__name__")
        )
        # wrap instantiations of scorer so they carry registry info and metrics
        @wraps(scorer_type)
        def scorer_wrapper(*args: P.args, **kwargs: P.kwargs) -> Scorer:
            scorer = scorer_type(*args, **kwargs)
            if not is_callable_coroutine(scorer):
                raise TypeError(
                    f"'{scorer_name}' is not declared as an async callable."
                )
            registry_tag(
                scorer_type,
                scorer,
                RegistryInfo(
                    type="scorer",
                    name=scorer_name,
                    metadata={SCORER_METRICS: metrics} | metadata,
                ),
                *args,
                **kwargs,
            )
            return scorer
        # register the scorer
        return scorer_register(
            scorer=cast(Callable[P, Scorer], scorer_wrapper),
            name=scorer_name,
            metadata={SCORER_METRICS: metrics} | metadata,
        )
    return wrapper
def as_scorer_spec(scorer: Scorer) -> ScorerSpec:
    if not is_registry_object(scorer):
        raise PrerequisiteError(
            f"The scorer {getattr(scorer, '__name__', '<unknown>')} was not created by a function decorated with @scorer so cannot be recorded."
        )
    name = registry_unqualified_name(scorer)
    metrics = scorer_metrics(scorer)
    resolved_metrics = resolve_metrics(metrics)
    args = registry_params(scorer)
    metadata = deepcopy(registry_info(scorer).metadata)
    del metadata[SCORER_METRICS]
    return ScorerSpec(
        scorer=name, args=args, metadata=metadata, metrics=resolved_metrics
    )
def resolve_metrics(
    metrics: list[Metric | dict[str, list[Metric]]] | dict[str, list[Metric]],
) -> (
    list[MetricSpec | dict[str, list[MetricSpec]]] | dict[str, list[MetricSpec]] | None
):
    if isinstance(metrics, list):
        resolved_metrics: list[MetricSpec | dict[str, list[MetricSpec]]] = []
        for metric_item in metrics:
            if isinstance(metric_item, Metric):
                resolved_metrics.append(as_metric_spec(metric_item))
            else:
                resolved_metrics.append(
                    {
                        metric_group: [
                            as_metric_spec(metric) for metric in metrics_list
                        ]
                        for metric_group, metrics_list in metric_item.items()
                    }
                )
        return resolved_metrics
    else:
        return {
            metric_group: [as_metric_spec(metric) for metric in metrics_list]
            for metric_group, metrics_list in metrics.items()
        }
def scorer_metrics(
    scorer: Scorer,
) -> list[Metric | dict[str, list[Metric]]] | dict[str, list[Metric]]:
    metrics_raw = registry_info(scorer).metadata[SCORER_METRICS]
    if isinstance(metrics_raw, dict):
        return cast(dict[str, list[Metric]], metrics_raw)
    else:
        return cast(list[Metric | dict[str, list[Metric]]], metrics_raw)
def unique_scorer_name(scorer: Scorer | str, already_used_names: list[str]) -> str:
    base_name = scorer if isinstance(scorer, str) else registry_unqualified_name(scorer)
    scorer_name = base_name
    count = 1
    while scorer_name in already_used_names:
        scorer_name = f"{base_name}{count}"
        count = count + 1
    return scorer_name
SCORER_METRICS = "metrics"

================
File: src/inspect_ai/scorer/_target.py
================
from typing import Sequence, Union, overload
class Target(Sequence[str]):
    """Target for scoring against the current TaskState.
    Target is a sequence of one or more strings. Use the
    `text` property to access the value as a single string.
    """
    def __init__(self, target: str | list[str]) -> None:
        self.target = target if isinstance(target, list) else [target]
    @overload
    def __getitem__(self, index: int) -> str: ...
    @overload
    def __getitem__(self, index: slice) -> Sequence[str]: ...
    def __getitem__(self, index: Union[int, slice]) -> Union[str, Sequence[str]]:
        return self.target[index]
    def __len__(self) -> int:
        return len(self.target)
    @property
    def text(self) -> str:
        return "".join(self.target)

================
File: src/inspect_ai/solver/__init__.py
================
from inspect_ai._util.deprecation import relocated_module_attribute
from ._basic_agent import basic_agent
from ._bridge.bridge import bridge
from ._chain import chain
from ._critique import self_critique
from ._fork import fork
from ._human_agent.agent import human_agent
from ._limit import SampleLimitExceededError
from ._multiple_choice import MultipleChoiceTemplate, multiple_choice
from ._plan import Plan, plan
from ._prompt import (
    assistant_message,
    chain_of_thought,
    prompt_template,
    system_message,
    user_message,
)
from ._solver import Generate, Solver, SolverSpec, generate, solver
from ._task_state import Choice, Choices, TaskState
from ._use_tools import use_tools
__all__ = [
    "basic_agent",
    "bridge",
    "human_agent",
    "chain",
    "fork",
    "generate",
    "prompt_template",
    "chain_of_thought",
    "multiple_choice",
    "system_message",
    "user_message",
    "assistant_message",
    "self_critique",
    "use_tools",
    "plan",
    "Plan",
    "Solver",
    "SolverSpec",
    "solver",
    "Choice",
    "Choices",
    "TaskState",
    "Generate",
    "MultipleChoiceTemplate",
    "SampleLimitExceededError",
]
_TOOL_MODULE_VERSION_3_18 = "0.3.18"
_TOOL_MODULE_VERSION_3_19 = "0.3.19"
_SUBTASKS_MODULE_VERSION = "0.3.26"
_REMOVED_IN = "0.4"
relocated_module_attribute(
    "Tool", "inspect_ai.tool.Tool", _TOOL_MODULE_VERSION_3_18, _REMOVED_IN
)
relocated_module_attribute(
    "ToolEnvironment",
    "inspect_ai.util.SandboxEnvironment",
    _TOOL_MODULE_VERSION_3_18,
    _REMOVED_IN,
)
relocated_module_attribute(
    "ToolEnvironments",
    "inspect_ai.util.SandboxEnvironments",
    _TOOL_MODULE_VERSION_3_18,
    _REMOVED_IN,
)
relocated_module_attribute(
    "ToolEnvironmentSpec",
    "inspect_ai.util.SandboxEnvironmentSpec",
    _TOOL_MODULE_VERSION_3_18,
    _REMOVED_IN,
)
relocated_module_attribute(
    "ToolError", "inspect_ai.tool.ToolError", _TOOL_MODULE_VERSION_3_18, _REMOVED_IN
)
relocated_module_attribute(
    "ToolResult", "inspect_ai.tool.ToolResult", _TOOL_MODULE_VERSION_3_18, _REMOVED_IN
)
relocated_module_attribute(
    "tool", "inspect_ai.tool.tool", _TOOL_MODULE_VERSION_3_18, _REMOVED_IN
)
relocated_module_attribute(
    "tool_environment",
    "inspect_ai.util.sandbox",
    _TOOL_MODULE_VERSION_3_18,
    _REMOVED_IN,
)
relocated_module_attribute(
    "toolenv", "inspect_ai.util.sandboxenv", _TOOL_MODULE_VERSION_3_18, _REMOVED_IN
)
relocated_module_attribute(
    "bash", "inspect_ai.tool.bash", _TOOL_MODULE_VERSION_3_19, _REMOVED_IN
)
relocated_module_attribute(
    "python", "inspect_ai.tool.python", _TOOL_MODULE_VERSION_3_19, _REMOVED_IN
)
relocated_module_attribute(
    "web_search", "inspect_ai.tool.web_search", _TOOL_MODULE_VERSION_3_19, _REMOVED_IN
)
relocated_module_attribute(
    "Transcript",
    "inspect_ai.log.Transcript",
    _SUBTASKS_MODULE_VERSION,
    _REMOVED_IN,
)
relocated_module_attribute(
    "transcript",
    "inspect_ai.log.transcript",
    _SUBTASKS_MODULE_VERSION,
    _REMOVED_IN,
)
relocated_module_attribute(
    "Store",
    "inspect_ai.util.Store",
    _SUBTASKS_MODULE_VERSION,
    _REMOVED_IN,
)
relocated_module_attribute(
    "store",
    "inspect_ai.util.store",
    _SUBTASKS_MODULE_VERSION,
    _REMOVED_IN,
)
relocated_module_attribute(
    "Subtask",
    "inspect_ai.util.Subtask",
    _SUBTASKS_MODULE_VERSION,
    _REMOVED_IN,
)
relocated_module_attribute(
    "subtask",
    "inspect_ai.util.subtask",
    _SUBTASKS_MODULE_VERSION,
    _REMOVED_IN,
)

================
File: src/inspect_ai/solver/_basic_agent.py
================
from logging import getLogger
from typing import Awaitable, Callable, cast
from typing_extensions import TypedDict, Unpack
from inspect_ai._util._async import is_callable_coroutine
from inspect_ai.model._cache import CachePolicy
from inspect_ai.model._call_tools import call_tools
from inspect_ai.model._chat_message import ChatMessageTool, ChatMessageUser
from inspect_ai.model._model import get_model
from inspect_ai.scorer._metric import Score, ValueToFloat, value_to_float
from inspect_ai.scorer._score import score
from inspect_ai.solver._chain import chain
from inspect_ai.tool._tool import Tool, ToolResult, tool
from inspect_ai.tool._tool_with import tool_with
from ._limit import SampleLimitExceededError
from ._prompt import system_message
from ._solver import Generate, Solver, solver
from ._task_state import TaskState
from ._use_tools import use_tools
logger = getLogger(__name__)
DEFAULT_SYSTEM_MESSAGE = """
You are a helpful assistant attempting to submit the correct answer. You have
several functions available to help with finding the answer. Each message
may perform one function call. You will see the result of the function right
after sending the message. If you need to perform multiple actions, you can
always send more messages with subsequent function calls. Do some reasoning
before your actions, describing what function calls you are going to use and
how they fit into your plan.
When you have completed the task and have an answer, call the {submit}()
function to report it.
"""
DEFAULT_INCORRECT_MESSAGE = """
Your submission was incorrect. Please proceed and attempt to find the correct answer.
"""
DEFAULT_CONTINUE_MESSAGE = "Please proceed to the next step using your best judgement."
DEFAULT_SUBMIT_NAME = "submit"
DEFAULT_SUBMIT_DESCRIPTION = "Submit an answer for evaluation."
class BasicAgentDeprecatedArgs(TypedDict, total=False):
    max_messages: int | None
@solver
def basic_agent(
    *,
    init: Solver | list[Solver] | None = None,
    tools: list[Tool] | Solver | None = None,
    cache: bool | CachePolicy = False,
    max_attempts: int = 1,
    message_limit: int | None = None,
    token_limit: int | None = None,
    max_tool_output: int | None = None,
    score_value: ValueToFloat | None = None,
    incorrect_message: str
    | Callable[
        [TaskState, list[Score]], str | Awaitable[str]
    ] = DEFAULT_INCORRECT_MESSAGE,
    continue_message: str = DEFAULT_CONTINUE_MESSAGE,
    submit_name: str = DEFAULT_SUBMIT_NAME,
    submit_description: str = DEFAULT_SUBMIT_DESCRIPTION,
    **kwargs: Unpack[BasicAgentDeprecatedArgs],
) -> Solver:
    """Basic ReAct agent.
    Agent that runs a tool use loop until the model submits an answer using the
    `submit()` tool. Tailor the model's instructions by passing a `system_message()`
    and/or other steps to `init` (if no `init` is specified then a default system
    message will be used). Use `max_attempts` to support additional submissions if
    the initial submission(s) are incorrect.
    Submissions are evaluated using the task's main scorer, with value of 1.0
    indicating a correct answer. Scorer values are converted to float (e.g.
    "C" becomes 1.0) using the standard value_to_float() function. Provide an
    alternate conversion scheme as required via `score_value`.
    Args:
       init: Agent initialisation (defaults to system_message with basic ReAct prompt)
       tools: Tools available for the agent. Either a list of tools or a Solver that
          can yield dynamic tools per-sample.
       cache: Caching behaviour for generate responses (defaults to no caching).
       max_attempts: Maximum number of submissions to accept before terminating.
       message_limit: Limit on messages in sample before terminating agent.
          If not specified, will use limit_messages defined for the task. If there is none
          defined for the task, 50 will be used as a default.
       token_limit: Limit on tokens used in sample before terminating agent.
       max_tool_output: Maximum output length (in bytes).
          Defaults to max_tool_output from active GenerateConfig.
       score_value: Function used to extract float from scores (defaults
          to standard value_to_float())
       incorrect_message: User message reply for an incorrect submission from the model.
          Alternatively, a function which returns a message (function may optionally be async)
       continue_message: User message to urge the model to continue when it
          doesn't make a tool call.
       submit_name: Name for tool used to make submissions
          (defaults to 'submit')
       submit_description: Description of submit tool (defaults to
          'Submit an answer for evaluation')
       **kwargs: Deprecated arguments for backward compatibility.
    Returns:
        Plan for agent.
    """
    # resolve deprecated
    for arg, value in kwargs.items():
        if arg == "max_messages":
            # deprecated, don't warn yet
            message_limit = int(cast(int, value))
    # resolve init
    if init is None:
        init = system_message(DEFAULT_SYSTEM_MESSAGE, submit=submit_name)
    init = init if isinstance(init, list) else [init]
    # resolve tools
    if tools is None:
        tools = []
    tools = tools if isinstance(tools, Solver) else use_tools(tools, append=True)
    # resolve score_value function
    score_value_fn = score_value or value_to_float()
    # submission tool
    @tool
    def submit() -> Tool:
        async def execute(answer: str) -> ToolResult:
            """Submit an answer for evaluation.
            Args:
              answer (str): Submitted answer
            """
            return answer
        return execute
    # solver that adds submission tool
    @solver
    def submit_tool() -> Solver:
        async def solve(state: TaskState, generate: Generate) -> TaskState:
            state.tools.append(tool_with(submit(), submit_name, submit_description))
            return state
        return solve
    # helper to extract a submitted answer
    def submission(tool_results: list[ChatMessageTool]) -> str | None:
        return next(
            (result.text for result in tool_results if result.function == submit_name),
            None,
        )
    # main agent loop
    @solver
    def basic_agent_loop() -> Solver:
        async def solve(state: TaskState, generate: Generate) -> TaskState:
            # resolve message_limit -- prefer parameter then fall back to task
            # (if there is no message_limit then default to 50)
            state.message_limit = message_limit or state.message_limit or 50
            # resolve token limit
            state.token_limit = token_limit or state.token_limit
            # track attempts
            attempts = 0
            try:
                # main loop (state.completed checks message_limit and token_limit)
                while not state.completed:
                    # generate output and append assistant message
                    state.output = await get_model().generate(
                        input=state.messages, tools=state.tools, cache=cache
                    )
                    state.messages.append(state.output.message)
                    # check for context window overflow
                    if state.output.stop_reason == "model_length":
                        from inspect_ai.log._transcript import transcript
                        transcript().info(
                            "Agent terminated: model context window exceeded"
                        )
                        break
                    # resolve tools calls (if any)
                    if state.output.message.tool_calls:
                        # call tool functions
                        tool_results = await call_tools(
                            state.output.message,
                            state.tools,
                            max_output=max_tool_output,
                        )
                        state.messages.extend(tool_results)
                        # was an answer submitted?
                        answer = submission(tool_results)
                        if answer:
                            # set the output to the answer for scoring
                            state.output.completion = answer
                            # exit if we are at max_attempts
                            attempts += 1
                            if attempts >= max_attempts:
                                break
                            # exit if the submission is successful
                            answer_scores = await score(state)
                            if score_value_fn(answer_scores[0].value) == 1.0:
                                break
                            # otherwise notify the model that it was incorrect and continue
                            else:
                                if is_callable_coroutine(incorrect_message):
                                    response_message: str = await incorrect_message(
                                        state, answer_scores
                                    )  # type: ignore[misc,operator]
                                elif callable(incorrect_message):
                                    response_message = cast(
                                        str, incorrect_message(state, answer_scores)
                                    )
                                else:
                                    response_message = incorrect_message
                                state.messages.append(
                                    ChatMessageUser(content=response_message)
                                )
                    # no tool calls, urge the model to continue
                    else:
                        state.messages.append(ChatMessageUser(content=continue_message))
            # propagate current state along with sample limit exceeded
            except SampleLimitExceededError as ex:
                raise ex.with_state(state)
            return state
        return solve
    # return chain
    return chain(
        init
        + [
            tools,
            submit_tool(),
            basic_agent_loop(),
        ]
    )

================
File: src/inspect_ai/solver/_bridge/bridge.py
================
from typing import Any, Awaitable, Callable
from jsonschema import Draft7Validator
from pydantic import BaseModel, Field, ValidationError
from pydantic_core import to_json
from inspect_ai._util._async import is_callable_coroutine
from inspect_ai.model._chat_message import ChatMessage, ChatMessageUser
from inspect_ai.model._providers.providers import validate_openai_client
from inspect_ai.scorer._metric import Score
from .._solver import Generate, Solver, solver
from .._task_state import TaskState
@solver
def bridge(agent: Callable[[dict[str, Any]], Awaitable[dict[str, Any]]]) -> Solver:
    """Bridge an external agent into an Inspect Solver.
    See documentation at <https://inspect.ai-safety-institute.org.uk/agent-bridge.html>
    Args:
      agent: Callable which takes a sample `dict` and returns a result `dict`.
    Returns:
      Standard Inspect solver.
    """
    validate_openai_client("Solver bridge()")
    from openai.types.chat import ChatCompletionMessageParam
    from inspect_ai.model._openai import (
        chat_messages_from_openai,
        openai_chat_messages,
    )
    from .patch import openai_request_to_inspect_model
    class BridgeSample(BaseModel):
        sample_id: str
        epoch: int
        input: list[ChatCompletionMessageParam]
        metadata: dict[str, Any]
        target: list[str]
    class BridgeResult(BaseModel):
        output: str
        messages: list[ChatCompletionMessageParam] | None = Field(default=None)
        scores: dict[str, Score] | None = Field(default=None)
    result_schema = BridgeResult.model_json_schema()
    result_validator = Draft7Validator(result_schema)
    # validate that the agent is an async function
    if not is_callable_coroutine(agent):
        raise TypeError(f"'{agent.__name__}' is not declared as an async callable.")
    async def solve(state: TaskState, generate: Generate) -> TaskState:
        # resolve input to array
        input: list[ChatMessage] = (
            [ChatMessageUser(content=state.input)]
            if isinstance(state.input, str)
            else state.input
        )
        # create sample (use standard gpt-4 message encoding -- i.e. no 'developer' messages)
        sample = BridgeSample(
            sample_id=str(state.sample_id),
            epoch=state.epoch,
            input=await openai_chat_messages(input, model="gpt-4"),
            metadata=state.metadata,
            target=list(state.target),
        )
        # run target function
        async with openai_request_to_inspect_model():
            # call the function
            result_dict = await agent(sample.model_dump())
            try:
                result = BridgeResult.model_validate(result_dict)
            except ValidationError:
                # if we fail to validate provide a better human readable error
                errors = list(result_validator.iter_errors(result_dict))
                message = "\n".join(
                    ["Result returned from bridged solver is not valid:"]
                    + [f" - {error.message}" for error in errors]
                    + ["", to_json(result_dict, indent=2).decode()]
                )
                raise ValueError(message)
        # update and return state
        state.output.completion = result.output
        if result.messages is not None:
            state.messages = chat_messages_from_openai(result.messages)
        if result.scores is not None:
            state.scores = result.scores
        return state
    return solve

================
File: src/inspect_ai/solver/_bridge/patch.py
================
import contextlib
import re
from contextvars import ContextVar
from functools import wraps
from time import time
from typing import Any, AsyncGenerator, Optional, Type, cast
from openai._base_client import AsyncAPIClient, _AsyncStreamT
from openai._models import FinalRequestOptions
from openai._types import ResponseT
from openai.types.chat import (
    ChatCompletion,
    ChatCompletionMessageParam,
    ChatCompletionToolChoiceOptionParam,
    ChatCompletionToolParam,
)
from shortuuid import uuid
from inspect_ai.model._generate_config import GenerateConfig, ResponseSchema
from inspect_ai.model._model import get_model
from inspect_ai.model._openai import (
    chat_messages_from_openai,
    openai_chat_choices,
    openai_completion_usage,
)
from inspect_ai.solver._task_state import sample_state
from inspect_ai.tool._tool_choice import ToolChoice, ToolFunction
from inspect_ai.tool._tool_info import ToolInfo
from inspect_ai.tool._tool_params import ToolParams
from inspect_ai.util._json import JSONSchema
@contextlib.asynccontextmanager
async def openai_request_to_inspect_model() -> AsyncGenerator[None, None]:
    # ensure one time init
    init_openai_request_patch()
    # set the patch enabled for this context and child coroutines
    token = _patch_enabled.set(True)
    try:
        yield
    finally:
        _patch_enabled.reset(token)
_patch_initialised: bool = False
_patch_enabled: ContextVar[bool] = ContextVar(
    "openai_request_patch_enabled", default=False
)
def init_openai_request_patch() -> None:
    global _patch_initialised
    if not _patch_initialised:
        # get reference to original method
        original_request = getattr(AsyncAPIClient, "request")
        if original_request is None:
            raise RuntimeError("Couldn't find 'request' method on AsyncAPIClient")
        @wraps(original_request)
        async def patched_request(
            self: AsyncAPIClient,
            cast_to: Type[ResponseT],
            options: FinalRequestOptions,
            *,
            stream: bool = False,
            stream_cls: type[_AsyncStreamT] | None = None,
            remaining_retries: Optional[int] = None,
        ) -> Any:
            # we have patched the underlying request method so now need to figure out when to
            # patch and when to stand down
            if (
                # enabled for this coroutine
                _patch_enabled.get()
                # completions request
                and options.url == "/chat/completions"
            ):
                # must also be an explicit request for an inspect model
                json_data = cast(dict[str, Any], options.json_data)
                model_name = str(json_data["model"])
                if re.match(r"^inspect/?", model_name):
                    return await inspect_model_request(model_name, options)
            # otherwise just delegate
            return await original_request(
                self,
                cast_to,
                options,
                stream=stream,
                stream_cls=stream_cls,
                remaining_retries=remaining_retries,
            )
        setattr(AsyncAPIClient, "request", patched_request)
async def inspect_model_request(
    model_name: str, options: FinalRequestOptions
) -> ChatCompletion:
    # convert openai messages to inspect messages
    json_data = cast(dict[str, Any], options.json_data)
    messages: list[ChatCompletionMessageParam] = json_data["messages"]
    input = chat_messages_from_openai(messages)
    # convert openai tools to inspect tools
    tools: list[ChatCompletionToolParam] = json_data.get("tools", [])
    inspect_tools: list[ToolInfo] = []
    for tool in tools:
        function = tool["function"].copy()
        inspect_tools.append(
            ToolInfo(
                name=function["name"],
                description=function["description"],
                parameters=ToolParams.model_validate(function["parameters"]),
            )
        )
    # convert openai tool choice to inspect tool_choice
    inspect_tool_choice: ToolChoice | None = None
    tool_choice: ChatCompletionToolChoiceOptionParam | None = json_data.get(
        "tool_choice", None
    )
    if tool_choice is not None:
        match tool_choice:
            case "auto" | "none":
                inspect_tool_choice = tool_choice
            case "required":
                inspect_tool_choice = "any"
            case _:
                inspect_tool_choice = ToolFunction(name=tool_choice["function"]["name"])
    # resolve model
    if model_name == "inspect":
        model = get_model()
    else:
        model = get_model(model_name.removeprefix("inspect/"))
    output = await model.generate(
        input=input,
        tools=inspect_tools,
        tool_choice=inspect_tool_choice,
        config=generate_config_from_openai(options),
    )
    # if we are using the "default" inspect model for the task, update state.messages
    if model_name == "inspect":
        state = sample_state()
        if state:
            state.messages = input + [output.choices[0].message]
    # inspect completion to openai completion
    return ChatCompletion(
        id=uuid(),
        created=int(time()),
        object="chat.completion",
        choices=openai_chat_choices(output.choices),
        model=model_name,
        usage=openai_completion_usage(output.usage) if output.usage else None,
    )
def generate_config_from_openai(options: FinalRequestOptions) -> GenerateConfig:
    # get options dict
    json_data = cast(dict[str, Any], options.json_data)
    config = GenerateConfig()
    config.max_tokens = json_data.get(
        "max_completion_tokens", json_data.get("max_tokens", None)
    )
    config.top_p = json_data.get("top_p", None)
    config.temperature = json_data.get("temperature", None)
    stop = json_data.get("stop", None)
    if stop:
        config.stop_seqs = [stop] if isinstance(stop, str) else stop
    config.frequency_penalty = json_data.get("frequency_penalty", None)
    config.presence_penalty = json_data.get("presence_penalty", None)
    config.seed = json_data.get("seed", None)
    config.num_choices = json_data.get("n", None)
    config.logprobs = json_data.get("logprobs", None)
    config.top_logprobs = json_data.get("top_logprobs", None)
    config.logit_bias = json_data.get("logit_bias", None)
    config.parallel_tool_calls = json_data.get("parallel_tool_calls", None)
    config.reasoning_effort = json_data.get("reasoning_effort", None)
    # response format
    response_format: dict[str, Any] | None = json_data.get("response_format", None)
    if response_format is not None:
        json_schema: dict[str, Any] | None = response_format.get("json_schema", None)
        if json_schema is not None:
            config.response_schema = ResponseSchema(
                name=json_schema.get("name", "schema"),
                description=json_schema.get("description", None),
                json_schema=JSONSchema.model_validate(json_schema.get("schema", {})),
                strict=json_schema.get("strict", None),
            )
    return config

================
File: src/inspect_ai/solver/_chain.py
================
from typing import Sequence, overload
from typing_extensions import override
from ._solver import Generate, Solver, solver
from ._task_state import TaskState
@solver
def chain(*solvers: Solver | list[Solver]) -> Solver:
    """Compose a solver from multiple other solvers.
    Solvers are executed in turn, and a solver step event
    is added to the transcript for each. If a solver returns
    a state with `completed=True`, the chain is terminated
    early.
    Args:
      *solvers: One or more solvers or lists of solvers to chain together.
    Returns:
      Solver that executes the passed solvers as a chain.
    """
    # flatten lists and chains
    all_solvers: list[Solver] = []
    for s in solvers:
        all_solvers.extend(unroll(s))
    return Chain(all_solvers)
def unroll(solver: Solver | list[Solver]) -> list[Solver]:
    if isinstance(solver, Solver):
        if isinstance(solver, Chain):
            return unroll(solver._solvers)
        else:
            return [solver]
    else:
        unrolled: list[Solver] = []
        for s in solver:
            unrolled.extend(unroll(s))
        return unrolled
class Chain(Sequence[Solver], Solver):
    """Solver composed from multiple other solvers.
    Args:
      solvers (*Solver): One or more solvers to chain together.
    """
    def __init__(self, solvers: list[Solver]) -> None:
        self._solvers = solvers
    @overload
    def __getitem__(self, index: int) -> Solver: ...
    @overload
    def __getitem__(self, index: slice) -> Sequence[Solver]: ...
    @override
    def __getitem__(self, index: int | slice) -> Solver | Sequence[Solver]:
        return self._solvers[index]
    @override
    def __len__(self) -> int:
        return len(self._solvers)
    async def __call__(
        self,
        state: TaskState,
        generate: Generate,
    ) -> TaskState:
        from ._transcript import solver_transcript
        for slv in self._solvers:
            with solver_transcript(slv, state) as st:
                state = await slv(state, generate)
                st.complete(state)
            if state.completed:
                break
        # return state
        return state

================
File: src/inspect_ai/solver/_critique.py
================
from inspect_ai._util.dict import omit
from inspect_ai.model import (
    ChatMessageUser,
    Model,
    get_model,
)
from inspect_ai.util import resource
from ._solver import Generate, Solver, solver
from ._task_state import TaskState
@solver
def self_critique(
    critique_template: str | None = None,
    completion_template: str | None = None,
    model: str | Model | None = None,
) -> Solver:
    """Solver which uses a model to critique the original answer.
    The `critique_template` is used to generate a critique
    and the `completion_template` is used to play that critique
    back to the model for an improved response. Note that you
    can specify an alternate `model` for critique (you don't
    need to use the model being evaluated).
    Args:
      critique_template: String or path to file
         containing critique template. The template uses two
         variables: `question` and `completion`.
         Variables from sample `metadata` are also available
         in the template.
      completion_template: String or path to file
          containing completion template. The template uses
          three variables: `question`,  `completion`, and `critique`
      model: Alternate model to be used
         for critique (by default the model being evaluated
         is used).
    """
    # resolve templates
    critique_templ = resource(critique_template or DEFAULT_CRITIQUE_TEMPLATE)
    completion_templ = resource(
        completion_template or DEFAULT_CRITIQUE_COMPLETION_TEMPLATE
    )
    async def solve(state: TaskState, generate: Generate) -> TaskState:
        # resolve model
        nonlocal model
        model = model if isinstance(model, Model) else get_model(model)
        # metadata without critique template variables
        metadata = omit(state.metadata, ["question", "completion", "critique"])
        # run critique
        critique = await model.generate(
            critique_templ.format(
                question=state.input_text,
                completion=state.output.completion,
                **metadata,
            )
        )
        # add the critique as a user message
        state.messages.append(
            ChatMessageUser(
                content=completion_templ.format(
                    question=state.input_text,
                    completion=state.output.completion,
                    critique=critique.completion,
                    **metadata,
                ),
            )
        )
        # regenerate
        return await generate(state)
    return solve
DEFAULT_CRITIQUE_TEMPLATE = r"""
Given the following question and answer, please critique the answer. A good answer comprehensively answers the question and NEVER refuses to answer. If the answer is already correct do not provide critique - simply respond 'The original answer is fully correct'.
[BEGIN DATA]
***
[Question]: {question}
***
[Answer]: {completion}
***
[END DATA]
Critique: """
DEFAULT_CRITIQUE_COMPLETION_TEMPLATE = r"""
Given the following question, initial answer and critique please generate an improved answer to the question:
[BEGIN DATA]
***
[Question]: {question}
***
[Answer]: {completion}
***
[Critique]: {critique}
***
[END DATA]
If the original answer is already correct, just repeat the original answer exactly. Provide your answer at the end on its own line in the form "ANSWER: $ANSWER" (without quotes) where $ANSWER is the answer to the question.
"""

================
File: src/inspect_ai/solver/_fork.py
================
import functools
from contextvars import ContextVar
from copy import deepcopy
from typing import Any, cast
from typing_extensions import overload
from inspect_ai._util._async import tg_collect
from inspect_ai._util.registry import registry_log_name, registry_params
from inspect_ai.util._subtask import subtask
from ._chain import Chain
from ._solver import Generate, Solver
from ._task_state import TaskState
@overload
async def fork(state: TaskState, solvers: Solver) -> TaskState: ...
@overload
async def fork(state: TaskState, solvers: list[Solver]) -> list[TaskState]: ...
async def fork(
    state: TaskState, solvers: Solver | list[Solver]
) -> TaskState | list[TaskState]:
    """Fork the TaskState and evaluate it against multiple solvers in parallel.
    Run several solvers against independent copies of a TaskState. Each
    Solver gets its own copy of the TaskState and is run (in parallel)
    in an independent Subtask (meaning that is also has its own independent
    Store that doesn't affect the Store of other subtasks or the parent).
    Args:
      state: Beginning TaskState
      solvers: Solvers to apply on the TaskState.
        Each Solver will get a standalone copy of the TaskState.
    Returns:
      Single TaskState or list of TaskState (depending on the input)
      with the results of applying the solver(s) to a forked copy
      of the TaskState.
    """
    if isinstance(solvers, Solver):
        return await solver_subtask(state, solvers)
    else:
        return await tg_collect(
            [functools.partial(solver_subtask, state, solver) for solver in solvers]
        )
async def solver_subtask(state: TaskState, solver: Solver) -> TaskState:
    # get the generate function for the current task
    generate = _generate.get(None)
    if generate is None:
        raise RuntimeError("Called fork() outside of a running task.")
    # deepcopy the state
    state = deepcopy(state)
    # create a subtask so we get an independent store and transcript
    from ._transcript import solver_transcript
    # derive name and input
    if isinstance(solver, Chain):
        name = "chain"
        input: dict[str, Any] = {}
    else:
        name = registry_log_name(solver)
        input = registry_params(solver)
    @subtask(name=name, store=state.store, type="fork", input=input)  # type: ignore
    async def solve() -> TaskState:
        if not isinstance(solver, Chain):
            with solver_transcript(solver, state) as st:
                new_state = await solver(state, generate)
                st.complete(new_state)
            return new_state
        else:
            return await solver(state, generate)
    # call it and return TaskState
    return cast(TaskState, await solve())
def set_task_generate(generate: Generate) -> None:
    _generate.set(generate)
_generate: ContextVar[Generate] = ContextVar("_generate")

================
File: src/inspect_ai/solver/_human_agent/agent.py
================
from typing import cast
import anyio
from inspect_ai.util import display_type, input_panel, sandbox
from inspect_ai.util._sandbox.events import SandboxEnvironmentProxy
from .._solver import Generate, Solver, solver
from .._task_state import TaskState
from .commands import human_agent_commands
from .install import install_human_agent
from .panel import HumanAgentPanel
from .service import run_human_agent_service
from .view import ConsoleView, HumanAgentView
@solver
def human_agent(
    answer: bool | str = True,
    intermediate_scoring: bool = False,
    record_session: bool = True,
) -> Solver:
    """Human solver for agentic tasks that run in a Linux environment.
    The Human agent solver installs agent task tools in the default
    sandbox and presents the user with both task instructions and
    documentation for the various tools (e.g. `task submit`,
    `task start`, `task stop` `task instructions`, etc.). A human agent panel
    is displayed with instructions for logging in to the sandbox.
    If the user is running in VS Code with the Inspect extension,
    they will also be presented with links to login to the sandbox
    using a VS Code Window or Terminal.
    Args:
       answer: Is an explicit answer required for this task or is it scored
          based on files in the container? Pass a `str` with a regex to validate
          that the answer matches the expected format.
       intermediate_scoring: Allow the human agent to check their score while working.
       record_session: Record all user commands and outputs in the sandbox bash session.
    Returns:
       Solver: Human agent solver.
    """
    # we can only run one human agent interaction at a time (use lock to enforce)
    agent_lock = anyio.Lock()
    async def solve(state: TaskState, generate: Generate) -> TaskState:
        async with agent_lock:
            # ensure that we have a sandbox to work with
            try:
                connection = await sandbox().connection()
            except ProcessLookupError:
                raise RuntimeError("Human agent must run in a task with a sandbox.")
            except NotImplementedError:
                raise RuntimeError(
                    "Human agent must run with a sandbox that supports connections."
                )
            # helper function to run the agent (called for fullscreen vs. fallback below)
            async def run_human_agent(view: HumanAgentView) -> TaskState:
                sandbox_proxy = cast(SandboxEnvironmentProxy, sandbox())
                with sandbox_proxy.no_events():
                    # create agent commands
                    commands = human_agent_commands(
                        state, answer, intermediate_scoring, record_session
                    )
                    # install agent tools
                    await install_human_agent(state, commands, record_session)
                    # hookup the view ui
                    view.connect(connection)
                    # run sandbox service
                    return await run_human_agent_service(state, commands, view)
            # support both fullscreen ui and fallback
            if display_type() == "full":
                async with await input_panel(HumanAgentPanel) as panel:
                    return await run_human_agent(panel)
            else:
                return await run_human_agent(ConsoleView())
    return solve

================
File: src/inspect_ai/solver/_human_agent/commands/__init__.py
================
from inspect_ai.solver._task_state import TaskState
from .clock import StartCommand, StopCommand
from .command import HumanAgentCommand
from .instructions import InstructionsCommand
from .note import NoteCommand
from .score import ScoreCommand
from .status import StatusCommand
from .submit import QuitCommand, SubmitCommand, ValidateCommand
def human_agent_commands(
    state: TaskState,
    answer: bool | str,
    intermediate_scoring: bool,
    record_session: bool,
) -> list[HumanAgentCommand]:
    # base submit, validate, and quit
    commands = [
        SubmitCommand(record_session),
        ValidateCommand(answer),
        QuitCommand(record_session),
    ]
    # optional intermediate scoring
    if intermediate_scoring:
        commands.append(ScoreCommand(state))
    # remaining commands
    commands.extend(
        [
            NoteCommand(),
            StatusCommand(),
            StartCommand(),
            StopCommand(),
        ]
    )
    # with instructions (letting it see the other commands)
    return commands + [InstructionsCommand(commands)]

================
File: src/inspect_ai/solver/_human_agent/commands/clock.py
================
from argparse import Namespace
from typing import Awaitable, Callable, Literal
from pydantic import JsonValue
from inspect_ai._util.format import format_progress_time
from ..state import HumanAgentState
from .command import HumanAgentCommand, call_human_agent
from .status import render_status
class StartCommand(HumanAgentCommand):
    @property
    def name(self) -> str:
        return "start"
    @property
    def description(self) -> str:
        return "Start the task clock (resume working)."
    @property
    def group(self) -> Literal[1, 2, 3]:
        return 2
    def cli(self, args: Namespace) -> None:
        print(call_human_agent("start"))
    def service(self, state: HumanAgentState) -> Callable[..., Awaitable[JsonValue]]:
        async def start() -> str:
            if not state.running:
                state.running = True
                clock_action_event("start", state)
            return render_status(state)
        return start
class StopCommand(HumanAgentCommand):
    @property
    def name(self) -> str:
        return "stop"
    @property
    def description(self) -> str:
        return "Stop the task clock (pause working)."
    @property
    def group(self) -> Literal[1, 2, 3]:
        return 2
    def cli(self, args: Namespace) -> None:
        print(call_human_agent("stop"))
    def service(self, state: HumanAgentState) -> Callable[..., Awaitable[JsonValue]]:
        async def stop() -> str:
            if state.running:
                state.running = False
                clock_action_event("stop", state)
            return render_status(state)
        return stop
def clock_action_event(action: str, state: HumanAgentState) -> None:
    from inspect_ai.log._transcript import transcript
    transcript().info(
        {
            "action": action,
            "total_time": format_progress_time(state.time, False),
        },
        source="human_agent",
    )

================
File: src/inspect_ai/solver/_human_agent/commands/command.py
================
import abc
from argparse import Namespace
from typing import Any, Awaitable, Callable, Literal, NamedTuple
from pydantic import JsonValue
from ..state import HumanAgentState
class HumanAgentCommand:
    @property
    @abc.abstractmethod
    def name(self) -> str:
        """Command name (e.g. 'submit')"""
        ...
    @property
    @abc.abstractmethod
    def description(self) -> str:
        """Command description."""
        ...
    @property
    def group(self) -> Literal[1, 2, 3]:
        return 1
    @property
    def contexts(self) -> list[Literal["cli", "service"]]:
        """Contexts where this command runs (defaults to both cli and service)."""
        return ["cli", "service"]
    class CLIArg(NamedTuple):
        name: str
        description: str
        required: bool = False
    @property
    def cli_args(self) -> list[CLIArg]:
        """Positional command line arguments."""
        return []
    def cli(self, args: Namespace) -> None:
        """CLI command (runs in container). Required for context "cli"."""
        pass
    def service(self, state: HumanAgentState) -> Callable[..., Awaitable[JsonValue]]:
        """Service handler (runs in solver). Required for context "service"."""
        async def no_handler() -> None:
            pass
        return no_handler
# Dummy functions for implementation of call methods
def call_human_agent(method: str, **params: Any) -> Any:
    return None

================
File: src/inspect_ai/solver/_human_agent/commands/instructions.py
================
from argparse import Namespace
from typing import Awaitable, Callable, Literal
from pydantic import JsonValue
from rich.console import Group
from rich.panel import Panel
from rich.table import Table
from rich.text import Text
from inspect_ai._util.ansi import render_text
from inspect_ai._util.transcript import DOUBLE_LINE
from ..state import HumanAgentState
from .command import HumanAgentCommand, call_human_agent
class InstructionsCommand(HumanAgentCommand):
    def __init__(self, commands: list[HumanAgentCommand]) -> None:
        self._commands = commands.copy() + [self]
    @property
    def name(self) -> str:
        return "instructions"
    @property
    def description(self) -> str:
        return "Display task commands and instructions."
    @property
    def group(self) -> Literal[1, 2, 3]:
        return 3
    def cli(self, args: Namespace) -> None:
        print(call_human_agent("instructions", **vars(args)))
    def service(self, state: HumanAgentState) -> Callable[..., Awaitable[JsonValue]]:
        async def instructions() -> str:
            intro = "\nYou will be completing a task based on the instructions presented below. You can use the following commands as you work on the task:\n"
            commands_table = Table(box=None, show_header=False)
            commands_table.add_column("", justify="left")
            commands_table.add_column("", justify="left")
            def add_command_group(group: int) -> None:
                for command in filter(
                    lambda c: "cli" in c.contexts and c.group == group, self._commands
                ):
                    commands_table.add_row(f"task {command.name}", command.description)
                if group != 3:
                    commands_table.add_row("", "")
            for i in range(1, 4):
                add_command_group(i)
            header_panel = Panel(
                Group(intro, commands_table),
                title=Text.from_markup("[bold]Human Agent Task[/bold]"),
                box=DOUBLE_LINE,
                padding=(0, 0),
            )
            instructions_panel = Panel(
                f"{state.instructions.strip()}",
                title="Task Instructions",
                padding=(1, 1),
            )
            return render_text(
                ["", header_panel, instructions_panel],
                styles=False,
                no_color=True,
                width=90,
            )
        return instructions

================
File: src/inspect_ai/solver/_human_agent/commands/note.py
================
from argparse import Namespace
from typing import Awaitable, Callable, Literal
from pydantic import JsonValue
from ..state import HumanAgentState
from .command import HumanAgentCommand, call_human_agent
class NoteCommand(HumanAgentCommand):
    @property
    def name(self) -> str:
        return "note"
    @property
    def description(self) -> str:
        return "Record a note in the task transcript."
    @property
    def group(self) -> Literal[1, 2, 3]:
        return 1
    def cli(self, args: Namespace) -> None:
        print(
            "Enter a multiline markdown note (Press Ctrl+D on a new line to finish):\n"
        )
        lines = ["## Human Agent Note"]
        try:
            while True:
                line = input()
                lines.append(line)
        except EOFError:
            pass
        call_human_agent("note", content="\n".join(lines))
    def service(self, state: HumanAgentState) -> Callable[..., Awaitable[JsonValue]]:
        from inspect_ai.log._transcript import transcript
        async def note(content: str) -> None:
            transcript().info(content, source="human_agent")
        return note

================
File: src/inspect_ai/solver/_human_agent/commands/score.py
================
from argparse import Namespace
from copy import deepcopy
from typing import Awaitable, Callable, Literal
from pydantic import JsonValue
from inspect_ai._util.ansi import render_text
from inspect_ai.model._model_output import ModelOutput
from inspect_ai.scorer._score import score
from ..._task_state import TaskState
from ..state import HumanAgentState, IntermediateScoring
from .command import HumanAgentCommand, call_human_agent
class ScoreCommand(HumanAgentCommand):
    def __init__(self, state: TaskState):
        self._state = state
    @property
    def name(self) -> str:
        return "score"
    @property
    def description(self) -> str:
        return "Score the task to check progress."
    @property
    def group(self) -> Literal[1, 2, 3]:
        return 1
    @property
    def cli_args(self) -> list[HumanAgentCommand.CLIArg]:
        return [
            HumanAgentCommand.CLIArg(
                name="answer",
                description="Answer to submit for scoring (optional, not required for all tasks)",
            )
        ]
    def cli(self, args: Namespace) -> None:
        # first validate (print and exit if we get a str back)
        call_args = vars(args)
        error = call_human_agent("validate", **call_args)
        if error:
            print(error)
            return
        print(call_human_agent("score", **call_args))
    def service(self, state: HumanAgentState) -> Callable[..., Awaitable[JsonValue]]:
        async def score_task(answer: str | None) -> str:
            # make a copy of TaskState, add the answer, then score
            if answer:
                task_state = deepcopy(self._state)
                task_state.output = ModelOutput.from_content("human_agent", answer)
                result = await score(task_state)
            else:
                result = await score(self._state)
            # record the scoring action in our state
            state.scorings.append(IntermediateScoring(time=state.time, scores=result))
            # notify user
            return render_text(
                f"[bold]Answer:[/bold] {result[0].answer}, [bold]Score:[/bold] {result[0].as_str()}"
            )
        return score_task

================
File: src/inspect_ai/solver/_human_agent/commands/status.py
================
from argparse import Namespace
from typing import Awaitable, Callable, Literal
from pydantic import JsonValue
from rich.console import RenderableType
from rich.table import Table
from rich.text import Text
from inspect_ai._util.ansi import render_text
from inspect_ai._util.format import format_progress_time
from ..state import HumanAgentState
from .command import HumanAgentCommand, call_human_agent
class StatusCommand(HumanAgentCommand):
    @property
    def name(self) -> str:
        return "status"
    @property
    def description(self) -> str:
        return "Print task status (clock, scoring, etc.)"
    @property
    def group(self) -> Literal[1, 2, 3]:
        return 2
    def cli(self, args: Namespace) -> None:
        print(call_human_agent("status"))
    def service(self, state: HumanAgentState) -> Callable[..., Awaitable[JsonValue]]:
        async def status() -> str:
            return render_status(state)
        return status
def render_status(state: HumanAgentState) -> str:
    content: list[RenderableType] = [""]
    content.append(
        f"[bold]Status:[/bold] {'Running' if state.running else 'Stopped'}  "
        + f"[bold]Time:[/bold] {format_progress_time(state.time, pad_hours=False)}"
    )
    if len(state.scorings) > 0:
        content.append("")
        content.append(Text.from_markup("[italic]Intermediate Scores[/italic]"))
        scores_table = Table(box=None, min_width=35, padding=(0, 0))
        scores_table.add_column("Answer", justify="left")
        scores_table.add_column("Score", justify="center")
        scores_table.add_column("Time", justify="right")
        for score in state.scorings:
            scores_table.add_row(
                score.scores[0].answer,
                score.scores[0].as_str(),
                format_progress_time(score.time),
            )
        content.append(scores_table)
    return render_text(content, highlight=False)

================
File: src/inspect_ai/solver/_human_agent/commands/submit.py
================
from argparse import Namespace
from logging import getLogger
from pathlib import PurePosixPath
from re import Pattern, compile, match
from typing import Awaitable, Callable, Literal
from pydantic import JsonValue
from inspect_ai._util.ansi import render_text
from inspect_ai.util._sandbox import sandbox
from ..install import RECORD_SESSION_DIR
from ..state import HumanAgentState
from .command import HumanAgentCommand, call_human_agent
logger = getLogger(__name__)
class SessionEndCommand(HumanAgentCommand):
    def __init__(self, record_session: bool):
        super().__init__()
        self._record_session = record_session
    @property
    def group(self) -> Literal[1, 2, 3]:
        return 1
    async def _read_session_logs(self) -> dict[str, str]:
        # retreive session logs (don't fail)
        sessions_dir = PurePosixPath(RECORD_SESSION_DIR)
        result = await sandbox().exec(["ls", "-1", sessions_dir.as_posix()])
        if not result.success:
            logger.warning(f"Error listing human agent session logs: {result.stderr}")
            return {}
        # read logs
        session_logs: dict[str, str] = {}
        for session_log in result.stdout.strip().splitlines():
            try:
                session_logs[session_log] = await sandbox().read_file(
                    (sessions_dir / session_log).as_posix()
                )
            except Exception as ex:
                logger.warning(f"Error reading human agent session log: {ex}")
        return session_logs
class QuitCommand(SessionEndCommand):
    @property
    def name(self) -> str:
        return "quit"
    @property
    def description(self) -> str:
        return "Quit the task without submitting an answer."
    def cli(self, args: Namespace) -> None:
        # verify that the user wants to proceed
        action = "quit the task without submitting an answer (ending the exercise)"
        while True:
            response = (
                input(
                    f"\nDo you definitely want to {action}?\n\nThis will disconnect you from the task environment and you won't be able to reconnect.\n\nYes (y) or No (n): "
                )
                .lower()
                .strip()
            )
            if response in ["yes", "y"]:
                break
            elif response in ["no", "n"]:
                return
            else:
                print("Please enter yes or no.")
        # thank the user!
        print(
            "\nThank you for working on this task!\n\n"
            + "Your task will now be scored and you will be disconnected from this container.\n"
        )
        call_human_agent("quit")
    def service(self, state: HumanAgentState) -> Callable[..., Awaitable[JsonValue]]:
        async def submit() -> None:
            if self._record_session:
                state.logs = await self._read_session_logs()
            state.running = False
            state.answer = ""
        return submit
class SubmitCommand(SessionEndCommand):
    @property
    def name(self) -> str:
        return "submit"
    @property
    def description(self) -> str:
        return "Submit your final answer for the task."
    @property
    def cli_args(self) -> list[HumanAgentCommand.CLIArg]:
        return [
            HumanAgentCommand.CLIArg(
                name="answer",
                description="Answer to submit for scoring (optional, not required for all tasks)",
            )
        ]
    def cli(self, args: Namespace) -> None:
        # read cli args
        call_args = vars(args)
        # first validate (print and exit if we get a str back)
        error = call_human_agent("validate", **call_args)
        if error:
            print(error)
            return
        # verify that the user wants to proceed
        answer = call_args.get("answer", None)
        answer_text = f" '{answer}'" if answer else ""
        action = f"end the task and submit{answer_text}"
        while True:
            response = (
                input(
                    f"\nDo you definitely want to {action}?\n\nThis will disconnect you from the task environment and you won't be able to reconnect.\n\nYes (y) or No (n): "
                )
                .lower()
                .strip()
            )
            if response in ["yes", "y"]:
                break
            elif response in ["no", "n"]:
                return
            else:
                print("Please enter yes or no.")
        # thank the user!
        print(
            "\nThank you for working on this task!\n\n"
            + "Your task will now be scored and you will be disconnected from this container.\n"
        )
        call_human_agent("submit", **call_args)
    def service(self, state: HumanAgentState) -> Callable[..., Awaitable[JsonValue]]:
        async def submit(answer: str) -> None:
            if self._record_session:
                state.logs = await self._read_session_logs()
            state.running = False
            state.answer = answer
        return submit
class ValidateCommand(HumanAgentCommand):
    def __init__(self, answer: bool | str) -> None:
        self._answer = compile(answer) if isinstance(answer, str) else answer
    @property
    def name(self) -> str:
        return "validate"
    @property
    def description(self) -> str:
        return "Validate a task submission."
    @property
    def contexts(self) -> list[Literal["cli", "service"]]:
        return ["service"]
    def service(self, state: HumanAgentState) -> Callable[..., Awaitable[JsonValue]]:
        async def validate(answer: str | None) -> str | None:
            def failed(reason: str) -> str:
                return render_text(f"[bold]FAILED:[/bold] {reason}")
            if not state.running:
                return failed("Task is stopped (use 'task start' to start)")
            if self._answer:
                answer = answer.strip() if isinstance(answer, str) else answer
                if not answer:
                    return failed(
                        "An explicit answer is required for scoring this task."
                    )
                elif isinstance(self._answer, Pattern) and not match(
                    self._answer, answer
                ):
                    return failed(
                        "Your answer was not in the required format (please review the task instructions)"
                    )
            return None  # made it through verification
        return validate

================
File: src/inspect_ai/solver/_human_agent/install.py
================
import inspect
from textwrap import dedent
from inspect_ai.util import sandbox
from .._task_state import TaskState
from .commands.command import HumanAgentCommand
INSTALL_DIR = "human_agent_install"
HUMAN_AGENT_DIR = "/opt/human_agent"
TASK_PY = "task.py"
INSTALL_SH = "install.sh"
BASHRC = ".bashrc"
WELCOME_FILE = "welcome.txt"
WELCOME_LOGIN_FILE = "welcome_login.txt"
INSTRUCTIONS_FILE = "instructions.txt"
RECORD_SESSION_DIR = "/var/tmp/user-sessions"
async def install_human_agent(
    state: TaskState, commands: list[HumanAgentCommand], record_session: bool
) -> None:
    # see if we have already installed
    if not (await sandbox().exec(["mkdir", HUMAN_AGENT_DIR])).success:
        return
    # setup installation directory
    await checked_exec(["mkdir", "-p", INSTALL_DIR])
    # generate task.py
    task_py = human_agent_commands(commands)
    await checked_write_file(f"{INSTALL_DIR}/{TASK_PY}", task_py, executable=True)
    # generate .bashrc
    bash_rc = human_agent_bashrc(commands, record_session)
    await checked_write_file(f"{INSTALL_DIR}/{BASHRC}", bash_rc, executable=True)
    # write and run installation script
    install_sh = human_agent_install_sh()
    await checked_write_file(f"{INSTALL_DIR}/{INSTALL_SH}", install_sh, executable=True)
    await checked_exec(["bash", f"./{INSTALL_SH}"], cwd=INSTALL_DIR)
    await checked_exec(["rm", "-rf", INSTALL_DIR])
def human_agent_commands(commands: list[HumanAgentCommand]) -> str:
    # filter out hidden commands
    commands = [command for command in commands if "cli" in command.contexts]
    # standard imports (including any dependencies that call methods carry)
    imports = dedent("""
    import argparse
    import sys
    from argparse import Namespace
    from pathlib import Path
    sys.path.append("/var/tmp/sandbox-services/human_agent")
    from human_agent import call_human_agent
    def format_time(t):
        minutes, seconds = divmod(t, 60)
        hours, minutes = divmod(minutes, 60)
        return f"{hours:.0f}:{minutes:02.0f}:{seconds:02.0f}"
    """)
    # command handler source code (extracted from call methods)
    command_handlers = "\n\n".join(
        dedent(
            inspect.getsource(command.cli).replace("cli(self, ", f"{command.name}(", 1)
        )
        for command in commands
    )
    # parse commands
    command_parsers: list[str] = []
    for command in commands:
        command_parsers.append(
            dedent(f"""
        {command.name}_parser = subparsers.add_parser("{command.name}", help="{command.description}")
        """).lstrip()
        )
        for arg in command.cli_args:
            if arg.name.startswith("--"):
                extras = 'action="store_true", default=False'
            else:
                extras = f"""nargs={1 if arg.required else '"?"'}"""
            command_parsers.append(
                dedent(f"""
                {command.name}_parser.add_argument("{arg.name}", {extras}, help="{arg.description}")
                """).strip()
            )
    parse = (
        dedent("""
    parser = argparse.ArgumentParser(description="Human agent tools.")
    subparsers = parser.add_subparsers(dest="command")
    """)
        + "\n"
        + "\n".join(command_parsers)
    )
    # dispatch commands
    command_dispatchers: list[str] = []
    for i, command in enumerate(commands):
        conditional = "if" if i == 0 else "elif"
        command_dispatchers.append(
            f'{conditional} command == "{command.name}": {command.name}(args)'
        )
    command_dispatchers.append("else: parser.print_help()")
    dispatch = dedent("""
    args = parser.parse_args()
    command = args.command
    delattr(args, 'command')
    """) + "\n".join(command_dispatchers)
    return "\n".join([imports, command_handlers, parse, dispatch]) + "\n"
def human_agent_bashrc(commands: list[HumanAgentCommand], record_session: bool) -> str:
    # only run in interative terminals
    TERMINAL_CHECK = dedent("""
    ### Inspect Human Agent Setup #########################################=
    # only run if shell is interactive
    case $- in
        *i*) ;;
        *) return ;;
    esac
    # only run if attached to a terminal
    if ! tty -s; then
        return
    fi
    """)
    # shell alias and completions
    command_names = " ".join(
        [f"{command.name}" for command in commands if "cli" in command.contexts]
    )
    COMMANDS = dedent(f"""
    # shell alias for human agent commands
    alias task='python3 {HUMAN_AGENT_DIR}/{TASK_PY}'
    # completion handler
    _task_completion() {{
        local cur
        cur="${{COMP_WORDS[COMP_CWORD]}}"
        if [ "$COMP_CWORD" -eq 1 ]; then
            local commands="{command_names}"
            # Generate completion matches
            COMPREPLY=($(compgen -W "${{commands}}" -- ${{cur}}))
        fi
    }}
    complete -F _task_completion task
    """)
    # session recording
    if record_session:
        RECORDING = dedent(f"""
        # record human agent session transcript
        if [ -z "$SCRIPT_RUNNING" ]; then
            export SCRIPT_RUNNING=1
            LOGDIR={RECORD_SESSION_DIR}
            mkdir -p "$LOGDIR"
            TIMESTAMP=$(date +%Y%m%d_%H%M%S)
            INPUTFILE="$LOGDIR/$(whoami)_$TIMESTAMP.input"
            OUTPUTFILE="$LOGDIR/$(whoami)_$TIMESTAMP.output"
            TIMINGFILE="$LOGDIR/$(whoami)_$TIMESTAMP.timing"
            exec script -q -f -m advanced -I "$INPUTFILE" -O "$OUTPUTFILE" -T "$TIMINGFILE" -c "bash --login -i"
        fi
        """)
    else:
        RECORDING = ""
    # display task instructions
    INSTRUCTIONS = dedent("""
    if [ -z "$INSTRUCTIONS_SHOWN" ]; then
        export INSTRUCTIONS_SHOWN=1
        task instructions > instructions.txt
        cat instructions.txt
    fi
    """).lstrip()
    # return .bashrc
    return "\n".join([TERMINAL_CHECK, COMMANDS, RECORDING, INSTRUCTIONS])
def human_agent_install_sh() -> str:
    return dedent(f"""
    #!/usr/bin/env bash
    # create installation directory
    HUMAN_AGENT="{HUMAN_AGENT_DIR}"
    mkdir -p $HUMAN_AGENT
    # copy command script
    cp {TASK_PY} $HUMAN_AGENT
    # append to .bashrc
    cat {BASHRC} >> ~/{BASHRC}
    """)
async def checked_exec(
    cmd: list[str],
    input: str | bytes | None = None,
    cwd: str | None = None,
) -> str:
    result = await sandbox().exec(cmd, input=input, cwd=cwd)
    if not result.success:
        raise RuntimeError(f"Error executing command {' '.join(cmd)}: {result.stderr}")
    return result.stdout
async def checked_write_file(
    file: str, contents: str, executable: bool = False
) -> None:
    await checked_exec(["tee", "--", file], input=contents)
    if executable:
        await checked_exec(["chmod", "+x", file])

================
File: src/inspect_ai/solver/_human_agent/panel.py
================
from typing import cast
from textual.app import ComposeResult
from textual.containers import (
    Container,
    Horizontal,
    VerticalScroll,
)
from textual.reactive import reactive
from textual.widgets import (
    Button,
    ContentSwitcher,
    Label,
    Link,
    LoadingIndicator,
    Static,
)
from inspect_ai._util.format import format_progress_time
from inspect_ai._util.vscode import (
    VSCodeCommand,
    can_execute_vscode_commands,
    execute_vscode_commands,
)
from inspect_ai.util import InputPanel, SandboxConnection, throttle
from .state import HumanAgentState
class HumanAgentPanel(InputPanel):
    DEFAULT_TITLE = "Human Agent"
    SANDBOX_VIEW_ID = "human-agent-sandbox-view"
    SANDBOX_INSTRUCTIONS_ID = "sandbox-instructions"
    VSCODE_LINKS_ID = "vscode-links"
    LOGIN_VSCODE_TERMINAL_ID = "login-vscode-terminal"
    LOGIN_VSCODE_WINDOW_ID = "login-vscode-window"
    COMMAND_INSTRUCTIONS_ID = "command-instructions"
    SANDBOX_COMMAND_ID = "sandbox-command"
    INSTRUCTIONS_CLASS = "instructions"
    LINK_LABEL_CLASS = "link-label"
    DEFAULT_CSS = f"""
    #{SANDBOX_VIEW_ID} {{
        scrollbar-size-vertical: 1;
    }}
    HumanAgentPanel .{INSTRUCTIONS_CLASS} {{
        color: $text-muted;
        margin-bottom: 1;
    }}
    #{SANDBOX_COMMAND_ID} {{
        color: $secondary;
    }}
    HumanAgentPanel .{LINK_LABEL_CLASS} {{
        color: $text-muted;
    }}
    HumanAgentPanel VSCodeLink {{
        margin-left: 1;
        margin-right: 2;
    }}
    HumanAgentPanel #{VSCODE_LINKS_ID} {{
        height: 1;
        margin-bottom: 1;
    }}
    """
    connection: reactive[SandboxConnection | None] = reactive(None)
    # implement HumanAgentView
    def connect(self, connection: SandboxConnection) -> None:
        self.connection = connection
    @throttle(1)
    def update_state(self, state: HumanAgentState) -> None:
        status_bar = self.query_one(StatusBar)
        status_bar.running = state.running
        status_bar.time = state.time
    def compose(self) -> ComposeResult:
        with ContentSwitcher(initial=LoadingView.ID):
            yield LoadingView()
            with VerticalScroll(id=self.SANDBOX_VIEW_ID):
                yield StatusBar()
                yield Static(
                    id=self.SANDBOX_INSTRUCTIONS_ID,
                    classes=self.INSTRUCTIONS_CLASS,
                    markup=False,
                )
                with Horizontal(id=self.VSCODE_LINKS_ID):
                    yield Label("Login:", classes=self.LINK_LABEL_CLASS)
                    yield VSCodeLink(
                        "VS Code Window",
                        id=self.LOGIN_VSCODE_WINDOW_ID,
                    )
                    yield Label("Login:", classes=self.LINK_LABEL_CLASS)
                    yield VSCodeLink(
                        "VS Code Terminal",
                        id=self.LOGIN_VSCODE_TERMINAL_ID,
                    )
                yield Static(
                    id=self.COMMAND_INSTRUCTIONS_ID,
                    classes=self.INSTRUCTIONS_CLASS,
                    markup=False,
                )
                yield Static(id=self.SANDBOX_COMMAND_ID, markup=False)
    def watch_connection(self, connection: SandboxConnection | None) -> None:
        if connection:
            # switch to sandbox view
            self.query_one(ContentSwitcher).current = self.SANDBOX_VIEW_ID
            # note whether we are in vscode
            vscode = can_execute_vscode_commands()
            # suffix for instructions based on whether we are in vscode
            instructions_command = "Login to the system with the following command (hold down Alt or Option to select text for copying):"
            instructions_vscode = (
                "Use the links below to login to the system within VS Code:"
            )
            # update instructions
            instructions_lbl = cast(
                Static, self.query_one(f"#{self.SANDBOX_INSTRUCTIONS_ID}")
            )
            instructions = f"""You are completing a task on a Linux system (task instructions will be presented when you login). {instructions_vscode if vscode else instructions_command}"""
            instructions_lbl.update(instructions)
            # login: vscode terminal
            vscode_links = self.query_one(f"#{self.VSCODE_LINKS_ID}")
            vscode_links.display = vscode
            terminal_btn = cast(
                VSCodeLink, self.query_one(f"#{self.LOGIN_VSCODE_TERMINAL_ID}")
            )
            terminal_btn.commands = [
                VSCodeCommand(
                    command="workbench.action.terminal.new", args=[{"location": 2}]
                ),
                VSCodeCommand(
                    command="workbench.action.terminal.sendSequence",
                    args=[{"text": f"{connection.command}\n"}],
                ),
            ]
            # login: vscode window
            window_btn = cast(
                VSCodeLink, self.query_one(f"#{self.LOGIN_VSCODE_WINDOW_ID}")
            )
            if connection.vscode_command is not None:
                window_btn.commands = [
                    VSCodeCommand(
                        command=connection.vscode_command[0],
                        args=connection.vscode_command[1:],
                    )
                ]
            # command (always available)
            command_instructions_lbl = cast(
                Static, self.query_one(f"#{self.COMMAND_INSTRUCTIONS_ID}")
            )
            command_instructions_lbl.display = vscode
            command_instructions_lbl.update(
                instructions_command.replace("Login", "Alternatively, login", 1)
            )
            command_lbl = cast(Static, self.query_one(f"#{self.SANDBOX_COMMAND_ID}"))
            command_lbl.update(connection.command)
class StatusBar(Horizontal):
    STATUS_ID = "task-status"
    TIME_ID = "task-time"
    LABEL_CLASS = "status-label"
    VALUE_CLASS = "status-value"
    DEFAULT_CSS = f"""
    StatusBar {{
        width: 1fr;
        height: 1;
        background: $surface;
        margin-bottom: 1;
        layout: grid;
        grid-size: 4 1;
        grid-columns: auto auto auto auto;
        grid-gutter: 1;
    }}
    .{LABEL_CLASS} {{
        color: $primary;
    }}
    .{VALUE_CLASS} {{
        color: $foreground;
    }}
    StatusBar Link {{
        dock: right;
        margin-right: 1;
    }}
    """
    running: reactive[bool] = reactive(True)
    time: reactive[float] = reactive(0)
    def __init__(self) -> None:
        super().__init__()
    def compose(self) -> ComposeResult:
        yield Label("Status:", classes=self.LABEL_CLASS)
        yield Static(
            "Running", id=self.STATUS_ID, classes=self.VALUE_CLASS, markup=False
        )
        yield Label(" Time:", classes=self.LABEL_CLASS)
        yield Static("0:00:00", id=self.TIME_ID, classes=self.VALUE_CLASS, markup=False)
    def watch_running(self, running: bool) -> None:
        cast(Static, self.query_one(f"#{self.STATUS_ID}")).update(
            "Running" if running else "Stopped"
        )
    def watch_time(self, time: float) -> None:
        time_display = format_progress_time(time)
        cast(Static, self.query_one(f"#{self.TIME_ID}")).update(time_display)
class LoadingView(Container):
    ID = "human-agent-loading-view"
    def __init__(self) -> None:
        super().__init__(id=self.ID)
    def compose(self) -> ComposeResult:
        yield LoadingIndicator()
        yield Button()  # add focusable widget so the tab can activate
class VSCodeLink(Link):
    def __init__(
        self,
        text: str,
        *,
        url: str | None = None,
        tooltip: str | None = None,
        name: str | None = None,
        id: str | None = None,
        classes: str | None = None,
        disabled: bool = False,
    ) -> None:
        super().__init__(
            text,
            url=url,
            tooltip=tooltip,
            name=name,
            id=id,
            classes=classes,
            disabled=disabled,
        )
        self.commands: list[VSCodeCommand] = []
    def on_click(self) -> None:
        execute_vscode_commands(self.commands)

================
File: src/inspect_ai/solver/_human_agent/service.py
================
from inspect_ai.model import ModelOutput
from inspect_ai.util._sandbox import sandbox
from inspect_ai.util._sandbox.service import sandbox_service
from .._task_state import TaskState
from .commands.command import HumanAgentCommand
from .state import HumanAgentState
from .view import HumanAgentView
async def run_human_agent_service(
    state: TaskState, commands: list[HumanAgentCommand], view: HumanAgentView | None
) -> TaskState:
    # initialise agent state
    instructions = "\n\n".join([message.text for message in state.messages]).strip()
    agent_state = HumanAgentState(instructions=instructions)
    # extract service methods from commands
    methods = {
        command.name: command.service(agent_state)
        for command in commands
        if "service" in command.contexts
    }
    # callback to check if task is completed (use this to periodically
    # update the view with the current state)
    def task_is_completed() -> bool:
        if view:
            view.update_state(agent_state)
        return agent_state.answer is not None
    # run the service
    await sandbox_service(
        name="human_agent",
        methods=methods,
        until=task_is_completed,
        sandbox=sandbox(),
    )
    # set the answer if we have one
    if agent_state.answer is not None:
        state.output = ModelOutput.from_content("human_agent", agent_state.answer)
    # return state
    return state

================
File: src/inspect_ai/solver/_human_agent/state.py
================
from time import time as current_time
from pydantic import BaseModel, Field
from inspect_ai.scorer._metric import Score
from inspect_ai.util._store_model import StoreModel
class IntermediateScoring(BaseModel):
    time: float
    scores: list[Score]
class HumanAgentState(StoreModel):
    instructions: str
    """Task instructions."""
    @property
    def running(self) -> bool:
        """Is the task currently running?"""
        return self.running_state
    @running.setter
    def running(self, running: bool) -> None:
        """Set current running state."""
        # if we are flipping to running mode then update started running
        if not self.running_state and running:
            self.started_running = current_time()
        # if we are exiting running mode then update accumulated time
        if self.running_state and not running:
            self.accumulated_time = self.time
        # update running
        self.running_state = running
    @property
    def time(self) -> float:
        """Total time spend on task."""
        running_time = current_time() - self.started_running if self.running else 0
        return self.accumulated_time + running_time
    scorings: list[IntermediateScoring] = Field(default_factory=list)
    """Intermediate scorings yielded by `task score`"""
    answer: str | None = Field(default=None)
    """Final answer provided in `task submit`"""
    logs: dict[str, str] = Field(default_factory=dict)
    """Session logs generated by `script` """
    # internal state variables used by running and time properties
    running_state: bool = Field(default=True)
    started_running: float = Field(default_factory=current_time)
    accumulated_time: float = Field(default=0.0)

================
File: src/inspect_ai/solver/_human_agent/view.py
================
from typing import Protocol
from inspect_ai.util import SandboxConnection
from .state import HumanAgentState
class HumanAgentView(Protocol):
    def connect(self, connection: SandboxConnection) -> None: ...
    def update_state(self, state: HumanAgentState) -> None: ...
class ConsoleView(HumanAgentView):
    """Fallback view for when we aren't running fullscreen UI."""
    def connect(self, connection: SandboxConnection) -> None:
        print(
            "You are completing a task on a Linux system (task instructions will be presented "
            + "when you login). Login to the system with the following command:\n"
        )
        print(f"{connection.command}\n")
    def update_state(self, state: HumanAgentState) -> None:
        pass

================
File: src/inspect_ai/solver/_limit.py
================
from typing import Literal
from ._task_state import TaskState
class SampleLimitExceededError(Exception):
    """Exception raised when a sample limit is exceeded.
    Args:
       type: Type of limit exceeded.
       value: Value compared to
       limit: Limit applied.
       message (str | None): Optional. Human readable message.
    """
    def __init__(
        self,
        type: Literal["message", "time", "working", "token", "operator", "custom"],
        *,
        value: int,
        limit: int,
        message: str | None = None,
        state: TaskState | None = None,
    ) -> None:
        self.type = type
        self.value = value
        self.limit = limit
        self.message = f"Exceeded {type} limit: {limit:,}"
        self.state = state
        super().__init__(message)
    def with_state(self, state: TaskState) -> "SampleLimitExceededError":
        return SampleLimitExceededError(
            self.type,
            value=self.value,
            limit=self.limit,
            message=self.message,
            state=state,
        )

================
File: src/inspect_ai/solver/_multiple_choice.py
================
import logging
import re
from enum import Enum
from random import Random
from typing import Match, TypedDict
from typing_extensions import Unpack
from inspect_ai._util.logger import warn_once
from inspect_ai.util import resource
from ._solver import Generate, Solver, solver
from ._task_state import Choices, TaskState
logger = logging.getLogger(__name__)
SINGLE_ANSWER_TEMPLATE = r"""
Answer the following multiple choice question. The entire content of your response should be of the following format: 'ANSWER: $LETTER' (without quotes) where LETTER is one of {letters}.
{question}
{choices}
""".strip()
SINGLE_ANSWER_TEMPLATE_COT = r"""
Answer the following multiple choice question. The last line of your response should be of the following format: 'ANSWER: $LETTER' (without quotes) where LETTER is one of {letters}. Think step by step before answering.
{question}
{choices}
""".strip()
MULTIPLE_ANSWER_TEMPLATE = r"""
Answer the following multiple choice question where multiple answers may be correct. The entire content of your response should be of the following format: 'ANSWER: $LETTERS' (without quotes) where LETTERS is one or more of {letters}.
{question}
{choices}
""".strip()
MULTIPLE_ANSWER_TEMPLATE_COT = r"""
Answer the following multiple choice question where multiple answers may be correct. The last line of your response should be of the following format: 'ANSWER: $LETTERS' (without quotes) where LETTERS is one or more of {letters}. Think step by step before answering.
{question}
{choices}
""".strip()
def unshuffle_choices(choices: Choices) -> Choices:
    # `sorted` returns `list[Choice]`, but for consistency we wrap this back
    # into a `Choices` object
    return Choices(sorted(choices, key=lambda choice: choice.original_position))
def answer_options(choices: Choices) -> str:
    r"""
    Returns the `choices` formatted as a multiple choice question, e.g.:
    ["choice 1", "choice 2", "choice 3"] ->
        "A) choice 1\nB) choice 2\nC) choice 3"
    """
    indexes = list(range(len(choices)))
    return "\n".join(
        [f"{chr(65 + i)}) {choices[j].value}" for i, j in enumerate(indexes)]
    )
def answer_character(index: int) -> str:
    r"""
    Helper to go from array index to char, for example:
        0 -> 'A', 1 -> 'B', etc
    """
    return chr(ord("A") + index)
def answer_index(char: str) -> int:
    r"""
    Helper to go from char to array index, for example:
        'A' -> 0, 'B' -> 1, etc
    """
    return ord(char.upper()) - ord("A")
def prompt(question: str, choices: Choices, template: str) -> str:
    choices_text = answer_options(choices)
    letters = ",".join(chr(65 + i) for i in range(len(choices)))
    return template.format(
        choices=choices_text,
        letters=letters,
        question=question,
    )
def parse_answers(state: TaskState) -> Match[str] | None:
    """
    Convenience function for extracting answers from the state output.
    The generated response must be in the format 'ANSWER: <answers>',
    otherwise we can't extract what the model thinks is "true". We can be a
    bit flexible whether these are "AB" vs "A,B" vs "A B".
    However, if the answer isn't in the expected format the model has
    failed in the task so we'll ultimately just mark it as incorrect
    """
    # First check whether the string strictly ends with the expected answer
    # In this case, we're looking for a single line which contains the expected
    # ANSWER: B,C string with only whitespace after it
    match = re.search(
        r"(?i)^ANSWER\s*:\s*([A-Za-z ,]+)\s*(?:$|\n)",
        state.output.completion,
        flags=re.MULTILINE,
    )
    # If we couldn't match the strict version, we can try the less strict
    # version for backward compatibility
    if match is None:
        return re.search(
            r"(?i)ANSWER\s*:\s*([A-Za-z ,]+)(?:[^\w]|\n|$)", state.output.completion
        )
    else:
        return match
def set_choices_based_on_generated_response(state: TaskState, answers: str) -> None:
    true_answers = [answer_index(letter) for letter in answers]
    for i in range(len(state.choices)):
        if i in true_answers:
            state.choices.mark_choice(i, True)
        else:
            state.choices.mark_choice(i, False)
def pretend_we_didnt_shuffle(
    state: TaskState, original_question: str, template: str
) -> None:
    """
    If we shuffled the choices, revert them to their unshuffled versions in the message history
    This is essentially just for usability. Without doing this, matching up the
    sample choices to the target value(s) can be misleading:
        * You may be expecting a particular result from your dataset which
          doesn't show up in the logs, for example you're expecting all correct
          answers to be "A" but they're not.
        * The Log Viewer knows nothing about the `TaskState` or shuffling, it's
          just looking at the messages and the Score. This leads to
          inconsistencies between the raw `Target` and the answers we're getting
          back from the models.
    By pretending we didn't shuffle in the message history, these
    inconsistencies are easily resolved as the output is what's expected for a
    given `Sample` and `Target`.
    Note that this just rewrites message history. The `TaskState.choices` are
    left shuffled, to allow us to be transparent about this elsewhere (e.g. in
    the scoring explanation).
    """
    # First, change the prompt to the unshuffled version
    pretend_prompt = prompt(
        question=original_question,
        choices=unshuffle_choices(state.choices),
        template=template,
    )
    state.user_prompt.text = pretend_prompt
    # Then, change the last message to appear as though we received unshuffled
    # answers
    answer_text = ", ".join(
        sorted(
            [
                answer_character(choice.original_position)
                for choice in state.choices
                if choice.correct is True
            ]
        )
    )
    pretend_answer = f"ANSWER: {answer_text}"
    state.output.completion = pretend_answer
    state.messages[-1].content = pretend_answer
def valid_template(template: str) -> bool:
    """Check if a template has the required capture groups for a multiple choice question"""
    return bool(
        re.search(r"\{question\}", template) and re.search(r"\{choices\}", template)
    )
class MultipleChoiceTemplate(str, Enum):
    """Templates for multiple choice questions.
    Based on the multiple choice template in openai simple evals:
    https://github.com/openai/simple-evals/blob/main/mmlu_eval.py
    """
    SINGLE_ANSWER = SINGLE_ANSWER_TEMPLATE
    SINGLE_ANSWER_COT = SINGLE_ANSWER_TEMPLATE_COT
    MULTIPLE_ANSWER = MULTIPLE_ANSWER_TEMPLATE
    MULTIPLE_ANSWER_COT = MULTIPLE_ANSWER_TEMPLATE_COT
class DeprecatedArgs(TypedDict, total=False):
    shuffle: bool | Random
@solver
def multiple_choice(
    *,
    template: str | None = None,
    cot: bool = False,
    multiple_correct: bool = False,
    **kwargs: Unpack[DeprecatedArgs],
) -> Solver:
    """Multiple choice question solver. Formats a multiple choice question prompt, then calls `generate()`.
    Note that due to the way this solver works, it has some constraints:
    1. The `Sample` must have the `choices` attribute set.
    2. The only built-in compatible scorer is the `choice` scorer.
    3. It calls `generate()` internally, so you don't need to call it again
    Args:
      template: Template to use for the multiple choice question.
        The defaults vary based on the options and are taken from the `MultipleChoiceTemplate` enum. The template will have questions and possible answers substituted into it before being sent to the model. Consequently it requires three specific template variables:
          - `{question}`: The question to be asked.
          - `{choices}`: The choices available, which will be formatted as a
            list of A) ... B) ... etc. before sending to the model.
          - `{letters}`: (optional) A string of letters representing the choices, e.g.
            "A,B,C". Used to be explicit to the model about the possible answers.
      cot: Default `False`. Whether the solver should perform chain-of-thought
        reasoning before answering. NOTE: this has no effect if you provide a custom template.
      multiple_correct: Default `False`. Whether to allow multiple
        answers to the multiple choice question. For example, "What numbers are
        squares? A) 3, B) 4, C) 9" has multiple correct answers, B and C. Leave
        as `False` if there's exactly one correct answer from the choices
        available. NOTE: this has no effect if you provide a custom template.
      **kwargs (Any): Deprecated arguments for backward compatibility.
    #### Shuffling
    You can shuffle choices when you load your dataset by using the `shuffle_choices` method or parameter of the datasets API.
    """
    shuffle: bool | Random = False
    if "shuffle" in kwargs:
        shuffle = kwargs["shuffle"]
        if shuffle:
            warn_once(
                logger,
                "The multiple choice shuffle parameter is deprecated. Please shuffle choices at the time your dataset is read by using the shuffle_choices method/parameter of the datasets API.",
            )
    if template and not valid_template(template):
        raise ValueError(
            "The template must contain '{question}' and '{choices}' placeholders for string substitution."
        )
    if template is None:
        if multiple_correct:
            if cot:
                template = MULTIPLE_ANSWER_TEMPLATE_COT
            else:
                template = MULTIPLE_ANSWER_TEMPLATE
        else:
            if cot:
                template = SINGLE_ANSWER_TEMPLATE_COT
            else:
                template = SINGLE_ANSWER_TEMPLATE
    template = resource(template)
    if shuffle is True:
        shuffle = Random()
    async def solve(state: TaskState, generate: Generate) -> TaskState:
        if not state.choices:
            raise ValueError("The multiple_choice solver requires samples with choices")
        if isinstance(shuffle, Random):
            state.choices.shuffle(shuffle)
        # Memoise the current prompt (which is the raw "question" part of the
        # sample). Required in case we unshuffle, because we then alter message
        # history based on the multiple-choice template.
        original_question = state.user_prompt.text
        state.user_prompt.text = prompt(
            question=state.user_prompt.text,
            choices=state.choices,
            template=str(template),
        )
        state = await generate(state)
        answers = parse_answers(state)
        if answers and answers.group(1):
            # If we've found answers, update the state appropriately
            set_choices_based_on_generated_response(
                state=state, answers=answers.group(1)
            )
            if shuffle:
                pretend_we_didnt_shuffle(
                    state=state,
                    original_question=original_question,
                    template=str(template),
                )
        return state
    return solve

================
File: src/inspect_ai/solver/_plan.py
================
import inspect
from logging import getLogger
from typing import Any, Awaitable, Callable, TypeVar, cast
from inspect_ai._util.interrupt import check_sample_interrupt
from inspect_ai._util.registry import (
    RegistryInfo,
    is_registry_object,
    registry_add,
    registry_create,
    registry_info,
    registry_name,
    registry_tag,
)
from ._solver import Generate, Solver
from ._task_state import TaskState
logger = getLogger(__name__)
class Plan(Solver):
    """Task plan: List of solvers with an optional finishing solver.
    The optional `finish` solver is called after executing the steps (including in the case
    where the steps were exited early due to `TaskState.completed = True`, `message_limit`,
    or `token_limit`).
    The optional `cleanup` function is called when the plan is complete (even if the plan
    is terminated due to an exception).
    """
    def __init__(
        self,
        steps: Solver | list[Solver],
        finish: Solver | None = None,
        cleanup: Callable[[TaskState], Awaitable[None]] | None = None,
        name: str | None = None,
        internal: bool = False,
    ) -> None:
        """Create a task plan.
        Args:
          steps (list[Solver]): Solvers to run for this plan.
          finish (Solver | None): Finishing solver that is always run even for early exit.
            Note that this solver is NOT run when exception are thrown (use `cleanup` for this)
          cleanup (Callable[[TaskState], Awaitable[None]] | None): Optional cleanup handler that
            is called at the end (even if an exception occurs). Note that this function takes
            a `TaskState` but does not return one (it is only for cleanup not for transforming
            the state).
          name (str | None): Optional name for plan (for log files).
          internal (bool): Internal use of Plan (prevent deprecation warning)
        """
        if isinstance(steps, Solver):
            self.steps = [steps]
        else:
            self.steps = steps
        self.finish = finish
        self.cleanup = cleanup
        self._name = name
        if not internal:
            from inspect_ai._util.logger import warn_once
            warn_once(
                logger,
                "Plan is deprecated: use chain() to compose a list of solvers.",
            )
    @property
    def name(self) -> str:
        if self._name is not None:
            return self._name
        elif is_registry_object(self):
            return registry_info(self).name
        else:
            return "plan"
    steps: list[Solver]
    """Solvers to run for this plan."""
    finish: Solver | None = None
    """Finishing solver that is always run even for early exit."""
    cleanup: Callable[[TaskState], Awaitable[None]] | None = None
    """Function  called at the end of the plan (even if an exception occurs).
    Note that this function takes a `TaskState` but does not return one
    (it is only for cleanup not for transforming the state). Note also that
    this function should be declared `async`.
    """
    async def __call__(
        self,
        state: TaskState,
        generate: Generate,
    ) -> TaskState:
        from ._transcript import solver_transcript
        try:
            # execute steps
            for index, solver in enumerate(self.steps):
                # run solver
                with solver_transcript(solver, state) as st:
                    state = await solver(state, generate)
                    st.complete(state)
                # check for completed
                if state.completed:
                    # exit loop
                    break
            # execute finish
            if self.finish:
                with solver_transcript(self.finish, state) as st:
                    state = await self.finish(state, generate)
                    st.complete(state)
                check_sample_interrupt()
        finally:
            # always do cleanup if we have one
            if self.cleanup:
                try:
                    await self.cleanup(state)
                    check_sample_interrupt()
                except Exception as ex:
                    logger.warning(f"Exception occurred during plan cleanup: {ex}")
        return state
PlanType = TypeVar("PlanType", bound=Callable[..., Plan])
def plan(*plan: PlanType | None, name: str | None = None, **attribs: Any) -> Any:
    r"""Decorator for registering plans.
    Args:
      *plan (PlanType): Function returning `Plan` targeted by
        plain plan decorator without attributes (e.g. `@plan`)
      name (str | None):
        Optional name for plan. If the decorator has no name
        argument then the name of the function
        will be used to automatically assign a name.
      **attribs: (dict[str,Any]): Additional plan attributes.
    Returns:
        Plan with registry attributes.
    """
    def create_plan_wrapper(plan_type: PlanType) -> PlanType:
        # get the name and params
        plan_name = registry_name(plan_type, name or getattr(plan_type, "__name__"))
        params = list(inspect.signature(plan_type).parameters.keys())
        # create and return the wrapper
        def wrapper(*w_args: Any, **w_kwargs: Any) -> Plan:
            # create the plan
            plan = plan_type(*w_args, **w_kwargs)
            # tag it
            registry_tag(
                plan_type,
                plan,
                RegistryInfo(
                    type="plan",
                    name=plan_name,
                    metadata=dict(attribs=attribs, params=params),
                ),
                *w_args,
                **w_kwargs,
            )
            # return it
            return plan
        return plan_register(
            plan=cast(PlanType, wrapper),
            name=plan_name,
            attribs=attribs,
            params=params,
        )
    from inspect_ai._util.logger import warn_once
    warn_once(
        logger,
        "@plan is deprecated: use @solver and chain() to compose a list of solvers.",
    )
    if plan:
        return create_plan_wrapper(cast(PlanType, plan[0]))
    else:
        return create_plan_wrapper
def plan_register(
    plan: PlanType, name: str, attribs: dict[str, Any], params: list[str]
) -> PlanType:
    r"""Register a plan.
    Args:
        plan (PlanType): function that returns a Plan
        name (str): Name of plan
        attribs (dict[str,Any]): Attributes of plan decorator
        params (list[str]): Plan parameter names
    Returns:
        Plan with registry attributes.
    """
    registry_add(
        plan,
        RegistryInfo(
            type="plan", name=name, metadata=dict(attribs=attribs, params=params)
        ),
    )
    return plan
def plan_create(name: str, **kwargs: Any) -> Plan:
    r"""Create a Plan based on its registered name.
    Args:
        name (str): Name of plan
        **kwargs (dict): Optional creation arguments for the plan
    Returns:
        Plan with registry info attribute
    """
    return cast(Plan, registry_create("plan", name, **kwargs))

================
File: src/inspect_ai/solver/_prompt.py
================
from typing import Any
from inspect_ai._util.dict import omit
from inspect_ai._util.format import format_template
from inspect_ai.model._chat_message import (
    ChatMessageAssistant,
    ChatMessageSystem,
    ChatMessageUser,
)
from inspect_ai.util import resource
from ._solver import Generate, Solver, solver
from ._task_state import TaskState
from ._util import append_system_message
@solver
def prompt_template(template: str, **params: Any) -> Solver:
    """Parameterized prompt template.
    Prompt template containing a `{prompt}` placeholder and any
    number of additional `params`. All values contained in sample
    `metadata` and `store` are also automatically included in the
    `params`.
    Args:
      template: Template for prompt.
      **params: Parameters to fill into the template.
    Returns:
      A solver that uses the specified prompt template.
    """
    # determine the prompt template
    prompt_template = resource(template)
    async def solve(state: TaskState, generate: Generate) -> TaskState:
        prompt = state.user_prompt
        kwargs = omit(state.metadata | state.store._data, ["prompt"]) | params
        prompt.text = format_template(prompt_template, {"prompt": prompt.text} | kwargs)
        return state
    return solve
@solver
def system_message(template: str, **params: Any) -> Solver:
    """Solver which inserts a system message into the conversation.
    System message template containing any number of optional `params`.
    for substitution using the `str.format()` method. All values
    contained in sample `metadata` and `store` are also automatically
    included in the `params`.
    The new message will go after other system messages (if there
    are none it will be inserted at the beginning of the conversation).
    Args:
      template: Template for system message.
      **params: Parameters to fill into the template.
    Returns:
      A solver that inserts the parameterised system message.
    """
    # read template
    content = resource(template)
    async def solve(state: TaskState, generate: Generate) -> TaskState:
        kwargs = state.metadata | state.store._data | params
        append_system_message(
            state.messages, ChatMessageSystem(content=format_template(content, kwargs))
        )
        return state
    return solve
@solver
def user_message(template: str, **params: Any) -> Solver:
    """Solver which inserts a user message into the conversation.
    User message template containing any number of optional `params`.
    for substitution using the `str.format()` method. All values
    contained in sample `metadata` and `store` are also automatically
    included in the `params`.
    Args:
      template: Template for user message.
      **params: Parameters to fill into the template.
    Returns:
      A solver that inserts the parameterised user message.
    """
    # read template
    content = resource(template)
    async def solve(state: TaskState, generate: Generate) -> TaskState:
        kwargs = state.metadata | state.store._data | params
        state.messages.append(ChatMessageUser(content=format_template(content, kwargs)))
        return state
    return solve
@solver
def assistant_message(template: str, **params: Any) -> Solver:
    """Solver which inserts an assistant message into the conversation.
    Assistant message template containing any number of optional `params`.
    for substitution using the `str.format()` method. All values
    contained in sample `metadata` and `store` are also automatically
    included in the `params`.
    Args:
      template: Template for assistant message.
      **params: Parameters to fill into the template.
    Returns:
      A solver that inserts the parameterised assistant message.
    """
    # read template
    content = resource(template)
    async def solve(state: TaskState, generate: Generate) -> TaskState:
        kwargs = state.metadata | state.store._data | params
        state.messages.append(
            ChatMessageAssistant(content=format_template(content, kwargs))
        )
        return state
    return solve
DEFAULT_COT_TEMPLATE = r"""
{prompt}
Before answering, reason in a step-by-step manner as to get the right answer. Provide your answer at the end on its own line in the form "ANSWER: $ANSWER" (without quotes) where $ANSWER is the answer to the question.
"""
@solver
def chain_of_thought(template: str = DEFAULT_COT_TEMPLATE) -> Solver:
    """Solver which modifies the user prompt to encourage chain of thought.
    Args:
       template: String or path to file containing CoT template.
          The template uses a single variable: `prompt`.
    """
    async def solve(state: TaskState, generate: Generate) -> TaskState:
        state.user_prompt.text = template.format(prompt=state.user_prompt.text)
        return state
    return solve

================
File: src/inspect_ai/solver/_solver.py
================
import inspect
from dataclasses import dataclass, field
from functools import wraps
from typing import (
    Any,
    Callable,
    Literal,
    ParamSpec,
    Protocol,
    cast,
    overload,
    runtime_checkable,
)
from typing_extensions import Unpack
from inspect_ai._util._async import is_callable_coroutine
from inspect_ai._util.interrupt import check_sample_interrupt
from inspect_ai._util.registry import (
    RegistryInfo,
    registry_add,
    registry_create,
    registry_name,
    registry_tag,
)
from inspect_ai.model import CachePolicy, GenerateConfigArgs
from ._task_state import TaskState, set_sample_state
@runtime_checkable
class Generate(Protocol):
    async def __call__(
        self,
        state: TaskState,
        tool_calls: Literal["loop", "single", "none"] = "loop",
        cache: bool | CachePolicy = False,
        **kwargs: Unpack[GenerateConfigArgs],
    ) -> TaskState:
        """Generate using the model and add the assistant message to the task state.
        Args:
            state: Beginning task state.
            tool_calls:
                - `"loop"` resolves tools calls and then invokes `generate()`,
                    proceeding in a loop which terminates when there are no more
                    tool calls, or `message_limit` or `token_limit` is exceeded.
                    This is the default behavior.
                - `"single"` resolves at most a single set of tool calls and then returns.
                - `"none"` does not resolve tool calls at all (in this
                    case you will need to invoke `call_tools()` directly).
            cache: Caching behaviour for generate responses (defaults to no caching).
            **kwargs: Optional generation config arguments.
        Returns:
            Updated TaskState.
        """
        ...
@dataclass(frozen=True)
class SolverSpec:
    """Solver specification used to (re-)create solvers."""
    solver: str
    """Solver name (simple name or file.py@name)."""
    args: dict[str, Any] = field(default_factory=dict)
    """Solver arguments."""
@runtime_checkable
class Solver(Protocol):
    async def __call__(
        self,
        state: TaskState,
        generate: Generate,
    ) -> TaskState:
        r"""Contribute to solving an evaluation task.
        Transform a `TaskState`, returning the new state. Solvers may
        optionally call the `generate()` function to create a new
        state resulting from model generation. Solvers may also do
        prompt engineering or other types of elicitation.
        Args:
          state: State for tasks being evaluated.
          generate: Function for generating outputs.
        Returns:
          Updated TaskState.
        Examples:
          ```python
          @solver
          def prompt_cot(template: str) -> Solver:
              def solve(state: TaskState, generate: Generate) -> TaskState:
                  # insert chain of thought prompt
                  return state
              return solve
          ```
        """
        ...
P = ParamSpec("P")
def solver_register(solver: Callable[P, Solver], name: str = "") -> Callable[P, Solver]:
    r"""Register a function or class as a solver.
    Args:
        solver (Callable[P, Solver]):
            Function that returns a Solver or class derived Solver.
        name (str): Name of solver (Optional, defaults to object name)
    Returns:
        Solver with registry attributes.
    """
    solver_name = name if name else getattr(solver, "__name__")
    registry_add(solver, RegistryInfo(type="solver", name=solver_name))
    return solver
def solver_create(name: str, **kwargs: Any) -> Solver:
    r"""Create a Solver based on its registered name.
    Args:
        name (str): Name of solver (Optional, defaults to object name)
        **kwargs (dict): Optional creation arguments for the solver
    Returns:
        Solver with registry info attribute
    """
    return cast(Solver, registry_create("solver", name, **kwargs))
@overload
def solver(name: str) -> Callable[[Callable[P, Solver]], Callable[P, Solver]]: ...
@overload
def solver(name: Callable[P, Solver]) -> Callable[P, Solver]: ...
def solver(
    name: str | Callable[P, Solver],
) -> Callable[[Callable[P, Solver]], Callable[P, Solver]] | Callable[P, Solver]:
    r"""Decorator for registering solvers.
    Args:
        name:
            Optional name for solver. If the decorator has no name
            argument then the name of the underlying Callable[P, Solver]
            object will be used to automatically assign a name.
    Returns:
        Solver with registry attributes.
    Examples:
        ```python
        @solver
        def prompt_cot(template: str) -> Solver:
            def solve(state: TaskState, generate: Generate) -> TaskState:
                # insert chain of thought prompt
                return state
            return solve
        ```
    """
    # create_solver_wrapper:
    #  (a) Add the SolverType to the registry using the appropriately
    #      package-namespaced name
    #  (b) Ensure that instances of Solver created by SolverType also
    #      carry registry info.
    def create_solver_wrapper(
        solver_type: Callable[P, Solver], name: str | None = None
    ) -> Callable[P, Solver]:
        solver_name = registry_name(
            solver_type, name if name else getattr(solver_type, "__name__")
        )
        @wraps(solver_type)
        def solver_wrapper(*args: P.args, **kwargs: P.kwargs) -> Solver:
            solver = solver_type(*args, **kwargs)
            if not is_callable_coroutine(solver):
                raise TypeError(f"'{solver}' is not declared as an async callable.")
            # if the solver is a class then we inject state tracking
            # by patching the __call__ method (this is because we
            # want to preserve the type, especially for code that e.g.
            # checks for Chain or Plan)
            if inspect.isclass(type(solver)):
                original_call = solver.__call__
                @wraps(original_call)
                async def call_with_state(
                    state: TaskState, generate: Generate
                ) -> TaskState:
                    state = await original_call(state, generate)
                    check_sample_interrupt()
                    set_sample_state(state)
                    return state
                registered_solver = solver
                setattr(registered_solver, "__call__", call_with_state)
            # if its a function then use ordinary @wraps to preserve
            # the wrapped solver
            else:
                @wraps(solver)
                async def registered_solver(
                    state: TaskState, generate: Generate
                ) -> TaskState:
                    state = await solver(state, generate)
                    check_sample_interrupt()
                    set_sample_state(state)
                    return state
            registry_tag(
                solver_type,
                registered_solver,
                RegistryInfo(type="solver", name=solver_name),
                *args,
                **kwargs,
            )
            return registered_solver
        # functools.wraps overrides the return type annotation of the inner function, so
        # we explicitly set it again
        solver_wrapper.__annotations__["return"] = Solver
        return solver_register(cast(Callable[P, Solver], solver_wrapper), solver_name)
    # for decorators with an explicit name, one more wrapper for the name
    if isinstance(name, str):
        def wrapper(solver_type: Callable[..., Solver]) -> Callable[..., Solver]:
            return create_solver_wrapper(solver_type, name)
        return wrapper
    # create a solver wrapper for the passed solver_type
    else:
        solver_type = name
        return create_solver_wrapper(solver_type)
@solver
def generate(
    tool_calls: Literal["loop", "single", "none"] = "loop",
    cache: bool | CachePolicy = False,
    **kwargs: Unpack[GenerateConfigArgs],
) -> Solver:
    r"""Generate output from the model and append it to task message history.
    generate() is the default solver if none is specified for a given task.
    Args:
      tool_calls (Literal["loop", "single", "none"]): Resolve tool calls:
        - `"loop"` resolves tools calls and then invokes `generate()`,
            proceeding in a loop which terminates when there are no more
            tool calls or `message_limit` or `token_limit` is exceeded.
            This is the default behavior.
        - `"single"` resolves at most a single set of tool calls and then returns.
        - `"none"` does not resolve tool calls at all (in this
            case you will need to invoke `call_tools()` directly).
      cache: (bool | CachePolicy):
        Caching behaviour for generate responses (defaults to no caching).
      **kwargs: Optional generation config arguments.
    """
    # call generate on the tasks
    async def solve(state: TaskState, generate: Generate) -> TaskState:
        return await generate(state, tool_calls=tool_calls, cache=cache, **kwargs)
    # return solve
    return solve

================
File: src/inspect_ai/solver/_task_state.py
================
from collections.abc import Sequence
from contextvars import ContextVar
from copy import deepcopy
from dataclasses import dataclass
from itertools import tee
from random import Random
from typing import Any, Iterable, SupportsIndex, Type, Union, cast, overload
from pydantic_core import to_jsonable_python
from shortuuid import uuid
from inspect_ai._util.interrupt import check_sample_interrupt
from inspect_ai.dataset._dataset import MT, Sample, metadata_as
from inspect_ai.model import (
    ChatMessage,
    ChatMessageUser,
    ModelName,
    ModelOutput,
)
from inspect_ai.model._call_tools import tools_info
from inspect_ai.model._chat_message import ChatMessageBase
from inspect_ai.model._model import sample_total_tokens
from inspect_ai.scorer._metric import Score
from inspect_ai.scorer._target import Target
from inspect_ai.tool import Tool, ToolChoice
from inspect_ai.tool._tool_def import ToolDef
from inspect_ai.util._store import Store, store_jsonable
from inspect_ai.util._store_model import SMT
@dataclass
class Choice:
    """
    A `Choice` represents a single choice in a multiple choice question.
    It is only relevant for the `multiple_choice` solver and corresponding
    `choice` scorer.
    """
    value: str
    """The original value of the choice from the `Sample`."""
    correct: bool | None
    """Did the model think this choice satisfies the question? `None`
    indicates this has not been set yet"""
    original_position: int
    """Choices may be re-ordered during processing, this represents the
    original position in the sample's list of choices"""
class Choices(Sequence[Choice]):
    """
    Wrapper class for a list of `Choice` objects.
    Primarily simply to abstract away implementations of choice-specific
    functionality from the already-big `TaskState` class.
    """
    def __init__(self, choices: list[str] | list[Choice]) -> None:
        """
        Setter for choices, intended to only be used with the `multiple_choice` scorer.
        Choices come from a list of choices for the sample, specifically used by
        the `multiple_choice` scorer.
        For example, if the sample was a multiple choice question like "What is
        the capital of France? A) Paris B) London C) Berlin", we would store the
        possible answers here.
        """
        self._choices: list[Choice] = []
        for i, choice in enumerate(choices):
            if isinstance(choice, str):
                self._choices.append(
                    Choice(value=choice, correct=None, original_position=i)
                )
            elif isinstance(choice, Choice):
                self._choices.append(choice)
    @overload
    def __getitem__(self, index: int) -> Choice: ...
    @overload
    def __getitem__(self, index: slice) -> Sequence[Choice]: ...
    def __getitem__(self, index: Union[int, slice]) -> Union[Choice, Sequence[Choice]]:
        return self._choices[index]
    def __len__(self) -> int:
        return len(self._choices)
    def mark_choice(self, index: int, correct: bool) -> None:
        """Set the value of a specific choice"""
        self._choices[index].correct = correct
    def shuffle(self, rand: Random = Random()) -> None:
        """
        Shuffle the choice order, setting the `original_position` so they can be mapped back to their original order.
        Some evals will shuffle the choices from the original sample to try to
        avoid the model answering correctly due to fine-tuning (or similar) on
        specific datasets.
        """
        shuffled_positions = list(range(len(self._choices)))
        rand.shuffle(shuffled_positions)
        shuffled_choices = [Choice("notachoice", None, -1)] * len(self._choices)
        for i, shuffled_position in enumerate(shuffled_positions):
            shuffled_choices[i] = self._choices[shuffled_position]
            shuffled_choices[i].original_position = shuffled_position
        self._choices = shuffled_choices
    def prompt(self, question: str, template: str) -> str:
        """Format a prompt for these choices.
        Args:
          question (str): Question to pose.
          template (str): Prompt template
        Returns:
          Prompt for question and choices using the provided template.
        """
        from ._multiple_choice import prompt
        return prompt(question, self, template)
class TaskState:
    """
    The `TaskState` represents the internal state of the `Task` being run for a single `Sample`.
    The `TaskState` is passed to and returned from each solver during a sample's
    evaluation. It allows us to manipulated the message history, the tools
    available to the model, the final output of the model, and whether the task
    is completed or has hit a limit.
    """
    def __init__(
        self,
        model: ModelName,
        sample_id: int | str,
        epoch: int,
        input: str | list[ChatMessage],
        messages: list[ChatMessage],
        target: Target = Target(""),
        choices: list[str] | None = [],
        output: ModelOutput | None = None,
        message_limit: int | None = None,
        token_limit: int | None = None,
        completed: bool = False,
        metadata: dict[str, Any] = {},
    ) -> None:
        self._model = model
        self._sample_id = sample_id
        self._epoch = epoch
        self._input = input
        self._target = target
        self._metadata = metadata
        self._messages: list[ChatMessage] = ChatMessageList(messages, self)
        self._tools: list[Tool] = []
        self._output = output if output else ModelOutput(model=str(model))
        self._message_limit = message_limit
        self._token_limit = token_limit
        self._completed = completed
        self._store = Store()
        self._uuid = uuid()
        if choices:
            self.choices = Choices(choices)
        else:
            self.choices = Choices([])
    @property
    def model(self) -> ModelName:
        """Name of model being evaluated."""
        return self._model
    @property
    def sample_id(self) -> int | str:
        """Unique id for sample."""
        return self._sample_id
    @property
    def epoch(self) -> int:
        """Epoch number for sample."""
        return self._epoch
    @property
    def input(self) -> str | list[ChatMessage]:
        """Input from the `Sample`, should be considered immutable."""
        return self._input
    @property
    def input_text(self) -> str:
        """
        Convenience function for accessing the initial input from the `Sample` as a string.
        If the `input` is a `list[ChatMessage]`, this will return the text from
        the first chat message
        """
        if isinstance(self._input, str):
            return self._input
        else:
            input = next(
                (message.text for message in self._input if message.role == "user"),
                None,
            )
            if input:
                return input
            else:
                raise ValueError(
                    "input_text requested from TaskState but none available"
                )
    @property
    def user_prompt(self) -> ChatMessageUser:
        """User prompt for this state.
        Tasks are very general and can have may types of inputs.
        However, in many cases solvers assume they can interact with
        the state as a "chat" in a predictable fashion (e.g. prompt
        engineering solvers). This property enables easy read and
        write access to the user chat prompt. Raises an
        exception if there is no user prompt
        """
        prompt = next((m for m in self.messages if m.role == "user"), None)
        if prompt:
            return prompt
        else:
            raise ValueError("user_prompt requested from TaskState but none available")
    @property
    def metadata(self) -> dict[str, Any]:
        """Metadata from the `Sample` for this `TaskState`"""
        return self._metadata
    @metadata.setter
    def metadata(self, metadata: dict[str, Any]) -> None:
        self._metadata = metadata
    @property
    def messages(self) -> list[ChatMessage]:
        """
        Chat conversation history for sample.
        This will generally get appended to every time a `generate` call is made
        to the model. Useful for both debug and for solvers/scorers to assess
        model performance or choose the next step.
        """
        return self._messages
    @messages.setter
    def messages(self, messages: list[ChatMessage]) -> None:
        self._messages = ChatMessageList(messages, self)
    @property
    def output(self) -> ModelOutput:
        """
        The 'final' model output once we've completed all solving.
        For simple evals this may just be the last `message` from the
        conversation history, but more complex solvers may set this directly.
        """
        return self._output
    @output.setter
    def output(self, output: ModelOutput) -> None:
        self._output = output
    @property
    def store(self) -> Store:
        """Store for shared data"""
        return self._store
    @property
    def tools(self) -> list[Tool]:
        """Tools available to the model."""
        return self._tools
    @tools.setter
    def tools(self, tools: list[Tool | ToolDef]) -> None:
        self._tools.clear()
        for tool in tools:
            self._tools.append(tool if isinstance(tool, Tool) else tool.as_tool())
    tool_choice: ToolChoice | None = None
    """Tool choice directive."""
    @property
    def max_messages(self) -> int | None:
        """Deprecated (use message_limit)."""
        return self.message_limit
    @max_messages.setter
    def max_messages(self, messages: int | None) -> None:
        """Deprecated (use message_limit)."""
        self.message_limit = messages
    @property
    def message_limit(self) -> int | None:
        """Limit on total messages allowed per conversation."""
        return self._message_limit
    @message_limit.setter
    def message_limit(self, messages: int | None) -> None:
        """Set limit on total messages allowed per conversation."""
        self._message_limit = messages
        from inspect_ai.log._samples import set_active_sample_message_limit
        set_active_sample_message_limit(messages)
    @property
    def token_limit(self) -> int | None:
        """Limit on total tokens allowed per conversation."""
        return self._token_limit
    @token_limit.setter
    def token_limit(self, tokens: int | None) -> None:
        """Set limit on total tokens allowed per conversation."""
        self._token_limit = tokens
        from inspect_ai.log._samples import set_active_sample_token_limit
        set_active_sample_token_limit(tokens)
    @property
    def token_usage(self) -> int:
        """Total tokens used for the current sample."""
        return sample_total_tokens()
    @property
    def completed(self) -> bool:
        """Is the task completed.
        Additionally, checks message and token limits and raises if they are exceeded, and also checks for an operator interrupt of the sample.
        """
        from inspect_ai.log._samples import set_active_sample_total_messages
        from ._limit import SampleLimitExceededError
        # update messages
        set_active_sample_total_messages(len(self.messages))
        if self._completed:
            return True
        elif self.message_limit and len(self.messages) >= self.message_limit:
            raise SampleLimitExceededError(
                "message",
                value=len(self.messages),
                limit=self.message_limit,
                state=self,
            )
        elif self.token_limit and self.token_usage >= self.token_limit:
            raise SampleLimitExceededError(
                "token", value=self.token_usage, limit=self.token_limit, state=self
            )
        else:
            check_sample_interrupt()
            return self._completed
    @completed.setter
    def completed(self, completed: bool) -> None:
        """Set the completed status."""
        self._completed = completed
    @property
    def target(self) -> Target:
        """The scoring target for this `Sample`."""
        return self._target
    scores: dict[str, Score] | None = None
    """Scores yielded by running task."""
    @property
    def uuid(self) -> str:
        """Globally unique identifier for sample run."""
        return self._uuid
    def metadata_as(self, metadata_cls: Type[MT]) -> MT:
        """Pydantic model interface to metadata.
        Args:
          metadata_cls: Pydantic model type
        Returns:
          BaseModel: Instance of metadata_cls bound to current Store.
        """
        if not self.metadata:
            raise ValueError("Sample does not have metadata")
        return metadata_as(self.metadata, metadata_cls)
    def store_as(self, model_cls: Type[SMT]) -> SMT:
        """Pydantic model interface to the store.
        Args:
          model_cls: Pydantic model type (must derive from StoreModel)
        Returns:
          StoreModel: Instance of model_cls bound to current Store.
        """
        return model_cls(store=self.store)
def sample_state() -> TaskState | None:
    return _sample_state.get(None)
def set_sample_state(state: TaskState) -> None:
    _sample_state.set(state)
_sample_state: ContextVar[TaskState] = ContextVar("sample_state")
def state_jsonable(state: TaskState | None = None) -> dict[str, Any]:
    # use current sample state if none specified
    if state is None:
        state = sample_state()
        if state is None:
            return dict()
    def as_jsonable(value: Any) -> Any:
        return to_jsonable_python(value, exclude_none=True, fallback=lambda _x: None)
    state_data = dict(
        messages=as_jsonable(state.messages),
        tools=tools_info(state.tools),
        tool_choice=state.tool_choice,
        store=store_jsonable(state.store),
        output=state.output,
        completed=state.completed,
        metadata=as_jsonable(state.metadata),
    )
    jsonable = as_jsonable(state_data)
    return cast(dict[str, Any], deepcopy(jsonable))
def sample_jsonable(sample: Sample) -> dict[str, Any]:
    jsonable = to_jsonable_python(sample, exclude_none=True, fallback=lambda _x: None)
    return cast(dict[str, Any], deepcopy(jsonable))
class ChatMessageList(list[ChatMessage]):
    def __init__(self, iterable: Iterable[ChatMessage], parent_state: TaskState):
        self.parent_state = parent_state
        items, length = self._iterable_length(iterable)
        self._check_size(length)
        super().__init__(items)
    def _check_size(self, additional_items: int = 1) -> None:
        from inspect_ai.log._samples import active_sample_message_limit
        from ._limit import SampleLimitExceededError
        messages_limit = active_sample_message_limit()
        if messages_limit is not None:
            messages = len(self) + additional_items
            if messages > messages_limit:
                raise SampleLimitExceededError(
                    "message",
                    value=messages,
                    limit=messages_limit,
                    message=None,
                    state=self.parent_state,
                )
    def append(self, item: ChatMessage) -> None:
        self._check_size()
        super().append(item)
    def extend(self, items: Iterable[ChatMessage]) -> None:
        items, length = self._iterable_length(items)
        self._check_size(length)
        super().extend(items)
    def insert(self, index: SupportsIndex, item: ChatMessage) -> None:
        self._check_size()
        super().insert(index, item)
    @overload
    def __setitem__(self, index: SupportsIndex, item: ChatMessage) -> None: ...
    @overload
    def __setitem__(self, index: slice, item: Iterable[ChatMessage]) -> None: ...
    def __setitem__(
        self, index: SupportsIndex | slice, item: ChatMessage | Iterable[ChatMessage]
    ) -> None:
        if isinstance(index, slice) and not isinstance(item, ChatMessageBase):
            item, length = self._iterable_length(item)
            size_change = length - len(self[index])
            if size_change > 0:
                self._check_size(size_change)
        super().__setitem__(index, item)  # type: ignore[assignment,index]
    def _iterable_length(
        self, items: Iterable[ChatMessage]
    ) -> tuple[Iterable[ChatMessage], int]:
        items, counter = tee(items)
        length = sum(1 for _ in counter)
        return items, length

================
File: src/inspect_ai/solver/_transcript.py
================
import contextlib
from typing import Iterator
from inspect_ai._util.json import json_changes
from inspect_ai._util.registry import registry_log_name
from ._solver import Solver
from ._task_state import TaskState, state_jsonable
class SolverTranscript:
    def __init__(self, name: str, before_state: TaskState) -> None:
        self.name = name
        self.before = state_jsonable(before_state)
    def complete(self, after_state: TaskState) -> None:
        from inspect_ai.log._transcript import StateEvent, transcript
        after = state_jsonable(after_state)
        changes = json_changes(self.before, after)
        if changes:
            transcript()._event(StateEvent(changes=changes))
@contextlib.contextmanager
def solver_transcript(
    solver: Solver, state: TaskState, name: str | None = None
) -> Iterator[SolverTranscript]:
    from inspect_ai.log._transcript import transcript
    name = registry_log_name(name or solver)
    with transcript().step(name=name, type="solver"):
        yield SolverTranscript(name, state)

================
File: src/inspect_ai/solver/_use_tools.py
================
from inspect_ai.tool import Tool, ToolChoice
from inspect_ai.tool._tool_def import ToolDef
from ._solver import Generate, Solver, solver
from ._task_state import TaskState
@solver
def use_tools(
    *tools: Tool | list[Tool],
    tool_choice: ToolChoice | None = "auto",
    append: bool = False,
) -> Solver:
    """
    Inject tools into the task state to be used in generate().
    Args:
      *tools: One or more tools or lists of tools
        to make available to the model. If no tools are passed, then
        no change to the currently available set of `tools` is made.
      tool_choice: Directive indicating which
        tools the model should use. If `None` is passed, then no
        change to `tool_choice` is made.
      append: If `True`, then the passed-in tools are appended
        to the existing tools; otherwise any existing tools are
        replaced (the default)
    Returns:
        A solver that injects the tools and tool_choice into the task state.
    """
    async def solve(state: TaskState, generate: Generate) -> TaskState:
        # build up tools
        tools_update: list[Tool] = []
        # add tool function to take care of tool/tool_def
        def add_tool(tool: Tool | ToolDef) -> None:
            if isinstance(tool, ToolDef):
                tool = tool.as_tool()
            tools_update.append(tool)
        for tool in tools:
            if isinstance(tool, list):
                for t in tool:
                    add_tool(t)
            else:
                add_tool(tool)
        if len(tools_update) > 0:
            if append:
                existing_tools = state.tools
                state.tools = existing_tools + tools_update
            else:
                state.tools = tools_update
        # set tool choice if specified
        if tool_choice is not None:
            state.tool_choice = tool_choice
        # return state
        return state
    return solve

================
File: src/inspect_ai/solver/_util.py
================
from inspect_ai.model import ChatMessage, ChatMessageSystem
def append_system_message(
    messages: list[ChatMessage], message: ChatMessageSystem
) -> None:
    # find last index of any existing system message
    lastIndex = -1
    for i in list(reversed(range(0, len(messages)))):
        if isinstance(messages[i], ChatMessageSystem):
            lastIndex = i
            break
    # insert it
    messages.insert(lastIndex + 1, message)

================
File: src/inspect_ai/tool/__init__.py
================
from inspect_ai._util.content import (
    Content,
    ContentAudio,
    ContentImage,
    ContentReasoning,
    ContentText,
    ContentVideo,
)
from inspect_ai._util.deprecation import relocated_module_attribute
from ._tool import Tool, ToolError, ToolResult, tool
from ._tool_call import (
    ToolCall,
    ToolCallContent,
    ToolCallError,
    ToolCallModelInput,
    ToolCallView,
    ToolCallViewer,
)
from ._tool_choice import ToolChoice, ToolFunction
from ._tool_def import ToolDef
from ._tool_info import ToolInfo
from ._tool_params import ToolParam, ToolParams
from ._tool_with import tool_with
from ._tools._computer import computer
from ._tools._execute import bash, python
from ._tools._web_browser import web_browser
from ._tools._web_search import web_search
__all__ = [
    "bash",
    "computer",
    "python",
    "web_browser",
    "web_search",
    "tool",
    "tool_with",
    "Tool",
    "ToolCallError",
    "ToolError",
    "ToolResult",
    "Content",
    "ContentAudio",
    "ContentImage",
    "ContentReasoning",
    "ContentText",
    "ContentVideo",
    "ToolCall",
    "ToolCallContent",
    "ToolCallModelInput",
    "ToolCallView",
    "ToolCallViewer",
    "ToolChoice",
    "ToolDef",
    "ToolFunction",
    "ToolInfo",
    "ToolParam",
    "ToolParams",
]
_UTIL_MODULE_VERSION = "0.3.19"
_JSON_MODULE_VERSION = "0.3.73"
_REMOVED_IN = "0.4"
relocated_module_attribute(
    "JSONType",
    "inspect_ai.util.JSONType",
    _JSON_MODULE_VERSION,
    _REMOVED_IN,
)
relocated_module_attribute(
    "ToolEnvironment",
    "inspect_ai.util.SandboxEnvironment",
    _UTIL_MODULE_VERSION,
    _REMOVED_IN,
)
relocated_module_attribute(
    "ToolEnvironments",
    "inspect_ai.util.SandboxEnvironments",
    _UTIL_MODULE_VERSION,
    _REMOVED_IN,
)
relocated_module_attribute(
    "ToolEnvironmentSpec",
    "inspect_ai.util.SandboxEnvironmentSpec",
    _UTIL_MODULE_VERSION,
    _REMOVED_IN,
)
relocated_module_attribute(
    "tool_environment",
    "inspect_ai.util.sandbox",
    _UTIL_MODULE_VERSION,
    _REMOVED_IN,
)
relocated_module_attribute(
    "toolenv", "inspect_ai.util.sandboxenv", _UTIL_MODULE_VERSION, _REMOVED_IN
)
relocated_module_attribute(
    "web_browser_tools",
    "inspect_ai.tool.web_browser",
    "0.3.19",
    _REMOVED_IN,
)

================
File: src/inspect_ai/tool/_tool_call.py
================
from dataclasses import dataclass, field
from typing import Any, Callable, Literal
from pydantic import BaseModel, Field
from inspect_ai._util.content import Content
class ToolCallContent(BaseModel):
    """Content to include in tool call view."""
    title: str | None = Field(default=None)
    """Optional (plain text) title for tool call content."""
    format: Literal["text", "markdown"]
    """Format (text or markdown)."""
    content: str
    """Text or markdown content."""
class ToolCallView(BaseModel):
    """Custom view of a tool call.
    Both `context` and `call` are optional. If `call` is not specified
    then the view will default to a syntax highlighted Python function call.
    """
    context: ToolCallContent | None = Field(default=None)
    """Context for the tool call (i.e. current tool state)."""
    call: ToolCallContent | None = Field(default=None)
    """Custom representation of tool call."""
@dataclass
class ToolCall:
    id: str
    """Unique identifier for tool call."""
    function: str
    """Function called."""
    arguments: dict[str, Any]
    """Arguments to function."""
    type: Literal["function"]
    """Type of tool call (currently only 'function')"""
    parse_error: str | None = field(default=None)
    """Error which occurred parsing tool call."""
    view: ToolCallContent | None = field(default=None)
    """Custom view of tool call input."""
@dataclass
class ToolCallError:
    """Error raised by a tool call."""
    type: Literal[
        "parsing",
        "timeout",
        "unicode_decode",
        "permission",
        "file_not_found",
        "is_a_directory",
        "output_limit",
        "approval",
        "unknown",
    ]
    """Error type."""
    message: str
    """Error message."""
ToolCallViewer = Callable[[ToolCall], ToolCallView]
"""Custom view renderer for tool calls."""
ToolCallModelInput = Callable[[int, int, str | list[Content]], str | list[Content]]
"""Determine how tool call results are played back as model input.
The first argument is an index into the total number of tool results
for this tool in the message history, the second is the total number.
"""

================
File: src/inspect_ai/tool/_tool_choice.py
================
from dataclasses import dataclass
from typing import Literal, Union
@dataclass
class ToolFunction:
    """Indicate that a specific tool function should be called."""
    name: str
    """The name of the tool function to call."""
ToolChoice = Union[Literal["auto", "any", "none"], ToolFunction]
"""Specify which tool to call.
"auto" means the model decides; "any" means use at least one tool,
"none" means never call a tool; ToolFunction instructs the model
to call a specific function.
"""

================
File: src/inspect_ai/tool/_tool_def.py
================
from copy import copy
from typing import (
    Any,
    Callable,
    NamedTuple,
)
from inspect_ai._util.registry import (
    RegistryInfo,
    is_registry_object,
    registry_info,
    set_registry_info,
    set_registry_params,
)
from ._tool import TOOL_MODEL_INPUT, TOOL_PARALLEL, TOOL_PROMPT, TOOL_VIEWER, Tool
from ._tool_call import ToolCallModelInput, ToolCallViewer
from ._tool_description import (
    ToolDescription,
    set_tool_description,
    tool_description,
)
from ._tool_info import parse_tool_info
from ._tool_params import ToolParams
class ToolDef:
    """Tool definition."""
    def __init__(
        self,
        tool: Callable[..., Any],
        name: str | None = None,
        description: str | None = None,
        parameters: dict[str, str] | ToolParams | None = None,
        parallel: bool | None = None,
        viewer: ToolCallViewer | None = None,
        model_input: ToolCallModelInput | None = None,
    ) -> None:
        """Create a tool definition.
        Args:
          tool: Callable to execute tool.
          name: Name of tool. Discovered automatically if not specified.
          description: Description of tool. Discovered automatically
            by parsing doc comments if not specified.
          parameters: Tool parameter descriptions and types.
             Discovered automatically by parsing doc comments if not specified.
          parallel: Does the tool support parallel execution
             (defaults to True if not specified)
          viewer: Optional tool call viewer implementation.
          model_input: Optional function that determines how
              tool call results are played back as model input.
        Returns:
          Tool definition.
        """
        # tool
        self.tool = tool
        # if this is already a tool then initialise defaults from the tool
        if is_registry_object(tool) and registry_info(tool).type == "tool":
            tdef = tool_def_fields(tool)
            self.name = name or tdef.name
            self.description = description or tdef.description
            if isinstance(parameters, ToolParams):
                self.parameters = parameters
            else:
                self.parameters = tdef.parameters
                if parameters is not None:
                    apply_description_overrides(self.parameters, parameters)
            parameters = parameters if parameters is not None else tdef.parameters
            self.parallel = parallel if parallel is not None else tdef.parallel
            self.viewer = viewer or tdef.viewer
            self.model_input = model_input or tdef.model_input
        # if its not a tool then extract tool_info if all fields have not
        # been provided explicitly
        else:
            if (
                name is None
                or description is None
                or parameters is None
                or not isinstance(parameters, ToolParams)
            ):
                tool_info = parse_tool_info(tool)
                self.name = name or tool_info.name
                self.description = description or tool_info.description
                if parameters:
                    if not isinstance(parameters, ToolParams):
                        self.parameters = copy(tool_info.parameters)
                        apply_description_overrides(self.parameters, parameters)
                    else:
                        self.parameters = parameters
                else:
                    self.parameters = tool_info.parameters
            else:
                self.name = name
                self.description = description
                self.parameters = parameters
            # behavioral attributes
            self.parallel = parallel is not False
            self.viewer = viewer
            self.model_input = model_input
    tool: Callable[..., Any]
    """Callable to execute tool."""
    name: str
    """Tool name."""
    description: str
    """Tool description."""
    parameters: ToolParams
    """Tool parameter descriptions."""
    parallel: bool
    """Supports parallel execution."""
    viewer: ToolCallViewer | None
    """Custom viewer for tool call"""
    model_input: ToolCallModelInput | None
    """Custom model input presenter for tool calls."""
    def as_tool(self) -> Tool:
        """Convert a ToolDef to a Tool."""
        tool = self.tool
        info = RegistryInfo(
            type="tool",
            name=self.name,
            metadata={TOOL_PARALLEL: self.parallel, TOOL_VIEWER: self.viewer},
        )
        set_registry_info(tool, info)
        set_registry_params(tool, {})
        set_tool_description(
            tool,
            ToolDescription(
                name=self.name,
                description=self.description,
                parameters=self.parameters,
            ),
        )
        return tool
# helper function to apply description overrides
def apply_description_overrides(target: ToolParams, overrides: dict[str, str]) -> None:
    for param, value in overrides.items():
        if param not in target.properties.keys():
            raise ValueError(
                f"'{param}' is not a valid parameter for the target function."
            )
        target.properties[param].description = value
def tool_defs(
    tools: list[Tool] | list[ToolDef] | list[Tool | ToolDef],
) -> list[ToolDef]:
    return [ToolDef(tool) if isinstance(tool, Tool) else tool for tool in tools]
class ToolDefFields(NamedTuple):
    name: str
    description: str
    parameters: ToolParams
    parallel: bool
    viewer: ToolCallViewer | None
    model_input: ToolCallModelInput | None
def tool_def_fields(tool: Tool) -> ToolDefFields:
    # get tool_info
    name, prompt, parallel, viewer, model_input = tool_registry_info(tool)
    tool_info = parse_tool_info(tool)
    # if there is a description then append any prompt to the
    # the description (note that 'prompt' has been depreacted
    # in favor of just providing a description in the doc comment.
    if tool_info.description:
        if prompt:
            tool_info.description = f"{tool_info.description}. {prompt}"
    # if there is no description and there is a prompt, then use
    # the prompt as the description
    elif prompt:
        tool_info.description = prompt
    # no description! we can't proceed without one
    else:
        raise ValueError(f"Description not provided for tool function '{name}'")
    # validate that we have types/descriptions for paramters
    for param_name, param in tool_info.parameters.properties.items():
        def raise_not_provided_error(context: str) -> None:
            raise ValueError(
                f"{context} not provided for parameter '{param_name}' of tool function '{name}'."
            )
        if param.type is None and not param.anyOf and not param.enum:
            raise_not_provided_error("Unsupported type or type annotation")
        elif not param.description:
            raise_not_provided_error("Description")
    # see if the user has overriden any of the tool's descriptions
    desc = tool_description(tool)
    if desc.name:
        name = desc.name
    if desc.description:
        tool_info.description = desc.description
    if desc.parameters:
        for key, param in desc.parameters.properties.items():
            if key in tool_info.parameters.properties.keys():
                tool_info.parameters.properties[key].description = param.description
    # build tool def
    return ToolDefFields(
        name=name,
        description=tool_info.description,
        parameters=tool_info.parameters,
        parallel=parallel,
        viewer=viewer,
        model_input=model_input,
    )
def tool_registry_info(
    tool: Tool,
) -> tuple[str, str | None, bool, ToolCallViewer | None, ToolCallModelInput | None]:
    info = registry_info(tool)
    name = info.name.split("/")[-1]
    prompt = info.metadata.get(TOOL_PROMPT, None)
    parallel = info.metadata.get(TOOL_PARALLEL, True)
    viewer = info.metadata.get(TOOL_VIEWER, None)
    model_input = info.metadata.get(TOOL_MODEL_INPUT, None)
    return name, prompt, parallel, viewer, model_input

================
File: src/inspect_ai/tool/_tool_description.py
================
from dataclasses import dataclass
from ._tool import Tool
from ._tool_params import ToolParams
@dataclass
class ToolDescription:
    name: str | None = None
    description: str | None = None
    parameters: ToolParams | None = None
def tool_description(tool: Tool) -> ToolDescription:
    return getattr(tool, TOOL_DESCRIPTION, ToolDescription())
def set_tool_description(tool: Tool, description: ToolDescription) -> None:
    setattr(tool, TOOL_DESCRIPTION, description)
TOOL_DESCRIPTION = "__TOOL_DESCRIPTION__"

================
File: src/inspect_ai/tool/_tool_info.py
================
import inspect
from typing import (
    Any,
    Callable,
    Dict,
    get_args,
    get_type_hints,
)
from docstring_parser import Docstring, parse
from pydantic import BaseModel, Field
from inspect_ai.util._json import JSONType, json_schema
from ._tool_description import tool_description
from ._tool_params import ToolParam, ToolParams
class ToolInfo(BaseModel):
    """Specification of a tool (JSON Schema compatible)
    If you are implementing a ModelAPI, most LLM libraries can
    be passed this object (dumped to a dict) directly as a function
    specification. For example, in the OpenAI provider:
    ```python
    ChatCompletionToolParam(
        type="function",
        function=tool.model_dump(exclude_none=True),
    )
    ```
    In some cases the field names don't match up exactly. In that case
    call `model_dump()` on the `parameters` field. For example, in the
    Anthropic provider:
    ```python
    ToolParam(
        name=tool.name,
        description=tool.description,
        input_schema=tool.parameters.model_dump(exclude_none=True),
    )
    ```
    """
    name: str
    """Name of tool."""
    description: str
    """Short description of tool."""
    parameters: ToolParams = Field(default_factory=ToolParams)
    """JSON Schema of tool parameters object."""
def parse_tool_info(func: Callable[..., Any]) -> ToolInfo:
    # tool may already have registry attributes w/ tool info
    description = tool_description(func)
    if (
        description.name
        and description.description
        and description.parameters is not None
    ):
        return ToolInfo(
            name=description.name,
            description=description.description,
            parameters=description.parameters,
        )
    signature = inspect.signature(func)
    type_hints = get_type_hints(func)
    docstring = inspect.getdoc(func)
    parsed_docstring: Docstring | None = parse(docstring) if docstring else None
    info = ToolInfo(name=func.__name__, description="")
    for param_name, param in signature.parameters.items():
        tool_param = ToolParam()
        # Parse docstring
        docstring_info = parse_docstring(docstring, param_name)
        # Get type information from type annotations
        if param_name in type_hints:
            tool_param = json_schema(type_hints[param_name])
        # as a fallback try to parse it from the docstring
        # (this is minimally necessary for backwards compatiblity
        #  with tools gen1 type parsing, which only used docstrings)
        elif "docstring_type" in docstring_info:
            json_type = python_type_to_json_type(docstring_info["docstring_type"])
            if json_type and (json_type in get_args(JSONType)):
                tool_param = ToolParam(type=json_type)
        # Get default value
        if param.default is param.empty:
            info.parameters.required.append(param_name)
        else:
            tool_param.default = param.default
        # Add description from docstring
        if "description" in docstring_info:
            tool_param.description = docstring_info["description"]
        # append the tool param
        info.parameters.properties[param_name] = tool_param
    # Add function description if available
    if parsed_docstring:
        if parsed_docstring.description:
            info.description = parsed_docstring.description.strip()
        elif parsed_docstring.long_description:
            info.description = parsed_docstring.long_description.strip()
        elif parsed_docstring.short_description:
            info.description = parsed_docstring.short_description.strip()
        # Add examples if available
        if parsed_docstring.examples:
            examples = "\n\n".join(
                [(example.description or "") for example in parsed_docstring.examples]
            )
            info.description = f"{info.description}\n\nExamples\n\n{examples}"
    return info
def parse_docstring(docstring: str | None, param_name: str) -> Dict[str, str]:
    if not docstring:
        return {}
    parsed_docstring: Docstring = parse(docstring)
    for param in parsed_docstring.params:
        if param.arg_name == param_name:
            schema: Dict[str, str] = {"description": param.description or ""}
            if param.type_name:
                schema["docstring_type"] = param.type_name
            return schema
    return {}
def python_type_to_json_type(python_type: str) -> JSONType | None:
    match python_type:
        case "str":
            return "string"
        case "int":
            return "integer"
        case "float":
            return "number"
        case "bool":
            return "boolean"
        case "list":
            return "array"
        case "dict":
            return "object"
        case "None":
            return "null"
        case _:
            return None

================
File: src/inspect_ai/tool/_tool_params.py
================
from typing import (
    Literal,
    TypeAlias,
)
from pydantic import BaseModel, Field
from inspect_ai.util._json import JSONSchema
ToolParam: TypeAlias = JSONSchema
"""Description of tool parameter in JSON Schema format."""
class ToolParams(BaseModel):
    """Description of tool parameters object in JSON Schema format."""
    type: Literal["object"] = Field(default="object")
    """Params type (always 'object')"""
    properties: dict[str, ToolParam] = Field(default_factory=dict)
    """Tool function parameters."""
    required: list[str] = Field(default_factory=list)
    """List of required fields."""
    additionalProperties: bool = Field(default=False)
    """Are additional object properties allowed? (always `False`)"""

================
File: src/inspect_ai/tool/_tool_transcript.py
================
from pydantic import JsonValue
from rich.console import RenderableType
from rich.text import Text
from typing_extensions import Protocol
from inspect_ai._util.transcript import transcript_function, transcript_markdown
from ._tool_call import ToolCallContent
class TranscriptToolCall(Protocol):
    function: str
    arguments: dict[str, JsonValue]
    view: ToolCallContent | None
def transcript_tool_call(call: TranscriptToolCall) -> list[RenderableType]:
    content: list[RenderableType] = []
    if call.view:
        if call.view.title:
            content.append(Text.from_markup(f"[bold]{call.view.title}[/bold]\n"))
        if call.view.format == "markdown":
            content.append(transcript_markdown(call.view.content))
        else:
            content.append(call.view.content)
    else:
        content.append(transcript_function(call.function, call.arguments))
    return content

================
File: src/inspect_ai/tool/_tool_with.py
================
from copy import deepcopy
from inspect_ai._util.registry import (
    registry_info,
    registry_params,
    set_registry_info,
    set_registry_params,
)
from inspect_ai.tool._tool_call import ToolCallModelInput, ToolCallViewer
from ._tool import TOOL_MODEL_INPUT, TOOL_PARALLEL, TOOL_VIEWER, Tool
from ._tool_description import ToolDescription, set_tool_description
from ._tool_info import parse_tool_info
def tool_with(
    tool: Tool,
    name: str | None = None,
    description: str | None = None,
    parameters: dict[str, str] | None = None,
    parallel: bool | None = None,
    viewer: ToolCallViewer | None = None,
    model_input: ToolCallModelInput | None = None,
) -> Tool:
    """Tool with modifications to name and descriptions.
    Args:
       tool: Tool instance to copy and add descriptions to.
       name: Tool name (optional).
       description: Tool description (optional).
       parameters: Parameter descriptions (optional)
       parallel: Does the tool support parallel execution
          (defaults to True if not specified)
       viewer: Optional tool call viewer implementation.
       model_input: Optional function that determines how
           tool call results are played back as model input.
    Returns:
       A copy of the passed tool with the specified descriptive information.
    """
    # get the existing tool info
    tool_info = parse_tool_info(tool)
    # provide parameter overrides
    if parameters:
        signature_param_names = tool_info.parameters.properties.keys()
        for param_name in parameters.keys():
            if param_name not in signature_param_names:
                raise ValueError(
                    f"tool_with error: no parameter named '{param_name}' "
                    + f"(valid parameters are {', '.join(signature_param_names)})"
                )
            tool_info.parameters.properties[param_name].description = parameters[
                param_name
            ]
    # copy the tool and set the descriptions on the new copy
    tool_copy = deepcopy(tool)
    info = registry_info(tool).model_copy()
    if parallel is not None:
        info.metadata[TOOL_PARALLEL] = parallel
    elif viewer is not None:
        info.metadata[TOOL_VIEWER] = viewer
    elif model_input is not None:
        info.metadata[TOOL_MODEL_INPUT] = model_input
    set_registry_info(tool_copy, info)
    set_registry_params(tool_copy, registry_params(tool))
    set_tool_description(
        tool_copy,
        ToolDescription(
            name=name, description=description, parameters=tool_info.parameters
        ),
    )
    return tool_copy

================
File: src/inspect_ai/tool/_tool.py
================
import re
from functools import wraps
from logging import getLogger
from typing import (
    Any,
    Callable,
    ParamSpec,
    Protocol,
    cast,
    overload,
    runtime_checkable,
)
from inspect_ai._util.content import (
    ContentAudio,
    ContentImage,
    ContentReasoning,
    ContentText,
    ContentVideo,
)
from inspect_ai._util.registry import (
    RegistryInfo,
    registry_add,
    registry_name,
    registry_tag,
)
from ._tool_call import ToolCallModelInput, ToolCallViewer
logger = getLogger(__name__)
ToolResult = (
    str
    | int
    | float
    | bool
    | ContentText
    | ContentReasoning
    | ContentImage
    | ContentAudio
    | ContentVideo
    | list[ContentText | ContentReasoning | ContentImage | ContentAudio | ContentVideo]
)
"""Valid types for results from tool calls."""
class ToolError(Exception):
    """Exception thrown from tool call.
    If you throw a `ToolError` form within a tool call,
    the error will be reported to the model for further
    processing (rather than ending the sample). If you want
    to raise a fatal error from a tool call use an appropriate
    standard exception type (e.g. `RuntimeError`, `ValueError`, etc.)
    """
    def __init__(self, message: str) -> None:
        """Create a ToolError.
        Args:
          message: Error message to report to the model.
        """
        super().__init__(message)
        self.message = message
class ToolParsingError(ToolError):
    def __init__(self, message: str) -> None:
        super().__init__(message)
class ToolApprovalError(ToolError):
    def __init__(self, message: str | None) -> None:
        super().__init__(message or "Tool call not approved.")
@runtime_checkable
class Tool(Protocol):
    async def __call__(
        self,
        *args: Any,
        **kwargs: Any,
    ) -> ToolResult:
        r"""Additional tool that an agent can use to solve a task.
        Args:
          *args: Arguments for the tool.
          **kwargs: Keyword arguments for the tool.
        Returns:
            Result of tool call.
        Examples:
          ```python
          @tool
          def add() -> Tool:
              async def execute(x: int, y: int) -> int:
                  return x + y
              return execute
          ```
        """
        ...
P = ParamSpec("P")
def tool_register(tool: Callable[P, Tool], name: str) -> Callable[P, Tool]:
    r"""Register a function or class as a tool.
    Args:
        tool (ToolType):
            Tool function or a class derived from Tool.
        docstring (Docstring): Docstring for the tool. Used to extract arg descriptions.
        name (str): Name of tool (Optional, defaults to object name)
    Returns:
        Tool with registry attributes.
    """
    registry_add(
        tool,
        RegistryInfo(type="tool", name=name),
    )
    return tool
@overload
def tool(func: Callable[P, Tool]) -> Callable[P, Tool]: ...
@overload
def tool() -> Callable[[Callable[P, Tool]], Callable[P, Tool]]: ...
@overload
def tool(
    *,
    name: str | None = None,
    viewer: ToolCallViewer | None = None,
    model_input: ToolCallModelInput | None = None,
    parallel: bool = True,
    prompt: str | None = None,
) -> Callable[[Callable[P, Tool]], Callable[P, Tool]]: ...
def tool(
    func: Callable[P, Tool] | None = None,
    *,
    name: str | None = None,
    viewer: ToolCallViewer | None = None,
    model_input: ToolCallModelInput | None = None,
    parallel: bool = True,
    prompt: str | None = None,
) -> Callable[P, Tool] | Callable[[Callable[P, Tool]], Callable[P, Tool]]:
    r"""Decorator for registering tools.
    Args:
        func: Tool function
        name: Optional name for tool. If the decorator has no name
            argument then the name of the tool creation function
            will be used as the name of the tool.
        viewer: Provide a custom view of tool call and context.
        model_input: Provide a custom function for playing back tool results as model input.
        parallel: Does this tool support parallel execution? (defaults to `True`).
        prompt: Deprecated (provide all descriptive information about
            the tool within the tool function's doc comment)
    Returns:
        Tool with registry attributes.
    Examples:
        ```python
        @tool
        def add() -> Tool:
            async def execute(x: int, y: int) -> int:
                return x + y
            return execute
        ```
    """
    if prompt:
        from inspect_ai._util.logger import warn_once
        warn_once(
            logger,
            "The prompt parameter is deprecated (please relocate "
            + "to the tool function's description doc comment)",
        )
        prompt = re.sub(r"\s+", " ", prompt)
    def create_tool_wrapper(tool_type: Callable[P, Tool]) -> Callable[P, Tool]:
        # determine the name (explicit or implicit from object)
        tool_name = registry_name(
            tool_type, name if name else getattr(tool_type, "__name__")
        )
        # wrap instantiations of scorer so they carry registry info and metrics
        @wraps(tool_type)
        def tool_wrapper(*args: P.args, **kwargs: P.kwargs) -> Tool:
            tool = tool_type(*args, **kwargs)
            registry_tag(
                tool_type,
                tool,
                RegistryInfo(
                    type="tool",
                    name=tool_name,
                    metadata={
                        TOOL_PROMPT: prompt,
                        TOOL_PARALLEL: parallel,
                        TOOL_VIEWER: viewer,
                        TOOL_MODEL_INPUT: (
                            model_input or getattr(tool, TOOL_INIT_MODEL_INPUT, None)
                        ),
                    },
                ),
                *args,
                **kwargs,
            )
            return tool
        # register
        return tool_register(cast(Callable[P, Tool], tool_wrapper), tool_name)
    if func is not None:
        return create_tool_wrapper(func)
    else:
        return create_tool_wrapper
TOOL_PROMPT = "prompt"
TOOL_PARALLEL = "parallel"
TOOL_VIEWER = "viewer"
TOOL_MODEL_INPUT = "model_input"
TOOL_INIT_MODEL_INPUT = "__TOOL_INIT_MODEL_INPUT__"

================
File: src/inspect_ai/tool/_tools/_computer/__init__.py
================
from ._computer import computer
__all__ = ["computer"]

================
File: src/inspect_ai/tool/_tools/_computer/_common.py
================
import json
from textwrap import dedent
from typing import Literal
from pydantic import BaseModel, Field
from inspect_ai._util.content import ContentText
from inspect_ai._util.error import PrerequisiteError
from inspect_ai.model import ContentImage
from inspect_ai.tool import ToolError, ToolResult
from inspect_ai.util._sandbox.context import sandbox_with
from inspect_ai.util._sandbox.environment import SandboxEnvironment
class ToolExecResult(BaseModel):
    output: str | None = Field(default=None)
    error: str | None = Field(default=None)
    base64_image: str | None = Field(default=None)
async def cursor_position(timeout: int | None = None) -> ToolResult:
    return await _send_cmd(["cursor_position"], timeout=timeout)
async def screenshot(timeout: int | None = None) -> ToolResult:
    return await _send_cmd(["screenshot"], timeout=timeout)
async def wait(duration: int, timeout: int | None = None) -> ToolResult:
    return await _send_cmd(["wait", "--duration", f"{duration}"], timeout=timeout)
async def mouse_move(coordinate: list[int], timeout: int | None = None) -> ToolResult:
    return await _send_cmd(
        ["mouse_move", "--coordinate", f"{coordinate[0]}", f"{coordinate[1]}"],
        timeout=timeout,
    )
async def left_mouse_down(timeout: int | None = None) -> ToolResult:
    return await _send_cmd(["left_mouse_down"], timeout=timeout)
async def left_mouse_up(timeout: int | None = None) -> ToolResult:
    return await _send_cmd(["left_mouse_up"], timeout=timeout)
async def left_click(coordinate: list[int], timeout: int | None = None) -> ToolResult:
    return await _send_cmd(
        ["left_click", "--coordinate", f"{coordinate[0]}", f"{coordinate[1]}"],
        timeout=timeout,
    )
async def left_click_drag(
    start_coordinate: list[int], coordinate: list[int], timeout: int | None = None
) -> ToolResult:
    return await _send_cmd(
        [
            "left_click_drag",
            "--start_coordinate",
            f"{start_coordinate[0]}",
            f"{start_coordinate[1]}",
            "--coordinate",
            f"{coordinate[0]}",
            f"{coordinate[1]}",
        ],
        timeout=timeout,
    )
async def right_click(coordinate: list[int], timeout: int | None = None) -> ToolResult:
    return await _send_cmd(
        ["right_click", "--coordinate", f"{coordinate[0]}", f"{coordinate[1]}"],
        timeout=timeout,
    )
async def middle_click(coordinate: list[int], timeout: int | None = None) -> ToolResult:
    return await _send_cmd(
        ["middle_click", "--coordinate", f"{coordinate[0]}", f"{coordinate[1]}"],
        timeout=timeout,
    )
async def double_click(coordinate: list[int], timeout: int | None = None) -> ToolResult:
    return await _send_cmd(
        ["double_click", "--coordinate", f"{coordinate[0]}", f"{coordinate[1]}"],
        timeout=timeout,
    )
async def triple_click(coordinate: list[int], timeout: int | None = None) -> ToolResult:
    return await _send_cmd(
        ["triple_click", "--coordinate", f"{coordinate[0]}", f"{coordinate[1]}"],
        timeout=timeout,
    )
async def scroll(
    scroll_amount: int,
    scroll_direction: Literal["up", "down", "left", "right"],
    coordinate: list[int] | None,
    timeout: int | None = None,
) -> ToolResult:
    return await _send_cmd(
        [
            "scroll",
            "--scroll_amount",
            f"{scroll_amount}",
            "--scroll_direction",
            f"{scroll_direction}",
        ]
        + (
            ["--coordinate", f"{coordinate[0]}", f"{coordinate[1]}"]
            if coordinate
            else []
        ),
        timeout=timeout,
    )
async def press_key(key: str, timeout: int | None = None) -> ToolResult:
    return await _send_cmd(["key", "--text", key], timeout=timeout)
async def hold_key(key: str, duration: int, timeout: int | None = None) -> ToolResult:
    return await _send_cmd(
        ["hold_key", "--text", key, "--duration", f"{duration}"], timeout=timeout
    )
async def type(text: str, timeout: int | None = None) -> ToolResult:
    return await _send_cmd(["type", "--text", text], timeout=timeout)
async def _send_cmd(cmdTail: list[str], timeout: int | None = None) -> ToolResult:
    from inspect_ai.log._samples import sample_active
    sample = sample_active()
    assert sample
    sample_id = sample.sample.id
    assert sample_id
    cmd = ["python3", "/opt/inspect/tool/computer_tool.py"] + cmdTail
    raw_exec_result = await (await computer_sandbox()).exec(cmd, timeout=timeout)
    if not raw_exec_result.success:
        raise RuntimeError(
            f"Failure executing command: ${cmd} {raw_exec_result.stderr}"
        )
    result = ToolExecResult(**json.loads(raw_exec_result.stdout))
    if result.error:
        raise ToolError(result.error)
    image = (
        ContentImage(image=f"data:image/png;base64,{result.base64_image}")
        if result.base64_image
        else None
    )
    text = result.output if result.output and len(result.output) > 0 else None
    if text is not None and image is not None:
        return [ContentText(text=text), image]
    if text is not None:
        return text
    if image is not None:
        return [image]
    return "OK"
async def computer_sandbox() -> SandboxEnvironment:
    sb = await sandbox_with("/opt/inspect/tool/computer_tool.py")
    if sb:
        return sb
    else:
        raise PrerequisiteError(
            dedent("""
                The computer tool service was not found in any of the sandboxes for this sample. Please add the computer tool service to your configuration. For example, the following Docker compose file uses the aisiuk/inspect-computer-tool:latest image as its default sandbox:
                services:
                  default:
                    image: "aisiuk/inspect-computer-tool:latest"
                    init: true
                """).strip()
        )

================
File: src/inspect_ai/tool/_tools/_computer/_computer.py
================
from typing import Awaitable, Callable, Literal, TypeVar
from inspect_ai._util.content import Content, ContentImage, ContentText
from inspect_ai.tool import Tool, ToolResult, tool
from inspect_ai.tool._tool import TOOL_INIT_MODEL_INPUT, ToolParsingError
from inspect_ai.tool._tool_call import ToolCallModelInput
from . import _common as common
from ._resources.tool._constants import Action
ActionFunction = Callable[[str], ToolResult | Awaitable[ToolResult]]
@tool
def computer(max_screenshots: int | None = 1, timeout: int | None = 180) -> Tool:
    """Desktop computer tool.
    See documentation at <https://inspect.ai-safety-institute.org.uk/tools.html#sec-computer>.
    Args:
      max_screenshots: The maximum number of screenshots to play
        back to the model as input. Defaults to 1 (set to `None` to have no limit).
      timeout: Timeout in seconds for computer tool actions.
        Defaults to 180 (set to `None` for no timeout).
    """
    async def execute(
        action: Action,
        coordinate: list[int] | None = None,
        duration: int | None = None,
        scroll_amount: int | None = None,
        scroll_direction: Literal["up", "down", "left", "right"] | None = None,
        start_coordinate: list[int] | None = None,
        text: str | None = None,
    ) -> ToolResult:
        """
        Use this tool to interact with a computer.
        Use a mouse and keyboard to interact with a computer's desktop GUI.
        Keep in mind that icons require double clicks to open while other UI affordances like menu items and buttons require a single click.
        Args:
          action (Action): The action to perform.
              - `key`: Press a key or key-combination on the keyboard.
                  - Example: execute(action="key", text="ctrl+s")
                  - Text can be any key name supported by xdotool's `key` such as:
                      "Return", "Escape", "alt+Tab", "BackSpace", "Tab", "alt+Tab", "ctrl+s", "Up", "KP_0" (for the numpad 0 key),
                      "Insert", "Delete", "Home", "End", "Prior", "Next", "Left", "Up", "Right", "Down",
                      "F1", "F2", "F3", "F4", "F5", "F6", "F7", "F8", "F9", "F10", "F11", "F12",
                      "Shift_L", "Shift_R", "Control_L", "Control_R", "Alt_L", "Alt_R", "Scroll_Lock", "Num_Lock", "Caps_Lock", "Pause",
                      "KP_Multiply", "KP_Home", "KP_Up", "KP_Prior", "KP_Subtract", "KP_Left", "KP_Begin", "KP_Right", "KP_Add", "KP_End","KP_Down",
                      "KP_Next", "KP_Insert", "KP_Delete", "KP_Enter", "KP_Divide", "KP_Equal", "KP_Decimal",
              - 'hold_key': Hold down a key or multiple keys for a specified duration (in seconds). Supports the same syntax as `key`.
              - `type`: Type a string of text on the keyboard. If the text contains spaces, enclose it in quotes.
                  - Example: execute(action="type", text="The crux of the biscuit is the apostrophe!")
              - `cursor_position`: Get the current (x, y) pixel coordinate of the cursor on the screen.
              - `mouse_move`: Move the cursor to a specified (x, y) pixel coordinate on the screen.
                  - Example: execute(action="mouse_move", coordinate=(100, 200))
              - `left_mouse_down`: Press the left mouse button.
              - `left_mouse_up`: Release the left mouse button.
              - `left_click`: Click the left mouse button.
              - `left_click_drag`: Click and drag the cursor to a specified (x, y) pixel coordinate on the screen.
                  - Example: execute(action="left_click_drag", coordinate=(150, 250))
              - `right_click`: Click the right mouse button.
              - `middle_click`: Click the middle mouse button.
              - `double_click`: Double-click the left mouse button.
              - `triple_click`: Double-click the left mouse button.
              - `wait`: Wait for a specified duration (in seconds).
              - `screenshot`: Take a screenshot.
          coordinate (tuple[int, int] | None): The (x, y) pixel coordinate on the screen to which to move or drag. Required only by `action=mouse_move` and `action=left_click_drag`.
          duration (int | None): The duration to wait or hold the key down for. Required only by `action=hold_key` and `action=wait`.
          scroll_amount (int | None): The number of 'clicks' to scroll. Required only by `action=scroll`.
          scroll_direction (Literal["up", "down", "left", "right] | None): The direction to scroll the screen. Required only by `action=scroll`.
          start_coordinate (tuple[int, int] | None): The (x, y) pixel coordinate on the screen from which to initiate a drag. Required only by `action=scroll`.
          text (str | None): The text to type or the key to press. Required when action is "key" or "type".
        Returns:
          The output of the command. Many commands will include a screenshot reflecting the result of the command in their output.
        """
        match action:
            case "key":
                return await common.press_key(not_none(text, "text"), timeout=timeout)
            case "hold_key":
                return await common.hold_key(
                    not_none(text, "text"),
                    not_none(duration, "duration"),
                    timeout=timeout,
                )
            case "type":
                return await common.type(not_none(text, "text"), timeout=timeout)
            case "cursor_position":
                return await common.cursor_position(timeout=timeout)
            case "mouse_move":
                return await common.mouse_move(
                    not_none(coordinate, "coordinate"), timeout=timeout
                )
            case "left_mouse_down":
                return await common.left_mouse_down(timeout=timeout)
            case "left_mouse_up":
                return await common.left_mouse_up(timeout=timeout)
            case "left_click":
                return await common.left_click(
                    not_none(coordinate, "coordinate"), timeout=timeout
                )
            case "left_click_drag":
                return await common.left_click_drag(
                    not_none(start_coordinate, "start_coordinate"),
                    not_none(coordinate, "coordinate"),
                    timeout=timeout,
                )
            case "right_click":
                return await common.right_click(
                    not_none(coordinate, "coordinate"), timeout=timeout
                )
            case "middle_click":
                return await common.middle_click(
                    not_none(coordinate, "coordinate"), timeout=timeout
                )
            case "double_click":
                return await common.double_click(
                    not_none(coordinate, "coordinate"), timeout=timeout
                )
            case "triple_click":
                return await common.triple_click(
                    not_none(coordinate, "coordinate"), timeout=timeout
                )
            case "scroll":
                return await common.scroll(
                    not_none(scroll_amount, "scroll_amount"),
                    not_none(scroll_direction, "scroll_direction"),
                    coordinate,
                    timeout=timeout,
                )
            case "wait":
                return await common.wait(
                    not_none(duration, "duration"), timeout=timeout
                )
            case "screenshot":
                return await common.screenshot(timeout=timeout)
        raise ToolParsingError(f"Invalid action: {action}")
    # if max_screenshots is specified then polk model input into where @tool can find it
    if max_screenshots is not None:
        setattr(execute, TOOL_INIT_MODEL_INPUT, _computer_model_input(max_screenshots))
    return execute
def _computer_model_input(max_screenshots: int) -> ToolCallModelInput:
    def model_input(
        message_index: int, message_total: int, content: str | list[Content]
    ) -> str | list[Content]:
        # nothing to do for scalars
        if isinstance(content, str):
            return content
        # if we are inside max_screenshots then return as is
        elif (message_total - message_index) <= max_screenshots:
            return content
        # otherwise convert images to text placeholdrs
        else:
            input_content: list[Content] = []
            for c in content:
                if isinstance(c, ContentImage):
                    input_content.append(
                        ContentText(
                            text="Screenshot removed to reduce size of input. Please consult the latest screenshots for the most up to date state of the screen."
                        )
                    )
                else:
                    input_content.append(c)
            return input_content
    return model_input
T = TypeVar("T")
def not_none(value: T | None, name: str) -> T:
    if value is None:
        raise ToolParsingError(f"{name} must be provided")
    return value

================
File: src/inspect_ai/tool/_tools/_computer/_resources/README.md
================
# About This Image

This image was inspired by Anthropic's Computer Use Demo [here](https://github.com/anthropics/anthropic-quickstarts/tree/main/computer-use-demo/image).

Its goal is to provide the minimum infrastructure to support the use of Inspect's `computer_tool` to interact with the computer via X11 and `xdotool`, while also providing observability and interaction via VNC and noVNC.

The image extends this minimal functionality by adding a few basic applications  VS Code, Firefox, XPaint, and galculator.

## Entrypoint Directory

1. **Xvfb (X Virtual Framebuffer)**
   - **Script:** `xvfb_startup.sh`
   - **Description:** Xvfb is a display server that implements the X11 display server protocol. It runs in memory and does not require a physical display, useful for running graphical applications in a headless environment.

1. **xfce4**
   - **Script:** `xfce4_startup.sh`
   - **Description:** xfce4 is a lightweight desktop environment for UNIX-like operating systems. It aims to be fast, low on system resources, and user-friendly.

1. **x11vnc**
   - **Script:** `x11vnc_startup.sh`
   - **Description:** x11vnc is a VNC server that allows remote access to the X11 display. It enables users to connect to the virtual display environment from a remote machine using a VNC client.

1. **noVNC**
   - **Script:** `novnc_startup.sh`
   - **Description:** noVNC is a VNC client that runs in a web browser. It allows users to access the virtual display environment through a web interface without needing a separate VNC client application.

## Desktop Directory

The `Desktop` directory contains launchers for VS Code, Firefox and XPaint.

================
File: src/inspect_ai/tool/_tools/_computer/_resources/tool/_args.py
================
from argparse import Action, ArgumentParser, Namespace
from typing import Sequence
def parse_arguments(args: Sequence[str] | None = None) -> Namespace:
    return _create_parser().parse_args(args)
def _create_parser() -> ArgumentParser:
    parser = ArgumentParser(prog="computer_tool")
    subparsers = parser.add_subparsers(dest="action", required=True)
    # these take no additional arguments
    subparsers.add_parser(
        "screenshot",
        aliases=["cursor_position", "left_mouse_down", "left_mouse_up"],
    )
    key_and_type = subparsers.add_parser("type", aliases=["key"])
    _add_text(key_and_type)
    hold_key = subparsers.add_parser("hold_key")
    _add_text(hold_key)
    _add_duration(hold_key)
    mouse_move = subparsers.add_parser("mouse_move")
    _add_coordinate(mouse_move)
    click = subparsers.add_parser(
        "left_click",
        aliases=["right_click", "middle_click", "double_click", "triple_click"],
    )
    _add_coordinate(click, False)
    _add_text(click, False)
    left_click_drag = subparsers.add_parser("left_click_drag")
    _add_start_coordinate(left_click_drag)
    _add_coordinate(left_click_drag)
    _add_text(left_click_drag, False)
    scroll = subparsers.add_parser("scroll")
    _add_scroll_direction(scroll)
    _add_scroll_amount(scroll)
    # despite what the doc says, the model doesn't always provide a coordinate
    _add_coordinate(scroll, False)
    wait = subparsers.add_parser("wait")
    _add_duration(wait)
    return parser
def _add_scroll_direction(subparser: ArgumentParser) -> Action:
    return subparser.add_argument(
        "--scroll_direction", choices=["up", "down", "left", "right"], required=True
    )
def _add_scroll_amount(subparser: ArgumentParser) -> Action:
    return subparser.add_argument("--scroll_amount", type=int, required=True)
def _add_coordinate(subparser: ArgumentParser, required: bool = True) -> Action:
    return subparser.add_argument("--coordinate", type=int, nargs=2, required=required)
def _add_start_coordinate(subparser: ArgumentParser) -> Action:
    return subparser.add_argument(
        "--start_coordinate", type=int, nargs=2, required=True
    )
def _add_duration(subparser: ArgumentParser) -> Action:
    return subparser.add_argument("--duration", type=int, required=True)
def _add_text(subparser: ArgumentParser, required: bool = True) -> Action:
    return subparser.add_argument("--text", type=str, required=required)

================
File: src/inspect_ai/tool/_tools/_computer/_resources/tool/_constants.py
================
from typing import Literal
Action = Literal[
    "key",
    "hold_key",
    "type",
    "cursor_position",
    "mouse_move",
    "left_mouse_down",
    "left_mouse_up",
    "left_click",
    "left_click_drag",
    "right_click",
    "middle_click",
    "double_click",
    "triple_click",
    "scroll",
    "wait",
    "screenshot",
]

================
File: src/inspect_ai/tool/_tools/_computer/_resources/tool/_logger.py
================
import logging
def setup_logger(level=logging.INFO):
    """
    This logger emits all of its output to PID 1's stdout.
    This makes it so that logging from invocations of the computer_tool cli show up in `docker logs` output.
    """
    new_logger = logging.getLogger("computer_tool")
    new_logger.setLevel(level)
    stdout_handler = logging.FileHandler("/proc/1/fd/1", mode="w")
    stdout_handler.setLevel(level)
    stdout_handler.setFormatter(
        logging.Formatter("%(name)s(pid=%(process)d) - %(levelname)s - %(message)s")
    )
    if not new_logger.handlers:
        new_logger.addHandler(stdout_handler)
    return new_logger

================
File: src/inspect_ai/tool/_tools/_computer/_resources/tool/_run.py
================
"""Utility to run shell commands asynchronously with a timeout."""
import asyncio
TRUNCATED_MESSAGE: str = "<response clipped><NOTE>To save on context only part of this file has been shown to you. You should retry this tool after you have searched inside the file with `grep -n` in order to find the line numbers of what you are looking for.</NOTE>"
MAX_RESPONSE_LEN: int = 16000
def maybe_truncate(content: str, truncate_after: int | None = MAX_RESPONSE_LEN):
    """Truncate content and append a notice if content exceeds the specified length."""
    return (
        content
        if not truncate_after or len(content) <= truncate_after
        else content[:truncate_after] + TRUNCATED_MESSAGE
    )
async def run(
    cmd: str,
    timeout: float | None = 120.0,  # seconds
    truncate_after: int | None = MAX_RESPONSE_LEN,
):
    """Run a shell command asynchronously with a timeout."""
    process = await asyncio.create_subprocess_shell(
        cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE
    )
    try:
        stdout, stderr = await asyncio.wait_for(process.communicate(), timeout=timeout)
        return (
            process.returncode or 0,
            maybe_truncate(stdout.decode(), truncate_after=truncate_after),
            maybe_truncate(stderr.decode(), truncate_after=truncate_after),
        )
    except (TimeoutError, asyncio.TimeoutError) as exc:
        try:
            process.kill()
        except ProcessLookupError:
            pass
        raise TimeoutError(
            f"Command '{cmd}' timed out after {timeout} seconds"
        ) from exc

================
File: src/inspect_ai/tool/_tools/_computer/_resources/tool/_tool_result.py
================
from dataclasses import dataclass, fields, replace
@dataclass(kw_only=True, frozen=True)
class ToolResult:
    """Represents the result of a tool execution."""
    output: str | None = None
    error: str | None = None
    base64_image: str | None = None
    def __bool__(self):
        return any(getattr(self, field.name) for field in fields(self))
    def __add__(self, other: "ToolResult"):
        def combine_fields(
            field: str | None, other_field: str | None, concatenate: bool = True
        ):
            if field and other_field:
                if concatenate:
                    return field + other_field
                raise ValueError("Cannot combine tool results")
            return field or other_field
        return ToolResult(
            output=combine_fields(self.output, other.output),
            error=combine_fields(self.error, other.error),
            base64_image=combine_fields(self.base64_image, other.base64_image, False),
        )
    def replace(self, **kwargs):
        """Returns a new ToolResult with the given fields replaced."""
        return replace(self, **kwargs)

================
File: src/inspect_ai/tool/_tools/_computer/_resources/tool/_x11_client.py
================
"""Inspired by https://github.com/anthropics/anthropic-quickstarts/blob/main/computer-use-demo/computer_use_demo/tools/computer.py"""
import asyncio
import base64
import logging
import os
import shlex
from pathlib import Path
from typing import Literal, TypedDict
from uuid import uuid4
from _run import run
from _tool_result import ToolResult
OUTPUT_DIR = "/tmp/outputs"
TYPING_DELAY_MS = 12
TYPING_GROUP_SIZE = 50
ColorCount = Literal[4096, 2048, 1024, 512, 256, 128, 64, 32, 16, 8, 4]
class X11ClientError(Exception):
    def __init__(self, message):
        self.message = message
class Resolution(TypedDict):
    width: int
    height: int
# sizes above XGA/WXGA are not recommended (see README.md)
# scale down to one of these targets if ComputerTool._scaling_enabled is set
MAX_SCALING_TARGETS: dict[str, Resolution] = {
    "XGA": Resolution(width=1024, height=768),  # 4:3
    "WXGA": Resolution(width=1280, height=800),  # 16:10
    "FWXGA": Resolution(width=1366, height=768),  # ~16:9
}
ScalingSource = Literal["computer", "api"]
class ComputerToolOptions(TypedDict):
    display_height_px: int
    display_width_px: int
    display_number: int | None
def chunks(s: str, chunk_size: int) -> list[str]:
    return [s[i : i + chunk_size] for i in range(0, len(s), chunk_size)]
class X11Client:
    """
    A tool that allows the agent to interact with the screen, keyboard, and mouse of the current computer.
    The tool parameters are defined by Anthropic and are not editable.
    """
    width: int
    height: int
    display_num: int | None
    # TODO: Complete plumbing this or remove it
    color_count: ColorCount | None = 256
    _screenshot_delay = 2.0
    _scaling_enabled = True
    @property
    def options(self) -> ComputerToolOptions:
        width, height = self._scale_coordinates("computer", self.width, self.height)
        return {
            "display_width_px": width,
            "display_height_px": height,
            "display_number": self.display_num,
        }
    def __init__(self):
        super().__init__()
        self.width = int(os.getenv("WIDTH") or 0)
        self.height = int(os.getenv("HEIGHT") or 0)
        assert self.width and self.height, "WIDTH, HEIGHT must be set"
        if (display_num := os.getenv("DISPLAY_NUM")) is not None:
            self.display_num = int(display_num)
            self._display_prefix = f"DISPLAY=:{self.display_num} "
        else:
            self.display_num = None
            self._display_prefix = ""
        self.xdotool = f"{self._display_prefix}xdotool"
    async def key(self, text: str) -> ToolResult:
        return await self._shell(f"{self.xdotool} key -- {_key_arg_for_text(text)}")
    async def hold_key(self, text: str, duration: int) -> ToolResult:
        key_arg = _key_arg_for_text(text)
        await self._shell(f"{self.xdotool} keydown -- {key_arg}", False)
        await asyncio.sleep(duration)
        return await self._shell(f"{self.xdotool} keyup -- {key_arg}")
    async def type(self, text: str) -> ToolResult:
        results: list[ToolResult] = []
        for chunk in chunks(text, TYPING_GROUP_SIZE):
            cmd = (
                f"{self.xdotool} type --delay {TYPING_DELAY_MS} -- {shlex.quote(chunk)}"
            )
            results.append(await self._shell(cmd, take_screenshot=False))
        screenshot_base64 = await self._take_screenshot_after_delay()
        return ToolResult(
            output="".join(result.output or "" for result in results),
            error="".join(result.error or "" for result in results),
            base64_image=screenshot_base64,
        )
    async def cursor_position(self) -> ToolResult:
        result = await self._shell(
            f"{self.xdotool} getmouselocation --shell",
            take_screenshot=False,
        )
        output = result.output or ""
        x, y = self._scale_coordinates(
            "computer",
            int(output.split("X=")[1].split("\n")[0]),
            int(output.split("Y=")[1].split("\n")[0]),
        )
        return result.replace(output=f"X={x},Y={y}")
    async def left_mouse_down(self) -> ToolResult:
        return await self._shell(f"{self.xdotool} mousedown 1")
    async def left_mouse_up(self) -> ToolResult:
        return await self._shell(f"{self.xdotool} mouseup 1")
    async def mouse_move(self, coordinate: tuple[int, int]) -> ToolResult:
        return await self._mouse_move_and("mouse_move", coordinate, None)
    async def left_click(
        self, coordinate: tuple[int, int] | None, text: str | None
    ) -> ToolResult:
        return await self._mouse_move_and("left_click", coordinate, text)
    async def right_click(
        self, coordinate: tuple[int, int] | None, text: str | None
    ) -> ToolResult:
        return await self._mouse_move_and("right_click", coordinate, text)
    async def middle_click(
        self, coordinate: tuple[int, int] | None, text: str | None
    ) -> ToolResult:
        return await self._mouse_move_and("middle_click", coordinate, text)
    async def double_click(
        self, coordinate: tuple[int, int] | None, text: str | None
    ) -> ToolResult:
        return await self._mouse_move_and("double_click", coordinate, text)
    async def triple_click(
        self, coordinate: tuple[int, int] | None, text: str | None
    ) -> ToolResult:
        return await self._mouse_move_and("triple_click", coordinate, text)
    async def left_click_drag(
        self, start_coordinate: tuple[int, int], coordinate: tuple[int, int]
    ) -> ToolResult:
        await self._move_mouse_to_coordinate(start_coordinate, False)
        x, y = self._scale_coordinates("api", *coordinate)
        return await self._shell(
            f"{self.xdotool} mousedown 1 mousemove --sync {x} {y} mouseup 1"
        )
    async def scroll(
        self,
        scroll_direction: Literal["up", "down", "left", "right"],
        scroll_amount: int,
        coordinate: tuple[int, int] | None,
        text: str | None,
    ) -> ToolResult:
        if coordinate:
            await self._move_mouse_to_coordinate(coordinate, False)
        scroll_button = {
            "up": 4,
            "down": 5,
            "left": 6,
            "right": 7,
        }[scroll_direction]
        if text:
            key_arg = _key_arg_for_text(text)
            await self._shell(f"{self.xdotool} keydown -- {key_arg}", False)
            await self._shell(
                f"{self.xdotool} click --repeat {scroll_amount} {scroll_button}",
                False,
            )
            return await self._shell(f"{self.xdotool} keyup -- {key_arg}")
        else:
            return await self._shell(
                f"{self.xdotool} click --repeat {scroll_amount} {scroll_button}"
            )
    async def wait(self, duration: int) -> ToolResult:
        await asyncio.sleep(duration)
        return await self.screenshot()
    async def screenshot(self) -> ToolResult:
        return await self._screenshot()
    async def _mouse_move_and(
        self,
        action: Literal[
            "mouse_move",
            "left_click",
            "right_click",
            "middle_click",
            "double_click",
            "triple_click",
        ],
        coordinate: tuple[int, int] | None,
        text: str | None,
    ):
        should_move = action == "mouse_move" or coordinate
        if should_move:
            assert coordinate  # coding/type safety error
            move_result = await self._move_mouse_to_coordinate(
                coordinate, action == "mouse_move"
            )
            if action == "mouse_move":
                return move_result
        click_arg = {
            "left_click": "1",
            "right_click": "3",
            "middle_click": "2",
            "double_click": "--repeat 2 --delay 300 1",
            "triple_click": "--repeat 3 --delay 300 1",
        }[action]
        if text:
            key_arg = _key_arg_for_text(text)
            await self._shell(f"{self.xdotool} keydown -- {key_arg}", False)
            await self._shell(f"{self.xdotool} click {click_arg}", False)
            return await self._shell(f"{self.xdotool} keyup -- {key_arg}")
        else:
            return await self._shell(f"{self.xdotool} click {click_arg}")
    async def _move_mouse_to_coordinate(
        self, coordinate: tuple[int, int], take_screenshot: bool
    ):
        x, y = self._scale_coordinates("api", *coordinate)
        return await self._shell(
            f"{self.xdotool} mousemove --sync {x} {y}", take_screenshot=take_screenshot
        )
    async def _screenshot(self):
        """Take a screenshot of the current screen and return the base64 encoded image."""
        output_dir = Path(OUTPUT_DIR)
        output_dir.mkdir(parents=True, exist_ok=True)
        path = output_dir / f"screenshot_{uuid4().hex}.png"
        result = await self._shell(
            f"{self._display_prefix}scrot --silent -p {path}", take_screenshot=False
        )
        if self._scaling_enabled:
            x, y = self._scale_coordinates("computer", self.width, self.height)
            convert_cmd = f"convert {path} -resize {x}x{y}!"
            if self.color_count is not None:
                convert_cmd += f" -colors {self.color_count}"
            convert_cmd += f" {path}"
            await self._shell(convert_cmd, take_screenshot=False)
        if path.exists():
            return result.replace(
                base64_image=base64.b64encode(path.read_bytes()).decode()
            )
        raise X11ClientError(f"Failed to take screenshot: {result.error}")
    async def _shell(self, command: str, take_screenshot=True) -> ToolResult:
        """Run a shell command and return the output, error, and optionally a screenshot."""
        logging.debug(f"running shell command {command}")
        _, stdout, stderr = await run(command)
        logging.debug(f"shell command returned stdout: {stdout}, stderr: {stderr}")
        return ToolResult(
            output=stdout,
            error=stderr,
            base64_image=(await self._take_screenshot_after_delay())
            if take_screenshot
            else None,
        )
    async def _take_screenshot_after_delay(self) -> str:
        # delay to let things settle before taking a screenshot
        await asyncio.sleep(self._screenshot_delay)
        return (await self._screenshot()).base64_image
    def _scale_coordinates(self, source: ScalingSource, x: int, y: int):
        """Scale coordinates to a target maximum resolution."""
        if not self._scaling_enabled:
            return x, y
        ratio = self.width / self.height
        target_dimension = None
        for dimension in MAX_SCALING_TARGETS.values():
            # allow some error in the aspect ratio - not ratios are exactly 16:9
            if abs(dimension["width"] / dimension["height"] - ratio) < 0.02:
                if dimension["width"] < self.width:
                    target_dimension = dimension
                break
        if target_dimension is None:
            return x, y
        # should be less than 1
        x_scaling_factor = target_dimension["width"] / self.width
        y_scaling_factor = target_dimension["height"] / self.height
        if source == "api":
            if x > self.width or y > self.height:
                raise X11ClientError(f"Coordinates {x}, {y} are out of bounds")
            # scale up
            return round(x / x_scaling_factor), round(y / y_scaling_factor)
        # scale down
        return round(x * x_scaling_factor), round(y * y_scaling_factor)
def _key_arg_for_text(text: str) -> str:
    return " ".join(shlex.quote(part) for part in text.split())

================
File: src/inspect_ai/tool/_tools/_computer/_resources/tool/computer_tool.py
================
import asyncio
import json
import logging
import os
import sys
import time
from argparse import Namespace
from typing import TypeVar
from _args import parse_arguments
from _constants import Action
from _logger import setup_logger
from _tool_result import ToolResult
from _x11_client import X11Client
class ComputerToolError(Exception):
    def __init__(self, message):
        self.message = message
# This is a bit sketchy. We really want to use relative imports here. Using absolute imports
# works at runtime, but it prevents intellisense from working. However, when this folder is
# copied to the container, by default relative imports won't work if this file is launched
# normally. To overcome this, two things need to happen:
# 1. PYTHONPATH must be set to the parent of the container folder. `PYTHONPATH=/opt`
# 2. The program must be launched with the -m flag. `python3 -m computer_tool.computer_tool`
#
# TODO: There's got to be a cleaner way.
my_logger = setup_logger(logging.INFO)
def main():
    try:
        args = parse_arguments()
        my_logger.info(f"({args})")
        result = asyncio.run(execute_action(args))
        print(
            json.dumps(
                {
                    "output": result.output,
                    "error": result.error,
                    "base64_image": result.base64_image,
                }
            )
        )
        my_logger.debug("SUCCESS")
    except Exception as e:
        my_logger.warning(f"An error occurred: {e}")
        print(f"An error occurred: {e}", file=sys.stderr)
        sys.exit(1)
async def execute_action(args: Namespace) -> ToolResult:
    # we can't do anything until X11 is ready to go.
    await wait_for_file("/tmp/xfce_started")
    computer = X11Client()
    action: Action = args.action
    match action:
        case "key":
            return await computer.key(not_none(args.text, "text"))
        case "hold_key":
            return await computer.hold_key(
                not_none(args.text, "text"), not_none(args.duration, "duration")
            )
        case "type":
            return await computer.type(not_none(args.text, "text"))
        case "cursor_position":
            return await computer.cursor_position()
        case "left_mouse_down":
            return await computer.left_mouse_down()
        case "left_mouse_up":
            return await computer.left_mouse_up()
        case "mouse_move":
            return await computer.mouse_move(not_none(args.coordinate, "coordinate"))
        case "left_click":
            return await computer.left_click(
                getattr(args, "coordinate", None), getattr(args, "text", None)
            )
        case "right_click":
            return await computer.right_click(
                getattr(args, "coordinate", None), getattr(args, "text", None)
            )
        case "middle_click":
            return await computer.middle_click(
                getattr(args, "coordinate", None), getattr(args, "text", None)
            )
        case "double_click":
            return await computer.double_click(
                getattr(args, "coordinate", None), getattr(args, "text", None)
            )
        case "triple_click":
            return await computer.triple_click(
                getattr(args, "coordinate", None), getattr(args, "text", None)
            )
        case "left_click_drag":
            return await computer.left_click_drag(
                not_none(args.start_coordinate, "start_coordinate"),
                not_none(args.coordinate, "coordinate"),
            )
        case "scroll":
            return await computer.scroll(
                not_none(args.scroll_direction, "scroll_direction"),
                not_none(args.scroll_amount, "scroll_amount"),
                getattr(args, "coordinate", None),
                getattr(args, "text", None),
            )
        case "wait":
            return await computer.wait(not_none(args.duration, "duration"))
        case "screenshot":
            return await computer.screenshot()
    raise ComputerToolError(f"Invalid action: {action}")
async def wait_for_file(file_path, check_interval=1):
    if os.path.exists(file_path):
        return
    my_logger.info(f"Waiting for {file_path}")
    start_time = time.time()
    while not os.path.exists(file_path):
        await asyncio.sleep(check_interval)
    my_logger.info(
        f"Done waiting for {file_path} after {time.time() - start_time:.1f} seconds"
    )
T = TypeVar("T")
def not_none(value: T | None, name: str) -> T:
    if value is None:
        raise ComputerToolError(f"{name} must be provided")
    return value
if __name__ == "__main__":
    main()

================
File: src/inspect_ai/tool/_tools/_computer/test_args.py
================
import pytest
from ._resources.tool._args import parse_arguments
def test_parse_args_screenshot() -> None:
    args = parse_arguments(["screenshot"])
    assert args.action == "screenshot"
def test_parse_args_cursor_position() -> None:
    args = parse_arguments(["cursor_position"])
    assert args.action == "cursor_position"
def test_parse_args_type() -> None:
    args = parse_arguments(["type", "--text", "hello"])
    assert args.action == "type"
    assert args.text == "hello"
def test_parse_args_mouse_move() -> None:
    args = parse_arguments(["mouse_move", "--coordinate", "100", "200"])
    assert args.action == "mouse_move"
    assert args.coordinate == [100, 200]
def test_parse_args_left_click() -> None:
    args = parse_arguments(["left_click", "--coordinate", "100", "200"])
    assert args.action == "left_click"
    assert args.coordinate == [100, 200]
def test_parse_args_right_click() -> None:
    args = parse_arguments(["right_click", "--coordinate", "100", "200"])
    assert args.action == "right_click"
    assert args.coordinate == [100, 200]
def test_parse_args_middle_click() -> None:
    args = parse_arguments(["middle_click", "--coordinate", "100", "200"])
    assert args.action == "middle_click"
    assert args.coordinate == [100, 200]
def test_parse_args_double_click() -> None:
    args = parse_arguments(["double_click", "--coordinate", "100", "200"])
    assert args.action == "double_click"
    assert args.coordinate == [100, 200]
def test_parse_args_triple_click() -> None:
    args = parse_arguments(["triple_click", "--coordinate", "100", "200"])
    assert args.action == "triple_click"
    assert args.coordinate == [100, 200]
def test_parse_args_hold_key() -> None:
    args = parse_arguments(["hold_key", "--text", "a", "--duration", "5"])
    assert args.action == "hold_key"
    assert args.text == "a"
    assert args.duration == 5
def test_parse_args_left_click_drag() -> None:
    args = parse_arguments(
        [
            "left_click_drag",
            "--start_coordinate",
            "100",
            "200",
            "--coordinate",
            "300",
            "400",
            "--text",
            "drag",
        ]
    )
    assert args.action == "left_click_drag"
    assert args.start_coordinate == [100, 200]
    assert args.coordinate == [300, 400]
    assert args.text == "drag"
def test_parse_args_scroll() -> None:
    args = parse_arguments(
        [
            "scroll",
            "--scroll_direction",
            "up",
            "--scroll_amount",
            "10",
            "--coordinate",
            "100",
            "200",
        ]
    )
    assert args.action == "scroll"
    assert args.scroll_direction == "up"
    assert args.scroll_amount == 10
    assert args.coordinate == [100, 200]
def test_parse_args_wait() -> None:
    args = parse_arguments(["wait", "--duration", "5"])
    assert args.action == "wait"
    assert args.duration == 5
def test_parse_args_type_missing_text() -> None:
    with pytest.raises(SystemExit):
        parse_arguments(["type"])
def test_parse_args_invalid_action() -> None:
    with pytest.raises(SystemExit):
        parse_arguments(["invalid_action"])
def test_parse_args_mouse_move_missing_coordinate() -> None:
    with pytest.raises(SystemExit):
        parse_arguments(["mouse_move"])
def test_parse_args_click_invalid_coordinate() -> None:
    with pytest.raises(SystemExit):
        parse_arguments(["left_click", "--coordinate", "100"])
def test_parse_args_hold_key_missing_duration() -> None:
    with pytest.raises(SystemExit):
        parse_arguments(["hold_key", "--text", "a"])
def test_parse_args_left_click_drag_missing_start_coordinate() -> None:
    with pytest.raises(SystemExit):
        parse_arguments(
            ["left_click_drag", "--coordinate", "300", "400", "--text", "drag"]
        )
def test_parse_args_scroll_missing_scroll_direction() -> None:
    with pytest.raises(SystemExit):
        parse_arguments(
            ["scroll", "--scroll_amount", "10", "--coordinate", "100", "200"]
        )
def test_parse_args_wait_missing_duration() -> None:
    with pytest.raises(SystemExit):
        parse_arguments(["wait"])

================
File: src/inspect_ai/tool/_tools/_execute.py
================
from inspect_ai.util import sandbox as sandbox_env
from .._tool import Tool, tool
from .._tool_call import ToolCall, ToolCallContent, ToolCallView, ToolCallViewer
# custom viewer for bash and python code blocks
def code_viewer(language: str, code_param: str) -> ToolCallViewer:
    def viewer(tool_call: ToolCall) -> ToolCallView:
        code = tool_call.arguments.get(code_param, None)
        code = (code or tool_call.function).strip()
        call = ToolCallContent(
            title=language,
            format="markdown",
            content=f"```{language}\n" + code + "\n```\n",
        )
        return ToolCallView(call=call)
    return viewer
@tool(viewer=code_viewer("bash", "cmd"))
def bash(
    timeout: int | None = None, user: str | None = None, sandbox: str | None = None
) -> Tool:
    """Bash shell command execution tool.
    Execute bash shell commands using a sandbox environment (e.g. "docker").
    Args:
      timeout: Timeout (in seconds) for command.
      user: User to execute commands as.
      sandbox: Optional sandbox environmnent name.
    Returns:
      String with command output (stdout) or command error (stderr).
    """
    async def execute(cmd: str) -> str:
        """
        Use this function to execute bash commands.
        Args:
          cmd (str): The bash command to execute.
        Returns:
          The output of the command.
        """
        # execute the command
        result = await sandbox_env(sandbox).exec(
            cmd=["bash", "--login", "-c", cmd], timeout=timeout, user=user
        )
        # return output (including stderr if any)
        output = ""
        if result.stderr:
            output = f"{result.stderr}\n"
        return f"{output}{result.stdout}"
    return execute
@tool(viewer=code_viewer("python", "code"))
def python(
    timeout: int | None = None, user: str | None = None, sandbox: str | None = None
) -> Tool:
    """Python code execution tool.
    Execute Python code using a sandbox environment (e.g. "docker").
    Args:
      timeout: Timeout (in seconds) for command.
      user: User to execute commands as.
      sandbox: Optional sandbox environmnent name.
    Returns:
      String with command output (stdout) or command error (stderr).
    """
    async def execute(code: str) -> str:
        """
        Use the python function to execute Python code.
        The Python tool executes single-run Python scripts. Important notes:
        1. Each execution is independent - no state is preserved between runs
        2. You must explicitly use print() statements to see any output
        3. Simply writing expressions (like in notebooks) will not display results
        4. The script cannot accept interactive input during execution
        5. Return statements alone won't produce visible output
        6. All variables and imports are cleared between executions
        7. Standard output (via print()) is the only way to see results
        Args:
          code (str): The python code to execute.
        Returns:
          The output of the Python code.
        """
        result = await sandbox_env(sandbox).exec(
            cmd=["python3"], input=code, timeout=timeout, user=user
        )
        # return output (including stderr if any)
        output = ""
        if result.stderr:
            output = f"{result.stderr}\n"
        return f"{output}{result.stdout}"
    return execute

================
File: src/inspect_ai/tool/_tools/_web_browser/__init__.py
================
from ._web_browser import web_browser
__all__ = ["web_browser"]

================
File: src/inspect_ai/tool/_tools/_web_browser/_resources/accessibility_tree_node.py
================
from functools import reduce
from typing import Optional
from cdp.a11y import (
    AXNode,
    AXNodeId,
    AXProperty,
    AXPropertyName,
    node_has_property,
    string_from_ax_value,
)
from cdp.dom_snapshot import (
    DOMSnapshotContext,
    bounds_for_node_index,
    current_url_src_for_node_index,
    index_for_node_id,
    text_value_for_node_index,
)
from rectangle import Rectangle
# Properties to ignore when printing out the accessibility tree.
_IGNORED_AT_PROPERTIES: set[AXPropertyName] = {
    "focusable",
    "readonly",
    "level",
    "settable",
    "multiline",
    "invalid",
}
# The AXRole enum is here (https://chromium.googlesource.com/chromium/chromium/+/HEAD/ui/accessibility/ax_enums.idl#62)
_ROLES_IGNORED = {"horizontal_rule", "ignored"}
# These are noisy and provide no real benefit unless there's a name associated with them
_ROLES_REQUIRING_A_NAME = {"generic", "paragraph", "div"}
# Used for debugging, include the elements bounding rect during output.
_INCLUDE_BOUNDS_IN_OUTPUT = False
class AccessibilityTreeNode:
    def __init__(
        self,
        *,
        ax_node: AXNode,
        ax_nodes: dict[AXNodeId, AXNode],
        parent: Optional["AccessibilityTreeNode"],
        all_accessibility_tree_nodes: dict[AXNodeId, "AccessibilityTreeNode"],
        snapshot_context: DOMSnapshotContext,
        device_scale_factor: float,
        window_bounds: Rectangle,
    ):
        all_accessibility_tree_nodes[ax_node.nodeId] = self
        self._is_expanded = node_has_property(ax_node, "expanded")
        self._ax_node = ax_node
        self._parent = parent
        self._is_ignored: bool | None = None
        self._bounds: Rectangle | None = None
        self._is_visible: bool | None = None
        self._current_src_url: str | None = None
        self._input: str | None = None
        self.__closest_non_ignored_parent: AccessibilityTreeNode | None = None
        dom_node_index = (
            index_for_node_id(ax_node.backendDOMNodeId, snapshot_context)
            if ax_node.backendDOMNodeId
            else None
        )
        # The following three variables (_bounds, _is_visible, and _current_src_url) depend on
        # info from the DOM element associated with this AXNode
        if dom_node_index:
            self._bounds = (
                Rectangle(*layout_bounds).scale(1 / device_scale_factor)
                if (
                    layout_bounds := bounds_for_node_index(
                        dom_node_index, snapshot_context
                    )
                )
                is not None
                else None
            )
            self._is_visible = (
                (self._bounds.has_area and self._bounds.overlaps(window_bounds))
                if self._bounds
                else None
            )
            self._current_src_url = (
                current_url_src_for_node_index(dom_node_index, snapshot_context)
                if self.role == "image"
                else None
            )
        if node_has_property(ax_node, "editable"):
            # Sometimes a node will have it's input stored as 'value' other times we
            # need to go looking through the DOM tree to find its matching string.
            self._input = (
                string_from_ax_value(ax_node.value)
                if ax_node.value
                else text_value_for_node_index(dom_node_index, snapshot_context)
                if dom_node_index is not None
                else None
            )
        # this is a little awkward, but the old code ensured that menuitems were visible in given the following condition
        if self.role == "menuitem" and parent is not None and parent.is_expanded:
            self._is_visible = True
        # It's important to set all other properties before creating children in case
        # the children sample parent.xxx
        self.children = (
            [
                AccessibilityTreeNode(
                    ax_node=ax_nodes[child_id],
                    ax_nodes=ax_nodes,
                    parent=self,
                    all_accessibility_tree_nodes=all_accessibility_tree_nodes,
                    snapshot_context=snapshot_context,
                    device_scale_factor=device_scale_factor,
                    window_bounds=window_bounds,
                )
                for child_id in ax_node.childIds
            ]
            if ax_node.childIds
            else None
        )
    def __str__(self) -> str:
        bounds_string = str(self.bounds) if _INCLUDE_BOUNDS_IN_OUTPUT else None
        input_string = f'Current input: "{self._input}"' if self._input else None
        name_string = f'"{self.name}"' if self.name else None
        # Without bounds we can't click on the element, and therefore the ID
        # can not be used. Still useful to show these elements though (e.g.)
        # to let user know about options.
        id_string = f"[{'*' if not self._bounds or not self._bounds.has_area else self._ax_node.nodeId}]"
        url_string = self._current_src_url if self._current_src_url else None
        url_string = _maybe_redact_base64_url(self._current_src_url)
        return " ".join(
            [
                item
                for item in [
                    id_string,
                    self.role,
                    name_string,
                    url_string,
                    input_string,
                    self._property_string(),
                    bounds_string,
                ]
                if item is not None
            ]
        )
    @property
    def node_id(self) -> AXNodeId:
        return self._ax_node.nodeId
    @property
    def name(self) -> str:
        return string_from_ax_value(self._ax_node.name)
    @property
    def is_expanded(self) -> bool:
        return self._is_expanded
    @property
    def is_visible(self) -> bool:
        return self._is_visible or False
    @property
    def bounds(self) -> Rectangle | None:
        return self._bounds
    @property
    def current_src_url(self) -> str | None:
        return self._current_src_url
    @property
    def input(self) -> str | None:
        return self._input
    @property
    def is_ignored(self) -> bool:
        # computing _is_ignored is very expensive, do it on demand and remember the result
        if self._is_ignored is None:
            self._is_ignored = self._compute_is_ignored()
        return self._is_ignored
    @property
    def role(self) -> str:
        return string_from_ax_value(self._ax_node.role)
    @property
    def _closest_non_ignored_parent(self) -> Optional["AccessibilityTreeNode"]:
        if self._parent and self.__closest_non_ignored_parent is None:
            self.__closest_non_ignored_parent = (
                None
                if not self._parent
                else self._parent
                if not self._parent.is_ignored
                else self._parent._closest_non_ignored_parent  # pylint: disable=protected-access
            )
        return self.__closest_non_ignored_parent
    def render_accessibility_tree(self, indent: int = 0) -> str:
        """Returns the string representation of the node and its children."""
        result = ""
        if not self.is_ignored:
            result = "  " * indent + str(self) + "\n"
            indent += 1
        children_strings = [
            child.render_accessibility_tree(indent) for child in self.children or []
        ]
        result = result + "".join([s for s in children_strings if s])
        return result
    def render_main_content(self) -> str | None:
        return (
            main_node._extract_text()  # pylint: disable=protected-access
            if (main_node := self._get_main_content_node())
            else None
        )
    def _compute_is_ignored(self) -> bool:
        if self._ax_node.ignored or not self.is_visible or self.role in _ROLES_IGNORED:
            return True
        # if the parent is a link/button with a name, and this node is within the parent's bounds, we can ignore it.
        if (
            (closest_parent := self._closest_non_ignored_parent)
            and closest_parent.bounds
            and self.bounds
            and (closest_parent.role == "link" or closest_parent.role == "button")
            and closest_parent.name
            and self.bounds.within(closest_parent.bounds)
        ):
            return True
        if bool(self.name):  # excludes None or ""
            # This simplifies things like links/buttons whose contents are sub-elements
            # e.g.
            #   [158] button "Menu"
            #     [1166] StaticText "Menu"
            # becomes
            #   [158] button "Menu"
            return self._parent is not None and self._parent.name == self.name
        return self.role in _ROLES_REQUIRING_A_NAME
    def _get_main_content_node(self) -> Optional["AccessibilityTreeNode"]:
        if self.role == "main":
            return self
        return next(
            (
                result
                for child in self.children or []
                if (result := child._get_main_content_node())  # pylint: disable=protected-access
            ),
            None,
        )
    def _extract_text(self):
        """
        Recursively extracts and concatenates text from the accessibility tree nodes.
        This method traverses the accessibility tree starting from the current node,
        concatenating the text of each node.
        Returns:
          str: The concatenated text from the accessibility tree nodes, with
             appropriate spacing based on node roles.
        """
        if not self.is_visible:
            return ""
        def reducer(acc, child):
            child_text = child._extract_text()  # pylint: disable=protected-access
            return (
                acc
                + (
                    "\n\n"
                    if child.role
                    in {"paragraph", "note", "row", "div", "heading", "region"}
                    else " "
                )
                + child_text
            )
        return reduce(reducer, self.children or [], self.name).strip()
    def _property_string(self) -> str:
        properties = [
            _prop_name_value_str(node_property)
            if node_property.value.value
            else f"{node_property.name}"
            for node_property in self._ax_node.properties or ()
            if node_property.name not in _IGNORED_AT_PROPERTIES
        ]
        return " [" + ", ".join(properties) + "]" if properties else ""
def _prop_name_value_str(node_property: AXProperty) -> str:
    # pre-commit hook (debug-statements) crashes if value is inlined below
    value = (
        _maybe_redact_base64_url(node_property.value.value)
        if node_property.name == "url" and isinstance(node_property.value.value, str)
        else node_property.value.value
    )
    return f"{node_property.name}: {value}"
def _maybe_redact_base64_url(url: str | None) -> str | None:
    return (
        "data:<base64-data-removed>"
        if url and url.startswith("data:") and "base64" in url
        else url
    )

================
File: src/inspect_ai/tool/_tools/_web_browser/_resources/accessibility_tree.py
================
from functools import reduce
from typing import Iterable, TypedDict
from accessibility_tree_node import AccessibilityTreeNode
from cdp.a11y import AXNode, AXNodeId
from cdp.dom_snapshot import DOMSnapshot, create_snapshot_context
from rectangle import Rectangle
_AccType = tuple[
    AXNode | None,
    dict[AXNodeId, AXNode],
]
class AccessibilityTree(TypedDict):
    root: AccessibilityTreeNode
    nodes: dict[AXNodeId, AccessibilityTreeNode]
def create_accessibility_tree(
    *,
    ax_nodes: Iterable[AXNode],
    dom_snapshot: DOMSnapshot,
    device_scale_factor: float,
    window_bounds: Rectangle,
) -> AccessibilityTree | None:
    """
    Creates an accessibility tree from the given Chrome DevTools Protocol AX nodes and DOM snapshot.
    Args:
      ax_nodes (Iterable[AXNode]): An iterable of AXNode objects representing the accessibility nodes.
      dom_snapshot (DOMSnapshot): A snapshot of the DOM at the time of accessibility tree creation.
      device_scale_factor (float): The scale factor of the device.
      window_bounds (Bounds): The bounds of the window.
    Returns:
      AccessibilityTree: The accessibility tree.
    """
    # first make a dict of AXNodeId's to AXNode's and find the root on the way
    def reducer(acc: _AccType, ax_node: AXNode) -> _AccType:
        root_node, nodes = acc
        nodes[ax_node.nodeId] = ax_node
        return (
            # TODO: What do we want for multiple roots?
            root_node or (ax_node if ax_node.parentId is None else None),
            nodes,
        )
    initial_acc: _AccType = (None, {})  # The inference engine is weak
    root_node, nodes = reduce(reducer, ax_nodes, initial_acc)
    if not root_node:
        return None
    # Now create the AccessibilityTreeNode hierarchy
    snapshot_context = create_snapshot_context(dom_snapshot)
    all_accessibility_tree_nodes: dict[AXNodeId, AccessibilityTreeNode] = {}
    return AccessibilityTree(
        root=AccessibilityTreeNode(
            ax_node=root_node,
            ax_nodes=nodes,
            parent=None,
            all_accessibility_tree_nodes=all_accessibility_tree_nodes,
            snapshot_context=snapshot_context,
            device_scale_factor=device_scale_factor,
            window_bounds=window_bounds,
        ),
        nodes=all_accessibility_tree_nodes,
    )

================
File: src/inspect_ai/tool/_tools/_web_browser/_resources/cdp/__init__.py
================
"""
Types and pure functional helpers associated with Chrome DevTools Protocol
https://chromedevtools.github.io/devtools-protocol/
"""

================
File: src/inspect_ai/tool/_tools/_web_browser/_resources/cdp/a11y.py
================
"""
Types and pure functional helpers associated with Chrome DevTools Protocol's 'Accessibility' Domain
https://chromedevtools.github.io/devtools-protocol/tot/Accessibility/
"""
from typing import Literal, NewType, Optional
from pydantic import BaseModel
from cdp.dom import DOMBackendNodeId
# brand these str's so that we don't confuse them with other str's
PageFrameId = NewType("PageFrameId", str)
AXNodeId = NewType("AXNodeId", str)
AXPropertyName = Literal[
    "actions",
    "busy",
    "disabled",
    "editable",
    "focusable",
    "focused",
    "hidden",
    "hiddenRoot",
    "invalid",
    "keyshortcuts",
    "settable",
    "roledescription",
    "live",
    "atomic",
    "relevant",
    "root",
    "autocomplete",
    "hasPopup",
    "level",
    "multiselectable",
    "orientation",
    "multiline",
    "readonly",
    "required",
    "valuemin",
    "valuemax",
    "valuetext",
    "checked",
    "expanded",
    "modal",
    "pressed",
    "selected",
    "activedescendant",
    "controls",
    "describedby",
    "details",
    "errormessage",
    "flowto",
    "labelledby",
    "owns",
    "url",
]
AXValueType = Literal[
    "boolean",
    "tristate",
    "booleanOrUndefined",
    "idref",
    "idrefList",
    "integer",
    "node",
    "nodeList",
    "number",
    "string",
    "computedString",
    "token",
    "tokenList",
    "domRelation",
    "role",
    "internalRole",
    "valueUndefined",
]
class AXRelatedNode(BaseModel, frozen=True):
    backendDOMNodeId: DOMBackendNodeId
    """The BackendNodeId of the related DOM node."""
    idref: str | None = None
    """The IDRef value provided, if any."""
    text: str | None = None
    """The text alternative of this node in the current context."""
AXValueSourceType = Literal[
    "attribute", "implicit", "stylet", "contents", "placeholder", "relatedElement"
]
AXValueNativeSourceType = Literal[
    "description",
    "figcaption",
    "label",
    "labelfor",
    "labelwrapped",
    "legend",
    "rubyannotation",
    "tablecaption",
    "title",
    "other",
]
class AXValueSource(BaseModel, frozen=True):
    """A single source for a computed AX property."""
    type: AXValueSourceType
    """What type of source this is."""
    value: Optional["AXValue"] = None
    """The value of this property source."""
    attribute: str | None = None
    """The name of the relevant attribute, if any."""
    attributeValue: Optional["AXValue"] = None
    """The value of the relevant attribute, if any."""
    superseded: bool | None = None
    """Whether this source is superseded by a higher priority source."""
    nativeSource: AXValueNativeSourceType | None = None
    """The native markup source for this value, e.g. a `<label>` element."""
    nativeSourceValue: Optional["AXValue"] = None
    """The value, such as a node or node list, of the native source."""
    invalid: bool | None = None
    """Whether the value for this property is invalid."""
    invalidReason: str | None = None
    """Reason for the value being invalid, if it is."""
class AXValue(BaseModel, frozen=True):
    """A single computed AX property."""
    type: AXValueType
    """The type of this value."""
    value: object | None = None
    """The computed value of this property."""
    relatedNodes: tuple[AXRelatedNode, ...] | None = None
    """One or more related nodes, if applicable."""
    sources: tuple[AXValueSource, ...] | None = None
    """The sources which contributed to the computation of this property."""
class AXProperty(BaseModel, frozen=True):
    name: AXPropertyName
    """The name of this property."""
    value: AXValue
    """The value of this property."""
# AXNode's ignoredReasons is documented to be a list[AXProperty]. However, that seems to
# be erroneous since the namespace for ignored reason names is completely separate from property names.
class AXIgnoredReason(BaseModel, frozen=True):
    # name: Literal[
    #     "uninteresting",
    #     "ariaHiddenSubtree",
    #     "ariaHiddenElement",
    #     "notRendered",
    #     "notVisible",
    #     "emptyAlt",
    # ]
    name: str  # since this is all undocumented, it's best to be loose
    """The name of this ignored reason."""
    value: AXValue
    """The value of this ignored reason."""
class AXNode(BaseModel, frozen=True):
    """A node in the accessibility tree."""
    nodeId: AXNodeId
    """Unique identifier for this node."""
    ignored: bool
    """Wether this node is ignore3d for accessibility"""
    ignoredReasons: tuple[AXIgnoredReason, ...] | None = None
    """Collection of reasons why this node is hidden."""
    role: AXValue | None = None
    """This `Node`'s role, wether explicit or implicit."""
    chromeRole: AXValue | None = None
    """This `Node`'s Chrome raw role."""
    name: AXValue | None = None
    """The accessible name for this `Node`."""
    description: AXValue | None = None
    """The accessible description for this `Node`."""
    value: AXValue | None = None
    """The value for this `Node`."""
    properties: tuple[AXProperty, ...] | None = None
    """All other properties"""
    parentId: AXNodeId | None = None
    """ID for this node's parent."""
    childIds: tuple[AXNodeId, ...] | None = None
    """IDs for each of this node's child nodes."""
    backendDOMNodeId: DOMBackendNodeId | None = None
    """The backend ID for the associated DOM node, if any."""
    frameId: PageFrameId | None = None
    """The frame ID for the frame associated with this nodes document."""
class AXTree(BaseModel, frozen=True):
    nodes: tuple[AXNode, ...]
# CDP defines `AXValue.value` as `Any`, so this is a coercion function
def string_from_ax_value(value: AXValue | None) -> str:
    """
    Coerces an AXValue to a string.
    Args:
      value (AXValue | None): The AXValue to be coerced.
    Returns:
      str: The string representation of the AXValue, or an empty string if the value is None or not a string.
    """
    return value.value if value and isinstance(value.value, str) else ""
def node_has_property(node: AXNode, property_name: AXPropertyName) -> bool:
    """
    Checks if an AXNode has a specific property.
    Args:
      node (AXNode): The AXNode to check.
      property_name (AXPropertyName): The name of the property to look for.
    Returns:
      bool: True if the node has the property, False otherwise.
    """
    return node_property(node, property_name) is not None
def node_property(node: AXNode, property_name: AXPropertyName) -> AXProperty | None:
    """
    Retrieves a specific property from an AXNode.
    Args:
      node (AXNode): The AXNode to retrieve the property from.
      property_name (AXPropertyName): The name of the property to retrieve.
    Returns:
      AXProperty | None: The property if found, otherwise None.
    """
    return next(
        (prop for prop in node.properties or () if prop.name == property_name), None
    )
def node_bool_property(node: AXNode, property_name: AXPropertyName) -> bool:
    """
    Retrieves a boolean property from an AXNode.
    Args:
      node (AXNode): The AXNode to retrieve the property from.
      property_name (AXPropertyName): The name of the property to retrieve.
    Returns:
      bool: The boolean value of the property, or False if the property is not found or not a boolean.
    """
    return bool(prop.value) if (prop := node_property(node, property_name)) else False
def node_str_property(node: AXNode, property_name: AXPropertyName) -> str | None:
    """
    Retrieves a str property from an AXNode.
    Args:
      node (AXNode): The AXNode to retrieve the property from.
      property_name (AXPropertyName): The name of the property to retrieve.
    Returns:
      str: The str value of the property, or None if the property is not found or not a str.
    """
    prop = node_property(node, property_name)
    if prop is None:
        return None
    v = prop.value
    return v.value if isinstance(v.value, str) else None

================
File: src/inspect_ai/tool/_tools/_web_browser/_resources/cdp/dom_snapshot.py
================
"""
Types and pure functional helpers associated with Chrome DevTools Protocol's 'DOMSnapshot' Domain
https://chromedevtools.github.io/devtools-protocol/tot/DOMSnapshot/
"""
from typing import Literal, NewType, TypedDict
from pydantic import BaseModel
from cdp.dom import DOMBackendNodeId
StringIndex = NewType("StringIndex", int)
Rectangle = tuple[float, ...]
ArrayOfStrings = tuple[StringIndex, ...]
class RareStringData(BaseModel, frozen=True):
    """Data that is only present on rare nodes."""
    index: tuple[int, ...]
    value: tuple[StringIndex, ...]
class RareIntegerData(BaseModel, frozen=True):
    index: tuple[int, ...]
    value: tuple[int, ...]
class RareBooleanData(BaseModel, frozen=True):
    index: tuple[int, ...]
class NodeTreeSnapshot(BaseModel, frozen=True):
    """Table containing nodes."""
    parentIndex: tuple[int, ...] | None = None
    """Parent node index."""
    nodeType: tuple[int, ...] | None = None
    """`Node`'s nodeType"""
    shadowRootType: RareStringData | None = None
    """Type of the shadow root the `Node` is in. String values are equal to the `ShadowRootType` enum."""
    nodeName: tuple[StringIndex, ...] | None = None
    """`Node`'s nodeName."""
    nodeValue: tuple[StringIndex, ...] | None = None
    """`Node`'s nodeValue."""
    backendNodeId: tuple[DOMBackendNodeId, ...] | None = None
    """`Node`'s id, corresponds to DOM.Node.backendNodeId."""
    attributes: tuple[ArrayOfStrings, ...] | None = None
    # attributes: list[int] | None = None
    """Attributes of an `Element` node. Flatten name, value pairs."""
    textValue: RareStringData | None = None
    """Only set for textarea elements, contains the text value."""
    inputValue: RareStringData | None = None
    """Only set for input elements, contains the input's associated text value."""
    inputChecked: RareBooleanData | None = None
    """Only set for radio and checkbox input elements, indicates if the element has been checked"""
    optionSelected: RareBooleanData | None = None
    """Only set for option elements, indicates if the element has been selected"""
    contentDocumentIndex: RareIntegerData | None = None
    """The index of the document in the list of the snapshot documents."""
    pseudoType: RareStringData | None = None
    """Type of a pseudo element node."""
    pseudoIdentifier: RareStringData | None = None
    """Pseudo element identifier for this node. Only present if there is a valid pseudoType."""
    isClickable: RareBooleanData | None = None
    """Whether this DOM node responds to mouse clicks. This includes nodes that have had click event listeners attached via JavaScript as well as anchor tags that naturally navigate when clicked."""
    currentSourceURL: RareStringData | None = None
    """The selected url for nodes with a srcset attribute."""
    originURL: RareStringData | None = None
    """The url of the script (if any) that generates this node."""
class LayoutTreeSnapshot(BaseModel, frozen=True):
    """Table of details of an element in the DOM tree with a `LayoutObject`."""
    nodeIndex: tuple[int, ...]
    """Index of the corresponding node in the `NodeTreeSnapshot` array returned by `captureSnapshot`."""
    styles: tuple[ArrayOfStrings, ...]
    """Array of indexes specifying computed style strings, filtered according to the computedStyles parameter passed to captureSnapshot."""
    bounds: tuple[Rectangle, ...]
    """The absolute position bounding box."""
    text: tuple[StringIndex, ...]
    """Contents of the `LayoutText`, if any."""
    stackingContexts: RareBooleanData
    """Stacking context information."""
    paintOrders: tuple[int, ...] | None = None
    """Global paint order index, which is determined by the stacking order of the nodes. Nodes that are painted together will have the same index. Only provided if `includePaintOrder` in `captureSnapshot` was true."""
    offsetRects: tuple[Rectangle, ...] | None = None
    """The offset rect of nodes. Only available when `includeDOMRects` is set to true"""
    scrollRects: tuple[Rectangle, ...] | None = None
    """The scroll rect of nodes. Only available when `includeDOMRects` is set to true"""
    clientRects: tuple[Rectangle, ...] | None = None
    """The client rect of nodes. Only available when `includeDOMRects` is set to true"""
    blendedBackgroundColors: tuple[StringIndex, ...] | None = None
    """The list of background colors that are blended with colors of overlapping elements. Experimental"""
    textColorOpacities: tuple[float, ...] | None = None
    """The list of computed text opacities. Experimental"""
class TextBoxSnapshot(BaseModel, frozen=True):
    """Table of details of the post layout rendered text positions. The exact layout should not be regarded as stable and may change between versions."""
    layoutIndex: tuple[int, ...]
    """Index of the layout tree node that owns this box collection."""
    bounds: tuple[Rectangle, ...]
    """The absolute position bounding box."""
    start: tuple[int, ...]
    """The starting index in characters, for this post layout textbox substring. Characters that would be represented as a surrogate pair in UTF-16 have length 2."""
    length: tuple[int, ...]
    """The number of characters in this post layout textbox substring. Characters that would be represented as a surrogate pair in UTF-16 have length 2."""
class DocumentSnapshot(BaseModel, frozen=True):
    documentURL: StringIndex
    """Document URL that `Document` or `FrameOwner` node points to."""
    title: StringIndex
    """Document title."""
    baseURL: StringIndex
    """Base URL that `Document` or `FrameOwner` node uses for URL completion."""
    contentLanguage: StringIndex
    """Contains the document's content language."""
    encodingName: StringIndex
    """Contains the document's character set encoding."""
    publicId: StringIndex
    """`DocumentType` node's publicId."""
    systemId: StringIndex
    """`DocumentType` node's systemId."""
    frameId: StringIndex
    """Frame ID for frame owner elements and also for the document node."""
    nodes: NodeTreeSnapshot
    """A table with dom nodes."""
    layout: LayoutTreeSnapshot
    """The nodes in the layout tree."""
    textBoxes: TextBoxSnapshot
    """The post-layout inline text nodes."""
    scrollOffsetX: float | None = None
    """Horizontal scroll offset."""
    scrollOffsetY: float | None = None
    """Vertical scroll offset."""
    contentWidth: float | None = None
    """Document content width."""
    contentHeight: float | None = None
    """Document content height."""
class DOMSnapshot(BaseModel, frozen=True):
    documents: tuple[DocumentSnapshot, ...]
    """The nodes in the DOM tree. The DOMNode at index 0 corresponds to the root document."""
    strings: tuple[str, ...]
    """Shared string table that all string properties refer to with indexes."""
NodeIndex = NewType("NodeIndex", int)
LayoutIndex = NewType("LayoutIndex", int)
class DOMSnapshotContext(TypedDict):
    dom_snapshot: DOMSnapshot
    node_id_to_node_index: dict[DOMBackendNodeId, NodeIndex]
    # Below are dicts to help with non-1:1 index mappings like layouts and RareStringData's
    node_index_to_layout_index: dict[NodeIndex, LayoutIndex]
    node_index_to_current_src_url_index: dict[NodeIndex, StringIndex]
    node_index_to_text_value_index: dict[NodeIndex, StringIndex]
def create_snapshot_context(dom_snapshot: DOMSnapshot) -> DOMSnapshotContext:
    """
    Creates a DOMSnapshotContext that can be used to efficiently extract data from a DOMSnapshot.
    Given the fact that a DOMSnapshot uses long flat correlated lists (see below), this context
    enables efficient lookups that avoid full list scans.
    NOTE: Currently, this context and associated code assumes document[0]
    The DOMSnapshot domain represents the DOM tree as as set of objects each containing
    parallel flat arrays  NodeTreeSnapshot, LayoutTreeSnapshot, etc.
    Each node in the DOM tree is represented in the NodeTreeSnapshot object by corresponding
    entries at the same index across multiple arrays (parentIndex, nodeType, backendNodeId, etc.):
    NodeTreeSnapshot:
                     0       1    |   2    |   3    |   4
    
    actual tag     <html>  <body>  <div>    <p>    <span>
    parentIndex     -1       0       1       2       3
    backendNodeId   666     745     103     421      7
    attributes      ...     ...     ...     ...     ...
    nodeValue       ...     ...     ...     ...     ...
    etc             ...     ...     ...     ...     ...
    Because layout is independent of node ordering, entries in the LayoutTreeSnapshot object maintain
    a reference back to their corresponding node through the nodeIndex field along with layout specific
    properties (bounds, paintOrders, etc).
    LayoutTreeSnapshot:
                     0        1      2       3       4
    
    bounds          ...     ...     ...     ...     ...
    etc             ...     ...     ...     ...     ...
    nodeIndex        3       0       4       2       1
    
                                                 
    backendNodeId   421     666      7      103     745
    actual tag      <p>    <html>  <span>  <div>   <body>
    """
    document = dom_snapshot.documents[0]
    return {
        "dom_snapshot": dom_snapshot,
        "node_id_to_node_index": {
            node_id: index
            for index, node_id in enumerate(document.nodes.backendNodeId or [])
        },
        "node_index_to_layout_index": {
            node_index: layout_index
            for layout_index, node_index in enumerate(document.layout.nodeIndex)
        },
        "node_index_to_current_src_url_index": _dict_for_rare_string_data(
            document.nodes.currentSourceURL
        ),
        "node_index_to_text_value_index": _dict_for_rare_string_data(
            document.nodes.textValue
        ),
    }
def index_for_node_id(
    node_id: DOMBackendNodeId, context: DOMSnapshotContext
) -> NodeIndex | None:
    return context["node_id_to_node_index"].get(node_id, None)
def bounds_for_node_index(
    node_index: NodeIndex, context: DOMSnapshotContext
) -> Rectangle | None:
    return (
        context["dom_snapshot"].documents[0].layout.bounds[layout_index]
        if (layout_index := _layout_index_for_node_index(node_index, context))
        is not None
        else None
    )
def current_url_src_for_node_index(
    node_index: NodeIndex, context: DOMSnapshotContext
) -> str | None:
    return _rare_string_for_node_index(
        node_index, "node_index_to_current_src_url_index", context
    )
def text_value_for_node_index(
    node_index: NodeIndex, context: DOMSnapshotContext
) -> str | None:
    return _rare_string_for_node_index(
        node_index, "node_index_to_text_value_index", context
    )
def _layout_index_for_node_index(
    node_index: NodeIndex, context: DOMSnapshotContext
) -> LayoutIndex | None:
    """Returns the index of the layout entry, if any, corresponding to the given node id."""
    return context["node_index_to_layout_index"].get(node_index, None)
def _rare_string_for_node_index(
    node_index: NodeIndex,
    dict_name: Literal[
        "node_index_to_text_value_index", "node_index_to_current_src_url_index"
    ],
    context: DOMSnapshotContext,
) -> str | None:
    return (
        _string_for_string_index(text_value_index, context)
        if (text_value_index := context[dict_name].get(node_index, None))
        else None
    )
def _string_for_string_index(index: StringIndex, context: DOMSnapshotContext) -> str:
    return context["dom_snapshot"].strings[index]
def _dict_for_rare_string_data(
    rare_string_data: RareStringData,
) -> dict[NodeIndex, StringIndex]:
    """Creates a dictionary from a RareStringData object enabling lookups given a node index."""
    return {
        index: value
        for index, value in zip(rare_string_data.index, rare_string_data.value)
    }

================
File: src/inspect_ai/tool/_tools/_web_browser/_resources/cdp/dom.py
================
"""
Types associated with Chrome DevTools Protocol's 'DOM' Domain
https://chromedevtools.github.io/devtools-protocol/tot/DOM/
"""
from typing import NewType
DOMBackendNodeId = NewType("DOMBackendNodeId", int)

================
File: src/inspect_ai/tool/_tools/_web_browser/_resources/cdp/page.py
================
"""
Types associated with Chrome DevTools Protocol's 'Page' Domain
https://chromedevtools.github.io/devtools-protocol/tot/Page/
"""
from typing import Literal, NewType
from pydantic import BaseModel
FrameId = NewType("FrameId", str)
class SecurityOriginDetails(BaseModel, frozen=True):
    """Additional information about the frame document's security origin."""
    isLocalhost: bool
    """Indicates whether the frame document's security origin is one of the local hostnames (e.g. "localhost") or IP addresses (IPv4 127.0.0.0/8 or IPv6 ::1)."""
AdFrameType = Literal["none", "child", "root"]
AdFrameExplanation = Literal["ParentIsAd", "CreatedByAdScript", "MatchedBlockingRule"]
class AdFrameStatus(BaseModel, frozen=True):
    """Indicates whether a frame has been identified as an ad and why."""
    adFrameType: AdFrameType
    explanations: tuple[AdFrameExplanation, ...] | None = None
SecureContextType = Literal[
    "Secure", "SecureLocalhost", "InsecureScheme", "InsecureAncestor"
]
CrossOriginIsolatedContextType = Literal[
    "Isolated", "NotIsolated", "NotIsolatedFeatureDisabled"
]
GatedAPIFeatures = Literal[
    "SharedArrayBuffers",
    "SharedArrayBuffersTransferAllowed",
    "PerformanceMeasureMemory",
    "PerformanceProfile",
]
class Frame(BaseModel, frozen=True):
    """Information about the Frame on the page."""
    id: FrameId
    """Frame unique identifier."""
    parentId: FrameId | None = None
    """Parent frame identifier."""
    loaderId: object  # Network.LoaderId
    """Identifier of the loader associated with this frame."""
    name: str | None = None
    """Frame's name as specified in the tag."""
    url: str
    """Frame document's URL without fragment."""
    urlFragment: str | None = None
    """Frame document's URL fragment including the '#'."""
    domainAndRegistry: str
    """Frame document's registered domain, taking the public suffixes list into account. Extracted from the Frame's url. Example URLs: http://www.google.com/file.html -> "google.com" http://a.b.co.uk/file.html -> "b.co.uk"""
    securityOrigin: str
    """Frame document's security origin."""
    securityOriginDetails: SecurityOriginDetails | None = None
    """Additional details about the frame document's security origin."""
    mimeType: str
    """Frame document's mimeType as determined by the browser."""
    unreachableUrl: str | None = None
    """If the frame failed to load, this contains the URL that could not be loaded. Note that unlike url above, this URL may contain a fragment."""
    adFrameStatus: AdFrameStatus | None = None
    """Indicates whether this frame was tagged as an ad and why."""
    secureContextType: SecureContextType
    """Indicates whether the main document is a secure context and explains why that is the case."""
    crossOriginIsolatedContextType: CrossOriginIsolatedContextType
    """Indicates whether this is a cross origin isolated context."""
    gatedAPIFeatures: tuple[GatedAPIFeatures, ...]
    """Indicated which gated APIs / features are available."""
class FrameTree(BaseModel, frozen=True):
    frame: Frame
    """Frame information for this tree item."""
    childFrames: tuple["FrameTree", ...] | None = None
    """Child frames."""
class FrameTrees(BaseModel, frozen=True):
    frameTree: FrameTree

================
File: src/inspect_ai/tool/_tools/_web_browser/_resources/constants.py
================
SERVER_PORT = 5555
DEFAULT_SESSION_NAME = "WebBrowser"

================
File: src/inspect_ai/tool/_tools/_web_browser/_resources/mock_environment.py
================
"""A mock dm_env for unit testing."""
from typing import Any
import dm_env
from dm_env import specs
class MockEnvironment(dm_env.Environment):
    """A Mock DM environment."""
    def __init__(self, context):
        """Initializes the environment."""
        super().__init__()
        self._last_command = ""
    def reset(self) -> dm_env.TimeStep:
        """Starts a new sequence and returns the first `TimeStep` of this sequence."""
        self._last_command = ""
        return dm_env.restart(observation=self.get_observations())
    def step(self, action: list[str]) -> dm_env.TimeStep:
        """Updates the environment according to the action and returns a `TimeStep`."""
        self._last_command = " ".join(action)
        return dm_env.transition(
            reward=0.0,
            observation=self.get_observations(),
        )
    def observation_spec(self) -> dict[str, specs.Array]:
        """Defines the observations provided by the environment."""
        obs_shapes = {
            "last_command": specs.Array(shape=(), dtype=str, name="last_command"),
        }
        return obs_shapes
    def action_spec(self) -> specs.Array:
        """Defines the actions that should be provided to `step`."""
        return specs.Array(shape=(), dtype=str, name="command")
    def get_observations(self) -> dict[str, Any]:
        """Returns dictionary containing observations."""
        return {
            "last_command": self._last_command,
        }

================
File: src/inspect_ai/tool/_tools/_web_browser/_resources/playwright_browser.py
================
import logging
from os import getenv
from playwright.async_api import Browser, BrowserContext, Playwright, async_playwright
class PlaywrightBrowser:
    """Stores the browser and creates new contexts."""
    WIDTH = 1280
    HEIGHT = 1080
    _playwright: Playwright | None = None
    @classmethod
    async def create(cls, headless: bool | None = None) -> "PlaywrightBrowser":
        if PlaywrightBrowser._playwright is None:
            PlaywrightBrowser._playwright = await async_playwright().start()
        headless = True if headless is None else headless
        logging.info(
            "Starting chromium in %s mode.", "headless" if headless else "headful"
        )
        return PlaywrightBrowser(
            await PlaywrightBrowser._playwright.chromium.launch(
                headless=headless,
                # Required for Gmail signup see
                # https://stackoverflow.com/questions/65139098/how-to-login-to-google-account-with-playwright
                args=["--disable-blink-features=AutomationControlled"],
            )
        )
    def __init__(self, browser: Browser) -> None:
        self._browser = browser
    async def get_new_context(self) -> BrowserContext:
        return await self._browser.new_context(
            geolocation={"longitude": -0.12, "latitude": 51},
            locale="en-GB",
            permissions=["geolocation"],
            timezone_id="Europe/London",
            viewport={"width": self.WIDTH, "height": self.HEIGHT},
            ignore_https_errors=getenv("IGNORE_HTTPS_ERRORS", "") != "",
        )
    async def close(self) -> None:
        await self._browser.close()
        if PlaywrightBrowser._playwright is not None:
            await PlaywrightBrowser._playwright.stop()
            PlaywrightBrowser._playwright = None

================
File: src/inspect_ai/tool/_tools/_web_browser/_resources/playwright_crawler.py
================
"""A crawler implementation using Playwright.
Largely based on  https://github.com/web-arena-x/webarena
"""
from __future__ import annotations
from asyncio.futures import Future
from playwright.async_api import BrowserContext, Page
from playwright_page_crawler import PageCrawler
class PlaywrightCrawler:
    @classmethod
    async def create(
        cls, browser_context: BrowserContext, device_scale_factor: float | None = None
    ) -> PlaywrightCrawler:
        page_crawler = await PageCrawler.create(
            await browser_context.new_page(), device_scale_factor
        )
        return PlaywrightCrawler(browser_context, page_crawler, device_scale_factor)
    def __init__(
        self,
        browser_context: BrowserContext,
        page_crawler: PageCrawler,
        device_scale_factor: float | None,
    ):
        self._device_scale_factor = device_scale_factor
        self._page_future = Future[PageCrawler]()
        self._page_future.set_result(page_crawler)
        browser_context.on("page", self._on_page)
    @property
    async def current_page(self) -> PageCrawler:
        return await self._page_future
    async def _on_page(self, new_page: Page):
        # we know we're switching pages, but it will take time to get the new page crawler, so
        # reset the future to force new callers to wait.
        # TODO: A race remains in the case that we get multiple on_pages before the first one sets the result
        self._page_future = Future()
        self._page_future.set_result(
            await PageCrawler.create(new_page, self._device_scale_factor)
        )

================
File: src/inspect_ai/tool/_tools/_web_browser/_resources/playwright_page_crawler.py
================
"""A crawler implementation using Playwright.
Portions based on  https://github.com/web-arena-x/webarena
"""
from __future__ import annotations
import asyncio
import re
from typing import Literal
from playwright.async_api import CDPSession, Page
from accessibility_tree import AccessibilityTree, create_accessibility_tree
from accessibility_tree_node import AccessibilityTreeNode
from cdp.a11y import AXNodeId, AXTree
from cdp.dom_snapshot import DOMSnapshot
from rectangle import Rectangle
# Number of seconds to wait for possible click induced navigation before proceeding
_WAIT_FOR_NAVIGATION_TIME = 2.0
# The waiting strategy to use between browser commands.
# see https://playwright.dev/docs/api/class-page.
_WAIT_STRATEGY: Literal["domcontentloaded"] = "domcontentloaded"
class PageCrawler:
    @classmethod
    async def create(
        cls, page: Page, device_scale_factor: float | None = None
    ) -> PageCrawler:
        # Enable chrome development tools, and accessibility tree output.
        cdp_session = await page.context.new_cdp_session(page)
        await cdp_session.send("Accessibility.enable")
        return PageCrawler(
            page,
            cdp_session,
            device_scale_factor or await page.evaluate("window.devicePixelRatio"),
        )
    def __init__(
        self, page: Page, cdp_session: CDPSession, device_scale_factor: float
    ) -> None:
        self._page = page
        self._cdp_session = cdp_session
        # Start with an empty accessibility tree
        self._rendered_main_content: str | None = None
        self._rendered_accessibility_tree: str = ""
        self._accessibility_tree: AccessibilityTree | None = None
        self._device_scale_factor = device_scale_factor
    @property
    def page(self) -> Page:
        return self._page
    @property
    def url(self) -> str:
        return self._page.url
    def lookup_node(self, node_id_or_tag: int | str) -> AccessibilityTreeNode:
        """Looks up the node by id or tag.
        Args:
          node_id_or_tag: Either the id number (as int or str), or <tag_name>
        Returns:
          AccessibilityNode.
        Raise:
          LookupError if node is not matched.
        """
        node: AccessibilityTreeNode | None = None
        node_id_or_tag = str(node_id_or_tag)
        nodes = self._accessibility_tree["nodes"] if self._accessibility_tree else {}
        if re.match("^<.*>", node_id_or_tag):
            tag = node_id_or_tag[1:-1].lower()
            # This is a smart tag, try to resolve it.
            if node := next(
                # We match on anything that starts with the code, this is potentially
                # a little brittle, can be replaced with an RE if there are issues.
                (
                    n
                    for n in nodes.values()
                    if n.name.lower().startswith(tag) and not n.is_ignored
                ),
                None,
            ):
                return node
            else:
                raise LookupError(
                    f"Could not find tag {node_id_or_tag} from {[node.name for node in nodes.values() if node.name]}"
                )
        else:
            if (
                node := nodes.get(AXNodeId(node_id_or_tag), None)
            ) and not node.is_ignored:
                return node
            else:
                raise LookupError(f"Could not find element with id {node_id_or_tag}")
    async def update(self) -> None:
        """Updates the accessibility tree and DOM from current page."""
        await self._page.wait_for_load_state(_WAIT_STRATEGY)
        available_retries = 2
        retry_delay = 0.25
        while available_retries:
            self._accessibility_tree = create_accessibility_tree(
                ax_nodes=AXTree(
                    **await self._cdp_session.send("Accessibility.getFullAXTree", {})
                ).nodes,
                dom_snapshot=DOMSnapshot(
                    **await self._cdp_session.send(
                        "DOMSnapshot.captureSnapshot",
                        {
                            "computedStyles": [],
                            "includeDOMRects": True,
                        },
                    )
                ),
                device_scale_factor=self._device_scale_factor,
                window_bounds=Rectangle(
                    await self._page.evaluate("window.pageXOffset"),
                    await self._page.evaluate("window.pageYOffset"),
                    await self._page.evaluate("window.screen.width"),
                    await self._page.evaluate("window.screen.height"),
                ),
            )
            self._rendered_main_content, self._rendered_accessibility_tree = (
                (
                    self._accessibility_tree["root"].render_main_content(),
                    self._accessibility_tree["root"].render_accessibility_tree(),
                )
                if self._accessibility_tree
                else (None, "")
            )
            if self._rendered_accessibility_tree:
                return
            # sometimes, the entire tree is initially ignored. in such cases, it's typically
            # because we're sampling too soon. Waiting a small amount of time and trying again
            # resolves the issue.
            available_retries = available_retries - 1
            await asyncio.sleep(retry_delay)
    def render_at(self) -> str:
        """Returns the current webpage accessibility tree.
        Only elements visible on the screen will be rendered.
        """
        return self._rendered_accessibility_tree
    def render_main_content(self) -> str | None:
        return self._rendered_main_content
    async def go_to_url(self, url: str) -> None:
        """Goes to the given url.
        Args:
          url: The url to redirect crawler to.
        """
        if "://" not in url:
            url = f"https://{url}"
        try:
            await self._page.goto(url, wait_until=_WAIT_STRATEGY)
        except Exception as e:
            print(f"caught {e}")
            raise
    async def click(self, element_id: int | str) -> None:
        """Clicks the element with the given id.
        Args:
          element_id: The id for the element we want to click on.
        """
        element = self.lookup_node(element_id)
        if element.bounds is None:
            raise LookupError(f"Element with id {element_id} has no layout info.")
        # Mouse.click() requires coordinates relative to the viewport:
        # https://playwright.dev/python/docs/api/class-mouse#mouse-click,
        # thus adjusting the Y coordinate since we only scroll up/down.
        scroll_y = await self._page.evaluate("window.scrollY")
        await self._click_and_await_navigation(
            element.bounds.center_x, element.bounds.center_y - scroll_y
        )
    async def clear(self, element_id: int | str) -> None:
        """Clears text within a field."""
        await self.click(element_id)
        await self._page.keyboard.press("Control+A")
        await self._page.keyboard.press("Backspace")
    async def type(self, element_id: int | str, text: str) -> None:
        """Types into the element with the given id."""
        await self.click(element_id)
        await self._page.keyboard.type(text)
    async def scroll(self, direction: Literal["up", "down"]) -> None:
        """Scrolls the page to the given direction.
        Args:
          direction: The direction to scroll in ('up' or 'down')
        """
        match direction.lower():
            case "up":
                await self._page.evaluate(
                    "(document.scrollingElement || document.body).scrollTop ="
                    " (document.scrollingElement || document.body).scrollTop -"
                    " window.innerHeight;"
                )
            case "down":
                await self._page.evaluate(
                    "(document.scrollingElement || document.body).scrollTop ="
                    " (document.scrollingElement || document.body).scrollTop +"
                    " window.innerHeight;"
                )
            case _:
                raise ValueError(f"Invalid scroll direction {direction}")
    async def forward(self) -> None:
        """Move browser forward one history step."""
        await self._page.go_forward(wait_until=_WAIT_STRATEGY)
    async def back(self) -> None:
        """Move browser backward one history step."""
        await self._page.go_back(wait_until=_WAIT_STRATEGY)
    async def refresh(self) -> None:
        """Refresh (reload) the page."""
        await self._page.reload(wait_until=_WAIT_STRATEGY)
    async def _click_and_await_navigation(self, x: float, y: float) -> None:
        """
        Clicks on the specified coordinates and waits for navigation (if any) to occur.
        This function sets up event listeners to detect in-page navigation or new page
        navigation, performs a mouse click at the given coordinates, and waits for the
        navigation to complete within the specified timeout period.
        The point of this is to allow enough time to switch our page in the event of a new
        page being opened. The problem is that it takes some amount of time, and the challenge
        is determining how long to wait.
        A nave approach would simply sleep for some amount of time. However, this time may
        not be long enough AND it would delay the common case by that delay waiting for a new
        page navigation that never comes.
        This approach accomplishes waiting the minimal amount of time in the common cases of
        a click inducing an in page or new page navigation. The downside is that clicks that
        do not induce navigation are delayed by the timeout. Since navigating clicks are much
        more common, this is a reasonable approach.
        """
        future = asyncio.Future[None]()
        async def on_in_page_navigation(_frame):
            if not future.done():
                await self._page.wait_for_load_state(_WAIT_STRATEGY)
                future.set_result()
        async def on_new_page(new_page):
            if not future.done():
                await new_page.wait_for_load_state(_WAIT_STRATEGY)
                future.set_result(None)
        self._page.once("framenavigated", on_in_page_navigation)
        self._page.context.once("page", on_new_page)
        await self._page.mouse.click(x, y)
        try:
            await asyncio.wait_for(future, timeout=_WAIT_FOR_NAVIGATION_TIME)
            # a navigation of some sort has occurred and gotten to domcontentloaded
        except (asyncio.TimeoutError, TimeoutError):
            # No navigation occurred within the timeout period
            pass

================
File: src/inspect_ai/tool/_tools/_web_browser/_resources/README.md
================
## Headless Browser Tool

This directory contains an implementation for the Headless Browser Tool which can be used to test web browsing agents.

### Usage

#### 1. Start the Docker container

A web server with the headless browser will be launched automatically on starting of the docker container and will be ready to receive client requests.

#### 2. Send the command

Use the following format:

```
# Inside the Docker container
$ python web_client.py [COMMAND] [args]
```

###### Commands

The following commands are available at the moment:

* **web_go \<URL\>** - goes to the specified url.
* **web_click \<ELEMENT_ID\>** - clicks on a given element. 
* **web_scroll \<up/down\>** - scrolls up or down one page.
* **web_forward** - navigates forward a page.
* **web_back** - navigates back a page.
* **web_refresh** - reloads current page (F5).
* **web_type \<ELEMENT_ID\> \<TEXT\>** - types the specified text into the input with the specified id.
* **web_type_submit \<ELEMENT_ID\> \<TEXT\>** - types the specified text into the input with the specified id and presses ENTER to submit the form.

#### 3. Read the resulting observations

The result will be printed out in _stdout_ in the following format:

```
# Inside the Docker container
error: <an ERROR message if one occurred>
info: <general info about the container>
web_url: <the URL of the page the browser is currently at>
web_at: <accessibility tree of the visible elements of the page>
```

### Design

The following diagram describes the design and the intended usage of the tool:

![diagram](images/usage_diagram.svg)

The tool consists of the following components:

- [WebServer](web_server.py) - a server which launches a stateful session with the headless chromium browser and interacts with it through the [Playwright API](https://playwright.dev/python/docs/intro) upon receiving client commands. The server components are:

  - _dm_env_servicer.py_ - an implementation for the gRPC Service based on [dm_env_rpc protocol](https://github.com/google-deepmind/dm_env_rpc).
  - _web_environment.py_ - an environment which gets instantiated by the servicer and which launches the browser, stores its state and maps client commands to Playwright API.
  - _playwright_crawler.py_ - a wrapper over the sync Playwright API.

- [WebClient](web_client.py) - a simple stateless client to interact with the server. When launched, the client:
  1. creates a connection with the server;
  2. sends user command to the server;
  3. receives the response in the form of observations and prints them to stdout;
  4. Destroys the connection.

================
File: src/inspect_ai/tool/_tools/_web_browser/_resources/rectangle.py
================
from __future__ import annotations
class Rectangle:
    def __init__(self, left: float, top: float, width: float, height: float):
        self._left = int(left)
        self._width = int(width)
        self._top = int(top)
        self._height = int(height)
    @classmethod
    def _from_left_right_top_bottom(
        cls, left: float, right: float, top: float, bottom: float
    ) -> Rectangle:
        return cls(left, top, right - left, bottom - top)
    @property
    def _right(self) -> int:
        return self._left + self._width
    @property
    def _bottom(self) -> int:
        return self._top + self._height
    @property
    def center_x(self) -> int:
        return self._left + self._width // 2
    @property
    def center_y(self) -> int:
        return self._top + self._height // 2
    @property
    def has_area(self) -> bool:
        return self._width > 0 and self._height > 0
    def __str__(self) -> str:
        return f"({self._left}, {self._top}, {self._width}, {self._height})"
    def scale(self, scale: float) -> Rectangle:
        return self._from_left_right_top_bottom(
            self._left * scale,
            self._right * scale,
            self._top * scale,
            self._bottom * scale,
        )
    def overlaps(self, other: Rectangle) -> bool:
        """Returns if the two rectangles intersect."""
        return (
            other._left < self._right  # pylint: disable=protected-access
            and other._right > self._left  # pylint: disable=protected-access
            and other._top < self._bottom  # pylint: disable=protected-access
            and other._bottom > self._top  # pylint: disable=protected-access
        )
    def within(self, other: Rectangle) -> bool:
        """Returns if this rectangle is within the other rectangle."""
        return (
            other._left <= self._left  # pylint: disable=protected-access
            and other._right >= self._left  # pylint: disable=protected-access
            and other._top <= self._bottom  # pylint: disable=protected-access
            and other._bottom >= self._top  # pylint: disable=protected-access
        )

================
File: src/inspect_ai/tool/_tools/_web_browser/_resources/rpc_client_helpers.py
================
"""
This module provides helper functions and classes for making strongly typed RPC calls.
The module is designed to be generic and should not contain any use case specific logic.
It uses the `httpx` library for making HTTP requests, `jsonrpcclient` for handling JSON-RPC responses,
and `pydantic` for validating and parsing response data into Python objects.
Classes:
  RPCError: Custom exception for handling RPC errors.
Functions:
  typed_rpc_call: Makes a typed RPC call and returns the response as a Pydantic model.
"""
from typing import Generic, Mapping, Type, TypedDict, TypeVar
from httpx import (
    URL,
    Client,
    ConnectError,
    ConnectTimeout,
    HTTPStatusError,
    ReadTimeout,
    Response,
)
from jsonrpcclient import Error, Ok, parse, request
from pydantic import BaseModel
from tenacity import (
    retry,
    retry_if_exception,
    stop_after_attempt,
    stop_after_delay,
    wait_exponential_jitter,
)
class RPCError(RuntimeError):
    def __init__(self, *, code: int, message: str, data: object):
        self.code = code
        self.message = message
        self.data = data
        super().__init__(f"RPCError {code}: {message}")
    def __str__(self):
        return f"RPCError {self.code}: {self.message} (data: {self.data})"
TBaseModel = TypeVar("TBaseModel", bound=BaseModel)
def rpc_call(
    url: URL | str,
    method: str,
    params: dict[str, object] | None,
    response_class: Type[TBaseModel],
) -> TBaseModel:
    """
    Makes an RPC call to the specified URL with the given method and parameters, and returns the response as a parsed and validated instance of the specified response class.
    Args:
      url (URL | str): The URL to which the RPC call is made.
      method (str): The RPC method to be called.
      response_class (Type[TBaseModel]): The class to which the response should be deserialized.
      params (dict[str, object] | None, optional): The parameters to be sent with the RPC call. Defaults to None.
    Returns:
      TBaseModel: An instance of the response class containing the result of the RPC call.
    Raises:
      RPCError: If the RPC call returns an error response.
      RuntimeError: If an unexpected response is received.
    """
    match parse(_retrying_post(url, request(method, params)).json()):
        case Ok(ok_result):
            return response_class(**ok_result)
        case Error(code, message, data):
            raise RPCError(code=code, message=message, data=data)
        case _:
            raise RuntimeError("how did we get here")
def _retrying_post(url: URL | str, json: object | None = None) -> Response:
    max_retries = 3
    total_timeout = 180
    @retry(
        wait=wait_exponential_jitter(),
        stop=(stop_after_attempt(max_retries) | stop_after_delay(total_timeout)),
        retry=retry_if_exception(httpx_should_retry),
    )
    def do_post() -> Response:
        with Client() as client:
            return client.post(url, json=json, timeout=30)
    return do_post()
RPCArgsType = Type[Mapping[str, object]]
class RPCCallTypes(TypedDict, Generic[TBaseModel]):
    args_type: RPCArgsType
    response_class: Type[TBaseModel]
# TODO: cloned from inspect_ai repo code that is unavailable in the container
# fix this by copying that source file into the container
def httpx_should_retry(ex: BaseException) -> bool:
    """Check whether an exception raised from httpx should be retried.
    Implements the strategy described here: https://cloud.google.com/storage/docs/retry-strategy
    Args:
      ex (BaseException): Exception to examine for retry behavior
    Returns:
      True if a retry should occur
    """
    # httpx status exception
    if isinstance(ex, HTTPStatusError):
        # request timeout
        if ex.response.status_code == 408:
            return True
        # lock timeout
        elif ex.response.status_code == 409:
            return True
        # rate limit
        elif ex.response.status_code == 429:
            return True
        # internal errors
        elif ex.response.status_code >= 500:
            return True
        else:
            return False
    # connection error
    elif is_httpx_connection_error(ex):
        return True
    # don't retry
    else:
        return False
def is_httpx_connection_error(ex: BaseException) -> bool:
    return isinstance(ex, ConnectTimeout | ConnectError | ConnectionError | ReadTimeout)

================
File: src/inspect_ai/tool/_tools/_web_browser/_resources/scale_factor.py
================
import subprocess
import sys
# Playwright launches Chromium with --force-device-scale-factor=1 by default, which
# ensures consistent rendering and measurement behavior using CSS pixels instead of
# native device pixels, regardless of the actual display's DPI. On HiDPI displays,
# like Apple Retina, the ratio of native pixels to CSS pixels is typically 2x or even 3x.
#
# However, the Chrome DevTools Protocol (CDP) has an inconsistency when running on
# HiDPI devices with --force-device-scale-factor=1. Although window.devicePixelRatio
# correctly reports as 1 (honoring the forced scale factor), CDP's LayoutTreeSnapshot.bounds
# still returns coordinates in native device pixels without applying the forced scale factor,
# resulting in a mismatch between CSS and native pixels.
#
# To work around this inconsistency, we determine the native scale factor independently
# of browser-reported values, using system-level data via the get_screen_scale_factor helper
# function. We then apply this actual scale factor to adjust the window bounds, ensuring
# they are in the same coordinate space as LayoutTreeSnapshot.bounds. This alignment
# allows the code to accurately calculate node visibility.
def get_screen_scale_factor() -> float:
    if sys.platform == "darwin":
        from AppKit import NSScreen
        return NSScreen.mainScreen().backingScaleFactor()
    elif sys.platform == "win32":
        try:
            # Using GetDpiForSystem from Windows API
            import ctypes
            user32 = ctypes.windll.user32
            user32.SetProcessDPIAware()
            return user32.GetDpiForSystem() / 96.0
        except Exception:
            return 1.0
    elif sys.platform.startswith("linux"):
        try:
            # Try to get scaling from gsettings (GNOME)
            result = subprocess.run(
                ["gsettings", "get", "org.gnome.desktop.interface", "scaling-factor"],
                capture_output=True,
                text=True,
            )
            if result.returncode == 0:
                return float(result.stdout.strip())
            # Try to get scaling from xrandr (X11)
            result = subprocess.run(
                ["xrandr", "--current"], capture_output=True, text=True
            )
            if result.returncode == 0:
                # Parse xrandr output to find scaling
                # This is a simplified check - might need adjustment
                for line in result.stdout.splitlines():
                    if "connected primary" in line and "x" in line:
                        # Look for something like "3840x2160+0+0"
                        resolution = line.split()[3].split("+")[0]
                        if "3840x2160" in resolution:  # 4K
                            return 2.0
            return 1.0
        except Exception:
            return 1.0
    return 1.0  # Default fallback

================
File: src/inspect_ai/tool/_tools/_web_browser/_resources/test_accessibility_tree_node.py
================
from cdp.dom_snapshot import (
    DocumentSnapshot,
    DOMSnapshot,
    LayoutTreeSnapshot,
    NodeTreeSnapshot,
    RareBooleanData,
    StringIndex,
    TextBoxSnapshot,
)
simple_node_tree = NodeTreeSnapshot()
simple_layout_tree = LayoutTreeSnapshot(
    nodeIndex=(),
    styles=(),
    bounds=(),
    text=(),
    stackingContexts=RareBooleanData(index=()),
)
simple_text_boxes = TextBoxSnapshot(layoutIndex=(), bounds=(), start=(), length=())
simple_doc_snapshot = DocumentSnapshot(
    documentURL=StringIndex(0),
    title=StringIndex(1),
    baseURL=StringIndex(0),
    contentLanguage=StringIndex(2),
    encodingName=StringIndex(3),
    publicId=StringIndex(666),
    systemId=StringIndex(666),
    frameId=StringIndex(666),
    nodes=simple_node_tree,
    layout=simple_layout_tree,
    textBoxes=simple_text_boxes,
)
simple_dom_snapshot = DOMSnapshot(
    documents=(), strings=("documentUrl", "title", "contentLanguage", "encoding")
)
# THIS TEST MODULE IS DISABLED FOR NOW. IT WILL COME BACK
# def test_foo():
#     ax_node = cast(
#         AXNode,
#         {"nodeId": "666", "name": {"value": "Test"}, "role": {"value": "button"}},
#     )
#     ax_nodes = {ax_node.nodeId: ax_node}
#     nodes: dict[AXNodeId, AccessibilityTreeNode] = {}
#     snapshot_context = create_snapshot_context(simple_dom_snapshot)
#     window_bounds = Rectangle(0, 0, 1024, 768)
#     node = AccessibilityTreeNode(
#         ax_node={"name": {"value": "Test"}, "role": {"value": "button"}},
#         ax_nodes=ax_nodes,
#         parent=None,
#         all_accessibility_tree_nodes=nodes,
#         snapshot_context=snapshot_context,
#         device_scale_factor=1,
#         window_bounds=window_bounds,
#     )
# class TestAccessibilityTreeNode(absltest.TestCase):
#     def test_getitem(self):
#         node_data = {"name": {"value": "Test"}, "role": {"value": "button"}}
#         node = AccessibilityTreeNode(node_data)
#         self.assertEqual(node["name"], {"value": "Test"})
#         self.assertEqual(node["role"], {"value": "button"})
#         self.assertIsNone(node["invalid_key"])
#     def test_setitem(self):
#         node_data = {"name": {"value": "Test"}}
#         node = AccessibilityTreeNode(node_data)
#         node["role"] = {"value": "button"}
#         self.assertEqual(
#             node._node, {"name": {"value": "Test"}, "role": {"value": "button"}}
#         )
#     def test_str_role_name(self):
#         node_data = {
#             "nodeId": "1",
#             "role": {"value": "button"},
#             "name": {"value": "Test Button"},
#         }
#         node = AccessibilityTreeNode(node_data, bounds=Rectangle(10, 10, 20, 20))
#         self.assertIn('[1] button "Test Button"', str(node))
#     def test_str_image_src(self):
#         node_data = {
#             "nodeId": "1",
#             "role": {"value": "image"},
#             "name": {"value": "Test Image"},
#             "src": "image.png",
#         }
#         node = AccessibilityTreeNode(node_data, bounds=Rectangle(10, 10, 20, 20))
#         self.assertIn('[1] image "Test Image" image.png', str(node))
#     def test_str_property(self):
#         node_data = {
#             "nodeId": "1",
#             "role": {"value": "link"},
#             "name": {"value": "Test Link"},
#             "properties": [{"name": "url", "value": {"value": "www.example.com"}}],
#         }
#         node = AccessibilityTreeNode(node_data, bounds=Rectangle(10, 10, 20, 20))
#         self.assertIn('[1] link "Test Link" [url: www.example.com]', str(node))
#     def test_str_empty(self):
#         node_data = {"nodeId": "1"}
#         node = AccessibilityTreeNode(node_data)
#         self.assertIn('[*]  ""', str(node))
#     def test_link_children(self):
#         node_data = {"nodeId": "1", "childIds": [2, 3]}
#         node = AccessibilityTreeNode(node_data)
#         child1 = AccessibilityTreeNode({"nodeId": "2"})
#         child2 = AccessibilityTreeNode({"nodeId": "3"})
#         node_lookup = {2: child1, 3: child2}
#         node.link_children(node_lookup)
#         self.assertEqual(node.children, [child1, child2])
#         self.assertEqual(child1._parent, node)
#         self.assertEqual(child2._parent, node)
#     def test_get_property(self):
#         node_data = {"name": {"value": "Test"}, "role": {"value": "button"}}
#         node = AccessibilityTreeNode(node_data)
#         self.assertEqual(node.get_property("name"), {"value": "Test"})
#         self.assertEqual(node.get_property("role"), {"value": "button"})
#         self.assertIsNone(node.get_property("invalid_key"))
#     def test_render(self):
#         node_data = {
#             "nodeId": "1",
#             "ignored": False,
#             "childIds": [2],
#             "role": {"value": "button"},
#             "name": {"value": "Test Button"},
#         }
#         node = AccessibilityTreeNode(node_data, bounds=Rectangle(20, 20, 30, 30))
#         self.assertIn('[1] button "Test Button"', node.render())
#     def test_to_render_with_children(self):
#         node_data = {
#             "nodeId": "1",
#             "ignored": False,
#             "childIds": [2, 3],
#             "role": {"value": "button"},
#             "name": {"value": "Test Button"},
#         }
#         node = AccessibilityTreeNode(node_data, bounds=Rectangle(20, 20, 30, 30))
#         child_data = {
#             "nodeId": "2",
#             "ignored": False,
#             "role": {"value": "text"},
#             "name": {"value": "Child Text"},
#         }
#         child = AccessibilityTreeNode(child_data, bounds=Rectangle(20, 20, 30, 30))
#         ignored_child_data = {
#             "nodeId": "3",
#             "ignored": True,
#             "role": {"value": "text"},
#             "name": {"value": "Child Text"},
#         }
#         ignored_child = AccessibilityTreeNode(
#             ignored_child_data, bounds=Rectangle(20, 20, 30, 30)
#         )
#         node.link_children({2: child, 3: ignored_child})
#         self.assertIn('[2] text "Child Text"', node.render())
#         self.assertNotIn('[3] text "Child Text"', node.render())
#     def test_property_string(self):
#         node_data = {"properties": [{"name": "checked", "value": {"value": True}}]}
#         node = AccessibilityTreeNode(node_data)
#         self.assertEqual(node.property_string(), " [checked: True]")
#     def test_property_string_with_ignored_property(self):
#         node_data = {
#             "properties": [{"name": "focusable", "value": {"value": "some_value"}}]
#         }
#         node = AccessibilityTreeNode(node_data)
#         # 'focusable' is in _IGNORED_ACTREE_PROPERTIES
#         self.assertEqual(node.property_string(), "")

================
File: src/inspect_ai/tool/_tools/_web_browser/_resources/test_playwright_crawler.py
================
from absl.testing import parameterized
import playwright_browser
import playwright_crawler
class TestPlaywrightCrawler(parameterized.TestCase):
    def setUp(self):
        self._browser = playwright_browser.PlaywrightBrowser()
        self._crawler = playwright_crawler.PlaywrightCrawler(
            self._browser.get_new_context()
        )
    def test_go_to_page_changes_url(self):
        self.assertEqual(self._crawler.url, "about:blank")
        self._crawler.go_to_page("https://www.example.com")
        self.assertEqual(self._crawler.url, "https://www.example.com/")
    def test_go_to_page_adds_missing_protocol(self):
        self._crawler.go_to_page("www.example.com")
        self.assertEqual(self._crawler.url, "https://www.example.com/")
    def test_nodes_change_on_update(self):
        self._crawler.go_to_page("https://www.example.com")
        self.assertFalse(self._crawler._nodes)
        self._crawler.update()
        self.assertTrue(self._crawler._nodes)
    def test_render_accessibility_tree(self):
        self._crawler.go_to_page("https://www.example.com")
        at_no_update = self._crawler.render_at(
            playwright_crawler.CrawlerOutputFormat.AT
        )
        self.assertEqual(at_no_update, "")
        self._crawler.update()
        at_update = self._crawler.render_at(playwright_crawler.CrawlerOutputFormat.AT)
        nodes = at_update.splitlines()
        self.assertEqual(len(nodes), 3)
        self.assertTrue(
            nodes[0].startswith(
                '[5] RootWebArea "Example Domain" [focused: True, url: https://www.example.com/]'
            )
        )
        self.assertTrue(
            nodes[1].startswith(
                '  [3] StaticText "This domain is for use in illustrative examples in documents'
            )
        )
        self.assertTrue(
            nodes[2].startswith(
                '  [12] link "More information..." [url: https://www.iana.org/domains/example]'
            )
        )
    def test_click_adjusts_to_scrolling_position(self):
        test_html = """
            <!DOCTYPE html>
            <html lang="en">
            <head>
              <meta charset="UTF-8">
              <title>Scrolling Test Page</title>
              <style>
                body { height: 3000px; }
                .my-button { position: absolute; top: 1500px; }
              </style>
            </head>
            <body>
              <button class="my-button" onclick="changeText(this)">Click Me</button>
              <script>
                function changeText(button) {
                  button.textContent = "Text Changed!";
                }
              </script>
            </body>
            </html>
            """
        self._crawler._page_future.set_content(test_html)
        self._crawler.update()
        at_before_scroll = self._crawler.render_at(
            playwright_crawler.CrawlerOutputFormat.AT
        )
        self.assertIn("Scrolling Test Page", at_before_scroll)
        self.assertNotIn("Click Me", at_before_scroll)
        self._crawler.scroll("down")
        self._crawler.update()
        at_after_scroll = self._crawler.render_at(
            playwright_crawler.CrawlerOutputFormat.AT
        )
        self.assertIn("Click Me", at_after_scroll)
        self._crawler.click("17")
        self._crawler.update()
        at_after_click = self._crawler.render_at(
            playwright_crawler.CrawlerOutputFormat.AT
        )
        self.assertIn("Text Changed!", at_after_click)

================
File: src/inspect_ai/tool/_tools/_web_browser/_resources/test_rectangle.py
================
from rectangle import Rectangle
def test_overlaps():
    bounds1 = Rectangle(10, 20, 30, 40)
    bounds2 = Rectangle(20, 30, 40, 50)
    assert bounds1.overlaps(bounds2)
    bounds3 = Rectangle(50, 60, 20, 30)
    assert not bounds1.overlaps(bounds3)
def test_within():
    bounds1 = Rectangle(231, 167, 2, 9)
    bounds2 = Rectangle(231, 167, 26, 9)
    assert bounds1.within(bounds2)

================
File: src/inspect_ai/tool/_tools/_web_browser/_resources/test_web_client.py
================
import sys
from web_browser_rpc_types import GoArgs
from web_client import _parse_args
def test_parse_args_session_name_handling() -> None:
    test_cases: list[tuple[list[str], str, object]] = [
        (
            ["cli", "--session_name=my_session", "web_go", "boston.com"],
            "web_go",
            GoArgs(session_name="my_session", url="boston.com"),
        ),
        (
            ["cli", "web_go", "boston.com", "--session_name=my_session"],
            "web_go",
            GoArgs(session_name="my_session", url="boston.com"),
        ),
        (
            ["cli", "web_go", "--session_name=my_session", "boston.com"],
            "web_go",
            GoArgs(session_name="my_session", url="boston.com"),
        ),
        (
            ["cli", "--session_name", "my_session", "web_go", "boston.com"],
            "web_go",
            GoArgs(session_name="my_session", url="boston.com"),
        ),
        (
            ["cli", "web_go", "boston.com", "--session_name", "my_session"],
            "web_go",
            GoArgs(session_name="my_session", url="boston.com"),
        ),
        (
            ["cli", "web_go", "--session_name", "my_session", "boston.com"],
            "web_go",
            GoArgs(session_name="my_session", url="boston.com"),
        ),
    ]
    for argv, expected_cmd, expected_params in test_cases:
        sys.argv = argv
        cmd, params = _parse_args()
        assert cmd == expected_cmd
        assert params == expected_params

================
File: src/inspect_ai/tool/_tools/_web_browser/_resources/web_browser_rpc_types.py
================
from typing import Literal, TypedDict
from pydantic import BaseModel
class NewSessionArgs(TypedDict):
    headful: bool
class CrawlerBaseArgs(TypedDict):
    session_name: str
class GoArgs(CrawlerBaseArgs):
    url: str
class ClickArgs(CrawlerBaseArgs):
    element_id: str
class ScrollArgs(CrawlerBaseArgs):
    direction: Literal["up", "down"]
class TypeOrSubmitArgs(CrawlerBaseArgs):
    element_id: str
    text: str
class NewSessionResponse(BaseModel):
    session_name: str
class CrawlerResponse(BaseModel):
    web_url: str
    main_content: str | None = None
    web_at: str
    error: str | None = None

================
File: src/inspect_ai/tool/_tools/_web_browser/_resources/web_client_new_session.py
================
import argparse
import sys
from constants import SERVER_PORT
from rpc_client_helpers import RPCError, rpc_call
from web_browser_rpc_types import NewSessionArgs, NewSessionResponse
def main() -> None:
    parser = argparse.ArgumentParser(prog="web_client_new_session")
    parser.add_argument(
        "--headful", action="store_true", help="Run in headful mode for testing"
    )
    args_class = parser.parse_args()
    args_dict = vars(args_class)
    # TODO: Frick. this does no validation
    params_typed_dict = NewSessionArgs(headful=args_dict["headful"])
    params = dict(params_typed_dict)
    try:
        print(
            rpc_call(
                f"http://localhost:{SERVER_PORT}/",
                "new_session",
                params,
                NewSessionResponse,
            ).session_name
        )
    except RPCError as rpc_error:
        print(rpc_error, file=sys.stderr)
        sys.exit(1)
if __name__ == "__main__":
    main()

================
File: src/inspect_ai/tool/_tools/_web_browser/_resources/web_client.py
================
import argparse
import sys
from typing import Literal
from constants import DEFAULT_SESSION_NAME, SERVER_PORT
from rpc_client_helpers import RPCError, rpc_call
from web_browser_rpc_types import (
    ClickArgs,
    CrawlerBaseArgs,
    CrawlerResponse,
    GoArgs,
    NewSessionArgs,
    NewSessionResponse,
    ScrollArgs,
    TypeOrSubmitArgs,
)
_SERVER_URL = f"http://localhost:{SERVER_PORT}/"
def main() -> None:
    if len(sys.argv) > 1:
        command, params = _parse_args()
        _execute_command(command, params)
    else:
        _interactive_mode()
def _execute_command(
    command: str,
    params: NewSessionArgs
    | GoArgs
    | ClickArgs
    | TypeOrSubmitArgs
    | ScrollArgs
    | CrawlerBaseArgs,
) -> None:
    try:
        if command == "new_session":
            print(
                rpc_call(
                    _SERVER_URL, command, dict(params), NewSessionResponse
                ).session_name
            )
        else:
            response = rpc_call(
                _SERVER_URL,
                command,
                dict(params),
                CrawlerResponse,
            )
            for key, value in vars(response).items():
                if value is not None:
                    print(key, ": ", value)
    except RPCError as rpc_error:
        _return_error(f"error: {rpc_error}")
def _interactive_mode() -> None:
    print(
        "Welcome to the Playwright Crawler interactive mode!\n"
        "commands:\n"
        "  web_go <URL> - goes to the specified url.\n"
        "  web_click <ELEMENT_ID> - clicks on a given element.\n"
        "  web_scroll <up/down> - scrolls up or down one page.\n"
        "  web_forward - navigates forward a page.\n"
        "  web_back - navigates back a page.\n"
        "  web_refresh - reloads current page (F5).\n"
        "  web_type <ELEMENT_ID> <TEXT> - types the specified text into the input with the specified id.\n"
        "  web_type_submit <ELEMENT_ID> <TEXT> - types the specified text into the input with the specified id and presses ENTER to submit the form."
    )
    session_created = False
    while True:
        try:
            user_input = input("Enter command: ").strip()
            if user_input.lower() in {"exit", "quit"}:
                break
            args = user_input.split()
            sys.argv = ["cli"] + args
            command, params = _parse_args()
            print(f"command: {command}, params: {params}")
            if not session_created:
                _execute_command("new_session", NewSessionArgs(headful=True))
                session_created = True
            _execute_command(command, params)
        except Exception as e:  # pylint: disable=broad-exception-caught
            print(f"Error: {e}")
def _return_error(error: str) -> None:
    print(error, file=sys.stderr)
    sys.exit(1)
def _create_main_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(prog="web_client")
    parser.add_argument(
        "--session_name",
        type=str,
        required=False,
        default=DEFAULT_SESSION_NAME,
        help="Session name",
    )
    return parser
def _create_command_parser() -> argparse.ArgumentParser:
    result = argparse.ArgumentParser(prog="web_client")
    subparsers = result.add_subparsers(dest="command", required=True)
    go_parser = subparsers.add_parser("web_go")
    go_parser.add_argument("url", type=str, help="URL to navigate to")
    click_parser = subparsers.add_parser("web_click")
    click_parser.add_argument("element_id", type=str, help="ID of the element to click")
    scroll_parser = subparsers.add_parser("web_scroll")
    scroll_parser.add_argument(
        "direction",
        type=str,
        choices=["up", "down"],
        help="Direction to scroll (up or down)",
    )
    subparsers.add_parser("web_forward")
    subparsers.add_parser("web_back")
    subparsers.add_parser("web_refresh")
    type_parser = subparsers.add_parser("web_type")
    type_parser.add_argument(
        "element_id", type=str, help="ID of the element to type into"
    )
    type_parser.add_argument("text", type=str, help="The text to type")
    submit_parser = subparsers.add_parser("web_type_submit")
    submit_parser.add_argument(
        "element_id",
        type=str,
        help="ID of the element to type into and submit",
    )
    submit_parser.add_argument("text", type=str, help="The text to type")
    # Add common argument to all subparsers
    for name, subparser in subparsers.choices.items():
        if name != "new_session":
            subparser.add_argument(
                "--session_name",
                type=str,
                nargs="?",
                required=False,
                default=DEFAULT_SESSION_NAME,
                help="Session name",
            )
    return result
main_parser = _create_main_parser()
command_parser = _create_command_parser()
def _parse_args() -> (
    tuple[Literal["web_go"], GoArgs]
    | tuple[Literal["web_click"], ClickArgs]
    | tuple[Literal["web_type", "web_type_submit"], TypeOrSubmitArgs]
    | tuple[Literal["web_scroll"], ScrollArgs]
    | tuple[Literal["web_forward", "web_back", "web_refresh"], CrawlerBaseArgs]
):
    # web_client.py supports a very non-standard command line. It has a required named
    # parameter, --session_name, before the command.
    # Unfortunately, because we can't break backwards compatibility, we're stuck
    # with that. To properly parse it, we'll be forced to have a separate parser
    # for --session_name and merge the results with the normal command parser.
    main_args, remaining_args = main_parser.parse_known_args()
    session_name = main_args.session_name or DEFAULT_SESSION_NAME
    command_args = command_parser.parse_args(remaining_args)
    command_args_dict = vars(command_args)
    match command_args.command:
        case "web_go":
            return command_args_dict["command"], GoArgs(
                url=command_args_dict["url"],
                session_name=session_name,
            )
        case "web_click":
            return command_args_dict["command"], ClickArgs(
                element_id=command_args_dict["element_id"],
                session_name=session_name,
            )
        case "web_type" | "web_type_submit":
            return command_args_dict["command"], TypeOrSubmitArgs(
                element_id=command_args_dict["element_id"],
                text=command_args_dict["text"],
                session_name=session_name,
            )
        case "web_scroll":
            return command_args_dict["command"], ScrollArgs(
                direction=command_args_dict["direction"],
                session_name=session_name,
            )
        case "web_forward" | "web_back" | "web_refresh":
            return command_args_dict["command"], CrawlerBaseArgs(
                session_name=session_name,
            )
        case _:
            raise ValueError("Unexpected command")
if __name__ == "__main__":
    main()

================
File: src/inspect_ai/tool/_tools/_web_browser/_resources/web_server.py
================
import threading
from typing import Awaitable, Callable, Unpack
from aiohttp.web import Application, Request, Response, run_app
from jsonrpcserver import Result, Success, async_dispatch, method
from constants import DEFAULT_SESSION_NAME, SERVER_PORT
from playwright_browser import PlaywrightBrowser
from playwright_crawler import PlaywrightCrawler
from scale_factor import get_screen_scale_factor
from web_browser_rpc_types import (
    ClickArgs,
    CrawlerBaseArgs,
    CrawlerResponse,
    GoArgs,
    NewSessionArgs,
    NewSessionResponse,
    ScrollArgs,
    TypeOrSubmitArgs,
)
class Sessions:
    def __init__(self) -> None:
        self._lock = threading.Lock()
        self._browser: PlaywrightBrowser | None = None
        self._sessions: dict[str, PlaywrightCrawler] = {}
    async def new_session(self, headful: bool) -> str:
        with self._lock:
            if not self._browser:
                self._browser = await PlaywrightBrowser.create(headless=not headful)
            current_count = len(self._sessions)
            name = (
                DEFAULT_SESSION_NAME
                if current_count == 0
                else f"{DEFAULT_SESSION_NAME}_{current_count}"
            )
            crawler = await PlaywrightCrawler.create(
                await self._browser.get_new_context(),
                device_scale_factor=get_screen_scale_factor() if headful else 1,
            )
            self._sessions[name] = crawler
            return name
    async def get_crawler_for_session(self, name: str) -> PlaywrightCrawler:
        if not self._sessions:
            await self.new_session(False)
        return self._sessions[name]
sessions = Sessions()
@method
async def new_session(**kwargs: Unpack[NewSessionArgs]) -> NewSessionResponse:
    return Success(
        NewSessionResponse(
            session_name=await sessions.new_session(kwargs.get("headful", False))
        ).model_dump()
    )
@method
async def web_go(**kwargs: Unpack[GoArgs]) -> Result:
    async def handler(crawler: PlaywrightCrawler):
        await (await crawler.current_page).go_to_url(kwargs["url"])
    return await _execute_crawler_command(kwargs["session_name"], handler)
@method
async def web_click(**kwargs: Unpack[ClickArgs]) -> Result:
    async def handler(crawler: PlaywrightCrawler):
        await (await crawler.current_page).click(kwargs["element_id"])
    return await _execute_crawler_command(kwargs["session_name"], handler)
@method
async def web_scroll(**kwargs: Unpack[ScrollArgs]) -> Result:
    async def handler(crawler: PlaywrightCrawler):
        await (await crawler.current_page).scroll(kwargs["direction"])
    return await _execute_crawler_command(kwargs["session_name"], handler)
@method
async def web_forward(**kwargs: Unpack[CrawlerBaseArgs]) -> Result:
    async def handler(crawler: PlaywrightCrawler):
        await (await crawler.current_page).forward()
    return await _execute_crawler_command(kwargs["session_name"], handler)
@method
async def web_back(**kwargs: Unpack[CrawlerBaseArgs]) -> Result:
    async def handler(crawler: PlaywrightCrawler):
        await (await crawler.current_page).back()
    return await _execute_crawler_command(kwargs["session_name"], handler)
@method
async def web_refresh(**kwargs: Unpack[CrawlerBaseArgs]) -> Result:
    async def handler(crawler: PlaywrightCrawler):
        await (await crawler.current_page).refresh()
    return await _execute_crawler_command(kwargs["session_name"], handler)
@method
async def web_type(**kwargs: Unpack[TypeOrSubmitArgs]) -> Result:
    async def handler(crawler: PlaywrightCrawler):
        await (await crawler.current_page).type(
            kwargs["element_id"], _str_from_str_or_list(kwargs["text"])
        )
    return await _execute_crawler_command(kwargs["session_name"], handler)
@method
async def web_type_submit(**kwargs: Unpack[TypeOrSubmitArgs]) -> Result:
    async def handler(crawler: PlaywrightCrawler):
        await (await crawler.current_page).clear(kwargs["element_id"])
        await (await crawler.current_page).type(
            kwargs["element_id"], _str_from_str_or_list(kwargs["text"]) + "\n"
        )
    return await _execute_crawler_command(kwargs["session_name"], handler)
async def _execute_crawler_command(
    session_name: str, handler: Callable[[PlaywrightCrawler], Awaitable[None]]
) -> Result:
    if not sessions:
        await new_session()
    try:
        crawler = await sessions.get_crawler_for_session(session_name)
        await handler(crawler)
        await (await crawler.current_page).update()
        # If there's a cookies message click to sort it out.
        await _auto_click_cookies(crawler)
        return Success(
            CrawlerResponse(
                web_url=(await crawler.current_page).url.split("?")[0],
                main_content=(await crawler.current_page).render_main_content(),
                web_at=(await crawler.current_page).render_at(),
                error=None,
            ).model_dump()
        )
    except Exception as e:  # pylint: disable=broad-exception-caught
        return Success(
            CrawlerResponse(
                web_url=(await crawler.current_page).url.split("?")[0],
                web_at="encountered error",
                error=str(e),
            ).model_dump()
        )
def _str_from_str_or_list(str_or_list: str | list[str]) -> str:
    return str_or_list if isinstance(str_or_list, str) else " ".join(str_or_list)
async def _auto_click_cookies(crawler: PlaywrightCrawler):
    """Autoclick any cookies popup."""
    try:
        accept_node = (await crawler.current_page).lookup_node("<Accept all>")
    except LookupError:
        return
    await (await crawler.current_page).click(accept_node.node_id)
    await (await crawler.current_page).update()
def main():
    async def handle_request(request: Request) -> Response:
        return Response(
            text=await async_dispatch(await request.text()),
            content_type="application/json",
        )
    app = Application()
    app.router.add_post("/", handle_request)
    run_app(app, port=SERVER_PORT)
if __name__ == "__main__":
    main()

================
File: src/inspect_ai/tool/_tools/_web_browser/_web_browser.py
================
import re
from textwrap import dedent
from pydantic import Field
from inspect_ai._util.content import ContentText
from inspect_ai._util.error import PrerequisiteError
from inspect_ai.tool._tool import Tool, ToolError, ToolResult, tool
from inspect_ai.tool._tool_call import ToolCall, ToolCallContent, ToolCallView
from inspect_ai.tool._tool_info import parse_tool_info
from inspect_ai.tool._tool_with import tool_with
from inspect_ai.util._sandbox import SandboxEnvironment, sandbox_with
from inspect_ai.util._sandbox.docker.internal import INSPECT_WEB_BROWSER_IMAGE_DOCKERHUB
from inspect_ai.util._store_model import StoreModel, store_as
def web_browser(interactive: bool = True) -> list[Tool]:
    """Tools used for web browser navigation.
     See documentation at <https://inspect.ai-safety-institute.org.uk/tools.html#sec-web-browser>.
    Args:
       interactive: Provide interactive tools (enable
          clicking, typing, and submitting forms). Defaults
          to True.
    Returns:
       List of tools used for web browser navigation.
    """
    # start with go tool (excluding interactive docs if necessary)
    go = web_browser_go()
    if not interactive:
        go = go_without_interactive_docs(go)
    tools = [go]
    # add interactive tools if requested
    if interactive:
        tools = tools + [
            web_browser_click(),
            web_browser_type_submit(),
            web_browser_type(),
        ]
    # add navigational tools
    return tools + [
        web_browser_scroll(),
        web_browser_back(),
        web_browser_forward(),
        web_browser_refresh(),
    ]
@tool(parallel=False)
def web_browser_go() -> Tool:
    """Web Browser tool for navigation to a URL.
    Returns:
       Web browser navigation tool.
    """
    async def execute(url: str) -> ToolResult:
        """Navigate the web browser to a URL.
        Once you have navigated to a page, you will be presented with a web accessibility tree of the elements on the page. Each element has an ID, which is displayed in brackets at the beginning of its line. For example:
        ```
        [1] RootWebArea "Google" [focused: True, url: https://www.google.com/]
          [76] link "About" [url: https://about.google/]
          [85] link "Gmail " [url: https://mail.google.com/mail/&ogbl]
            [4] StaticText "Gmail"
          [91] button "Google apps" [expanded: False]
          [21] combobox "Search" [editable: plaintext, autocomplete: both, hasPopup: listbox, required: False, expanded: False, controls: Alh6id]
        ```
        To execute a Google Search for 'dogs', you would type into the "Search" combobox with element ID 21 and then press ENTER using the web_browser_type_submit tool:
        web_browser_type_submit(21, "dogs")
        You should only attempt to navigate the web browser one page at a time (parallel calls to web browser tools are not permitted).
        Args:
          url (str): URL to navigate to.
        Returns:
          Web accessibility tree of the visible elements of the web page. The element_id of each element is displayed in brackets at the beginning of the line.
        """
        return await web_browser_cmd("web_go", url)
    return execute
def go_without_interactive_docs(tool: Tool) -> Tool:
    tool_info = parse_tool_info(tool)
    description_lines = tool_info.description.splitlines()
    description_lines = [
        line for line in description_lines if "web_browser_type_submit" not in line
    ]
    return tool_with(tool, description="\n".join(description_lines))
# custom viewer for interactive tool calls that shows a truncated
# version of current the web accessibility tree if available
class WebBrowserStore(StoreModel):
    main_content: str = Field(default_factory=str)
    web_at: str = Field(default_factory=str)
    session_id: str = Field(default_factory=str)
def web_at_viewer(call: ToolCall) -> ToolCallView:
    # get the web accessibility tree, if we have it create a view from it
    web_at = store_as(WebBrowserStore).web_at
    element_id = call.arguments.get("element_id", 0)
    if web_at and element_id:
        lines = web_at.splitlines()
        pattern = re.compile(rf"^\s+\[{element_id}\] .*$")
        for i, line in enumerate(lines):
            if pattern.match(line):
                snippet = (
                    lines[0:1]
                    + ["  ..."]
                    + lines[max(i - 2, 1) : i]
                    + [line.replace(" ", "*", 1)]
                    + lines[i + 1 : min(i + 3, len(lines))]
                    + ["  ..."]
                )
                return ToolCallView(
                    context=ToolCallContent(format="text", content="\n".join(snippet))
                )
    # no view found
    return ToolCallView()
@tool(viewer=web_at_viewer, parallel=False)
def web_browser_click() -> Tool:
    """Web Browser tool for clicking an element on a web page.
    Returns:
       Web browser clicking tool.
    """
    async def execute(element_id: int) -> ToolResult:
        """Click an element on the page currently displayed by the web browser.
        For example, with the following web accessibility tree:
        ```
        [304] RootWebArea "Poetry Foundation" [focused: True, url: https://www.poetryfoundation.org/]
          [63] StaticText "POETRY FOUNDATION"
          [427] button "POEMS & POETS" [expanded: False]
          [434] button "FEATURES" [expanded: False]
        ```
        You could click on the "POEMS & POETS" button with:
        web_browser_click(427)
        Args:
           element_id (int): ID of the element to click.
        Returns:
           Web accessibility tree of the visible elements of the web page. The element_id of each element is displayed in brackets at the beginning of the line.
        """
        return await web_browser_cmd("web_click", str(element_id))
    return execute
@tool(viewer=web_at_viewer, parallel=False)
def web_browser_type_submit() -> Tool:
    """Web Browser tool for typing and submitting input.
    Returns:
       Web browser type and submit tool.
    """
    async def execute(element_id: int, text: str) -> ToolResult:
        """Type text into a form input on a web browser page and press ENTER to submit the form.
        For example, to execute a search for "Yeats" from this page:
        ```
        [2] RootWebArea "Moon - Wikipedia" [focused: True, url: https://en.wikipedia.org/wiki/Moon]
          [91] StaticText "Jump to content"
          [682] button "Main menu" [hasPopup: menu]
          [751] searchbox "Search Wikipedia" [editable: plaintext, keyshortcuts: Alt+f]
          [759] button "Search"
          [796] button "Personal tools" [hasPopup: menu]
        ```
        You would type into the searchbox and press ENTER using the following tool call:
        web_browser_type_submit(751, "Yeats")
        Args:
           element_id (int): ID of the element to type text into.
           text (str): Text to type.
        Returns:
           Web accessibility tree of the visible elements of the web page. The element_id of each element is displayed in brackets at the beginning of the line.
        """
        return await web_browser_cmd("web_type_submit", str(element_id), text)
    return execute
@tool(viewer=web_at_viewer, parallel=False)
def web_browser_type() -> Tool:
    """Web Browser tool for typing into inputs.
    Returns:
       Web browser typing tool.
    """
    async def execute(element_id: int, text: str) -> ToolResult:
        """Type text into an input on a web browser page.
        For example, to type "Norah" into the "First Name" search box on this page:
        ```
        [106] RootWebArea "My Profile" [focused: True, url: https://profile.example.com]
          [305] link "My library" [url: https://profile.example.com/library]
          [316] textbox "First Name" [focused: True, editable: plaintext, required: False]
          [316] textbox "Last Name" [focused: True, editable: plaintext, required: False]
        ```
        You would use the following command:
        web_browser_type(316, "Norah")
        Note that the web_browser_type_submit tool is typically much more useful than the web_browser_type tool since it enters input and submits the form. You would typically only need to use the web_browser_type tool to fill out forms with multiple inputs.
        Args:
           element_id (int): ID of the element to type text into.
           text (str): Text to type.
        Returns:
           Web accessibility tree of the visible elements of the web page. The element_id of each element is displayed in brackets at the beginning of the line.
        """
        return await web_browser_cmd("web_type", str(element_id), text)
    return execute
@tool(parallel=False)
def web_browser_scroll() -> Tool:
    """Web Browser tool for scrolling up or down one page.
    Returns:
       Web browser scrolling tool.
    """
    async def execute(direction: str) -> ToolResult:
        """Scroll the web browser up or down by one page.
        Occasionally some very long pages don't display all of their content at once. To see additional content you can scroll the page down with:
        web_browser_scroll("down")
        You can then return to the previous scroll position with:
        web_browser_scroll("up")
        Args:
           direction (str): "up" or "down"
        Returns:
           Web accessibility tree of the visible elements of the web page. The element_id of each element is displayed in brackets at the beginning of the line.
        """
        return await web_browser_cmd("web_scroll", direction)
    return execute
@tool(parallel=False)
def web_browser_back() -> Tool:
    """Web Browser tool for navigating back in the browser history.
    Returns:
       Web browser back navigation tool.
    """
    async def execute() -> ToolResult:
        """Navigate the web browser back in the browser history.
        If you want to view a page that you have previously browsed (or perhaps just didn't find what you were looking for on a page and want to backtrack) use the web_browser_back tool.
        Returns:
           Web accessibility tree of the visible elements of the web page. The element_id of each element is displayed in brackets at the beginning of the line.
        """
        return await web_browser_cmd("web_back")
    return execute
@tool(parallel=False)
def web_browser_forward() -> Tool:
    """Web Browser tool for navigating forward in the browser history.
    Returns:
       Web browser forward navigation tool.
    """
    async def execute() -> ToolResult:
        """Navigate the web browser forward in the browser history.
        If you have navigated back in the browser history and then want to navigate forward use the web_browser_forward tool.
        Returns:
           Web accessibility tree of the visible elements of the web page. The element_id of each element is displayed in brackets at the beginning of the line.
        """
        return await web_browser_cmd("web_forward")
    return execute
@tool(parallel=False)
def web_browser_refresh() -> Tool:
    """Web Browser tool for refreshing the current page.
    Returns:
       Web browser page refresh tool.
    """
    async def execute() -> ToolResult:
        """Refresh the current page of the web browser.
        If you have interacted with a page by clicking buttons and want to reset it to its original state, use the web_browser_refresh tool.
        Returns:
           Web accessibility tree of the visible elements of the web page. The element_id of each element is displayed in brackets at the beginning of the line.
        """
        return await web_browser_cmd("web_refresh")
    return execute
WEB_CLIENT_REQUEST = "/app/web_browser/web_client.py"
WEB_CLIENT_NEW_SESSION = "/app/web_browser/web_client_new_session.py"
async def web_browser_cmd(cmd: str, *args: str) -> ToolResult:
    sandbox_env = await sandbox_with(WEB_CLIENT_NEW_SESSION)
    session_flag = ""
    if sandbox_env:
        store = store_as(WebBrowserStore)
        if not store.session_id:
            result = await sandbox_env.exec(
                ["python3", WEB_CLIENT_NEW_SESSION], timeout=180
            )
            if not result.success:
                raise RuntimeError(
                    f"Error creating new web browser session: {result.stderr}"
                )
            store.session_id = result.stdout.strip("\n")
        session_flag = f"--session_name={store.session_id}"
    else:
        sandbox_env = await web_browser_sandbox()
    arg_list = None
    if session_flag:
        arg_list = ["python3", WEB_CLIENT_REQUEST, session_flag, cmd] + list(args)
    else:
        arg_list = ["python3", WEB_CLIENT_REQUEST, cmd] + list(args)
    result = await sandbox_env.exec(arg_list, timeout=180)
    if not result.success:
        raise RuntimeError(
            f"Error executing web browser command {cmd}({', '.join(args)}): {result.stderr}"
        )
    else:
        response = parse_web_browser_output(result.stdout)
        if "error" in response and response.get("error", "").strip() != "":
            raise ToolError(str(response.get("error")) or "(unknown error)")
        elif "web_at" in response:
            main_content = str(response.get("main_content")) or None
            web_at = (
                str(response.get("web_at")) or "(no web accessibility tree available)"
            )
            # Remove base64 data from images.
            web_at_lines = web_at.split("\n")
            web_at_lines = [
                line.partition("data:image/png;base64")[0] for line in web_at_lines
            ]
            store_as(WebBrowserStore).main_content = (
                main_content or "(no main text summary)"
            )
            store_as(WebBrowserStore).web_at = web_at
            web_at = "\n".join(web_at_lines)
            return (
                [
                    ContentText(text=f"main content:\n{main_content}\n\n"),
                    ContentText(text=f"accessibility tree:\n{web_at}"),
                ]
                if main_content
                else web_at
            )
        else:
            raise RuntimeError(
                f"web_browser output must contain either 'error' or 'web_at' field: {result.stdout}"
            )
async def web_browser_sandbox() -> SandboxEnvironment:
    sb = await sandbox_with(WEB_CLIENT_REQUEST)
    if sb:
        return sb
    else:
        msg = dedent(f"""
                The web browser service was not found in any of the sandboxes for this sample. Please add the web browser service to your configuration. For example, the following Docker compose file uses the {INSPECT_WEB_BROWSER_IMAGE_DOCKERHUB} image as its default sandbox:
                services:
                  default:
                    image: "{INSPECT_WEB_BROWSER_IMAGE_DOCKERHUB}"
                    init: true
                Alternatively, this Docker compose file creates a dedicated image for the web browser service:
                services:
                  default:
                    image: "python:3.12-bookworm"
                    init: true
                    command: "tail -f /dev/null"
                  web_browser:
                    image: "{INSPECT_WEB_BROWSER_IMAGE_DOCKERHUB}"
                    init: true
                """).strip()
        raise PrerequisiteError(msg)
def parse_web_browser_output(output: str) -> dict[str, str]:
    response: dict[str, str] = dict(
        web_url="", main_content="", web_at="", info="", error=""
    )
    active_field: str | None = None
    active_field_lines: list[str] = []
    def collect_active_field() -> None:
        if active_field is not None:
            response[active_field] = "\n".join(active_field_lines)
        active_field_lines.clear()
    for line in output.splitlines():
        field_match = re.match(
            r"^(error|main_content|web_at|web_url|info)\s*:\s*(.+)$", line
        )
        if field_match:
            collect_active_field()
            active_field = field_match.group(1)
            active_field_lines.append(field_match.group(2))
        else:
            active_field_lines.append(line)
    collect_active_field()
    return response

================
File: src/inspect_ai/tool/_tools/_web_search.py
================
import os
from typing import Literal, Protocol, runtime_checkable
import anyio
import httpx
from bs4 import BeautifulSoup, NavigableString
from tenacity import (
    retry,
    retry_if_exception,
    stop_after_attempt,
    stop_after_delay,
    wait_exponential_jitter,
)
from inspect_ai._util.error import PrerequisiteError
from inspect_ai._util.httpx import httpx_should_retry, log_httpx_retry_attempt
from inspect_ai.util._concurrency import concurrency
from .._tool import Tool, ToolResult, tool
DEFAULT_RELEVANCE_PROMPT = """I am trying to answer the following question and need to find the most relevant information on the web. Please let me know if the following content is relevant to the question or not. You should just respond with "yes" or "no".
Question: {question}
Page Content: {text}
"""
class SearchLink:
    def __init__(self, url: str, snippet: str) -> None:
        self.url = url
        self.snippet = snippet
@runtime_checkable
class SearchProvider(Protocol):
    async def __call__(self, query: str, start_idx: int) -> list[SearchLink]: ...
@tool
def web_search(
    provider: Literal["google"] = "google",
    num_results: int = 3,
    max_provider_calls: int = 3,
    max_connections: int = 10,
    model: str | None = None,
) -> Tool:
    """Web search tool.
    A tool that can be registered for use by models to search the web. Use
    the `use_tools()` solver to make the tool available (e.g. `use_tools(web_search())`))
    A web search is conducted using the specified provider, the results are parsed for relevance
    using the specified model, and the top 'num_results' relevant pages are returned.
    See further documentation at <https://inspect.ai-safety-institute.org.uk/tools.html#sec-web-search>.
    Args:
      provider: Search provider (defaults to "google", currently
        the only provider). Possible future providers include "brave" and "bing".
      num_results: Number of web search result pages to return to the model.
      max_provider_calls: Maximum number of search calls to make to the search provider.
      max_connections: Maximum number of concurrent connections to API
        endpoint of search provider.
      model: Model used to parse web pages for relevance.
    Returns:
       A tool that can be registered for use by models to search the web.
    """
    # get search client
    client = httpx.AsyncClient()
    if provider == "google":
        search_provider = google_search_provider(client)
    else:
        raise ValueError(
            f"Provider {provider} not supported. Only 'google' is supported."
        )
    # resolve provider (only google for now)
    async def execute(query: str) -> ToolResult:
        """
        Use the web_search tool to perform keyword searches of the web.
        Args:
            query (str): Search query.
        """
        # limit number of concurrent searches
        page_contents: list[str] = []
        urls: list[str] = []
        snippets: list[str] = []
        search_calls = 0
        # Paginate through search results until we have successfully extracted num_results pages or we have reached max_provider_calls
        while len(page_contents) < num_results and search_calls < max_provider_calls:
            async with concurrency(f"{provider}_web_search", max_connections):
                links = await search_provider(query, start_idx=search_calls * 10)
            async with anyio.create_task_group() as tg:
                async def process_link(link: SearchLink) -> None:
                    try:
                        page = await page_if_relevant(link.url, query, model, client)
                        if page:
                            page_contents.append(page)
                            urls.append(link.url)
                            snippets.append(link.snippet)
                    # exceptions fetching pages are very common!
                    except Exception:
                        pass
                for lk in links:
                    tg.start_soon(process_link, lk)
            search_calls += 1
        all_page_contents = "\n\n".join(page_contents)
        if all_page_contents == "":
            response: ToolResult = (
                "I'm sorry, I couldn't find any relevant information on the web."
            )
        else:
            response = (
                "Here are your web search results. Please read them carefully as they may be useful later! "
                + all_page_contents
            )
        return response
    return execute
async def page_if_relevant(
    link: str, query: str, relevance_model: str | None, client: httpx.AsyncClient
) -> str | None:
    """
    Use parser model to determine if a web page contents is relevant to a query.
    Args:
        link (str): Web page link.
        query (str): Search query.
        relevance_model (Model): Model used to parse web pages for relevance.
        client: (httpx.Client): HTTP client to use to fetch the page
    Returns:
        str: Web page contents if relevant, else None.
    """
    # resolve model
    from inspect_ai.model._model import get_model
    model = get_model(relevance_model)
    # retrieve document
    try:
        response = await client.get(link)
        response.raise_for_status()
    except httpx.HTTPError as exc:
        raise Exception(f"HTTP error occurred: {exc}")
    # parse it
    encoding_scheme = response.encoding or "utf-8"
    soup = BeautifulSoup(response.content.decode(encoding_scheme), "html.parser")
    main_content = soup.find("main") or soup.find("body") or soup
    if not isinstance(main_content, NavigableString):
        paragraphs = main_content.find_all("p")
        full_text = ""
        for p in paragraphs:
            full_text += p.get_text(strip=True, separator=" ")
            if len(full_text.split()) > 2000:
                break
    else:
        full_text = " ".join(
            main_content.get_text(strip=True, separator=" ").split()[:2000]
        )
    is_relevant = (
        await model.generate(
            DEFAULT_RELEVANCE_PROMPT.format(question=query, text=full_text)
        )
    ).message.text
    if "yes" in is_relevant.lower():
        return full_text
    else:
        return None
def google_search_provider(client: httpx.AsyncClient) -> SearchProvider:
    google_api_key = os.environ.get("GOOGLE_CSE_API_KEY", None)
    google_cse_id = os.environ.get("GOOGLE_CSE_ID", None)
    if not google_api_key or not google_cse_id:
        raise PrerequisiteError(
            "GOOGLE_CSE_ID and/or GOOGLE_CSE_API_KEY not set in the environment. Please ensure these variables are defined to use Google Custom Search with the web_search tool.\n\nLearn more about the Google web search provider at https://inspect.ai-safety-institute.org.uk/tools.html#google-provider"
        )
    async def search(query: str, start_idx: int) -> list[SearchLink]:
        # List of allowed parameters can be found https://developers.google.com/custom-search/v1/reference/rest/v1/cse/list
        search_params = {
            "q": query,
            "key": google_api_key,
            "cx": google_cse_id,
            "start": start_idx,
        }
        search_url = "https://www.googleapis.com/customsearch/v1?" + "&".join(
            [f"{key}={value}" for key, value in search_params.items()]
        )
        # retry up to 5 times over a period of up to 1 minute
        @retry(
            wait=wait_exponential_jitter(),
            stop=stop_after_attempt(5) | stop_after_delay(60),
            retry=retry_if_exception(httpx_should_retry),
            before_sleep=log_httpx_retry_attempt(search_url),
        )
        async def execute_search() -> httpx.Response:
            return await client.get(search_url)
        result = await execute_search()
        data = result.json()
        if "items" in data:
            return [SearchLink(item["link"], item["snippet"]) for item in data["items"]]
        else:
            return []
    return search

================
File: src/inspect_ai/tool/beta.py
================
from inspect_ai._util.deprecation import relocated_module_attribute
relocated_module_attribute("computer", "inspect_ai.tool.computer", "0.3.62", "0.4")

================
File: src/inspect_ai/util/__init__.py
================
from inspect_ai._util.trace import trace_action, trace_message
from ._concurrency import concurrency
from ._console import input_screen
from ._display import DisplayType, display_counter, display_type
from ._json import JSONSchema, JSONType, json_schema
from ._panel import InputPanel, input_panel
from ._resource import resource
from ._sandbox import (
    OutputLimitExceededError,
    SandboxConnection,
    SandboxEnvironment,
    SandboxEnvironmentConfigType,
    SandboxEnvironmentLimits,
    SandboxEnvironments,
    SandboxEnvironmentSpec,
    SandboxEnvironmentType,
    sandbox,
    sandbox_with,
    sandboxenv,
)
from ._store import Store, store
from ._store_model import StoreModel, store_as
from ._subprocess import (
    ExecResult,
    subprocess,
)
from ._subtask import Subtask, subtask
from ._throttle import throttle
__all__ = [
    "ExecResult",
    "concurrency",
    "DisplayType",
    "display_counter",
    "display_type",
    "InputPanel",
    "input_panel",
    "input_screen",
    "JSONType",
    "JSONSchema",
    "json_schema",
    "OutputLimitExceededError",
    "resource",
    "subprocess",
    "SandboxEnvironment",
    "SandboxEnvironmentConfigType",
    "SandboxEnvironmentLimits",
    "SandboxEnvironments",
    "SandboxEnvironmentSpec",
    "SandboxEnvironmentType",
    "SandboxConnection",
    "sandboxenv",
    "sandbox",
    "sandbox_with",
    "Store",
    "store",
    "StoreModel",
    "store_as",
    "Subtask",
    "subtask",
    "throttle",
    "trace_action",
    "trace_message",
]

================
File: src/inspect_ai/util/_concurrency.py
================
import contextlib
import time
from dataclasses import dataclass
from typing import AsyncIterator
import anyio
from inspect_ai._util.working import report_sample_waiting_time
@contextlib.asynccontextmanager
async def concurrency(
    name: str,
    concurrency: int,
    key: str | None = None,
) -> AsyncIterator[None]:
    """Concurrency context manager.
    A concurrency context can be used to limit the number of coroutines
    executing a block of code (e.g calling an API). For example, here
    we limit concurrent calls to an api ('api-name') to 10:
    ```python
    async with concurrency("api-name", 10):
        # call the api
    ```
    Note that concurrency for model API access is handled internally
    via the `max_connections` generation config option. Concurrency
    for launching subprocesses is handled via the `subprocess` function.
    Args:
      name: Name for concurrency context. This serves as the
         display name for the context, and also the unique context
         key (if the `key` parameter is omitted)
      concurrency: Maximum number of coroutines that can
         enter the context.
      key: Unique context key for this context. Optional.
         Used if the unique key isn't human readable -- e.g. includes
         api tokens or account ids so that the more readable `name`
         can be presented to users e.g in console UI>
    """
    # sort out key
    key = key if key else name
    # do we have an existing semaphore? if not create one and store it
    semaphore = _concurrency_semaphores.get(key, None)
    if semaphore is None:
        semaphore = ConcurencySempahore(name, concurrency, anyio.Semaphore(concurrency))
        _concurrency_semaphores[key] = semaphore
    # wait and yield to protected code
    start_wait = time.monotonic()
    async with semaphore.semaphore:
        report_sample_waiting_time(time.monotonic() - start_wait)
        yield
def concurrency_status() -> dict[str, tuple[int, int]]:
    status: dict[str, tuple[int, int]] = {}
    for c in _concurrency_semaphores.values():
        status[c.name] = (c.concurrency - c.semaphore.value, c.concurrency)
    return status
def init_concurrency() -> None:
    _concurrency_semaphores.clear()
@dataclass
class ConcurencySempahore:
    name: str
    concurrency: int
    semaphore: anyio.Semaphore
_concurrency_semaphores: dict[str, ConcurencySempahore] = {}

================
File: src/inspect_ai/util/_console.py
================
import os
if os.name == "posix":
    # This makes Rich console input better if imported, but
    # raises a ModuleNotFound error on Windows, so posix only
    import readline  # noqa: F401
from contextlib import contextmanager
from typing import Iterator
from rich.console import Console
@contextmanager
def input_screen(
    header: str | None = None,
    transient: bool | None = None,
    width: int | None = None,
) -> Iterator[Console]:
    """Input screen for receiving user input.
    Context manager that clears the task display and provides a
    screen for receiving console input.
    Args:
      header (str | None): Header line to print above console
        content (defaults to printing no header)
      transient (bool): Return to task progress display after
        the user completes input (defaults to `True` for normal
        sessions and `False` when trace mode is enabled).
      width (int): Input screen width in characters (defaults to
        full width)
    Returns:
       Console to use for input.
    """
    from inspect_ai._display.core.active import task_screen
    with task_screen().input_screen(
        header=header, transient=transient, width=width
    ) as console:
        yield console

================
File: src/inspect_ai/util/_conversation.py
================
from rich import print
from rich.console import RenderableType
from rich.text import Text
from inspect_ai._util.transcript import transcript_panel
def conversation_panel(
    title: str,
    *,
    subtitle: str | None = None,
    content: RenderableType | list[RenderableType] = [],
) -> None:
    """Trace content into a standard trace panel display.
    Typically you would call `display_type() == "conversation"` to confirm that
    we are in conversation mode before calling `conversation_panel()`.
    Args:
      title (str): Panel title.
      subtitle (str | None): Panel subtitle. Optional.
      content (RenderableType | list[RenderableType]): One or more Rich renderables.
    """
    print(
        transcript_panel(title, subtitle, content),
        Text(),
    )

================
File: src/inspect_ai/util/_display.py
================
import os
from logging import getLogger
from typing import Literal
from inspect_ai._util._async import configured_async_backend
from inspect_ai._util.constants import DEFAULT_DISPLAY
from inspect_ai._util.thread import is_main_thread
logger = getLogger(__name__)
DisplayType = Literal["full", "conversation", "rich", "plain", "none"]
"""Console display type."""
_display_type: DisplayType | None = None
def init_display_type(display: str | None = None) -> DisplayType:
    global _display_type
    display = (
        display or os.environ.get("INSPECT_DISPLAY", DEFAULT_DISPLAY).lower().strip()
    )
    # if trio is configured as the backend then throttle down to "rich"
    # (as textual uses asyncio directly so is not compatible with trio)
    if configured_async_backend() == "trio" and display == "full":
        display = "rich"
    # if we are on a background thread then throttle down to "plain"
    # ("full" requires textual which cannot run in a background thread
    # b/c it calls the Python signal function; "rich" assumes exclusive
    # display access which may not be the case for threads)
    if display in ["full", "rich"] and not is_main_thread():
        display = "plain"
    match display:
        case "full" | "conversation" | "rich" | "plain" | "none":
            _display_type = display
        case _:
            logger.warning(
                f"Unknown display type '{display}' (setting display to 'full')"
            )
            _display_type = "full"
    return _display_type
def display_type() -> DisplayType:
    """Get the current console display type.
    Returns:
       DisplayType: Display type.
    """
    global _display_type
    if _display_type:
        return _display_type
    else:
        return init_display_type()
def display_type_initialized() -> bool:
    global _display_type
    return _display_type is not None
def display_counter(caption: str, value: str) -> None:
    """Display a counter in the UI.
    Args:
        caption: The counter's caption e.g. "HTTP rate limits".
        value: The counter's value e.g. "42".
    """
    from inspect_ai._display.core.active import display
    display().display_counter(caption, value)

================
File: src/inspect_ai/util/_json.py
================
import types
import typing
from dataclasses import is_dataclass
from typing import (
    Any,
    Dict,
    List,
    Literal,
    Optional,
    Tuple,
    Type,
    Union,
    get_args,
    get_origin,
    get_type_hints,
    is_typeddict,
)
from pydantic import BaseModel, Field
JSONType = Literal["string", "integer", "number", "boolean", "array", "object", "null"]
"""Valid types within JSON schema."""
class JSONSchema(BaseModel):
    """JSON Schema for type."""
    type: JSONType | None = Field(default=None)
    """JSON type of tool parameter."""
    description: str | None = Field(default=None)
    """Parameter description."""
    default: Any = Field(default=None)
    """Default value for parameter."""
    enum: list[Any] | None = Field(default=None)
    """Valid values for enum parameters."""
    items: Optional["JSONSchema"] = Field(default=None)
    """Valid type for array parameters."""
    properties: dict[str, "JSONSchema"] | None = Field(default=None)
    """Valid fields for object parametrs."""
    additionalProperties: Optional["JSONSchema"] | bool | None = Field(default=None)
    """Are additional properties allowed?"""
    anyOf: list["JSONSchema"] | None = Field(default=None)
    """Valid types for union parameters."""
    required: list[str] | None = Field(default=None)
    """Required fields for object parameters."""
def json_schema(t: Type[Any]) -> JSONSchema:
    """Provide a JSON Schema for the specified type.
    Schemas can be automatically inferred for a wide variety of
    Python class types including Pydantic BaseModel, dataclasses,
    and typed dicts.
    Args:
        t: Python type
    Returns:
        JSON Schema for type.
    """
    origin = get_origin(t)
    args = get_args(t)
    if origin is None:
        if t is int:
            return JSONSchema(type="integer")
        elif t is float:
            return JSONSchema(type="number")
        elif t is str:
            return JSONSchema(type="string")
        elif t is bool:
            return JSONSchema(type="boolean")
        elif t is list:
            return JSONSchema(type="array", items=JSONSchema())
        elif t is dict:
            return JSONSchema(type="object", additionalProperties=JSONSchema())
        elif (
            is_dataclass(t)
            or is_typeddict(t)
            or (isinstance(t, type) and issubclass(t, BaseModel))
        ):
            return cls_json_schema(t)
        elif t is type(None):
            return JSONSchema(type="null")
        else:
            return JSONSchema()
    elif origin is list or origin is List or origin is tuple or origin is Tuple:
        return JSONSchema(
            type="array", items=json_schema(args[0]) if args else JSONSchema()
        )
    elif origin is dict or origin is Dict:
        return JSONSchema(
            type="object",
            additionalProperties=json_schema(args[1])
            if len(args) > 1
            else JSONSchema(),
        )
    elif origin is Union or origin is types.UnionType:
        return JSONSchema(anyOf=[json_schema(arg) for arg in args])
    elif origin is Optional:
        return JSONSchema(
            anyOf=[json_schema(arg) for arg in args] + [JSONSchema(type="null")]
        )
    elif origin is typing.Literal:
        return JSONSchema(enum=list(args))
    return JSONSchema()  # Default case if we can't determine the type
def cls_json_schema(cls: Type[Any]) -> JSONSchema:
    properties: Dict[str, JSONSchema] = {}
    required: List[str] = []
    if is_dataclass(cls):
        fields = cls.__dataclass_fields__  # type: ignore
        for name, field in fields.items():
            properties[name] = json_schema(field.type)  # type: ignore
            if field.default == field.default_factory:
                required.append(name)
    elif isinstance(cls, type) and issubclass(cls, BaseModel):
        schema = cls.model_json_schema()
        for name, prop in schema.get("properties", {}).items():
            properties[name] = JSONSchema(**prop)
        required = schema.get("required", [])
    elif is_typeddict(cls):
        annotations = get_type_hints(cls)
        for name, type_hint in annotations.items():
            properties[name] = json_schema(type_hint)
            if name in cls.__required_keys__:
                required.append(name)
    return JSONSchema(
        type="object",
        properties=properties,
        required=required if required else None,
        additionalProperties=False,
    )
def python_type_to_json_type(python_type: str | None) -> JSONType:
    match python_type:
        case "str":
            return "string"
        case "int":
            return "integer"
        case "float":
            return "number"
        case "bool":
            return "boolean"
        case "list":
            return "array"
        case "dict":
            return "object"
        case "None":
            return "null"
        # treat 'unknown' as string as anything can be converted to string
        case None:
            return "string"
        case _:
            raise ValueError(
                f"Unsupported type: {python_type} for Python to JSON conversion."
            )

================
File: src/inspect_ai/util/_panel.py
================
from typing import Any, Protocol, TypeVar
from textual.containers import Container
from typing_extensions import Self
class InputPanel(Container):
    """Base class for for Inspect input panels."""
    DEFAULT_TITLE = "Panel"
    DEFAULT_CLASSES = "task-input-panel"
    DEFAULT_CSS = """
    InputPanel {
        padding: 0 1 1 1;
    }
    """
    class Host(Protocol):
        def set_title(self, title: str) -> None: ...
        def activate(self) -> None: ...
        def deactivate(self) -> None: ...
        def close(self) -> None: ...
    def __init__(self, host: Host) -> None:
        """Initialise the panel.
        Panels are created as required by the input_panel() function so
        you should NOT override __init__ with your own initisation (rather,
        you should define reactive props and/or methods that perform
        initialisation).
        You should also override the `DEFAULT_TITLE` variable for your panel to
        provide a default tab title (you can change the table dynamically as
        required using the `title` property).
        Args:
           host (InputPanel.Host): Interface to UI host of input panel.
        """
        super().__init__()
        self._title = self.DEFAULT_TITLE
        self._host = host
    async def __aenter__(self) -> Self:
        self.activate()
        return self
    async def __aexit__(
        self,
        *execinfo: Any,
    ) -> None:
        self.close()
    @property
    def title(self) -> str:
        return self._title
    @title.setter
    def title(self, title: str) -> None:
        self._title = title
        self._host.set_title(title)
    def activate(self) -> None:
        self._host.activate()
    def deactivate(self) -> None:
        self._host.deactivate()
    def close(self) -> None:
        self._host.close()
    def update(self) -> None:
        """Update method (called periodically e.g. once every second)"""
        pass
TP = TypeVar("TP", bound=InputPanel, covariant=True)
async def input_panel(panel: type[TP]) -> TP:
    """Create an input panel in the task display.
    There can only be a single instance of an InputPanel with a given
    'title' running at once. Therefore, if the panel doesn't exist it
    is created, otherwise a reference to the existing panel is returned.
    Examples:
        Create/activate an input panel (the panel will remain after
        the scope exits -- see below for open/close semantics)
        ```python
        panel = await input_panel(CustomPanel)
        panel.activate()
        ```
        Activate and close an input panel using a context manager:
        ```python
        async with await input_panel(CustomPanel) as panel:
            ...
        ```
    Args:
       panel (type[TP]): Type of panel widget (must derive from `InputPanel`)
    Returns:
       InputPanel: Instance of widget running in the task display.
    Raises:
       NotImplementedError: If Inspect is not running in display='full' model.
    """
    from inspect_ai._display.core.active import task_screen
    return await task_screen().input_panel(panel)

================
File: src/inspect_ai/util/_resource.py
================
import errno
from typing import Any, Literal
from urllib.parse import urlparse
from urllib.request import url2pathname
from inspect_ai._util.file import file, filesystem
def resource(
    resource: str,
    type: Literal["auto", "file"] = "auto",
    fs_options: dict[str, Any] = {},
) -> str:
    """Read and resolve a resource to a string.
    Resources are often used for templates, configuration, etc.
    They are sometimes hard-coded strings, and sometimes paths
    to external resources (e.g. in the local filesystem or
    remote stores e.g. s3:// or https://).
    The `resource()` function will resolve its argument to
    a resource string. If a protocol-prefixed file name
    (e.g. s3://) or the path to a local file that exists
    is passed then it will be read and its contents returned.
    Otherwise, it will return the passed `str` directly
    This function is mostly intended as a helper for other
    functions that take either a string or a resource path
    as an argument, and want to easily resolve them to
    the underlying content.
    If you want to ensure that only local or remote files
    are consumed, specify `type="file"`. For example:
    `resource("templates/prompt.txt", type="file")`
    Args:
        resource: Path to local or remote (e.g. s3://)
            resource, or for `type="auto"` (the default),
            a string containing the literal resource value.
        type: For "auto" (the default),
            interpret the resource as a literal string if its not
            a valid path. For "file", always interpret it as
            a file path.
        fs_options: Optional. Additional
            arguments to pass through to the `fsspec` filesystem
            provider (e.g. `S3FileSystem`). Use `{"anon": True }`
            if you are accessing a public S3 bucket with no
            credentials.
    Returns:
       Text content of resource.
    """
    # helper function to read the resource as a file
    def read_resource() -> str:
        with file(resource, "r", fs_options=fs_options) as f:
            return f.read()
    if type == "file":
        return read_resource()
    else:
        # parse the url
        try:
            parsed = urlparse(resource)
        except (ValueError, OSError):
            return resource
        # if it has a scheme then its likely a file
        if parsed.scheme:
            try:
                return read_resource()
            except (ValueError, FileNotFoundError):
                return resource
            except OSError as ex:
                if ex.errno == errno.ENAMETOOLONG:
                    return resource
                else:
                    raise ex
        # no scheme means either a local file or a string
        else:
            # extract the path
            try:
                path = url2pathname(parsed.path)
            except (ValueError, OSError):
                return resource
            # check if there is a path (otherwise return the str)
            if len(path) == 0:
                return resource
            # return it if it exists (otherwise return the str)
            try:
                fs = filesystem(path)
                if fs.exists(path):
                    return read_resource()
                else:
                    return resource
            except ValueError:
                return resource

================
File: src/inspect_ai/util/_sandbox/__init__.py
================
# note: unused imports are still required to ensure that our built-in sandbox environments are registered
from .context import sandbox, sandbox_with
from .docker.docker import DockerSandboxEnvironment  # noqa: F401
from .environment import (
    SandboxConnection,
    SandboxEnvironment,
    SandboxEnvironmentConfigType,
    SandboxEnvironments,
    SandboxEnvironmentSpec,
    SandboxEnvironmentType,
)
from .limits import OutputLimitExceededError, SandboxEnvironmentLimits
from .local import LocalSandboxEnvironment  # noqa: F401
from .registry import sandboxenv
__all__ = [
    "OutputLimitExceededError",
    "SandboxEnvironment",
    "SandboxEnvironmentConfigType",
    "SandboxEnvironmentLimits",
    "SandboxEnvironments",
    "SandboxEnvironmentSpec",
    "SandboxEnvironmentType",
    "SandboxConnection",
    "sandboxenv",
    "sandbox",
    "sandbox_with",
]

================
File: src/inspect_ai/util/_sandbox/context.py
================
from contextvars import ContextVar
from logging import getLogger
from typing import Any, NoReturn, cast
from shortuuid import uuid
from inspect_ai._util.constants import SANDBOX_SETUP_TIMEOUT
from inspect_ai.util._sandbox.events import SandboxEnvironmentProxy
from .environment import (
    SampleCleanup,
    SampleInit,
    SandboxConnection,
    SandboxEnvironment,
    SandboxEnvironmentConfigType,
)
from .registry import registry_find_sandboxenv
logger = getLogger(__name__)
def sandbox(name: str | None = None) -> SandboxEnvironment:
    """Get the SandboxEnvironment for the current sample.
    Args:
      name (str | None): Optional sandbox environmnent name.
    Return:
      SandboxEnvironment instance.
    Raises:
      ProcessLookupError: If there are no sandboxes available.
      ValueError: If an invalid sandbox name is specified.
    """
    # verify we have a context
    environments = sandbox_environments_context_var.get(None)
    if not environments:
        raise raise_no_sandbox()
    # For None, 'default', or a single environment only take the first environment
    if name is None or name == "default" or len(environments) == 1:
        return list(environments.values())[0]
    else:
        environment = environments.get(name, None)
        if not environment:
            raise ValueError(
                f"SandboxEnvironment '{name}' is not a recoginized environment name."
            )
        return environment
async def sandbox_with(file: str) -> SandboxEnvironment | None:
    """Get the SandboxEnvironment for the current sample that has the specified file.
    Args:
      file (str): Path to file to check for.
    Return:
      SandboxEnvironment instance or None if no sandboxes had the file.
    """
    # get environments and with mapping
    environments = sandbox_environments_context_var.get(None)
    if environments is None:
        raise_no_sandbox()
    environments_with = sandbox_with_environments_context_var.get(None)
    if environments_with is None:
        raise_no_sandbox()
    # if we've already disovered the sandbox for this file then return it
    environment = environments_with.get(file, None)
    if environment is not None:
        return environment
    # look in each sandbox
    for _, environment in environments.items():
        try:
            # can we read the file?
            await environment.read_file(file)
            # if so this is our environment, cache and return it
            environments_with[file] = environment
            return environment
        # allow exception types known to be raised from read_file
        except (
            FileNotFoundError,
            UnicodeDecodeError,
            PermissionError,
            IsADirectoryError,
        ):
            pass
    # not found
    return None
async def sandbox_connections() -> dict[str, SandboxConnection]:
    environments = sandbox_environments_context_var.get(None)
    if environments:
        connections: dict[str, SandboxConnection] = {}
        for name, environment in environments.items():
            try:
                connections[name] = await environment.connection()
            except (NotImplementedError, ConnectionError):
                pass
        return connections
    else:
        return {}
def raise_no_sandbox() -> NoReturn:
    raise ProcessLookupError(
        "No sandbox environment has been provided for the current sample or task. "
        + "Please specify a sandbox for the sample or a global default sandbox for the task"
    )
async def init_sandbox_environments_sample(
    sandboxenv_type: type[SandboxEnvironment],
    task_name: str,
    config: SandboxEnvironmentConfigType | None,
    files: dict[str, bytes],
    setup: bytes | None,
    metadata: dict[str, Any],
) -> dict[str, SandboxEnvironment]:
    # get setup and cleanup functions
    sample_init = cast(SampleInit, getattr(sandboxenv_type, "sample_init"))
    sample_cleanup = cast(SampleCleanup, getattr(sandboxenv_type, "sample_cleanup"))
    # create environments
    environments = await sample_init(task_name, config, metadata)
    # verify that there is at least one environment and a 'default' env
    validate_sandbox_environments(sandboxenv_type, environments)
    # proxy environments (for recording SandboxEvent)
    environments = {k: SandboxEnvironmentProxy(v) for k, v in environments.items()}
    try:
        # copy files into environments
        await copy_sandbox_environment_files(files, environments)
        # run setup script
        if setup:
            await setup_sandbox_environment(setup, environments)
        # set context
        sandbox_environments_context_var.set(environments)
        sandbox_with_environments_context_var.set({})
        # return environments
        return environments
    except Exception as ex:
        environments = unproxy_environments(environments)
        await sample_cleanup(task_name, config, environments, True)
        raise ex
async def cleanup_sandbox_environments_sample(
    type: str,
    task_name: str,
    config: SandboxEnvironmentConfigType | None,
    environments: dict[str, SandboxEnvironment],
    interrupted: bool,
) -> None:
    sandboxenv_type = registry_find_sandboxenv(type)
    sample_cleanup = cast(SampleCleanup, getattr(sandboxenv_type, "sample_cleanup"))
    environments = unproxy_environments(environments)
    await sample_cleanup(task_name, config, environments, interrupted)
def unproxy_environments(
    environments: dict[str, SandboxEnvironment],
) -> dict[str, SandboxEnvironment]:
    return {
        k: v._sandbox
        for k, v in cast(dict[str, SandboxEnvironmentProxy], environments).items()
    }
async def copy_sandbox_environment_files(
    files: dict[str, bytes], environments: dict[str, SandboxEnvironment]
) -> None:
    default_environment = default_sandbox_environment(environments)
    for file, contents in files.items():
        # does it have an environment prefix? if so target that env
        parts = file.split(":", maxsplit=1)
        if len(parts) > 1:
            envname = parts[0]
            file = parts[1]
            target_env = environments.get(envname, None)
            if not target_env:
                raise RuntimeError(
                    f"Environment referenced in sample file not found: '{envname}:{file}'. "
                    + "Note that ':' can be optionally used to specify an explicit environment name for sample files (e.g. 'envname:file') so cannot be used as a character within filenames."
                )
        else:
            target_env = default_environment
        await target_env.write_file(file, contents)
async def setup_sandbox_environment(
    setup: bytes, environments: dict[str, SandboxEnvironment]
) -> None:
    # get default sandboxenv
    env = default_sandbox_environment(environments)
    # copy to container
    setup_file = f"/tmp/{uuid()}"
    await env.write_file(setup_file, setup)
    # execute and then remove setup script (don't retry it on timeout
    # in case it is not idempotent)
    try:
        await env.exec(["chmod", "+x", setup_file], timeout=30)
        result = await env.exec(
            ["env", setup_file], timeout=SANDBOX_SETUP_TIMEOUT, timeout_retry=False
        )
        if not result.success:
            raise RuntimeError(
                f"Failed to execute setup script for sample: {result.stderr}"
            )
        await env.exec(["rm", setup_file], timeout=30)
    except TimeoutError:
        raise RuntimeError("Timed out executing setup command in sandbox")
def default_sandbox_environment(
    environments: dict[str, SandboxEnvironment],
) -> SandboxEnvironment:
    return list(environments.values())[0]
def validate_sandbox_environments(
    type: type[SandboxEnvironment], environments: dict[str, SandboxEnvironment]
) -> None:
    if len(environments) == 0:
        raise ValueError(
            "No environments returned from sample_init() method "
            + f"of '{type.__name__}'. Did you provide an implementation "
            + "of the sample_init() class method? "
        )
sandbox_environments_context_var = ContextVar[dict[str, SandboxEnvironment]](
    "sandbox_environments"
)
sandbox_with_environments_context_var = ContextVar[dict[str, SandboxEnvironment]](
    "sandbox_with_environments"
)

================
File: src/inspect_ai/util/_sandbox/docker/cleanup.py
================
from contextvars import ContextVar
from pathlib import Path
from typing import Awaitable, Callable, Set
import anyio
from rich import box, print
from rich.panel import Panel
from rich.table import Table
from inspect_ai._util._async import coro_print_exceptions
from .compose import compose_down, compose_ls, compose_ps
from .config import is_auto_compose_file, safe_cleanup_auto_compose
from .util import ComposeProject
def project_cleanup_startup() -> None:
    _running_projects.set([])
    _auto_compose_files.set(set())
    _cleanup_completed.set(False)
def project_startup(project: ComposeProject) -> None:
    # track running projects
    running_projects().append(project)
    # track auto compose we need to cleanup
    if project.config and is_auto_compose_file(project.config):
        auto_compose_files().add(project.config)
async def project_cleanup(project: ComposeProject, quiet: bool = True) -> None:
    # bring down services
    await compose_down(project=project, quiet=quiet)
    # remove the project from the list of running projects
    if project in running_projects():
        running_projects().remove(project)
async def project_cleanup_shutdown(cleanup: bool) -> None:
    # cleanup is global so we do it only once
    if not _cleanup_completed.get():
        # get projects that still need shutting down
        shutdown_projects = running_projects().copy()
        # full cleanup if requested
        if len(shutdown_projects) > 0:
            if cleanup:
                await cleanup_projects(shutdown_projects)
            elif not _cleanup_completed.get():
                print("")
                table = Table(
                    title="Docker Sandbox Environments (not yet cleaned up):",
                    box=box.SQUARE_DOUBLE_HEAD,
                    show_lines=True,
                    title_style="bold",
                    title_justify="left",
                )
                table.add_column("Sample ID")
                table.add_column("Epoch")
                table.add_column("Container(s)", no_wrap=True)
                for project in shutdown_projects:
                    containers = await compose_ps(project, all=True)
                    table.add_row(
                        str(project.sample_id) if project.sample_id is not None else "",
                        str(project.epoch if project.epoch is not None else ""),
                        "\n".join(container["Name"] for container in containers),
                    )
                print(table)
                print(
                    "\n"
                    "Cleanup all containers  : [blue]inspect sandbox cleanup docker[/blue]\n"
                    "Cleanup single container: [blue]inspect sandbox cleanup docker <container-id>[/blue]",
                    "\n",
                )
        # remove auto-compose files
        for file in auto_compose_files().copy():
            safe_cleanup_auto_compose(file)
        _cleanup_completed.set(True)
async def cleanup_projects(
    projects: list[ComposeProject],
    cleanup_fn: Callable[[ComposeProject, bool], Awaitable[None]] = project_cleanup,
) -> None:
    # urge the user to let this operation complete
    print(
        Panel(
            "[bold][blue]Cleaning up Docker environments "
            + "(please do not interrupt this operation!):[/blue][/bold]",
        )
    )
    # cleanup all of the projects in parallel
    async with anyio.create_task_group() as tg:
        for project in projects:
            tg.start_soon(
                coro_print_exceptions,
                "cleaning up Docker environment",
                cleanup_fn,
                project,
                False,
            )
async def cli_cleanup(project_name: str | None) -> None:
    # enumerate all inspect projects
    projects = await compose_ls()
    # filter by project name
    if project_name:
        projects = list(filter(lambda p: p.Name == project_name, projects))
    # if the config files are missing then blank them out so we get auto-compose
    for project in projects:
        if project.ConfigFiles and not Path(project.ConfigFiles).exists():
            project.ConfigFiles = None
    # clean them up
    if len(projects) > 0:
        # create compose projects
        compose_projects = [
            await ComposeProject.create(name=project.Name, config=project.ConfigFiles)
            for project in projects
        ]
        # do the cleanup
        await cleanup_projects(compose_projects, cleanup_fn=compose_down)
        # remove auto compose files
        for compose_project in compose_projects:
            safe_cleanup_auto_compose(compose_project.config)
def running_projects() -> list[ComposeProject]:
    return _running_projects.get()
def auto_compose_files() -> Set[str]:
    return _auto_compose_files.get()
_running_projects: ContextVar[list[ComposeProject]] = ContextVar(
    "docker_running_projects", default=[]
)
_auto_compose_files: ContextVar[Set[str]] = ContextVar("docker_auto_compose_files")
_cleanup_completed: ContextVar[bool] = ContextVar(
    "docker_cleanup_executed", default=False
)

================
File: src/inspect_ai/util/_sandbox/docker/compose.py
================
import json
import os
import shlex
from logging import getLogger
from pathlib import Path
from typing import Any, Literal, cast
import yaml
from pydantic import BaseModel
from inspect_ai._util.error import PrerequisiteError
from inspect_ai._util.trace import trace_message
from inspect_ai.util._display import display_type
from inspect_ai.util._subprocess import ExecResult, subprocess
from .prereqs import (
    DOCKER_COMPOSE_REQUIRED_VERSION_PULL_POLICY,
    validate_docker_compose,
)
from .service import ComposeService, services_healthcheck_time
from .util import ComposeProject, is_inspect_project
logger = getLogger(__name__)
# How long to wait for compose environment to pass a health check
COMPOSE_WAIT = 120
async def compose_up(
    project: ComposeProject, services: dict[str, ComposeService]
) -> ExecResult[str]:
    # compute the maximum amount of time we will
    up_command = ["up", "--detach", "--wait"]
    # are there healthchecks in the service definitions? if so then peg our timeout
    # at the maximum total wait time. otherwise, pick a reasonable default
    healthcheck_time = services_healthcheck_time(services)
    if healthcheck_time > 0:
        timeout: int = healthcheck_time
        trace_message(logger, "Docker", "Docker services heathcheck timeout: {timeout}")
    else:
        timeout = COMPOSE_WAIT
    # align global wait timeout to maximum healthcheck timeout
    up_command.extend(["--wait-timeout", str(timeout + 1)])
    # Start the environment. Note that we don't check the result because docker will
    # return a non-zero exit code for services that exit (even successfully) when
    # passing the --wait flag (see https://github.com/docker/compose/issues/10596).
    # In practice, we will catch any errors when calling compose_check_running()
    # immediately after we call compose_up().
    result = await compose_command(up_command, project=project, timeout=timeout)
    return result
async def compose_down(project: ComposeProject, quiet: bool = True) -> None:
    # set cwd to config file directory
    cwd = os.path.dirname(project.config) if project.config else None
    # shut down docker containers. default internal timeout is 10 seconds
    # but we've seen reports of this handing, so add a proess timeout
    # of 60 seconds for belt and suspenders
    TIMEOUT = 60
    try:
        result = await compose_command(
            ["down", "--volumes"],
            project=project,
            cwd=cwd,
            timeout=TIMEOUT,
            capture_output=quiet,
            ansi="never",
        )
        if not result.success:
            msg = f"Failed to stop docker service {result.stderr}"
            logger.warning(msg)
    except TimeoutError:
        logger.warning(
            f"Docker compose down for project '{project.name}' timed out after {TIMEOUT} seconds."
        )
    try:
        await compose_cleanup_images(project=project, cwd=cwd, timeout=TIMEOUT)
    except TimeoutError:
        logger.warning(
            f"Docker image cleanup for project '{project.name}' timed out after {TIMEOUT} seconds."
        )
async def compose_cp(
    src: str,
    dest: str,
    project: ComposeProject,
    cwd: str | Path | None = None,
    output_limit: int | None = None,
) -> None:
    result = await compose_command(
        ["cp", "--", src, dest],
        project=project,
        timeout=120,  # 2-minute timeout for file copies
        cwd=cwd,
        output_limit=output_limit,
    )
    if not result.success:
        msg = f"Failed to copy file from '{src}' to '{dest}': {result.stderr}"
        raise RuntimeError(msg)
async def compose_check_running(
    services: list[str], project: ComposeProject
) -> list[str]:
    # Check to ensure that the status of containers is healthy
    running_services = await compose_ps(project=project, status="running")
    exited_services = await compose_ps(project=project, status="exited")
    successful_services = running_services + [
        service for service in exited_services if service["ExitCode"] == 0
    ]
    if len(successful_services) > 0:
        if len(successful_services) != len(services):
            unhealthy_services = services
            for successful_service in successful_services:
                unhealthy_services.remove(successful_service["Service"])
            return []
    else:
        return []
    return [service["Service"] for service in running_services]
async def compose_ps(
    project: ComposeProject,
    status: Literal[
        "paused", "restarting", "removing", "running", "dead", "created", "exited"
    ]
    | None = None,
    all: bool = False,
) -> list[dict[str, Any]]:
    command = ["ps", "--format", "json"]
    if all:
        command.append("--all")
    if status:
        command = command + ["--status", status]
    result = await compose_command(command, project=project, timeout=60)
    if not result.success:
        msg = f"Error querying for running services: {result.stderr}"
        raise RuntimeError(msg)
    output = result.stdout.strip()
    if len(output) > 0:
        return [
            cast(dict[str, Any], json.loads(service)) for service in output.split("\n")
        ]
    else:
        return []
async def compose_build(project: ComposeProject, capture_output: bool = False) -> None:
    result = await compose_command(
        ["build"],
        project=project,
        timeout=None,  # no timeout for build
        capture_output=capture_output,
    )
    if not result.success:
        msg = "Failed to build docker containers"
        raise PrerequisiteError(msg)
async def compose_pull(
    service: str, project: ComposeProject, capture_output: bool = False
) -> ExecResult[str]:
    await validate_docker_compose(DOCKER_COMPOSE_REQUIRED_VERSION_PULL_POLICY)
    return await compose_command(
        ["pull", "--ignore-buildable", "--policy", "missing", service],
        project=project,
        timeout=None,  # no timeout for pull
        capture_output=capture_output,
    )
async def compose_exec(
    command: list[str],
    *,
    project: ComposeProject,
    timeout: int | None,
    timeout_retry: bool = True,
    input: str | bytes | None = None,
    output_limit: int | None = None,
) -> ExecResult[str]:
    return await compose_command(
        ["exec"] + command,
        project=project,
        timeout=timeout,
        timeout_retry=timeout_retry,
        input=input,
        forward_env=False,
        output_limit=output_limit,
    )
async def compose_services(project: ComposeProject) -> dict[str, ComposeService]:
    result = await compose_command(["config"], project=project, timeout=60)
    if not result.success:
        raise RuntimeError(f"Error reading docker config: {result.stderr}")
    return cast(dict[str, ComposeService], yaml.safe_load(result.stdout)["services"])
class Project(BaseModel):
    Name: str
    Status: str
    ConfigFiles: str | None
async def compose_ls() -> list[Project]:
    result = await subprocess(["docker", "compose", "ls", "--all", "--format", "json"])
    if result.success:
        projects: list[dict[str, Any]] = json.loads(result.stdout)
        projects = list(filter(lambda p: is_inspect_project(p["Name"]), projects))
        return [Project(**project) for project in projects]
    else:
        raise RuntimeError(result.stderr)
async def compose_cleanup_images(
    project: ComposeProject,
    *,
    cwd: str | None = None,
    timeout: int | None,
) -> None:
    # List the images that would be created for this compose
    images_result = await compose_command(
        ["config", "--images"], project=project, timeout=timeout, cwd=cwd
    )
    # Remove those images explicitly
    if images_result.success:
        for image in images_result.stdout.strip().split("\n"):
            # See if this image was created by
            # inspect directly
            if image.startswith(project.name):
                # see if this image is present
                image_result = await subprocess(
                    ["docker", "images", "-q", image],
                    timeout=timeout,
                    capture_output=True,
                )
                remove_image = True
                if image_result.success:
                    remove_image = len(image_result.stdout) != 0
                # remove the image
                if remove_image:
                    result = await subprocess(
                        ["docker", "rmi", image],
                        timeout=timeout,
                        capture_output=True,
                    )
                    if not result.success:
                        msg = f"Failed to cleanup docker image {result.stderr}"
                        logger.warning(msg)
async def compose_command(
    command: list[str],
    *,
    project: ComposeProject,
    timeout: int | None,
    timeout_retry: bool = True,
    input: str | bytes | None = None,
    cwd: str | Path | None = None,
    forward_env: bool = True,
    capture_output: bool = True,
    output_limit: int | None = None,
    ansi: Literal["never", "always", "auto"] | None = None,
) -> ExecResult[str]:
    # The base docker compose command
    compose_command = ["docker", "compose"]
    # env to forward
    env = project.env if (project.env and forward_env) else {}
    # ansi (apply global override)
    if display_type() == "plain":
        ansi = "never"
    if ansi:
        compose_command = compose_command + ["--ansi", ansi]
    # quiet if display is none
    if display_type() == "none":
        compose_command = compose_command + ["--progress", "quiet"]
    # add project scope
    compose_command = compose_command + ["--project-name", project.name]
    # add config file if specified
    if project.config:
        compose_command = compose_command + ["-f", project.config]
    # build final command
    compose_command = compose_command + command
    # function to run command
    async def run_command(command_timeout: int | None) -> ExecResult[str]:
        result = await subprocess(
            compose_command,
            input=input,
            cwd=cwd,
            env=env,
            timeout=command_timeout,
            capture_output=capture_output,
            output_limit=output_limit,
        )
        return result
    # we have observed underlying unreliability in docker compose in some linux
    # environments on EC2 -- this exhibits in very simple commands (e.g. compose config)
    # simply never returning. this tends to happen when we know there is a large
    # number of commands in flight (task/sample init) so could be some sort of
    # timing issue / race condition in the docker daemon. we've also observed that
    # these same commands succeed if you just retry them. therefore, we add some
    # extra resiliance by retrying commands with a timeout once. we were observing
    # commands hanging at a rate of ~ 1/1000, so we retry up to twice (tweaking the
    # retry time down) to make the odds of hanging vanishingly small
    if timeout is not None:
        MAX_RETRIES = 2
        retries = 0
        while True:
            try:
                command_timeout = (
                    timeout if retries == 0 else (min(timeout, 60) // retries)
                )
                return await run_command(command_timeout)
            except TimeoutError:
                retries += 1
                if timeout_retry and (retries <= MAX_RETRIES):
                    logger.info(
                        f"Retrying docker compose command: {shlex.join(compose_command)}"
                    )
                else:
                    raise
    else:
        return await run_command(timeout)

================
File: src/inspect_ai/util/_sandbox/docker/config.py
================
import os
from logging import getLogger
from pathlib import Path
logger = getLogger(__name__)
CONFIG_FILES = [
    "compose.yaml",
    "compose.yml",
    "docker-compose.yaml",
    "docker-compose.yml",
]
DOCKERFILE = "Dockerfile"
def resolve_compose_file(parent: str = "") -> str:
    # existing compose file provides all the config we need
    compose = find_compose_file(parent)
    if compose is not None:
        return Path(os.path.join(parent, compose)).resolve().as_posix()
    # temporary auto-compose
    if has_auto_compose_file(parent):
        return Path(os.path.join(parent, AUTO_COMPOSE_YAML)).resolve().as_posix()
    # dockerfile just needs a compose.yaml synthesized
    elif has_dockerfile(parent):
        return auto_compose_file(
            COMPOSE_DOCKERFILE_YAML.format(dockerfile=DOCKERFILE), parent
        )
    # otherwise provide a generic python container
    else:
        return auto_compose_file(COMPOSE_GENERIC_YAML, parent)
def find_compose_file(parent: str = "") -> str | None:
    for file in CONFIG_FILES:
        if os.path.isfile(os.path.join(parent, file)):
            return file
    return None
def is_dockerfile(file: str) -> bool:
    path = Path(file)
    return path.stem == DOCKERFILE or path.suffix == f".{DOCKERFILE}"
def has_dockerfile(parent: str = "") -> bool:
    return os.path.isfile(os.path.join(parent, DOCKERFILE))
def has_auto_compose_file(parent: str = "") -> bool:
    return os.path.isfile(os.path.join(parent, AUTO_COMPOSE_YAML))
def is_auto_compose_file(file: str) -> bool:
    return os.path.basename(file) == AUTO_COMPOSE_YAML
def ensure_auto_compose_file(file: str | None) -> None:
    if file is not None and is_auto_compose_file(file) and not os.path.exists(file):
        resolve_compose_file(os.path.dirname(file))
def safe_cleanup_auto_compose(file: str | None) -> None:
    if file:
        try:
            if is_auto_compose_file(file) and os.path.exists(file):
                os.unlink(file)
        except Exception as ex:
            logger.warning(f"Error cleaning up compose file: {ex}")
AUTO_COMPOSE_YAML = ".compose.yaml"
COMPOSE_COMMENT = """# inspect auto-generated docker compose file
# (will be removed when task is complete)"""
COMPOSE_GENERIC_YAML = f"""{COMPOSE_COMMENT}
services:
  default:
    image: "python:3.12-bookworm"
    command: "tail -f /dev/null"
    init: true
    network_mode: none
    stop_grace_period: 1s
"""
COMPOSE_DOCKERFILE_YAML = f"""{COMPOSE_COMMENT}
services:
  default:
    build:
      context: "."
      dockerfile: "{{dockerfile}}"
    command: "tail -f /dev/null"
    init: true
    network_mode: none
    stop_grace_period: 1s
"""
def auto_compose_file(contents: str, parent: str = "") -> str:
    path = os.path.join(parent, AUTO_COMPOSE_YAML)
    with open(path, "w", encoding="utf-8") as f:
        f.write(contents)
    return Path(path).resolve().as_posix()

================
File: src/inspect_ai/util/_sandbox/docker/docker.py
================
import base64
import errno
import json
import os
import tempfile
from logging import getLogger
from pathlib import Path, PurePosixPath
from typing import Literal, Union, overload
from typing_extensions import override
from inspect_ai._util.error import PrerequisiteError
from inspect_ai.util._subprocess import ExecResult, subprocess
from ..environment import (
    HostMapping,
    PortMapping,
    SandboxConnection,
    SandboxEnvironment,
    SandboxEnvironmentConfigType,
)
from ..limits import (
    SandboxEnvironmentLimits,
    verify_exec_result_size,
    verify_read_file_size,
)
from ..registry import sandboxenv
from .cleanup import (
    cli_cleanup,
    project_cleanup,
    project_cleanup_shutdown,
    project_cleanup_startup,
    project_startup,
)
from .compose import (
    compose_build,
    compose_check_running,
    compose_cleanup_images,
    compose_cp,
    compose_exec,
    compose_ps,
    compose_pull,
    compose_services,
    compose_up,
)
from .config import CONFIG_FILES, DOCKERFILE
from .internal import build_internal_image, is_internal_image
from .prereqs import validate_prereqs
from .util import ComposeProject, task_project_name
logger = getLogger(__name__)
@sandboxenv(name="docker")
class DockerSandboxEnvironment(SandboxEnvironment):
    @classmethod
    def config_files(cls) -> list[str]:
        return CONFIG_FILES + [DOCKERFILE]
    @classmethod
    def default_concurrency(cls) -> int | None:
        count = os.cpu_count() or 1
        return 2 * count
    @classmethod
    async def task_init(
        cls, task_name: str, config: SandboxEnvironmentConfigType | None
    ) -> None:
        # validate prereqs
        await validate_prereqs()
        # intialize project cleanup
        project_cleanup_startup()
        try:
            # create project
            project = await ComposeProject.create(
                name=task_project_name(task_name), config=config
            )
            # build containers which are out of date
            await compose_build(project)
            # cleanup images created during build
            await compose_cleanup_images(project, timeout=60)
            services = await compose_services(project)
            for name, service in services.items():
                # if the service has an explicit container_name then
                # error (as this won't work w/ epochs > 1)
                container_name = service.get("container_name", None)
                if container_name:
                    raise PrerequisiteError(
                        f"ERROR: Docker service '{name}' includes an explicitly configured container_name ('{container_name}'). This is not permitted, as container names should be provisioned by Docker compose and an explicit container_name will not work with epochs > 1."
                    )
                # build internal images
                image = service.get("image", None)
                if image and is_internal_image(image):
                    await build_internal_image(image)
                # pull any remote images
                elif (
                    service.get("build", None) is None
                    and service.get("x-local", None) is None
                ):
                    pull_result = await compose_pull(name, project)
                    if not pull_result.success:
                        image = service.get("image", "(unknown)")
                        logger.error(
                            f"Failed to pull docker image '{image}' from remote registry. If this is a locally built image add 'x-local: true' to the the service definition to prevent this error."
                        )
            # provide some space above task display
            print("")
        except BaseException as ex:
            await project_cleanup_shutdown(True)
            raise ex
    @override
    @classmethod
    async def sample_init(
        cls,
        task_name: str,
        config: SandboxEnvironmentConfigType | None,
        metadata: dict[str, str],
    ) -> dict[str, SandboxEnvironment]:
        # create environment variables for sample metadata
        env: dict[str, str] = {}
        if isinstance(config, str) and Path(config).exists():
            # read the config file
            with open(config, "r") as f:
                config_text = f.read()
            # only add metadata files if the key is in the file
            for key, value in metadata.items():
                key = f"SAMPLE_METADATA_{key.replace(' ', '_').upper()}"
                if key in config_text:
                    env[key] = str(value)
        # create project
        from inspect_ai.log._samples import sample_active
        sample = sample_active()
        project = await ComposeProject.create(
            name=task_project_name(task_name),
            config=config,
            sample_id=sample.sample.id if sample is not None else None,
            epoch=sample.epoch if sample is not None else None,
            env=env,
        )
        try:
            # enumerate the services that will be created
            services = await compose_services(project)
            # start the services
            result = await compose_up(project, services)
            # check to ensure that the services are running
            running_services = await compose_check_running(
                list(services.keys()), project=project
            )
            if not running_services:
                raise RuntimeError(
                    f"No services started.\nCompose up stderr: {result.stderr}"
                )
            # note that the project is running
            project_startup(project)
            # create sandbox environments for all running services
            default_service: str | None = None
            environments: dict[str, SandboxEnvironment] = {}
            for service, service_info in services.items():
                if service in running_services:
                    # update the project w/ the working directory
                    working_dir = await container_working_dir(service, project)
                    # create the docker sandbox environemnt
                    docker_env = DockerSandboxEnvironment(service, project, working_dir)
                    # save reference to default service if requested
                    if service_info.get("x-default", False):
                        default_service = service
                    # record service => environment
                    environments[service] = docker_env
            # confirm that we have a 'default' environemnt
            if environments.get("default", None) is None and default_service is None:
                raise RuntimeError(
                    "No 'default' service found in Docker compose file. "
                    + "You should either name a service 'default' or add "
                    + "'x-default: true' to one of your service definitions."
                )
            # ensure that the default service is first in the dictionary
            default_service = default_service or "default"
            default_environment = environments[default_service]
            del environments[default_service]
            environments = {default_service: default_environment} | environments
        except BaseException as ex:
            await project_cleanup(project, True)
            raise ex
        return environments
    @override
    @classmethod
    async def sample_cleanup(
        cls,
        task_name: str,
        config: SandboxEnvironmentConfigType | None,
        environments: dict[str, SandboxEnvironment],
        interrupted: bool,
    ) -> None:
        # if we were interrupted then wait unil the end of the task to cleanup
        # (this enables us to show output for the cleanup operation)
        if not interrupted:
            # extract project from first environment
            project = (
                next(iter(environments.values()))
                .as_type(DockerSandboxEnvironment)
                ._project
            )
            # cleanup the project
            await project_cleanup(project=project, quiet=True)
    @classmethod
    async def task_cleanup(
        cls, task_name: str, config: SandboxEnvironmentConfigType | None, cleanup: bool
    ) -> None:
        await project_cleanup_shutdown(cleanup)
    @classmethod
    async def cli_cleanup(cls, id: str | None) -> None:
        await cli_cleanup(id)
    def __init__(self, service: str, project: ComposeProject, working_dir: str) -> None:
        super().__init__()
        self._service = service
        self._project = project
        self._working_dir = working_dir
    @override
    async def exec(
        self,
        cmd: list[str],
        input: str | bytes | None = None,
        cwd: str | None = None,
        env: dict[str, str] = {},
        user: str | None = None,
        timeout: int | None = None,
        timeout_retry: bool = True,
    ) -> ExecResult[str]:
        # additional args
        args = []
        final_cwd = PurePosixPath(self._working_dir if cwd is None else cwd)
        if not final_cwd.is_absolute():
            final_cwd = self._working_dir / final_cwd
        args.append("--workdir")
        args.append(str(final_cwd))
        if user:
            args.append("--user")
            args.append(user)
        # Forward environment commands to docker compose exec so they
        # will be available to the bash command
        if len(env.items()) > 0:
            for key, value in env.items():
                args.append("--env")
                args.append(f"{key}={value}")
        exec_result = await compose_exec(
            args + [self._service] + cmd,
            project=self._project,
            timeout=timeout,
            timeout_retry=timeout_retry,
            input=input,
            output_limit=SandboxEnvironmentLimits.MAX_EXEC_OUTPUT_SIZE,
        )
        verify_exec_result_size(exec_result)
        if exec_result.returncode == 126 and "permission denied" in exec_result.stdout:
            raise PermissionError(f"Permission denied executing command: {exec_result}")
        return exec_result
    @override
    async def write_file(self, file: str, contents: str | bytes) -> None:
        # defualt timeout for write_file operations
        TIMEOUT = 180
        # resolve relative file paths
        file = self.container_file(file)
        # ensure that the directory exists
        parent = Path(file).parent.as_posix()
        if parent != ".":
            result = await self.exec(["mkdir", "-p", parent])
            if not result.success:
                msg = f"Failed to create container directory {parent}: {result.stderr}"
                raise RuntimeError(msg)
        # write the file
        if isinstance(contents, str):
            result = await self.exec(
                ["sh", "-e", "-c", 'tee -- "$1"', "write_file_script", file],
                input=contents,
                timeout=TIMEOUT,
            )
        else:
            base64_contents = base64.b64encode(contents).decode("US-ASCII")
            result = await self.exec(
                [
                    "sh",
                    "-e",
                    "-c",
                    'base64 -d | tee -- "$1" > /dev/null',
                    "write_file_script",
                    file,
                ],
                input=base64_contents,
                timeout=TIMEOUT,
            )
        if result.returncode != 0:
            if "permission denied" in result.stderr.casefold():
                ls_result = await self.exec(["ls", "-la", "."])
                error_string = f"Permission was denied. Error details: {result.stderr}; ls -la: {ls_result.stdout}"
                raise PermissionError(error_string)
            elif (
                "cannot overwrite directory" in result.stderr.casefold()
                or "is a directory" in result.stderr.casefold()
            ):
                raise IsADirectoryError(
                    f"Failed to write file: {file} because it is a directory already"
                )
            else:
                raise RuntimeError(f"failed to copy during write_file: {result}")
    @overload
    async def read_file(self, file: str, text: Literal[True] = True) -> str: ...
    @overload
    async def read_file(self, file: str, text: Literal[False]) -> bytes: ...
    @override
    async def read_file(self, file: str, text: bool = True) -> Union[str, bytes]:
        # Write the contents to a temp file
        with tempfile.TemporaryDirectory(ignore_cleanup_errors=True) as temp_dir:
            # resolve relative file paths
            original_file = file
            file = self.container_file(file)
            # copy the file
            dest_file = os.path.join(temp_dir, os.path.basename(file))
            try:
                await compose_cp(
                    src=f"{self._service}:{file}",
                    dest=os.path.basename(dest_file),
                    project=self._project,
                    cwd=os.path.dirname(dest_file),
                    output_limit=SandboxEnvironmentLimits.MAX_READ_FILE_SIZE,
                )
            except RuntimeError as ex:
                # extract the message and normalise case
                message = str(ex).lower()
                # FileNotFoundError
                if "could not find the file" in message:
                    raise FileNotFoundError(
                        errno.ENOENT, "No such file or directory.", original_file
                    )
                # PermissionError
                elif "permission denied" in message:
                    raise PermissionError(
                        errno.EACCES, "Permission denied.", original_file
                    )
                else:
                    raise ex
            verify_read_file_size(dest_file)
            # read and return w/ appropriate encoding
            if text:
                with open(dest_file, "r", newline="", encoding="utf-8") as f:
                    return f.read()
            else:
                with open(dest_file, "rb") as f:
                    return f.read()
    @override
    async def connection(self) -> SandboxConnection:
        # find container for service
        services = await compose_ps(project=self._project)
        container = next(
            (
                service["Name"]
                for service in services
                if service["Service"] == self._service
            ),
            None,
        )
        # return container connection
        if container:
            return SandboxConnection(
                type="docker",
                command=f"docker exec -it {container} bash -l",
                vscode_command=[
                    "remote-containers.attachToRunningContainer",
                    container,
                ],
                ports=await get_ports_info(container),
                container=container,
            )
        # error (not currently running)
        else:
            raise ConnectionError(
                f"Service '{self._service} is not currently running.'"
            )
    def container_file(self, file: str) -> str:
        path = Path(file)
        if not path.is_absolute():
            path = Path(self._working_dir) / path
        return path.as_posix()
async def container_working_dir(
    service: str, project: ComposeProject, default: str = "/"
) -> str:
    result = await compose_exec(
        [service, "sh", "-c", "pwd"], timeout=60, project=project
    )
    if result.success:
        return result.stdout.strip()
    else:
        logger.warning(
            f"Failed to get working directory for docker container '{service}': "
            + f"{result.stderr}"
        )
        return default
async def get_ports_info(container: str) -> list[PortMapping] | None:
    try:
        result = await subprocess(
            [
                "docker",
                "inspect",
                container,
                "--format",
                "{{json .NetworkSettings.Ports}}",
            ],
            timeout=60,
        )
        if not result.success:
            raise RuntimeError(result.stderr)
        return parse_docker_inspect_ports(result.stdout)
    # It's currently a policy decision to let docker timeouts to be silent.
    except TimeoutError:
        return None
def parse_docker_inspect_ports(json_str: str) -> list[PortMapping] | None:
    """
    Parses the JSON output from `docker inspect {container_name} --format='{{json .NetworkSettings.Ports}}'` to extract port mappings.
    Args:
        json_str (str): A JSON string representing the `NetworkSettings.Ports` output of `docker inspect`. e.g.
          ```
          {
              "5900/tcp": [{"HostIp": "0.0.0.0", "HostPort": "54023"}],
              "8080/tcp": [{"HostIp": "0.0.0.0", "HostPort": "54024"}]
          }
          ```
    Returns:
        list[PortMapping] | None: A list of PortMapping objects if any port mappings are found,
                                   otherwise None.
    """
    data = json.loads(json_str)
    port_mappings = []
    for port_protocol, mappings in data.items():
        if mappings is None:
            continue
        container_port, protocol = port_protocol.split("/")
        host_mappings = [
            HostMapping(host_ip=mapping["HostIp"], host_port=int(mapping["HostPort"]))
            for mapping in mappings
        ]
        port_mapping = PortMapping(
            container_port=int(container_port),
            protocol=protocol,
            mappings=host_mappings,
        )
        port_mappings.append(port_mapping)
    return port_mappings if port_mappings else None

================
File: src/inspect_ai/util/_sandbox/docker/internal.py
================
from inspect_ai._util.constants import PKG_PATH
from inspect_ai._util.error import PrerequisiteError
from inspect_ai.util._display import display_type
from inspect_ai.util._subprocess import subprocess
INSPECT_WEB_BROWSER_IMAGE_DOCKERHUB = "aisiuk/inspect-web-browser-tool"
INSPECT_WEB_BROWSER_IMAGE = "inspect_web_browser"
INSPECT_COMPUTER_IMAGE = "inspect-computer-tool"
INTERNAL_IMAGES = {
    INSPECT_WEB_BROWSER_IMAGE: PKG_PATH
    / "tool"
    / "_tools"
    / "_web_browser"
    / "_resources",
    INSPECT_COMPUTER_IMAGE: PKG_PATH / "tool" / "beta" / "_computer" / "_resources",
}
async def is_internal_image_built(image: str) -> bool:
    result = await subprocess(
        ["docker", "images", "--filter", f"reference={image}", "--format", "json"]
    )
    return len(result.stdout.strip()) > 0
async def build_internal_image(image: str) -> None:
    args = [
        "docker",
        "build",
        "--tag",
        image,
        "--progress",
        "plain" if display_type() == "plain" else "auto",
    ]
    if display_type() == "none":
        args.append("--quiet")
    result = await subprocess(
        args + [INTERNAL_IMAGES[image].as_posix()],
        capture_output=False,
    )
    if not result.success:
        raise PrerequisiteError(f"Unexpected error building Docker image '{image}'")
def is_internal_image(image: str) -> bool:
    return any([image == internal for internal in INTERNAL_IMAGES.keys()])

================
File: src/inspect_ai/util/_sandbox/docker/prereqs.py
================
import json
import shlex
from logging import getLogger
from typing import Callable
import semver
from pydantic import BaseModel
from inspect_ai._util.error import PrerequisiteError
from inspect_ai.util._subprocess import subprocess
logger = getLogger(__name__)
class DockerClientVersion(BaseModel):
    Version: str
    ApiVersion: str
class DockerVersion(BaseModel):
    Client: DockerClientVersion
async def validate_prereqs() -> None:
    await validate_docker_engine()
    await validate_docker_compose()
# Version that corresponds to Docker Desktop w/ Compose v2.21.0
# Linux versions of Docker Engine (docker-ce) also include
# Docker Compose as a dependency as of this version
# https://docs.docker.com/engine/release-notes/24.0/#2407
DOCKER_ENGINE_REQUIRED_VERSION = "24.0.6"
async def validate_docker_engine(version: str = DOCKER_ENGINE_REQUIRED_VERSION) -> None:
    def parse_version(stdout: str) -> semver.Version:
        version = DockerVersion(**json.loads(stdout)).Client.Version
        return semver.Version.parse(version)
    await validate_version(
        cmd=["docker", "version", "--format", "json"],
        parse_fn=parse_version,
        required_version=version,
        feature="Docker Engine",
    )
# We require Compose v2.21.0, however if we are going to use
# the pull '--policy' option we call this again with 2.22.0
DOCKER_COMPOSE_REQUIRED_VERSION = "2.21.0"
DOCKER_COMPOSE_REQUIRED_VERSION_PULL_POLICY = "2.22.0"
async def validate_docker_compose(
    version: str = DOCKER_COMPOSE_REQUIRED_VERSION,
) -> None:
    def parse_version(stdout: str) -> semver.Version:
        version = json.loads(stdout)["version"].removeprefix("v").split("+")[0]
        return semver.Version.parse(version)
    await validate_version(
        cmd=["docker", "compose", "version", "--format", "json"],
        parse_fn=parse_version,
        required_version=version,
        feature="Docker Compose",
    )
async def validate_version(
    cmd: list[str],
    parse_fn: Callable[[str], semver.Version],
    required_version: str,
    feature: str,
) -> None:
    # attempt to read version
    try:
        version = semver.Version(0)
        result = await subprocess(cmd)
        if result.success:
            version = parse_fn(result.stdout)
    except Exception as ex:
        # we expect FileNotFoundError (when docker is not installed) however
        # other errors would be a surprise so we alert the user w/ a warning
        if not isinstance(ex, FileNotFoundError):
            logger.warning(f"Unexpected error executing docker: {ex}")
        raise PrerequisiteError(
            "ERROR: Docker sandbox environments require Docker Engine\n\n"
            + "Install: https://docs.docker.com/engine/install/"
        )
    if not result.success:
        raise PrerequisiteError(
            "ERROR: Docker sandbox environments require a working Docker Engine\n\n"
            + f"{cmd[0]} exited with return code {result.returncode} when executing: {shlex.join(cmd)}\n"
            + result.stderr
        )
    # validate version
    if version.compare(required_version) < 0:
        raise PrerequisiteError(
            f"ERROR: Docker sandbox environments require {feature} >= {required_version} (current: {version})\n\n"
            + "Upgrade: https://docs.docker.com/engine/install/"
        )

================
File: src/inspect_ai/util/_sandbox/docker/service.py
================
import re
from dataclasses import dataclass
from typing import TypedDict
class ComposeServiceHealthcheck(TypedDict, total=False):
    start_period: str
    interval: str
    retries: int
    timeout: str
ComposeService = TypedDict(
    "ComposeService",
    {
        "image": str,
        "build": str,
        "container_name": str,
        "x-default": bool,
        "x-local": bool,
        "healthcheck": ComposeServiceHealthcheck,
    },
    total=False,
)
def services_healthcheck_time(services: dict[str, ComposeService]) -> int:
    max_time = 0
    for _, service in services.items():
        service_time = service_healthcheck_time(service)
        max_time = max(max_time, service_time)
    return max_time
def service_healthcheck_time(service: ComposeService) -> int:
    """
    Calculate the maximum time a single service's healthcheck could take.
    The total time is:
    (retries * (interval + timeout))
    Default values (from Docker documentation):
    - retries: 3
    - interval: 30s
    - timeout: 30s
    """
    healthcheck = service.get("healthcheck", None)
    if healthcheck is None:
        return 0
    # Parse duration strings with defaults
    retries = healthcheck.get("retries", 3)
    interval = parse_duration(healthcheck.get("interval", "30s"))
    timeout = parse_duration(healthcheck.get("timeout", "30s"))
    # Calculate total time in seconds
    total_time = retries * (interval.seconds + timeout.seconds)
    return int(total_time)
@dataclass
class Duration:
    nanoseconds: int
    @property
    def seconds(self) -> float:
        return self.nanoseconds / 1_000_000_000
def parse_duration(duration_str: str) -> Duration:
    """Parse a Docker compose style duration string."""
    if not duration_str:
        return Duration(0)
    units = {
        "ns": 1,
        "us": 1_000,
        "ms": 1_000_000,
        "s": 1_000_000_000,
        "m": 60_000_000_000,
        "h": 3_600_000_000_000,
    }
    duration_str = "".join(duration_str.split())
    pattern = re.compile(r"(\d+)([a-z]+)")
    matches = pattern.findall(duration_str)
    if not matches:
        raise ValueError(f"Invalid duration format: {duration_str}")
    total_nanoseconds = 0
    for number, unit in matches:
        if unit not in units:
            raise ValueError(f"Invalid unit: {unit}")
        total_nanoseconds += int(number) * units[unit]
    return Duration(total_nanoseconds)

================
File: src/inspect_ai/util/_sandbox/docker/util.py
================
import re
from dataclasses import dataclass
from logging import getLogger
from pathlib import Path
from shortuuid import uuid
from ..environment import SandboxEnvironmentConfigType
from .config import (
    COMPOSE_DOCKERFILE_YAML,
    auto_compose_file,
    ensure_auto_compose_file,
    is_dockerfile,
    resolve_compose_file,
)
logger = getLogger(__name__)
@dataclass
class ComposeProject:
    name: str
    config: str | None
    sample_id: int | str | None
    epoch: int | None
    env: dict[str, str] | None
    @classmethod
    async def create(
        cls,
        name: str,
        config: SandboxEnvironmentConfigType | None,
        *,
        sample_id: int | str | None = None,
        epoch: int | None = None,
        env: dict[str, str] = {},
    ) -> "ComposeProject":
        # resolve config to full path if we have one
        config_path = None
        if isinstance(config, str):
            config_path = Path(config).resolve()
        elif config is not None:
            raise ValueError(f"Unsupported config type: {type(config)}. Expected str.")
        # if its a Dockerfile, then config is the auto-generated .compose.yaml
        if config_path and is_dockerfile(config_path.name):
            config = auto_compose_file(
                COMPOSE_DOCKERFILE_YAML.format(dockerfile=config_path.name),
                config_path.parent.as_posix(),
            )
        # if its another config file, just take its path
        elif config_path:
            config = config_path.as_posix()
        # no config passed, look for 'auto-config' (compose.yaml, Dockerfile, etc.)
        else:
            config = resolve_compose_file()
        # this could be a cleanup where docker has tracked a .compose.yaml file
        # as part of its ConfigFiles and passed it back to us -- we in the
        # meantime have cleaned it up so we re-create it here as required
        ensure_auto_compose_file(config)
        # return project
        return ComposeProject(name, config, sample_id=sample_id, epoch=epoch, env=env)
    def __init__(
        self,
        name: str,
        config: str | None,
        sample_id: int | str | None,
        epoch: int | None,
        env: dict[str, str],
    ) -> None:
        self.name = name
        self.config = config
        self.sample_id = sample_id
        self.epoch = epoch
        self.env = env
    def __eq__(self, other: object) -> bool:
        if not isinstance(other, ComposeProject):
            return NotImplemented
        else:
            return self.name == other.name
def task_project_name(task: str) -> str:
    # ensure that task conforms to docker project name constraints
    task = task.lower()
    task = re.sub(r"[^a-z\d\-_]", "-", task)
    task = re.sub(r"-+", "-", task)
    if len(task) == 0:
        task = "task"
    # _- breaks docker project name constraints so we strip trailing underscores.
    return f"inspect-{task[:12].rstrip('_')}-i{uuid().lower()[:6]}"
inspect_project_pattern = r"^inspect-[a-z\d\-_]*-i[a-z\d]{6,}$"
def is_inspect_project(name: str) -> bool:
    return re.match(inspect_project_pattern, name) is not None

================
File: src/inspect_ai/util/_sandbox/environment.py
================
from __future__ import annotations
import abc
from dataclasses import dataclass, field
from typing import (
    Annotated,
    Any,
    Awaitable,
    Callable,
    Literal,
    Type,
    TypeVar,
    Union,
    cast,
    overload,
)
from pydantic import BaseModel, Field, model_validator
from .._subprocess import ExecResult
ST = TypeVar("ST", bound="SandboxEnvironment")
TaskInit = Callable[[str, Union["SandboxEnvironmentConfigType", None]], Awaitable[None]]
TaskCleanup = Callable[
    [str, Union["SandboxEnvironmentConfigType", None], bool], Awaitable[None]
]
SampleInit = Callable[
    [str, Union["SandboxEnvironmentConfigType", None], dict[str, str]],
    Awaitable[dict[str, "SandboxEnvironment"]],
]
SampleCleanup = Callable[
    [
        str,
        Union["SandboxEnvironmentConfigType", None],
        dict[str, "SandboxEnvironment"],
        bool,
    ],
    Awaitable[None],
]
ConfigDeserialize = Callable[[dict[str, Any]], BaseModel]
class HostMapping(BaseModel):
    host_ip: str
    host_port: int
class PortMapping(BaseModel):
    container_port: int
    protocol: Literal["tcp", "udp"]
    mappings: list[HostMapping]
class SandboxConnection(BaseModel):
    """Information required to connect to sandbox."""
    type: str
    """Sandbox type name (e.g. 'docker', 'local', etc.)"""
    command: str
    """Shell command to connect to sandbox."""
    vscode_command: list[Any] | None = Field(default=None)
    """Optional vscode command (+args) to connect to sandbox."""
    ports: list[PortMapping] | None = Field(default=None)
    """Optional list of port mappings into container"""
    container: str | None = Field(default=None)
    """Optional container name (does not apply to all sandboxes)."""
class SandboxEnvironment(abc.ABC):
    """Environment for executing arbitrary code from tools.
    Sandbox environments provide both an execution environment as well as a per-sample
    filesystem context to copy samples files into and resolve relative paths to.
    """
    @abc.abstractmethod
    async def exec(
        self,
        cmd: list[str],
        input: str | bytes | None = None,
        cwd: str | None = None,
        env: dict[str, str] = {},
        user: str | None = None,
        timeout: int | None = None,
        timeout_retry: bool = True,
    ) -> ExecResult[str]:
        """Execute a command within a sandbox environment.
        The current working directory for execution will be the per-sample
        filesystem context.
        Each output stream (stdout and stderr) is limited to 10 MiB. If exceeded, an
        `OutputLimitExceededError` will be raised.
        Args:
          cmd: Command or command and arguments to execute.
          input: Standard input (optional).
          cwd: Current working dir (optional). If relative, will be relative to the per-sample filesystem context.
          env: Environment variables for execution.
          user: Optional username or UID to run the command as.
          timeout: Optional execution timeout (seconds).
          timeout_retry: Retry the command in the case that it times out.
            Commands will be retried up to twice, with a timeout of no greater
            than 60 seconds for the first retry and 30 for the second.
        Returns:
          Execution result (status code, stderr/stdout, etc.)
        Raises:
          TimeoutError: If the specified `timeout` expires
            (and `timeout_retry` attempts also timeout).
          UnicodeDecodeError: If an error occurs while
            decoding the command output.
          PermissionError: If the user does not have
            permission to execute the command.
          OutputLimitExceededError: If an output stream
            exceeds the 10 MiB limit.
        """
        ...
    @abc.abstractmethod
    async def write_file(self, file: str, contents: str | bytes) -> None:
        """Write a file into the sandbox environment.
        If the parent directories of the file path do not exist they
        should be automatically created.
        Args:
          file: Path to file (relative file paths will resolve to the
            per-sample working directory).
          contents: Text or binary file contents.
        Raises:
          PermissionError: If the current user does not have permission to
            write to the specified path.
          IsADirectoryError: If the file exists already and
            is a directory.
        """
        ...
    @overload
    async def read_file(self, file: str, text: Literal[True] = True) -> str: ...
    @overload
    async def read_file(self, file: str, text: Literal[False]) -> bytes: ...
    @abc.abstractmethod
    async def read_file(self, file: str, text: bool = True) -> Union[str | bytes]:
        """Read a file from the sandbox environment.
        File size is limited to 100 MiB.
        When reading text files, implementations should preserve newline constructs
        (e.g. crlf should be preserved not converted to lf). This is equivalent
        to specifying `newline=""` in a call to the Python `open()` function.
        Args:
          file: Path to file (relative file paths will resolve to the
            per-sample working directory).
          text: Read as a utf-8 encoded text file.
        Returns:
          Contents of file (as str or bytes for binary files)
        Raises:
          FileNotFoundError: If the file does not exist.
          UnicodeDecodeError: If an encoding error occurs
            while reading the file.
            (only applicable when `text = True`)
          PermissionError: If the user does not have
            permission to read from the specified path.
          IsADirectoryError: If the file is a directory.
          OutputLimitExceededError: If the file size
            exceeds the 100 MiB limit.
        """
        ...
    async def connection(self) -> SandboxConnection:
        """Information required to connect to sandbox environment.
        Returns:
           SandboxConnection: connection information
        Raises:
           NotImplementedError: For sandboxes that don't provide connections
           ConnectionError: If sandbox is not currently running.
        """
        raise NotImplementedError("connection not implemented")
    def as_type(self, sandbox_cls: Type[ST]) -> ST:
        """Verify and return a reference to a subclass of SandboxEnvironment.
        Args:
           sandbox_cls: Class of sandbox (subclass of SandboxEnvironment)
        Returns:
           Reference to the sandbox using the requested type.
        Raises:
           TypeError: If the sandbox is not of the requested type.
        """
        if isinstance(self, sandbox_cls):
            return self
        else:
            raise TypeError(
                f"Expected instance of {sandbox_cls.__name__}, got {type(self).__name__}"
            )
    @classmethod
    def default_concurrency(cls) -> int | None:
        """Default max_sandboxes for this provider (`None` means no maximum)"""
        return None
    @classmethod
    async def task_init(
        cls, task_name: str, config: SandboxEnvironmentConfigType | None
    ) -> None:
        """Called at task startup initialize resources.
        Args:
          task_name: Name of task using the sandbox environment.
          config: Implementation defined configuration (optional).
        """
        pass
    @classmethod
    async def sample_init(
        cls,
        task_name: str,
        config: SandboxEnvironmentConfigType | None,
        metadata: dict[str, str],
    ) -> dict[str, "SandboxEnvironment"]:
        """Initialize sandbox environments for a sample.
        Args:
          task_name: Name of task using the sandbox environment.
          config: Implementation defined configuration (optional).
          metadata: Sample `metadata` field
        Returns:
          Dictionary of named sandbox environments. The environment which represents
          the default environment (resolved by `sandbox("default")` or `sandbox()`) must
          be the first key/value pair in the dictionary.
        """
        return {}
    @classmethod
    @abc.abstractmethod
    async def sample_cleanup(
        cls,
        task_name: str,
        config: SandboxEnvironmentConfigType | None,
        environments: dict[str, "SandboxEnvironment"],
        interrupted: bool,
    ) -> None:
        """Cleanup sandbox environments.
        Args:
          task_name: Name of task using the sandbox environment.
          config: Implementation defined configuration (optional).
          environments: Sandbox environments created for this sample.
          interrupted: Was the task interrupted by an error or cancellation
        """
        ...
    @classmethod
    async def task_cleanup(
        cls, task_name: str, config: SandboxEnvironmentConfigType | None, cleanup: bool
    ) -> None:
        """Called at task exit as a last chance to cleanup resources.
        Args:
          task_name: Name of task using the sandbox environment.
          config: Implementation defined configuration (optional).
          cleanup: Whether to actually cleanup environment resources
            (False if `--no-sandbox-cleanup` was specified)
        """
        pass
    @classmethod
    async def cli_cleanup(cls, id: str | None) -> None:
        """Handle a cleanup invoked from the CLI (e.g. inspect sandbox cleanup).
        Args:
          id: Optional ID to limit scope of cleanup.
        """
        pass
    @classmethod
    def config_files(cls) -> list[str]:
        """Standard config files for this provider (used for automatic discovery)"""
        return []
    @classmethod
    def config_deserialize(cls, config: dict[str, Any]) -> BaseModel:
        """Deserialize a sandbox-specific configuration model from a dict.
        Override this method if you support a custom configuration model.
        A basic implementation would be: `return MySandboxEnvironmentConfig(**config)`
        Args:
          config: Configuration dictionary produced by serializing the configuration
            model.
        Returns:
          The sandbox-specific configuration model.
        """
        raise NotImplementedError(
            "The SandboxEnvironment provider has not implemented config_deserialize."
        )
@dataclass
class SandboxEnvironments:
    """Collection of sandbox environments used for an evaluation."""
    environments: dict[str, SandboxEnvironment]
    """Sandbox environments by name."""
    cleanup: Callable[[bool], Awaitable[None]] | None = field(default=None)
    """Optional global cleanup function.
    Called with a boolean indicating whether the sample was cancelled.
    """
class SandboxEnvironmentSpec(BaseModel, frozen=True):
    """Specification of a SandboxEnvironment."""
    type: str
    """Sandbox type (e.g. 'local', 'docker')"""
    # Any is used to prevent Pydantic from trying to initialise a BaseModel.
    config: Annotated[Any, "BaseModel, str or None"] = None
    """Sandbox configuration (filename or config object)."""
    def __init__(self, type: str, config: BaseModel | str | None = None):
        super().__init__(type=type, config=config)
    @model_validator(mode="before")
    @classmethod
    def load_config_model(cls, data: dict[str, Any]) -> dict[str, Any]:
        type = data["type"]
        config = data.get("config")
        # Pydantic won't know what concrete type to instantiate for config, so
        # ask the relevant sandbox environment to deserialize it.
        if isinstance(config, dict) and len(config) > 0:
            data["config"] = deserialize_sandbox_specific_config(type, config)
        return data
SandboxEnvironmentConfigType = BaseModel | str
SandboxEnvironmentType = SandboxEnvironmentSpec | str | tuple[str, str]
"""SandboxEnvironmentSpec and str and tuple shorthands for it.
A plain str, e.g. "docker", is equivalent to SandboxEnvironmentSpec("docker")
A tuple, e.g. ("docker", "compose.yaml"), is equivalent to SandboxEnvironmentSpec("docker", "compose.yaml")
"""
def resolve_sandbox_environment(
    sandbox: SandboxEnvironmentType | None,
) -> SandboxEnvironmentSpec | None:
    # do the resolution
    if isinstance(sandbox, str):
        return SandboxEnvironmentSpec(type=sandbox)
    elif isinstance(sandbox, SandboxEnvironmentSpec):
        return sandbox
    elif isinstance(sandbox, tuple):
        return SandboxEnvironmentSpec(sandbox[0], sandbox[1])
    else:
        return None
def deserialize_sandbox_specific_config(type: str, config: dict[str, Any]) -> BaseModel:
    # Avoid circular import
    from inspect_ai.util._sandbox.registry import registry_find_sandboxenv
    sandboxenv_type = registry_find_sandboxenv(type)
    config_deserialize = cast(
        ConfigDeserialize, getattr(sandboxenv_type, "config_deserialize")
    )
    return config_deserialize(config)

================
File: src/inspect_ai/util/_sandbox/events.py
================
import contextlib
import shlex
from datetime import datetime
from typing import Iterator, Literal, Type, Union, overload
from pydantic import JsonValue
from pydantic_core import to_jsonable_python
from typing_extensions import override
from inspect_ai._util.text import truncate_lines
from inspect_ai.util._subprocess import ExecResult
from .environment import (
    ST,
    SandboxConnection,
    SandboxEnvironment,
    SandboxEnvironmentConfigType,
)
class SandboxEnvironmentProxy(SandboxEnvironment):
    def __init__(self, sandbox: SandboxEnvironment) -> None:
        self._sandbox = sandbox
        self._events = True
    @override
    async def exec(
        self,
        cmd: list[str],
        input: str | bytes | None = None,
        cwd: str | None = None,
        env: dict[str, str] = {},
        user: str | None = None,
        timeout: int | None = None,
        timeout_retry: bool = True,
    ) -> ExecResult[str]:
        from inspect_ai.log._transcript import SandboxEvent, transcript
        # started
        timestamp = datetime.now()
        # make call
        result = await self._sandbox.exec(
            cmd, input, cwd, env, user, timeout, timeout_retry
        )
        # yield event
        options: dict[str, JsonValue] = {}
        if cwd:
            options["cwd"] = cwd
        if env:
            options["env"] = to_jsonable_python(env)
        if user:
            options["user"] = user
        if timeout is not None:
            options["timeout"] = timeout
        if timeout_retry is not True:
            options["timeout_retry"] = timeout_retry
        if self._events:
            transcript()._event(
                SandboxEvent(
                    timestamp=timestamp,
                    action="exec",
                    cmd=" ".join([shlex.quote(c) for c in cmd]),
                    input=content_display(input) if input is not None else None,
                    options=options,
                    result=result.returncode,
                    output=content_display(
                        f"{result.stderr}\n\n{result.stdout}"
                        if result.stderr
                        else result.stdout
                    ),
                    completed=datetime.now(),
                )
            )
        # return result
        return result
    @override
    async def write_file(self, file: str, contents: str | bytes) -> None:
        from inspect_ai.log._transcript import SandboxEvent, transcript
        timestamp = datetime.now()
        # make call
        await self._sandbox.write_file(file, contents)
        # yield event
        if self._events:
            transcript()._event(
                SandboxEvent(
                    timestamp=timestamp,
                    action="write_file",
                    file=file,
                    input=content_display(contents),
                    completed=datetime.now(),
                )
            )
    @overload
    async def read_file(self, file: str, text: Literal[True] = True) -> str: ...
    @overload
    async def read_file(self, file: str, text: Literal[False]) -> bytes: ...
    @override
    async def read_file(self, file: str, text: bool = True) -> Union[str | bytes]:
        from inspect_ai.log._transcript import SandboxEvent, transcript
        timestamp = datetime.now()
        # make call
        if text is True:
            output: str | bytes = await self._sandbox.read_file(file, True)
        else:
            output = await self._sandbox.read_file(file, False)
        # yield event
        if self._events:
            transcript()._event(
                SandboxEvent(
                    timestamp=timestamp,
                    action="read_file",
                    file=file,
                    output=content_display(output),
                    completed=datetime.now(),
                )
            )
        # return result
        return output
    @override
    async def connection(self) -> SandboxConnection:
        return await self._sandbox.connection()
    @override
    def as_type(self, sandbox_cls: Type[ST]) -> ST:
        if isinstance(self._sandbox, sandbox_cls):
            return self._sandbox
        else:
            raise TypeError(
                f"Expected instance of {sandbox_cls.__name__}, got {type(self._sandbox).__name__}"
            )
    @contextlib.contextmanager
    def no_events(self) -> Iterator[None]:
        self._events = False
        try:
            yield
        finally:
            self._events = True
    @classmethod
    async def sample_cleanup(
        cls,
        task_name: str,
        config: SandboxEnvironmentConfigType | None,
        environments: dict[str, SandboxEnvironment],
        interrupted: bool,
    ) -> None:
        pass
def content_display(content: str | bytes) -> str:
    if isinstance(content, str):
        content, truncated = truncate_lines(content, 20)
        if truncated:
            content = f"{content}\n\nOutput truncated ({truncated} additional lines)"
        return content
    else:
        return f"binary ({pretty_size(len(content))})"
def pretty_size(size: int) -> str:
    if size < 1024:
        return f"{size} B"
    if size < 1024 * 1024:
        return f"{size / 1024:.2f} KB"
    return f"{size / (1024 * 1024):.2f} MB"

================
File: src/inspect_ai/util/_sandbox/limits.py
================
from pathlib import Path
from inspect_ai._util.text import truncate_string_to_bytes
from inspect_ai.util._subprocess import ExecResult
class SandboxEnvironmentLimits:
    """Encapsulates limits for sandbox environments."""
    MAX_EXEC_OUTPUT_SIZE = 10 * 1024**2
    MAX_EXEC_OUTPUT_SIZE_STR = "10 MiB"
    MAX_READ_FILE_SIZE = 100 * 1024**2
    MAX_READ_FILE_SIZE_STR = "100 MiB"
class OutputLimitExceededError(Exception):
    """Exception raised when a sandbox invocation results in excessive output."""
    def __init__(self, limit_str: str, truncated_output: str | None) -> None:
        self.limit_str = limit_str
        self.truncated_output = truncated_output
        super().__init__(
            f"The sandbox output stream limit of {self.limit_str} was exceeded."
        )
def verify_exec_result_size(exec_result: ExecResult[str]) -> None:
    """Verify the size of the output streams in an `ExecResult`.
    Raises:
      OutputLimitExceededError: If an output stream exceeds the limit.
    """
    limit = SandboxEnvironmentLimits.MAX_EXEC_OUTPUT_SIZE
    stdout_truncated = truncate_string_to_bytes(exec_result.stdout, limit)
    stderr_truncated = truncate_string_to_bytes(exec_result.stderr, limit)
    if not stdout_truncated and not stderr_truncated:
        return
    stdout = stdout_truncated.output if stdout_truncated else exec_result.stdout
    stderr = stderr_truncated.output if stderr_truncated else exec_result.stderr
    raise OutputLimitExceededError(
        limit_str=SandboxEnvironmentLimits.MAX_EXEC_OUTPUT_SIZE_STR,
        truncated_output=f"{stdout}{stderr}",
    )
def verify_read_file_size(file: str) -> None:
    """Verify the size of a file to be read into memory.
    Raises:
      OutputLimitExceededError: If the file size exceeds the 100 MiB limit.
    """
    file_size = Path(file).stat().st_size
    if file_size > SandboxEnvironmentLimits.MAX_READ_FILE_SIZE:
        raise OutputLimitExceededError(
            limit_str=SandboxEnvironmentLimits.MAX_READ_FILE_SIZE_STR,
            # The potentially large, and potentially binary content is not included.
            truncated_output=None,
        )

================
File: src/inspect_ai/util/_sandbox/local.py
================
import tempfile
import warnings
from pathlib import Path
from typing import Literal, Union, overload
from typing_extensions import override
from .._subprocess import ExecResult, subprocess
from .environment import (
    SandboxEnvironment,
    SandboxEnvironmentConfigType,
)
from .limits import (
    SandboxEnvironmentLimits,
    verify_exec_result_size,
    verify_read_file_size,
)
from .registry import sandboxenv
@sandboxenv(name="local")
class LocalSandboxEnvironment(SandboxEnvironment):
    @override
    @classmethod
    async def sample_init(
        cls,
        task_name: str,
        config: SandboxEnvironmentConfigType | None,
        metadata: dict[str, str],
    ) -> dict[str, SandboxEnvironment]:
        return {"default": LocalSandboxEnvironment()}
    @override
    @classmethod
    async def sample_cleanup(
        cls,
        task_name: str,
        config: SandboxEnvironmentConfigType | None,
        environments: dict[str, SandboxEnvironment],
        interrupted: bool,
    ) -> None:
        for environment in environments.values():
            sandbox = environment.as_type(LocalSandboxEnvironment)
            sandbox.directory.cleanup()
    def __init__(self) -> None:
        self.directory = tempfile.TemporaryDirectory(ignore_cleanup_errors=True)
    @override
    async def exec(
        self,
        cmd: list[str],
        input: str | bytes | None = None,
        cwd: str | None = None,
        env: dict[str, str] = {},
        user: str | None = None,
        timeout: int | None = None,
        timeout_retry: bool = True,
    ) -> ExecResult[str]:
        if user is not None:
            warnings.warn(
                "The 'user' parameter is ignored in LocalSandboxEnvironment. Commands will run as the current user.",
                UserWarning,
            )
        final_cwd = Path(self.directory.name if cwd is None else cwd)
        if not final_cwd.is_absolute():
            final_cwd = self.directory.name / final_cwd
        result = await subprocess(
            args=cmd,
            input=input,
            cwd=final_cwd,
            env=env,
            timeout=timeout,
            output_limit=SandboxEnvironmentLimits.MAX_EXEC_OUTPUT_SIZE,
        )
        verify_exec_result_size(result)
        return result
    @override
    async def write_file(self, file: str, contents: str | bytes) -> None:
        # resolve file and ensure the parent dir exists
        file = self._resolve_file(file)
        Path(file).parent.mkdir(parents=True, exist_ok=True)
        if isinstance(contents, str):
            with open(file, "w", encoding="utf-8") as f:
                f.write(contents)
        else:
            with open(file, "wb") as f:
                f.write(contents)
    @overload
    async def read_file(self, file: str, text: Literal[True] = True) -> str: ...
    @overload
    async def read_file(self, file: str, text: Literal[False]) -> bytes: ...
    @override
    async def read_file(self, file: str, text: bool = True) -> Union[str | bytes]:
        file = self._resolve_file(file)
        verify_read_file_size(file)
        if text:
            with open(file, "r", newline="", encoding="utf-8") as f:
                return f.read()
        else:
            with open(file, "rb") as f:
                return f.read()
    def _resolve_file(self, file: str) -> str:
        path = Path(file)
        if path.is_absolute():
            return file
        else:
            return (Path(self.directory.name) / Path(file)).as_posix()

================
File: src/inspect_ai/util/_sandbox/registry.py
================
from typing import Callable, Type, TypeVar, cast
from inspect_ai._util.registry import (
    RegistryInfo,
    registry_add,
    registry_find,
    registry_name,
    registry_unqualified_name,
)
from .environment import SandboxEnvironment
T = TypeVar("T", bound=SandboxEnvironment)
def sandboxenv(name: str) -> Callable[..., Type[T]]:
    r"""Decorator for registering sandbox environments.
    Args:
        name (str): Name of SandboxEnvironment type
    """
    def wrapper(sandboxenv_type: Type[T] | Callable[..., Type[T]]) -> Type[T]:
        # resolve if its a function
        if not isinstance(sandboxenv_type, type):
            sandboxenv_type = sandboxenv_type()
        # determine name
        sandboxenv_name = registry_name(sandboxenv_type, name)
        # register
        return sandboxenv_register(sandboxenv_type, sandboxenv_name)
    return wrapper
def sandboxenv_register(sandboxenv_type: Type[T], name: str) -> Type[T]:
    registry_add(
        sandboxenv_type,
        RegistryInfo(type="sandboxenv", name=name),
    )
    return sandboxenv_type
def registry_find_sandboxenv(envtype: str) -> type[SandboxEnvironment]:
    # find a matching sandboxenv_type
    sanxboxenv_types = registry_find(registry_match_sandboxenv(envtype))
    if len(sanxboxenv_types) > 0:
        sandboxenv_type = cast(type[SandboxEnvironment], sanxboxenv_types[0])
        return sandboxenv_type
    else:
        raise ValueError(f"SandboxEnvironment type '{envtype}' not recognized.")
def registry_has_sandboxenv(envtype: str) -> bool:
    # see if we have this type
    return len(registry_find(registry_match_sandboxenv(envtype))) > 0
def registry_match_sandboxenv(envtype: str) -> Callable[[RegistryInfo], bool]:
    # check for sandboxenv name matching unqualified name (package prefix not
    # required as sandboxenv providers are registred globally for ease of
    # use from the command line and .env files)
    def match(info: RegistryInfo) -> bool:
        if info.type == "sandboxenv" and registry_unqualified_name(info) == envtype:
            return True
        else:
            return False
    return match

================
File: src/inspect_ai/util/_sandbox/self_check.py
================
from typing import Any, Callable, Coroutine, Generic, Optional, Type, TypeVar
from unittest import mock
import pytest
from inspect_ai.util import (
    OutputLimitExceededError,
    SandboxEnvironment,
    SandboxEnvironmentLimits,
)
# If you're wondering these tests are not using pytest fixtures,
# see the discussion https://github.com/UKGovernmentBEIS/inspect_ai/pull/347
# It's not ideal, so a PR to fix this would be welcome.
#
# If you are struggling to debug a failing one of these, two tips:
# 1. Comment out everything apart from the failing test in the list in the `self_check` function
# 2. Get rid of the try/catch in check_test_fn (the body can just be `await fn(sandbox_env); return True`
async def check_test_fn(
    fn: Callable[[SandboxEnvironment], Coroutine[Any, Any, None]],
    sandbox_env: SandboxEnvironment,
) -> bool | str:
    try:
        await fn(sandbox_env)
        return True
    except AssertionError as e:
        return f"FAILED: [{str(e)}]"
    except Exception as e:
        return f"ERROR: [{repr(e)}]"
async def self_check(sandbox_env: SandboxEnvironment) -> dict[str, bool | str]:
    # Note that these tests reuse the same sandbox environment. This means that
    # if a test fails to clean up after itself, it may affect other tests.
    results = {}
    for fn in [
        test_read_and_write_file_text,
        test_read_and_write_file_binary,
        test_read_and_write_large_file_binary,
        test_write_file_text_utf,
        test_read_and_write_file_including_directory_absolute,
        test_read_and_write_file_including_directory_relative,
        test_read_file_zero_length,
        test_read_file_not_found,
        test_read_file_not_allowed,
        test_read_file_is_directory,
        test_read_file_nonsense_name,
        test_read_file_limit,
        test_write_text_file_zero_length,
        test_write_text_file_space,
        test_write_text_file_is_directory,
        test_write_text_file_without_permissions,
        test_write_text_file_exists,
        test_write_binary_file_zero_length,
        test_write_binary_file_space,
        test_write_binary_file_is_directory,
        test_write_binary_file_without_permissions,
        test_write_binary_file_exists,
        test_exec_output,
        test_exec_stderr,
        test_exec_returncode,
        test_exec_timeout,
        test_exec_permission_error,
        test_exec_as_user,
        test_exec_as_nonexistent_user,
        test_cwd_unspecified,
        test_cwd_custom,
        test_cwd_relative,
        test_cwd_absolute,
        test_exec_stdout_is_limited,
        test_exec_stderr_is_limited,
    ]:
        print(f"self_check: running {fn.__name__}")
        results[fn.__name__] = await check_test_fn(fn, sandbox_env)
    return results
async def _cleanup_file(sandbox_env: SandboxEnvironment, filename: str) -> None:
    res = await sandbox_env.exec(["rm", "-f", "--", filename])
    assert res.success
async def test_read_and_write_file_text(sandbox_env: SandboxEnvironment) -> None:
    file_name = "test_read_and_write_file_text.file"
    await sandbox_env.write_file(file_name, "great #content\nincluding newlines")
    written_file_string = await sandbox_env.read_file(file_name, text=True)
    assert "great #content\nincluding newlines" == written_file_string, (
        f"unexpected content: [{written_file_string}]"
    )
    await _cleanup_file(sandbox_env, file_name)
async def test_write_file_text_utf(sandbox_env: SandboxEnvironment) -> None:
    utf_content = ""
    file_name = "test_write_file_text_utf.file"
    await sandbox_env.write_file(file_name, utf_content)
    file_with_utf_content = await sandbox_env.read_file(file_name, text=True)
    assert isinstance(file_with_utf_content, str), (
        f"Expected file content to be a string, got {type(file_with_utf_content)}"
    )
    assert file_with_utf_content == utf_content, (
        f"UTF-8 content should match, got {file_with_utf_content=}; expected {utf_content=}"
    )
    await _cleanup_file(sandbox_env, file_name)
async def test_read_and_write_file_binary(sandbox_env: SandboxEnvironment) -> None:
    file_name = "test_read_and_write_file_binary.file"
    await sandbox_env.write_file(
        file_name, b"\xc3\x28"
    )  # invalid UTF-8 from https://stackoverflow.com/a/17199164/116509
    written_file_bytes = await sandbox_env.read_file(file_name, text=False)
    assert b"\xc3\x28" == written_file_bytes, "Binary content should match"
    await _cleanup_file(sandbox_env, file_name)
async def test_read_and_write_large_file_binary(
    sandbox_env: SandboxEnvironment,
) -> None:
    file_name = "test_read_and_write_large_file_binary.file"
    long_bytes = b"\xc3" * 5_000_000
    await sandbox_env.write_file(file_name, long_bytes)
    written_file_bytes = await sandbox_env.read_file(file_name, text=False)
    assert long_bytes == written_file_bytes, "Large binary content should match"
    await _cleanup_file(sandbox_env, file_name)
async def test_read_and_write_file_including_directory_absolute(
    sandbox_env: SandboxEnvironment,
) -> None:
    file_name = "/tmp/test_rw_including_directory_absolute/test.file"
    await sandbox_env.write_file(file_name, "absolutely enjoying being in a directory")
    written_file_string = await sandbox_env.read_file(file_name, text=True)
    assert "absolutely enjoying being in a directory" == written_file_string, (
        f"Absolute directory content should match, got {written_file_string=}"
    )
    await _cleanup_file(sandbox_env, file_name)
    await sandbox_env.exec(["rmdir", "/tmp/test_rw_including_directory_absolute"])
async def test_read_and_write_file_including_directory_relative(
    sandbox_env: SandboxEnvironment,
) -> None:
    file_name = "test_rw_including_directory_relative/test.file"
    await sandbox_env.write_file(file_name, "relatively enjoying being in a directory")
    written_file_string = await sandbox_env.read_file(file_name, text=True)
    assert "relatively enjoying being in a directory" == written_file_string, (
        f"Relative directory content should match, got {written_file_string=}"
    )
    await _cleanup_file(sandbox_env, file_name)
    await sandbox_env.exec(["rmdir", "test_rw_including_directory_relative"])
async def test_read_file_zero_length(sandbox_env: SandboxEnvironment) -> None:
    file_name = "zero_length_file.file"
    await sandbox_env.exec(["touch", file_name])
    zero_length = await sandbox_env.read_file(file_name, text=True)
    assert isinstance(zero_length, str), (
        f"Zero-length file should return a string, got {type(zero_length)}"
    )
    assert zero_length == "", (
        f"Zero-length file should be an empty string, got {zero_length=}"
    )
    await _cleanup_file(sandbox_env, file_name)
async def test_read_file_not_found(sandbox_env: SandboxEnvironment) -> None:
    file_name = "nonexistent"
    with Raises(FileNotFoundError) as e_info:
        await sandbox_env.read_file(file_name, text=True)
    assert e_info is not None, "FileNotFoundError should be raised"
    assert file_name in str(e_info.value), (
        f"FileNotFoundError should contain the filename, got {e_info.value=}"
    )
async def test_read_file_not_allowed(sandbox_env: SandboxEnvironment) -> None:
    file_name = "test_read_file_not_allowed.file"
    await sandbox_env.write_file(file_name, "inaccessible #content")
    await sandbox_env.exec(["chmod", "-r", file_name])
    with Raises(PermissionError) as e_info:
        await sandbox_env.read_file(file_name, text=True)
    assert e_info is not None, "PermissionError should be raised"
    assert file_name in str(e_info.value), (
        f"PermissionError should contain the filename, got {e_info.value=}"
    )
    await sandbox_env.exec(["chmod", "+r", file_name])
    await _cleanup_file(sandbox_env, file_name)
async def test_read_file_is_directory(sandbox_env: SandboxEnvironment) -> None:
    file_name = "/etc"
    with Raises(IsADirectoryError) as e_info:
        await sandbox_env.read_file(file_name, text=True)
        assert e_info is not None, "IsADirectoryError should be raised"
    assert "directory" in str(e_info.value), (
        f"IsADirectoryError should mention 'directory', got {e_info.value=}"
    )
async def test_read_file_nonsense_name(
    sandbox_env: SandboxEnvironment,
) -> None:
    file_name = "https:/en.wikipedia.org/wiki/Bart%C5%82omiej_Kasprzykowski"
    with Raises(FileNotFoundError) as e_info:
        await sandbox_env.read_file(file_name, text=True)
    assert e_info is not None, "FileNotFoundError should be raised"
    assert "wikipedia" in str(e_info.value), (
        f"FileNotFoundError should contain the filename, got {e_info.value=}"
    )
async def test_read_file_limit(sandbox_env: SandboxEnvironment) -> None:
    file_name = "large.file"
    await sandbox_env.write_file(file_name, "a" * 2048)  # 2 KiB
    # Patch limit down to 1KiB for the test to save us from writing a 100 MiB file.
    with mock.patch.object(SandboxEnvironmentLimits, "MAX_READ_FILE_SIZE", 1024):
        with Raises(OutputLimitExceededError) as e_info:
            await sandbox_env.read_file(file_name, text=True)
    assert e_info is not None, "OutputLimitExceededError should be raised"
    assert "limit of 100 MiB was exceeded" in str(e_info.value), (
        f"OutputLimitExceededError should mention the limit, got {e_info.value=}"
    )
    await _cleanup_file(sandbox_env, file_name)
async def test_write_text_file_zero_length(sandbox_env: SandboxEnvironment) -> None:
    file_name = "zero_length_file.file"
    await sandbox_env.write_file(file_name, "")
    zero_length = await sandbox_env.read_file(file_name, text=True)
    assert isinstance(zero_length, str), (
        f"Zero-length file should return a string, got {type(zero_length)}"
    )
    assert zero_length == "", (
        f"Zero-length file should be an empty string, got {zero_length=}"
    )
    await _cleanup_file(sandbox_env, file_name)
async def test_write_text_file_space(sandbox_env: SandboxEnvironment) -> None:
    space = "to the moon"
    file_name = "file with space.file"
    await sandbox_env.write_file(file_name, space)
    file_with_space = await sandbox_env.read_file(file_name, text=True)
    assert isinstance(file_with_space, str), (
        f"File with space should return a string, got {type(file_with_space)}"
    )
    assert file_with_space == space, (
        f"File with space content should match, got {file_with_space=}; expected {space=}"
    )
    await _cleanup_file(sandbox_env, file_name)
async def test_write_text_file_is_directory(
    sandbox_env: SandboxEnvironment,
) -> None:
    # ensure /tmp/directory exists
    await sandbox_env.write_file(
        "/tmp/inspect_ai_test_write_text_file_is_directory/file", "unused content"
    )
    with Raises(IsADirectoryError) as e_info:
        await sandbox_env.write_file(
            "/tmp/inspect_ai_test_write_text_file_is_directory",
            "content cannot go in a directory, dummy",
        )
    assert e_info is not None, "IsADirectoryError should be raised"
    assert "directory" in str(e_info.value), (
        f"IsADirectoryError should mention 'directory', got {e_info.value=}"
    )
    await sandbox_env.exec(
        ["rm", "-rf", "/tmp/inspect_ai_test_write_text_file_is_directory"]
    )
async def test_write_text_file_without_permissions(
    sandbox_env: SandboxEnvironment,
) -> None:
    file_name = "test_write_text_file_without_permissions.file"
    await sandbox_env.write_file(file_name, "impervious #content")
    await sandbox_env.exec(["chmod", "-w", file_name])
    with Raises(PermissionError) as e_info:
        await sandbox_env.write_file(file_name, "this won't stick")
    assert e_info is not None, "PermissionError should be raised"
    assert file_name in str(e_info.value), (
        f"PermissionError should contain the filename, got {e_info.value=}"
    )
    await sandbox_env.exec(["chmod", "+w", file_name])
    await _cleanup_file(sandbox_env, file_name)
async def test_write_text_file_exists(
    sandbox_env: SandboxEnvironment,
) -> None:
    file_name = "file_exists.file"
    await sandbox_env.write_file(file_name, "mundane content")
    await sandbox_env.write_file(file_name, "altered content")
    altered_content = await sandbox_env.read_file(file_name, text=True)
    assert altered_content == "altered content", (
        f"Existing file content should be overwritten, got {altered_content=}"
    )
    await _cleanup_file(sandbox_env, file_name)
async def test_write_binary_file_zero_length(sandbox_env: SandboxEnvironment) -> None:
    file_name = "zero_length_file.file"
    await sandbox_env.write_file(file_name, b"")
    zero_length = await sandbox_env.read_file(file_name, text=False)
    assert isinstance(zero_length, bytes), (
        f"Zero-length file should return bytes, got {type(zero_length)}"
    )
    assert zero_length == b"", (
        f"Zero-length file should be empty bytes, got {zero_length=}"
    )
    await _cleanup_file(sandbox_env, file_name)
async def test_write_binary_file_space(sandbox_env: SandboxEnvironment) -> None:
    binary_content = b"\xc3\x28"
    file_name = "file with space.file"
    await sandbox_env.write_file(file_name, binary_content)
    file_with_space = await sandbox_env.read_file(file_name, text=False)
    assert isinstance(file_with_space, bytes), (
        f"File with space should return bytes, got {type(file_with_space)}"
    )
    assert file_with_space == binary_content, "File with space content should match"
    await _cleanup_file(sandbox_env, file_name)
async def test_write_binary_file_is_directory(
    sandbox_env: SandboxEnvironment,
) -> None:
    # ensure /tmp/directory exists
    await sandbox_env.write_file(
        "/tmp/inspect_ai_test_write_binary_file_is_directory/file", "unused content"
    )
    with Raises(IsADirectoryError) as e_info:
        await sandbox_env.write_file(
            "/tmp/inspect_ai_test_write_binary_file_is_directory",
            b"\xc3\x28",
        )
    assert e_info is not None, "IsADirectoryError should be raised"
    assert "directory" in str(e_info.value), (
        f"IsADirectoryError should mention 'directory', got {e_info.value=}"
    )
    await sandbox_env.exec(
        ["rm", "-rf", "/tmp/inspect_ai_test_write_binary_file_is_directory"]
    )
async def test_write_binary_file_without_permissions(
    sandbox_env: SandboxEnvironment,
) -> None:
    file_name = "test_write_binary_file_without_permissions.file"
    await sandbox_env.write_file(file_name, "impervious #content")
    await sandbox_env.exec(["chmod", "-w", file_name])
    with Raises(PermissionError) as e_info:
        await sandbox_env.write_file(file_name, b"\xc3\x28")
    assert e_info is not None, "PermissionError should be raised"
    assert file_name in str(e_info.value), (
        f"PermissionError should contain the filename, got {e_info.value=}"
    )
    await sandbox_env.exec(["chmod", "+w", file_name])
    await _cleanup_file(sandbox_env, file_name)
async def test_write_binary_file_exists(
    sandbox_env: SandboxEnvironment,
) -> None:
    file_name = "file_exists.file"
    await sandbox_env.write_file(file_name, b"\xc3\x28")
    await sandbox_env.write_file(file_name, b"\xc3\x29")
    altered_content = await sandbox_env.read_file(file_name, text=False)
    assert altered_content == b"\xc3\x29", "Existing file content should be overwritten"
    await _cleanup_file(sandbox_env, file_name)
async def test_exec_output(sandbox_env: SandboxEnvironment) -> None:
    exec_result = await sandbox_env.exec(["sh", "-c", "echo foo; echo bar"])
    expected = "foo\nbar\n"
    # in the assertion message, we show the actual bytes to help debug newline issues
    assert exec_result.stdout == expected, (
        f"Unexpected output:expected {expected.encode('UTF-8')!r}; got {exec_result.stdout.encode('UTF-8')!r}"
    )
async def test_exec_stderr(sandbox_env: SandboxEnvironment) -> None:
    exec_result = await sandbox_env.exec(["sh", "-c", "echo boof; echo baz >&2"])
    assert exec_result.stderr == "baz\n", (
        f"stderr output should match; got {exec_result.stderr=}, expected 'baz\n'"
    )
async def test_exec_returncode(sandbox_env: SandboxEnvironment) -> None:
    exec_result = await sandbox_env.exec(["sh", "-c", "echo foo; exit 70"])
    assert exec_result.returncode == 70, (
        f"Return code should match, got {exec_result.returncode=}, expected 70"
    )
async def test_exec_timeout(sandbox_env: SandboxEnvironment) -> None:
    with Raises(TimeoutError):
        await sandbox_env.exec(["sleep", "4"], timeout=2)
async def test_exec_permission_error(sandbox_env: SandboxEnvironment) -> None:
    with Raises(PermissionError):
        # /etc/password is not an executable file so this should fail
        await sandbox_env.exec(["/etc/passwd"])
async def test_exec_as_user(sandbox_env: SandboxEnvironment) -> None:
    username = "inspect-ai-test-exec-as-user"
    # Neither adduser nor useradd are part of POSIX, so we need some brittle logic here
    adduser_help_exec_result = await sandbox_env.exec(["adduser", "--help"])
    adduser_help_text = (
        adduser_help_exec_result.stdout + adduser_help_exec_result.stderr
    )
    if "BusyBox" in adduser_help_text:
        adduser_command = ["adduser", "-D", username]
    else:
        adduser_command = [
            "adduser",
            "--comment",
            "self_check.py",
            "--disabled-password",
            username,
        ]
    try:
        # Create a new user
        add_user_result = await sandbox_env.exec(
            adduser_command,
            user="root",
            timeout=10,  # in one case adduser decided to ask for input which caused the test to hang indefinitely
        )
        assert add_user_result.success, f"Failed to add user: {add_user_result.stderr}"
        # Test exec as different users
        root_result = await sandbox_env.exec(["whoami"], user="root")
        assert root_result.stdout.strip() == "root", (
            f"Expected 'root', got '{root_result.stdout.strip()}'"
        )
        myuser_result = await sandbox_env.exec(["whoami"], user=username)
        assert myuser_result.stdout.strip() == username, (
            f"Expected '{username}', got '{myuser_result.stdout.strip()}'"
        )
    finally:
        # Clean up
        await sandbox_env.exec(["userdel", "-r", username], user="root")
async def test_exec_as_nonexistent_user(sandbox_env: SandboxEnvironment) -> None:
    nonexistent_username = "nonexistent"
    result = await sandbox_env.exec(["whoami"], user=nonexistent_username)
    assert not result.success, "Command should have failed for nonexistent user"
    assert (
        nonexistent_username in result.stdout or nonexistent_username in result.stderr
    ), (
        f"Error not found in command output: '{result.stdout}' nor stderr '{result.stderr}"
    )
async def test_cwd_unspecified(sandbox_env: SandboxEnvironment) -> None:
    file_name = "test_cwd_unspecified.file"
    await sandbox_env.write_file(file_name, "ls me plz")
    current_dir_contents = (await sandbox_env.exec(["ls", "-1"])).stdout
    assert file_name in current_dir_contents, (
        f"File should be in current directory contents; got {current_dir_contents=}"
    )
    await _cleanup_file(sandbox_env, file_name)
async def test_cwd_custom(sandbox_env: SandboxEnvironment) -> None:
    current_dir_contents = (await sandbox_env.exec(["ls"], cwd="/usr/bin")).stdout
    assert "env" in current_dir_contents, (
        f"env should be in /usr/bin; got {current_dir_contents=}"
    )
async def test_cwd_relative(sandbox_env: SandboxEnvironment) -> None:
    cwd_subdirectory = "subdir"
    await sandbox_env.exec(["mkdir", cwd_subdirectory])
    file_name = "test_cwd_relative.file"
    file_path = cwd_subdirectory + "/" + file_name
    await sandbox_env.write_file(file_path, "ls me plz")
    current_dir_contents = (await sandbox_env.exec(["ls"], cwd=cwd_subdirectory)).stdout
    assert file_name in current_dir_contents, (
        f"{file_name} not found in {current_dir_contents}"
    )
    await _cleanup_file(sandbox_env, file_path)
async def test_cwd_absolute(sandbox_env: SandboxEnvironment) -> None:
    cwd_directory = "/tmp/test_cwd_absolute"
    await sandbox_env.exec(["mkdir", cwd_directory])
    file_name = "/tmp/test_cwd_absolute/test_cwd_absolute.file"
    await sandbox_env.write_file(file_name, "ls me plz")
    current_dir_contents = (await sandbox_env.exec(["ls"], cwd=cwd_directory)).stdout
    assert "test_cwd_absolute.file" in current_dir_contents, (
        f"File should be in current directory contents, got {current_dir_contents=}"
    )
    await _cleanup_file(sandbox_env, file_name)
    await sandbox_env.exec(["rmdir", cwd_directory])
async def test_exec_stdout_is_limited(sandbox_env: SandboxEnvironment) -> None:
    output_size = 10 * 1024**2 + 1024  # 10 MiB + 1 KiB
    with pytest.raises(OutputLimitExceededError) as e_info:
        await sandbox_env.exec(["sh", "-c", f"yes | head -c {output_size}"])
    assert e_info is not None, "OutputLimitExceededError should be raised"
    assert "limit of 10 MiB was exceeded" in str(e_info.value), (
        "OutputLimitExceededError should mention the limit; got {e_info.value=}"
    )
    truncated_output = e_info.value.truncated_output
    # `yes` outputs 'y\n' (ASCII) so the size equals the string length.
    # some shells additionally output 'canceled\n' so we add fudge factor for that
    assert truncated_output and (len(truncated_output) - 10 * 1024**2) < 10, (
        f"output not truncated or wrong length; start of truncated output = {'' if not truncated_output else truncated_output[:10]}; len(truncated_output): {'n/a' if not truncated_output else len(truncated_output)}"
    )
async def test_exec_stderr_is_limited(sandbox_env: SandboxEnvironment) -> None:
    output_size = 10 * 1024**2 + 1024  # 10 MiB + 1 KiB
    with pytest.raises(OutputLimitExceededError) as e_info:
        await sandbox_env.exec(["sh", "-c", f"yes | head -c {output_size} 1>&2"])
    assert e_info is not None, "OutputLimitExceededError should be raised"
    assert "limit of 10 MiB was exceeded" in str(e_info.value), (
        "OutputLimitExceededError should mention the limit; got {e_info.value=}"
    )
    truncated_output = e_info.value.truncated_output
    assert (
        truncated_output
        and truncated_output[0] == "y"
        and len(truncated_output) <= 10 * 1024**2
        and len(truncated_output) > 0
    ), (
        f"output not truncated or wrong length; start of truncated output = {'' if not truncated_output else truncated_output[:10]}; len(truncated_output): {'n/a' if not truncated_output else len(truncated_output)}"
    )
# TODO: write a test for when cwd doesn't exist
# Generic type variable for exceptions
E = TypeVar("E", bound=BaseException)
class Raises(Generic[E]):
    def __init__(self, expected_exception: Type[E]):
        self.expected_exception = expected_exception
        self.value: Optional[E] = None  # Store the caught exception
    def __enter__(self) -> "Raises[E]":
        return self
    def __exit__(
        self,
        exc_type: Optional[Type[BaseException]],
        exc_value: Optional[BaseException],
        traceback: Optional[Any],
    ) -> bool:
        if exc_type is None:
            raise AssertionError(
                f"Expected exception {self.expected_exception.__name__} but no exception was raised."
            )
        if not issubclass(exc_type, self.expected_exception):
            raise AssertionError(
                f"Expected exception {self.expected_exception.__name__}, but got {exc_type.__name__}."
            )
        self.value = exc_value  # type: ignore
        return True

================
File: src/inspect_ai/util/_sandbox/service.py
================
import json
from logging import getLogger
from pathlib import PurePosixPath
from textwrap import dedent
from typing import (
    Awaitable,
    Callable,
    cast,
)
import anyio
from pydantic import JsonValue
from inspect_ai._util._async import coro_log_exceptions
from inspect_ai.util._subprocess import ExecResult
from .environment import SandboxEnvironment
logger = getLogger(__name__)
REQUESTS_DIR = "requests"
RESPONSES_DIR = "responses"
SERVICES_DIR = "/var/tmp/sandbox-services"
ID = "id"
METHOD = "method"
PARAMS = "params"
ERROR = "error"
RESULT = "result"
POLLING_INTERVAL = 0.1
SandboxServiceMethod = Callable[..., Awaitable[JsonValue]]
async def sandbox_service(
    name: str,
    methods: list[SandboxServiceMethod] | dict[str, SandboxServiceMethod],
    until: Callable[[], bool],
    sandbox: SandboxEnvironment,
) -> None:
    """Run a service that is callable from within a sandbox.
    Args:
        name (str): Service name
        methods (dict[str, SandboxServiceMethod]): Service methods.
        until (Callable[[], bool]): Function used to check whether
          the service should stop.
        sandbox (SandboxEnvironment): Sandbox to publish service to.
    """
    # setup and start service
    service = SandboxService(name, sandbox)
    if isinstance(methods, list):
        methods = {v.__name__: v for v in methods}
    for name, method in methods.items():
        service.add_method(name, method)
    await service.start()
    # wait for and process methods
    while not until():
        await anyio.sleep(POLLING_INTERVAL)
        await service.handle_requests()
class SandboxService:
    """Sandbox service.
    Service that makes available a set of methods to a sandbox
    for calling back into the main Inspect process.
    To use the service from within a sandbox, either add it to the sys path
    or use importlib. For example, if the service is named 'foo':
    ```python
    import sys
    sys.path.append("/var/tmp/sandbox-services/foo")
    import foo
    ```
    Or:
    ```python
    import importlib.util
    spec = importlib.util.spec_from_file_location(
        "foo", "/var/tmp/sandbox-services/foo/foo.py"
    )
    foo = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(foo)
    ```
    """
    def __init__(self, name: str, sandbox: SandboxEnvironment) -> None:
        """Create a SandboxService.
        Args:
            name (str): Service name
            sandbox (SandboxEnvironment): Sandbox to publish service to.
        """
        self._name = name
        self._sandbox = sandbox
        self._service_dir = PurePosixPath(SERVICES_DIR, self._name)
        self._methods: dict[str, SandboxServiceMethod] = {}
        self._requests_dir: str = ""
        self._responses_dir: str = ""
        self._client_script: str = ""
    def add_method(self, name: str, method: SandboxServiceMethod) -> None:
        """Add a method to the service.
        Args:
            name (str): Method name.
            method (SandboxServiceMethod): Function that implements method.
        """
        self._methods[name] = method
    async def start(self) -> None:
        """Start running the service."""
        # requests dir
        assert not self._requests_dir
        self._requests_dir = await self._create_rpc_dir(REQUESTS_DIR)
        # responses dir
        assert not self._responses_dir
        self._responses_dir = await self._create_rpc_dir(RESPONSES_DIR)
        # client script
        assert not self._client_script
        client_script = PurePosixPath(self._service_dir, f"{self._name}.py").as_posix()
        client_code = self._generate_client()
        await self._write_text_file(client_script, client_code)
        self._client_script = client_script
    async def handle_requests(self) -> None:
        """Handle all pending service requests."""
        # list pending requests
        list_requests = f"ls -1 {self._requests_dir}/*.json"
        result = await self._exec(["bash", "-c", list_requests])
        # process requests
        if result.success:
            request_files = result.stdout.strip().splitlines()
            if request_files:
                async with anyio.create_task_group() as tg:
                    for file in request_files:
                        tg.start_soon(
                            coro_log_exceptions,
                            logger,
                            "handling sandbox service request",
                            self._handle_request,
                            file,
                        )
    async def _handle_request(self, request_file: str) -> None:
        # read request
        read_request = f"cat {request_file}"
        result = await self._exec(["bash", "-c", read_request])
        if not result.success:
            raise RuntimeError(
                f"Error reading request for service {self._name}: '{read_request}' ({result.stderr})"
            )
        # parse request (decode error could occur if its incomplete so bypass this)
        try:
            request_data = json.loads(result.stdout)
        except json.JSONDecodeError:
            logger.warning(
                f"JSON decoding error reading service request: {result.stdout}"
            )
            return None
        if not isinstance(request_data, dict):
            raise TypeError(f"Service request is not a dict (type={request_data})")
        # read id (after we have this we can write responses)
        request_id = request_data.get(ID, None)
        if not isinstance(request_id, str):
            raise TypeError(
                f"Service request id is not a string (type={type(request_id)})"
            )
        # helpers to write responses and errors
        async def write_response(
            result: JsonValue | None, error: str | None = None
        ) -> None:
            # form response payload
            response_data = {
                ID: request_id,
                RESULT: result,
                ERROR: error,
            }
            # compute response path
            response_path = PurePosixPath(
                self._responses_dir, f"{request_id}.json"
            ).as_posix()
            # write response
            await self._write_text_file(response_path, json.dumps(response_data))
            # remove request file
            exec_rm = await self._exec(["rm", "-f", request_file])
            if not exec_rm.success:
                raise RuntimeError(
                    f"Error removing request file '{request_file}': {exec_rm.stderr}"
                )
        async def write_error_response(error: str) -> None:
            await write_response(None, error)
        # read and validate params
        method_name = request_data.get(METHOD, None)
        params = request_data.get(PARAMS, None)
        if not isinstance(method_name, str):
            await write_error_response(
                f"Service {METHOD} not passed or not a string (type={type(method_name)})"
            )
        elif method_name not in self._methods:
            await write_error_response(f"Unknown method '{method_name}'")
        elif not isinstance(params, dict):
            await write_error_response(
                f"{PARAMS} not passed or not a dict (type={params})"
            )
        # all clear, call the method
        else:
            try:
                params = cast(dict[str, JsonValue], request_data.get(PARAMS))
                method = self._methods[method_name]
                await write_response(await method(**params))
            except Exception as err:
                await write_error_response(f"Error calling method {method_name}: {err}")
    async def _create_rpc_dir(self, name: str) -> str:
        rpc_dir = PurePosixPath(self._service_dir, name).as_posix()
        result = await self._exec(["rm", "-rf", rpc_dir])
        result = await self._exec(["mkdir", "-p", rpc_dir])
        if not result.success:
            raise RuntimeError(
                f"Error creating rpc directory '{name}' for sandbox '{self._name}': {result.stderr}"
            )
        return rpc_dir
    async def _write_text_file(self, file: str, contents: str) -> None:
        result = await self._exec(["tee", "--", file], input=contents)
        if not result.success:
            msg = f"Failed to write file '{file}' into container: {result.stderr}"
            raise RuntimeError(msg)
    async def _exec(self, cmd: list[str], input: str | None = None) -> ExecResult[str]:
        try:
            return await self._sandbox.exec(cmd, input=input, timeout=30)
        except TimeoutError:
            raise RuntimeError(
                f"Timed out executing command {' '.join(cmd)} in sandbox"
            )
    def _generate_client(self) -> str:
        return dedent(f"""
        from typing import Any
        def call_{self._name}(method: str, **params: Any) -> Any:
            from time import sleep
            request_id = _write_{self._name}_request(method, **params)
            while True:
                sleep({POLLING_INTERVAL})
                success, result = _read_{self._name}_response(request_id)
                if success:
                    return result
        async def call_{self._name}_async(method: str, **params: Any) -> Any:
            from asyncio import sleep
            request_id = _write_{self._name}_request(method, **params)
            while True:
                await sleep({POLLING_INTERVAL})
                success, result = _read_{self._name}_response(request_id)
                if success:
                    return result
        def _write_{self._name}_request(method: str, **params: Any) -> str:
            from json import dump
            from pathlib import Path
            from uuid import uuid4
            requests_dir = Path("{SERVICES_DIR}", "{self._name}", "{REQUESTS_DIR}")
            request_id = str(uuid4())
            request_data = dict({ID}=request_id, {METHOD}=method, {PARAMS}=params)
            request_path = requests_dir / (request_id + ".json")
            with open(request_path, "w") as f:
                dump(request_data, f)
            return request_id
        def _read_{self._name}_response(request_id: str) -> tuple[bool, Any]:
            from json import JSONDecodeError, load
            from pathlib import Path
            responses_dir = Path("{SERVICES_DIR}", "{self._name}", "{RESPONSES_DIR}")
            response_path = responses_dir / (request_id + ".json")
            if response_path.exists():
                # read and remove the file
                with open(response_path, "r") as f:
                    # it's possible the file is still being written so
                    # just catch and wait for another retry if this occurs
                    try:
                        response = load(f)
                    except JSONDecodeError:
                        return False, None
                response_path.unlink()
                # raise error if we have one
                if response.get("{ERROR}", None) is not None:
                    raise Exception(response["{ERROR}"])
                # return response if we have one
                elif "{RESULT}" in response:
                    return True, response["{RESULT}"]
                # invalid response
                else:
                    raise RuntimeError(
                        "No {ERROR} or {RESULT} field in response for method " + method
                    )
            else:
                return False, None
        """)

================
File: src/inspect_ai/util/_store_model.py
================
from typing import Any, Type, TypeVar
from pydantic import BaseModel, ConfigDict, Field
from ._store import Store, store
class StoreModel(BaseModel):
    """Store backed Pydandic BaseModel.
    The model is initialised from a Store, so that Store should
    either already satisfy the validation constraints of the model
    OR you should provide Field(default=) annotations for all of
    your model fields (the latter approach is recommended).
    """
    store: Store = Field(exclude=True, default_factory=store)
    def model_post_init(self, __context: Any) -> None:
        for name in self.model_fields.keys():
            if name == "store":
                continue
            # if its in the store, then have our dict reflect that
            ns_name = self._ns_name(name)
            if ns_name in self.store:
                self.__dict__[name] = self.store.get(ns_name)
            # if its not in the store, then reflect dict into store
            elif name in self.__dict__.keys():
                self.store.set(ns_name, self.__dict__[name])
    def __getattribute__(self, name: str) -> Any:
        # sidestep dunders and pydantic fields
        if name.startswith("__") or name.startswith("model_"):
            return object.__getattribute__(self, name)
        # handle model_fields (except 'store') by reading the store
        elif name in object.__getattribute__(self, "model_fields") and name != "store":
            store_key = self._ns_name(name)
            if store_key in self.store:
                return self.store.get(store_key)
            else:
                return object.__getattribute__(self, name)
        # default to super
        else:
            return super().__getattribute__(name)
    def __setattr__(self, name: str, value: Any) -> None:
        if name in self.model_fields:
            # validate with the new value (can throw ValidationError)
            temp_data = self.store._data.copy()
            temp_data[self._ns_name(name)] = value
            self._validate_store(temp_data)
            # update the store and sync the underlying __dict__
            self.store.set(self._ns_name(name), value)
            self.__dict__[name] = value
        else:
            super().__setattr__(name, value)
    def model_dump(self, *args: Any, **kwargs: Any) -> dict[str, Any]:
        self._sync_model()  # in case store was updated behind our back
        return super().model_dump(*args, **kwargs)
    def model_dump_json(self, *args: Any, **kwargs: Any) -> str:
        self._sync_model()  # in case store was updated behind our back
        return super().model_dump_json(*args, **kwargs)
    def _sync_model(self) -> None:
        self._validate_store()
        for field_name in self.model_fields.keys():
            if field_name == "store":
                continue
            store_value = self.store.get(self._ns_name(field_name))
            self.__dict__[field_name] = store_value
    def _validate_store(self, data: dict[str, Any] | None = None) -> None:
        # validate store or custom dict
        data = data if data is not None else self.store._data
        # pick out keys to validate
        validate: dict[str, Any] = {}
        for k, v in data.items():
            if k.startswith(f"{self.__class__.__name__}:"):
                unprefixed = self._un_ns_name(k)
                validate[unprefixed] = v
        # perform validation
        self.__class__.model_validate(validate)
    def _ns_name(self, name: str) -> str:
        return f"{self.__class__.__name__}:{name}"
    def _un_ns_name(self, name: str) -> str:
        return name.replace(f"{self.__class__.__name__}:", "", 1)
    model_config = ConfigDict(arbitrary_types_allowed=True)
SMT = TypeVar("SMT", bound=StoreModel)
def store_as(model_cls: Type[SMT]) -> SMT:
    """Get a Pydantic model interface to the store.
    Args:
      model_cls: Pydantic model type (must derive from StoreModel)
    Returns:
      StoreModel: Instance of model_cls bound to current Store.
    """
    return model_cls(store=store())

================
File: src/inspect_ai/util/_store.py
================
from contextvars import ContextVar
from copy import deepcopy
from typing import (
    Any,
    ItemsView,
    KeysView,
    TypeVar,
    ValuesView,
    cast,
    overload,
)
from pydantic_core import to_jsonable_python
from inspect_ai._util.json import JsonChange, json_changes
VT = TypeVar("VT")
class Store:
    """The `Store` is used to record state and state changes.
    The `TaskState` for each sample has a `Store` which can be
    used when solvers and/or tools need to coordinate changes
    to shared state. The `Store` can be accessed directly from
    the `TaskState` via `state.store` or can be accessed using
    the `store()` global function.
    Note that changes to the store that occur are automatically
    recorded to transcript as a `StoreEvent`. In order to be
    serialised to the transcript, values and objects must be
    JSON serialisable (you can make objects with several fields
    serialisable using the `@dataclass` decorator or by
    inheriting from Pydantic `BaseModel`)
    """
    def __init__(self, data: dict[str, Any] | None = None) -> None:
        self._data = deepcopy(data) if data else {}
    @overload
    def get(self, key: str, default: None = None) -> Any: ...
    @overload
    def get(self, key: str, default: VT) -> VT: ...
    def get(self, key: str, default: VT | None = None) -> VT | Any:
        """Get a value from the store.
        Provide a `default` to automatically initialise a named
        store value with the default when it does not yet exist.
        Args:
           key (str): Name of value to get
           default (VT | None): Default value (defaults to `None`)
        Returns:
           Value if is exists, otherwise default.
        """
        if default is not None:
            if key not in self._data.keys():
                self._data[key] = default
        return cast(VT, self._data.get(key, default))
    def set(self, key: str, value: Any) -> None:
        """Set a value into the store.
        Args:
           key (str): Name of value to set
           value (Any): Value to set
        """
        self._data[key] = value
    def delete(self, key: str) -> None:
        """Remove a value from the store.
        Args:
           key (str): Name of value to remove
        """
        del self._data[key]
    def keys(self) -> KeysView[str]:
        """View of keys within the store."""
        return self._data.keys()
    def values(self) -> ValuesView[Any]:
        """View of values within the store."""
        return self._data.values()
    def items(self) -> ItemsView[str, Any]:
        """View of items within the store."""
        return self._data.items()
    def __contains__(self, key: object) -> bool:
        return key in self._data
    def __eq__(self, other: object) -> bool:
        return self._data.__eq__(other)
    def __ne__(self, value: object) -> bool:
        return self._data.__ne__(value)
def store() -> Store:
    """Get the currently active `Store`."""
    return _subtask_store.get()
def init_subtask_store(store: Store) -> None:
    _subtask_store.set(store)
_subtask_store: ContextVar[Store] = ContextVar("subtask_store", default=Store())
def store_changes(
    before: Store | dict[str, Any], after: Store | dict[str, Any]
) -> list[JsonChange] | None:
    if isinstance(before, Store):
        before = store_jsonable(before)
    if isinstance(after, Store):
        after = store_jsonable(after)
    return json_changes(before, after)
def store_jsonable(store: Store) -> dict[str, Any]:
    return deepcopy(dict_jsonable(store._data))
def dict_jsonable(data: dict[str, Any]) -> dict[str, Any]:
    return cast(
        dict[str, Any],
        to_jsonable_python(data, exclude_none=True, fallback=lambda _x: None),
    )

================
File: src/inspect_ai/util/_subprocess.py
================
import functools
import io
import os
import shlex
from contextlib import aclosing
from contextvars import ContextVar
from dataclasses import dataclass
from logging import getLogger
from pathlib import Path
from subprocess import DEVNULL, PIPE
from typing import AsyncGenerator, Generic, Literal, TypeVar, Union, cast, overload
import anyio
from anyio import open_process
from anyio.abc import ByteReceiveStream, Process
from inspect_ai._util._async import tg_collect
from inspect_ai._util.trace import trace_action
from ._concurrency import concurrency
logger = getLogger(__name__)
T = TypeVar("T", str, bytes)
@dataclass
class ExecResult(Generic[T]):
    """Execution result from call to `subprocess()`."""
    success: bool
    """Did the process exit with success."""
    returncode: int
    """Return code from process exit."""
    stdout: T
    """Contents of stdout."""
    stderr: T
    """Contents of stderr."""
@overload
# type: ignore
async def subprocess(
    args: str | list[str],
    text: Literal[True] = True,
    input: str | bytes | memoryview | None = None,
    cwd: str | Path | None = None,
    env: dict[str, str] = {},
    capture_output: bool = True,
    output_limit: int | None = None,
    timeout: int | None = None,
) -> ExecResult[str]: ...
@overload
async def subprocess(
    args: str | list[str],
    text: Literal[False] = False,
    input: str | bytes | memoryview | None = None,
    cwd: str | Path | None = None,
    env: dict[str, str] = {},
    capture_output: bool = True,
    output_limit: int | None = None,
    timeout: int | None = None,
) -> ExecResult[bytes]: ...
async def subprocess(
    args: str | list[str],
    text: bool = True,
    input: str | bytes | memoryview | None = None,
    cwd: str | Path | None = None,
    env: dict[str, str] = {},
    capture_output: bool = True,
    output_limit: int | None = None,
    timeout: int | None = None,
) -> Union[ExecResult[str], ExecResult[bytes]]:
    """Execute and wait for a subprocess.
    Convenience method for solvers, scorers, and tools to launch
    subprocesses. Automatically enforces a limit on concurrent
    subprocesses (defaulting to os.cpu_count() but controllable
    via the `max_subprocesses` eval config option).
    Args:
       args (str | list[str]): Command and arguments to execute.
       text (bool): Return stdout and stderr as text (defaults to True)
       input (str | bytes | memoryview | None): Optional stdin
          for subprocess.
       cwd (str | Path | None): Switch to directory for execution.
       env (dict[str, str]): Additional environment variables.
       capture_output (bool): Capture stderr and stdout into ExecResult
          (if False, then output is redirected to parent stderr/stdout)
       output_limit (int | None): Stop reading output if it exceeds
          the specified limit (in bytes).
       timeout (int | None): Timeout. If the timeout expires then
          a `TimeoutError` will be raised.
    Returns:
       Subprocess result (text or binary depending on `text` param)
    Raises:
       TimeoutError: If the specified `timeout` expires.
    """
    # resolve input
    input = (
        input.encode()
        if isinstance(input, str)
        else bytes(input)
        if input is not None
        else None
    )
    async def run_command() -> AsyncGenerator[
        Union[Process, ExecResult[str], ExecResult[bytes]], None
    ]:
        async with await open_process(
            args,
            stdin=PIPE if input else DEVNULL,
            stdout=PIPE if capture_output else None,
            stderr=PIPE if capture_output else None,
            cwd=cwd,
            env={**os.environ, **env},
        ) as process:
            # yield the process so the caller has a handle to it
            yield process
            # write to stdin (convert input to bytes)
            if process.stdin and input:
                await process.stdin.send(input)
                await process.stdin.aclose()
            # read streams incrementally so we can check output limits
            async def read_stream(stream: ByteReceiveStream | None) -> bytes:
                # return early for no stream
                if stream is None:
                    return bytes()
                written = 0
                buffer = io.BytesIO()
                async for chunk in stream:
                    buffer.write(chunk)
                    written += len(chunk)
                    if output_limit is not None and written > output_limit:
                        process.kill()
                        break
                return buffer.getvalue()
            stdout, stderr = await tg_collect(
                [
                    functools.partial(read_stream, process.stdout),
                    functools.partial(read_stream, process.stderr),
                ]
            )
            returncode = await process.wait()
            success = returncode == 0
            if text:
                yield ExecResult[str](
                    success=success,
                    returncode=returncode,
                    stdout=stdout.decode() if capture_output else "",
                    stderr=stderr.decode() if capture_output else "",
                )
            else:
                yield ExecResult[bytes](
                    success=success,
                    returncode=returncode,
                    stdout=stdout if capture_output else bytes(),
                    stderr=stderr if capture_output else bytes(),
                )
    # wrapper for run command that implements timeout
    async def run_command_timeout() -> Union[ExecResult[str], ExecResult[bytes]]:
        # run the command and capture the process handle
        async with aclosing(run_command()) as rc:
            proc = cast(Process, await anext(rc))
            # await result wrapped in timeout handler if requested
            if timeout:
                try:
                    with anyio.fail_after(timeout):
                        result = await anext(rc)
                        return cast(Union[ExecResult[str], ExecResult[bytes]], result)
                except TimeoutError:
                    # terminate timed out process -- try for graceful termination
                    # then be more forceful if requied
                    with anyio.CancelScope(shield=True):
                        try:
                            proc.terminate()
                            await anyio.sleep(2)
                            if proc.returncode is None:
                                proc.kill()
                        except Exception:
                            pass
                    raise
            # await result without timeout
            else:
                result = await anext(rc)
                return cast(Union[ExecResult[str], ExecResult[bytes]], result)
    # run command
    async with concurrency("subprocesses", max_subprocesses_context_var.get()):
        message = args if isinstance(args, str) else shlex.join(args)
        with trace_action(logger, "Subprocess", message):
            return await run_command_timeout()
def init_max_subprocesses(max_subprocesses: int | None = None) -> None:
    max_subprocesses = (
        max_subprocesses if max_subprocesses else default_max_subprocesses()
    )
    max_subprocesses_context_var.set(max_subprocesses)
def default_max_subprocesses() -> int:
    cpus = os.cpu_count()
    return cpus if cpus else 1
max_subprocesses_context_var = ContextVar[int](
    "max_subprocesses", default=default_max_subprocesses()
)

================
File: src/inspect_ai/util/_subtask.py
================
import inspect
from datetime import datetime
from functools import wraps
from logging import getLogger
from typing import (
    Any,
    Callable,
    Protocol,
    TypeVar,
    cast,
    overload,
    runtime_checkable,
)
from inspect_ai._util._async import is_callable_coroutine, tg_collect
from inspect_ai._util.content import Content
from inspect_ai._util.trace import trace_action
from inspect_ai._util.working import sample_waiting_time
from inspect_ai.util._store import Store, dict_jsonable, init_subtask_store
SubtaskResult = str | int | float | bool | list[Content]
RT = TypeVar("RT", SubtaskResult, Any)
logger = getLogger(__name__)
@runtime_checkable
class Subtask(Protocol):
    async def __call__(
        self,
        *args: Any,
        **kwargs: Any,
    ) -> Any:
        """Subtask with distinct `Store` and `Transcript`.
        Args:
            *args (Any): Arguments for the subtask.
            **kwargs (Any): Keyword arguments for the subtask.
        Returns:
            Result of subtask.
        """
        ...
@overload
def subtask(
    name: str,
    store: Store | None = None,
    type: str | None = None,
    input: dict[str, Any] | None = None,
) -> Callable[..., Subtask]: ...
@overload
def subtask(
    name: Subtask,
    store: Store | None = None,
    type: str | None = None,
    input: dict[str, Any] | None = None,
) -> Subtask: ...
def subtask(
    name: str | Subtask,
    store: Store | None = None,
    type: str | None = None,
    input: dict[str, Any] | None = None,
) -> Callable[..., Subtask] | Subtask:
    r"""Decorator for subtasks.
    Args:
        name: Name for subtask (defaults to function name)
        store: Store to use for subtask
        type: Type to use for subtask
        input: Input to log for subtask
    Returns:
        Function which runs the Subtask, providing an isolated
        `Store` and `Transcript`, and recording a `SubtaskEvent`
        when it is complete.
    """
    def create_subtask_wrapper(func: Subtask, name: str | None = None) -> Subtask:
        from inspect_ai.log._transcript import (
            Event,
            SubtaskEvent,
            track_store_changes,
            transcript,
        )
        @wraps(func)
        async def run_subtask(*args: Any, **kwargs: Any) -> RT:
            # resolve name
            subtask_name = (
                name
                if name is not None
                else cast(str, getattr(func, "__name__", "subtask"))
            )
            # verify async
            if not is_callable_coroutine(func):
                raise TypeError(
                    f"'{subtask_name}' is not declared as an async callable."
                )
            # capture input for logging if required
            if input is not None:
                log_input = dict_jsonable(input)
            else:
                log_input = {}
                if len(args) > 0:
                    params = list(inspect.signature(func).parameters.keys())
                    for i, arg in enumerate(args):
                        log_input[params[i]] = arg
                log_input = dict_jsonable(log_input | kwargs)
            # create coroutine so we can provision a subtask contextvars
            async def run() -> tuple[RT, list[Event]]:
                # initialise subtask (provisions store and transcript)
                init_subtask(subtask_name, store if store else Store())
                # run the subtask
                with trace_action(logger, "Subtask", subtask_name):
                    with track_store_changes():  # type: ignore
                        result = await func(*args, **kwargs)
                # return result and event
                return result, list(transcript().events)
            # create subtask event
            waiting_time_start = sample_waiting_time()
            event = SubtaskEvent(
                name=subtask_name, input=log_input, type=type, pending=True
            )
            transcript()._event(event)
            # create and run the task as a coroutine
            result, events = (await tg_collect([run]))[0]
            # time accounting
            completed = datetime.now()
            waiting_time_end = sample_waiting_time()
            event.completed = completed
            event.working_time = (completed - event.timestamp).total_seconds() - (
                waiting_time_end - waiting_time_start
            )
            # update event
            event.result = result
            event.events = events
            event.pending = None
            # return result
            return result
        return run_subtask
    if isinstance(name, str):
        def wrapper(func: Subtask) -> Subtask:
            return create_subtask_wrapper(func, name)
        return wrapper
    else:
        return create_subtask_wrapper(name)
def init_subtask(name: str, store: Store) -> Any:
    from inspect_ai.log._transcript import (
        Transcript,
        init_transcript,
    )
    init_subtask_store(store)
    transcript = Transcript(name=name)
    init_transcript(transcript)
    return transcript

================
File: src/inspect_ai/util/_throttle.py
================
import time
from functools import wraps
from typing import Any, Callable
def throttle(seconds: float) -> Callable[..., Any]:
    """Throttle a function to ensure it is called no more than every n seconds.
    Args:
       seconds (float): Throttle time.
    Returns:
       Callable: Throttled function.
    """
    def decorator(func: Callable[..., Any]) -> Callable[..., Any]:
        last_called: float = 0
        last_result: Any = None
        @wraps(func)
        def wrapped(*args: Any, **kwargs: Any) -> Any:
            nonlocal last_called
            nonlocal last_result
            current_time = time.time()
            if current_time - last_called >= seconds:
                last_result = func(*args, **kwargs)
                last_called = current_time
            return last_result
        return wrapped
    return decorator

================
File: tests/approval/test_approval.py
================
from pathlib import Path
from test_helpers.tools import addition
from test_helpers.utils import ensure_test_package_installed
from inspect_ai import Task, eval
from inspect_ai.approval import ApprovalDecision, ApprovalPolicy, auto_approver
from inspect_ai.dataset import Sample
from inspect_ai.log._log import EvalLog
from inspect_ai.log._transcript import ApprovalEvent, ToolEvent
from inspect_ai.model import ModelOutput, get_model
from inspect_ai.scorer import match
from inspect_ai.solver import generate, use_tools
def check_approval(
    policy: str | ApprovalPolicy | list[ApprovalPolicy] | None,
    decision: ApprovalDecision,
    approver: str = "auto",
    task_policy: str | ApprovalPolicy | list[ApprovalPolicy] | None = None,
) -> ApprovalEvent:
    if policy is not None:
        if isinstance(policy, str):
            policy = (Path(__file__).parent / policy).as_posix()
        policy = policy if isinstance(policy, list | str) else [policy]
    if task_policy is not None:
        if isinstance(task_policy, str):
            task_policy = (Path(__file__).parent / task_policy).as_posix()
        task_policy = (
            task_policy if isinstance(task_policy, list | str) else [task_policy]
        )
    model = get_model(
        "mockllm/model",
        custom_outputs=[
            ModelOutput.for_tool_call(
                "mockllm/model",
                tool_name="addition",
                tool_arguments={"x": 1, "y": 1},
            ),
            ModelOutput.from_content("mockllm/model", content="2"),
        ],
    )
    task = Task(
        dataset=[Sample(input="What is 1 + 1?", target="2")],
        solver=[use_tools(addition()), generate()],
        scorer=match(numeric=True),
        approval=task_policy,
    )
    log = eval(task, model=model, approval=policy)[0]
    approval = find_approval(log)
    assert approval
    assert approval.approver == approver
    assert approval.decision == decision
    return approval
approve_all_policy = ApprovalPolicy(approver=auto_approver(), tools="*")
reject_all_policy = ApprovalPolicy(approver=auto_approver("reject"), tools="*")
def test_approve():
    check_approval(approve_all_policy, decision="approve")
def test_approve_reject():
    check_approval(reject_all_policy, decision="reject")
    check_approval(None, decision="reject", task_policy=reject_all_policy)
def test_approve_pattern():
    check_approval(
        ApprovalPolicy(approver=auto_approver(), tools="add*"), decision="approve"
    )
    check_approval(
        ApprovalPolicy(approver=auto_approver(), tools="foo*"),
        decision="reject",
        approver="policy",
        task_policy=approve_all_policy,
    )
def test_approve_multi_pattern():
    check_approval(
        ApprovalPolicy(approver=auto_approver(), tools=["spoo*", "add*"]),
        decision="approve",
        task_policy=reject_all_policy,
    )
def test_approve_escalate():
    check_approval(
        [
            ApprovalPolicy(approver=auto_approver("escalate"), tools="add*"),
            ApprovalPolicy(approver=auto_approver("approve"), tools="add*"),
        ],
        decision="approve",
    )
def test_approve_no_reject():
    check_approval(
        None,
        decision="approve",
        task_policy=[
            ApprovalPolicy(approver=auto_approver("reject"), tools="foo*"),
            ApprovalPolicy(approver=auto_approver("approve"), tools="add*"),
        ],
    )
def test_approve_modify():
    # also tests loading an approver from a package
    ensure_test_package_installed()
    event = check_approval(
        "modify.yaml",
        decision="modify",
        approver="inspect_package/renamer",
    )
    assert event.modified
    assert event.modified.function == "newname"
def test_approve_config():
    check_approval("approve.yaml", decision="approve")
def test_approve_config_reject():
    check_approval(None, decision="reject", task_policy="reject.yaml")
def test_approve_config_terminate():
    check_approval("terminate.yaml", decision="terminate", task_policy="reject.yaml")
def test_approve_config_escalate():
    check_approval("escalate.yaml", decision="reject", approver="policy")
def find_approval(log: EvalLog) -> ApprovalEvent | None:
    if log.samples:
        tool_event: ToolEvent | None = next(
            (
                event
                for event in log.samples[0].transcript.events
                if isinstance(event, ToolEvent)
            ),
            None,
        )
        if tool_event:
            return next(
                (
                    ev
                    for ev in reversed(tool_event.events)
                    if isinstance(ev, ApprovalEvent)
                ),
                None,
            )
    return None

================
File: tests/conftest.py
================
import importlib.util
import os
import shutil
import subprocess
import sys
import time
import warnings
import boto3
import pytest
from moto.server import ThreadedMotoServer
sys.path.append(os.path.join(os.path.dirname(__file__), "helpers"))
def pytest_addoption(parser):
    parser.addoption(
        "--runslow", action="store_true", default=False, help="run slow tests"
    )
    parser.addoption(
        "--runapi", action="store_true", default=False, help="run API tests"
    )
def pytest_configure(config):
    config.addinivalue_line("markers", "slow: mark test as slow to run")
    config.addinivalue_line("markers", "api: mark test as requiring API access")
def pytest_collection_modifyitems(config, items):
    if not config.getoption("--runslow"):
        skip_slow = pytest.mark.skip(reason="need --runslow option to run")
        for item in items:
            if "slow" in item.keywords:
                item.add_marker(skip_slow)
    if not config.getoption("--runapi"):
        skip_api = pytest.mark.skip(reason="need --runapi option to run")
        for item in items:
            if "api" in item.keywords:
                item.add_marker(skip_api)
@pytest.fixture(scope="module")
def mock_s3():
    server = ThreadedMotoServer(port=19100)
    server.start()
    # Give the server a moment to start up
    time.sleep(1)
    existing_env = {
        key: os.environ.get(key, None)
        for key in [
            "AWS_ENDPOINT_URL",
            "AWS_ACCESS_KEY_ID",
            "AWS_SECRET_ACCESS_KEY",
            "AWS_DEFAULT_REGION",
        ]
    }
    os.environ["AWS_ENDPOINT_URL"] = "http://127.0.0.1:19100"
    os.environ["AWS_ACCESS_KEY_ID"] = "unused_id_mock_s3"
    os.environ["AWS_SECRET_ACCESS_KEY"] = "unused_key_mock_s3"
    os.environ["AWS_DEFAULT_REGION"] = "us-west-1"
    s3_client = boto3.client("s3")
    s3_client.create_bucket(
        Bucket="test-bucket",
        CreateBucketConfiguration={"LocationConstraint": "us-west-1"},
    )
    yield
    # Unfortunately, we can't just throw away moto after the test,
    # because there is caching of S3 bucket state (e.g. ownership)
    # somewhere in s3fs or boto. So we have to go through
    # the charade of emptying and deleting the mocked bucket.
    s3 = boto3.resource("s3")
    s3_bucket = s3.Bucket("test-bucket")
    bucket_versioning = s3.BucketVersioning("test-bucket")
    if bucket_versioning.status == "Enabled":
        s3_bucket.object_versions.delete()
    else:
        s3_bucket.objects.all().delete()
    s3_client.delete_bucket(Bucket="test-bucket")
    server.stop()
    for key, value in existing_env.items():
        if value is None:
            del os.environ[key]
        else:
            os.environ[key] = value
def pytest_sessionfinish(session, exitstatus):
    if importlib.util.find_spec("inspect_package"):
        try:
            subprocess.check_call(
                [sys.executable, "-m", "pip", "uninstall", "-y", "inspect_package"]
            )
            shutil.rmtree("tests/test_package/build")
            shutil.rmtree("tests/test_package/inspect_package.egg-info")
        except subprocess.CalledProcessError as ex:
            warnings.warn(f"Error occurred uninstalling inspect_package: {ex}")
        except BaseException:
            pass

================
File: tests/dataset/test_dataset.py
================
import os
from pathlib import Path
from typing import Type, TypeVar
import pytest
from pydantic import BaseModel
from test_helpers.utils import skip_if_github_action
from inspect_ai._util.content import ContentImage
from inspect_ai.dataset import (
    Dataset,
    FieldSpec,
    Sample,
    csv_dataset,
    example_dataset,
    file_dataset,
    json_dataset,
)
from inspect_ai.model._chat_message import ChatMessageUser
T_ds = TypeVar("T_ds")
# test functions are parameterized by dataset type and input file
csv = (csv_dataset, "samples.csv")
json = (json_dataset, "samples.json")
jsonl = (file_dataset, "samples.jsonl")
dataset_params = [csv, json, jsonl]
dataset_md_params = [
    (param[0], param[1].replace(".", "-md.")) for param in dataset_params
]
# test reading a dataset using default configuration
@pytest.mark.parametrize("type,file", dataset_params)
def test_dataset(type: Type[T_ds], file: str) -> None:
    dataset: Dataset = type.__call__(dataset_path(file))
    assert_sample(dataset[0])
# test reading a dataset with an explicit fields specification
@pytest.mark.parametrize("type,file", dataset_params)
def test_dataset_fields(type: Type[T_ds], file: str) -> None:
    dataset: Dataset = type.__call__(
        dataset_path(file), sample_fields=sample_field_spec
    )
    assert_sample(dataset[0])
    assert isinstance(dataset[0].sandbox, BaseModel)
    assert dataset[0].sandbox.type == "docker"
# test reading a dataset with a custom data_to_sample function
@pytest.mark.parametrize("type,file", dataset_params)
def test_dataset_fields_fn(type: Type[T_ds], file: str) -> None:
    dataset: Dataset = type.__call__(
        dataset_path(file),
        sample_fields=data_to_sample,
    )
    assert len(dataset) == 1
    assert_sample(dataset[0])
@pytest.mark.parametrize("type,file", dataset_params)
def test_dataset_multiple_samples_fn(type: Type[T_ds], file: str):
    dataset: Dataset = type.__call__(
        dataset_path(file),
        sample_fields=data_to_sample_multiple,
    )
    assert len(dataset) == 2
# test reading metadata field
@pytest.mark.parametrize("type,file", dataset_md_params)
def test_dataset_metadata(type: Type[T_ds], file: str) -> None:
    sample_fields = (
        FieldSpec(metadata=["name", "age", "foo"]) if file.endswith(".json") else None
    )
    dataset: Dataset = type.__call__(dataset_path(file), sample_fields=sample_fields)
    assert dataset[0].metadata and dataset[0].metadata.get("foo") == "bar"
# test pydantic metadata handling
@pytest.mark.parametrize("type,file", dataset_md_params)
def test_dataset_metadata_pydantic(type: Type[T_ds], file: str) -> None:
    class Metadata(BaseModel, frozen=True):
        name: str
        age: int
        foo: str
    dataset: Dataset = type.__call__(
        dataset_path(file), sample_fields=FieldSpec(metadata=Metadata)
    )
    assert dataset[0].metadata and dataset[0].metadata.get("foo") == "bar"
    metadata = dataset[0].metadata_as(Metadata)
    assert metadata.name == "jim"
    assert metadata.age == 42
    assert metadata.foo == "bar"
    class MetadataSlice(BaseModel, frozen=True):
        foo: str
    dataset = type.__call__(
        dataset_path(file), sample_fields=FieldSpec(metadata=MetadataSlice)
    )
    metadata_slice = dataset[0].metadata_as(MetadataSlice)
    assert metadata_slice.foo == "bar"
    class MetadataInvalid(BaseModel, frozen=True):
        x: int
        y: int
    with pytest.raises(ValueError):
        dataset = type.__call__(
            dataset_path(file), sample_fields=FieldSpec(metadata=MetadataInvalid)
        )
    class MetadataNotFrozen(BaseModel):
        name: str
        age: int
        foo: str
    with pytest.raises(ValueError):
        dataset = type.__call__(
            dataset_path(file), sample_fields=FieldSpec(metadata=MetadataNotFrozen)
        )
@skip_if_github_action
def test_dataset_read_id() -> None:
    dataset = example_dataset(
        "biology_qa",
        FieldSpec(input="question", target="answer", id="id"),
    )
    assert dataset[0].id == "q1"
def test_example_dataset_not_found() -> None:
    with pytest.raises(ValueError):
        example_dataset("not_found")
def test_dataset_image_paths() -> None:
    dataset = json_dataset(dataset_path("images.jsonl"))
    sample = dataset[0]
    assert not isinstance(sample.input, str)
    assert isinstance(sample.input[0], ChatMessageUser)
    assert isinstance(sample.input[0].content[1], ContentImage)
    image = Path(sample.input[0].content[1].image)
    assert image.exists()
def test_dataset_auto_id() -> None:
    dataset = json_dataset(dataset_path("dataset.jsonl"))
    assert all(sample.id is None for sample in dataset)
    dataset = json_dataset(dataset_path("dataset.jsonl"), auto_id=True)
    assert [sample.id for sample in dataset] == [id for id in range(1, 11)]
sample_field_spec = FieldSpec(input="input", target="label", metadata=["extra"])
def data_to_sample(data: dict) -> Sample:
    return Sample(
        input=str(data.get("input")),
        target=str(data.get("label")),
        metadata={"extra": data.get("extra")},
    )
def data_to_sample_multiple(data: dict) -> list[Sample]:
    return [data_to_sample(data), data_to_sample(data)]
def assert_sample(sample: Sample) -> None:
    assert sample.input == "Say 'Hello, World'"
    assert sample.target == "Hello, World"
    if sample.metadata:
        assert sample.metadata.get("extra") == "data"
def dataset_path(file: str) -> str:
    return os.path.join("tests", "dataset", "test_dataset", file)
def example_path(*paths: str) -> str:
    return os.path.join("examples", "/".join(paths))

================
File: tests/log/test_eval_log.py
================
import math
import os
import tempfile
import pytest
from pydantic_core import PydanticSerializationError
from inspect_ai import Task, eval
from inspect_ai._util.file import filesystem
from inspect_ai.dataset import Sample
from inspect_ai.log import read_eval_log
from inspect_ai.log._file import read_eval_log_sample, write_eval_log
from inspect_ai.log._log import EvalLog
from inspect_ai.solver import (
    Generate,
    TaskState,
    generate,
    solver,
)
def log_path(file: str) -> str:
    return os.path.join("tests", "log", "test_eval_log", f"{file}.txt")
def read_log(file: str, header_only: bool = False) -> EvalLog:
    return read_eval_log(file, header_only=header_only, format="json")
class NotSerializable:
    name: str
def test_ignore_unserializable():
    @solver
    def inject_unserializable():
        async def solve(state: TaskState, generate: Generate):
            state.metadata["not serializable"] = NotSerializable
            return state
        return solve
    task = Task(
        dataset=[Sample(input="Say hello.", target="Hello")],
        solver=[inject_unserializable(), generate()],
    )
    try:
        eval(tasks=task, model="mockllm/model")
    except PydanticSerializationError:
        assert False, "Eval raised Pydantic serialization error."
def test_read_nan():
    def check_for_nan(log):
        assert math.isnan(log.results.metrics.get("accuracy").value)
    log_file = log_path("log_with_nan")
    check_for_nan(read_log(log_file))
    check_for_nan(read_log(log_file, header_only=True))
def test_fail_invalid():
    check_log_raises(log_path("log_invalid"))
def test_fail_version():
    check_log_raises(log_path("log_version_3"))
def test_valid_log_header():
    log = read_log(log_path("log_valid"), header_only=True)
    assert log.eval.metadata["meaning_of_life"] == 42
def test_migrate_length_stop_reason():
    log = read_log(log_path("log_length_stop_reason"))
    assert log.samples[0].output.stop_reason == "max_tokens"
def test_read_sample():
    log_file = os.path.join("tests", "log", "test_eval_log", "log_formats.json")
    with tempfile.TemporaryDirectory() as tmpdirname:
        # try out json file
        sample = read_eval_log_sample(log_file, 1, 1)
        assert sample.target == " Yes"
        # try out eval file
        log = read_eval_log(log_file)
        eval_log_path = os.path.join(tmpdirname, "new_log.eval")
        write_eval_log(log, eval_log_path)
        sample = read_eval_log_sample(eval_log_path, 1, 1)
        assert sample.target == " Yes"
def test_log_location():
    json_log_file = os.path.join("tests", "log", "test_eval_log", "log_formats.json")
    check_log_location(json_log_file)
    eval_log_file = os.path.join("tests", "log", "test_eval_log", "log_streaming.eval")
    check_log_location(eval_log_file)
def check_log_location(log_file: str):
    location = filesystem(log_file).info(log_file).name
    log = read_eval_log(location)
    assert log.location == location
    log = read_eval_log(location, header_only=True)
    assert log.location == location
def check_log_raises(log_file):
    with pytest.raises(ValueError):
        read_log(log_file)
    with pytest.raises(ValueError):
        read_log(log_file, header_only=True)

================
File: tests/log/test_eval_log/log_invalid.txt
================
{
    "version": 1, "status":

================
File: tests/log/test_eval_log/log_length_stop_reason.txt
================
{
  "version": 2,
  "status": "success",
  "eval": {
    "run_id": "jNQ3Z5KuoJScyNa9x6JBYv",
    "created": "2024-09-25T10:26:43+00:00",
    "task": "hello_world",
    "task_id": "g4Midu49ctDtsVPJFFrZjX",
    "task_version": 0,
    "task_file": "examples/hello_world.py",
    "task_attribs": {},
    "task_args": {},
    "dataset": {
      "samples": 1,
      "shuffled": false
    },
    "model": "anthropic/claude-3-5-sonnet-20240620",
    "model_args": {},
    "config": {},
    "revision": {
      "type": "git",
      "origin": "git@github.com:UKGovernmentBEIS/inspect_ai.git",
      "commit": "4f3bd1f7"
    },
    "packages": {
      "inspect_ai": "0.3.25.dev21+g3247021.d20240823"
    }
  },
  "plan": {
    "name": "plan",
    "steps": [
      {
        "solver": "generate",
        "params": {}
      }
    ],
    "config": {
      "max_tokens": 1
    }
  },
  "results": {
    "total_samples": 1,
    "completed_samples": 1,
    "scores": [
      {
        "name": "exact",
        "scorer": "exact",
        "params": {},
        "metrics": {
          "mean": {
            "name": "mean",
            "value": 0.0,
            "options": {}
          },
          "stderr": {
            "name": "stderr",
            "value": 0.0,
            "options": {}
          }
        }
      }
    ]
  },
  "stats": {
    "started_at": "2024-09-25T10:26:43+00:00",
    "completed_at": "2024-09-25T10:26:44+00:00",
    "model_usage": {
      "anthropic/claude-3-5-sonnet-20240620": {
        "input_tokens": 12,
        "output_tokens": 1,
        "total_tokens": 13
      }
    }
  },
  "samples": [
    {
      "id": 1,
      "epoch": 1,
      "input": "Just reply with Hello World",
      "target": "Hello World",
      "messages": [
        {
          "content": "Just reply with Hello World",
          "source": "input",
          "role": "user"
        },
        {
          "content": [
            {
              "type": "text",
              "text": "Hello"
            }
          ],
          "source": "generate",
          "role": "assistant"
        }
      ],
      "output": {
        "model": "claude-3-5-sonnet-20240620",
        "choices": [
          {
            "message": {
              "content": [
                {
                  "type": "text",
                  "text": "Hello"
                }
              ],
              "source": "generate",
              "role": "assistant"
            },
            "stop_reason": "length"
          }
        ],
        "usage": {
          "input_tokens": 12,
          "output_tokens": 1,
          "total_tokens": 13
        }
      },
      "scores": {
        "exact": {
          "value": "I",
          "answer": "Hello"
        }
      },
      "metadata": {},
      "store": {},
      "transcript": {
        "events": [
          {
            "timestamp": "2024-09-25T10:26:43.695045+00:00",
            "event": "sample_init",
            "sample": {
              "input": "Just reply with Hello World",
              "target": "Hello World",
              "id": 1
            },
            "state": {
              "messages": [
                {
                  "content": "Just reply with Hello World",
                  "source": "input",
                  "role": "user"
                }
              ],
              "tools": [],
              "tool_choice": null,
              "store": {},
              "output": {
                "model": "anthropic/claude-3-5-sonnet-20240620",
                "choices": []
              },
              "completed": false,
              "metadata": {}
            }
          },
          {
            "timestamp": "2024-09-25T10:26:43.695402+00:00",
            "event": "step",
            "action": "begin",
            "type": "solver",
            "name": "generate"
          },
          {
            "timestamp": "2024-09-25T10:26:44.362896+00:00",
            "event": "model",
            "model": "anthropic/claude-3-5-sonnet-20240620",
            "input": [
              {
                "content": "Just reply with Hello World",
                "source": "input",
                "role": "user"
              }
            ],
            "tools": [],
            "tool_choice": "none",
            "config": {
              "max_tokens": 1
            },
            "output": {
              "model": "claude-3-5-sonnet-20240620",
              "choices": [
                {
                  "message": {
                    "content": [
                      {
                        "type": "text",
                        "text": "Hello"
                      }
                    ],
                    "source": "generate",
                    "role": "assistant"
                  },
                  "stop_reason": "length"
                }
              ],
              "usage": {
                "input_tokens": 12,
                "output_tokens": 1,
                "total_tokens": 13
              }
            },
            "call": {
              "request": {
                "messages": [
                  {
                    "role": "user",
                    "content": "Just reply with Hello World"
                  }
                ],
                "tools": [],
                "model": "claude-3-5-sonnet-20240620",
                "max_tokens": 1
              },
              "response": {
                "id": "msg_01VTWDhmKcdu11wjVamoYLgp",
                "content": [
                  {
                    "text": "Hello",
                    "type": "text"
                  }
                ],
                "model": "claude-3-5-sonnet-20240620",
                "role": "assistant",
                "stop_reason": "max_tokens",
                "stop_sequence": null,
                "type": "message",
                "usage": {
                  "input_tokens": 12,
                  "output_tokens": 1
                }
              }
            }
          },
          {
            "timestamp": "2024-09-25T10:26:44.363643+00:00",
            "event": "state",
            "changes": [
              {
                "op": "add",
                "path": "/messages/1",
                "value": {
                  "content": [
                    {
                      "type": "text",
                      "text": "Hello"
                    }
                  ],
                  "source": "generate",
                  "role": "assistant"
                }
              },
              {
                "op": "add",
                "path": "/output/usage",
                "value": {
                  "input_tokens": 12,
                  "output_tokens": 1,
                  "total_tokens": 13
                }
              },
              {
                "op": "replace",
                "path": "/output/model",
                "value": "claude-3-5-sonnet-20240620"
              },
              {
                "op": "add",
                "path": "/output/choices/0",
                "value": {
                  "message": {
                    "content": [
                      {
                        "type": "text",
                        "text": "Hello"
                      }
                    ],
                    "source": "generate",
                    "role": "assistant"
                  },
                  "stop_reason": "length"
                }
              }
            ]
          },
          {
            "timestamp": "2024-09-25T10:26:44.363685+00:00",
            "event": "step",
            "action": "end",
            "type": "solver",
            "name": "generate"
          },
          {
            "timestamp": "2024-09-25T10:26:44.363751+00:00",
            "event": "step",
            "action": "begin",
            "type": "scorer",
            "name": "exact"
          },
          {
            "timestamp": "2024-09-25T10:26:44.364169+00:00",
            "event": "score",
            "score": {
              "value": "I",
              "answer": "Hello"
            }
          },
          {
            "timestamp": "2024-09-25T10:26:44.364410+00:00",
            "event": "step",
            "action": "end",
            "type": "scorer",
            "name": "exact"
          }
        ],
        "content": {}
      }
    }
  ]
}

================
File: tests/log/test_eval_log/log_valid.txt
================
{
    "version": 2,
    "status": "success",
    "eval": {
        "task": "wikipedia",
        "task_version": 0,
        "task_file": "examples/langchain/wikipedia.py",
        "task_id": "YAdbKczyeSb6mEgPd3R9Qs",
        "run_id": "i5LyrzaUdD9K4EW5WTAd5t",
        "created": "2024-05-05T07:59:35",
        "dataset": {
            "name": "wikipedia",
            "location": "wikipedia.jsonl"
        },
        "model": "openai/gpt-4",
        "task_attribs": {},
        "task_args": {},
        "model_args": {},
        "config": {
            "limit": 20
        },
        "metadata": {
            "meaning_of_life": 42
        }
    },
    "plan": {
        "name": "plan",
        "steps": [
            {
                "solver": "wikipedia_search",
                "params": {}
            }
        ],
        "config": {}
    },
    "results": {
        "scorers": [{
            "name": "model_graded_fact",
            "params": {},
            "metrics": {
                "accuracy": {
                    "name": "accuracy",
                    "value": 1,
                    "options": {}
                },
                "bootstrap_std": {
                    "name": "bootstrap_std",
                    "value": 0.0,
                    "options": {}
                }
            }
        }]
    },
    "stats": {
        "started_at": "2024-05-05T07:59:35",
        "completed_at": "2024-05-05T08:00:03",
        "model_usage": {
            "openai/gpt-4": {
                "input_tokens": 8868,
                "output_tokens": 1351,
                "total_tokens": 10219
            }
        }
    },
    "logging": []
}

================
File: tests/log/test_eval_log/log_version_3.txt
================
{
    "version": 3,
    "status": "success",
    "eval": {
        "task": "wikipedia",
        "task_version": 0,
        "task_file": "examples/langchain/wikipedia.py",
        "task_id": "YAdbKczyeSb6mEgPd3R9Qs",
        "run_id": "i5LyrzaUdD9K4EW5WTAd5t",
        "created": "2024-05-05T07:59:35",
        "dataset": {
            "name": "wikipedia",
            "location": "wikipedia.jsonl"
        },
        "model": "openai/gpt-4",
        "task_attribs": {},
        "task_args": {},
        "model_args": {},
        "config": {
            "limit": 20
        }
    },
    "plan": {
        "name": "plan",
        "steps": [
            {
                "solver": "wikipedia_search",
                "params": {}
            }
        ],
        "config": {}
    },
    "results": {
        "scorers": [{
            "name": "model_graded_fact",
            "params": {},
            "metrics": {
                "accuracy": {
                    "name": "accuracy",
                    "value": 1,
                    "options": {}
                },
                "bootstrap_std": {
                    "name": "bootstrap_std",
                    "value": 0.0,
                    "options": {}
                }
            }
        }]
    },
    "stats": {
        "started_at": "2024-05-05T07:59:35",
        "completed_at": "2024-05-05T08:00:03",
        "model_usage": {
            "openai/gpt-4": {
                "input_tokens": 8868,
                "output_tokens": 1351,
                "total_tokens": 10219
            }
        }
    },
    "logging": []
}

================
File: tests/log/test_eval_log/log_with_nan.txt
================
{
    "version": 1,
    "status": "success",
    "eval": {
        "task": "wikipedia",
        "task_version": 0,
        "task_file": "examples/langchain/wikipedia.py",
        "task_id": "YAdbKczyeSb6mEgPd3R9Qs",
        "run_id": "i5LyrzaUdD9K4EW5WTAd5t",
        "created": "2024-05-05T07:59:35",
        "dataset": {
            "name": "wikipedia",
            "location": "wikipedia.jsonl"
        },
        "model": "openai/gpt-4",
        "task_attribs": {},
        "task_args": {},
        "model_args": {},
        "config": {
            "limit": 20
        }
    },
    "plan": {
        "name": "plan",
        "steps": [
            {
                "solver": "wikipedia_search",
                "params": {}
            }
        ],
        "config": {}
    },
    "results": {
        "scorer": {
            "name": "model_graded_fact",
            "params": {}
        },
        "metrics": {
            "accuracy": {
                "name": "accuracy",
                "value": NaN,
                "options": {}
            },
            "bootstrap_std": {
                "name": "bootstrap_std",
                "value": 0.0,
                "options": {}
            }
        }
    },
    "stats": {
        "started_at": "2024-05-05T07:59:35",
        "completed_at": "2024-05-05T08:00:03",
        "model_usage": {
            "openai/gpt-4": {
                "input_tokens": 8868,
                "output_tokens": 1351,
                "total_tokens": 10219
            }
        }
    },
    "logging": []
}

================
File: tests/log/test_list_logs.py
================
from os.path import dirname, join
from pathlib import Path
from inspect_ai.log import list_eval_logs
file = Path(__file__)
ignored_files = ["ignore.json"]
def test_list_logs():
    logs = list_eval_logs(
        join(dirname(file), "test_list_logs"), formats=["eval", "json"]
    )
    names = [log.name for log in logs]
    assert len(logs) == 3
    assert all(file not in names for file in ignored_files)

================
File: tests/log/test_log_attachments.py
================
import os
from inspect_ai.log._condense import (
    ATTACHMENT_PROTOCOL,
    condense_sample,
    resolve_sample_attachments,
)
from inspect_ai.log._file import read_eval_log
def test_log_attachments_condense():
    # read and resolve attachments
    log_file = log_path("log_images.json")
    log = read_eval_log(log_file)
    assert log.samples
    log.samples = [resolve_sample_attachments(sample) for sample in log.samples]
    # confirm there are no attachment refs
    assert len(log.samples[0].attachments) == 0
    assert ATTACHMENT_PROTOCOL not in log.model_dump_json()
    # now condense and confirm we have attachment refs
    log.samples = [condense_sample(sample) for sample in log.samples]
    assert ATTACHMENT_PROTOCOL in log.model_dump_json()
def test_log_attachments_migration():
    # check for old-style content ref
    log_file = log_path("log_images_tc.json")
    assert "tc://" in log_str(log_file)
    # read log and confirm we have migrated into attachments
    log = read_eval_log(log_path("log_images_tc.json"))
    assert log.samples
    assert list(log.samples[0].attachments.values())[0].startswith(
        "data:image/png;base64"
    )
    # also confirm that we've preserved (now deprecated) transcript
    assert len(log.samples[0].transcript.events) > 0
def log_path(log: str) -> str:
    return os.path.join("tests", "log", "test_eval_log", log)
def log_str(log: str) -> str:
    with open(log, "r") as f:
        return f.read()

================
File: tests/log/test_log_formats.py
================
import json
import os
import shutil
import tempfile
import zipfile
from pathlib import Path
import pytest
from inspect_ai.log import read_eval_log, write_eval_log
@pytest.fixture
def original_log():
    """Fixture to provide a starting log file."""
    log_file = os.path.join("tests", "log", "test_eval_log", "log_formats.json")
    log = read_eval_log(log_file)
    log.location = None
    return log
@pytest.fixture
def temp_dir():
    """Fixture to create a temporary directory."""
    with tempfile.TemporaryDirectory() as tmpdirname:
        yield Path(tmpdirname)
def test_log_format_round_trip_single(original_log, temp_dir):
    """Test round-trip consistency within the same format."""
    formats = ["json", "eval"]
    for format in formats:
        # Write it to a new file in the current format
        new_log_path = (temp_dir / f"new_log.{format}").as_posix()
        write_eval_log(original_log, new_log_path, format=format)
        # Read the new log file
        new_log = read_eval_log(new_log_path, format=format)
        new_log.location = None
        # Compare the logs
        assert original_log == new_log, f"Round-trip failed for {format} format"
def test_eval_format_round_trip_overwrite(original_log, temp_dir):
    format = "eval"
    # Write it to a new file in the current format
    new_log_path = (temp_dir / f"new_log.{format}").as_posix()
    write_eval_log(original_log, new_log_path, format=format)
    # make a copy of the log file for later comparison
    copy_log_path = (temp_dir / f"new_log_copy.{format}").as_posix()
    shutil.copy(new_log_path, copy_log_path)
    # Overwrite the file
    write_eval_log(original_log, new_log_path, format=format)
    # ensure the zip file matches the original after overwriting
    assert compare_zip_contents(new_log_path, copy_log_path), (
        "EVAL zip file contents changed after rewriting file"
    )
def test_log_format_round_trip_cross(original_log, temp_dir):
    """Test round-trip consistency across formats."""
    # Write it to EVAL format
    eval_log_path = (temp_dir / "cross_format.eval").as_posix()
    write_eval_log(original_log, eval_log_path, format="eval")
    # Read the EVAL log
    eval_log = read_eval_log(eval_log_path, format="eval")
    eval_log.location = None
    # Write it back to JSON
    new_json_log_path = (temp_dir / "cross_format.json").as_posix()
    write_eval_log(eval_log, new_json_log_path, format="json")
    # Read the new JSON log
    new_json_log = read_eval_log(new_json_log_path, format="json")
    new_json_log.location = None
    # Compare the logs
    assert original_log == new_json_log, "Cross-format round-trip failed"
def test_log_format_equality(original_log, temp_dir):
    """Test that identical log files are created for both formats."""
    # Write it to both formats
    json_log_path = (temp_dir / "test_equality.json").as_posix()
    eval_log_path = (temp_dir / "test_equality.eval").as_posix()
    write_eval_log(original_log, json_log_path, format="json")
    write_eval_log(original_log, eval_log_path, format="eval")
    # Read both logs back
    json_log = read_eval_log(json_log_path, format="json")
    json_log.location = None
    eval_log = read_eval_log(eval_log_path, format="eval")
    eval_log.location = None
    # Compare the logs
    assert json_log == eval_log, "Logs in different formats are not identical"
def test_log_format_detection(original_log, temp_dir):
    """Test that auto format detection works correctly."""
    # Write it to both formats
    json_log_path = (temp_dir / "auto_test.json").as_posix()
    eval_log_path = (temp_dir / "auto_test.eval").as_posix()
    write_eval_log(original_log, json_log_path, format="auto")
    write_eval_log(original_log, eval_log_path, format="auto")
    # Read both logs back using auto format
    json_log = read_eval_log(json_log_path, format="auto")
    json_log.location = None
    eval_log = read_eval_log(eval_log_path, format="auto")
    eval_log.location = None
    # Compare the logs
    assert json_log == eval_log, "Auto format detection failed"
def test_log_format_eval_zip_structure(original_log, temp_dir):
    """Test that the .eval zip file structure is preserved across round trips."""
    # Write it to EVAL format
    eval_log_path = (temp_dir / "test.eval").as_posix()
    write_eval_log(original_log, eval_log_path, format="eval")
    # Read the EVAL log and write it back to a new EVAL file
    eval_log = read_eval_log(eval_log_path, format="eval")
    new_eval_log_path = (temp_dir / "test_new.eval").as_posix()
    write_eval_log(eval_log, new_eval_log_path, format="eval")
    # Compare the two EVAL files
    assert compare_zip_contents(eval_log_path, new_eval_log_path), (
        "EVAL zip file contents changed after round trip"
    )
def test_log_format_eval_zip_json_integrity(original_log, temp_dir):
    """Test that JSON files within the .eval zip remain intact and parseable."""
    # Write it to EVAL format
    eval_log_path = (temp_dir / "test.eval").as_posix()
    write_eval_log(original_log, eval_log_path, format="eval")
    with zipfile.ZipFile(eval_log_path, "r") as zip_file:
        for file_name in zip_file.namelist():
            if file_name.endswith(".json"):
                with zip_file.open(file_name) as json_file:
                    content = json_file.read().decode("utf-8")
                    try:
                        json.loads(content)
                    except json.JSONDecodeError:
                        pytest.fail(f"Invalid JSON in {file_name} within EVAL zip")
def test_log_format_eval_zip_roundtrip(original_log, temp_dir):
    """Test that JSON content is preserved when roundtripping through EVAL format."""
    # Write it to EVAL format
    eval_log_path = (temp_dir / "test.eval").as_posix()
    write_eval_log(original_log, eval_log_path, format="eval")
    # Read the EVAL log and write it back to JSON
    eval_log = read_eval_log(eval_log_path, format="eval")
    new_json_log_path = (temp_dir / "test_new.json").as_posix()
    write_eval_log(eval_log, new_json_log_path, format="json")
    # Read the new JSON log
    new_json_log = read_eval_log(new_json_log_path, format="json")
    new_json_log.location = None
    # Compare the original and new JSON logs
    assert original_log == new_json_log, (
        "JSON content changed after roundtrip through EVAL format"
    )
def compare_zip_contents(zip_file1: Path, zip_file2: Path) -> bool:
    """Compare the contents of two zip files."""
    with (
        zipfile.ZipFile(zip_file1, "r") as zip1,
        zipfile.ZipFile(zip_file2, "r") as zip2,
    ):
        if zip1.namelist() != zip2.namelist():
            return False
        for file_name in zip1.namelist():
            with zip1.open(file_name) as file1, zip2.open(file_name) as file2:
                if file1.read() != file2.read():
                    return False
    return True

================
File: tests/log/test_log_level.py
================
from logging import DEBUG, getLogger
from inspect_ai import Task, eval
from inspect_ai.log._log import EvalLog
from inspect_ai.log._transcript import LoggerEvent
from inspect_ai.model import get_model
from inspect_ai.solver import Generate, TaskState, solver
logger = getLogger(__name__)
@solver
def logging_solver(level_no: int):
    async def solve(state: TaskState, generate: Generate):
        logger.log(level_no, "sandbox log entry")
        return state
    return solve
def find_logger_event(log: EvalLog) -> LoggerEvent | None:
    if log.samples:
        return next(
            (
                event
                for event in log.samples[0].transcript.events
                if isinstance(event, LoggerEvent)
            ),
            None,
        )
    else:
        return None
def test_log_file_exclude_level() -> None:
    log = eval(
        Task(solver=logging_solver(DEBUG)),
        model=get_model("mockllm/model"),
        log_level="debug",
        log_level_transcript="warning",
    )[0]
    event = find_logger_event(log)
    assert event is None

================
File: tests/log/test_log_message_id.py
================
import os
from inspect_ai.log._file import read_eval_log
from inspect_ai.model._chat_message import ChatMessageUser
def test_message_id_auto_assign():
    message = ChatMessageUser(content="foo")
    assert message.id
def test_message_id_no_deserialize():
    log = read_eval_log(log_path("log_formats.json"))
    assert log.samples
    assert log.samples[0].messages[0].id is None
    log = read_eval_log(log_path("log_formats.eval"))
    assert log.samples
    assert log.samples[0].messages[0].id is None
def log_path(file: str) -> str:
    return os.path.join("tests", "log", "test_eval_log", f"{file}")

================
File: tests/log/test_log_streaming.py
================
import os
import tempfile
from typing import Set
import pytest
from test_helpers.utils import failing_solver_deterministic
from inspect_ai import Task, eval, task
from inspect_ai.dataset import Sample
from inspect_ai.log import list_eval_logs, read_eval_log_sample, read_eval_log_samples
@pytest.fixture
def streaming_log():
    return os.path.join("tests", "log", "test_eval_log", "log_streaming.eval")
def test_read_eval_log_sample(streaming_log: str) -> None:
    sample = read_eval_log_sample(streaming_log, 1, 1)
    assert sample.id == 1
    assert sample.epoch == 1
def test_read_eval_log_sample_index_error(streaming_log: str) -> None:
    with pytest.raises(IndexError):
        read_eval_log_sample(streaming_log, 3, 1)
def test_read_eval_log_samples(streaming_log: str) -> None:
    ids: Set[str | int] = set()
    epochs: Set[int] = set()
    total_samples = 0
    for sample in read_eval_log_samples(streaming_log):
        ids.add(sample.id)
        epochs.add(sample.epoch)
        total_samples += 1
    assert total_samples == 4
    assert ids == set([1, 2])
    assert epochs == set([1, 2])
def test_read_eval_log_samples_with_error():
    @task
    def failing_task():
        return Task(
            dataset=[Sample("Hello"), Sample("Hello")],
            plan=[failing_solver_deterministic([False, True])],
        )
    with tempfile.TemporaryDirectory() as log_dir:
        eval(
            failing_task(),
            model="mockllm/model",
            log_format="eval",
            log_dir=log_dir,
            max_samples=1,
        )
        log_file = list_eval_logs(log_dir)[0]
        # we expect a runtime error when we know the second sample failed
        # (as we will fail validation of status == "complete")
        with pytest.raises(RuntimeError):
            for _ in read_eval_log_samples(log_file):
                pass
        # we should be able to ignore the index error with all_samples_required=False
        total_samples = 0
        for _ in read_eval_log_samples(log_file, all_samples_required=False):
            total_samples += 1
        assert total_samples == 2

================
File: tests/log/test_log_tags.py
================
from inspect_ai import Task, eval
def test_log_tags():
    tags = ["foo,bar"]
    log = eval(Task(), model="mockllm/model", tags=tags)[0]
    assert log.eval.tags == tags

================
File: tests/model/providers/test_anthropic.py
================
import pytest
from test_helpers.utils import skip_if_no_anthropic
from inspect_ai.model import GenerateConfig, get_model
@pytest.mark.anyio
@skip_if_no_anthropic
async def test_anthropic_api() -> None:
    model = get_model(
        "anthropic/claude-2.1",
        config=GenerateConfig(
            frequency_penalty=0.0,
            stop_seqs=None,
            max_tokens=50,
            presence_penalty=0.0,
            seed=None,
            temperature=0.0,
            top_p=1.0,
        ),
    )
    message = "This is a test string. What are you?"
    response = await model.generate(input=message)
    assert len(response.completion) >= 1

================
File: tests/model/providers/test_azureai.py
================
import pytest
from test_helpers.tasks import minimal_task
from test_helpers.utils import skip_if_no_azureai
from inspect_ai import eval_async
from inspect_ai.model import GenerateConfig, Model, get_model
@pytest.mark.anyio
@skip_if_no_azureai
async def test_azureai_api() -> None:
    model = get_azureai_model()
    message = "This is a test string. What are you?"
    response = await model.generate(input=message)
    assert len(response.completion) >= 1
@pytest.mark.anyio
@skip_if_no_azureai
async def test_azureai_api_repeat_eval() -> None:
    model = get_azureai_model()
    _ = await eval_async(tasks=minimal_task, model=model)
    eval_log = await eval_async(tasks=minimal_task, model=model)
    assert eval_log[0].error is None, "Error on running consecutive evaluations"
def get_azureai_model() -> Model:
    return get_model(
        model="azureai/Meta-Llama-3-1-405B-Instruct-twe",
        azure=True,
        config=GenerateConfig(
            frequency_penalty=0.0,
            stop_seqs=None,
            max_tokens=2,
            presence_penalty=0.0,
            seed=None,
            temperature=0.0,
            top_p=1.0,
        ),
    )

================
File: tests/model/providers/test_cloudflare.py
================
import pytest
from test_helpers.utils import skip_if_no_cloudflare
from inspect_ai.model import get_model
@pytest.mark.anyio
@skip_if_no_cloudflare
async def test_cloudflare_api() -> None:
    async with get_model("cf/meta/llama-2-7b-chat-fp16") as model:
        message = "This is a test string. What are you?"
        response = await model.generate(input=message)
        assert len(response.completion) >= 1

================
File: tests/model/providers/test_goodfire.py
================
import pytest
from test_helpers.utils import skip_if_no_goodfire
from inspect_ai.model._chat_message import ChatMessageUser
from inspect_ai.model._generate_config import GenerateConfig
from inspect_ai.model._model import get_model
@pytest.mark.anyio
@skip_if_no_goodfire
async def test_goodfire_api() -> None:
    """Test the Goodfire API provider."""
    model = get_model(
        "goodfire/meta-llama/Meta-Llama-3.1-8B-Instruct",
        config=GenerateConfig(
            max_tokens=50,  # Match other tests
        ),
    )
    message = ChatMessageUser(content="What is 2+2?")
    response = await model.generate(input=[message])
    assert len(response.completion) >= 1

================
File: tests/model/providers/test_google.py
================
from google.genai.types import Candidate, Content, FinishReason  # type: ignore
from test_helpers.utils import skip_if_no_google, skip_if_trio
from inspect_ai import Task, eval
from inspect_ai.dataset import Sample
from inspect_ai.model._providers.google import completion_choice_from_candidate
from inspect_ai.scorer import includes
@skip_if_no_google
@skip_if_trio
def test_google_safety_settings():
    safety_settings = dict(
        dangerous_content="medium_and_above",
        hate_speech="low_and_above",
    )
    # run with safety settings
    log = eval(
        Task(
            dataset=[Sample(input="What is 1 + 1?", target=["2", "2.0", "Two"])],
            scorer=includes(),
        ),
        model="google/gemini-1.5-flash",
        model_args=dict(safety_settings=safety_settings),
    )[0]
    log_json = log.model_dump_json(indent=2)
    assert '"HARM_CATEGORY_DANGEROUS_CONTENT": "BLOCK_MEDIUM_AND_ABOVE"' in log_json
    assert '"HARM_CATEGORY_HATE_SPEECH": "BLOCK_LOW_AND_ABOVE"' in log_json
    assert '"HARM_CATEGORY_HARASSMENT": "BLOCK_NONE"' in log_json
@skip_if_no_google
@skip_if_trio
def test_google_block_reason():
    safety_settings = dict(harassment="low")
    eval(
        Task(
            # TODO: we can't seem to get a content filter to trigger!
            dataset=[Sample(input="you are a shameful model")],
        ),
        model="google/gemini-2.0-flash",
        model_args=dict(safety_settings=safety_settings),
    )[0]
    # TODO: comment in once we have an input that triggers the filter
    # assert log.samples
    # assert log.samples[0].output.stop_reason == "content_filter"
def test_completion_choice_malformed_function_call():
    # Copied from the ``Candidate`` object actually returned by the Google API
    candidate = Candidate(
        content=Content(parts=None, role=None),
        finish_reason=FinishReason.MALFORMED_FUNCTION_CALL,
        citation_metadata=None,
        finish_message=None,
        token_count=None,
        avg_logprobs=None,
        grounding_metadata=None,
        index=None,
        logprobs_result=None,
        safety_ratings=None,
    )
    choice = completion_choice_from_candidate(candidate)
    # Verify the conversion
    assert choice.message.content == ""  # Empty content for malformed calls
    assert choice.stop_reason == "unknown"  # MALFORMED_FUNCTION_CALL maps to "unknown"
    assert (
        choice.message.tool_calls is None
    )  # No tool calls for malformed function calls

================
File: tests/model/providers/test_grok.py
================
import pytest
from test_helpers.utils import skip_if_no_grok
from inspect_ai.model import (
    ChatMessageUser,
    GenerateConfig,
    get_model,
)
@pytest.mark.anyio
@skip_if_no_grok
async def test_grok_api() -> None:
    model = get_model(
        "grok/grok-beta",
        config=GenerateConfig(
            frequency_penalty=0.0,
            stop_seqs=None,
            max_tokens=50,
            presence_penalty=0.0,
            seed=None,
            temperature=0.0,
            top_p=1.0,
        ),
    )
    message = ChatMessageUser(content="This is a test string. What are you?")
    response = await model.generate(input=[message])
    assert len(response.completion) >= 1

================
File: tests/model/providers/test_groq.py
================
import pytest
from test_helpers.utils import skip_if_no_groq
from inspect_ai.model import (
    ChatMessageUser,
    GenerateConfig,
    get_model,
)
@pytest.mark.anyio
@skip_if_no_groq
async def test_groq_api() -> None:
    model = get_model(
        "groq/llama3-70b-8192",
        config=GenerateConfig(
            frequency_penalty=0.0,
            stop_seqs=None,
            max_tokens=50,
            presence_penalty=0.0,
            seed=None,
            temperature=0.0,
            top_p=1.0,
        ),
    )
    message = ChatMessageUser(content="This is a test string. What are you?")
    response = await model.generate(input=[message])
    assert len(response.completion) >= 1

================
File: tests/model/providers/test_hf.py
================
import pytest
from test_helpers.utils import (
    skip_if_github_action,
    skip_if_no_accelerate,
    skip_if_no_transformers,
)
from inspect_ai.model import (
    ChatMessageUser,
    GenerateConfig,
    get_model,
)
@pytest.fixture
def model():
    return get_model(
        "hf/EleutherAI/pythia-70m",
        config=GenerateConfig(
            max_tokens=1,
            seed=42,
            temperature=0.01,
        ),
        # this allows us to run base models with the chat message scaffolding:
        chat_template="{% for message in messages %}{{ message.content }}{% endfor %}",
        tokenizer_call_args={"truncation": True, "max_length": 2},
    )
@pytest.fixture
def model_with_stop_seqs():
    DEFAULT_CHAT_TEMPLATE = (
        "{% for message in messages %}{{ message.content }}{% endfor %}"
    )
    model = get_model(
        "hf/EleutherAI/pythia-70m",
        config=GenerateConfig(
            max_tokens=5,
            seed=42,
            temperature=0.001,
            stop_seqs=["w3"],
        ),
        # this allows us to run base models with the chat message scaffolding:
        chat_template=DEFAULT_CHAT_TEMPLATE,
        tokenizer_call_args={"truncation": True, "max_length": 10},
    )
    # Chat template is not propagated by default from get_model to the model's tokenizer.
    model.api.tokenizer.chat_template = DEFAULT_CHAT_TEMPLATE
    return model
@pytest.mark.anyio
@skip_if_github_action
@skip_if_no_transformers
@skip_if_no_accelerate
async def test_hf_api(model) -> None:
    message = ChatMessageUser(content="Lorem ipsum dolor")
    response = await model.generate(input=[message])
    assert response.usage.input_tokens == 2
    assert len(response.completion) >= 1
@pytest.mark.anyio
@skip_if_github_action
@skip_if_no_transformers
@skip_if_no_accelerate
async def test_hf_api_with_stop_seqs(model_with_stop_seqs) -> None:
    # This generates "https://www.w3.org" with pythia-70m greedy decoding
    message = ChatMessageUser(content="https://")
    response = await model_with_stop_seqs.generate(input=[message])
    assert response.completion == "www.w3"
@pytest.mark.anyio
@skip_if_github_action
@skip_if_no_transformers
async def test_hf_api_fails(model) -> None:
    temp_before = model.config.temperature
    try:
        model.config.temperature = 0.0
        message = ChatMessageUser(content="Lorem ipsum dolor")
        with pytest.raises(Exception):
            await model.generate(input=[message])
    finally:
        model.config.temperature = temp_before

================
File: tests/model/providers/test_llama_cpp_python.py
================
import pytest
from test_helpers.utils import skip_if_no_llama_cpp_python
from inspect_ai.model import (
    ChatMessageUser,
    GenerateConfig,
    get_model,
)
@pytest.mark.anyio
@skip_if_no_llama_cpp_python
async def test_llama_cpp_python_api() -> None:
    model = get_model(
        "llama-cpp-python/default",
        config=GenerateConfig(
            frequency_penalty=0.0,
            stop_seqs=None,
            max_tokens=50,
            presence_penalty=0.0,
            logit_bias=dict([(42, 10), (43, -10)]),
            seed=None,
            temperature=0.0,
            top_p=1.0,
        ),
    )
    message = ChatMessageUser(content="This is a test string. What are you?")
    response = await model.generate(input=[message])
    assert len(response.completion) >= 1

================
File: tests/model/providers/test_openai_o1.py
================
from inspect_ai.model import ChatMessageAssistant
def test_openai_o1_tool_call_parsing() -> None:
    from inspect_ai.model._providers.openai_o1 import O1PreviewChatAPIHandler
    handler = O1PreviewChatAPIHandler()
    resp: ChatMessageAssistant = handler.parse_assistant_response(
        response="""I will enter the search term into the search box and submit the search.
<tool_call>{"name": "web_browser_type_submit", "arguments": {"element_id": 11399, "text": "hilarious cat videos"}}</tool_call>""",
        tools=[],
    )
    assert resp.tool_calls is not None
    assert len(resp.tool_calls) == 1
    assert resp.tool_calls[0].function == "web_browser_type_submit"
    assert resp.tool_calls[0].arguments == {
        "element_id": 11399,
        "text": "hilarious cat videos",
    }
def test_openai_o1_tool_call_parsing_empty_arguments() -> None:
    from inspect_ai.model._providers.openai_o1 import O1PreviewChatAPIHandler
    handler = O1PreviewChatAPIHandler()
    resp: ChatMessageAssistant = handler.parse_assistant_response(
        response="""I need to return to the search results to explore other potential sources for the answer.
<tool_call>
{"name": "web_browser_back", "arguments": {}}
</tool_call>""",
        tools=[],
    )
    assert resp.tool_calls is not None
    assert len(resp.tool_calls) == 1
    assert resp.tool_calls[0].function == "web_browser_back"
    assert resp.tool_calls[0].arguments == {}
def test_openai_o1_tool_call_parsing_no_arguments() -> None:
    from inspect_ai.model._providers.openai_o1 import O1PreviewChatAPIHandler
    handler = O1PreviewChatAPIHandler()
    resp: ChatMessageAssistant = handler.parse_assistant_response(
        response="""I need to return to the search results to explore other potential sources for the answer.
<tool_call>
{"name": "web_browser_back"}
</tool_call>""",
        tools=[],
    )
    assert resp.tool_calls is not None
    assert len(resp.tool_calls) == 1
    assert resp.tool_calls[0].function == "unknown"
    assert resp.tool_calls[0].parse_error is not None

================
File: tests/model/providers/test_openai.py
================
import pytest
from test_helpers.utils import skip_if_no_openai
from inspect_ai.model import (
    ChatMessageUser,
    GenerateConfig,
    get_model,
)
from inspect_ai.model._chat_message import ChatMessageSystem
@pytest.mark.anyio
@skip_if_no_openai
async def test_openai_api() -> None:
    model = get_model(
        "openai/gpt-3.5-turbo",
        config=GenerateConfig(
            frequency_penalty=0.0,
            stop_seqs=None,
            max_tokens=50,
            presence_penalty=0.0,
            logit_bias=dict([(42, 10), (43, -10)]),
            seed=None,
            temperature=0.0,
            top_p=1.0,
        ),
    )
    message = ChatMessageUser(content="This is a test string. What are you?")
    response = await model.generate(input=[message])
    assert len(response.completion) >= 1
@pytest.mark.anyio
@skip_if_no_openai
async def test_openai_o_series_developer_messages() -> None:
    async def check_developer_messages(model_name: str):
        model = get_model(
            model_name,
            config=GenerateConfig(reasoning_effort="medium", parallel_tool_calls=True),
        )
        await model.generate(
            [
                ChatMessageSystem(content="I am a helpful assistant."),
                ChatMessageUser(content="What are you?"),
            ]
        )
    await check_developer_messages("openai/o1")
    await check_developer_messages("openai/o1-mini")
    await check_developer_messages("openai/o3-mini")
@pytest.mark.anyio
@skip_if_no_openai
async def test_openai_o_series_reasoning_effort() -> None:
    async def check_reasoning_effort(model_name: str):
        model = get_model(
            model_name,
            config=GenerateConfig(reasoning_effort="medium", parallel_tool_calls=True),
        )
        message = ChatMessageUser(content="This is a test string. What are you?")
        response = await model.generate(input=[message])
        assert len(response.completion) >= 1
        print(response)
    await check_reasoning_effort("openai/o1")
    await check_reasoning_effort("openai/o1-mini")
    await check_reasoning_effort("openai/o3-mini")
@pytest.mark.anyio
@skip_if_no_openai
async def test_openai_o_series_max_tokens() -> None:
    async def check_max_tokens(model_name: str):
        model = get_model(
            model_name,
            config=GenerateConfig(max_tokens=4096, reasoning_effort="low"),
        )
        message = ChatMessageUser(content="This is a test string. What are you?")
        response = await model.generate(input=[message])
        assert len(response.completion) >= 1
    await check_max_tokens("openai/o1")
    await check_max_tokens("openai/o1-mini")
    await check_max_tokens("openai/o3-mini")

================
File: tests/model/providers/test_vertex.py
================
import pytest
from test_helpers.utils import skip_if_no_vertex
from inspect_ai import Task, eval
from inspect_ai.dataset import Sample
from inspect_ai.model import (
    ChatMessageUser,
    GenerateConfig,
    get_model,
)
from inspect_ai.scorer import includes
@pytest.mark.anyio
@skip_if_no_vertex
async def test_vertex_api() -> None:
    model = get_model(
        "vertex/gemini-1.5-flash",
        config=GenerateConfig(
            frequency_penalty=0.0,
            stop_seqs=None,
            max_tokens=50,
            presence_penalty=0.0,
            logit_bias=dict([(42, 10), (43, -10)]),
            seed=None,
            temperature=0.0,
            top_p=1.0,
        ),
    )
    message = ChatMessageUser(content="This is a test string. What are you?")
    response = await model.generate(input=[message])
    assert len(response.completion) >= 1
@skip_if_no_vertex
def test_vertex_safety_settings():
    safety_settings = dict(
        dangerous_content="medium_and_above",
        hate_speech="low_and_above",
        unspecified="low_and_above",
    )
    # note: the log doesn't currently have evidence
    # that the safety settings are successfully passed
    # to google, but to the extent we know they are,
    # this tests whether they are correctly mapped to
    # the requisite enum values and that the google
    # api accepts them without error
    eval(
        Task(
            dataset=[Sample(input="What is 1 + 1?", target=["2", "2.0", "Two"])],
            scorer=includes(),
        ),
        model="vertex/gemini-1.5-flash",
        safety_settings=safety_settings,
    )

================
File: tests/model/providers/test_vllm.py
================
import pytest
from test_helpers.utils import skip_if_github_action, skip_if_no_vllm
from inspect_ai.model import (
    ChatMessageUser,
    GenerateConfig,
    get_model,
)
@pytest.mark.anyio
@skip_if_github_action
@skip_if_no_vllm
async def test_vllm_api() -> None:
    model = get_model(
        "vllm/EleutherAI/pythia-70m",
        config=GenerateConfig(
            max_tokens=1,
            seed=42,
            temperature=0.7,
            top_p=0.9,
            top_k=None,
            frequency_penalty=0.0,
            presence_penalty=0.0,
        ),
        device=0,
        # this allows us to run base models with the chat message scaffolding:
        chat_template="{% for message in messages %}{{ message.content }}{% endfor %}",
    )
    message = ChatMessageUser(content="Lorem ipsum dolor")
    response = await model.generate(input=[message])
    assert len(response.completion) >= 1

================
File: tests/model/test_api_key.py
================
import os
import pytest
from test_helpers.utils import (
    skip_if_no_anthropic,
    skip_if_no_google,
    skip_if_no_grok,
    skip_if_no_groq,
    skip_if_no_mistral,
    skip_if_no_openai,
    skip_if_no_together,
    skip_if_trio,
)
from inspect_ai.model import get_model
@pytest.mark.anyio
async def check_explicit_api_key(model_name, env_var):
    api_key = os.environ[env_var]
    del os.environ[env_var]
    try:
        model = get_model(model_name, api_key=api_key)
        await model.generate("What color is your parachute?")
    finally:
        os.environ[env_var] = api_key
@pytest.mark.anyio
@skip_if_no_groq
async def test_groq_api_key():
    await check_explicit_api_key("groq/llama3-70b-8192", "GROQ_API_KEY")
@pytest.mark.anyio
@skip_if_no_openai
async def test_openai_api_key():
    await check_explicit_api_key("openai/gpt-4", "OPENAI_API_KEY")
@pytest.mark.anyio
@skip_if_no_grok
async def test_grok_api_key():
    await check_explicit_api_key("grok/grok-beta", "GROK_API_KEY")
@pytest.mark.anyio
@skip_if_no_anthropic
async def test_anthropic_api_key():
    await check_explicit_api_key(
        "anthropic/claude-3-sonnet-20240229", "ANTHROPIC_API_KEY"
    )
@pytest.mark.anyio
@skip_if_no_google
@skip_if_trio
async def test_google_api_key():
    await check_explicit_api_key("google/gemini-1.5-pro", "GOOGLE_API_KEY")
@pytest.mark.anyio
@skip_if_no_mistral
async def test_mistral_api_key():
    await check_explicit_api_key("mistral/mistral-large-latest", "MISTRAL_API_KEY")
@pytest.mark.anyio
@skip_if_no_together
async def test_together_api_key():
    await check_explicit_api_key("together/google/gemma-2b-it", "TOGETHER_API_KEY")

================
File: tests/model/test_cache.py
================
from datetime import datetime
from test_helpers.utils import run_example
from inspect_ai import Task, eval
from inspect_ai.dataset import Sample
from inspect_ai.log import EvalSample, ModelEvent
from inspect_ai.solver import generate
def test_cache_examples():
    logs = run_example("cache.py", model="mockllm/model")
    assert all(log.status == "success" for log in logs)
def test_cache():
    # helper to check for cache hit
    def sample_cache_hit(sample: EvalSample) -> bool:
        return (
            sum(
                1
                for event in sample.events
                if (isinstance(event, ModelEvent) and event.cache == "read")
            )
            > 0
        )
    timestamp = str(datetime.now())
    def check_eval_with_cache(cache_hit: bool):
        log = eval(
            Task(
                dataset=[Sample(input=f"What is the timestamp: {timestamp}")],
                solver=[generate(cache=True)],
            ),
            model="mockllm/model",
        )[0]
        assert log.samples
        assert sample_cache_hit(log.samples[0]) == cache_hit
    # first eval should miss the cache and the second should hit it
    check_eval_with_cache(False)
    check_eval_with_cache(True)

================
File: tests/model/test_collapse_assistant_message.py
================
import pytest
from inspect_ai.model import (
    ChatMessageAssistant,
    ChatMessageUser,
    ContentImage,
    ContentText,
)
from inspect_ai.model._model import collapse_consecutive_assistant_messages
@pytest.fixture
def user_message_str():
    return ChatMessageUser(content="User message")
@pytest.fixture
def user_message_image_and_str():
    return ChatMessageUser(
        content=[ContentImage(image="foo"), ContentText(text="Message")]
    )
@pytest.fixture
def assistant_message_str():
    return ChatMessageAssistant(content="Assistant message")
def test_collapse_consecutive_assistant_messages_single_assistant_message(
    assistant_message_str,
):
    messages = [assistant_message_str]
    assert collapse_consecutive_assistant_messages(messages) == messages
def test_collapse_consecutive_assistant_messages_alternating_messages(
    user_message_str, user_message_image_and_str, assistant_message_str
):
    messages = [user_message_str]
    assert collapse_consecutive_assistant_messages(messages) == messages
    messages = [user_message_str, assistant_message_str]
    assert collapse_consecutive_assistant_messages(messages) == messages
    messages = [user_message_str, assistant_message_str, user_message_str]
    assert collapse_consecutive_assistant_messages(messages) == messages
    messages = [user_message_str, assistant_message_str, user_message_image_and_str]
    assert collapse_consecutive_assistant_messages(messages) == messages
def test_collapse_consecutive_assistant_messages_consecutive_assistant_messages(
    assistant_message_str,
):
    messages = [assistant_message_str, assistant_message_str, assistant_message_str]
    assert len(collapse_consecutive_assistant_messages(messages)) == 1

================
File: tests/model/test_collapse_user_message.py
================
import pytest
from test_helpers.tools import addition
from test_helpers.utils import skip_if_no_anthropic
from inspect_ai.model import (
    ChatMessageAssistant,
    ChatMessageTool,
    ChatMessageUser,
    ContentImage,
    ContentText,
    get_model,
)
from inspect_ai.model._model import collapse_consecutive_user_messages
from inspect_ai.tool import ToolCall
@pytest.fixture
def user_message_str():
    return ChatMessageUser(content="User message")
@pytest.fixture
def user_message_image_and_str():
    return ChatMessageUser(
        content=[ContentImage(image="foo"), ContentText(text="Message")]
    )
@pytest.fixture
def assistant_message():
    return ChatMessageAssistant(content="Assistant message")
@pytest.fixture
def combined_user_message():
    return ChatMessageUser(
        content=[ContentText(text="Message 1"), ContentText(text="Message 2")]
    )
def test_collapse_consecutive_user_messages_single_user_message(user_message_str):
    messages = [user_message_str]
    assert collapse_consecutive_user_messages(messages) == messages
def test_collapse_consecutive_user_messages_alternating_messages(
    user_message_str, assistant_message
):
    messages = [user_message_str, assistant_message, user_message_str]
    assert collapse_consecutive_user_messages(messages) == messages
def test_collapse_consecutive_user_messages_consecutive_user_messages(user_message_str):
    messages = [user_message_str, user_message_str, user_message_str]
    assert len(collapse_consecutive_user_messages(messages)) == 1
def test_collapse_consecutive_user_messages_with_image_message(
    user_message_image_and_str,
):
    messages = [user_message_image_and_str, user_message_image_and_str]
    assert len(collapse_consecutive_user_messages(messages)) == 1
    assert isinstance(
        collapse_consecutive_user_messages(messages)[0].content[0], ContentImage
    )
@pytest.mark.anyio
@skip_if_no_anthropic
async def test_anthropic_user_tool_messages() -> None:
    # Anthropic converts 'tool' messages into 'user' messages with tool content.
    # In the case where an additional user message is appended after the tool
    # call response, this results in an error unless the user messages are
    # correctly combined into a single one w/ tool and text content.
    try:
        model = get_model("anthropic/claude-3-haiku-20240307")
        await model.generate(
            input=[
                ChatMessageUser(content="What is 1 + 1?"),
                ChatMessageAssistant(
                    content="I can proceed with calling the add tool to compute the result.",
                    tool_calls=[
                        ToolCall(
                            id="toolu_01AhP9RozXEJSnuxMLcY8Xaf",
                            type="function",
                            function="addition",
                            arguments={"x": 1, "y": 1},
                        )
                    ],
                ),
                ChatMessageTool(
                    content="2",
                    tool_call_id="toolu_01AhP9RozXEJSnuxMLcY8Xaf",
                    function="addition",
                ),
                ChatMessageUser(content="Keep going!"),
            ],
            tools=[addition()],
        )
    except Exception as ex:
        pytest.fail(f"Exception raised: {ex}")

================
File: tests/model/test_disable.py
================
import pytest
from test_helpers.utils import skip_if_no_openai
from inspect_ai import Task, eval
from inspect_ai._util.environ import environ_var
from inspect_ai.model._model import get_model
@skip_if_no_openai
def test_disable_model_api_raise_error():
    with environ_var("INSPECT_DISABLE_MODEL_API", "1"):
        with pytest.raises(RuntimeError):
            eval(Task(), model="openai/gpt-4o", debug_errors=True)
@skip_if_no_openai
def test_disable_model_api():
    with environ_var("INSPECT_DISABLE_MODEL_API", "1"):
        log = eval(Task(), model="openai/gpt-4o")[0]
        assert log.status == "error"
        assert log.error
        assert "INSPECT_DISABLE_MODEL_API" in log.error.message
def test_disable_model_api_mockllm():
    with environ_var("INSPECT_DISABLE_MODEL_API", "1"):
        eval(Task(), model="mockllm/model")
@skip_if_no_openai
def test_disable_model_api_only_on_generate():
    with environ_var("INSPECT_DISABLE_MODEL_API", "1"):
        get_model("openai/gpt-4o")

================
File: tests/model/test_get_model.py
================
import pytest
from test_helpers.utils import (
    skip_if_no_openai,
    skip_if_no_openai_package,
    skip_if_trio,
)
from inspect_ai.model import GenerateConfig, get_model
@pytest.mark.anyio
@skip_if_no_openai_package
@skip_if_trio
async def test_model_caching_enabled():
    """Test that identical model requests return the same cached instance."""
    # Get the same model twice
    model1 = get_model("bedrock/model")
    model2 = get_model("bedrock/model")
    # Verify we got the same instance
    assert model1 is model2
@pytest.mark.anyio
@skip_if_no_openai_package
@skip_if_trio
async def test_model_caching_disabled():
    """Test that caching can be disabled."""
    # Get the same model twice with caching disabled
    model1 = get_model("bedrock/model", memoize=False)
    model2 = get_model("bedrock/model", memoize=False)
    # Verify we got different instances
    assert model1 is not model2
@pytest.mark.anyio
@skip_if_no_openai_package
@skip_if_trio
async def test_different_configs_different_cache():
    """Test that different configs result in different cached instances."""
    config1 = GenerateConfig(temperature=0.7)
    config2 = GenerateConfig(temperature=0.9)
    model1 = get_model("bedrock/model", config=config1)
    model2 = get_model("bedrock/model", config=config2)
    # Verify we got different instances
    assert model1 is not model2
@pytest.mark.anyio
@skip_if_trio
async def test_different_provider_params_different_cache():
    """Test that different provider params result in different cached instances."""
    model1 = get_model("bedrock/model", param1=["value1"])
    model2 = get_model("bedrock/model", param1=["value2"])
    # Verify we got different instances
    assert model1 is not model2
@pytest.mark.anyio
@skip_if_no_openai
async def test_context_manager():
    """Test the async context manager functionality."""
    async with get_model("openai/gpt-4o-mini") as model:
        await model.generate("Say hello")
    # Verify the model was closed
    assert hasattr(model, "_closed") and model._closed
@pytest.mark.anyio
@skip_if_no_openai_package
@skip_if_trio
async def test_context_manager_exception_handling():
    """Test that the context manager properly handles exceptions."""
    with pytest.raises(ValueError):
        async with get_model("bedrock/model") as model:
            raise ValueError("Test error")
    # Verify the model was closed despite the exception
    assert hasattr(model, "_closed") and model._closed
@pytest.mark.anyio
@skip_if_no_openai_package
@skip_if_trio
async def test_cache_consistency():
    """Test that cached models maintain consistent state."""
    model1 = get_model("bedrock/model")
    # Set some attribute on the first instance
    model1._test_attr = "test_value"
    # Get the same model again
    model2 = get_model("bedrock/model")
    # Verify the attribute is present on the cached instance
    assert hasattr(model2, "_test_attr")
    assert model2._test_attr == "test_value"

================
File: tests/model/test_is_rate_limit.py
================
import logging
from inspect_ai._util.constants import PKG_NAME
from inspect_ai.model._chat_message import ChatMessage
from inspect_ai.model._generate_config import GenerateConfig
from inspect_ai.model._model import Model, ModelAPI
from inspect_ai.model._model_call import ModelCall
from inspect_ai.model._model_output import ModelOutput
from inspect_ai.tool._tool_choice import ToolChoice
from inspect_ai.tool._tool_info import ToolInfo
def test_is_rate_limit_deprecation(caplog):
    # class which implements deprecated api
    class TestAPI(ModelAPI):
        def __init__(self) -> None:
            super().__init__("model")
        async def generate(
            self,
            input: list[ChatMessage],
            tools: list[ToolInfo],
            tool_choice: ToolChoice,
            config: GenerateConfig,
        ) -> ModelOutput | tuple[ModelOutput | Exception, ModelCall]:
            return ModelOutput.from_content("model", "foo")
        def is_rate_limit(self, ex: BaseException) -> bool:
            return True
    # add our logging handler to the caplog handler
    logger = logging.getLogger(PKG_NAME)
    logger.propagate = True
    try:
        # confirm that the deprecated api is called and triggers a warning
        model = Model(TestAPI(), GenerateConfig())
        assert model.should_retry(Exception()) is True
        assert any(
            "deprecated is_rate_limit() method" in record.message
            for record in caplog.records
        ), "Expected warning not found in logs"
    finally:
        logger.propagate = False

================
File: tests/model/test_logprobs.py
================
import pytest
from test_helpers.utils import (
    skip_if_github_action,
    skip_if_no_accelerate,
    skip_if_no_grok,
    skip_if_no_llama_cpp_python,
    skip_if_no_openai,
    skip_if_no_together,
    skip_if_no_transformers,
    skip_if_no_vllm,
)
from inspect_ai.model import ChatMessageUser, GenerateConfig, ModelOutput, get_model
async def generate_with_logprobs(model_name, **model_kwargs) -> ModelOutput:
    model = get_model(
        model_name,
        config=GenerateConfig(
            logprobs=True, top_logprobs=2, temperature=0.001, max_tokens=50
        ),
        **model_kwargs,
    )
    message = ChatMessageUser(content="Hello.")
    return await model.generate(input=[message])
@pytest.mark.anyio
@skip_if_no_openai
async def test_openai_logprobs() -> None:
    response = await generate_with_logprobs("openai/gpt-3.5-turbo")
    assert response.choices[0].logprobs is not None
    assert response.choices[0].logprobs.content[0].top_logprobs is not None
    assert len(response.choices[0].logprobs.content[0].top_logprobs) == 2
@pytest.mark.anyio
@skip_if_no_grok
async def test_grok_logprobs() -> None:
    response = await generate_with_logprobs("grok/grok-beta")
    assert response.choices[0].logprobs is not None
    assert response.choices[0].logprobs.content[0].top_logprobs is not None
@pytest.mark.anyio
@skip_if_no_together
async def test_together_logprobs() -> None:
    response = await generate_with_logprobs(
        "together/meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo"
    )
    assert (
        response.choices[0].logprobs is not None
        and response.choices[0].logprobs.content[0].top_logprobs
        is None  # together only ever returns top-1, so top_logprobs should always be None
    )
@pytest.mark.anyio
@skip_if_github_action
@skip_if_no_transformers
@skip_if_no_accelerate
async def test_hf_logprobs() -> None:
    response = await generate_with_logprobs(
        "hf/EleutherAI/pythia-70m",
        chat_template="{% for message in messages %}{{ message.content }}{% endfor %}",
    )
    assert (
        response.choices[0].logprobs
        and response.choices[0].logprobs.content[0].top_logprobs is not None
    )
    assert len(response.choices[0].logprobs.content[0].top_logprobs) == 2
@pytest.mark.anyio
@skip_if_github_action
@skip_if_no_vllm
async def test_vllm_logprobs() -> None:
    response = await generate_with_logprobs(
        "vllm/EleutherAI/pythia-70m",
        chat_template="{% for message in messages %}{{ message.content }}{% endfor %}",
    )
    assert (
        response.choices[0].logprobs
        and response.choices[0].logprobs.content[0].top_logprobs is not None
    )
    assert len(response.choices[0].logprobs.content[0].top_logprobs) == 2
@pytest.mark.anyio
@skip_if_github_action
@skip_if_no_llama_cpp_python
async def test_llama_cpp_python_logprobs() -> None:
    response = await generate_with_logprobs("llama-cpp-python/default")
    assert (
        response.choices[0].logprobs
        and response.choices[0].logprobs.content[0].top_logprobs is not None
    )
    assert len(response.choices[0].logprobs.content[0].top_logprobs) == 2

================
File: tests/model/test_mock_model_llm.py
================
import pytest
from test_helpers.utils import skip_if_trio
from inspect_ai import Task, eval
from inspect_ai.dataset import Sample
from inspect_ai.model import ModelOutput, get_model
from inspect_ai.model._providers.mockllm import MockLLM
from inspect_ai.scorer import includes
from inspect_ai.solver import generate
@pytest.mark.asyncio
@skip_if_trio
async def test_mock_generate_default() -> None:
    model = get_model("mockllm/model")
    response = await model.generate(input="unused")
    assert response.completion == MockLLM.default_output
@pytest.mark.asyncio
@skip_if_trio
async def test_mock_generate_custom_valid() -> None:
    custom_content_str = "custom #content"
    model = get_model(
        "mockllm/model",
        custom_outputs=[
            ModelOutput.from_content(model="mockllm", content=custom_content_str)
        ],
    )
    response = await model.generate(input="unused input")
    assert response.completion == custom_content_str
@pytest.mark.asyncio
@skip_if_trio
async def test_mock_generate_custom_invalid() -> None:
    model = get_model(
        "mockllm/model",
        custom_outputs=["this list of strings should actually be a ModelOutput"],
    )
    with pytest.raises(ValueError) as e_info:
        await model.generate([])
    assert "must be an instance of ModelOutput" in str(e_info.value)
@pytest.mark.asyncio
@skip_if_trio
async def test_mock_generate_custom_invalid_iterable_string() -> None:
    model = get_model(
        "mockllm/model",
        custom_outputs="this string should actually be a ModelOutput",
    )
    with pytest.raises(ValueError) as e_info:
        await model.generate([])
    assert "must be an instance of ModelOutput" in str(e_info.value)
@pytest.mark.asyncio
@skip_if_trio
async def test_mock_generate_custom_invalid_iterable_number() -> None:
    with pytest.raises(ValueError) as e_info:
        get_model(
            "mockllm/model",
            custom_outputs=0,
        )
    assert "must be an Iterable or a Generator" in str(e_info.value)
@pytest.mark.asyncio
@skip_if_trio
async def test_mock_generate_not_enough() -> None:
    model = get_model(
        "mockllm/model",
        custom_outputs=[
            ModelOutput.from_content(model="mockllm", content="first response"),
            ModelOutput.from_content(model="mockllm", content="second response"),
        ],
    )
    await model.generate(input="unused input")
    await model.generate(input="unused input")
    with pytest.raises(ValueError) as e_info:
        await model.generate(input="unused input")
        assert "custom_outputs ran out of values" in str(e_info.value)
def test_mock_model_eval():
    task = Task(
        dataset=[
            Sample(
                input="your responses are laughably predictable",
                target=MockLLM.default_output,
            ),
        ],
        solver=[generate()],
        scorer=includes(),
    )
    result = eval(task, model="mockllm/model")[0]
    assert result.status == "success"
    assert result.samples[0].messages[-1].content == MockLLM.default_output

================
File: tests/model/test_no_model.py
================
from test_helpers.utils import identity_solver
from inspect_ai import Task, eval
def test_no_model():
    log = eval(Task(solver=identity_solver()), model=None)[0]
    assert log.status == "success"
    assert log.eval.model == "none/none"

================
File: tests/model/test_num_choices.py
================
import pytest
from test_helpers.utils import (
    skip_if_github_action,
    skip_if_no_grok,
    skip_if_no_openai,
    skip_if_no_together,
    skip_if_no_vllm,
)
from inspect_ai.model import GenerateConfig, get_model
async def generate(model_name):
    model = get_model(model_name)
    return await model.generate(input="Hello.", config=GenerateConfig(num_choices=3))
async def check_num_choices(model_name):
    model = get_model(model_name)
    response = await model.generate(
        input="Hello.", config=GenerateConfig(num_choices=3)
    )
    assert len(response.choices) == 3
@pytest.mark.anyio
@skip_if_no_openai
async def test_openai_num_choices() -> None:
    await check_num_choices("openai/gpt-3.5-turbo")
@skip_if_no_grok
async def test_grok_num_choices() -> None:
    await check_num_choices("grok/grok-beta")
@pytest.mark.anyio
@skip_if_no_together
async def test_together_num_choices() -> None:
    await check_num_choices("together/google/gemma-2b-it")
@pytest.mark.anyio
@skip_if_github_action
@skip_if_no_vllm
async def test_vllm_num_choices() -> None:
    await check_num_choices("vllm/EleutherAI/pythia-70m")

================
File: tests/model/test_parse_tool_call.py
================
from inspect_ai.model._call_tools import parse_tool_call
from inspect_ai.tool import ToolInfo, ToolParam, ToolParams
testing_tool = ToolInfo(
    name="testing_tool",
    description="This is a testing tool.",
    parameters=ToolParams(
        properties={
            "param1": ToolParam(
                type="string",
                description="This is parameter1",
            )
        },
        required=["param1"],
    ),
)
testing_tool_bool = ToolInfo(
    name="testing_tool",
    description="This is a testing tool, with a boolean parameter.",
    parameters=ToolParams(
        properties={
            "param1": ToolParam(
                type="boolean",
                description="This is parameter1",
            )
        },
        required=["param1"],
    ),
)
def test_parse_tool_call_with_a_dict():
    tool_call = parse_tool_call(
        "id", "testing_tool", '{"param1": "I am a dictionary!"}', [testing_tool]
    )
    assert "param1" in tool_call.arguments
    assert tool_call.arguments["param1"] == "I am a dictionary!"
def test_parse_string_tool_call_without_a_dict():
    tool_call = parse_tool_call(
        "id", "testing_tool", "I am not a dictionary!", [testing_tool]
    )
    assert "param1" in tool_call.arguments
    # When passed a non-dictonary input, for tools with a single argument, the tool should try to set that argument to the value of the input string.
    assert tool_call.arguments["param1"] == "I am not a dictionary!"
def test_parse_bool_tool_call_without_a_dict():
    # Tests if the parser correctly handles boolean values.
    tool_call = parse_tool_call("id", "testing_tool", "True", [testing_tool_bool])
    assert "param1" in tool_call.arguments
    assert tool_call.arguments["param1"]
def test_parse_single_quotes_in_string_tool_call_without_a_dict():
    # Tests if the parser correctly handles a string with a single quote, when they haven't been given as part of a JSON.
    tool_call = parse_tool_call("id", "testing_tool", "'", [testing_tool])
    assert "param1" in tool_call.arguments
    assert tool_call.arguments["param1"] == "'"

================
File: tests/model/test_reasoning_claude.py
================
import pytest
from test_helpers.utils import skip_if_no_anthropic
from inspect_ai import Task, eval
from inspect_ai._util.content import ContentReasoning
from inspect_ai.dataset import Sample
from inspect_ai.model._generate_config import GenerateConfig
from .test_reasoning_content import check_reasoning_content
@pytest.mark.anyio
@skip_if_no_anthropic
async def test_reasoning_claude():
    await check_reasoning_content("anthropic/claude-3-7-sonnet-20250219")
@pytest.mark.anyio
@skip_if_no_anthropic
async def test_reasoning_claude_ignore_unsupported():
    await check_reasoning_content(
        "anthropic/claude-3-7-sonnet-20250219",
        config=GenerateConfig(temperature=0.9, top_p=3, top_k=3),
    )
@pytest.mark.anyio
@skip_if_no_anthropic
async def test_reasoning_claude_force_history():
    await check_reasoning_content(
        "anthropic/claude-3-7-sonnet-20250219",
        config=GenerateConfig(reasoning_history="none"),
    )
def check_max_tokens(max_tokens: int | None, reasoning_tokens: int, check_tokens: int):
    task = Task(dataset=[Sample(input="Please say 'hello, world'")])
    log = eval(
        task,
        log_format="json",
        model="anthropic/claude-3-7-sonnet-20250219",
        max_tokens=max_tokens,
        reasoning_tokens=reasoning_tokens,
    )[0]
    log_json = log.model_dump_json(indent=2)
    assert f'"max_tokens": {check_tokens}' in log_json
    assert log.status == "success"
DEFAULT_MAX_TOKENS = 4096
@skip_if_no_anthropic
def test_reasoning_claude_max_tokens():
    check_max_tokens(None, 1024, DEFAULT_MAX_TOKENS + 1024)
    check_max_tokens(5000, 2000, 5000)
    check_max_tokens(None, 8096, DEFAULT_MAX_TOKENS + 8096)
@skip_if_no_anthropic
def test_reasoning_claude_streaming():
    reasoning_tokens = 32 * 1024
    check_max_tokens(None, reasoning_tokens, DEFAULT_MAX_TOKENS + reasoning_tokens)
@skip_if_no_anthropic
def test_reasoning_claude_redacted():
    task = Task(
        dataset=[
            Sample(
                input="ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB"
            )
        ]
    )
    log = eval(
        task,
        model="anthropic/claude-3-7-sonnet-20250219",
        reasoning_tokens=1024,
    )[0]
    assert log.samples
    output = log.samples[0].output
    content = output.choices[0].message.content
    assert isinstance(content, list)
    assert isinstance(content[0], ContentReasoning)
    assert content[0].redacted
@skip_if_no_anthropic
def test_reasoning_claude_token_count():
    task = Task(dataset=[Sample(input="Please say 'hello, world'")])
    log = eval(
        task,
        model="anthropic/claude-3-7-sonnet-20250219",
        reasoning_tokens=1024,
    )[0]
    assert log.samples
    output = log.samples[0].output
    assert output.usage.reasoning_tokens > 0

================
File: tests/model/test_reasoning_content.py
================
from typing import Literal
import pytest
from test_helpers.utils import (
    skip_if_no_google,
    skip_if_no_groq,
    skip_if_no_together,
    skip_if_trio,
)
from inspect_ai import Task, eval
from inspect_ai._util.content import ContentReasoning
from inspect_ai.dataset._dataset import Sample
from inspect_ai.log._condense import resolve_sample_attachments
from inspect_ai.model._generate_config import GenerateConfig
from inspect_ai.model._model import get_model
from inspect_ai.solver._prompt import user_message
from inspect_ai.solver._solver import generate
@pytest.mark.anyio
@skip_if_no_together
async def test_reasoning_content_together():
    await check_reasoning_content("together/deepseek-ai/DeepSeek-R1")
@pytest.mark.anyio
@skip_if_no_groq
async def test_reasoning_content_groq():
    await check_reasoning_content("groq/deepseek-r1-distill-llama-70b")
@pytest.mark.slow
@skip_if_no_together
def test_reasoning_history_none():
    check_reasoning_history("none", 0)
@pytest.mark.slow
@skip_if_no_together
def test_reasoning_history_all():
    check_reasoning_history("all", 2)
@pytest.mark.slow
@skip_if_no_together
def test_reasoning_history_last():
    check_reasoning_history("last", 1)
@skip_if_no_google
@skip_if_trio
def test_reasoning_content_google():
    log = eval(
        Task(dataset=[Sample(input="Solve 3*x^3-5*x=1")]),
        model="google/gemini-2.0-flash-thinking-exp",
    )[0]
    assert log.samples
    content = log.samples[0].output.message.content
    assert isinstance(content, list)
    assert isinstance(content[0], ContentReasoning)
async def check_reasoning_content(
    model_name: str, config: GenerateConfig = GenerateConfig()
):
    model = get_model(model_name)
    output = await model.generate(
        "Please say 'hello, world'",
        config=config.merge(
            GenerateConfig(reasoning_effort="low", reasoning_tokens=1024)
        ),
    )
    assert "<think>" not in output.completion
    content = output.choices[0].message.content
    assert isinstance(content, list)
    assert isinstance(content[0], ContentReasoning)
def check_reasoning_history(
    reasoning_history: Literal["none", "all", "last"], expected_thinking_blocks: int
):
    task = Task(
        dataset=[Sample(input="Please say hello, world")],
        solver=[
            generate(),
            user_message("Great!, now say 'goodbye, world'"),
            generate(),
            user_message("Great!, Now explain what you just did."),
            generate(),
        ],
    )
    log = eval(
        task,
        model="together/deepseek-ai/DeepSeek-R1",
        reasoning_history=reasoning_history,
    )[0]
    assert log.samples
    sample = resolve_sample_attachments(log.samples[0])
    last_model_event = [event for event in sample.events if event.event == "model"][-1]
    # count thinking in payload sent to server
    thinking_blocks = sum(
        1 for message in last_model_event.input if "<think>" in message.text
    )
    assert thinking_blocks == expected_thinking_blocks

================
File: tests/model/test_reasoning_parse.py
================
from inspect_ai.model._reasoning import parse_content_with_reasoning
def test_reasoning_parse_basic():
    result = parse_content_with_reasoning("<think>Simple reasoning</think>Normal text")
    assert result is not None
    assert result.reasoning == "Simple reasoning"
    assert result.content == "Normal text"
    assert result.signature is None
    assert result.redacted is False
def test_reasoning_parse_with_leading_whitespace():
    result = parse_content_with_reasoning(
        "   \n  <think>Indented reasoning</think>Text"
    )
    assert result is not None
    assert result.reasoning == "Indented reasoning"
    assert result.content == "Text"
    assert result.signature is None
    assert result.redacted is False
def test_reasoning_parse_with_trailing_whitespace():
    result = parse_content_with_reasoning("<think>Reasoning</think>   \n  Text   \n")
    assert result is not None
    assert result.reasoning == "Reasoning"
    assert result.content == "Text"
    assert result.signature is None
    assert result.redacted is False
def test_reasoning_parse_with_newlines_in_reasoning():
    result = parse_content_with_reasoning("<think>Multi\nline\nreasoning</think>Text")
    assert result is not None
    assert result.reasoning == "Multi\nline\nreasoning"
    assert result.content == "Text"
    assert result.signature is None
    assert result.redacted is False
def test_reasoning_parse_empty():
    result = parse_content_with_reasoning("<think></think>Text")
    assert result is not None
    assert result.reasoning == ""
    assert result.content == "Text"
    assert result.signature is None
    assert result.redacted is False
def test_reasoning_parse_empty_content():
    result = parse_content_with_reasoning("<think>Just reasoning</think>")
    assert result is not None
    assert result.reasoning == "Just reasoning"
    assert result.content == ""
    assert result.signature is None
    assert result.redacted is False
def test_reasoning_parse_whitespace_everywhere():
    result = parse_content_with_reasoning("""
        <think>
            Messy
            reasoning
        </think>
            Messy
            text
    """)
    assert result is not None
    assert result.reasoning == "Messy\n            reasoning"
    assert result.content == "Messy\n            text"
    assert result.signature is None
    assert result.redacted is False
def test_reasoning_parse_no_think_tag():
    result = parse_content_with_reasoning("No think tag here")
    assert result is None
def test_reasoning_parse_unclosed_tag():
    result = parse_content_with_reasoning("<think>Unclosed reasoning")
    assert result is None
# New tests for signature attribute
def test_reasoning_parse_with_signature():
    result = parse_content_with_reasoning(
        '<think signature="45ef5ab">Reasoning with signature</think>Content'
    )
    assert result is not None
    assert result.reasoning == "Reasoning with signature"
    assert result.content == "Content"
    assert result.signature == "45ef5ab"
    assert result.redacted is False
# New tests for redacted attribute
def test_reasoning_parse_with_redacted():
    result = parse_content_with_reasoning(
        '<think redacted="true">Redacted reasoning</think>Content'
    )
    assert result is not None
    assert result.reasoning == "Redacted reasoning"
    assert result.content == "Content"
    assert result.signature is None
    assert result.redacted is True
# New tests for both attributes
def test_reasoning_parse_with_signature_and_redacted():
    result = parse_content_with_reasoning(
        '<think signature="45ef5ab" redacted="true">Both attributes</think>Content'
    )
    assert result is not None
    assert result.reasoning == "Both attributes"
    assert result.content == "Content"
    assert result.signature == "45ef5ab"
    assert result.redacted is True
# Test with whitespace in attributes
def test_reasoning_parse_with_whitespace_in_attributes():
    result = parse_content_with_reasoning(
        '<think  signature="45ef5ab"  redacted="true" >Whitespace in attributes</think>Content'
    )
    assert result is not None
    assert result.reasoning == "Whitespace in attributes"
    assert result.content == "Content"
    assert result.signature == "45ef5ab"
    assert result.redacted is True
# Test with attributes and multiline content
def test_reasoning_parse_with_attributes_and_multiline():
    result = parse_content_with_reasoning("""
        <think signature="45ef5ab" redacted="true">
            Complex
            reasoning
        </think>
            Content
            here
    """)
    assert result is not None
    assert result.reasoning == "Complex\n            reasoning"
    assert result.content == "Content\n            here"
    assert result.signature == "45ef5ab"
    assert result.redacted is True

================
File: tests/model/test_stop_reason.py
================
import pytest
from test_helpers.utils import (
    skip_if_no_anthropic,
    skip_if_no_grok,
    skip_if_no_groq,
    skip_if_no_mistral,
    skip_if_no_openai,
    skip_if_no_together,
    skip_if_trio,
)
from inspect_ai.model import GenerateConfig, ModelOutput, get_model
async def generate(model_name) -> ModelOutput:
    model = get_model(model_name)
    return await model.generate(input="Hello.")
async def generate_token_limit(model_name) -> ModelOutput:
    model = get_model(model_name)
    return await model.generate(
        input="Tell me a story.", config=GenerateConfig(max_tokens=2)
    )
async def check_stop_reason(model_name):
    response = await generate(model_name)
    assert response.choices[0].stop_reason == "stop"
    response = await generate_token_limit(model_name)
    assert response.choices[0].stop_reason == "max_tokens"
@pytest.mark.asyncio
@skip_if_no_groq
@skip_if_trio
async def test_groq_stop_reason() -> None:
    await check_stop_reason("groq/llama3-70b-8192")
@pytest.mark.asyncio
@skip_if_no_openai
@skip_if_trio
async def test_openai_stop_reason() -> None:
    await check_stop_reason("openai/gpt-3.5-turbo")
@pytest.mark.asyncio
@skip_if_no_anthropic
@skip_if_trio
async def test_anthropic_stop_reason() -> None:
    await check_stop_reason("anthropic/claude-3-haiku-20240307")
@pytest.mark.asyncio
@skip_if_no_mistral
@skip_if_trio
async def test_mistral_stop_reason() -> None:
    await check_stop_reason("mistral/mistral-medium-latest")
@pytest.mark.asyncio
@skip_if_no_grok
@skip_if_trio
async def test_grok_stop_reason() -> None:
    await check_stop_reason("grok/grok-beta")
@pytest.mark.asyncio
@skip_if_no_together
@skip_if_trio
async def test_together_stop_reason() -> None:
    await check_stop_reason("together/google/gemma-2b-it")

================
File: tests/model/test_structured_output.py
================
from pydantic import BaseModel, ValidationError
from test_helpers.utils import (
    skip_if_no_google,
    skip_if_no_mistral,
    skip_if_no_openai,
    skip_if_trio,
)
from inspect_ai import Task, eval, task
from inspect_ai.dataset import Sample
from inspect_ai.model import GenerateConfig, ResponseSchema
from inspect_ai.scorer import (
    CORRECT,
    INCORRECT,
    Score,
    Target,
    accuracy,
    scorer,
    stderr,
)
from inspect_ai.solver import TaskState, generate
from inspect_ai.util import json_schema
class Color(BaseModel):
    red: int
    green: int
    blue: int
@task
def rgb_color():
    return Task(
        dataset=[
            Sample(
                input="What is the RGB color for white?",
                target="255,255,255",
            )
        ],
        solver=generate(),
        scorer=score_color(),
        config=GenerateConfig(
            response_schema=ResponseSchema(name="color", json_schema=json_schema(Color))
        ),
    )
@scorer(metrics=[accuracy(), stderr()])
def score_color():
    async def score(state: TaskState, target: Target):
        try:
            color = Color.model_validate_json(state.output.completion)
            if f"{color.red},{color.green},{color.blue}" == target.text:
                value = CORRECT
            else:
                value = INCORRECT
            return Score(
                value=value,
                answer=state.output.completion,
            )
        except ValidationError as ex:
            return Score(
                value=INCORRECT,
                answer=state.output.completion,
                explanation=f"Error parsing response: {ex}",
            )
    return score
def check_structured_output(model):
    log = eval(rgb_color(), model=model)[0]
    assert log.status == "success"
    assert log.results.scores[0].metrics["accuracy"].value == 1
@skip_if_no_openai
def test_openai_structured_output():
    check_structured_output("openai/gpt-4o-mini")
@skip_if_no_google
@skip_if_trio
def test_google_structured_output():
    check_structured_output("google/gemini-2.0-flash")
@skip_if_no_mistral
def test_mistral_structured_output():
    check_structured_output("mistral/mistral-large-latest")

================
File: tests/model/test_tool_result_images_as_user_message.py
================
from inspect_ai._util.content import ContentText
from inspect_ai.model import ChatMessage, ChatMessageTool, ChatMessageUser, ContentImage
from inspect_ai.model._model import tool_result_images_as_user_message
# see bug description in https://github.com/UKGovernmentBEIS/inspect_ai/pull/1217
def test_multiple_tool_responses_remain_adjacent():
    tool_a = ChatMessageTool(
        tool_call_id="a",
        content=[ContentImage(image="image_for_a")],
    )
    tool_b = ChatMessageTool(
        tool_call_id="b",
        content=[ContentImage(image="image_for_b")],
    )
    execute_and_assert(
        [tool_a, tool_b],
        [
            _modified_image_response_message(tool_a),
            _modified_image_response_message(tool_b),
            ChatMessageUser(
                content=tool_a.content + tool_b.content,
                tool_call_id=["a", "b"],
            ),
        ],
    )
def test_multiple_tool_responses_remain_adjacent_when_not_at_end_of_list():
    tool_a = ChatMessageTool(
        tool_call_id="a",
        content=[ContentImage(image="image_for_a")],
    )
    tool_b = ChatMessageTool(
        tool_call_id="b",
        content=[ContentImage(image="image_for_b")],
    )
    user = ChatMessageUser(content="yo")
    execute_and_assert(
        [tool_a, tool_b, user],
        [
            _modified_image_response_message(tool_a),
            _modified_image_response_message(tool_b),
            ChatMessageUser(
                content=tool_a.content + tool_b.content,
                tool_call_id=["a", "b"],
            ),
            user,
        ],
    )
def execute_and_assert(input_messages: list[ChatMessage], expected: list[ChatMessage]):
    # transform the messages
    transformed = tool_result_images_as_user_message(input_messages)
    # compare based on content (as id can't be known in advance)
    assert all([t.content == e.content] for t, e in zip(transformed, expected))
def _modified_image_response_message(message: ChatMessageTool) -> ChatMessageTool:
    return message.model_copy(
        update={"content": [ContentText(text="Image content is included below.")]}
    )

================
File: tests/scorer/test_answer.py
================
import pytest
from test_helpers.utils import simple_task_state
from inspect_ai.scorer import CORRECT, INCORRECT, Target, answer
@pytest.mark.anyio
async def test_letter_success():
    scorer = answer("letter")
    state = simple_task_state(model_output="ANSWER: B")
    result = await scorer(state, Target(["B"]))
    assert result.text == CORRECT
@pytest.mark.anyio
async def test_letter_failure():
    scorer = answer("letter")
    state = simple_task_state(model_output="ANSWER: B")
    result = await scorer(state, Target(["C"]))
    assert result.text == INCORRECT
@pytest.mark.anyio
async def test_word_success():
    scorer = answer("word")
    state = simple_task_state(model_output="ANSWER: Yes")
    result = await scorer(state, Target(["Yes"]))
    assert result.text == CORRECT
@pytest.mark.anyio
async def test_word_failure():
    scorer = answer("letter")
    state = simple_task_state(model_output="ANSWER: Yes")
    result = await scorer(state, Target(["No"]))
    assert result.text == INCORRECT
@pytest.mark.anyio
async def test_line_success():
    scorer = answer("line")
    state = simple_task_state(model_output="ANSWER:\nThis is a whole new line")
    result = await scorer(state, Target(["This is a whole new line"]))
    assert result.text == CORRECT
@pytest.mark.anyio
async def test_line_failure():
    scorer = answer("line")
    state = simple_task_state(model_output="ANSWER:\nThis is a whole new line")
    result = await scorer(state, Target(["This doesn't match does it?"]))
    assert result.text == INCORRECT

================
File: tests/scorer/test_choice.py
================
from random import Random
import pytest
from test_helpers.utils import simple_task_state
from inspect_ai.scorer import CORRECT, INCORRECT, Target, choice
@pytest.mark.anyio
async def test_score_single_letter():
    scorer = choice()
    state = simple_task_state(
        model_output="ANSWER: A",
        choices=["choice 1", "choice 2"],
    )
    state.choices.mark_choice(0, True)
    state.choices.mark_choice(1, False)
    result = await scorer(state, Target("A"))
    assert result.text == CORRECT
    assert result.answer == "A"
    assert result.explanation == "ANSWER: A"
@pytest.mark.anyio
async def test_score_multiple_letters():
    scorer = choice()
    state = simple_task_state(
        model_output="ANSWER: A, B",
        choices=["choice 1", "choice 2"],
    )
    state.choices.mark_choice(0, True)
    state.choices.mark_choice(1, True)
    result = await scorer(state, Target(["A", "B"]))
    assert result.text == CORRECT
    assert result.answer == "A, B"
    assert result.explanation == "ANSWER: A, B"
@pytest.mark.anyio
async def test_letter_incorrect_if_false_positives_found():
    scorer = choice()
    state = simple_task_state(
        model_output="ANSWER: A, B",
        choices=["choice 1", "choice 2"],
    )
    state.choices.mark_choice(0, True)
    state.choices.mark_choice(1, True)
    # Model generated B as an answer but not valid
    result = await scorer(state, Target("A"))
    assert result.text == INCORRECT
    assert result.answer == "A, B"
    assert result.explanation == "ANSWER: A, B"
@pytest.mark.anyio
async def test_letter_incorrect_if_missed_correct_answers():
    scorer = choice()
    state = simple_task_state(
        model_output="ANSWER: A",
        choices=["choice 1", "choice 2"],
    )
    state.choices.mark_choice(0, True)
    state.choices.mark_choice(1, False)
    # Model generated B as an answer but not valid
    result = await scorer(state, Target(["A", "B"]))
    assert result.text == INCORRECT
    assert result.answer == "A"
    assert result.explanation == "ANSWER: A"
@pytest.mark.anyio
async def test_score_shuffled_positions_letter():
    scorer = choice()
    state = simple_task_state(
        # "A" is the correct answer ("choice 3") as far as the model is concerned
        model_output="ANSWER: A",
        # Choices initially in order from Sample, Target below says "choice 3"
        # is correct
        choices=["choice 1", "choice 2", "choice 3"],
    )
    state.choices.shuffle(Random(4))
    state.choices.mark_choice(0, True)  # choice 3 because of shuffling
    state.choices.mark_choice(1, False)  # choice 2 because of shuffling
    state.choices.mark_choice(2, False)  # choice 1 because of shuffling
    # The target is based on unshuffled choices, choice corresponding to
    # "choice 3" is correct
    result = await scorer(state, Target(["C"]))
    assert result.text == CORRECT
    assert result.answer == "C"
    assert (
        result.explanation
        == "Choices were shuffled before generating a response, the following was sent to the model:\n\nA) choice 3\nB) choice 2\nC) choice 1\nShuffled answer:\nANSWER: A"
    )
@pytest.mark.anyio
async def test_correct_single_answer():
    scorer = choice()
    state = simple_task_state(model_output="ANSWERS: A", choices=["A"])
    state.choices.mark_choice(0, True)
    result = await scorer(state, Target(["A"]))
    assert result.text == CORRECT
    assert result.answer == "A"
    assert result.explanation == "ANSWERS: A"
@pytest.mark.anyio
async def test_correct_multiple_answers_one_incorrect():
    scorer = choice()
    state = simple_task_state(model_output="ANSWERS: ", choices=["A"])
    state.choices.mark_choice(0, False)
    result = await scorer(state, Target([""]))
    assert result.text == CORRECT
    assert result.answer == ""
    assert result.explanation == "ANSWERS: "
@pytest.mark.anyio
async def test_correct_multiple_answers_all_incorrect():
    scorer = choice()
    state = simple_task_state(model_output="ANSWERS: ", choices=["A", "B"])
    state.choices.mark_choice(0, False)
    state.choices.mark_choice(1, False)
    result = await scorer(state, Target(""))
    assert result.text == CORRECT
    assert result.answer == ""
    assert result.explanation == "ANSWERS: "

================
File: tests/scorer/test_classification.py
================
import pytest
from test_helpers.utils import simple_task_state
from inspect_ai.scorer import Target
from inspect_ai.scorer._classification import exact, f1
from inspect_ai.scorer._metric import CORRECT, INCORRECT
@pytest.mark.anyio
async def test_exact_match():
    scorer = exact()
    state = simple_task_state(model_output="foo")
    result = await scorer(state, Target(["foo"]))
    assert result.text == CORRECT
@pytest.mark.anyio
async def test_exact_match_max():
    scorer = exact()
    state = simple_task_state(model_output="foo")
    result = await scorer(state, Target(["foobar", "boofar", "foo"]))
    assert result.text == CORRECT
@pytest.mark.anyio
async def test_exact_nonmatch():
    scorer = exact()
    state = simple_task_state(model_output="foo1")
    result = await scorer(state, Target(["foo"]))
    assert result.text == INCORRECT
@pytest.mark.anyio
async def test_f1_basic_match():
    scorer = f1()
    state = simple_task_state(model_output="foo")
    result = await scorer(state, Target(["foo"]))
    assert result.text == "1.0"
@pytest.mark.anyio
async def test_f1_basic_nonmatch():
    scorer = f1()
    state = simple_task_state(model_output="foo1")
    result = await scorer(state, Target(["foo"]))
    assert result.text == "0.0"
@pytest.mark.anyio
async def test_f1_good_match():
    scorer = f1()
    state = simple_task_state(model_output="Paris")
    result = await scorer(state, Target(["Paris, Texas", "Paris"]))
    assert result.text == "1.0"
@pytest.mark.anyio
async def test_f1_partial_match():
    scorer = f1()
    state = simple_task_state(model_output="Paris")
    result = await scorer(state, Target(["Paris, Texas"]))
    assert result.text == "0.67"
@pytest.mark.anyio
async def test_stop_words():
    scorer = f1(stop_words=["Paris"])
    state = simple_task_state(model_output="Paris")
    result = await scorer(state, Target(["Paris, Texas"]))
    assert result.text == "0.0"
@pytest.mark.anyio
async def test_stop_words2():
    scorer = f1(stop_words=["Texas"])
    state = simple_task_state(model_output="Paris")
    result = await scorer(state, Target(["Paris, Texas"]))
    assert result.text == "1.0"

================
File: tests/scorer/test_match.py
================
import pytest
from test_helpers.utils import simple_task_state
from inspect_ai.scorer import CORRECT, Target, match
@pytest.mark.anyio
async def test_number_eol():
    scorer = match(numeric=True)
    state = simple_task_state(model_output="28 + 32 = 60\nThis solves the problem.")
    result = await scorer(state, Target(["60"]))
    assert result.text == CORRECT
@pytest.mark.anyio
async def test_non_numeric_match():
    scorer = match(numeric=True, location="any")
    state = simple_task_state(model_output="28 + 32 = 60%\nThis solves the problem.")
    result = await scorer(state, Target(["60", "60%"]))
    assert result.text == CORRECT
@pytest.mark.anyio
async def test_non_numeric_match_end():
    scorer = match(numeric=True)
    state = simple_task_state(model_output="28 + 32 = 60%")
    result = await scorer(state, Target(["60%"]))
    assert result.text == CORRECT
@pytest.mark.anyio
async def test_non_numeric_match_plain():
    scorer = match(numeric=True)
    state = simple_task_state(model_output="28 + 32 = 60%")
    result = await scorer(state, Target(["32"]))
    assert result.text == CORRECT

================
File: tests/scorer/test_metric.py
================
from typing import Any, Callable, cast
from inspect_ai import Task, eval, score
from inspect_ai._util.constants import PKG_NAME
from inspect_ai._util.registry import registry_info
from inspect_ai.dataset import Sample
from inspect_ai.scorer import (
    Metric,
    Score,
    Scorer,
    Value,
    accuracy,
    includes,
    match,
    mean,
    metric,
    scorer,
    std,
    var,
)
from inspect_ai.scorer._metric import (
    MetricDeprecated,
    MetricProtocol,
    SampleScore,
    metric_create,
)
from inspect_ai.scorer._metrics.std import stderr
from inspect_ai.scorer._target import Target
from inspect_ai.solver._task_state import TaskState
# declare some metrics using the various forms supported (function,
# function returning Metric, class deriving from Metric) as well
# as using implicit and explicit names
@metric
def accuracy1(correct: str = "C") -> Metric:
    def metric(scores: list[SampleScore]) -> int | float:
        return 1
    return metric
@metric(name="accuracy2")
def acc_fn(correct: str = "C") -> Metric:
    def metric(scores: list[SampleScore]) -> int | float:
        return 1
    return metric
@metric
class Accuracy3(MetricDeprecated):
    def __init__(self, correct: str = "C") -> None:
        self.correct = correct
    def __call__(self, scores: list[Score]) -> int | float:
        return 1
@metric(name="accuracy4")
class AccuracyNamedCls(MetricProtocol):
    def __init__(self, correct: str = "C") -> None:
        self.correct = correct
    def __call__(self, scores: list[SampleScore]) -> int | float:
        return 1
@metric
def list_metric() -> Metric:
    def metric(scores: list[SampleScore]) -> Value:
        return [1, 2, 3]
    return metric
@metric
def deprecated_metric() -> Metric:
    def metric(scores: list[Score]) -> Value:
        return len(scores)
    return metric
@metric
def dict_metric() -> Metric:
    def metric(scores: list[SampleScore]) -> Value:
        return {"one": 1, "two": 2, "three": 3}
    return metric
def test_metric_registry() -> None:
    registry_assert(accuracy1, "accuracy1")
    registry_assert(acc_fn, "accuracy2")
    registry_assert(Accuracy3, "Accuracy3")
    registry_assert(AccuracyNamedCls, "accuracy4")
def test_metric_call() -> None:
    registry_assert(accuracy1(), "accuracy1")
    registry_assert(acc_fn(), "accuracy2")
    registry_assert(Accuracy3(), "Accuracy3")
    registry_assert(AccuracyNamedCls(), "accuracy4")
def test_metric_create() -> None:
    metric_create_assert("accuracy1", correct="C")
    metric_create_assert("accuracy1", correct="C")
    metric_create_assert("Accuracy3", correct="C")
    metric_create_assert("accuracy4", correct="C")
def test_inspect_metrics() -> None:
    registry_assert(accuracy, f"{PKG_NAME}/accuracy")
    registry_assert(accuracy(), f"{PKG_NAME}/accuracy")
def test_deprecated_metric() -> None:
    def check_log(log):
        assert log.results and (
            list(log.results.scores[0].metrics.keys()) == ["deprecated_metric"]
        )
    task = Task(
        dataset=[Sample(input="What is 1 + 1?", target=["2", "2.0", "Two"])],
        scorer=match(),
        metrics=[deprecated_metric()],
    )
    log = eval(tasks=task, model="mockllm/model")[0]
    check_log(log)
def test_list_metric() -> None:
    def check_log(log):
        assert log.results and (
            list(log.results.scores[0].metrics.keys())
            == ["list_metric-1", "list_metric-2", "list_metric-3"]
        )
    task = Task(
        dataset=[Sample(input="What is 1 + 1?", target=["2", "2.0", "Two"])],
        scorer=match(),
        metrics=[list_metric()],
    )
    # normal eval
    log = eval(tasks=task, model="mockllm/model")[0]
    check_log(log)
def test_dict_metric() -> None:
    def check_log(log):
        assert log.results and (
            list(log.results.scores[0].metrics.keys()) == ["one", "two", "three"]
        )
    task = Task(
        dataset=[Sample(input="What is 1 + 1?", target=["2", "2.0", "Two"])],
        scorer=match(),
        metrics=[dict_metric()],
    )
    # normal eval
    log = eval(tasks=task, model="mockllm/model")[0]
    check_log(log)
def test_alternative_metrics() -> None:
    # check that we get the alternative metrics
    def check_log(log):
        assert log.results and (
            list(log.results.scores[0].metrics.keys())
            == [
                "accuracy",
                "accuracy1",
                "Accuracy3",
                "std",
            ]
        )
    task = Task(
        dataset=[Sample(input="What is 1 + 1?", target=["2", "2.0", "Two"])],
        scorer=match(),
        metrics=[accuracy(), accuracy1(), Accuracy3(), std()],
    )
    # normal eval
    log = eval(tasks=task, model="mockllm/model")[0]
    check_log(log)
    # eval log w/ different scorer (that still uses accuracy)
    log = score(log, scorers=[includes()])
    check_log(log)
@metric
def complex_metric() -> Metric:
    def metric(scores: list[SampleScore]) -> int | float:
        total = 0.0
        for complex_score in scores:
            if isinstance(complex_score.score.value, dict):
                total = (
                    total
                    + cast(int, complex_score.score.value["one"])
                    + cast(int, complex_score.score.value["two"])
                    + cast(int, complex_score.score.value["three"])
                )
        return total
    return metric
@scorer(
    metrics=[
        {"one": [mean()], "two": [mean(), std()], "three": [mean(), std()]},
        complex_metric(),
    ]
)
def complex_scorer() -> Scorer:
    async def score(state: TaskState, target: Target) -> Score:
        return Score(value={"one": 1, "two": 2, "three": 3})
    return score
def test_complex_metrics() -> None:
    def check_log(log):
        assert len(log.results.scores) == 4
        assert log.results.scores[0].name == "complex_scorer"
        assert log.results.scores[0].metrics["complex_metric"].value == 6
    task = Task(
        dataset=[Sample(input="What is 1 + 1?", target=["2", "2.0", "Two"])],
        scorer=complex_scorer(),
    )
    # normal eval
    log = eval(tasks=task, model="mockllm/model")[0]
    check_log(log)
@scorer(
    metrics=[
        {"*": [mean()]},
    ]
)
def wildcard_scorer() -> Scorer:
    async def score(state: TaskState, target: Target) -> Score:
        return Score(value={"one": 1, "two": 2, "three": 3})
    return score
def test_wildcard() -> None:
    def check_log(log):
        assert len(log.results.scores) == 4
        assert log.results.scores[1].name == "one"
        assert log.results.scores[1].metrics["mean"].value == 1
    task = Task(
        dataset=[Sample(input="What is 1 + 1?", target=["2", "2.0", "Two"])],
        scorer=wildcard_scorer(),
    )
    # normal eval
    log = eval(tasks=task, model="mockllm/model")[0]
    check_log(log)
def registry_assert(metric: Metric | Callable[..., Metric], name: str) -> None:
    info = registry_info(metric)
    assert info.name == name
def metric_create_assert(name: str, **kwargs: Any) -> None:
    metric = metric_create(name, **kwargs)
    assert metric([]) == 1
@metric
def nested_dict_metric(correct: str = "C") -> Metric:
    def metric(scores: list[SampleScore]) -> Value:
        return {"key1": 1.0, "key2": 2.0}
    return metric
@scorer(
    metrics=[
        {"*": [nested_dict_metric()]},
    ]
)
def nested_dict_scorer() -> Scorer:
    async def score(state: TaskState, target: Target) -> Score:
        return Score(value={"one": 1, "two": 2, "three": 3})
    return score
def test_nested_dict_metrics() -> None:
    def check_log(log):
        assert len(log.results.scores) == 4
        assert log.results.scores[1].name == "one"
        assert len(log.results.scores[1].metrics.values()) == 2
        assert (
            log.results.scores[1].metrics["nested_dict_metric_key1"].name
            == "nested_dict_metric_key1"
        )
    task = Task(
        dataset=[Sample(input="What is 1 + 1?", target=["2", "2.0", "Two"])],
        scorer=nested_dict_scorer(),
    )
    # normal eval
    log = eval(tasks=task, model="mockllm/model")[0]
    check_log(log)
@metric
def nested_list_metric(correct: str = "C") -> Metric:
    def metric(scores: list[SampleScore]) -> Value:
        return [1.0, 2.0]
    return metric
@scorer(
    metrics=[
        {"*": [nested_list_metric()]},
    ]
)
def nested_list_scorer() -> Scorer:
    async def score(state: TaskState, target: Target) -> Score:
        return Score(value={"one": 1, "two": 2, "three": 3})
    return score
def test_nested_list_metrics() -> None:
    def check_log(log):
        assert len(log.results.scores) == 4
        assert log.results.scores[1].name == "one"
        assert len(log.results.scores[1].metrics.values()) == 2
        assert (
            log.results.scores[1].metrics["nested_list_metric_0"].name
            == "nested_list_metric_0"
        )
    task = Task(
        dataset=[Sample(input="What is 1 + 1?", target=["2", "2.0", "Two"])],
        scorer=nested_list_scorer(),
    )
    # normal eval
    log = eval(tasks=task, model="mockllm/model")[0]
    check_log(log)
def test_variance():
    metric = var()
    result = metric(scores=[SampleScore(score=Score(value=i)) for i in range(10)])
    assert round(result, 3) == 9.167
    assert metric([SampleScore(score=Score(value=4))]) == 0.0
def test_stderr():
    metric = stderr()
    se = metric([SampleScore(score=Score(value=i)) for i in range(10)])
    assert round(se, 3) == 0.957
def test_clustered_stderr():
    metric = stderr(cluster="my_cluster")
    se = metric(
        [
            SampleScore(score=Score(value=i), sample_metadata={"my_cluster": i % 4})
            for i in range(20)
        ]
    )
    assert round(se, 3) == 0.645

================
File: tests/scorer/test_model_graded.py
================
from typing import Callable
from inspect_ai import Task, eval
from inspect_ai.dataset import Sample
from inspect_ai.model import ChatMessageAssistant, ChatMessageUser
from inspect_ai.scorer import model_graded_fact
from inspect_ai.solver._task_state import TaskState
def include_history_task(include_history: bool | Callable[[TaskState], str]) -> Task:
    return Task(
        dataset=[
            Sample(
                input=[
                    ChatMessageUser(content="Who wrote 'The 39 Steps'?"),
                    ChatMessageAssistant(
                        content="Do you mean the movie or the adaption for the stage?"
                    ),
                    ChatMessageUser(content="The movie."),
                ],
                target="Alfred Hitchcock",
            )
        ],
        scorer=model_graded_fact(
            include_history=include_history, model="mockllm/model"
        ),
    )
def test_model_graded_include_history():
    def check_include_history(include_history: bool | Callable[[TaskState], str]):
        log = eval(include_history_task(include_history), model="mockllm/model")[0]
        assert log.samples
        assert "Do you mean the movie" in log.samples[0].model_dump_json()
    check_include_history(True)
    check_include_history(
        lambda state: "\n".join([message.text for message in state.messages])
    )

================
File: tests/scorer/test_multiscorer.py
================
import random
from inspect_ai import Task, eval
from inspect_ai.dataset import Sample
from inspect_ai.scorer import Score, Target, mean, scorer, stderr
from inspect_ai.solver import TaskState
@scorer(metrics=[mean(), stderr()])
def rand_score():
    async def score(state: TaskState, target: Target):
        answer = state.output.completion
        return Score(value=random.randint(1, 100), answer=answer)
    return score
@scorer(metrics=[mean(), stderr()])
def another_rand_score():
    async def score(state: TaskState, target: Target):
        answer = state.output.completion
        return Score(value=random.randint(1, 100), answer=answer)
    return score
@scorer(metrics={"a_count": [mean(), stderr()], "e_count": [mean(), stderr()]})
def letter_count():
    async def score(state: TaskState, target: Target):
        answer = state.output.completion
        a_count = answer.count("a")
        e_count = answer.count("e")
        return Score(value={"a_count": a_count, "e_count": e_count}, answer=answer)
    return score
def check_log(log, scorers, metrics):
    # core checks
    assert log.results
    assert log.results.scores
    assert len(log.results.scores) == len(scorers)
    scorer_names = [scorer.name for scorer in log.results.scores]
    assert all(scorer in scorer_names for scorer in scorers)
    assert all(
        all(metric in scorer.metrics for metric in metrics)
        for scorer in log.results.scores
    )
    # test deprecated fields for now
    assert log.results.scorer is not None
    assert log.results.metrics is not None
# test a single scorer
def test_single_scorer() -> None:
    task = Task(
        dataset=[Sample(input="What is 1 + 1?", target=["2", "2.0", "Two"])],
        scorer=rand_score(),
    )
    # normal eval
    log = eval(tasks=task, model="mockllm/model")[0]
    check_log(log, ["rand_score"], ["mean", "stderr"])
# test two scorers
def test_multi_scorer() -> None:
    task = Task(
        dataset=[Sample(input="What is 1 + 1?", target=["2", "2.0", "Two"])],
        scorer=[rand_score(), another_rand_score()],
    )
    # normal eval
    log = eval(tasks=task, model="mockllm/model")[0]
    check_log(log, ["rand_score", "another_rand_score"], ["mean", "stderr"])
# test dictionary scorer
def test_dict_scorer() -> None:
    task = Task(
        dataset=[Sample(input="What is 1 + 1?", target=["2", "2.0", "Two"])],
        scorer=letter_count(),
    )
    # normal eval
    log = eval(tasks=task, model="mockllm/model")[0]
    check_log(log, ["a_count", "e_count"], ["mean", "stderr"])
# test blend of dictionary and simple scorers
def test_blend_scorer() -> None:
    task = Task(
        dataset=[Sample(input="What is 1 + 1?", target=["2", "2.0", "Two"])],
        scorer=[letter_count(), rand_score()],
    )
    # normal eval
    log = eval(tasks=task, model="mockllm/model")[0]
    check_log(log, ["a_count", "e_count", "rand_score"], ["mean", "stderr"])

================
File: tests/scorer/test_pattern.py
================
import pytest
from test_helpers.utils import simple_task_state
from inspect_ai.scorer import CORRECT, INCORRECT, Target, pattern
@pytest.mark.anyio
async def test_single_match_success():
    scorer = pattern("(foo)")
    state = simple_task_state(model_output="foo")
    result = await scorer(state, Target(["foo"]))
    assert result.text == CORRECT
@pytest.mark.anyio
async def test_single_match_failure_with_target():
    scorer = pattern("(foo)")
    state = simple_task_state(model_output="foo")
    result = await scorer(state, Target(["target doesn't match"]))
    assert result.text == INCORRECT
@pytest.mark.anyio
async def test_single_match_failure_from_model():
    scorer = pattern("(foo)")
    state = simple_task_state(model_output="model doesn't match")
    result = await scorer(state, Target(["foo"]))
    assert result.text == INCORRECT
@pytest.mark.anyio
async def test_single_match_case_sensitive():
    scorer = pattern(pattern="(FOO)", ignore_case=True)
    state = simple_task_state(model_output="foo")
    result = await scorer(state, Target(["foo"]))
    assert result.text == CORRECT
@pytest.mark.anyio
async def test_multi_match_success_on_first_match():
    scorer = pattern("(foo) (bar)")
    state = simple_task_state(model_output="foo bar")
    result = await scorer(state, Target(["foo"]))
    assert result.text == CORRECT
    assert result.answer == "foo"
@pytest.mark.anyio
async def test_multi_match_success_on_subsequent_match():
    scorer = pattern("(foo) (bar)")
    state = simple_task_state(model_output="foo bar")
    result = await scorer(state, Target(["bar"]))
    assert result.text == CORRECT
    assert result.answer == "bar"
@pytest.mark.anyio
async def test_multi_match_success_all_match():
    scorer = pattern("(foo) (foo)", match_all=True)
    state = simple_task_state(model_output="foo foo")
    result = await scorer(state, Target(["foo"]))
    assert result.text == CORRECT
    assert result.answer == "foo"
@pytest.mark.anyio
async def test_multi_match_failure_when_matching_all():
    scorer = pattern("(foo|bar) (foo|bar)", match_all=True)
    state = simple_task_state(model_output="foo bar")
    result = await scorer(state, Target(["bar"]))
    assert result.text == INCORRECT
    assert result.answer is None
@pytest.mark.anyio
async def test_multi_match_failure_with_target():
    scorer = pattern("(foo) (bar)")
    state = simple_task_state(model_output="foo bar")
    result = await scorer(state, Target(["target doesn't match"]))
    assert result.text == INCORRECT
@pytest.mark.anyio
async def test_multi_match_failure_from_model():
    scorer = pattern("(foo) (bar)")
    state = simple_task_state(model_output="model doesn't match")
    result = await scorer(state, Target(["bar"]))
    assert result.text == INCORRECT
@pytest.mark.anyio
async def test_only_returns_exact_target_matches():
    scorer = pattern("(f[oz]o) (b[az]r)")
    state = simple_task_state(model_output="foo bzr")
    result = await scorer(state, Target(["bar"]))
    assert result.text == INCORRECT
@pytest.mark.anyio
async def test_one_match_group_returns_incorrect_match():
    scorer = pattern(
        "ANSWER: (A|B)",
        ignore_case=False,
        match_all=False,
    )
    state = simple_task_state(model_output="ANSWER: A")
    result = await scorer(state, Target(["B"]))
    assert result.answer == "A"
    assert result.text == INCORRECT
@pytest.mark.anyio
async def test_multiple_match_group_returns_none():
    scorer = pattern(
        "ANSWER: (A|B) ALTERNATE_ANSWER: (A|B)",
        ignore_case=False,
        match_all=False,
    )
    state = simple_task_state(model_output="ANSWER: A ALTERNATE_ANSWER: A")
    result = await scorer(state, Target(["B"]))
    assert result.answer is None
    assert result.text == INCORRECT

================
File: tests/scorer/test_reducers.py
================
from collections import namedtuple
from dataclasses import dataclass
from datetime import datetime
from enum import Enum
from functools import reduce
from inspect_ai import Epochs, Task, eval
from inspect_ai._eval.score import score
from inspect_ai.dataset import Sample
from inspect_ai.scorer import (
    Score,
    ScoreReducer,
    ValueToFloat,
    at_least,
    match,
    max_score,
    mean_score,
    median_score,
    mode_score,
    score_reducer,
    value_to_float,
)
from inspect_ai.scorer._reducer import create_reducers
avg_reducer = mean_score()
median_reducer = median_score()
mode_reducer = mode_score()
max_reducer = max_score()
at_least_3_reducer = at_least(3)
at_least_4_reducer = at_least(4)
at_least_5_reducer = at_least(5, 3)
def test_simple_reducers() -> None:
    simple_scores = [
        Score(value=6),
        Score(value=0),
        Score(value=0),
        Score(value=0),
        Score(value=8),
        Score(value=4),
    ]
    assert avg_reducer(simple_scores).value == 3
    assert median_reducer(simple_scores).value == 2
    assert mode_reducer(simple_scores).value == 0
    assert max_reducer(simple_scores).value == 8
    assert at_least_3_reducer(simple_scores).value == 1
    assert at_least_4_reducer(simple_scores).value == 0
def test_list_reducers() -> None:
    list_scores = [
        Score(value=[1, 2]),
        Score(value=[4, 3]),
        Score(value=[3, 1]),
        Score(value=[1, 2]),
        Score(value=[1, 2]),
    ]
    assert avg_reducer(list_scores).value == [2, 2]
    assert median_reducer(list_scores).value == [1, 2]
    assert mode_reducer(list_scores).value == [1, 2]
    assert max_reducer(list_scores).value == [4, 3]
    assert at_least_3_reducer(list_scores).value == [1, 1]
    assert at_least_4_reducer(list_scores).value == [1, 1]
def test_dict_reducers() -> None:
    dict_scores = [
        Score(value={"coolness": 5, "spiciness": 1}),
        Score(value={"coolness": 4, "spiciness": 1}),
        Score(value={"coolness": 3, "spiciness": 1}),
        Score(value={"coolness": 2, "spiciness": 1}),
        Score(value={"coolness": 1, "spiciness": 21}),
    ]
    assert avg_reducer(dict_scores).value == {"coolness": 3, "spiciness": 5}
    assert median_reducer(dict_scores).value == {"coolness": 3, "spiciness": 1}
    assert mode_reducer(dict_scores).value == {"coolness": 5, "spiciness": 1}
    assert max_reducer(dict_scores).value == {"coolness": 5, "spiciness": 21}
    assert at_least_3_reducer(dict_scores).value == {"coolness": 1, "spiciness": 1}
    assert at_least_4_reducer(dict_scores).value == {"coolness": 1, "spiciness": 1}
    assert at_least_5_reducer(dict_scores).value == {"coolness": 0, "spiciness": 0}
def test_reducer_preserve_metadata() -> None:
    simple_scores = [
        # first five scores are identical
        Score(
            value=1, answer="1", explanation="An explanation", metadata={"foo": "bar"}
        ),
        Score(
            value=1, answer="1", explanation="An explanation", metadata={"foo": "bar"}
        ),
        Score(
            value=1, answer="1", explanation="An explanation", metadata={"foo": "bar"}
        ),
        Score(
            value=1, answer="1", explanation="An explanation", metadata={"foo": "bar"}
        ),
        Score(
            value=1, answer="1", explanation="An explanation", metadata={"foo": "bar"}
        ),
        # last score is different
        Score(
            value=2,
            answer="2",
            explanation="Different explanation",
            metadata={"foo": "BAZ"},
        ),
    ]
    reducers = [
        avg_reducer,
        median_reducer,
        mode_reducer,
        max_reducer,
        at_least_3_reducer,
    ]
    # verify that other fields are set only if equal across all samples
    for reducer in reducers:
        # reduce all scores _including_ the last one that's different
        reduced = reducer(simple_scores)
        assert reduced.answer is None
        assert reduced.explanation is None
        assert reduced.metadata == simple_scores[0].metadata
        # reduce all scores _except_ the last one
        reduced = reducer(simple_scores[:-1])
        assert reduced.answer == simple_scores[0].answer
        assert reduced.explanation == simple_scores[0].explanation
        assert reduced.metadata == simple_scores[0].metadata
    # verify that other fields are preserved for a single epoch
    for reducer in reducers:
        reduced = reducer([simple_scores[0]])
        assert reduced.answer == simple_scores[0].answer
        assert reduced.explanation == simple_scores[0].explanation
        assert reduced.metadata == simple_scores[0].metadata
@dataclass
class Foo:
    foo: bool
class DifficultyLevel(Enum):
    EASY = 1
    MEDIUM = 2
    HARD = 3
Point = namedtuple("Point", ["x", "y"])  # noqa: F821
def test_complex_metadata_reduce():
    list_scores = [
        Score(
            value=1,
            answer="A",
            explanation="It is A",
            metadata={
                "foo": Foo(foo=True),
                "count": 5,
                "probability": 0.75,
                "tags": ["math", "algebra"],
                "user": {"id": 123, "name": "John"},
                "timestamp": datetime.now(),
                "difficulty": DifficultyLevel.MEDIUM,
                "optional_data": None,
                "stats": {"attempts": 3, "success_rate": 0.67},
                "frozen_set": frozenset([1, 2, 3]),
                "point": Point(x=1, y=2),
            },
        ),
        Score(
            value=1,
            answer="A",
            explanation="It is A",
            metadata={
                "foo": Foo(foo=True),
                "count": 5,
                "probability": 0.75,
                "tags": ["math", "algebra"],
                "user": {"id": 123, "name": "John"},
                "timestamp": datetime.now(),
                "difficulty": DifficultyLevel.MEDIUM,
                "optional_data": None,
                "stats": {"attempts": 3, "success_rate": 0.67},
                "frozen_set": frozenset([1, 2, 3]),
                "point": Point(x=1, y=2),
            },
        ),
        Score(
            value=1,
            answer="A",
            explanation="It is A",
            metadata={
                "foo": Foo(foo=True),
                "count": 5,
                "probability": 0.75,
                "tags": ["math", "algebra"],
                "user": {"id": 123, "name": "John"},
                "timestamp": datetime.now(),
                "difficulty": DifficultyLevel.MEDIUM,
                "optional_data": None,
                "stats": {"attempts": 3, "success_rate": 0.67},
                "frozen_set": frozenset([1, 2, 3]),
                "point": Point(x=1, y=2),
            },
        ),
    ]
    reduced = avg_reducer(list_scores)
    assert reduced.value == 1
    assert reduced.answer == "A"
    assert reduced.explanation == "It is A"
    assert reduced.metadata == list_scores[0].metadata
@score_reducer(name="add_em_up")
def sum(value_to_float: ValueToFloat = value_to_float()) -> ScoreReducer:
    def sum(scores: list[Score]) -> Score:
        value = reduce(lambda x, y: x + value_to_float(y.value), scores, 0.0)
        return Score(value=value)
    return sum
def test_scorer_lookup():
    assert create_reducers("add_em_up")
def eval_with_reducer():
    task = Task(dataset=[Sample(input="Say hello.", target="Hello")], scorer=match())
    return eval(task, model="mockllm/model", epochs=Epochs(5, max_score()))[0]
def test_reducer_by_name():
    task = Task(dataset=[Sample(input="Say hello.", target="Hello")], scorer=match())
    log = eval(task, model="mockllm/model", epochs=Epochs(5, "at_least_2"))[0]
    assert log.eval.config.epochs_reducer == ["at_least_2"]
def test_eval_reducer():
    log = eval_with_reducer()
    assert log.eval.config.epochs_reducer == ["max"]
def test_score_reducer():
    log = score(eval_with_reducer(), match())
    assert log.eval.config.epochs_reducer == ["max"]
    log = score(eval_with_reducer(), match(), [mode_score(), mean_score()])
    assert log.eval.config.epochs_reducer == ["mode", "mean"]
def test_main_reducer():
    str_scores = [
        Score(value="I"),
        Score(value="I"),
        Score(value="I"),
        Score(value="C"),
        Score(value="C"),
    ]
    assert mean_score()(str_scores).value == 0.4

================
File: tests/scorer/test_scorer.py
================
# type: ignore
import pytest
from test_helpers.utils import ensure_test_package_installed, run_example
from inspect_ai import Task, eval, score
from inspect_ai.dataset import Sample
from inspect_ai.scorer import Score, Scorer, Target, accuracy, includes, scorer
from inspect_ai.scorer._scorer import scorer_create
from inspect_ai.solver import TaskState
@scorer(metrics=[accuracy()], name="test_match")
def match() -> Scorer:
    async def score(state: TaskState, target: Target) -> Score:
        return (
            Score(value="C")
            if state.output.completion == target.text
            else Score(value="I")
        )
    return score
def test_scorer_lookup():
    scorer = scorer_create("test_match")
    assert scorer
def test_invalid_scorers_error():
    def not_async():
        def inner(state: TaskState, target: Target) -> Score:
            return Score(value="C")
        return inner
    class NotCallable:
        async def inner(self, state: TaskState, target: Target) -> Score:
            return Score(value="C")
    class NotAsyncCallable:
        def __call__(self, state: TaskState, target: Target) -> Score:
            return Score(value="C")
    for f in [not_async, NotCallable, NotAsyncCallable]:
        with pytest.raises(TypeError):
            scorer(metrics=[accuracy()], name=f.__name__)(f)()
def test_valid_scorers_succeed():
    def is_async():
        async def inner(state: TaskState, target: Target) -> Score:
            return Score(value="C")
        return inner
    class IsAsyncCallable:
        async def __call__(self, state: TaskState, target: Target) -> Score:
            return Score(value="C")
    for f in [is_async, IsAsyncCallable]:
        scorer(metrics=[accuracy()], name=f.__name__)(f)()
def test_no_scorer():
    task = Task(
        dataset=[Sample(input="What is 1 + 1?", target=["2", "2.0", "Two"])],
    )
    log = eval(tasks=task, model="mockllm/model")[0]
    assert log.samples[0].score is None
def test_score_function():
    log = run_example("popularity.py", "mockllm/model")
    log = score(log[0], includes())
    assert log.samples[0].score.value
def test_score_package_name():
    ensure_test_package_installed()
    from inspect_package import simple_score
    task = Task(
        dataset=[Sample(input="What is the capital of Kansas?", target=["Topeka"])],
        scorer=simple_score(),
    )
    eval_log = eval(tasks=task, model="mockllm/model")[0]
    assert eval_log.results.scores[0].name == "simple_score"
def test_score_unique():
    ensure_test_package_installed()
    from inspect_package import simple_score
    task = Task(
        dataset=[Sample(input="What is the capital of Kansas?", target=["Topeka"])],
        scorer=[simple_score(), simple_score(), simple_score()],
    )
    eval_log = eval(tasks=task, model="mockllm/model")[0]
    assert eval_log.results.scores[0].name == "simple_score"
    assert eval_log.results.scores[1].name == "simple_score1"
    assert eval_log.results.scores[2].name == "simple_score2"

================
File: tests/scorer/test_task_score.py
================
import os
import pytest
from inspect_ai._eval.score import task_score
from inspect_ai.log._file import read_eval_log_async
LOG_SCORED = os.path.join(
    "tests",
    "scorer",
    "logs",
    "2025-02-11T15-18-04-05-00_popularity_mj7khqpMM4GBCfVQozKgzB.eval",
)
LOG_UNSCORED = os.path.join(
    "tests",
    "scorer",
    "logs",
    "2025-02-11T15-17-00-05-00_popularity_dPiJifoWeEQBrfWsAopzWr.eval",
)
@pytest.mark.anyio
async def test_score_unscored():
    unscored_log = await read_eval_log_async(LOG_UNSCORED)
    scored_log = await task_score(log=unscored_log)
    assert len(scored_log.results.scores) == 1
    assert scored_log.results.scores[0].name == "match"
    metrics = scored_log.results.scores[0].metrics
    assert len(metrics.items()) == 2
@pytest.mark.anyio
async def test_score_unscored_new_scorer():
    unscored_log = await read_eval_log_async(LOG_UNSCORED)
    scored_log = await task_score(
        log=unscored_log, scorer="f1", scorer_args={"stop_words": ["roasted"]}
    )
    assert len(scored_log.results.scores) == 1
    assert scored_log.results.scores[0].name == "f1"
    assert scored_log.results.scores[0].params["stop_words"] == ["roasted"]
    metrics = scored_log.results.scores[0].metrics
    assert len(metrics.items()) == 2
@pytest.mark.anyio
async def test_score_scored_append():
    unscored_log = await read_eval_log_async(LOG_SCORED)
    scored_log = await task_score(
        log=unscored_log, scorer="f1", scorer_args={"stop_words": ["woah"]}
    )
    assert len(scored_log.results.scores) == 2
    metrics2 = scored_log.results.scores[0].metrics
    assert scored_log.results.scores[0].name == "f1"
    assert scored_log.results.scores[0].params["stop_words"] == ["woah"]
    assert len(metrics2.items()) == 2
    metrics = scored_log.results.scores[1].metrics
    assert scored_log.results.scores[1].name == "match"
    assert len(metrics.items()) == 2
@pytest.mark.anyio
async def test_score_scored_overwrite():
    unscored_log = await read_eval_log_async(LOG_SCORED)
    scored_log = await task_score(
        log=unscored_log,
        scorer="f1",
        scorer_args={"stop_words": ["clowns"]},
        action="overwrite",
    )
    assert len(scored_log.results.scores) == 1
    assert scored_log.results.scores[0].name == "f1"
    assert scored_log.results.scores[0].params["stop_words"] == ["clowns"]
    metrics = scored_log.results.scores[0].metrics
    assert len(metrics.items()) == 2

================
File: tests/scorer/test_value_to_float.py
================
from inspect_ai.scorer import CORRECT, PARTIAL, value_to_float
def test_value_to_float_numbers():
    fn = value_to_float()
    assert fn(1) == 1.0
    assert fn(0.5) == 0.5
    assert fn(True) == 1.0
    assert fn(False) == 0
def test_value_to_float_strings():
    fn = value_to_float()
    assert fn("1.0") == 1.0
    assert fn("0.5") == 0.5
    assert fn("0") == 0
    assert fn("yes") == 1.0
    assert fn("No") == 0.0
    assert fn(CORRECT) == 1.0
    assert fn(PARTIAL) == 0.5
def test_value_to_float_custom():
    fn = value_to_float(correct="correct", incorrect="incorrect")
    assert fn("correct") == 1.0
    assert fn("incorrect") == 0
def test_value_to_float_invalid():
    fn = value_to_float()
    assert fn("foo") == 0.0

================
File: tests/solver/test_basic_agent.py
================
from inspect_ai import Task, eval
from inspect_ai.dataset import Sample
from inspect_ai.log import EvalLog
from inspect_ai.model import ChatMessageUser, ModelOutput, get_model
from inspect_ai.scorer import Score, Target, accuracy, includes, scorer
from inspect_ai.solver import Solver, TaskState, basic_agent, solver, system_message
from inspect_ai.tool import Tool, tool
@tool
def addition():
    async def execute(x: int, y: int):
        """
        Add two numbers.
        Args:
            x (int): First number to add.
            y (int): Second number to add.
        Returns:
            The sum of the two numbers.
        """
        return x + y
    return execute
AGENT_SYSTEM_MESSAGE = """
You are a helpful assistant attempting to submit the correct answer. When you have completed the task and have a result, call the agent_submit() function to communicate it.
"""
AGENT_INCORRECT_MESSAGE = "Your submission was incorrect."
AGENT_CONTINUE_MESSAGE = "Please proceed."
AGENT_SUBMIT_TOOL_NAME = "agent_submit"
AGENT_SUBMIT_TOOL_DESCRIPTION = "Submit an answer."
def run_basic_agent(
    tools: list[Tool] | Solver, message_limit: int | None = 30
) -> EvalLog:
    task = Task(
        dataset=[Sample(input="What is 1 + 1?", target=["2", "2.0", "Two"])],
        solver=basic_agent(
            init=system_message(AGENT_SYSTEM_MESSAGE),
            tools=tools,
            submit_name=AGENT_SUBMIT_TOOL_NAME,
            submit_description=AGENT_SUBMIT_TOOL_DESCRIPTION,
        ),
        scorer=includes(),
        message_limit=message_limit,
    )
    model = get_model(
        "mockllm/model",
        custom_outputs=[
            ModelOutput.for_tool_call(
                model="mockllm/model",
                tool_name=AGENT_SUBMIT_TOOL_NAME,
                tool_arguments={"answer": "2"},
            )
        ],
    )
    return eval(task, model=model)[0]
def mockllm_model(answers: list[str]):
    return get_model(
        "mockllm/model",
        custom_outputs=[
            ModelOutput.for_tool_call(
                model="mockllm/model",
                tool_name="submit",
                tool_arguments={"answer": answer},
            )
            for answer in answers
        ],
    )
def test_basic_agent_solver():
    @solver
    def addition_tool() -> Solver:
        async def solve(state, generate):
            state.tools = [addition()]
            return state
        return solve
    log = run_basic_agent(addition_tool())
    assert log.status == "success"
    assert log.results.scores[0].metrics["accuracy"].value == 1
def test_basic_agent_custom_text():
    log = run_basic_agent([addition()])
    assert log.status == "success"
    assert log.samples[0].messages[0].content == AGENT_SYSTEM_MESSAGE
    tool_event = next(
        (event for event in log.samples[0].transcript.events if event.event == "tool")
    )
    assert tool_event
    assert tool_event.function == AGENT_SUBMIT_TOOL_NAME
    model_event = next(
        (event for event in log.samples[0].transcript.events if event.event == "model")
    )
    assert model_event
    assert model_event.tools[1].name == AGENT_SUBMIT_TOOL_NAME
    assert model_event.tools[1].description == AGENT_SUBMIT_TOOL_DESCRIPTION
def test_basic_agent_retries():
    def addition_task(max_attempts):
        return Task(
            dataset=[Sample(input="What is 1 + 1?", target=["2", "2.0", "Two"])],
            solver=basic_agent(
                tools=[addition()], max_attempts=max_attempts, message_limit=30
            ),
            scorer=includes(),
        )
    # incorrect answer with no retries
    log = eval(addition_task(1), mockllm_model(["5"]))[0]
    assert log.results.scores[0].metrics["accuracy"].value == 0
    # correct answer on the third try
    log = eval(addition_task(3), mockllm_model(["5", "4", "2"]))[0]
    assert log.results.scores[0].metrics["accuracy"].value == 1
    model_events = sum(
        1 for event in log.samples[0].transcript.events if event.event == "model"
    )
    assert model_events == 3
def test_basic_agent_retries_with_custom_incorrect_message():
    @scorer(metrics=[accuracy()])
    def compare_quantities():
        async def score(state: TaskState, target: Target) -> Score:
            answer = float(state.output.completion)
            target_value = float(target.text)
            if answer == target_value:
                return Score(value=1.0, answer=state.output.completion)
            elif answer > target_value:
                return Score(
                    value=0.0,
                    answer=state.output.completion,
                    explanation="Answer is too high",
                )
            else:
                return Score(
                    value=0.0,
                    answer=state.output.completion,
                    explanation="Answer is too low",
                )
        return score
    def custom_incorrect_message(state: TaskState, scores: list[Score]):
        return f"Your response to the input '{state.input}' was incorrect: {scores[0].explanation}"
    async def async_custom_incorrect_message(state: TaskState, scores: list[Score]):
        return f"Your response to the input '{state.input}' was incorrect: {scores[0].explanation}"
    def check_task(incorrect_message):
        addition_task = Task(
            dataset=[Sample(input="What is 1 + 1?", target="2")],
            solver=basic_agent(
                tools=[addition()],
                max_attempts=3,
                message_limit=30,
                incorrect_message=incorrect_message,
            ),
            scorer=compare_quantities(),
        )
        log = eval(addition_task, mockllm_model(["5", "1", "2"]), display="plain")[0]
        assert log.results.scores[0].metrics["accuracy"].value == 1
        user_msgs = [
            m.content for m in log.samples[0].messages if isinstance(m, ChatMessageUser)
        ]
        assert user_msgs == [
            "What is 1 + 1?",
            "Your response to the input 'What is 1 + 1?' was incorrect: Answer is too high",
            "Your response to the input 'What is 1 + 1?' was incorrect: Answer is too low",
        ]
    check_task(custom_incorrect_message)
    check_task(async_custom_incorrect_message)
if __name__ == "__main__":
    test_basic_agent_retries_with_custom_incorrect_message()

================
File: tests/solver/test_bridge.py
================
from textwrap import dedent
from typing import Any
from openai import BaseModel
from test_helpers.utils import skip_if_no_anthropic, skip_if_no_openai
from inspect_ai import Task, eval, task
from inspect_ai.dataset import Sample
from inspect_ai.model._openai import openai_chat_tools
from inspect_ai.scorer import includes
from inspect_ai.solver import bridge, solver
from inspect_ai.tool._tool_info import ToolInfo
from inspect_ai.tool._tool_params import ToolParam, ToolParams
from inspect_ai.util._json import json_schema
def agent(tools: bool):
    async def run(sample: dict[str, Any]) -> dict[str, Any]:
        from openai import AsyncOpenAI
        class Message(BaseModel):
            text: str
        client = AsyncOpenAI()
        params = dict(
            model="inspect",
            messages=sample["input"],
            temperature=0.8,
            top_p=0.5,
            stop=["foo"],
            frequency_penalty=1,
            presence_penalty=1.5,
            seed=42,
            n=3,
            # logit bias types are out of sync w/ api docs
            logit_bias=dict([(42, 10), (43, -10)]),
            reasoning_effort="low",
            timeout=200,
            response_format=dict(
                type="json_schema",
                json_schema=dict(
                    name="message",
                    schema=json_schema(Message).model_dump(exclude_none=True),
                ),
            ),
        )
        if tools:
            params["tools"] = openai_chat_tools([testing_tool_info()])
            params["tool_choice"] = "auto"
        else:
            params["logprobs"] = True
            params["top_logprobs"] = 3
        completion = await client.chat.completions.create(**params)
        return {"output": completion.choices[0].message.content}
    return run
@task
def bridged_task(tools: bool):
    return Task(
        dataset=[Sample(input="Please print the word 'hello'?", target="hello")],
        solver=bridge(agent(tools)),
        scorer=includes(),
    )
@task
def openai_api_task():
    # solver the calls the openai api directly (so should proceed unpatched)
    @solver
    def openai_api_solver():
        from openai import AsyncOpenAI
        async def solve(state, generate):
            client = AsyncOpenAI()
            await client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "You are a helpful assistant."},
                    {
                        "role": "user",
                        "content": "Write a haiku about recursion in programming.",
                    },
                ],
            )
            return state
        return solve
    return Task(solver=openai_api_solver())
def eval_bridged_task(model: str, tools: bool) -> str:
    log = eval(bridged_task(tools), model=model)[0]
    assert log.status == "success"
    return log.model_dump_json(exclude_none=True, indent=2)
def check_openai_log_json(log_json: str, tools: bool):
    assert r'"model": "gpt-4o"' in log_json
    if tools:
        assert r'"name": "testing_tool"' in log_json
        assert r'"tool_choice": "auto"' in log_json
    assert r'"frequency_penalty": 1' in log_json
    assert dedent("""
    "stop": [
      "foo"
    ]
    """)
    assert dedent("""
    "logit_bias": {
      "42": 10,
      "43": -10
    },
    """)
    assert r'"presence_penalty": 1.5' in log_json
    assert r'"seed": 42' in log_json
    assert r'"temperature": 0.8' in log_json
    assert r'"top_p": 0.5' in log_json
    assert r'"n": 3' in log_json
    if not tools:
        assert r'"logprobs": true' in log_json
        assert r'"top_logprobs": 3' in log_json
    assert r'"response_schema"' in log_json
    assert r'"logprobs"' in log_json
@skip_if_no_openai
def test_bridged_agent():
    log_json = eval_bridged_task("openai/gpt-4o", tools=False)
    check_openai_log_json(log_json, tools=False)
@skip_if_no_openai
def test_bridged_agent_tools():
    log_json = eval_bridged_task("openai/gpt-4o", tools=True)
    check_openai_log_json(log_json, tools=True)
def check_anthropic_log_json(log_json: str):
    assert r'"model": "anthropic/claude-3-haiku-20240307"' in log_json
    assert r'"temperature": 0.8' in log_json
    assert r'"top_p": 0.5' in log_json
    assert dedent("""
    "stop_sequences": [
      "foo"
    ]
    """)
@skip_if_no_anthropic
@skip_if_no_openai
def test_anthropic_bridged_agent():
    log_json = eval_bridged_task("anthropic/claude-3-haiku-20240307", tools=False)
    check_anthropic_log_json(log_json)
@skip_if_no_openai
def test_bridged_agent_context():
    logs = eval(
        [bridged_task(tools=False), openai_api_task()],
        max_tasks=2,
        model="anthropic/claude-3-haiku-20240307",
    )
    for log in logs:
        assert log.status == "success"
        if log.eval.task == "bridged_task":
            log_json = log.model_dump_json(exclude_none=True, indent=2)
            check_anthropic_log_json(log_json)
def testing_tool_info() -> ToolInfo:
    return ToolInfo(
        name="testing_tool",
        description="This is a testing tool.",
        parameters=ToolParams(
            properties={
                "param1": ToolParam(
                    type="string",
                    description="This is parameter1",
                )
            },
            required=["param1"],
        ),
    )
if __name__ == "__main__":
    test_bridged_agent()

================
File: tests/solver/test_chain.py
================
from copy import deepcopy
from inspect_ai.solver import (
    Generate,
    TaskState,
    chain,
    solver,
)
@solver
def identity():
    async def solve(state: TaskState, _generate: Generate):
        return state
    return solve
def test_solver_chain():
    solver1 = identity()
    chain1 = chain(identity(), identity(), identity())
    assert len(chain(solver1, chain1)) == 4
    chain2 = chain(solver1, chain1, chain(identity(), identity()))
    assert len(chain2) == 6
    assert len(chain(chain2, deepcopy(chain2))) == 12

================
File: tests/solver/test_fork.py
================
from inspect_ai import Task, eval, task
from inspect_ai.dataset import Sample
from inspect_ai.solver import (
    Generate,
    TaskState,
    fork,
    solver,
)
from inspect_ai.solver._chain import chain
from inspect_ai.util import store
def test_forking_solver():
    eval(forking_solver_task(), model="mockllm/model")
@task
def forking_solver_task():
    COOKIE = "cookie"
    MONSTER = "monster"
    @solver
    def forked_solver(cookie: str):
        async def solve(state: TaskState, generate: Generate):
            store().set(COOKIE, cookie)
            state.store.set(MONSTER, cookie)
            state.metadata[COOKIE] = cookie
            return await generate(state)
        return solve
    def check_state(state: TaskState, cookie: str):
        assert state.store.get(COOKIE) == cookie
        assert state.store.get(MONSTER) == cookie
        assert state.metadata[COOKIE] == cookie
    @solver
    def forking_solver():
        async def solve(state: TaskState, _generate: Generate):
            results = await fork(state, [forked_solver("foo"), forked_solver("bar")])
            check_state(results[0], "foo")
            check_state(results[1], "bar")
            state = await fork(state, forked_solver("foo"))
            check_state(state, "foo")
            state = await fork(
                state,
                chain(forked_solver("a"), forked_solver("b"), forked_solver("c")),
            )
            check_state(state, "c")
            return state
        return solve
    return Task(
        dataset=[Sample(input="Say Hello", target="Hello")], solver=forking_solver()
    )

================
File: tests/solver/test_multiple_choice.py
================
from random import Random
import pytest
from test_helpers.utils import simple_task_state
from inspect_ai.model import ChatMessageAssistant, ChatMessageUser, ModelOutput
from inspect_ai.scorer._choice import choice
from inspect_ai.scorer._metric import CORRECT
from inspect_ai.scorer._target import Target
from inspect_ai.solver import MultipleChoiceTemplate, TaskState, multiple_choice
from inspect_ai.solver._task_state import Choice
async def generate(state: TaskState) -> TaskState:
    state.messages.append(ChatMessageAssistant(content="ANSWER: A"))
    state.output = ModelOutput.from_content(model="model", content="ANSWER: A")
    return state
def generate_for_multiple_correct(answers: str):
    async def generate(state: TaskState) -> TaskState:
        state.messages.append(ChatMessageAssistant(content=answers))
        state.output = ModelOutput.from_content(model="model", content=answers)
        return state
    return generate
@pytest.mark.anyio
async def test_raises_exception_if_no_choices():
    solver = multiple_choice()
    state = simple_task_state()
    with pytest.raises(
        ValueError, match="The multiple_choice solver requires samples with choices"
    ):
        await solver(state=state, generate=generate)
@pytest.mark.anyio
async def test_single_multiple_choice():
    solver = multiple_choice()
    state = simple_task_state(
        choices=["only one choice here"],
        messages=[ChatMessageUser(content="What's the answer?", source="input")],
    )
    new_state = await solver(state=state, generate=generate)
    assert new_state.output.completion == "ANSWER: A"
    assert new_state.user_prompt.text == MultipleChoiceTemplate.SINGLE_ANSWER.format(
        letters="A", question="What's the answer?", choices="A) only one choice here"
    )
    assert new_state.choices[0].correct is True
@pytest.mark.anyio
async def test_maps_choices_without_shuffling():
    solver = multiple_choice()
    state = simple_task_state(
        choices=["choice 1", "choice 2", "choice 3"],
        messages=[ChatMessageUser(content="What's the answer?", source="input")],
    )
    new_state = await solver(state=state, generate=generate)
    assert new_state.messages[-1].content == "ANSWER: A"
    assert new_state.user_prompt.text == MultipleChoiceTemplate.SINGLE_ANSWER.format(
        letters="A,B,C",
        question="What's the answer?",
        choices="A) choice 1\nB) choice 2\nC) choice 3",
    )
    assert [choice.correct for choice in new_state.choices] == [
        True,
        False,
        False,
    ]
@pytest.mark.anyio
async def test_custom_template():
    solver = multiple_choice(template="Do this thing: {question} {choices}")
    state = simple_task_state(
        choices=["choice 1", "choice 2", "choice 3"],
        messages=[ChatMessageUser(content="What's the answer?", source="input")],
    )
    new_state = await solver(state=state, generate=generate)
    assert new_state.messages[-1].content == "ANSWER: A"
    assert (
        new_state.user_prompt.text
        == "Do this thing: What's the answer? A) choice 1\nB) choice 2\nC) choice 3"
    )
@pytest.mark.anyio
async def test_custom_template_raises_with_missing_fields():
    with pytest.raises(
        ValueError,
        match="The template must contain '{question}' and '{choices}' placeholders for string substitution.",
    ):
        multiple_choice(template="This template lacks substance")
@pytest.mark.anyio
async def test_can_shuffle_choices_when_calling_the_model():
    async def generate_shuffled(state: TaskState):
        # Ensure that the choices are shuffled before we call the model
        assert "A) choice 3" in state.user_prompt.text
        assert "B) choice 2" in state.user_prompt.text
        assert "C) choice 1" in state.user_prompt.text
        return await generate(state=state)
    solver = multiple_choice(shuffle=Random(4))
    state = simple_task_state(
        choices=["choice 1", "choice 2", "choice 3"],
        messages=[ChatMessageUser(content="What's the answer?", source="input")],
    )
    new_state = await solver(state=state, generate=generate_shuffled)
    assert new_state.output.completion == "ANSWER: C"
    assert new_state.messages[-1].content == "ANSWER: C"
    assert new_state.user_prompt.text == MultipleChoiceTemplate.SINGLE_ANSWER.format(
        letters="A,B,C",
        question="What's the answer?",
        choices="A) choice 1\nB) choice 2\nC) choice 3",
    )
    assert new_state.choices[0] == Choice(
        value="choice 3", correct=True, original_position=2
    )
    assert new_state.choices[1] == Choice(
        value="choice 2", correct=False, original_position=1
    )
    assert new_state.choices[2] == Choice(
        value="choice 1", correct=False, original_position=0
    )
@pytest.mark.anyio
async def test_multiple_correct():
    generate = generate_for_multiple_correct(answers="ANSWER: AB")
    solver = multiple_choice(multiple_correct=True)
    state = simple_task_state(
        choices=["choice 1", "choice 2", "choice 3"],
        messages=[ChatMessageUser(content="What's the answer?", source="input")],
    )
    new_state = await solver(state=state, generate=generate)
    assert new_state.output.completion == "ANSWER: AB"
    assert new_state.user_prompt.text == MultipleChoiceTemplate.MULTIPLE_ANSWER.format(
        letters="A,B,C",
        question="What's the answer?",
        choices="A) choice 1\nB) choice 2\nC) choice 3",
    )
    assert new_state.choices[0] == Choice(
        value="choice 1", correct=True, original_position=0
    )
    assert new_state.choices[1] == Choice(
        value="choice 2", correct=True, original_position=1
    )
    assert new_state.choices[2] == Choice(
        value="choice 3", correct=False, original_position=2
    )
@pytest.mark.anyio
async def test_multiple_correct_model_generated_commas():
    generate = generate_for_multiple_correct(answers="ANSWER: B, C")
    solver = multiple_choice(multiple_correct=True)
    state = simple_task_state(
        choices=["choice 1", "choice 2", "choice 3"],
        messages=[ChatMessageUser(content="What's the answer?", source="input")],
    )
    new_state = await solver(state=state, generate=generate)
    assert new_state.output.completion == "ANSWER: B, C"
    assert new_state.user_prompt.text == MultipleChoiceTemplate.MULTIPLE_ANSWER.format(
        letters="A,B,C",
        question="What's the answer?",
        choices="A) choice 1\nB) choice 2\nC) choice 3",
    )
    assert new_state.choices[0] == Choice(
        value="choice 1", correct=False, original_position=0
    )
    assert new_state.choices[1] == Choice(
        value="choice 2", correct=True, original_position=1
    )
    assert new_state.choices[2] == Choice(
        value="choice 3", correct=True, original_position=2
    )
@pytest.mark.anyio
async def test_multiple_shuffled_answers_one_answer():
    # Given the shuffling before calling generate, the actual answer is actually A
    actual_generate = generate_for_multiple_correct(answers="ANSWER: C")
    async def generate_shuffled(state: TaskState):
        # Ensure that the choices are shuffled before we call the model
        assert "A) choice 3" in state.user_prompt.text
        assert "B) choice 2" in state.user_prompt.text
        assert "C) choice 1" in state.user_prompt.text
        return await actual_generate(state=state)
    solver = multiple_choice(multiple_correct=True, shuffle=Random(4))
    state = simple_task_state(
        choices=["choice 1", "choice 2", "choice 3"],
        messages=[ChatMessageUser(content="What's the answer?", source="input")],
    )
    new_state = await solver(state=state, generate=generate_shuffled)
    assert new_state.output.completion == "ANSWER: A"
    assert new_state.messages[-1].content == "ANSWER: A"
    assert new_state.user_prompt.text == MultipleChoiceTemplate.MULTIPLE_ANSWER.format(
        letters="A,B,C",
        question="What's the answer?",
        choices="A) choice 1\nB) choice 2\nC) choice 3",
    )
    assert new_state.choices[0] == Choice(
        value="choice 3", correct=False, original_position=2
    )
    assert new_state.choices[1] == Choice(
        value="choice 2", correct=False, original_position=1
    )
    assert new_state.choices[2] == Choice(
        value="choice 1", correct=True, original_position=0
    )
@pytest.mark.anyio
async def test_multiple_shuffled_answers_more():
    # Given the shuffling before calling generate, the actual answers are B, C
    actual_generate = generate_for_multiple_correct(answers="ANSWER: A, D")
    async def generate_shuffled(state: TaskState):
        # Ensure that the choices are shuffled before we call the model
        assert "A) choice 3" in state.user_prompt.text
        assert "B) choice 1" in state.user_prompt.text
        assert "C) choice 4" in state.user_prompt.text
        assert "D) choice 2" in state.user_prompt.text
        return await actual_generate(state=state)
    solver = multiple_choice(multiple_correct=True, shuffle=Random(4))
    state = simple_task_state(
        choices=["choice 1", "choice 2", "choice 3", "choice 4"],
        messages=[ChatMessageUser(content="What's the answer?", source="input")],
    )
    new_state = await solver(state=state, generate=generate_shuffled)
    assert new_state.output.completion == "ANSWER: B, C"
    assert new_state.messages[-1].content == "ANSWER: B, C"
    assert new_state.user_prompt.text == MultipleChoiceTemplate.MULTIPLE_ANSWER.format(
        letters="A,B,C,D",
        question="What's the answer?",
        choices="A) choice 1\nB) choice 2\nC) choice 3\nD) choice 4",
    )
    assert new_state.choices[0] == Choice(
        value="choice 3", correct=True, original_position=2
    )
    assert new_state.choices[1] == Choice(
        value="choice 1", correct=False, original_position=0
    )
    assert new_state.choices[2] == Choice(
        value="choice 4", correct=False, original_position=3
    )
    assert new_state.choices[3] == Choice(
        value="choice 2", correct=True, original_position=1
    )
cot_complex = """
Let's approach this step-by-step:
1. First, we need to understand that the question is about what happens after fertilization has occurred. The sperm has already been injected into the egg, so we're dealing with post-fertilization issues.
2. Option B can be eliminated immediately because it talks about sperm binding to the egg, which has already happened in this scenario.
3. Option C is not correct because chromosomal recombination typically occurs during meiosis, which happens when gametes (sperm and egg) are formed, not after fertilization.
4. Now we're left with options A and D. Let's consider each:
   A) Epistatic interactions refer to how genes interact with each other. While this could potentially cause issues, it's not likely to be the main cause of zygote mortality.
   D) Chromosomal incompatibilities leading to failure of meiosis is more likely to be the main cause of zygote mortality.
5. Here's why D is the most probable answer:
   - Even though both species have the same number of chromosomes, the chromosomes themselves may be structurally different.
   - These structural differences can lead to problems during cell division.
   - The first cell division after fertilization is crucial for embryo development.
   - If the chromosomes can't properly align or separate due to incompatibilities, it would lead to cell division failure and subsequent death of the zygote.
6. While epistatic interactions (option A) could potentially cause issues later in development, chromosomal incompatibilities (option D) would cause problems immediately and be more likely to result in early zygote mortality.
Therefore, the most likely main cause of zygote mortality in this scenario would be chromosomal incompatibilities.
ANSWER: D"""
@pytest.mark.anyio
async def test_cot_complex_text():
    # Tests and end to end multiple choice generate and score
    # with a real cot response that can trick our answer parsing
    solver = multiple_choice()
    state = simple_task_state(
        choices=["A", "B", "C", "D"],
        messages=[ChatMessageUser(content="What's the answer?", source="input")],
    )
    async def generate_cot_text(state: TaskState) -> TaskState:
        state.messages.append(ChatMessageAssistant(content=cot_complex))
        state.output = ModelOutput.from_content(model="model", content=cot_complex)
        return state
    new_state = await solver(state=state, generate=generate_cot_text)
    scorer = choice()
    result = await scorer(new_state, Target("D"))
    print(vars(result))
    assert result.value == CORRECT
    assert result.answer == "D"

================
File: tests/solver/test_prompt.py
================
from test_helpers.utils import skip_if_no_openai
from inspect_ai import Task, eval
from inspect_ai.dataset import Sample
from inspect_ai.solver import Solver, generate, prompt_template, solver, system_message
from inspect_ai.solver._prompt import assistant_message, user_message
from inspect_ai.solver._solver import Generate
from inspect_ai.solver._task_state import TaskState
PROMPT_TEMPLATE = """
Please answer this question.
{param}
{variable}
{prompt}
{unknown}
{{escaped}}
"""
PARAM_VALUE = "param_value"
VARIABLE_VALUE = "variable_value"
PROMPT_VALUE = "prompt_value"
def check_template_variables(template_solver: Solver, index: int = 0):
    @solver
    def set_store_var(name: str, value: str) -> Solver:
        async def solve(state: TaskState, generate: Generate):
            state.store.set(name, value)
            return state
        return solve
    task = Task(
        dataset=[
            Sample(
                input="Say hello.",
                target="Hello",
                metadata=dict(prompt=PROMPT_VALUE),
            )
        ],
        solver=[set_store_var("variable", VARIABLE_VALUE), template_solver, generate()],
    )
    log = eval(task, model="mockllm/model")[0]
    assert log.samples
    message = log.samples[0].messages[index].text
    assert VARIABLE_VALUE in message
    assert PARAM_VALUE in message
    assert "{unknown}" in message
    assert "{escaped}" in message
    return message
def test_prompt_template_variables():
    message = check_template_variables(
        prompt_template(PROMPT_TEMPLATE, param=PARAM_VALUE)
    )
    assert PROMPT_VALUE not in message
def test_system_message_variables():
    check_template_variables(system_message(PROMPT_TEMPLATE, param=PARAM_VALUE))
def test_user_message_variables():
    check_template_variables(user_message(PROMPT_TEMPLATE, param=PARAM_VALUE), index=-2)
@skip_if_no_openai
def test_user_and_assistant_message():
    ASSISTANT_MESSAGE = "Did you mean 'hello' or 'hello, world'?"
    task = Task(
        dataset=[Sample(input="Say hello.", target="hello")],
        solver=[
            assistant_message(ASSISTANT_MESSAGE),
            user_message("I meant 'hello'"),
        ],
    )
    log = eval(task, model="openai/gpt-4o-mini")[0]
    assert log.status == "success"
    assert log.samples
    assert log.samples[0].messages[1].text == ASSISTANT_MESSAGE

================
File: tests/solver/test_score.py
================
from inspect_ai import Task, eval, task
from inspect_ai._eval.task.task import task_with
from inspect_ai.dataset import Sample
from inspect_ai.log._log import EvalLog
from inspect_ai.scorer import Score, includes
from inspect_ai.solver import Generate, TaskState, solver
@solver
def solver_with_score(score_name: str, score_value: float):
    async def solve(state: TaskState, _generate: Generate):
        state.scores = {score_name: Score(value=score_value)}
        return state
    return solve
@task
def scoring_task():
    return Task(
        dataset=[Sample(input="What is 1 + 1?", target=["2", "2.0", "Two"])],
        solver=solver_with_score("foo", 0.6),
    )
def check_scoring_log(log: EvalLog, scores: dict[str, float]):
    assert log.status == "success"
    assert log.results
    for scorer, value in scores.items():
        eval_score = next(
            (es for es in log.results.scores if es.scorer == scorer), None
        )
        assert eval_score
        assert eval_score.metrics["accuracy"].value == value
def test_solver_scoring():
    log = eval(scoring_task(), model="mockllm/model")[0]
    check_scoring_log(log, {"foo": 0.6})
def test_solver_scoring_ammend():
    task = task_with(scoring_task(), scorer=includes())
    log = eval(task, model="mockllm/model")[0]
    check_scoring_log(log, {"foo": 0.6, "includes": 0})

================
File: tests/solver/test_setup.py
================
from inspect_ai import Task, eval
from inspect_ai.solver import (
    Generate,
    TaskState,
    solver,
)
@solver
def setup():
    async def solve(state: TaskState, _generate: Generate):
        state.store.set("setup", True)
        return state
    return solve
def test_solver_setup():
    log = eval(Task(setup=setup()), model="mockllm/model")[0]
    assert log.samples
    assert log.samples[0].store.get("setup") is True

================
File: tests/solver/test_solver_decorator.py
================
from inspect_ai.solver import Solver, chain, generate, solver, system_message
from inspect_ai.solver._chain import Chain
@solver
def my_solver() -> Solver:
    return chain(system_message("System message"), generate())
def test_solver_decorator():
    x = my_solver()
    assert isinstance(x, Chain)

================
File: tests/solver/test_solver_spec.py
================
import os
from random import random
from test_helpers.utils import ensure_test_package_installed
from inspect_ai import Task, eval, eval_retry, task
from inspect_ai.dataset import Sample
from inspect_ai.log._log import EvalLog
from inspect_ai.solver import (
    Generate,
    SolverSpec,
    TaskState,
    generate,
    solver,
)
from inspect_ai.solver._chain import chain
from inspect_ai.solver._solver import Solver
@solver
def the_solver(rate=1.0):
    @solver
    def failing_solver(rate=1.0):
        async def solve(state: TaskState, generate: Generate):
            if random() <= rate:
                raise ValueError("Eval failed!")
            return state
        return solve
    return chain(failing_solver(rate), generate())
@task
def the_task(solver: Solver = generate()):
    return Task(
        dataset=[Sample(input="Say hello.", target="Hello")],
        solver=solver,
    )
def test_solver_spec():
    solver_file = f"{os.path.relpath(__file__)}"
    def check_solver_spec(solver_spec: str):
        log = eval(
            f"{solver_file}@the_task",
            solver=SolverSpec(solver_spec, {"rate": 1.0}),
            model="mockllm/model",
        )[0]
        check_solver(log, solver_spec)
    check_solver_spec("the_solver")
    check_solver_spec(solver_file)
    check_solver_spec(f"{solver_file}@the_solver")
def test_solver_extension():
    ensure_test_package_installed()
    log = eval(
        the_task(), solver=SolverSpec("inspect_package/cot"), model="mockllm/model"
    )[0]
    assert log.eval.solver == "inspect_package/cot"
    assert log.plan.steps[0].solver == "chain_of_thought"
def test_solver_retry():
    log = eval(the_task(), solver=the_solver(1.0), model="mockllm/model")[0]
    check_solver(log)
    log = eval_retry(log)[0]
    check_solver(log)
def check_solver(log: EvalLog, solver_name="the_solver"):
    assert log.eval.solver == solver_name
    assert log.eval.solver_args == {"rate": 1.0}
    assert log.plan.steps[0].params["rate"] == 1.0

================
File: tests/solver/test_solver.py
================
import pytest
from inspect_ai import Task, eval
from inspect_ai.dataset import Sample
from inspect_ai.model import ChatMessageUser, ModelOutput, get_model
from inspect_ai.scorer import match
from inspect_ai.solver import (
    Generate,
    TaskState,
    chain_of_thought,
    generate,
    solver,
)
from inspect_ai.solver._plan import Plan
def test_solvers_termination():
    @solver
    def user_input(input: str):
        async def solve(state: TaskState, generate: Generate):
            state.messages.append(ChatMessageUser(content=input))
            return state
        return solve
    @solver
    def complete_task():
        async def solve(state: TaskState, generate: Generate):
            state.completed = True
            return state
        return solve
    @solver
    def finish():
        async def solve(state: TaskState, generate: Generate):
            state.output = ModelOutput.from_content(
                model="mockllm/model", content="finished"
            )
            return state
        return solve
    model = get_model("mockllm/model")
    task = Task(
        dataset=[Sample(input="What is 1 + 1?", target=["2", "2.0", "Two"])],
        solver=Plan(
            [
                chain_of_thought(),
                generate(),
                user_input("How about multiplying the numbers?"),
                generate(),
                complete_task(),
                user_input("How about subtracting the numbers?"),
                generate(),
            ],
            finish=finish(),
            internal=True,
        ),
        scorer=match(),
    )
    log = eval(task, model=model)[0]
    assert len(log.samples[0].messages) == 4
    assert log.samples[0].output.completion == "finished"
    log = eval(task, model=model, message_limit=2)[0]
    assert len(log.samples[0].messages) == 2
def test_invalid_solvers_error():
    def not_async():
        def inner(state: TaskState, generate: Generate) -> TaskState:
            return state
        return inner
    class NotCallable:
        async def inner(self, state: TaskState, generate: Generate) -> TaskState:
            return state
    class NotAsyncCallable:
        def __call__(self, state: TaskState, target: Generate) -> TaskState:
            return state
    for f in [not_async, NotCallable, NotAsyncCallable]:
        with pytest.raises(TypeError):
            solver(name=f.__name__)(f)()
def test_valid_solvers_succeed():
    def is_async():
        async def inner(self, state: TaskState, generate: Generate) -> TaskState:
            return state
        return inner
    class IsAsyncCallable:
        async def __call__(self, state: TaskState, generate: Generate) -> TaskState:
            return state
    for f in [is_async, IsAsyncCallable]:
        solver(name=f.__name__)(f)()

================
File: tests/solver/test_store_model.py
================
from typing import Any
import pytest
from pydantic import BaseModel, Field, ValidationError
from inspect_ai import Task, eval
from inspect_ai.solver._solver import Solver, solver
from inspect_ai.util import Store, StoreModel, store, store_as
class MyModel(StoreModel):
    x: int = Field(default=5)
    y: str = Field(default="default_y")
    z: float = Field(default=1.23)
def check_solver(solver: Solver):
    log = eval(Task(solver=solver))[0]
    assert log.status == "success"
def test_store_model_basic():
    @solver
    def model_basic():
        async def solve(state, generate):
            model = MyModel()
            assert model.y == "default_y"
            assert model.z == 1.23
            return state
        return solve
    log = eval(Task(solver=model_basic()), model="mockllm/model")[0]
    assert log.status == "success"
def test_store_model_log() -> None:
    class Step(BaseModel):
        response: dict[str, Any]
        results: list[dict[str, Any]]
    class Trajectory(StoreModel):
        x: int = Field(default=5)
        y: str = Field(default="default_y")
        z: float = Field(default=1.23)
        steps: list[Step] = Field(default_factory=list)
    @solver
    def model_log():
        async def solve(state, generate):
            model = Trajectory()
            model.x = 1
            model.y = "a"
            model.steps.append(Step(response={"foo": "bar"}, results=[{"foo": "bar"}]))
            return state
        return solve
    log = eval(Task(solver=model_log()), model="mockllm/model")[0]
    assert log.samples
    # reconstruct the store from the sample
    store = Store(log.samples[0].store)
    assert store.get("Trajectory:x") == 1
    assert store.get("Trajectory:y") == "a"
    # reconstruct the store model from the sample
    my_model = Trajectory(store=store)
    assert my_model.x == 1
    assert my_model.y == "a"
    # access the store model via store_as
    my_model = log.samples[0].store_as(Trajectory)
    assert my_model.x == 1
    assert my_model.y == "a"
    assert isinstance(my_model.steps[0], Step)
def test_store_model_assignment():
    def check_values(s, m: MyModel):
        assert s.get("MyModel:x") == 42
        assert s.get("MyModel:y") == "new_value"
        assert s.get("MyModel:z") == 9.99
        # Also check the model itself
        assert m.x == 42
        assert m.y == "new_value"
        assert m.z == 9.99
    @solver
    def model_assignment():
        async def solve(state, generate):
            # test w/ explicit store
            s = Store()
            model = MyModel(store=s)
            model.x = 42
            model.y = "new_value"
            model.z = 9.99
            check_values(s, model)
            # test w/ default store
            model = store_as(MyModel)
            model.x = 42
            model.y = "new_value"
            model.z = 9.99
            check_values(store(), model)
            return state
        return solve
    assert (
        eval(Task(solver=model_assignment()), model="mockllm/model")[0].status
        == "success"
    )
def test_store_model_validation_error():
    store = Store()
    model = MyModel(store=store)
    with pytest.raises(ValidationError):
        model.x = "not an integer"
def test_store_model_behind_the_scenes_update():
    store = Store()
    model = MyModel(store=store)
    # Initially set:
    model.x = 1
    # Behind the scenes update:
    store.set("MyModel:x", 999)
    assert model.x == 999
def test_store_model_init_from_store():
    store = Store()
    store.set("MyModel:x", 999)
    model = MyModel(store=store)
    assert model.x == 999
def test_store_model_dump():
    store = Store()
    model = MyModel(store=store)
    model.x = 123
    model.y = "hello"
    dumped = model.model_dump()
    assert dumped["x"] == 123
    assert dumped["y"] == "hello"
    assert dumped["z"] == 1.23  # default
    store.set("MyModel:x", 10)
    dumped = model.model_dump()
    assert dumped["x"] == 10
def test_store_model_multiple_instances_same_store():
    store = Store()
    model1 = MyModel(store=store)
    model2 = MyModel(store=store)
    model1.x = 42
    assert model2.x == 42
    model2.y = "shared"
    assert model1.y == "shared"
def test_store_model_deletion():
    store = Store()
    model = MyModel(store=store)
    # Delete from store
    store.delete("MyModel:x")
    assert model.x == 5  # Should return to default value
    # Verify store state
    assert "MyModel:x" not in store
class NestedModel(BaseModel):
    name: str
    value: int
class ComplexModel(StoreModel):
    nested: NestedModel = Field(default=NestedModel(name="default", value=0))
    items: list[str] = Field(default_factory=list)
def test_store_model_complex_model_handling():
    store = Store()
    model = ComplexModel(store=store)
    # Test nested model assignment
    new_nested = NestedModel(name="test", value=42)
    model.nested = new_nested
    assert store.get("ComplexModel:nested").model_dump() == new_nested.model_dump()
    # Test list handling
    model.items = ["a", "b", "c"]
    assert store.get("ComplexModel:items") == ["a", "b", "c"]
def test_store_model_validation_on_update():
    store = Store()
    model = MyModel(store=store)
    # Test direct store update with invalid value
    with pytest.raises(ValidationError):
        store.set("MyModel:x", "invalid")
        model._sync_model()  # Should trigger validation
    # Test multiple field validation
    with pytest.raises(ValidationError):
        store.set("MyModel:x", "invalid")
        store.set("MyModel:z", "also invalid")
        model._sync_model()
def test_store_model_dump_options():
    model = MyModel()
    model.x = 42
    # Test exclude
    dumped = model.model_dump(exclude={"y"})
    assert "y" not in dumped
    assert dumped["x"] == 42
    # Test include
    dumped = model.model_dump(include={"x"})
    assert len(dumped) == 1
    assert dumped["x"] == 42
    # Test json dump
    json_dumped = model.model_dump_json()
    assert '"x":42' in json_dumped
class DerivedModel(MyModel):
    additional: str = Field(default="extra")
def test_store_model_inheritance():
    store = Store()
    derived = DerivedModel(store=store)
    # Test that base class fields work
    derived.x = 42
    assert store.get("DerivedModel:x") == 42
    # Test that new fields work
    derived.additional = "modified"
    assert store.get("DerivedModel:additional") == "modified"
    # Test that namespacing is correct
    base = MyModel(store=store)
    base.x = 100
    assert derived.x == 42  # Should not be affected by base model

================
File: tests/solver/test_store.py
================
from typing import Generator
from test_helpers.tool_call_utils import get_tool_calls, get_tool_response
from inspect_ai import Task, eval
from inspect_ai.dataset import Sample
from inspect_ai.model import ModelOutput, get_model
from inspect_ai.scorer import includes, match
from inspect_ai.solver import Generate, TaskState, generate, solver, use_tools
from inspect_ai.tool import tool
from inspect_ai.util._store import store
def test_sample_store():
    @solver
    def store_solver():
        async def solve(state: TaskState, generate: Generate):
            state.store.set("data", state.sample_id)
            return state
        return solve
    task = Task(
        dataset=[
            Sample(input="Say Hello", target="Hello"),
            Sample(input="Say Goodbye", target="Goodbye"),
        ],
        solver=[store_solver(), generate()],
        scorer=match(),
    )
    log = eval(task, model="mockllm/model")[0]
    assert log.samples[0].store["data"] == 1
    assert log.samples[1].store["data"] == 2
def test_tool_store():
    @tool
    def get_cookie():
        async def exec():
            """
            Tool for getting the cookie.
            If you are asked to get a cookie, call the get_cookie function.
            Returns: The cookie.
            """
            return store().get("cookie", 0)
        return exec
    @solver
    def store_solver():
        async def solve(state: TaskState, generate: Generate):
            state.store.set("cookie", 42)
            return state
        return solve
    task = Task(
        dataset=[
            Sample(input="Get the cookie using the available tools.", target="ignored"),
        ],
        solver=[use_tools(get_cookie()), store_solver(), generate()],
        scorer=includes(),
    )
    def custom_outputs() -> Generator[ModelOutput, None, None]:
        yield ModelOutput.for_tool_call(
            model="mockllm/model",
            tool_name="get_cookie",
            tool_arguments={},
        )
        while True:
            yield ModelOutput.from_content(
                model="mockllm/model",
                content="finished",
            )
    log = eval(
        task,
        get_model(
            "mockllm/model",
            custom_outputs=custom_outputs(),
        ),
        message_limit=5,
    )[0]
    assert (
        get_tool_response(
            log.samples[0].messages,
            get_tool_calls(log.samples[0].messages, "get_cookie")[0],
        ).text
        == "42"
    )
def test_store_create():
    @solver
    def store_solver():
        async def solve(state: TaskState, generate: Generate):
            COLORS = ["red", "green", "blue"]
            colors = state.store.get("colors", COLORS.copy())
            colors.append("orange")
            assert state.store.get("colors") == (COLORS + ["orange"])
            return state
        return solve
    task = Task(solver=[store_solver()])
    log = eval(task, model="mockllm/model")[0]
    assert log.status == "success"

================
File: tests/solver/test_subtask.py
================
# does get_model actually use the default task generate config?
# does it successfully override?
from inspect_ai import Task, eval
from inspect_ai.dataset import Sample
from inspect_ai.model import ModelOutput
from inspect_ai.scorer import match
from inspect_ai.solver import Generate, TaskState, generate, solver
from inspect_ai.util._store import store
from inspect_ai.util._subtask import subtask
@subtask
async def times_two(input: int) -> int:
    assert store().get("x", 0) == 0
    store().set("x", 84)
    return input * 2
def test_subtask():
    @solver
    def subtask_solver():
        async def solve(state: TaskState, generate: Generate):
            state.store.set("x", 42)
            # call twice so times_two can verify that state is reset
            result = await times_two(int(state.input_text))
            result = await times_two(int(state.input_text))
            state.output = ModelOutput.from_content(state.model.name, str(result))
            return state
        return solve
    task = Task(
        dataset=[
            Sample(input="1", target="2"),
        ],
        solver=[generate(), subtask_solver()],
        scorer=match(),
    )
    log = eval(task, model="mockllm/model")[0]
    assert log.samples[0].store.get("x") == 42
    assert log.samples[0].output.completion == "2"

================
File: tests/solver/test_transcript.py
================
from inspect_ai import Task, eval
from inspect_ai.dataset import Sample
from inspect_ai.log import transcript
from inspect_ai.scorer import match
from inspect_ai.solver import (
    Generate,
    TaskState,
    generate,
    solver,
)
def test_sample_transcript():
    @solver
    def transcript_solver():
        async def solve(state: TaskState, generate: Generate):
            with transcript().step("info"):
                state.metadata["foo"] = "bar"
                transcript().info(str(state.sample_id))
            return state
        return solve
    task = Task(
        dataset=[
            Sample(input="Say Hello", target="Hello"),
        ],
        solver=[transcript_solver(), generate()],
        scorer=match(),
    )
    log = eval(task, model="mockllm/model")[0]
    # we sometimes use this for debugging our transcript assertions
    # print(
    #     json.dumps(
    #         to_jsonable_python(log.samples[0].transcript, exclude_none=True), indent=2
    #     )
    # )
    assert log.samples[0].transcript.events[3].type == "solver"
    assert log.samples[0].transcript.events[5].data == "1"
    assert log.samples[0].transcript.events[8].event == "state"

================
File: tests/test_eval_config.py
================
import subprocess
import tempfile
from pathlib import Path
from inspect_ai import Task, eval, task
from inspect_ai.log import EvalLog
from inspect_ai.log._file import list_eval_logs, read_eval_log
from inspect_ai.solver import solver
def test_eval_config_task():
    log = eval(
        tasks="eval_config_task",
        task_args=config_path("task.yaml"),
        model="mockllm/model",
        model_args=config_path("model.yaml"),
    )[0]
    check_log(log)
def test_eval_config_task_cli():
    with tempfile.TemporaryDirectory() as log_dir:
        subprocess.run(
            [
                "inspect",
                "eval",
                "tests/test_eval_config.py@eval_config_task",
                "--task-config",
                config_path("task.yaml"),
                "-T",
                "color=green",
                "--model-config",
                config_path("model.yaml"),
                "--solver",
                "eval_config_solver",
                "--solver-config",
                config_path("solver.yaml"),
                "--log-dir",
                log_dir,
                "--model",
                "mockllm/model",
            ]
        )
        log = read_eval_log(list_eval_logs(log_dir)[0])
        check_log(log, "green")
@solver
def eval_config_solver(shape="square"):
    async def solve(state, generate):
        return await generate(state)
    return solve
@task
def eval_config_task(epochs=1, color="red") -> Task:
    return Task(epochs=epochs)
def check_log(log: EvalLog, color="purple") -> None:
    assert log.eval.config.epochs == 2
    assert log.eval.task_args["color"] == color
    assert log.eval.model_args["foo"] == "bar"
    if log.eval.solver_args:
        assert log.eval.solver_args["shape"] == "square"
TEST_EVAL_CONFIG_PATH = Path("tests/test_eval_config")
def config_path(file: str) -> str:
    return (TEST_EVAL_CONFIG_PATH / file).as_posix()

================
File: tests/test_eval_set.py
================
import shutil
import tempfile
from copy import deepcopy
from pathlib import Path
import pytest
from test_helpers.utils import (
    failing_solver,
    failing_task,
    failing_task_deterministic,
    keyboard_interrupt,
    skip_if_trio,
    sleep_for_solver,
)
from inspect_ai import Task, task
from inspect_ai._eval.evalset import (
    eval_set,
    latest_completed_task_eval_logs,
    list_all_eval_logs,
)
from inspect_ai.dataset import Sample
from inspect_ai.log._file import list_eval_logs, read_eval_log, write_eval_log
from inspect_ai.model import get_model
from inspect_ai.scorer._match import includes
from inspect_ai.solver import generate
def test_eval_set() -> None:
    # run eval with a solver that fails 10% of the time
    with tempfile.TemporaryDirectory() as log_dir:
        success, logs = eval_set(
            tasks=failing_task(rate=0.1, samples=10),
            log_dir=log_dir,
            retry_attempts=1000,
            retry_wait=0.1,
            model="mockllm/model",
        )
        assert success
        assert logs[0].status == "success"
        # read and write logs based on location
        for log in logs:
            log = read_eval_log(log.location)
            log.eval.metadata = {"foo": "bar"}
            write_eval_log(log)
            log = read_eval_log(log.location)
            assert log.eval.metadata
            log.eval.metadata["foo"] = "bar"
    # run eval that is guaranteed to fail
    with tempfile.TemporaryDirectory() as log_dir:
        success, logs = eval_set(
            tasks=failing_task(rate=1, samples=10),
            log_dir=log_dir,
            retry_attempts=1,
            retry_wait=0.1,
            model="mockllm/model",
        )
        assert not success
        assert logs[0].status == "error"
def test_eval_set_dynamic() -> None:
    with tempfile.TemporaryDirectory() as log_dir:
        dataset: list[Sample] = []
        for _ in range(0, 10):
            dataset.append(Sample(input="Say hello", target="hello"))
        task1 = Task(
            name="task1",
            dataset=deepcopy(dataset),
            solver=[failing_solver(0.2), generate()],
            scorer=includes(),
        )
        task2 = Task(
            name="task2",
            dataset=deepcopy(dataset),
            solver=[failing_solver(0.2), generate()],
            scorer=includes(),
        )
        success, logs = eval_set(
            tasks=[task1, task2],
            log_dir=log_dir,
            model=[get_model("mockllm/model"), get_model("mockllm/model2")],
            retry_attempts=100,
            retry_wait=0.1,
        )
        assert len(logs) == 4
        assert success
def test_eval_set_identifiers() -> None:
    dataset: list[Sample] = []
    for _ in range(0, 10):
        dataset.append(Sample(input="Say hello", target="hello"))
    @task
    def make_task(param="param"):
        return Task(
            dataset=deepcopy(dataset),
            solver=[failing_solver(0.2), generate()],
            scorer=includes(),
        )
    def eval_tasks(tasks: list[Task]):
        with tempfile.TemporaryDirectory() as log_dir:
            success, logs = eval_set(
                tasks=tasks,
                log_dir=log_dir,
                model=[get_model("mockllm/model")],
                retry_attempts=100,
                retry_wait=0.1,
            )
            assert success
    # test that task parameters create unique identfiers
    try:
        eval_tasks([make_task("a"), make_task("b")])
    except Exception:
        assert False
    # test that using identical params results in an error
    try:
        eval_tasks([make_task("a"), make_task("a")])
        assert False
    except Exception:
        pass
def test_latest_completed_task_eval_logs() -> None:
    # cleanup previous tests
    TEST_EVAL_SET_PATH = Path("tests/test_eval_set")
    clean_dir = TEST_EVAL_SET_PATH / "clean"
    if clean_dir.exists():
        shutil.rmtree(clean_dir.as_posix())
    # verify we correctly select only the most recent log
    all_logs = list_all_eval_logs(TEST_EVAL_SET_PATH.as_posix())
    assert len(all_logs) == 2
    latest = latest_completed_task_eval_logs(all_logs, False)
    assert len(latest) == 1
    # verify that we correctly clean when requested
    clean_dir.mkdir(exist_ok=True)
    try:
        for filename in TEST_EVAL_SET_PATH.glob("*.json"):
            destination = clean_dir / filename.name
            shutil.copy2(filename, destination)
        all_logs = list_all_eval_logs(clean_dir.as_posix())
        latest = latest_completed_task_eval_logs(all_logs, True)
        assert len(list_eval_logs(clean_dir.as_posix())) == 1
    finally:
        shutil.rmtree(clean_dir, ignore_errors=True)
@pytest.mark.slow
@skip_if_trio
def test_eval_set_s3(mock_s3) -> None:
    success, logs = eval_set(
        tasks=failing_task(rate=0, samples=1),
        log_dir="s3://test-bucket",
        retry_attempts=1,
        retry_wait=0.1,
        model="mockllm/model",
    )
    assert success
    assert logs[0].status == "success"
def test_eval_zero_retries() -> None:
    with tempfile.TemporaryDirectory() as log_dir:
        success, logs = eval_set(
            tasks=failing_task_deterministic([True, False]),
            log_dir=log_dir,
            retry_attempts=0,
            retry_wait=0.1,
            model="mockllm/model",
        )
        assert not success
@skip_if_trio  # throwing the keyboardinterrupt corrupts trio's internals
def test_eval_set_previous_task_args():
    with tempfile.TemporaryDirectory() as log_dir:
        def run_eval_set():
            eval_set(
                tasks=[sleep_for_3_task("foo"), sleep_for_1_task("bar")],
                log_dir=log_dir,
                max_tasks=2,
                model="mockllm/model",
            )
        # initial pass
        try:
            with keyboard_interrupt(2):
                run_eval_set()
        except KeyboardInterrupt:
            pass
        # second pass (no keyboard interrupt so runs to completion)
        run_eval_set()
        # re-run the eval-set again (it should complete without errors b/c
        # the logs in the directory are successfully matched against the
        # task args of the tasks passed to eval_set)
        run_eval_set()
def test_eval_set_retry_started():
    with tempfile.TemporaryDirectory() as log_dir:
        def run_eval_set():
            eval_set(
                tasks=[sleep_for_1_task("bar")],
                log_dir=log_dir,
                model="mockllm/model",
            )
        def eval_log_status():
            log_file = list_eval_logs(log_dir)[0].name
            log = read_eval_log(log_file)
            return log.status
        # run a first pass
        run_eval_set()
        # modify the log to be 'started' and save it
        log_file = list_eval_logs(log_dir)[0].name
        log = read_eval_log(log_file)
        log.status = "started"
        write_eval_log(log)
        assert eval_log_status() == "started"
        # re-run the eval set and confirm status 'succes'
        run_eval_set()
        assert eval_log_status() == "success"
@task
def sleep_for_1_task(task_arg: str):
    return Task(
        solver=[sleep_for_solver(1)],
    )
@task
def sleep_for_3_task(task_arg: str):
    return Task(
        solver=[sleep_for_solver(3)],
    )

================
File: tests/test_eval.py
================
import functools
from copy import deepcopy
import pytest
from inspect_ai import Epochs, Task, eval, eval_async
from inspect_ai._util._async import tg_collect
from inspect_ai.dataset import Sample
from inspect_ai.scorer import match
@pytest.mark.anyio
async def test_no_concurrent_eval_async():
    tasks = [
        Task(dataset=[Sample(input="Say Hello", target="Hello")], scorer=match())
        for i in range(0, 2)
    ]
    with pytest.raises(RuntimeError):
        await tg_collect(
            [
                functools.partial(eval_async, task, model="mockllm/model")
                for task in tasks
            ]
        )
def test_eval_config_override():
    task = Task(
        dataset=[Sample(input="Say Hello", target="Hello")],
        message_limit=10,
        epochs=Epochs(2, "at_least_1"),
        fail_on_error=True,
        scorer=match(),
    )
    log = eval(deepcopy(task), model="mockllm/model")[0]
    assert log.eval.config.message_limit == 10
    assert log.eval.config.epochs == 2
    assert log.eval.config.epochs_reducer == ["at_least_1"]
    assert log.eval.config.fail_on_error is True
    log = eval(
        deepcopy(task),
        message_limit=5,
        epochs=Epochs(5, "at_least_3"),
        fail_on_error=0.5,
        model="mockllm/model",
    )[0]
    assert log.eval.config.message_limit == 5
    assert log.eval.config.epochs == 5
    assert log.eval.config.epochs_reducer == ["at_least_3"]
    assert log.eval.config.fail_on_error == 0.5

================
File: tests/test_examples.py
================
from test_helpers.utils import run_example, skip_if_github_action
@skip_if_github_action
def test_examples():
    run_example(example="security_guide.py", model="mockllm/model")
    run_example(example="popularity.py", model="mockllm/model")

================
File: tests/test_extensions.py
================
import importlib
import pytest
from pydantic_core import to_jsonable_python
from test_helpers.tools import list_files
from test_helpers.utils import ensure_test_package_installed, skip_if_trio
from inspect_ai import Task, eval_async
from inspect_ai.dataset import Sample
from inspect_ai.model import get_model
from inspect_ai.scorer import includes
from inspect_ai.solver import generate, use_tools
from inspect_ai.util import SandboxEnvironmentSpec
@pytest.mark.asyncio
@skip_if_trio
async def test_extension_model():
    # ensure the package is installed
    ensure_test_package_installed()
    # call the model
    mdl = get_model("custom/gpt7")
    result = await mdl.generate([{"role": "user", "content": "hello"}], [], "none", {})
    assert result.completion == "Hello from gpt7"
@pytest.mark.anyio
async def test_extension_sandboxenv():
    # ensure the package is installed
    ensure_test_package_installed()
    # run a task using the sandboxenv
    task = Task(
        dataset=[
            Sample(
                input="Please use the list_files tool to list the files in the current directory"
            )
        ],
        solver=[use_tools(list_files()), generate()],
        scorer=includes(),
        sandbox="podman",
    )
    await eval_async(task, model="mockllm/model")
@pytest.mark.anyio
async def test_extension_sandboxenv_with_specialised_config():
    # ensure the package is installed
    ensure_test_package_installed()
    module = importlib.import_module("inspect_package.sandboxenv.podman")
    PodmanSandboxEnvironmentConfig = module.PodmanSandboxEnvironmentConfig
    # run a task using the sandboxenv
    task = Task(
        dataset=[
            Sample(
                input="Please use the list_files tool to list the files in the current directory"
            )
        ],
        solver=[use_tools(list_files()), generate()],
        scorer=includes(),
        sandbox=SandboxEnvironmentSpec(
            "podman", PodmanSandboxEnvironmentConfig(socket_path="/path/to/socket")
        ),
    )
    logs = await eval_async(task, model="mockllm/model")
    # Ensure that the PodmanSandboxEnvironmentConfig object is serializable.
    to_jsonable_python(logs[0].eval, exclude_none=True, fallback=lambda _x: None)
def test_can_roundtrip_specialised_config():
    ensure_test_package_installed()
    module = importlib.import_module("inspect_package.sandboxenv.podman")
    PodmanSandboxEnvironmentConfig = module.PodmanSandboxEnvironmentConfig
    # Historical issue: the SandboxEnvironmentSpec type was unable to determine which
    # sandbox-specific config Pydantic model to instantiate when deserializing from
    # JSON.
    spec = SandboxEnvironmentSpec(
        type="podman",
        config=PodmanSandboxEnvironmentConfig(socket_path="/path/to/socket"),
    )
    json_str = spec.model_dump_json()
    recreated = SandboxEnvironmentSpec.model_validate_json(json_str)
    assert recreated == spec
    assert recreated.config == spec.config
    assert isinstance(recreated.config, PodmanSandboxEnvironmentConfig)
def test_supports_str_config():
    spec = SandboxEnvironmentSpec(type="podman", config="/path/to/socket")
    json_str = spec.model_dump_json()
    recreated = SandboxEnvironmentSpec.model_validate_json(json_str)
    assert recreated == spec
    assert recreated.config == spec.config
    assert isinstance(recreated.config, str)

================
File: tests/test_fail_on_error.py
================
from random import random
from typing import Callable
from inspect_ai import Task, eval, eval_retry, task
from inspect_ai.dataset import Sample
from inspect_ai.scorer import includes
from inspect_ai.solver import Generate, TaskState, generate, solver
@solver
def failing_solver(fail: Callable[[TaskState], bool] = lambda state: True):
    async def solve(state: TaskState, generate: Generate):
        if fail(state):
            raise ValueError("Eval failed!")
        return state
    return solve
def create_failing_task(
    samples: int,
    fail_on_error: bool | float | None,
    fail: Callable[[TaskState], bool] = lambda s: True,
):
    dataset: list[Sample] = []
    for i in range(0, samples):
        dataset.append(Sample(input="Say hello.", target="Hello"))
    return Task(
        dataset=dataset,
        solver=[failing_solver(fail), generate()],
        fail_on_error=fail_on_error,
    )
def eval_failing_task(
    samples: int,
    fail_on_error: bool | float | None,
    fail: Callable[[TaskState], bool] = lambda s: True,
):
    task = create_failing_task(samples, fail_on_error, fail)
    return eval(task, model="mockllm/model")[0]
def test_fail_on_error():
    log = eval_failing_task(1, True)
    assert log.status == "error"
def test_no_fail_on_error():
    log = eval_failing_task(1, False)
    assert log.status == "success"
def test_fail_on_num_errors():
    log = eval_failing_task(
        samples=10, fail_on_error=4, fail=lambda state: state.sample_id < 5
    )
    assert log.status == "error"
    log = eval_failing_task(
        samples=10, fail_on_error=4, fail=lambda state: state.sample_id < 3
    )
    assert log.results.completed_samples == 8
    assert log.status == "success"
def test_fail_on_pct_errors():
    log = eval_failing_task(
        samples=10, fail_on_error=0.35, fail=lambda state: state.sample_id < 5
    )
    assert log.status == "error"
    log = eval_failing_task(
        samples=10, fail_on_error=0.7, fail=lambda state: state.sample_id < 7
    )
    assert log.results.completed_samples == 4
    assert log.status == "success"
def test_fail_on_error_override():
    task = create_failing_task(
        samples=10, fail_on_error=0.7, fail=lambda state: state.sample_id < 7
    )
    log = eval(task, fail_on_error=0.6, model="mockllm/model")[0]
    assert log.status == "error"
@task
def fail_on_error_failing_task():
    return Task(
        dataset=[
            Sample(input="Say hello", target="hello"),
            Sample(input="Say hello", target="hello"),
            Sample(input="Say hello", target="hello"),
        ],
        solver=[failing_solver(lambda _s: random() > 0.33), generate()],
        fail_on_error=False,
        scorer=includes(),
    )
def test_fail_on_error_retry():
    # run eval with a solver that fails 2/3 times
    log = eval(fail_on_error_failing_task, model="mockllm/model")[0]
    # note the task id so we can be certain it remains the same
    task_id = log.eval.task_id
    # retry until we succeed (confirming the task_id is stable)
    while not log.results or (
        log.results.completed_samples < log.results.total_samples
    ):
        log = eval_retry(log)[0]
        assert log.eval.task_id == task_id
@task
def always_fails():
    return Task(
        solver=[failing_solver(), generate()],
    )
def test_fail_on_error_retry_override():
    # fail the first time
    log = eval(always_fails(), model="mockllm/model")[0]
    assert log.status == "error"
    # try again with fail_on_error = False
    log = eval_retry(log, fail_on_error=False)[0]
    assert log.status == "success"

================
File: tests/test_helpers/file_runner.py
================
import subprocess
import sys
from pathlib import Path
def run_tests_by_file():
    args = sys.argv[1:]
    TESTS_DIR = Path(__file__).parent.parent
    test_files = TESTS_DIR.glob("**/test_*.py")
    for test_file in test_files:
        pytest_command = ["pytest", test_file.as_posix()] + args
        result = subprocess.run(pytest_command)
        if result.returncode != 0:
            sys.exit(result.returncode)
    print("All tests passed.")
if __name__ == "__main__":
    run_tests_by_file()

================
File: tests/test_helpers/tasks.py
================
from inspect_ai import Task, task
from inspect_ai.dataset import Sample
from inspect_ai.scorer import includes
from inspect_ai.solver import generate, use_tools
from inspect_ai.tool import Tool
@task
def empty_task() -> Task:
    return Task()
@task
def minimal_task() -> Task:
    return Task(
        dataset=[Sample(input="What is 1+1?", target="2")],
        solver=[generate()],
        scorer=includes(),
        metadata={"task_idx": 1},
        name="minimal",
    )
@task
def minimal_task_for_tool_use(tool: Tool) -> Task:
    return Task(
        dataset=[Sample(input="Please use the tool", target="n/a")],
        solver=[use_tools(tool), generate()],
        scorer=includes(),
        metadata={"task_idx": 1},
        message_limit=3,
        sandbox="docker",
    )

================
File: tests/test_helpers/tool_call_utils.py
================
from inspect_ai.log._log import EvalLog
from inspect_ai.log._transcript import ToolEvent
from inspect_ai.model import (
    ChatMessage,
    ChatMessageAssistant,
    ChatMessageTool,
)
from inspect_ai.tool import ToolCall
def get_tool_call(messages: list[ChatMessage], tool: str) -> ToolCall | None:
    tool_calls = get_tool_calls(messages, tool)
    if tool_calls:
        return tool_calls[0]
    else:
        return None
def get_tool_calls(messages: list[ChatMessage], tool: str) -> list[ToolCall]:
    tool_call_messages = [
        message
        for message in messages
        if isinstance(message, ChatMessageAssistant) and message.tool_calls
    ]
    tool_calls: list[ToolCall] = []
    for message in tool_call_messages:
        tool_calls.extend(
            [
                tool_call
                for tool_call in (message.tool_calls or [])
                if tool_call.function == tool
            ]
        )
    return tool_calls
def get_tool_response(
    messages: list[ChatMessage], tool_call: ToolCall
) -> ChatMessageTool | None:
    tool_messages = [
        message for message in messages if isinstance(message, ChatMessageTool)
    ]
    tool_response = next(
        (message for message in tool_messages if message.tool_call_id == tool_call.id),
        None,
    )
    if tool_response:
        return tool_response
    else:
        return None
def get_tool_event(log: EvalLog) -> ToolEvent | None:
    if log.samples:
        return next(
            (event for event in log.samples[0].events if isinstance(event, ToolEvent)),
            None,
        )
    else:
        return None

================
File: tests/test_helpers/tools.py
================
from inspect_ai.tool import ContentText, ToolError, tool
from inspect_ai.util import sandbox
# define tool
@tool
def addition():
    async def execute(x: int, y: int):
        """
        Add two numbers.
        Args:
            x (int): First number to add.
            y (int): Second number to add.
        Returns:
            The sum of the two numbers.
        """
        # return as list[Content] to confirm that codepath works
        return [ContentText(text=str(x + y))]
    return execute
@tool
def command_exec():
    async def execute(command: str) -> str:
        """Executes the given bash script in the agent execution environment, in the working directory.
        Args:
            command (str): A single string of the bash script to run in the container.
        Returns:
            out (str): output of the script (success/failure, return code, stdout, stderr).
        """
        result = await sandbox().exec(cmd=["bash", "-c", command], timeout=90)
        return str(result)
    return execute
@tool
def read_file():
    async def execute(file: str):
        """
        Read the contents of a file.
        Args:
          file (str): File to read.
        Returns:
          File contents
        """
        return await sandbox().read_file(file)
    return execute
@tool
def list_files():
    async def execute(dir: str):
        """List the files in a directory.
        Args:
            dir (str): Directory
        Returns:
            File listing of the directory
        """
        result = await sandbox().exec(["ls", dir])
        if result.success:
            return result.stdout
        else:
            raise ToolError(result.stderr)
    return execute
@tool
def raise_error():
    async def execute():
        """Raise an error."""
        raise RuntimeError("Raising an error.")
    return execute

================
File: tests/test_helpers/utils.py
================
import contextlib
import importlib.util
import os
import signal
import subprocess
import sys
from pathlib import Path
from random import random
from types import FrameType
from typing import Generator, Sequence
import anyio
import pytest
from inspect_ai import Task, eval, task
from inspect_ai._util._async import configured_async_backend
from inspect_ai.dataset import Sample
from inspect_ai.model import ChatMessage, ModelName, ModelOutput
from inspect_ai.scorer import match
from inspect_ai.solver import Generate, TaskState, generate, solver
def skip_if_env_var(var: str, exists=True):
    """
    Pytest mark to skip the test if the var environment variable is not defined.
    Use in combination with `pytest.mark.api` if the environment variable in
    question corresponds to a paid API. For example, see `skip_if_no_openai`.
    """
    condition = (var in os.environ.keys()) if exists else (var not in os.environ.keys())
    return pytest.mark.skipif(
        condition,
        reason=f"Test doesn't work without {var} environment variable defined.",
    )
def skip_if_no_groq(func):
    return pytest.mark.api(skip_if_env_var("GROQ_API_KEY", exists=False)(func))
def skip_if_no_goodfire(func):
    return pytest.mark.api(skip_if_env_var("GOODFIRE_API_KEY", exists=False)(func))
def skip_if_no_package(package):
    return pytest.mark.skipif(
        importlib.util.find_spec(package) is None,
        reason=f"Test doesn't work without package {package} installed",
    )
def skip_if_no_vllm(func):
    return skip_if_no_package("vllm")(func)
def skip_if_no_transformers(func):
    return skip_if_no_package("transformers")(func)
def skip_if_no_accelerate(func):
    return skip_if_no_package("accelerate")(func)
def skip_if_no_openai(func):
    return pytest.mark.api(
        pytest.mark.skipif(
            importlib.util.find_spec("openai") is None
            or os.environ.get("OPENAI_API_KEY") is None,
            reason="Test requires both OpenAI package and OPENAI_API_KEY environment variable",
        )(func)
    )
def skip_if_no_openai_package(func):
    return skip_if_no_package("openai")(func)
def skip_if_no_anthropic(func):
    return pytest.mark.api(skip_if_env_var("ANTHROPIC_API_KEY", exists=False)(func))
def skip_if_no_google(func):
    return pytest.mark.api(skip_if_env_var("GOOGLE_API_KEY", exists=False)(func))
def skip_if_no_mistral(func):
    return pytest.mark.api(skip_if_env_var("MISTRAL_API_KEY", exists=False)(func))
def skip_if_no_grok(func):
    return pytest.mark.api(skip_if_env_var("GROK_API_KEY", exists=False)(func))
def skip_if_no_cloudflare(func):
    return pytest.mark.api(skip_if_env_var("CLOUDFLARE_API_TOKEN", exists=False)(func))
def skip_if_no_together(func):
    return pytest.mark.api(skip_if_env_var("TOGETHER_API_KEY", exists=False)(func))
def skip_if_no_azureai(func):
    return pytest.mark.api(skip_if_env_var("AZUREAI_API_KEY", exists=False)(func))
def skip_if_no_llama_cpp_python(func):
    return pytest.mark.api(
        skip_if_env_var("ENABLE_LLAMA_CPP_PYTHON_TESTS", exists=False)(func)
    )
def skip_if_no_vertex(func):
    return pytest.mark.api(skip_if_env_var("ENABLE_VERTEX_TESTS", exists=False)(func))
def skip_if_github_action(func):
    return skip_if_env_var("GITHUB_ACTIONS", exists=True)(func)
def skip_if_no_docker(func):
    try:
        is_docker_installed = (
            subprocess.run(
                ["docker", "--version"],
                check=False,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
            ).returncode
            == 0
        )
    except FileNotFoundError:
        is_docker_installed = False
    return pytest.mark.skipif(
        not is_docker_installed, reason="Test doesn't work without Docker installed."
    )(func)
def skip_if_async_backend(backend):
    return pytest.mark.skipif(
        configured_async_backend() == backend,
        reason=f"Test not compatible with {backend} async backend.",
    )
def skip_if_trio(func):
    return skip_if_async_backend("trio")(func)
def skip_if_asyncio(func):
    return skip_if_async_backend("asyncio")(func)
def run_example(example: str, model: str):
    example_file = os.path.join("examples", example)
    return eval(example_file, model=model, limit=1)
# The intention of this `simple_task_state` helper is to remove some of the
# boiler plate of creating a task for use in solver checks where we just need
# "some" state. Over time this will likely expand and need to be extracted into
# its own helper file with multiple options.
def simple_task_state(
    choices: list[str] = [],
    messages: list[ChatMessage] = [],
    model_output: str = "",
) -> TaskState:
    return TaskState(
        choices=choices,
        epoch=0,
        input=[],
        messages=messages,
        model=ModelName(model="fake/model"),
        output=ModelOutput.from_content(model="model", content=model_output),
        sample_id=0,
    )
@solver
def file_check(file: str):
    async def solve(state: TaskState, generate: Generate):
        if not Path(file).exists():
            raise FileNotFoundError(f"File {file} does not exist.")
        return state
    return solve
@solver
def failing_solver(rate=0.5):
    async def solve(state: TaskState, generate: Generate):
        value = random()
        if value < rate:
            raise ValueError("Eval failed!")
        return state
    return solve
@task
def failing_task(rate=0.5, samples=1) -> Task:
    dataset: list[Sample] = []
    for _ in range(0, samples):
        dataset.append(Sample(input="Say hello", target="hello"))
    return Task(
        dataset=dataset,
        solver=[failing_solver(rate), generate()],
        scorer=match(),
    )
@solver
def failing_solver_deterministic(should_fail: Sequence[bool]):
    it = iter(should_fail)
    async def solve(state: TaskState, generate: Generate):
        should_fail_this_time = it.__next__()
        if should_fail_this_time:
            raise ValueError("Eval failed!")
        return state
    return solve
@task
def failing_task_deterministic(should_fail: Sequence[bool]) -> Task:
    dataset: list[Sample] = []
    for _ in range(0, len(should_fail)):
        dataset.append(Sample(input="Say hello", target="hello"))
    return Task(
        dataset=dataset,
        plan=[failing_solver_deterministic(should_fail), generate()],
        scorer=match(),
    )
@solver
def sleep_for_solver(seconds: int):
    async def solve(state: TaskState, generate: Generate):
        await anyio.sleep(seconds)
        return state
    return solve
@solver
def identity_solver():
    async def solve(state: TaskState, generate: Generate):
        return state
    return solve
def ensure_test_package_installed():
    try:
        import inspect_package  # type: ignore # noqa: F401
    except ImportError:
        subprocess.check_call(
            [sys.executable, "-m", "pip", "install", "--no-deps", "tests/test_package"]
        )
@contextlib.contextmanager
def keyboard_interrupt(seconds: int) -> Generator[None, None, None]:
    def handler(signum: int, frame: FrameType | None) -> None:
        raise KeyboardInterrupt
    original_handler = signal.signal(signal.SIGALRM, handler)
    signal.alarm(seconds)
    try:
        yield
    finally:
        signal.alarm(0)
        signal.signal(signal.SIGALRM, original_handler)

================
File: tests/test_list_task.py
================
from pathlib import Path
from typing import Callable
from inspect_ai import TaskInfo, list_tasks
TEST_TASKS_DIR = Path("tests/test_task_list")
def list_test_tasks_dir(
    globs: list[str], filter: Callable[[TaskInfo], bool] | None = None
):
    return list_tasks(globs, filter=filter, root_dir=TEST_TASKS_DIR)
def test_task_list_multiple_file():
    tasks = list_test_tasks_dir(["multiple.py"])
    assert len(tasks) == 2
    names = [task.name for task in tasks]
    assert "first" in names
    assert "second_task" in names
def test_task_list_multiple_dir():
    tasks = list_test_tasks_dir(["multiple_dir"])
    assert len(tasks) == 2
def test_task_list_attribs():
    tasks = list_test_tasks_dir(["attribs.ipynb"])
    assert tasks[0].attribs.get("light") is True
    assert tasks[0].attribs.get("type") == "bio"
def test_task_list_filter():
    tasks = list_test_tasks_dir(["*"], filter=lambda t: t.attribs.get("type") == "bio")
    assert len(tasks) == 1
def test_task_list_recurse():
    tasks = list_test_tasks_dir(["recurse"])
    assert len(tasks) == 3

================
File: tests/test_log_dir/example_task/example_task.py
================
from inspect_ai import Task, task
from inspect_ai.dataset import Sample
from inspect_ai.scorer import match
@task
def example_task() -> Task:
    task = Task(
        dataset=[Sample(input="Say Hello", target="Hello")],
        scorer=match(),
        metadata={"meaning_of_life": 42},
    )
    return task

================
File: tests/test_log_dir/test_log_dir.py
================
import os
import shutil
import pytest
from inspect_ai import eval_async
from inspect_ai.log import read_eval_log
@pytest.fixture(scope="session")
def base_tmp_dir():
    base_dir = "tmp_testing_log_dir"
    yield base_dir
    # Cleanup after all tests are done
    # TODO this does not work when the logs are saved in a different directory i.e. the test failed.
    shutil.rmtree(base_dir)
@pytest.mark.anyio
@pytest.mark.parametrize("log_dir", ["logs", "logs/my_custom_location"])
async def test_log_dir(log_dir, base_tmp_dir):
    log_dir = str(os.path.join(base_tmp_dir, log_dir))
    await eval_async(
        ["tests/test_log_dir/example_task"], model="mockllm/model", log_dir=log_dir
    )
    assert os.path.exists(log_dir)
    assert (
        len(os.listdir(log_dir)) >= 1
    )  # as currently a empty dir is created before cwd to task dir is called.
    # Check that metadata is being logged correctly
    for filename in os.listdir(log_dir):
        if filename.endswith(".json"):
            file_path = os.path.join(log_dir, filename)
            log = read_eval_log(file_path)
            assert log.eval.metadata["meaning_of_life"] == 42
            break

================
File: tests/test_package/inspect_package/__init__.py
================
from .score.scorer import simple_score  # noqa: F401
__all__ = ["simple_score"]

================
File: tests/test_package/inspect_package/_registry.py
================
# ruff: noqa: F401
from inspect_ai.model import modelapi
from inspect_ai.util import sandboxenv
# import components so they are available in the registry
from .approvers.renamer import renamer
from .score.scorer import simple_score
from .solvers.cot import cot
# delayed import for the model and sandbox allows us to only resolve the imports
# when they are actually requeasted (so that we don't end up requiring all
# of their dependencies when this package's entry points are loaded)
@modelapi(name="custom")
def custom():
    from .modelapi.custom import CustomModelAPI
    return CustomModelAPI
@sandboxenv(name="podman")
def podman():
    from .sandboxenv.podman import PodmanSandboxEnvironment
    return PodmanSandboxEnvironment

================
File: tests/test_package/inspect_package/approvers/renamer.py
================
from copy import copy
from inspect_ai.approval import Approval, Approver, approver
from inspect_ai.solver import TaskState
from inspect_ai.tool import ToolCall, ToolCallView
@approver
def renamer(function_name: str) -> Approver:
    async def approve(
        message: str,
        call: ToolCall,
        view: ToolCallView,
        state: TaskState | None = None,
    ) -> Approval:
        call = copy(call)
        call.function = function_name
        return Approval(decision="modify", modified=call)
    return approve

================
File: tests/test_package/inspect_package/modelapi/custom.py
================
from inspect_ai.model import (
    ChatMessage,
    GenerateConfig,
    ModelAPI,
    ModelOutput,
)
from inspect_ai.tool import ToolChoice, ToolInfo
class CustomModelAPI(ModelAPI):
    def __init__(
        self,
        model_name: str,
        base_url: str | None = None,
        api_key: str | None = None,
        config: GenerateConfig = GenerateConfig(),
    ) -> None:
        super().__init__(model_name, base_url, api_key, [], config)
    async def generate(
        self,
        input: list[ChatMessage],
        tools: list[ToolInfo],
        tool_choice: ToolChoice,
        config: GenerateConfig,
    ) -> ModelOutput:
        return ModelOutput.from_content(
            self.model_name, f"Hello from {self.model_name}"
        )

================
File: tests/test_package/inspect_package/sandboxenv/podman.py
================
from typing import Any, Literal, Union, overload
from pydantic import BaseModel
from typing_extensions import override
from inspect_ai.util import ExecResult, SandboxEnvironment, SandboxEnvironmentConfigType
class PodmanSandboxEnvironment(SandboxEnvironment):
    def __init__(self, socket_path: str | None) -> None:
        self.socket_path = socket_path
    @classmethod
    async def sample_init(
        cls,
        task_name: str,
        config: SandboxEnvironmentConfigType | None,
        metadata: dict[str, str],
    ) -> dict[str, SandboxEnvironment]:
        if isinstance(config, PodmanSandboxEnvironmentConfig):
            return {"default": PodmanSandboxEnvironment(config.socket_path)}
        return {"default": PodmanSandboxEnvironment(None)}
    @classmethod
    async def sample_cleanup(
        cls,
        task_name: str,
        config: SandboxEnvironmentConfigType | None,
        environments: dict[str, SandboxEnvironment],
        interrupted: bool,
    ) -> None:
        pass
    @override
    async def exec(
        self,
        cmd: list[str],
        input: str | bytes | None = None,
        cwd: str | None = None,
        env: dict[str, str] = {},
        user: str | None = None,
        timeout: int | None = None,
        timeout_retry: bool = True,
    ) -> ExecResult[str]:
        return ExecResult(success=True, returncode=0, stdout="Hello!", stderr="")
    @override
    async def write_file(self, file: str, contents: str | bytes) -> None:
        pass
    @overload
    async def read_file(self, file: str, text: Literal[True] = True) -> str: ...
    @overload
    async def read_file(self, file: str, text: Literal[False]) -> bytes: ...
    @override
    async def read_file(self, file: str, text: bool = True) -> Union[str | bytes]:
        return ""
    @classmethod
    def config_deserialize(cls, config: dict[str, Any]) -> BaseModel:
        return PodmanSandboxEnvironmentConfig(**config)
class PodmanSandboxEnvironmentConfig(BaseModel, frozen=True):
    socket_path: str

================
File: tests/test_package/inspect_package/score/scorer.py
================
from inspect_ai.scorer import (
    CORRECT,
    INCORRECT,
    Score,
    Target,
    accuracy,
    scorer,
    stderr,
)
from inspect_ai.solver import TaskState
@scorer(metrics=[accuracy(), stderr()])
def simple_score(ignore_case: bool = True):
    async def score(state: TaskState, target: Target):
        # check for correct
        answer = state.output.completion
        text = target.text
        if ignore_case:
            correct = answer.lower().rfind(text.lower()) != -1
        else:
            correct = answer.rfind(text) != -1
        # return score
        return Score(value=CORRECT if correct else INCORRECT, answer=answer)
    return score

================
File: tests/test_package/inspect_package/solvers/cot.py
================
from inspect_ai.solver import Solver, chain, chain_of_thought, generate, solver
@solver
def cot() -> Solver:
    return chain(chain_of_thought(), generate())

================
File: tests/test_retry.py
================
import tempfile
from test_helpers.utils import failing_task
from inspect_ai import eval, eval_retry
from inspect_ai.log import list_eval_logs, retryable_eval_logs
def test_eval_retry():
    # run eval with a solver that fails 2/3 times
    log = eval(failing_task, limit=1, model="mockllm/model")[0]
    # note the task id so we can be certain it remains the same
    task_id = log.eval.task_id
    # retry until we succeed (confirming the task_id is stable)
    while log.status != "success":
        log = eval_retry(log)[0]
        assert log.eval.task_id == task_id
def test_eval_retryable():
    with tempfile.TemporaryDirectory() as log_dir:
        # run eval with a solver that fails 2/3 of the time
        log = eval(tasks=failing_task, limit=1, model="mockllm/model", log_dir=log_dir)[
            0
        ]
        # note the task id so we can be certain it remains the same
        task_id = log.eval.task_id
        # retry until we succeed (confirming the task_id is stable)
        retryable = retryable_eval_logs(list_eval_logs(log_dir))
        while len(retryable) > 0:
            assert len(retryable) == 1
            assert retryable[0].task_id == task_id
            eval_retry(retryable, log_dir=log_dir)
            retryable = retryable_eval_logs(list_eval_logs(log_dir))

================
File: tests/test_sample_id.py
================
from inspect_ai import Task, eval
from inspect_ai.dataset import Sample
def test_sample_id():
    task = Task(dataset=[Sample(id=id, input=f"Input for {id}") for id in range(0, 10)])
    log = eval(task, sample_id=5, model="mockllm/model")[0]
    assert len(log.samples) == 1
    assert log.samples[0].id == 5
    log = eval(task, sample_id=[5, 9], model="mockllm/model")[0]
    assert len(log.samples) == 2
    assert log.samples[0].id == 5
    assert log.samples[1].id == 9
    task = Task(
        dataset=[
            Sample(id=f"sample-{id}", input=f"Input for {id}") for id in range(0, 10)
        ]
    )
    log = eval(task, sample_id="sample-5", model="mockllm/model")[0]
    assert len(log.samples) == 1
    assert log.samples[0].id == "sample-5"
    log = eval(task, sample_id=["sample-5", "sample-6"], model="mockllm/model")[0]
    assert len(log.samples) == 2
    assert log.samples[0].id == "sample-5"
    assert log.samples[1].id == "sample-6"

================
File: tests/test_sample_limits.py
================
from random import randint
import anyio
from test_helpers.utils import skip_if_no_openai, sleep_for_solver
from inspect_ai import Task, eval
from inspect_ai.dataset import Sample
from inspect_ai.log._log import EvalLog
from inspect_ai.log._transcript import SampleLimitEvent
from inspect_ai.model._chat_message import ChatMessageUser
from inspect_ai.scorer import match
from inspect_ai.scorer._metric import Score
from inspect_ai.scorer._metrics import mean
from inspect_ai.scorer._scorer import Scorer, scorer
from inspect_ai.scorer._target import Target
from inspect_ai.solver import Generate, TaskState, solver
from inspect_ai.solver._solver import Solver
from inspect_ai.util._concurrency import concurrency
@solver
def looping_solver(check_tokens: bool = False, sleep_for: float | None = None):
    async def solve(state: TaskState, generate: Generate):
        # first generate
        state = await generate(state)
        # verify we are successfully tracking tokens if requested
        if check_tokens:
            assert state.token_usage > 0
        # keep generating until we hit a limit
        while True:
            if sleep_for:
                await anyio.sleep(sleep_for)
            state.messages.append(state.user_prompt)
            state = await generate(state)
        return state
    return solve
@solver
def looping_concurrecy_solver():
    async def solve(state: TaskState, generate: Generate):
        # simulate waiting for shared resource
        async with concurrency("shared-resource", 1):
            await anyio.sleep(1)
        return state
    return solve
@solver
def appending_solver():
    async def solve(state: TaskState, generate: Generate):
        # keep appending until we hit a limit
        while True:
            state.messages.append(ChatMessageUser(content="hello"))
        return state
    return solve
@solver
def overwriting_solver(messages: int):
    async def solve(state: TaskState, generate: Generate):
        state.messages = [
            ChatMessageUser(content="message") for _ in range(0, messages)
        ]
        return state
    return solve
@scorer(metrics=[mean()])
def slow_scorer(seconds: int | None = 10) -> Scorer:
    async def score(state: TaskState, target: Target) -> Score:
        if seconds is not None:
            await anyio.sleep(seconds)
        return Score(value=1)
    return score
def check_message_limit(solver: Solver):
    message_limit = randint(1, 3) * 2
    task = Task(
        dataset=[Sample(input="Say Hello", target="Hello")],
        solver=looping_solver(),
        scorer=match(),
        message_limit=message_limit,
    )
    log = eval(task, model="mockllm/model")[0]
    assert log.samples
    assert len(log.samples[0].messages) == message_limit
    check_limit_event(log, "message")
def test_message_limit_generate():
    check_message_limit(looping_solver())
def test_message_limit_append():
    check_message_limit(appending_solver())
def test_message_limit_overwrite():
    check_message_limit(overwriting_solver(10))
@skip_if_no_openai
def test_token_limit():
    token_limit = 10
    task = Task(
        dataset=[Sample(input="Say Hello", target="Hello")],
        solver=looping_solver(check_tokens=True),
        scorer=match(),
        token_limit=token_limit,
    )
    model = "openai/gpt-4o-mini"
    log = eval(task, model=model)[0]
    total_tokens = log.stats.model_usage[model].total_tokens
    assert total_tokens > token_limit
    assert total_tokens < (token_limit * 3)
    check_limit_event(log, "token")
def test_time_limit():
    log = eval(Task(solver=sleep_for_solver(3)), model="mockllm/model", time_limit=2)[0]
    check_limit_event(log, "time")
def test_time_limit_scorer():
    log = eval(
        Task(scorer=slow_scorer()),
        model="mockllm/model",
        time_limit=2,
        fail_on_error=False,
    )[0]
    assert log.status == "success"
    check_limit_event(log, "time")
def test_solver_scorer_combined_timeout():
    log = eval(
        Task(solver=sleep_for_solver(1), scorer=slow_scorer(1)),
        model="mockllm/model",
        time_limit=3,
    )[0]
    assert log.status == "success"
def test_solver_scorer_combined_timeout_exceeded():
    log = eval(
        Task(solver=sleep_for_solver(1), scorer=slow_scorer(3)),
        model="mockllm/model",
        time_limit=3,
        fail_on_error=False,
    )[0]
    assert log.status == "success"
    check_limit_event(log, "time")
def test_solver_timeout_scored():
    log = eval(
        Task(solver=sleep_for_solver(2), scorer=slow_scorer(None)),
        model="mockllm/model",
        time_limit=1,
    )[0]
    assert log.status == "success"
def test_solver_timeout_not_scored():
    log = eval(
        Task(solver=sleep_for_solver(3), scorer=slow_scorer(2)),
        model="mockllm/model",
        time_limit=2,
    )[0]
    assert log.status == "error"
def test_working_limit():
    working_limit = 3
    log = eval(
        Task(solver=looping_solver(sleep_for=1)),
        model="mockllm/model",
        working_limit=working_limit,
    )[0]
    check_working_limit_event(log, working_limit)
def test_working_limit_reporting():
    log = eval(
        Task(
            dataset=[Sample(id=id, input=f"Input for {id}") for id in range(0, 3)],
            solver=looping_concurrecy_solver(),
        ),
        model="mockllm/model",
    )[0]
    assert log.samples
    waiting_time = 0
    for sample in log.samples:
        waiting_time += sample.total_time - sample.working_time + 0.1
    assert waiting_time > 3
def check_working_limit_event(log: EvalLog, working_limit: int):
    assert log.eval.config.working_limit == working_limit
    assert log.samples
    assert log.samples[0].total_time
    assert log.samples[0].working_time
    assert log.samples[0].total_time > log.samples[0].working_time
    check_limit_event(log, "working")
def check_limit_event(log: EvalLog, content: str) -> None:
    event = find_limit_event(log)
    assert event
    assert content == event.type
def find_limit_event(log: EvalLog) -> SampleLimitEvent | None:
    if log.samples:
        return next(
            (
                event
                for event in log.samples[0].events
                if isinstance(event, SampleLimitEvent)
            ),
            None,
        )
    else:
        return None

================
File: tests/test_task_attr.py
================
from test_helpers.tasks import empty_task
from inspect_ai._eval.task.constants import TASK_FILE_ATTR, TASK_RUN_DIR_ATTR
def test_local_module_attr():
    task = empty_task()
    assert getattr(task, TASK_FILE_ATTR, None)
    assert getattr(task, TASK_RUN_DIR_ATTR, None)

================
File: tests/test_task_chdir.py
================
import os
from pathlib import Path
import pytest
from test_helpers.utils import skip_if_asyncio, skip_if_trio
from inspect_ai import eval
TEST_TASK_CHDIR_PATH = Path("tests/test_task_chdir")
@skip_if_trio
def test_task_chdir():
    cwd = os.getcwd()
    log1, log2 = eval(
        [(TEST_TASK_CHDIR_PATH / task).as_posix() for task in ["task1", "task2"]],
        model="mockllm/model",
        max_tasks=2,
    )
    assert all([log.status == "success" for log in [log1, log2]])
    assert log1.eval.metadata["task_idx"] == 1
    assert log2.eval.metadata["task_idx"] == 2
    assert cwd == os.getcwd()
@skip_if_asyncio
def test_trio_chdir_error():
    with pytest.raises(RuntimeError):
        eval(
            (TEST_TASK_CHDIR_PATH / "task1").as_posix(),
            model="mockllm/model",
        )
def test_task_chdir_error():
    log3 = eval(
        (TEST_TASK_CHDIR_PATH / "task3").as_posix(),
        model="mockllm/model",
    )[0]
    assert log3.status == "error"

================
File: tests/test_task_chdir/task1/task1.py
================
from test_helpers.utils import file_check
from inspect_ai import Task, task
from inspect_ai.dataset import Sample
from inspect_ai.scorer import includes
from inspect_ai.solver import generate
@task(chdir=True)
def task1():
    return Task(
        dataset=[Sample(input="What is 1+1?", target="2")],
        solver=[file_check("task1.py"), generate()],
        scorer=includes(),
        metadata={"task_idx": 1},
    )

================
File: tests/test_task_chdir/task2/task2.py
================
from test_helpers.utils import file_check
from inspect_ai import Task, task
from inspect_ai.dataset import Sample
from inspect_ai.scorer import includes
from inspect_ai.solver import generate
@task(chdir=True)
def task2():
    return Task(
        dataset=[
            Sample(id=id, input="What is 1+1?", target="2") for id in range(0, 10)
        ],
        solver=[file_check("task2.py"), generate()],
        scorer=includes(),
        metadata={"task_idx": 2},
    )

================
File: tests/test_task_chdir/task3/task3.py
================
from test_helpers.utils import file_check
from inspect_ai import Task, task
from inspect_ai.dataset import Sample
from inspect_ai.scorer import includes
from inspect_ai.solver import generate
@task
def task3():
    return Task(
        dataset=[Sample(input="What is 1+1?", target="2")],
        solver=[file_check("task3.py"), generate()],
        scorer=includes(),
        metadata={"task_idx": 3},
    )

================
File: tests/test_task_list/multiple_dir/_decoy/testit.py
================
from inspect_ai import Task, task
@task
def foo():
    return Task([])

================
File: tests/test_task_list/multiple_dir/_decoy2.py
================
from inspect_ai import Task, task
@task
def decoy():
    return Task([])

================
File: tests/test_task_list/multiple_dir/bar.py
================
from inspect_ai import Task, task
@task
def foo():
    return Task([])

================
File: tests/test_task_list/multiple_dir/foo.py
================
from inspect_ai import Task, task
@task
def foo():
    return Task([])

================
File: tests/test_task_list/multiple.py
================
from inspect_ai import Task, task
@task
def first():
    return Task([])
@task(name="second_task")
def second():
    return Task([])

================
File: tests/test_task_list/recurse/.folder3/epsilon.py
================
from inspect_ai import Task, task
@task
def epsilon():
    return Task([])

================
File: tests/test_task_list/recurse/folder1/_decoy.py
================
from inspect_ai import Task, task
@task
def decoy():
    return Task([])

================
File: tests/test_task_list/recurse/folder1/theta.py
================
from inspect_ai import Task, task
@task
def theta():
    return Task([])

================
File: tests/test_task_list/recurse/folder2/.folder3/epsilon.py
================
from inspect_ai import Task, task
@task
def epsilon():
    return Task([])

================
File: tests/test_task_list/recurse/folder2/another.py
================
from inspect_ai import Task, task
@task
def beta():
    return Task([])

================
File: tests/test_task_list/recurse/folder2/first.py
================
from inspect_ai import Task, task
@task
def alpha():
    return Task([])

================
File: tests/test_task_model.py
================
from inspect_ai import Task, eval, eval_retry, task
from inspect_ai.model import Model, get_model
def check_task_model(task):
    log = eval(task)[0]
    assert log.status == "success"
    assert log.eval.model == "mockllm/model"
    return log
def test_task_model():
    task = Task(model="mockllm/model")
    check_task_model(task)
@task
def dynamic_model(model: str | Model):
    return Task(model=model)
def check_task_model_arg(model):
    task = dynamic_model(model)
    log = check_task_model(task)
    log = eval_retry(log)[0]
    assert log.status == "success"
    assert log.eval.model == "mockllm/model"
    return log
def test_task_model_str_arg():
    check_task_model_arg("mockllm/model")
def test_task_model_object_arg():
    check_task_model_arg(get_model("mockllm/model"))
def test_task_model_object_arg_with_args():
    log = check_task_model_arg(
        get_model("mockllm/model", base_url="https://example.com", foo="bar")
    )
    assert log.eval.model_base_url == "https://example.com"
    assert log.eval.model_args["foo"] == "bar"

================
File: tests/test_task_with.py
================
from test_helpers.tasks import minimal_task
from inspect_ai import task_with
def test_task_with_add_options():
    task = task_with(minimal_task(), time_limit=30)
    assert task.time_limit == 30
    assert task.metadata is not None
def test_task_with_remove_options():
    task = task_with(
        minimal_task(),
        scorer=None,
    )
    assert task.scorer is None
    assert task.metadata is not None
def test_task_with_edit_options():
    task = task_with(
        minimal_task(),
        metadata={"foo": "bar"},
    )
    assert task.metadata == {"foo": "bar"}
def test_task_with_name_option():
    task = task_with(minimal_task(), name="changed")
    assert task.name == "changed"

================
File: tests/tools/test_bash_tool.py
================
import pytest
from test_helpers.tasks import minimal_task_for_tool_use
from test_helpers.tool_call_utils import get_tool_call, get_tool_response
from test_helpers.utils import skip_if_no_docker
from inspect_ai import Task, eval
from inspect_ai.dataset import Sample
from inspect_ai.model import ModelOutput, get_model
from inspect_ai.scorer import includes
from inspect_ai.solver import generate, use_tools
from inspect_ai.tool import bash
@skip_if_no_docker
@pytest.mark.slow
def test_bash_simple_echo() -> None:
    task = minimal_task_for_tool_use(bash())
    result = eval(
        task,
        model=get_model(
            "mockllm/model",
            custom_outputs=[
                ModelOutput.for_tool_call(
                    model="mockllm/model",
                    tool_name=bash.__name__,
                    tool_arguments={"cmd": "echo 'testing bash tool'"},
                ),
            ],
        ),
    )[0]
    assert result.samples
    messages = result.samples[0].messages
    tool_call = get_tool_call(messages, bash.__name__)
    assert tool_call is not None
    tool_call_response = get_tool_response(messages, tool_call)
    assert tool_call_response is not None
    assert tool_call_response.content == "testing bash tool\n"
@skip_if_no_docker
@pytest.mark.slow
def test_bash_profile() -> None:
    tool_task = Task(
        dataset=[
            Sample(
                input="Please use the tool",
                target="n/a",
                files={
                    "/etc/profile.d/test_bash_profile.sh": "export ENV_VAR=custom_value\n"
                },
            )
        ],
        solver=[use_tools(bash()), generate()],
        scorer=includes(),
        metadata={"task_idx": 1},
        message_limit=3,
        sandbox="docker",
    )
    result = eval(
        tool_task,
        model=get_model(
            "mockllm/model",
            custom_outputs=[
                ModelOutput.for_tool_call(
                    model="mockllm/model",
                    tool_name=bash.__name__,
                    tool_arguments={"cmd": "echo $ENV_VAR"},
                ),
            ],
        ),
        sandbox_cleanup=False,
    )[0]
    assert result.samples
    messages = result.samples[0].messages
    tool_call = get_tool_call(messages, bash.__name__)
    assert tool_call is not None
    tool_call_response = get_tool_response(messages, tool_call)
    assert tool_call_response is not None
    assert tool_call_response.content == "custom_value\n"

================
File: tests/tools/test_max_exec_output.py
================
import pytest
from test_helpers.utils import skip_if_no_docker
from inspect_ai import Task, eval
from inspect_ai.dataset._dataset import Sample
from inspect_ai.model._generate_config import GenerateConfig
from inspect_ai.solver import Generate, Solver, TaskState, solver
from inspect_ai.util import sandbox
@solver
def big_output_solver() -> Solver:
    async def solve(state: TaskState, generate: Generate):
        await sandbox().exec(
            [
                "python",
                "-c",
                """#!/usr/bin/env python
import random
import string
foo_big = ""
for _ in range(1000):
    foo = ''.join(random.choices(string.ascii_letters + string.digits, k=1_000_000))
    # stream out another 1MB of data
    print(foo)
    # deliberately keep the big string in memory
    foo_big += foo
    print(len(foo_big))
            """,
            ]
        )
        state.completed = True
        return state
    return solve
# Run this test with a memory limit of around 200MB,
# e.g. systemd-run --user --scope -p MemoryMax=200M pytest tests/tools/test_max_exec_output.py --capture=no --runslow
@skip_if_no_docker
@pytest.mark.slow
def test_max_exec_output():
    task = Task(
        dataset=[
            Sample(
                input="Do whatever",
            )
        ],
        solver=big_output_solver(),
        config=GenerateConfig(max_tool_output=5),
    )
    log = eval(task, model="mockllm/model", sandbox="docker")[0]
    assert log.error

================
File: tests/tools/test_python_tool.py
================
import pytest
from test_helpers.tasks import minimal_task_for_tool_use
from test_helpers.tool_call_utils import get_tool_call, get_tool_response
from test_helpers.utils import skip_if_no_docker
from inspect_ai import eval
from inspect_ai.model import ModelOutput, get_model
from inspect_ai.tool import python
@skip_if_no_docker
@pytest.mark.slow
def test_python_simple_echo() -> None:
    task = minimal_task_for_tool_use(python())
    result = eval(
        task,
        model=get_model(
            "mockllm/model",
            custom_outputs=[
                ModelOutput.for_tool_call(
                    model="mockllm/model",
                    tool_name=python.__name__,
                    tool_arguments={"code": "print('testing python tool')"},
                ),
            ],
        ),
    )[0]
    assert result.samples
    messages = result.samples[0].messages
    tool_call = get_tool_call(messages, python.__name__)
    assert tool_call is not None
    tool_call_response = get_tool_response(messages, tool_call)
    assert tool_call_response is not None
    assert tool_call_response.content == "testing python tool\n"

================
File: tests/tools/test_sandbox_docker_and_local.py
================
from pathlib import Path
import pytest
from test_helpers.utils import skip_if_no_docker
from inspect_ai.util._sandbox.docker.docker import DockerSandboxEnvironment
from inspect_ai.util._sandbox.local import LocalSandboxEnvironment
from inspect_ai.util._sandbox.self_check import self_check
@skip_if_no_docker
@pytest.mark.slow
async def test_self_check_local(request) -> None:
    task_name = f"{__name__}_{request.node.name}_local"
    await LocalSandboxEnvironment.task_init(task_name=task_name, config=None)
    envs_dict = await LocalSandboxEnvironment.sample_init(
        task_name=task_name, config=None, metadata={}
    )
    known_failures = ["test_exec_as_user", "test_exec_as_nonexistent_user"]
    return await check_results_of_self_check(task_name, envs_dict, known_failures)
@skip_if_no_docker
@pytest.mark.slow
async def test_self_check_docker_custom_nonroot(request) -> None:
    task_name = f"{__name__}_{request.node.name}_docker_nonroot"
    # The default docker-compose used in Inspect uses the root user in the container.
    # The root user is allowed to overwrite files even if they're read-only.
    # This breaks the sematics of the sandbox, so we use a non-root user for these tests.
    config_file = str(Path(__file__) / ".." / "test_sandbox_compose.yaml")
    await DockerSandboxEnvironment.task_init(task_name=task_name, config=config_file)
    envs_dict = await DockerSandboxEnvironment.sample_init(
        task_name=task_name, config=config_file, metadata={}
    )
    return await check_results_of_self_check(task_name, envs_dict)
@skip_if_no_docker
@pytest.mark.slow
async def test_self_check_docker_custom_nonroot_alpine(request) -> None:
    task_name = f"{__name__}_{request.node.name}_docker_nonroot_alpine"
    # Alpine has busybox which is has a bunch of stuff "missing" if you are used to GNU.
    config_file = str(Path(__file__) / ".." / "test_sandbox_compose_alpine.yaml")
    await DockerSandboxEnvironment.task_init(task_name=task_name, config=config_file)
    envs_dict = await DockerSandboxEnvironment.sample_init(
        task_name=task_name, config=config_file, metadata={}
    )
    return await check_results_of_self_check(
        # alpine busybox is happy to overwrite a readonly file with cp
        task_name,
        envs_dict,
        [
            "test_write_text_file_without_permissions",
            "test_write_binary_file_without_permissions",
        ],
    )
@skip_if_no_docker
@pytest.mark.slow
async def test_self_check_docker_default_root(request) -> None:
    task_name = f"{__name__}_{request.node.name}_docker_root"
    await DockerSandboxEnvironment.task_init(task_name=task_name, config=None)
    envs_dict = await DockerSandboxEnvironment.sample_init(
        task_name=task_name, config=None, metadata={}
    )
    return await check_results_of_self_check(
        task_name,
        envs_dict,
        [
            "test_write_text_file_without_permissions",
            "test_write_binary_file_without_permissions",
        ],
    )
async def check_results_of_self_check(task_name, envs_dict, known_failures=[]):
    sandbox_env = envs_dict["default"]
    try:
        self_check_results = await self_check(sandbox_env)
        failures = []
        for test_name, result in self_check_results.items():
            if result is not True and test_name not in known_failures:
                failures.append(f"Test {test_name} failed: {result}")
        if failures:
            assert False, "\n".join(failures)
    finally:
        await sandbox_env.sample_cleanup(
            task_name=task_name,
            config=None,
            environments=envs_dict,
            interrupted=False,
        )
        await sandbox_env.task_cleanup(task_name=task_name, config=None, cleanup=True)

================
File: tests/tools/test_sandbox_dockerfile.py
================
import pytest
from pydantic import BaseModel
from test_helpers.utils import skip_if_no_docker
from inspect_ai import Task, eval
from inspect_ai.util._sandbox.environment import SandboxEnvironmentSpec
@skip_if_no_docker
@pytest.mark.slow
def test_sandbox_dockerfile():
    sandbox = SandboxEnvironmentSpec(
        "docker", "tests/tools/docker-compose-context/Dockerfile"
    )
    log = eval(Task(sandbox=sandbox))[0]
    assert log.status == "success"
    assert log.eval.sandbox == sandbox
@skip_if_no_docker
@pytest.mark.slow
def test_sandbox_dockerfile_raises_for_unsupported_config():
    class MyConfig(BaseModel, frozen=True):
        pass
    sandbox = SandboxEnvironmentSpec("docker", MyConfig())
    with pytest.raises(ValueError) as e_info:
        eval(Task(sandbox=sandbox))[0]
    assert "Unsupported config type" in str(e_info.value)

================
File: tests/tools/test_sandbox_tool_eval.py
================
import itertools
from pathlib import Path
from typing import Generator
import pytest
from test_helpers.tool_call_utils import get_tool_calls, get_tool_response
from test_helpers.tools import command_exec, list_files, read_file
from test_helpers.utils import skip_if_no_docker
from inspect_ai import Task, eval
from inspect_ai.dataset import Sample
from inspect_ai.model import ModelOutput, get_model
from inspect_ai.scorer import includes
from inspect_ai.solver import generate, use_tools
def find_tool_call(result, tool_call_id: str):
    messages = result.samples[0].messages
    return get_tool_response(messages, get_tool_calls(messages, tool_call_id)[0])
def test_sandbox_environment_read_file():
    dataset = [
        Sample(
            input="What are the contents of file foo.txt?",
            target="unused",
            files={"foo.txt": "contents_of_foo.txt"},
        )
    ]
    task = Task(
        dataset=dataset,
        solver=[use_tools([read_file(), list_files()]), generate()],
        scorer=includes(),
        sandbox="local",
    )
    result = eval(
        task,
        model=get_model(
            "mockllm/model",
            custom_outputs=[
                ModelOutput.for_tool_call(
                    model="mockllm/model",
                    tool_name="read_file",
                    tool_arguments={"file": "foo.txt"},
                ),
                ModelOutput.from_content(
                    model="mockllm/model", content="Contents of foo.txt."
                ),
            ],
        ),
        message_limit=5,  # otherwise we can get into an infinite loop if the tools error
    )[0]
    chat_message_tool = find_tool_call(result, "read_file")
    assert result.status == "success"
    assert chat_message_tool.text == "contents_of_foo.txt"
def test_sandbox_environment_list_files():
    dataset = [
        Sample(
            input="What files are there?",
            target="Hello",
            files={"bar.txt": "contents_of_bar.txt", "baz.txt": "contents_of_baz.txt"},
        )
    ]
    task = Task(
        dataset=dataset,
        solver=[use_tools([read_file(), list_files()]), generate()],
        scorer=includes(),
        sandbox="local",
    )
    result = eval(
        task,
        model=get_model(
            "mockllm/model",
            custom_outputs=[
                ModelOutput.for_tool_call(
                    model="mockllm/model",
                    tool_name="list_files",
                    tool_arguments={"dir": "."},
                ),
                ModelOutput.from_content(
                    model="mockllm/model", content="Lots of files!"
                ),
            ],
        ),
        message_limit=5,  # otherwise we can get into an infinite loop if the tools error
    )[0]
    chat_message_tool = find_tool_call(result, "list_files")
    assert result.status == "success"
    assert chat_message_tool.text == "bar.txt\nbaz.txt\n"
def test_sandbox_environment_read_file_error():
    dataset = [
        Sample(
            input="What are the contents of file nonexistent.txt?",
            target="unused",
        )
    ]
    task = Task(
        dataset=dataset,
        solver=[
            use_tools(
                [
                    read_file(),
                ]
            ),
            generate(),
        ],
        scorer=includes(),
        sandbox="local",
    )
    result = eval(
        task,
        model=get_model(
            "mockllm/model",
            custom_outputs=itertools.chain(
                [
                    ModelOutput.for_tool_call(
                        model="mockllm/model",
                        tool_name="read_file",
                        tool_arguments={"file": "nonexistent.txt"},
                    )
                ],
                (
                    ModelOutput.from_content(model="mockllm/model", content="finished")
                    for i in range(10)
                ),
            ),
        ),
        message_limit=5,  # otherwise we can get into an infinite loop if the tools error
    )[0]
    chat_message_tool = find_tool_call(result, "read_file")
    assert result.status == "success"
    assert chat_message_tool.error and "not found" in chat_message_tool.error.message
@skip_if_no_docker
@pytest.mark.slow
def test_sandbox_environment_nonroot_files():
    """Checks the file passed in as part of the Sample is actually readable by the Docker user in the container."""
    dataset = [
        Sample(
            input="What are the contents of file bar.txt?",
            files={"bar.txt": "hello"},
            target="unused",
        )
    ]
    task = Task(
        dataset=dataset,
        solver=[
            use_tools(
                [
                    command_exec(),
                    read_file(),
                ]
            ),
            generate(),
        ],
        scorer=includes(),
        sandbox=("docker", str(Path(__file__) / ".." / "test_sandbox_compose.yaml")),
    )
    def custom_outputs() -> Generator[ModelOutput, None, None]:
        yield ModelOutput.for_tool_call(
            model="mockllm/model",
            tool_name="command_exec",
            tool_arguments={"command": "id"},
        )
        yield ModelOutput.for_tool_call(
            model="mockllm/model",
            tool_name="read_file",
            tool_arguments={"file": "bar.txt"},
        )
        while True:
            yield ModelOutput.from_content(
                model="mockllm/model",
                content="finished",
            )
    result = eval(
        task,
        model=get_model(
            "mockllm/model",
            custom_outputs=custom_outputs(),
        ),
        message_limit=5,
        sandbox_cleanup=False,
    )[0]
    assert result.status == "success"
    id_chat_message_tool = find_tool_call(result, "command_exec")
    # the test is broken if it's ended up as the root user;
    # the point is to check the non-root user from the test_sandbox_compose config
    assert "uid=0(root)" not in id_chat_message_tool.text
    read_file_chat_message_tool = find_tool_call(result, "read_file")
    assert not read_file_chat_message_tool.error
    assert "hello" in read_file_chat_message_tool.text

================
File: tests/tools/test_tool_def.py
================
from inspect_ai import Task, eval
from inspect_ai.dataset import Sample
from inspect_ai.model import ModelOutput, get_model
from inspect_ai.scorer import match
from inspect_ai.solver import generate, use_tools
from inspect_ai.tool import ToolDef
def test_tool_def() -> None:
    model = get_model(
        "mockllm/model",
        custom_outputs=[
            ModelOutput.for_tool_call(
                "mockllm/model",
                tool_name="addition2",
                tool_arguments={"x": 1, "y": 1},
            ),
            ModelOutput.from_content("mockllm/model", content="2"),
        ],
    )
    async def addition(x: int, y: int):
        return x + y
    addition_tool = ToolDef(
        tool=addition,
        name="addition2",
        description="Add two numbers",
        parameters={"x": "Integer", "y": "Integer"},
    )
    task = Task(
        dataset=[Sample(input="What is 1 + 1?", target="2")],
        solver=[use_tools(addition_tool.as_tool()), generate()],
        scorer=match(numeric=True),
    )
    log = eval(task, model=model)[0]
    assert log.status == "success"

================
File: tests/tools/test_tool_images.py
================
from test_helpers.utils import (
    skip_if_no_anthropic,
    skip_if_no_google,
    skip_if_no_openai,
    skip_if_trio,
)
from inspect_ai import Task, eval, task
from inspect_ai._util.constants import PKG_PATH
from inspect_ai._util.images import file_as_data_uri
from inspect_ai.dataset import Sample
from inspect_ai.scorer import includes
from inspect_ai.solver import generate, use_tools
from inspect_ai.tool import ContentImage, tool
IMAGES_PATH = PKG_PATH / ".." / ".." / "tests" / "dataset" / "test_dataset" / "images"
@tool
def camera():
    async def execute() -> ContentImage:
        """
        Take a picture of the environment.
        Returns:
            Image with a picture of the environment
        """
        ballons = (IMAGES_PATH / "ballons.png").as_posix()
        return ContentImage(image=await file_as_data_uri(ballons))
    return execute
@task
def camera_task():
    return Task(
        dataset=[
            Sample(
                input="Use the 'camera' tool to take a picture of the environment. What do you see?",
                target="balloons",
            )
        ],
        solver=[use_tools(camera()), generate()],
        scorer=includes(),
    )
@skip_if_no_openai
def test_openai_tool_image_result():
    check_tool_image_result("openai/gpt-4o")
@skip_if_no_google
@skip_if_trio
def test_google_tool_image_result():
    check_tool_image_result("google/gemini-1.5-pro")
@skip_if_no_anthropic
def test_anthropic_tool_image_result():
    check_tool_image_result("anthropic/claude-3-5-sonnet-20240620")
def check_tool_image_result(model: str):
    log = eval(camera_task(), model=model)[0]
    assert log.status == "success"
    assert log.samples
    assert log.samples[0].scores
    assert log.samples[0].scores["includes"].as_str() == "C"

================
File: tests/tools/test_tool_parse.py
================
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, TypedDict, Union
from pydantic import BaseModel, Field
from inspect_ai.tool._tool_info import (
    parse_docstring,
    parse_tool_info,
)
from inspect_ai.util._json import JSONSchema, cls_json_schema, json_schema
# Test helper functions
def test_json_schema():
    assert json_schema(int) == JSONSchema(type="integer")
    assert json_schema(float) == JSONSchema(type="number")
    assert json_schema(str) == JSONSchema(type="string")
    assert json_schema(bool) == JSONSchema(type="boolean")
    assert json_schema(Any) == JSONSchema()
    assert json_schema(List[int]) == JSONSchema(
        type="array", items=JSONSchema(type="integer")
    )
    assert json_schema(Dict[str, int]) == JSONSchema(
        type="object", additionalProperties=JSONSchema(type="integer")
    )
    assert json_schema(Optional[str]) == JSONSchema(
        anyOf=[JSONSchema(type="string"), JSONSchema(type="null")]
    )
    assert json_schema(Union[int, str]) == JSONSchema(
        anyOf=[JSONSchema(type="integer"), JSONSchema(type="string")]
    )
def test_cls_json_schema() -> None:
    @dataclass
    class TestDataclass:
        field1: int
        field2: str = "default"
    class TestPydantic(BaseModel):
        field1: int
        field2: str = Field(default="default")
    dataclass_result = cls_json_schema(TestDataclass)
    assert dataclass_result.type == "object"
    assert dataclass_result.properties
    assert "field1" in dataclass_result.properties
    assert "field2" in dataclass_result.properties
    assert dataclass_result.required == ["field1"]
    pydantic_result = cls_json_schema(TestPydantic)
    assert pydantic_result.type == "object"
    assert pydantic_result.properties
    assert "field1" in pydantic_result.properties
    assert "field2" in pydantic_result.properties
    assert pydantic_result.required == ["field1"]
def test_parse_docstring():
    docstring = """
    This is a test function.
    Args:
        param1 (int): An integer parameter
        param2 (str): A string parameter
    Returns:
        bool: A boolean result
    """
    result = parse_docstring(docstring, "param1")
    assert result == {"description": "An integer parameter", "docstring_type": "int"}
    result = parse_docstring(docstring, "param3")
    assert result == {}
# Main test cases
def test_simple_function():
    def simple_func(a: int, b: str = "default") -> bool:
        """
        A simple function.
        Args:
            a (int): An integer parameter
            b (str): A string parameter
        Returns:
            bool: A boolean result
        """
        return True
    info = parse_tool_info(simple_func)
    assert info.name == "simple_func"
    assert info.description == "A simple function."
    assert len(info.parameters.properties) == 2
    assert info.parameters.properties["a"].type == "integer"
    assert info.parameters.properties["b"].type == "string"
    assert info.parameters.properties["b"].default == "default"
    assert info.parameters.required == ["a"]
def test_complex_types():
    def complex_func(
        a: List[int], b: Dict[str, Any], c: Optional[Union[int, str]] = None
    ):
        """A function with complex types."""
        pass
    info = parse_tool_info(complex_func)
    assert info.parameters.properties["a"].type == "array"
    assert info.parameters.properties["a"].items.type == "integer"
    assert info.parameters.properties["b"].type == "object"
    assert info.parameters.properties["c"].anyOf is not None
    assert len(info.parameters.properties["c"].anyOf) == 3  # int, str, null
def test_dataclass_parameter() -> None:
    @dataclass
    class TestDataclass:
        field1: int
        field2: str = "default"
    def dataclass_func(data: TestDataclass):
        """A function with a dataclass parameter."""
        pass
    info = parse_tool_info(dataclass_func)
    assert info.parameters.properties["data"].type == "object"
    assert info.parameters.properties["data"].properties
    assert "field1" in info.parameters.properties["data"].properties
    assert "field2" in info.parameters.properties["data"].properties
def test_pydantic_parameter() -> None:
    class TestPydantic(BaseModel):
        field1: int
        field2: str = Field(default="default")
    def pydantic_func(data: TestPydantic):
        """A function with a Pydantic parameter."""
        pass
    info = parse_tool_info(pydantic_func)
    assert info.parameters.properties["data"].type == "object"
    assert info.parameters.properties["data"].properties
    assert "field1" in info.parameters.properties["data"].properties
    assert "field2" in info.parameters.properties["data"].properties
def test_nested_types():
    def nested_func(a: List[Dict[str, Union[int, str]]]):
        """A function with nested types."""
        pass
    info = parse_tool_info(nested_func)
    assert info.parameters.properties["a"].type == "array"
    assert info.parameters.properties["a"].items.type == "object"
    assert info.parameters.properties["a"].items.additionalProperties.anyOf is not None
def test_no_type_hints():
    def no_hints(a, b="default"):
        """
        A function without type hints.
        Args:
            a: First parameter
            b: Second parameter
        """
        pass
    info = parse_tool_info(no_hints)
    assert "a" in info.parameters.properties
    assert "b" in info.parameters.properties
    assert info.parameters.properties["b"].default == "default"
def test_no_docstring():
    def no_docstring(a: int, b: str):
        pass
    info = parse_tool_info(no_docstring)
    assert info.description == ""
    assert "a" in info.parameters.properties
    assert "b" in info.parameters.properties
def test_custom_objects():
    class CustomObject:
        pass
    def custom_func(obj: CustomObject):
        """A function with a custom object parameter."""
        pass
    info = parse_tool_info(custom_func)
    assert "obj" in info.parameters.properties
def test_function_with_args_kwargs():
    def func_with_args_kwargs(*args: int, **kwargs: str):
        """A function with *args and **kwargs."""
        pass
    info = parse_tool_info(func_with_args_kwargs)
    assert "args" in info.parameters.properties
    assert "kwargs" in info.parameters.properties
def test_list_of_dicts():
    def func_with_list_of_dicts(data: List[Dict[str, int]]):
        """A function with a list of dictionaries parameter."""
        pass
    info = parse_tool_info(func_with_list_of_dicts)
    assert info.parameters.properties["data"].type == "array"
    assert info.parameters.properties["data"].items.type == "object"
    assert (
        info.parameters.properties["data"].items.additionalProperties.type == "integer"
    )
def test_list_of_dataclasses() -> None:
    @dataclass
    class Item:
        name: str
        value: int
    def func_with_list_of_dataclasses(items: List[Item]):
        """A function with a list of dataclasses parameter."""
        pass
    info = parse_tool_info(func_with_list_of_dataclasses)
    assert info.parameters.properties["items"].type == "array"
    assert info.parameters.properties["items"].items
    assert info.parameters.properties["items"].items.type == "object"
    assert info.parameters.properties["items"].items.properties
    assert "name" in info.parameters.properties["items"].items.properties
    assert "value" in info.parameters.properties["items"].items.properties
    assert info.parameters.properties["items"].items.properties["name"].type == "string"
    assert (
        info.parameters.properties["items"].items.properties["value"].type == "integer"
    )
def test_list_of_pydantic_models() -> None:
    class Product(BaseModel):
        id: int
        name: str
        price: float
    def func_with_list_of_pydantic_models(products: List[Product]):
        """A function with a list of Pydantic models parameter."""
        pass
    info = parse_tool_info(func_with_list_of_pydantic_models)
    assert info.parameters.properties["products"].type == "array"
    assert info.parameters.properties["products"].items
    assert info.parameters.properties["products"].items.type == "object"
    assert info.parameters.properties["products"].items.properties
    assert "id" in info.parameters.properties["products"].items.properties
    assert "name" in info.parameters.properties["products"].items.properties
    assert "price" in info.parameters.properties["products"].items.properties
    assert (
        info.parameters.properties["products"].items.properties["id"].type == "integer"
    )
    assert (
        info.parameters.properties["products"].items.properties["name"].type == "string"
    )
    assert (
        info.parameters.properties["products"].items.properties["price"].type
        == "number"
    )
def test_nested_list_of_dicts():
    def func_with_nested_list_of_dicts(data: List[List[Dict[str, Union[int, str]]]]):
        """A function with a nested list of dictionaries parameter."""
        pass
    info = parse_tool_info(func_with_nested_list_of_dicts)
    assert info.parameters.properties["data"].type == "array"
    assert info.parameters.properties["data"].items.type == "array"
    assert info.parameters.properties["data"].items.items.type == "object"
    assert (
        info.parameters.properties["data"].items.items.additionalProperties.anyOf
        is not None
    )
    assert (
        len(info.parameters.properties["data"].items.items.additionalProperties.anyOf)
        == 2
    )
def test_dict_with_list_of_pydantic_models() -> None:
    class User(BaseModel):
        id: int
        name: str
    def func_with_dict_list_pydantic(data: Dict[str, List[User]]):
        """A function with a dictionary containing lists of Pydantic models."""
        pass
    info = parse_tool_info(func_with_dict_list_pydantic)
    assert info.parameters.properties["data"].type == "object"
    assert isinstance(
        info.parameters.properties["data"].additionalProperties, JSONSchema
    )
    assert info.parameters.properties["data"].additionalProperties.type == "array"
    assert info.parameters.properties["data"].additionalProperties.items
    assert (
        info.parameters.properties["data"].additionalProperties.items.type == "object"
    )
    assert info.parameters.properties["data"].additionalProperties.items.properties
    assert (
        "id" in info.parameters.properties["data"].additionalProperties.items.properties
    )
    assert (
        "name"
        in info.parameters.properties["data"].additionalProperties.items.properties
    )
def test_simple_typeddict() -> None:
    class SimpleDict(TypedDict):
        name: str
        age: int
    def example_function(data: SimpleDict) -> None:
        pass
    tool_info = parse_tool_info(example_function)
    assert tool_info.name == "example_function"
    assert tool_info.parameters.type == "object"
    assert len(tool_info.parameters.properties) == 1
    data_param = tool_info.parameters.properties["data"]
    assert data_param.type == "object"
    assert data_param.properties
    assert len(data_param.properties) == 2
    assert data_param.properties["name"].type == "string"
    assert data_param.properties["age"].type == "integer"
    assert data_param.required
    assert set(data_param.required) == {"name", "age"}
def test_nested_typeddict() -> None:
    class Address(TypedDict):
        street: str
        city: str
    class Person(TypedDict):
        name: str
        address: Address
    def example_function(person: Person) -> None:
        pass
    tool_info = parse_tool_info(example_function)
    person_param = tool_info.parameters.properties["person"]
    assert person_param.type == "object"
    assert person_param.properties
    assert len(person_param.properties) == 2
    address_prop = person_param.properties["address"]
    assert address_prop.type == "object"
    assert address_prop.properties
    assert len(address_prop.properties) == 2
    assert address_prop.properties["street"].type == "string"
    assert address_prop.properties["city"].type == "string"
def test_optional_fields() -> None:
    class OptionalDict(TypedDict, total=False):
        name: str
        age: Optional[int]
    def example_function(data: OptionalDict) -> None:
        pass
    tool_info = parse_tool_info(example_function)
    data_param = tool_info.parameters.properties["data"]
    assert data_param.type == "object"
    assert data_param.properties
    assert len(data_param.properties) == 2
    assert not data_param.required  # No required fields
    age_prop = data_param.properties["age"]
    assert age_prop.anyOf
    assert len(age_prop.anyOf) == 2
    assert any(prop.type == "integer" for prop in age_prop.anyOf)
    assert any(prop.type == "null" for prop in age_prop.anyOf)

================
File: tests/tools/test_tool_truncate.py
================
from inspect_ai.model._call_tools import truncate_tool_output
def test_tool_truncate():
    long_tool_output = "TEST\n" * 100_000
    truncated_output = truncate_tool_output(
        tool_name="test", output=long_tool_output, max_output=1000
    )
    # Output should not contain leading/trailing whitespace on any line.
    assert all(line.strip() == line for line in truncated_output.output.split("\n"))

================
File: tests/tools/test_tool_types.py
================
from dataclasses import dataclass
from typing import TypedDict
import numpy as np
from pydantic import BaseModel
from test_helpers.utils import (
    skip_if_no_anthropic,
    skip_if_no_google,
    skip_if_no_openai,
    skip_if_no_vertex,
    skip_if_trio,
)
from inspect_ai import Task, eval
from inspect_ai.dataset import MemoryDataset, Sample
from inspect_ai.log._log import EvalLog
from inspect_ai.model import ChatMessageTool
from inspect_ai.solver import generate, use_tools
from inspect_ai.tool import ToolFunction, tool
from inspect_ai.tool._tool import Tool
@tool
def mean():
    async def execute(numbers: list[float]) -> float:
        """Take the mean of a set of numbers.
        Args:
          numbers: A list of integers to take the mean of
        Returns:
          The mean of the numbers
        """
        return np.mean(numbers).item()
    return execute
class Point(TypedDict):
    x: int
    y: int
@tool
def offset():
    async def execute(point: Point, offset: int) -> str:
        """
        Offset a point by the specified offset value
        Args:
          point: Point to offset
          offset: Offset value
        Returns:
          A Point with the x and y values offset
        """
        return str(Point(x=point["x"] + offset, y=point["y"] + offset))
    return execute
@dataclass
class PointDataclass:
    x: int
    y: int
@tool
def offset_dataclass():
    async def execute(point: PointDataclass, offset: int):
        """
        Offset a point by the specified offset value
        Args:
          point: Point to offset
          offset: Offset value
        Returns:
          A Point with the x and y values offset
        """
        return str(PointDataclass(x=point.x + offset, y=point.y + offset))
    return execute
class Word(BaseModel):
    type: str
    word: str
@tool
def extract_words():
    async def execute(extracted: list[Word]):
        """
        Accepts the extracted nouns and adjectives from a sentence
        Args:
          extracted: A list of Word objects each with a type and word.
        Returns:
          The words and their types in a list
        """
        return ", ".join([f"{x.word}: {x.type}" for x in extracted])
    return execute
@tool
def sound_check():
    async def execute(sound: str, extra: str = "stuff"):
        """
        Accepts the extracted nouns and adjectives from a sentence
        Args:
          sound: The sound to check
          extra: Optional extra stuff
        Returns:
          The sound that was passed to check.
        """
        return f"{sound} ({extra})"
    return execute
@tool
def computer_action():
    async def execute(
        action: str,
        text: str | None = None,
        coordinate: tuple[int, int] | None = None,
    ) -> str:
        """Take an action using a computer.
        Args:
          action: Action to take.
          text: Text related to the action
          coordinate: Coordinate related to the action.
        Returns:
          The sound that was passed to check.
        """
        return action
    return execute
def check_point(model: str, tool: Tool, function_name: str) -> None:
    task = Task(
        dataset=MemoryDataset(
            [
                Sample(
                    input="Start with the point x=10, y=10 then offset it by 5.",
                )
            ]
        ),
        solver=[
            use_tools([tool], tool_choice=ToolFunction(function_name)),
            generate(),
        ],
    )
    log = eval(task, model=model)[0]
    verify_tool_call(log, "15")
def check_typed_dict(model: str) -> None:
    check_point(model, offset(), "offset")
def check_dataclass(model: str) -> None:
    check_point(model, offset_dataclass(), "offset_dataclass")
def check_list_of_numbers(model: str) -> None:
    task = Task(
        dataset=MemoryDataset(
            [
                Sample(
                    input="Take the mean of the following numbers: 5, 10, 15",
                    target="10",
                )
            ]
        ),
        solver=[
            use_tools([mean()], tool_choice=ToolFunction("mean")),
            generate(),
        ],
    )
    log = eval(task, model=model)[0]
    verify_tool_call(log, "10")
def check_list_of_objects(model: str) -> None:
    # grok sometimes doesn't get this one (just says 'I have extracted, how would you like to proceed')
    if "grok" in model:
        return
    task = Task(
        dataset=MemoryDataset(
            [
                Sample(
                    input="Extract the nouns and adjectives from the following sentence.\nSentence:\nThe quick brown fox jumped over the lazy dog."
                )
            ]
        ),
        solver=[
            use_tools([extract_words()], tool_choice=ToolFunction("extract_words")),
            generate(),
        ],
    )
    log = eval(task, model=model)[0]
    verify_tool_call(log, "quick")
def check_optional_args(model: str) -> None:
    task = Task(
        dataset=MemoryDataset(
            [
                Sample(
                    input="Please call the sound_check tool with single argument 'boo' and report its output."
                )
            ]
        ),
        solver=[
            use_tools([sound_check()], tool_choice=ToolFunction("sound_check")),
            generate(),
        ],
    )
    log = eval(task, model=model)[0]
    verify_tool_call(log, "stuff")
def check_none_default_arg(model: str) -> None:
    task = Task(
        dataset=MemoryDataset(
            [
                Sample(
                    input="Please call the computer_action function with the action='click' argument."
                )
            ]
        ),
        solver=[
            use_tools([computer_action()], tool_choice=ToolFunction("computer_action")),
            generate(),
        ],
    )
    log = eval(task, model=model)[0]
    verify_tool_call(log, "click")
def check_tool_types(model: str):
    check_typed_dict(model)
    check_dataclass(model)
    check_list_of_numbers(model)
    check_list_of_objects(model)
    check_optional_args(model)
    check_none_default_arg(model)
@skip_if_no_openai
def test_openai_tool_types() -> None:
    check_tool_types("openai/gpt-4o")
@skip_if_no_anthropic
def test_anthropoic_tool_types() -> None:
    check_tool_types("anthropic/claude-3-5-sonnet-20240620")
@skip_if_no_google
@skip_if_trio
def test_google_tool_types() -> None:
    check_tool_types("google/gemini-1.5-pro")
@skip_if_no_vertex
def test_vertex_tool_types():
    check_tool_types("vertex/gemini-1.5-flash")
# mistral, grok, and groq tool calling is extremely unreliable and
# consequently cause failed tests that are red herrings. don't
# exercise these for now.
# @skip_if_no_mistral
# def test_mistral_tool_types() -> None:
#     check_tool_types("mistral/mistral-large-latest")
# @skip_if_no_grok
# def test_grok_tool_types() -> None:
#     check_tool_types("grok/grok-beta")
# @skip_if_no_groq
# def test_groq_tool_types() -> None:
#     check_tool_types("groq/mixtral-8x7b-32768")
def verify_tool_call(log: EvalLog, includes: str):
    assert log.samples
    tool_message = log.samples[0].messages[-2]
    assert isinstance(tool_message, ChatMessageTool)
    assert includes in log.samples[0].output.completion

================
File: tests/tools/test_tool_view.py
================
import pytest
from test_helpers.tool_call_utils import get_tool_event
from test_helpers.utils import skip_if_no_docker
from inspect_ai import Task, eval
from inspect_ai.dataset import Sample
from inspect_ai.model import ModelOutput, get_model
from inspect_ai.solver import generate, use_tools
from inspect_ai.tool import bash
@skip_if_no_docker
@pytest.mark.slow
def test_tool_view():
    task = Task(
        dataset=[
            Sample(
                input="Please use the bash tool to list the files in the current directory?"
            )
        ],
        solver=[use_tools(bash()), generate()],
        sandbox="docker",
    )
    log = eval(
        task,
        model=get_model(
            "mockllm/model",
            custom_outputs=[
                ModelOutput.for_tool_call(
                    model="mockllm/model",
                    tool_name="bash",
                    tool_arguments={"code": "ls ."},
                ),
                ModelOutput.from_content(model="mockllm/model", content="All done!."),
            ],
        ),
    )[0]
    event = get_tool_event(log)
    assert event.view is not None

================
File: tests/tools/test_tool_with.py
================
from test_helpers.tools import addition
from inspect_ai.tool import ToolDef, tool_with
NAME = "my_addition"
DESCRIPTION = "my description"
X_DESCRIPTION = "my x description"
Y_DESCRIPTION = "my y description"
def test_tool_with():
    tool = tool_with(
        addition(),
        name=NAME,
        description=DESCRIPTION,
        parameters={"x": X_DESCRIPTION, "y": Y_DESCRIPTION},
    )
    tdef = ToolDef(tool)
    assert tdef.name == NAME
    assert tdef.description == DESCRIPTION
    assert tdef.parameters.properties["x"].description == X_DESCRIPTION
    assert tdef.parameters.properties["y"].description == Y_DESCRIPTION
def test_tool_with_validation():
    try:
        tool_with(
            addition(),
            parameters={"p": X_DESCRIPTION, "q": Y_DESCRIPTION},
        )
        assert False
    except Exception:
        pass

================
File: tests/tools/test_tools.py
================
from random import randint
from typing import Literal
from test_helpers.tool_call_utils import (
    get_tool_call,
    get_tool_calls,
    get_tool_response,
)
from test_helpers.tools import addition, raise_error, read_file
from test_helpers.utils import (
    skip_if_no_anthropic,
    skip_if_no_google,
    skip_if_no_mistral,
    skip_if_no_openai,
    skip_if_no_vertex,
    skip_if_trio,
)
from inspect_ai import Task, eval
from inspect_ai.dataset import Sample
from inspect_ai.log import EvalLog
from inspect_ai.model import (
    ChatMessageUser,
    Model,
    get_model,
)
from inspect_ai.model._model_output import ModelOutput
from inspect_ai.scorer import match
from inspect_ai.solver import (
    Generate,
    TaskState,
    generate,
    solver,
    system_message,
    use_tools,
)
from inspect_ai.tool import ToolFunction, tool
# we define 3 versions of addition so we can test the ability to force the
# the model to use a certain tool via tool_choice=ToolFunction()
# define some other tools to test forcing tool usage
@tool
def addition2():
    async def add(x: int, y: int):
        """
        Add two numbers.
        Args:
            x (int): First number to add.
            y (int): Second number to add.
        Returns:
            The sum of the two numbers.
        """
        return x + y
    return add
# define some other tools to test forcing tool usage
@tool
def addition3():
    async def add(x: int, y: int):
        """
        Add two numbers.
        Args:
            x (int): First number to add.
            y (int): Second number to add.
        Returns:
            The sum of the two numbers.
        """
        return x + y
    return add
def check_tools(
    model: Model, disable: list[Literal["calls", "force", "none"]] = []
) -> None:
    if "calls" not in disable:
        check_tools_calls(model)
    if "force" not in disable:
        check_tools_force(model)
    if "none" not in disable:
        check_tools_none(model)
addition_dataset = [
    Sample(
        input="What is 1 + 1?", target=["2", "2.0", "Two"], metadata={"color": "red"}
    )
]
def check_tools_calls(model: Model, **model_args) -> None:
    model = get_model(model)
    task = Task(
        dataset=addition_dataset,
        solver=[use_tools(addition()), generate()],
        scorer=match("any", numeric=True),
    )
    # evaluate the task
    log: list[EvalLog] = eval(task, model=model, model_args=model_args)
    # check that we got the answer right
    assert log[0].results and log[0].results.scores[0].metrics["accuracy"].value == 1
    # check that there is a tool_call
    assert log[0].samples
    messages = log[0].samples[0].messages
    tool_call = get_tool_call(messages, "addition")
    assert tool_call
    # check that there is a tool response for this call
    assert get_tool_response(messages, tool_call)
def check_tools_none(model: Model, **model_args) -> None:
    model = get_model(model)
    task = Task(
        dataset=addition_dataset,
        solver=[use_tools(addition(), tool_choice="none"), generate()],
        scorer=match(),
    )
    # evaluate the task
    log: list[EvalLog] = eval(task, model=model, model_args=model_args)
    # confirm no tool calls
    assert log[0].samples
    messages = log[0].samples[0].messages
    tool_call = get_tool_call(messages, "addition")
    assert tool_call is None
def check_tools_force(model: Model, **model_args) -> None:
    model = get_model(model)
    task = Task(
        dataset=addition_dataset,
        solver=[
            use_tools(
                [addition(), addition2(), addition3()],
                tool_choice=ToolFunction(name="addition2"),
            ),
            generate(),
        ],
        scorer=match(),
    )
    # evaluate the task
    log: list[EvalLog] = eval(task, model=model, model_args=model_args)
    # confirm we called the right tool
    assert log[0].samples
    messages = log[0].samples[0].messages
    tool_call = get_tool_call(messages, "addition2")
    assert tool_call is not None and tool_call.function == "addition2"
@skip_if_no_openai
def test_openai_tools():
    check_tools("openai/gpt-4")
@skip_if_no_anthropic
def test_anthropic_tools():
    check_tools("anthropic/claude-3-sonnet-20240229", disable=["none"])
@skip_if_no_mistral
def test_mistral_tools():
    check_tools("mistral/mistral-large-latest")
# groq tool calling is extremely unreliable and consequently causes
# failed tests that are red herrings. don't exercise this for now.
# @skip_if_no_groq
# def test_groq_tools():
#     check_tools("groq/mixtral-8x7b-32768")
@skip_if_no_google
@skip_if_trio
def test_google_tools():
    check_tools("google/gemini-1.5-pro")
@skip_if_no_vertex
def test_vertex_tools():
    check_tools("vertex/gemini-1.5-flash")
def test_dynamic_tools():
    @tool
    def color():
        async def execute():
            """Pick a random color.
            Returns:
              Color
            """
            return ["green", "red", "blue"][randint(0, 2)]
        return execute
    @tool
    def shape():
        async def execute():
            """Pick a random shape.
            Returns:
               Shape
            """
            return ["triangle", "circle", "square"][randint(0, 2)]
        return execute
    @solver
    def dynamic_tools():
        async def solve(state: TaskState, generate: Generate):
            state.tools = [color()]
            state = await generate(state)
            state.messages.append(ChatMessageUser(content="Pick a random shape."))
            state.tools = [shape()]
            return await generate(state)
        return solve
    task = Task(
        dataset=[Sample(input="Pick a random color.")],
        solver=dynamic_tools(),
        scorer=match(),
    )
    model = get_model(
        "mockllm/model",
        custom_outputs=[
            ModelOutput.for_tool_call(
                model="mockllm/model",
                tool_name="color",
                tool_arguments={},
            ),
            ModelOutput.from_content(model="mockllm/model", content="color all set."),
            ModelOutput.for_tool_call(
                model="mockllm/model",
                tool_name="shape",
                tool_arguments={},
            ),
            ModelOutput.from_content(model="mockllm/model", content="shape all set."),
        ],
    )
    log = eval(task, model=model)[0]
    assert log.samples
    messages = log.samples[0].messages
    tool_call = get_tool_call(messages, "color")
    assert tool_call is not None and tool_call.function == "color"
    messages.reverse()
    tool_call = get_tool_call(messages, "shape")
    assert tool_call is not None and tool_call.function == "shape"
def test_tool_error():
    task = Task(
        dataset=[Sample(input="Please read the file 'foo.txt'")],
        solver=[use_tools([read_file()]), generate()],
        scorer=match(),
        sandbox="local",
    )
    model = get_model(
        "mockllm/model",
        custom_outputs=[
            ModelOutput.for_tool_call(
                model="mockllm/model",
                tool_name="read_file",
                tool_arguments={"file": "foo.txt"},
            ),
            ModelOutput.from_content(model="mockllm/model", content="All done."),
        ],
    )
    log = eval(task, model=model)[0]
    assert log.status == "success"
    assert log.samples
    messages = log.samples[0].messages
    tool_call = get_tool_call(messages, "read_file")
    assert tool_call
    response = get_tool_response(messages, tool_call)
    assert response
    assert response.error
def test_tool_eval_error():
    task = Task(
        dataset=[Sample(input="Please raise an error.")],
        solver=[use_tools([raise_error()]), generate()],
        scorer=match(),
        sandbox="local",
    )
    model = get_model(
        "mockllm/model",
        custom_outputs=[
            ModelOutput.for_tool_call(
                model="mockllm/model",
                tool_name="raise_error",
                tool_arguments={},
            ),
            ModelOutput.from_content(model="mockllm/model", content="All done."),
        ],
    )
    log = eval(task, model=model)[0]
    assert log.status == "error"
def test_tool_calls():
    @tool
    def add():
        async def exec(x: int, y: int):
            """Add two numbers.
            When using this tool, be sure to call it at least
            three consecutive times to ensure the correct result.
            If you fail to call it three times and only call it
            one time it will give an incorrect result.
            Args:
                x (int): First number to add.
                y (int): Second number to add.
            Returns:
                The sum of the two numbers.
            """
            return x + y
        return exec
    def task(tool_calls: Literal["loop", "single", "none"]):
        return Task(
            dataset=[Sample(input="What is 1+1?", target="2")],
            solver=[
                system_message(
                    "When using the add tool, be sure to call it at least three consecutive times to ensure the correct result."
                ),
                use_tools([add()]),
                generate(tool_calls=tool_calls),
            ],
            scorer=match(),
        )
    def check_messages(log, predicate):
        assert log.status == "success"
        assert log.samples
        messages = log.samples[0].messages
        assert predicate(messages)
    def mockllm_model() -> Model:
        return get_model(
            "mockllm/model",
            custom_outputs=[
                ModelOutput.for_tool_call(
                    model="mockllm/model",
                    tool_name="add",
                    tool_arguments={"x": 1, "y": 1},
                )
            ]
            * 3
            + [ModelOutput.from_content(model="mockllm/model", content="All done.")],
        )
    # tool_calls == "loop"
    log = eval(task("loop"), model=mockllm_model())[0]
    check_messages(log, lambda m: len(get_tool_calls(m, "add")) > 1)
    # tool_calls == "single"
    log = eval(task("single"), model=mockllm_model())[0]
    check_messages(log, lambda m: len(get_tool_calls(m, "add")) == 1)
    # tool_calls == "none"
    log = eval(task("none"), model=mockllm_model())[0]
    check_messages(log, lambda m: get_tool_response(m, "add") is None)

================
File: tests/tools/test_use_tools.py
================
from typing import Literal
import pytest
from test_helpers.tools import addition, list_files, read_file
from test_helpers.utils import simple_task_state
from typing_extensions import Unpack
from inspect_ai.model import CachePolicy, GenerateConfigArgs
from inspect_ai.solver import TaskState, use_tools
def null_generate(
    state: TaskState,
    tool_calls: Literal["loop", "single", "none"] = "loop",
    cache: bool | CachePolicy = False,
    **kwargs: Unpack[GenerateConfigArgs],
) -> TaskState:
    return state
@pytest.mark.anyio
async def test_use_tools():
    state = simple_task_state()
    addition_tool = addition()
    read_file_tool = read_file()
    state = await (use_tools([addition_tool, read_file_tool]))(state, null_generate)
    assert state.tools == [addition_tool, read_file_tool]
    state = await (use_tools([addition_tool, read_file_tool]))(state, null_generate)
    assert state.tools == [addition_tool, read_file_tool]
    state = await (use_tools(addition_tool, read_file_tool))(state, null_generate)
    assert state.tools == [addition_tool, read_file_tool]
    state = await (use_tools([addition_tool]))(state, null_generate)
    assert state.tools == [addition_tool]
    state = await (use_tools(addition_tool))(state, null_generate)
    assert state.tools == [addition_tool]
    state = await (use_tools(tool_choice="auto"))(state, null_generate)
    assert state.tools == [addition_tool]
    assert state.tool_choice == "auto"
@pytest.mark.anyio
async def test_use_tools_append():
    state = simple_task_state()
    addition_tool = addition()
    read_file_tool = read_file()
    list_files_tool = list_files()
    state = await (use_tools([addition_tool, read_file_tool]))(state, null_generate)
    assert state.tools == [addition_tool, read_file_tool]
    # append to the tools
    state = await (use_tools([list_files_tool], append=True))(state, null_generate)
    assert state.tools == [addition_tool, read_file_tool, list_files_tool]
    # now replace the tools
    state = await (use_tools([addition_tool]))(state, null_generate)
    assert state.tools == [addition_tool]

================
File: tests/tools/test_web_browser.py
================
from pathlib import Path
from typing import Literal
import pytest
from test_helpers.tool_call_utils import get_tool_call, get_tool_response
from test_helpers.utils import skip_if_no_docker, skip_if_no_openai
from inspect_ai import Task, eval
from inspect_ai.dataset import Sample
from inspect_ai.model import ModelOutput, get_model
from inspect_ai.solver import generate, use_tools
from inspect_ai.tool import web_browser
@skip_if_no_docker
@pytest.mark.slow
def test_web_browser_navigation():
    task = Task(
        dataset=[Sample(input="Please use the web_browser tool")],
        solver=[use_tools(web_browser()), generate()],
        sandbox=web_browser_sandbox(),
    )
    log = eval(
        task,
        model=get_model(
            "mockllm/model",
            custom_outputs=[
                ModelOutput.for_tool_call(
                    model="mockllm/model",
                    tool_name="web_browser_go",
                    tool_arguments={
                        "url": "https://github.com/UKGovernmentBEIS/inspect_ai"
                    },
                ),
                ModelOutput.for_tool_call(
                    model="mockllm/model",
                    tool_name="web_browser_scroll",
                    tool_arguments={"direction": "down"},
                ),
                ModelOutput.for_tool_call(
                    model="mockllm/model",
                    tool_name="web_browser_scroll",
                    tool_arguments={"direction": "up"},
                ),
                ModelOutput.for_tool_call(
                    model="mockllm/model",
                    tool_name="web_browser_go",
                    tool_arguments={
                        "url": "https://inspect.ai-safety-institute.org.uk/"
                    },
                ),
                ModelOutput.for_tool_call(
                    model="mockllm/model",
                    tool_name="web_browser_back",
                    tool_arguments={},
                ),
                ModelOutput.for_tool_call(
                    model="mockllm/model",
                    tool_name="web_browser_forward",
                    tool_arguments={},
                ),
                ModelOutput.for_tool_call(
                    model="mockllm/model",
                    tool_name="web_browser_refresh",
                    tool_arguments={},
                ),
                ModelOutput.from_content(
                    model="mockllm/model", content="We are all done here."
                ),
            ],
        ),
    )[0]
    def is_inspect_website(page: str) -> bool:
        return (
            'link "Anchor"  [url: https://inspect.ai-safety-institute.org.uk/#welcome]'
            in page
        )
    def is_inspect_repo(page: str) -> bool:
        return (
            'link "Skip to content"  [url: https://github.com/UKGovernmentBEIS/inspect_ai#start-of-content]'
            in page
        )
    assert log.samples
    def check_tool_call_page(tool: str, page: Literal["website", "repo"]) -> None:
        call = get_tool_call(log.samples[0].messages, tool)
        assert call
        response = get_tool_response(log.samples[0].messages, call)
        assert response
        if page == "website":
            assert is_inspect_website(response.text)
        elif page == "repo":
            assert is_inspect_repo(response.text)
    check_tool_call_page("web_browser_go", "repo")
    check_tool_call_page("web_browser_back", "repo")
    check_tool_call_page("web_browser_forward", "website")
    check_tool_call_page("web_browser_refresh", "website")
@skip_if_no_docker
@pytest.mark.slow
def test_web_browser_type_submit():
    task = Task(
        dataset=[Sample(input="Please use the web_browser tool")],
        solver=[use_tools(web_browser()), generate()],
        sandbox=web_browser_sandbox(),
    )
    log = eval(
        task,
        model=get_model(
            "mockllm/model",
            custom_outputs=[
                ModelOutput.for_tool_call(
                    model="mockllm/model",
                    tool_name="web_browser_go",
                    tool_arguments={
                        "url": "https://www.selenium.dev/selenium/web/web-form.html"
                    },
                ),
                ModelOutput.for_tool_call(
                    model="mockllm/model",
                    tool_name="web_browser_type_submit",
                    tool_arguments={"element_id": 295, "text": "A submission"},
                ),
                ModelOutput.from_content(
                    model="mockllm/model", content="We are all done here."
                ),
            ],
        ),
    )[0]
    assert log.samples
    assert "Form submitted" in log.samples[0].messages[4].text
@skip_if_no_docker
@pytest.mark.slow
def test_web_browser_open_new_page():
    task = Task(
        dataset=[Sample(input="Please use the web_browser tool")],
        solver=[use_tools(web_browser()), generate()],
        sandbox=web_browser_sandbox(),
    )
    log = eval(
        task,
        model=get_model(
            "mockllm/model",
            custom_outputs=[
                ModelOutput.for_tool_call(
                    model="mockllm/model",
                    tool_name="web_browser_go",
                    tool_arguments={
                        "url": "https://www.selenium.dev/selenium/web/window_switching_tests/page_with_frame.html"
                    },
                ),
                ModelOutput.for_tool_call(
                    model="mockllm/model",
                    tool_name="web_browser_click",
                    tool_arguments={"element_id": 10},
                ),
                ModelOutput.from_content(
                    model="mockllm/model", content="We are all done here."
                ),
            ],
        ),
    )[0]
    assert log.samples
    assert "Simple page with simple test." in log.samples[0].messages[4].text
    assert "Open new window" not in log.samples[0].messages[4].text
@skip_if_no_docker
@skip_if_no_openai
@pytest.mark.slow
def test_web_browser_click():
    task = Task(
        dataset=[
            Sample(
                input="Please use the web browser tool to navigate to https://inspect.ai-safety-institute.org.uk/. Then, once there, use the web_browser_click tool to click the link to the documentation on Solvers."
            )
        ],
        solver=[use_tools(web_browser()), generate()],
        sandbox=web_browser_sandbox(),
    )
    log = eval(task, model="openai/gpt-4o")[0]
    click_call = get_tool_call(log.samples[0].messages, "web_browser_click")
    assert click_call
    click_response = get_tool_response(log.samples[0].messages, click_call)
    assert click_response
@skip_if_no_docker
@skip_if_no_openai
@pytest.mark.slow
def test_web_browser_input():
    task = Task(
        dataset=[
            Sample(
                input="Please use the web browser tool to navigate to https://www.google.com. Then, once there, use the page's search interface to search for 'large language models'"
            )
        ],
        solver=[use_tools(web_browser()), generate()],
        sandbox=web_browser_sandbox(),
    )
    log = eval(task, model="openai/gpt-4o")[0]
    type_call = get_tool_call(log.samples[0].messages, "web_browser_type_submit")
    assert type_call
@skip_if_no_docker
@skip_if_no_openai
@pytest.mark.slow
def test_web_browser_displays_error():
    """This should return an error, because the web-browsing tool cannot access pdfs."""
    task = Task(
        dataset=[
            Sample(
                input="Please use the web browser tool to navigate to https://arxiv.org/pdf/2403.07974."
            )
        ],
        solver=[use_tools(web_browser()), generate()],
        sandbox=web_browser_sandbox(),
    )
    log = eval(task, model="openai/gpt-4o")[0]
    response = get_tool_response(
        log.samples[0].messages,
        get_tool_call(log.samples[0].messages, "web_browser_go"),
    )
    assert response.error is not None
def web_browser_sandbox() -> tuple[str, str]:
    return (
        "docker",
        (Path(__file__).parent / "test_web_browser_compose.yaml").as_posix(),
    )

================
File: tests/util/sandbox/test_docker_compose_multiple_services.py
================
from pathlib import Path
import pytest
from test_helpers.tool_call_utils import get_tool_calls, get_tool_response
from test_helpers.utils import skip_if_no_docker
from inspect_ai import Task, eval
from inspect_ai.dataset import Sample
from inspect_ai.model import ModelOutput, get_model
from inspect_ai.scorer import includes
from inspect_ai.solver import generate, use_tools
from inspect_ai.tool import Tool, tool
from inspect_ai.util import sandbox
CURRENT_DIRECTORY = Path(__file__).resolve().parent
@tool
def write_file_service_1() -> Tool:
    async def execute(file: str, content: str):
        """
        Writes the contents of a file.
        Args:
          file (str): File to write.
          content(str): Content to write to the file.
        """
        await sandbox("service_1").write_file(file, content)
    return execute
@tool
def write_file_service_2() -> Tool:
    async def execute(file: str, content: str):
        """
        Writes the contents of a file.
        Args:
          file (str): File to write.
          content(str): Content to write to the file.
        """
        await sandbox("service_2").write_file(file, content)
    return execute
@tool
def read_file_service_1() -> Tool:
    async def execute(file: str) -> str:
        """
        Reads the contents of a file.
        Args:
          file (str): File to read.
        Returns:
            str: Contents of the file.
        """
        return await sandbox("service_1").read_file(file, text=True)
    return execute
@tool
def read_file_service_2() -> Tool:
    async def execute(file: str) -> str:
        """
        Reads the contents of a file.
        Args:
          file (str): File to read.
        Returns:
            str: Contents of the file.
        """
        return await sandbox("service_2").read_file(file, text=True)
    return execute
@skip_if_no_docker
@pytest.mark.slow
def test_docker_compose_multiple_services_write_file():
    dataset = [
        Sample(
            input="Writes some files please",
            target="unused",
            files={"foo.txt": "contents_of_foo.txt"},
        )
    ]
    task = Task(
        dataset=dataset,
        solver=[
            use_tools(
                [
                    write_file_service_1(),
                    write_file_service_2(),
                    read_file_service_1(),
                    read_file_service_2(),
                ]
            ),
            generate(),
        ],
        scorer=includes(),
        sandbox=(
            "docker",
            str(
                CURRENT_DIRECTORY
                / "docker_compose_multiple_services/docker-compose.yaml"
            ),
        ),
    )
    def tool_calls():
        yield ModelOutput.for_tool_call(
            model="mockllm/model",
            tool_name="write_file_service_1",
            tool_arguments={"file": "foo.txt", "content": "unused_1"},
        )
        yield ModelOutput.for_tool_call(
            model="mockllm/model",
            tool_name="write_file_service_2",
            tool_arguments={"file": "foo.txt", "content": "unused_2"},
        )
        yield ModelOutput.for_tool_call(
            model="mockllm/model",
            tool_name="read_file_service_1",
            tool_arguments={"file": "foo.txt"},
        )
        yield ModelOutput.for_tool_call(
            model="mockllm/model",
            tool_name="read_file_service_2",
            tool_arguments={"file": "foo.txt"},
        )
        while True:
            yield ModelOutput.from_content(
                model="mockllm/model", content="nothing left"
            )
    result = eval(
        task,
        model=get_model("mockllm/model", custom_outputs=tool_calls()),
        message_limit=10,  # otherwise we can get into an infinite loop if the tools error
    )[0]
    assert result.status == "success"
    messages = result.samples[0].messages
    assert (
        get_tool_response(
            messages, get_tool_calls(messages, "read_file_service_1")[0]
        ).content
        == "unused_1"
    )
    assert (
        get_tool_response(
            messages, get_tool_calls(messages, "read_file_service_2")[0]
        ).content
        == "unused_2"
    )

================
File: tests/util/sandbox/test_docker_filenames.py
================
import pytest
from test_helpers.utils import skip_if_no_docker
from inspect_ai import Task, eval, task
from inspect_ai.util._sandbox.environment import SandboxEnvironmentSpec
@task
def docker_config_file(config_file: str | None):
    return Task(sandbox=SandboxEnvironmentSpec(type="docker", config=config_file))
def check_docker_config_file(config_file: str | None) -> None:
    log = eval(docker_config_file(config_file), model="mockllm/model")[0]
    assert log.status == "success"
@pytest.mark.slow
@skip_if_no_docker
def test_dockerfile_none():
    check_docker_config_file(None)
@pytest.mark.slow
@skip_if_no_docker
def test_dockerfile_standard():
    check_docker_config_file("Dockerfile")
@pytest.mark.slow
@skip_if_no_docker
def test_dockerfile_stem():
    check_docker_config_file("debug.Dockerfile")
@pytest.mark.slow
@skip_if_no_docker
def test_dockerfile_extension():
    check_docker_config_file("Dockerfile.debug")

================
File: tests/util/sandbox/test_docker_healthcheck.py
================
import pytest
from inspect_ai.util._sandbox.docker.service import (
    ComposeService,
    parse_duration,
    service_healthcheck_time,
    services_healthcheck_time,
)
# Duration Parser Tests
def test_parse_duration_simple():
    assert parse_duration("30s").seconds == 30.0
    assert parse_duration("1m").seconds == 60.0
    assert parse_duration("1h").seconds == 3600.0
def test_parse_duration_combined():
    assert parse_duration("1m30s").seconds == 90.0
    assert parse_duration("1h30m").seconds == 5400.0
    assert parse_duration("2h30m15s").seconds == 9015.0
def test_parse_duration_with_spaces():
    assert parse_duration("1h 30m").seconds == 5400.0
    assert parse_duration("1h 30m 15s").seconds == 5415.0
def test_parse_duration_milliseconds():
    assert parse_duration("100ms").seconds == 0.1
    assert parse_duration("1s500ms").seconds == 1.5
def test_parse_duration_empty():
    assert parse_duration("").seconds == 0.0
def test_parse_duration_invalid():
    with pytest.raises(ValueError):
        parse_duration("invalid")
    with pytest.raises(ValueError):
        parse_duration("30x")  # invalid unit
# Service Healthcheck Time Tests
def test_service_without_healthcheck() -> None:
    service: ComposeService = {
        "image": "nginx",
    }
    assert service_healthcheck_time(service) == 0.0
def test_service_with_default_values() -> None:
    service: ComposeService = {
        "image": "nginx",
        "healthcheck": {},
    }
    assert service_healthcheck_time(service) == 180.0
def test_service_with_custom_values() -> None:
    service: ComposeService = {
        "image": "nginx",
        "healthcheck": {
            "start_period": "10s",
            "interval": "5s",
            "timeout": "3s",
            "retries": 5,
        },
    }
    assert service_healthcheck_time(service) == 40.0
def test_service_with_partial_custom_values() -> None:
    service: ComposeService = {
        "image": "nginx",
        "healthcheck": {
            "start_period": "10s",
            "timeout": "3s",
        },
    }
    assert service_healthcheck_time(service) == 99.0
# Total Healthcheck Time Tests
def test_total_time_no_services() -> None:
    services: dict[str, ComposeService] = {}
    assert services_healthcheck_time(services) == 0.0
def test_total_time_no_healthchecks() -> None:
    services: dict[str, ComposeService] = {
        "web": {"image": "nginx"},
        "db": {
            "image": "postgres",
        },
    }
    assert services_healthcheck_time(services) == 0.0
def test_total_time_multiple_services() -> None:
    services: dict[str, ComposeService] = {
        "web": {
            "image": "nginx",
            "healthcheck": {
                "start_period": "10s",
                "interval": "5s",
                "timeout": "3s",
                "retries": 5,
            },
        },
        "db": {
            "image": "postgres",
            "healthcheck": {
                "start_period": "30s",
                "interval": "10s",
                "timeout": "5s",
                "retries": 3,
            },
        },
    }
    assert services_healthcheck_time(services) == 45.0
def test_total_time_mixed_services() -> None:
    services: dict[str, ComposeService] = {
        "web": {
            "image": "nginx",
            "healthcheck": {
                "start_period": "10s",
                "interval": "5s",
                "timeout": "3s",
                "retries": 5,
            },
        },
        "db": {
            "image": "postgres",
        },
    }
    assert services_healthcheck_time(services) == 40.0

================
File: tests/util/sandbox/test_sandbox_service.py
================
from textwrap import dedent
import anyio
import pytest
from inspect_ai import Task, eval
from inspect_ai.solver import Generate, Solver, TaskState, solver
from inspect_ai.util import sandbox
from inspect_ai.util._sandbox.service import sandbox_service
@pytest.mark.slow
def test_sandbox_service():
    log = eval(Task(solver=math_service()), model="mockllm/model", sandbox="docker")[0]
    assert log.status == "success"
    assert log.samples
    sample = log.samples[0]
    assert sample.store.get("result") == 8
@solver
def math_service() -> Solver:
    async def solve(state: TaskState, generate: Generate) -> TaskState:
        # generate a script that will exercise the service and copy it to the sandbox
        run_script = "run.py"
        run_script_code = dedent("""
        import asyncio
        async def run():
            # wait for service to come up
            import os
            service_dir = "/var/tmp/sandbox-services/math_service"
            while not os.path.exists(f"{service_dir}/math_service.py"):
                await asyncio.sleep(0.1)
            # import service
            import sys
            sys.path.append(service_dir)
            from math_service import call_math_service, call_math_service_async
            # call service
            result = await call_math_service_async("add", x=10, y=5)
            result = call_math_service("subtract", x=result, y=7)
            await call_math_service_async("finish", result=result)
        asyncio.run(run())
        """)
        await sandbox().write_file(run_script, run_script_code)
        # run the script and the math service
        async with anyio.create_task_group() as tg:
            async def run_service_script() -> None:
                script_error = ""
                try:
                    result = await sandbox().exec(["python3", run_script])
                    if not result.success:
                        script_error = (
                            f"Error running script '{run_script}': {result.stderr}"
                        )
                except Exception as e:
                    script_error = f"Exception in script: {str(e)}"
                if script_error:
                    print(script_error)
                    tg.cancel_scope.cancel()
            tg.start_soon(run_math_service, state)
            tg.start_soon(run_service_script)
        return state
    return solve
async def run_math_service(state: TaskState) -> None:
    finished = False
    async def add(x: int, y: int) -> int:
        return x + y
    async def subtract(x: int, y: int) -> int:
        return x - y
    async def finish(result: int) -> None:
        nonlocal finished
        finished = True
        state.store.set("result", result)
    await sandbox_service(
        name="math_service",
        methods=[add, subtract, finish],
        until=lambda: finished,
        sandbox=sandbox(),
    )

================
File: tests/util/sandbox/test_sandbox_setup.py
================
from pathlib import Path
import pytest
from test_helpers.utils import skip_if_no_docker
from inspect_ai import Task, eval
from inspect_ai.dataset import Sample
from inspect_ai.model import ModelOutput
from inspect_ai.scorer import CORRECT, includes
from inspect_ai.solver import Generate, Solver, TaskState, solver
from inspect_ai.util import sandbox
from inspect_ai.util._sandbox.docker.config import is_dockerfile
SANDBOX_SETUP_FILE = (Path(__file__).parent / "sandbox_setup.sh").as_posix()
SANDBOX_SETUP_ERROR_FILE = (Path(__file__).parent / "sandbox_setup_error.sh").as_posix()
with open(SANDBOX_SETUP_FILE, "r") as f:
    sandbox_setup = f.read()
@solver
def check_foo() -> Solver:
    async def solve(state: TaskState, generate: Generate) -> TaskState:
        try:
            await sandbox().read_file(state.metadata["file"])
            completion = "Yes"
        except FileNotFoundError:
            completion = "No"
        state.output = ModelOutput.from_content("mockllm/model", completion)
        return state
    return solve
@skip_if_no_docker
@pytest.mark.slow
def test_docker_sandbox_setup():
    def sample(file: str, target: str, setup: str) -> Sample:
        return Sample(
            input=f"Does the file '{file}' exist? Answer Yes or No",
            target=target,
            metadata={"file": file},
            setup=setup,
        )
    dataset = [
        sample("foo.txt", "Yes", sandbox_setup),
        sample("bar.txt", "No", sandbox_setup),
        sample("foo.txt", "Yes", SANDBOX_SETUP_FILE),
        sample("bar.txt", "No", SANDBOX_SETUP_FILE),
    ]
    task = Task(
        dataset=dataset,
        solver=check_foo(),
        scorer=includes(),
        sandbox="docker",
    )
    log = eval(task, model="mockllm/model")[0]
    assert log.samples
    for sample in log.samples:
        assert sample.scores["includes"].value == CORRECT
@skip_if_no_docker
@pytest.mark.slow
def test_docker_sandbox_setup_fail_on_error():
    task = Task(
        dataset=[Sample(input="Say hello.", setup=SANDBOX_SETUP_ERROR_FILE)],
        sandbox="docker",
    )
    # fail_on_error=True (entire eval fails)
    log = eval(task, model="mockllm/model", fail_on_error=True)[0]
    assert log.status == "error"
    # fail_on_error=False (sample fails not entire eval)
    log = eval(task, model="mockllm/model", fail_on_error=False)[0]
    assert log.status == "success"
    assert log.samples
    assert log.samples[0].error
def test_is_dockerfile():
    assert is_dockerfile("/path/to/Dockerfile")
    assert is_dockerfile("/path/to/name.Dockerfile")
    assert is_dockerfile("/path/to/Dockerfile.name")
    assert not is_dockerfile("/path/to/Dockerfile-name")
    assert not is_dockerfile("/path/to/Dockerfile_name")
    assert not is_dockerfile("/path/to/name-Dockerfile")
    assert not is_dockerfile("/path/to/name_Dockerfile")
    assert not is_dockerfile("/path/to/docker-compose.yaml")
    assert not is_dockerfile("/path/to/not_a_dockerfile.txt")
if __name__ == "__main__":
    test_docker_sandbox_setup()

================
File: tests/util/test_display_counter.py
================
from typing import get_args
import anyio
import pytest
from inspect_ai import Task, eval, task
from inspect_ai.dataset import Sample
from inspect_ai.solver import Generate, TaskState, solver
from inspect_ai.util import display_counter
from inspect_ai.util._display import DisplayType
@pytest.mark.parametrize("display_type", get_args(DisplayType))
def test_can_display_counter(display_type: DisplayType):
    # This test doesn't actually verify the UI; it just exercises the code path.
    result = eval(counter_task(), model="mockllm/mockllm", display=display_type)
    assert result[0].status == "success"
@task
def counter_task():
    return Task(
        dataset=[Sample(input="Just reply with Hello World")], solver=[counter_solver()]
    )
@solver
def counter_solver():
    async def solve(state: TaskState, generate: Generate):
        display_counter("My counter", "1")
        display_counter("My counter", "2")
        # The footer is throttled at 1 Hz, so sleep for longer than that.
        await anyio.sleep(1.1)
        display_counter("My counter", "3")
        return state
    return solve

================
File: tests/util/test_file.py
================
from inspect_ai._util.file import basename
def test_basename():
    MYFILE = "myfile.log"
    assert basename(f"s3://my-bucket/{MYFILE}") == MYFILE
    assert basename(f"/opt/files/{MYFILE}") == MYFILE
    assert basename(f"C:\\Documents\\{MYFILE}") == MYFILE
    MYDIR = "mydir"
    assert basename(f"s3://my-bucket/{MYDIR}") == MYDIR
    assert basename(f"s3://my-bucket/{MYDIR}/") == MYDIR
    assert basename(f"/opt/files/{MYDIR}") == MYDIR
    assert basename(f"/opt/files/{MYDIR}/") == MYDIR
    assert basename(f"C:\\Documents\\{MYDIR}") == MYDIR
    assert basename(f"C:\\Documents\\{MYDIR}\\") == MYDIR

================
File: tests/util/test_format_template.py
================
from dataclasses import dataclass, field
from typing import Any
import pytest
from inspect_ai._util.format import format_template
@dataclass
class FormatterCase:
    """Test case for format_template function."""
    params: dict[str, Any]
    template: str
    expected: str
    skip_unknown: bool = field(default=True)
    should_raise: bool = field(default=False)
def test_basic_substitution() -> None:
    """Test basic parameter substitution."""
    params = {"name": "World"}
    result = format_template("Hello {name}!", params)
    assert result == "Hello World!"
def test_multiple_substitutions() -> None:
    """Test multiple parameter substitutions."""
    params = {"first": "John", "last": "Doe"}
    result = format_template("Hello {first} {last}!", params)
    assert result == "Hello John Doe!"
def test_unknown_placeholder_skipping() -> None:
    """Test that unknown placeholders are preserved when skip_unknown is True."""
    result = format_template("Hello {unknown}!", {})
    assert result == "Hello {unknown}!"
def test_unknown_placeholder_raising() -> None:
    """Test that unknown placeholders raise KeyError when skip_unknown is False."""
    with pytest.raises(KeyError):
        format_template("Hello {unknown}!", {}, skip_unknown=False)
def test_escaped_braces() -> None:
    """Test that escaped braces are handled correctly."""
    test_cases: list[FormatterCase] = [
        FormatterCase(
            params={"name": "John"}, template="{{name}} {name}", expected="{name} John"
        ),
        FormatterCase(params={}, template="{{literal}}", expected="{literal}"),
    ]
    for case in test_cases:
        result = format_template(case.template, case.params)
        assert result == case.expected
def test_nested_attributes() -> None:
    """Test handling of nested attribute access."""
    @dataclass
    class Person:
        name: str
        age: int
    params = {"person": Person(name="John", age=30)}
    result = format_template("{person.name} is {person.age}", params)
    assert result == "John is 30"
def test_nested_dict() -> None:
    """Test handling of nested dictionary access."""
    params = {"obj": {"attr": "value"}}
    result = format_template("{obj[attr]}", params)
    assert result == "value"
def test_invalid_nested_attributes() -> None:
    """Test handling of invalid nested attribute access."""
    test_cases: list[FormatterCase] = [
        FormatterCase(
            params={"obj": {}}, template="{obj.missing}", expected="{obj.missing}"
        ),
        FormatterCase(
            params={"obj": None}, template="{obj.anything}", expected="{obj.anything}"
        ),
    ]
    for case in test_cases:
        result = format_template(case.template, case.params)
        assert result == case.expected
def test_format_specifications() -> None:
    """Test handling of format specifications."""
    test_cases: list[FormatterCase] = [
        FormatterCase(params={"num": 42}, template="{num:03d}", expected="042"),
        FormatterCase(params={"pi": 3.14159}, template="{pi:.2f}", expected="3.14"),
        FormatterCase(
            params={"text": "center"}, template="{text:^10}", expected="  center  "
        ),
    ]
    for case in test_cases:
        result = format_template(case.template, case.params)
        assert result == case.expected
def test_array_indexing() -> None:
    """Test handling of array indexing."""
    test_cases: list[FormatterCase] = [
        FormatterCase(
            params={"arr": [1, 2, 3]},
            template="{arr[0]} {arr[1]} {arr[2]}",
            expected="1 2 3",
        ),
        FormatterCase(
            params={"arr": ["first", "second"]}, template="{arr[0]}", expected="first"
        ),
    ]
    for case in test_cases:
        result = format_template(case.template, case.params)
        assert result == case.expected
def test_mixed_known_unknown() -> None:
    """Test mixing known and unknown parameters."""
    test_cases: list[FormatterCase] = [
        FormatterCase(
            params={"known": "value"},
            template="{known} and {unknown}",
            expected="value and {unknown}",
        ),
        FormatterCase(
            params={"a": 1, "b": 2},
            template="{a} {missing} {b}",
            expected="1 {missing} 2",
        ),
    ]
    for case in test_cases:
        result = format_template(case.template, case.params)
        assert result == case.expected
def test_empty_template() -> None:
    """Test handling of empty template."""
    result = format_template("", {})
    assert result == ""
def test_complex_nested_structures() -> None:
    """Test handling of complex nested data structures."""
    @dataclass
    class Address:
        street: str
        city: str
    @dataclass
    class Person:
        name: str
        address: Address
        scores: list[int]
    person = Person(
        name="John",
        address=Address(street="123 Main St", city="Anytown"),
        scores=[95, 87, 91],
    )
    test_cases: list[FormatterCase] = [
        FormatterCase(
            params={"person": person},
            template="{person.name} lives in {person.address.city}",
            expected="John lives in Anytown",
        ),
        FormatterCase(
            params={"person": person},
            template="Best score: {person.scores[0]}",
            expected="Best score: 95",
        ),
    ]
    for case in test_cases:
        result = format_template(case.template, case.params)
        assert result == case.expected

================
File: tests/util/test_images.py
================
import os
from test_helpers.utils import (
    skip_if_no_anthropic,
    skip_if_no_google,
    skip_if_no_grok,
    skip_if_no_mistral,
    skip_if_no_openai,
    skip_if_no_vertex,
    skip_if_trio,
)
from inspect_ai import Task, eval, task
from inspect_ai.dataset import json_dataset
from inspect_ai.scorer import match
from inspect_ai.solver import generate, system_message
SYSTEM_MESSAGE = """
For the following exercise, it is important that you answer with only a single word or numeric value in brackets. For example, [22] or [house]. Do not include any discussion, narrative, or rationale, just a single value in brackets.
"""
@task
def images():
    return Task(
        dataset=json_dataset(
            os.path.join("tests", "util", "test_images", "images.jsonl")
        ),
        solver=[system_message(SYSTEM_MESSAGE), generate()],
        scorer=match(),
    )
def check_images(model):
    eval(images, model)
@skip_if_no_google
@skip_if_trio
def test_google_images():
    check_images("google/gemini-1.5-flash")
@skip_if_no_openai
def test_openai_images():
    check_images("openai/gpt-4o")
@skip_if_no_vertex
def test_vertex_images():
    check_images("vertex/gemini-1.5-flash")
@skip_if_no_anthropic
def test_anthropic_images():
    check_images("anthropic/claude-3-sonnet-20240229")
@skip_if_no_mistral
def test_mistral_images():
    check_images("mistral/pixtral-12b-2409")
@skip_if_no_grok
def test_grok_images():
    check_images("grok/grok-vision-beta")

================
File: tests/util/test_kvstore.py
================
import sqlite3
from pathlib import Path
import pytest
from inspect_ai._util.kvstore import KVStore
@pytest.fixture
def db_path(tmp_path: Path) -> Path:
    """Provide a temporary database path for each test."""
    return tmp_path / "test.db"
@pytest.fixture
def store(db_path: Path) -> KVStore:
    """Provide a KVStore instance."""
    return KVStore(str(db_path))
def test_kvstore_basic_put_get(store: KVStore) -> None:
    with store:
        store.put("key1", "value1")
        assert store.get("key1") == "value1"
def test_kvstore_missing_key_returns_none(store: KVStore) -> None:
    with store:
        assert store.get("nonexistent") is None
def test_kvstore_update_existing_key(store: KVStore) -> None:
    with store:
        store.put("key1", "value1")
        store.put("key1", "value2")
        assert store.get("key1") == "value2"
def test_kvstore_delete_existing_key(store: KVStore) -> None:
    with store:
        store.put("key1", "value1")
        assert store.delete("key1") is True
        assert store.get("key1") is None
def test_kvstore_delete_nonexistent_key(store: KVStore) -> None:
    with store:
        assert store.delete("nonexistent") is False
def test_kvstore_affects_count(store: KVStore) -> None:
    with store:
        store.put("key1", "value1")
        store.put("key2", "value2")
        assert store.count() == 2
        store.delete("key1")
        assert store.count() == 1
def test_kvstore_count_empty_store(store: KVStore) -> None:
    with store:
        assert store.count() == 0
def test_kvstore_count_with_entries(store: KVStore) -> None:
    with store:
        store.put("key1", "value1")
        store.put("key2", "value2")
        assert store.count() == 2
def test_kvstore_rotation_with_max_entries(db_path: Path) -> None:
    store = KVStore(str(db_path), max_entries=2)
    with store:
        store.put("key1", "value1")
        store.put("key2", "value2")
        store.put("key3", "value3")
        assert store.count() == 2
        assert store.get("key1") is None  # Oldest entry should be removed
        assert store.get("key2") == "value2"
        assert store.get("key3") == "value3"
def test_kvstore_persistence_between_sessions(db_path: Path) -> None:
    # First session
    with KVStore(str(db_path)) as store:
        store.put("key1", "value1")
    # Second session
    with KVStore(str(db_path)) as store:
        assert store.get("key1") == "value1"
def test_kvstore_context_manager_closes_connection(store: KVStore) -> None:
    with store:
        store.put("key1", "value1")
    # Connection should be closed after context exit
    with pytest.raises(sqlite3.ProgrammingError):
        store.conn.execute("SELECT 1")
def test_kvstore_empty_string_values(store: KVStore) -> None:
    with store:
        store.put("key1", "")
        assert store.get("key1") == ""
def test_kvstore_special_characters_in_keys_and_values(store: KVStore) -> None:
    with store:
        special = "!@#$%^&*()_+-=[]{}|;:'\",.<>?"
        store.put(special, special)
        assert store.get(special) == special
def test_kvstore_very_long_strings(store: KVStore) -> None:
    with store:
        long_string = "x" * 10000
        store.put("long_key", long_string)
        assert store.get("long_key") == long_string
def test_kvstore_multiple_rotations(db_path: Path) -> None:
    store = KVStore(str(db_path), max_entries=3)
    with store:
        for i in range(5):
            store.put(f"key{i}", f"value{i}")
        assert store.count() == 3
        assert store.get("key0") is None
        assert store.get("key1") is None
        assert store.get("key2") == "value2"
        assert store.get("key3") == "value3"
        assert store.get("key4") == "value4"
def test_kvstore_using_store_without_context_manager() -> None:
    with pytest.raises(AttributeError):
        store = KVStore("test.db")
        store.put("key1", "value1")  # Should fail because conn isn't initialized

================
File: tests/util/test_media.py
================
import os
from test_helpers.utils import (
    skip_if_no_google,
    skip_if_no_openai,
    skip_if_no_vertex,
    skip_if_trio,
)
from inspect_ai import Task, eval, task
from inspect_ai.dataset import json_dataset
from inspect_ai.scorer import includes
from inspect_ai.solver import generate
@task
def audio():
    return Task(
        dataset=json_dataset(
            os.path.join("tests", "util", "test_media", "audio.jsonl")
        ),
        solver=[generate()],
        scorer=includes(),
    )
def check_audio(model):
    log = eval(audio(), model)[0]
    assert log.status == "success"
@task
def video():
    return Task(
        dataset=json_dataset(
            os.path.join("tests", "util", "test_media", "video.jsonl")
        ),
        solver=[generate()],
        scorer=includes(),
    )
def check_video(model):
    log = eval(video(), model)[0]
    assert log.status == "success"
@skip_if_no_google
@skip_if_trio
def test_media_google_audio():
    check_audio("google/gemini-1.5-flash")
@skip_if_no_google
@skip_if_trio
def test_media_google_video():
    check_video("google/gemini-1.5-flash")
@skip_if_no_vertex
def test_media_vertex_audio():
    check_audio("vertex/gemini-1.5-flash")
@skip_if_no_openai
def test_media_openai_audio():
    check_audio("openai/gpt-4o-audio-preview")

================
File: tests/util/test_package.py
================
import os
import httpx
import numpy as np
import pytest
from test_helpers.tools import addition, list_files
import inspect_ai
from inspect_ai._util.package import get_installed_package_name
def test_numpy_package():
    assert get_installed_package_name(np.array) == "numpy"
    assert get_installed_package_name(np.random.rand) == "numpy"
def test_httpx_package():
    assert get_installed_package_name(httpx.get) == "httpx"
    assert get_installed_package_name(httpx.Client) == "httpx"
def test_builtin_module():
    assert get_installed_package_name(os.path.join) is None
    assert get_installed_package_name(list.append) is None
def test_inspect_ai_package():
    assert get_installed_package_name(inspect_ai.eval) == "inspect_ai"
def test_local_module():
    assert get_installed_package_name(addition) is None
    assert get_installed_package_name(list_files) is None
def test_local_function():
    def local_func():
        pass
    assert get_installed_package_name(local_func) is None
def test_local_class():
    class LocalClass:
        pass
    assert get_installed_package_name(LocalClass) is None
def test_none_input():
    assert get_installed_package_name(None) is None
@pytest.mark.parametrize("value", [42, "string", [1, 2, 3]])
def test_builtin_types(value):
    assert get_installed_package_name(value) is None

================
File: tests/util/test_registry.py
================
from typing import cast
from inspect_ai._util.constants import PKG_NAME
from inspect_ai._util.registry import (
    registry_create_from_dict,
    registry_info,
    registry_lookup,
    registry_value,
)
from inspect_ai.scorer import Metric, metric
from inspect_ai.scorer._metric import SampleScore
from inspect_ai.solver import Plan, Solver, solver, use_tools
from inspect_ai.tool import Tool, bash
def test_registry_namespaces() -> None:
    # define a local metric which we can lookup by simple name
    @metric(name="local_accuracy")
    def accuracy1(correct: str = "C") -> Metric:
        def metric(scores: list[SampleScore]) -> int | float:
            return 1
        return metric
    assert registry_lookup("metric", "local_accuracy")
    # confirm that inspect_ai builtins have their namespace auto-appended
    info = registry_info(registry_lookup("metric", f"{PKG_NAME}/accuracy"))
    assert info
    assert info.name == f"{PKG_NAME}/accuracy"
def test_registry_dict() -> None:
    @solver
    def create_solver(tool: Tool) -> Solver:
        return use_tools(tool)
    mysolver = create_solver(bash(timeout=10))
    solver_dict = registry_value(mysolver)
    assert solver_dict["type"] == "solver"
    assert solver_dict["params"]["tool"]["type"] == "tool"
    mysolver2 = cast(Plan, registry_create_from_dict(solver_dict))
    assert isinstance(mysolver2, Solver)

================
File: tests/util/test_str_to_float.py
================
import pytest
from inspect_ai._util.text import str_to_float
def test_str_to_float_basic():
    assert str_to_float("1") == 1.0
    assert str_to_float("2") == 8.0
    assert str_to_float("5") == 625.0
    assert str_to_float("10") == 1.0
    assert str_to_float("3") == 3.0
def test_str_to_float_decimal_base():
    assert str_to_float("2.5") == 2.5**2
    assert str_to_float("0.1") == 0.1**3
def test_str_to_float_negative_base():
    assert str_to_float("-2") == (-2) ** 2
    assert str_to_float("-2") == (-2) ** 3
def test_str_to_float_multi_digit_exponent():
    assert str_to_float("2") == 2**45
    assert str_to_float("3") == 3**0  # Exponent is 0
def test_str_to_float_no_exponent():
    assert str_to_float("7") == 7.0
    assert str_to_float("0") == 0.0
def test_str_to_float_no_base():
    # When the base is missing, default to 1.0
    assert str_to_float("") == 1.0**5
    assert str_to_float("") == 1.0**0
def test_str_to_float_zero_exponent():
    assert str_to_float("5") == 1.0
    assert str_to_float("0") == 1.0  # 0^0 is considered 1 in this context
def test_str_to_float_invalid_input():
    with pytest.raises(ValueError):
        str_to_float("abc")
    with pytest.raises(ValueError):
        str_to_float("")
    with pytest.raises(ValueError):
        str_to_float("2^3")
    with pytest.raises(ValueError):
        str_to_float("")  # Unsupported superscript characters
def test_str_to_float_edge_cases():
    # Exponent with unsupported characters
    with pytest.raises(ValueError):
        str_to_float("2")
    # Base with unsupported characters
    with pytest.raises(ValueError):
        str_to_float("a")
    # Superscript after decimal point
    assert str_to_float("2.5") == 2.5**4

================
File: tests/util/test_subprocess.py
================
import os
from pathlib import Path
from random import random
import psutil
import pytest
from inspect_ai.util import subprocess
@pytest.mark.anyio
async def test_subprocess_execute():
    result = await subprocess(["python3", "-c", "print('foo')"])
    assert result.stdout.strip() == "foo"
@pytest.mark.anyio
async def test_subprocess_fail():
    result = await subprocess(["cat", "phantom.txt"])
    assert result.success is False
@pytest.mark.anyio
async def test_subprocess_stdin():
    input = "tell me a story"
    result = await subprocess(
        ["python3", "-c", "import sys; print(sys.stdin.read())"], input=input
    )
    assert result.stdout.strip() == input
@pytest.mark.anyio
async def test_subprocess_binary():
    input = "tell me a story".encode()
    result = await subprocess(
        ["python3", "-c", "import sys; print(sys.stdin.read())"],
        text=False,
        input=input,
    )
    assert result.stdout.decode().strip() == input.decode()
@pytest.mark.anyio
async def test_subprocess_cwd():
    parent_dir = Path(os.getcwd()).parent.as_posix()
    result = await subprocess(
        ["python3", "-c", "import os; print(os.getcwd())"], cwd=parent_dir
    )
    assert result.stdout.strip() == parent_dir
@pytest.mark.anyio
async def test_subprocess_env():
    ENV_VAR = "TEST_SUBPROCESS_ENV"
    ENV_VALUE = "test value"
    result = await subprocess(
        ["python3", "-c", f"import os; print(os.getenv('{ENV_VAR}'))"],
        env={ENV_VAR: ENV_VALUE},
    )
    assert result.stdout.strip() == ENV_VALUE
@pytest.mark.anyio
async def test_subprocess_timeout():
    def process_found(pattern: str) -> bool:
        return any(
            pattern in " ".join(p.info["cmdline"] or [])
            for p in psutil.process_iter(["cmdline"])
        )
    timeout_length = random() * 60
    subprocess_cmds = ["sleep", f"{2 + timeout_length}"]
    if process_found(" ".join(subprocess_cmds)):
        raise Exception(
            f"There is already a process matching {subprocess_cmds}; the test isn't going to work"
        )
    try:
        await subprocess(subprocess_cmds, timeout=1)
        assert False
    except TimeoutError:
        if process_found(" ".join(subprocess_cmds)):
            assert False, "Process was not killed"

================
File: tests/util/test_truncate.py
================
from inspect_ai._util.text import truncate
def test_basic_truncation():
    assert truncate("Hello World!", 8) == "Hello..."
    assert truncate("Short", 8) == "Short   "
def test_custom_overflow():
    assert truncate("Hello World!", 8, overflow=">>") == "Hello >>"
    assert truncate("Testing", 5, overflow="~") == "Test~"
def test_no_padding():
    assert truncate("Hi", 8, pad=False) == "Hi"
    assert truncate("Hello World!", 8, pad=False) == "Hello..."
def test_exact_length():
    assert truncate("12345678", 8) == "12345678"
    assert truncate("1234", 4, pad=False) == "1234"

================
File: tests/view/test_bundle.py
================
import os
import tempfile
import pytest
from test_helpers.utils import skip_if_trio
from inspect_ai import Task, eval
from inspect_ai._util.file import filesystem
from inspect_ai.dataset import Sample
from inspect_ai.log._bundle import bundle_log_dir
from inspect_ai.scorer import match
@pytest.mark.slow
@skip_if_trio
def test_s3_bundle(mock_s3) -> None:
    # run an eval to generate a log file to this directory
    s3_fs = filesystem("s3://test-bucket/")
    # target directories
    log_dir = "s3://test-bucket/test_s3_bundle/logs"
    output_dir = "s3://test-bucket/test_s3_bundle/view"
    eval(
        tasks=[
            Task(dataset=[Sample(input="Say Hello", target="Hello")], scorer=match())
            for i in range(0, 2)
        ],
        model="mockllm/model",
        log_dir=log_dir,
    )
    # bundle to the output dir
    bundle_log_dir(log_dir, output_dir)
    # ensure files are what we expect
    expected = [
        "index.html",
        "assets",
        "assets/index.js",
        "assets/index.css",
        "logs",
        "logs/logs.json",
    ]
    for exp in expected:
        assert s3_fs.exists(os.path.join(output_dir, exp))
def test_bundle() -> None:
    with tempfile.TemporaryDirectory() as working_dir:
        # run an eval to generate a log file to this directory
        log_dir = os.path.join(working_dir, "logs")
        output_dir = os.path.join(working_dir, "output")
        eval(
            tasks=[
                Task(
                    dataset=[Sample(input="Say Hello", target="Hello")], scorer=match()
                )
                for i in range(0, 2)
            ],
            model="mockllm/model",
            log_dir=log_dir,
        )
        # bundle to the output dir
        bundle_log_dir(log_dir, output_dir)
        # ensure files are what we expect
        expected = [
            "index.html",
            "assets",
            "assets/index.js",
            "assets/index.css",
            "logs",
            "logs/logs.json",
        ]
        for exp in expected:
            abs = os.path.join(output_dir, exp)
            assert os.path.exists(abs)
        # ensure there is a non-logs.json log file present in logs
        non_manifest_logs = [
            f
            for f in os.listdir(os.path.join(output_dir, "logs"))
            if f.endswith(".eval") and f != "logs.eval"
        ]
        assert len(non_manifest_logs) == 2

================
File: tools/vscode/CHANGELOG.md
================
# Changelog

## 0.3.58

- Re-use viewers when using the open log file command.

## 0.3.57

- Add open log file command (paste a path to a local or remote log file in the command palette to open it in the Inspect viewer)

## 0.3.56

- Improve startup performance and task detection performance

## 0.3.55

- Remove display of status icons in log listing (it causes performance issues).

## 0.3.54

- Add startup output channel to monitor extension startup performance
- Screen additional directories when scanning for tasks

## 0.3.53

- Fix 'Invalid Tree Item' error

## 0.3.52

- Don't add entries to the `.gitignore` file.

## 0.3.51

- Improve performance of log listing rendering by caching information in the workspace.

## 0.3.50

- Use the integrated terminal when debugging tasks.

## 0.3.49

- Improve code lense detection of Inspect tasks (ty @tobiasraabe)
- Use icon to reflect log status in log listing activity panel (red = error, yellow = cancelled, green = running)

## 0.3.48

- Properly shutdown the `inspect view` process when exiting VSCode.

## 0.3.47

- Minor improvements

## 0.3.46

- Update Bedrock models help link to point to more helpful page.

## 0.3.45

- `.eval` file links are now clickable in the terminal when an evaluation completes
- Improve task listing performance when rendering the task list in the activity bar

## 0.3.44

- Fix incorrect url encoding when copying links to remote log files.

## 0.3.43

- Add support for links which open the log viewer directly in VSCode. You may copy links to remote log files from the logs panel of the Inspect Activity Panel.

## 0.3.42

- Improve support for selecting text in the full screen terminal

## 0.3.41

- Add `Copy Path` context menu to activity bar logs panel

## 0.3.40

- When running and debugging Inspect evals, the extension will by default use any environments present in the task folder or parent folders (up to the workspace). To always use the workspace environment, change the 'Use Subdirectory Environments` setting.

## 0.3.39

- Fix an issue that would cause view to be unable to display when using VSCode extension with Inpsect version 0.3.42

## 0.3.38

- Improved behavior when previewing json files using the Inspect viewer.

## 0.3.37

This version includes a signficant rework of the overall workflow for interacting with Inspect. Changes include:

- The Inspect sidebar now includes a section which allows you to browse and open log files in the viewer. 
- The Inspect Viewer is no longer opened automatically when an evaluation is completed. Instead a notification is available to open the viewer. 
- New open log directory command allows you to select a log directory and open the Inspect Viewer for that directory
- Support for the new Inspect `eval` log format. More information [here](https://inspect.ai-safety-institute.org.uk/eval-logs.html#sec-log-format).

## 0.3.36

- Show Inspect version in status bar
- Release with internal changes to support future log viewing features.

## 0.3.33

- Fix bug that prevented run, debug buttons from appearing for tasks whose function declarations spanned more than single line.

## 0.3.32

- Properly resolve log directory when showing log view using `cmd+L`
- Fix an error when reading notebook selection

## 0.3.31

- Don't restrict the availability of the `Open with Inspect View` command based upon the filename.

## 0.3.30

- Further refinement to Inspect View behavior when a task is complete. We will now refresh the viewer without bringing it the foreground when it is already displaying and a task completes. This will ensure that focus is handled correctly in the editor. Use `cmd+shift+L` to bring the viewer to the foreground (or show it if it isn't showing).
- Rename 'Log View Auto' to 'Open Log View' since this better reflects the setting behavior (as it now controls whether the view is initially opened when a task is completed).

## 0.3.29

- Improve behavior of log viewer when a task completes and the view is already displaying
- Evaluation log files which exceed 100MB will be displayed without sample data (since the viewer becomes difficult to work with when files exceed this size).

## 0.3.28

- Fix incorrect environment variable when setting model base url (thx @hanbyul-kim)
- Fix extraneous error that might appear in console output

## 0.3.27

- Minor fix to view rendering across differing versions of Inspect.

## 0.3.26

- Properly preserve focus When showing the log viewer upon task completion.

## 0.3.25

- Poll more frequently to show new log files
- Add support for showing the log viewer in the Positron Viewer

## 0.3.24

- Properly deal with URI encoded characters in log directories when opening a log file

## 0.3.23

- Ensure the log view only opens in the correct window when debugging a task
- Changes to improve performance and usability of large log files

## 0.3.22

- Improve reliability of opening and viewing log files upon completion of evaluations
- Right click on log files in the Explorer sidebar and select `Open with Inspect View` to view a log file
- Improve reliability of viewing log files when clicking links in the terminal

## 0.3.21

- Properly reactivate log viewer when is has been activated but isn't visible (e.g another tab is visible in its column)

## 0.3.20

- Improve appearance of log view secondary header
- Fix issue where clicking model help could attempt to open more than one window

## 0.3.19

- Fix an issue showing the log viewer when an evaluation completes (specific to Inspect 0.3.10 or later)

## 0.3.18

- Fix issues with task params when type hints are provided
- Improve metric appearance in `inspect view`

## 0.3.17

- Improve `inspect view` title bar treatment

## 0.3.16

- Fix an issue that prevented the extension from loading when the `Pylance` extension was disabled or uninstalled.
- Don't send task params that have been removed from tasks
- Ensure that debugger breakpoints are available outside of user code
- Ensure that evaluations are run from the workspace directory
- Only show the logview in VS Code window that started an eval

## 0.3.14

- Fix issue where the run/debug task option would be disabled for the task configuration pane if a file containing no tasks was being editted.
- Improve Inspect binary detection on Linux platforms

## 0.3.13

-   Ensure that inspect CLI is in the path for terminals using a global Python environment
-   Add 'Show Logs' command to the environment panel.
-   Improve models in the environment panel
    -   Display literal provider names (rather than pretty names)
    -   Remember the last used model for each provider
    -   Allow free-form provide in model
    -   Add autocomplete for Ollama
-   Fix 'Restart' when debugging to properly restart the Inspect debugging session
-   Improve performance loading task tree, selecting tasks within outline, and navigating to tasks
-   Improve task selection behavior when the activity bar is first shown

================
File: tools/vscode/README.md
================
# inspect-vscode

VS Code extension for the Inspect framework for large language model evaluations. This extension provides support for developing evaluations using Inspect, including:

- Integrated viewer for evaluation log files
- Panel to browse, run, and debug tasks in the workspace
- Panel for editing Inspect `.env` file
- Panel for configuring task CLI options and args
- Commands and key-bindings for running tasks
- Commands and key-bindings for debugging tasks

## Log Viewer

The `inspect view` command is used to automatically display the log for tasks executed within the workspace (this behavior can be controlled with an option).

## Task Navigation

The Tasks panel displays a listing of all the Inspect tasks within your workspace. Selecting the source file or task within the listing will open the task source code in the source editor (or Notebook viewer). You can display a tree of tasks including folders and hierarchy or a flat list of tasks sorted alphabetically.

## Configuration Panel

Use the Configuration (.env) panel to edit common settings in your `.env.` file including the model provider and name, and the log directory and level.

## Task Panel

Use the Task panel to edit CLI options for a task, set task args, and run or debug a task. Values will be saved for each task and used whenever the task is run or debugged from within the Inspect VS Code extension.

## Running and Debugging

The Inspect VS Code extension includes commands and keyboard shortcuts for running or debugging tasks. After the task has been completed, `inspect view` is used behind the scenes to provide a results pane within VS Code alongside your source code.

Use the run or debug commands to execute the current task. You can alternatively use the <kbd>Ctrl+Shift+U</kbd> keyboard shortcut to run a task, or the <kbd>Ctrl+Shift+T</kbd> keyboard shortcut to debug a task.

> Note that on the Mac you should use `Cmd` rather than `Ctrl` as the prefix for all Inspect keyboard shortcuts.



================================================================
End of Codebase
================================================================
