This file is a merged representation of a subset of the codebase, containing specifically included files and files not matching ignore patterns, combined into a single document by Repomix.
The content has been processed where empty lines have been removed.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
- Pay special attention to the Repository Description. These contain important context and guidelines specific to this project.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: **/*.py, **/*.md, **/*.txt
- Files matching these patterns are excluded: **/.git/**, **/.github/**, CHANGELOG.md
- Empty lines have been removed from all files
- Files are sorted by Git change count (files with more changes are at the bottom)

Additional Info:
----------------
User Provided Header:
-----------------------
This file is a consolidated single-file compilation of all code in the repository generated by Repomix. Note that .ipynb files have been converted to .py files.

================================================================
Directory Structure
================================================================
benchmark/test_activations_store.py
benchmark/test_cache_activations_runner.py
benchmark/test_eval_all_loadable_saes.py
benchmark/test_language_model_sae_runner_multiple_devices.py
benchmark/test_language_model_sae_runner.py
docs/api.md
docs/citation.md
docs/contributing.md
docs/feature_dashboards.md
docs/generate_sae_table.py
docs/index.md
docs/roadmap.md
docs/training_saes.md
README.md
requirements.txt
sae_lens/__init__.py
sae_lens/analysis/hooked_sae_transformer.py
sae_lens/analysis/neuronpedia_integration.py
sae_lens/cache_activations_runner.py
sae_lens/config.py
sae_lens/evals.py
sae_lens/load_model.py
sae_lens/pretokenize_runner.py
sae_lens/sae_training_runner.py
sae_lens/sae.py
sae_lens/tokenization_and_batching.py
sae_lens/toolkit/pretrained_sae_loaders.py
sae_lens/toolkit/pretrained_saes_directory.py
sae_lens/training/activations_store.py
sae_lens/training/geometric_median.py
sae_lens/training/optim.py
sae_lens/training/sae_trainer.py
sae_lens/training/training_sae.py
sae_lens/training/upload_saes_to_huggingface.py
scripts/ansible/README.md
scripts/ansible/util/cache_acts.py
scripts/ansible/util/train_sae.py
scripts/caching_replication_how_train_saes.py
scripts/replication_how_train_saes_control.py
scripts/replication_how_train_saes.py
scripts/sweep-gpt2-blocks.py
scripts/sweep-gpt2.py
scripts/test-autocast-lm.py
scripts/training_a_sparse_autoencoder_othelloGPT.py
tests/analysis/test_hooked_sae_transformer.py
tests/analysis/test_hooked_sae.py
tests/analysis/test_neuronpedia_integration.py
tests/conftest.py
tests/helpers.py
tests/test_evals.py
tests/toolkit/test_pretrained_sae_loaders.py
tests/toolkit/test_pretrained_saes_directory.py
tests/training/test_activation_functions.py
tests/training/test_activations_store.py
tests/training/test_cache_activations_runner.py
tests/training/test_config.py
tests/training/test_gated_sae.py
tests/training/test_jumprelu_sae.py
tests/training/test_l1_scheduler.py
tests/training/test_load_model.py
tests/training/test_optim.py
tests/training/test_pretokenize_runner.py
tests/training/test_sae_basic.py
tests/training/test_sae_from_pretrained.py
tests/training/test_sae_initialization.py
tests/training/test_sae_trainer.py
tests/training/test_sae_training_runner.py
tests/training/test_sae_training.py
tests/training/test_tokenization_and_batching.py
tests/training/test_training_sae.py
tests/training/test_upload_saes_to_huggingface.py
tutorials/mamba_train_example.py
tutorials/tsea.py

================================================================
Files
================================================================

================
File: benchmark/test_activations_store.py
================
from time import perf_counter
from typing import Any, cast
from datasets import load_dataset
from transformer_lens import HookedTransformer
from sae_lens.config import PretokenizeRunnerConfig
from sae_lens.pretokenize_runner import pretokenize_dataset
from sae_lens.training.activations_store import ActivationsStore
from tests.helpers import build_sae_cfg
# The way to run this with this command:
# poetry run -k test_benchmark_activations_store_get_batch_tokens_pretokenized_vs_raw -ss
def test_benchmark_activations_store_get_batch_tokens_pretokenized_vs_raw():
    model = HookedTransformer.from_pretrained("gpt2")
    tokenizer = model.tokenizer
    assert tokenizer is not None
    cfg = build_sae_cfg(
        model_name="gpt2",
        dataset_path="NeelNanda/c4-10k",
        context_size=512,
    )
    dataset = load_dataset(cfg.dataset_path, split="train")
    pretokenize_cfg = PretokenizeRunnerConfig(
        tokenizer_name="gpt2",
        dataset_path=cfg.dataset_path,
        context_size=cfg.context_size,
        shuffle=False,
        num_proc=1,
        pretokenize_batch_size=None,
    )
    tokenized_dataset = pretokenize_dataset(
        cast(Any, dataset), tokenizer, pretokenize_cfg
    )
    text_dataset_store = ActivationsStore.from_config(
        model, cfg, override_dataset=dataset
    )
    pretokenized_dataset_store = ActivationsStore.from_config(
        model, cfg, override_dataset=tokenized_dataset
    )
    text_start_time = perf_counter()
    text_batch_toks = [text_dataset_store.get_batch_tokens(50) for _ in range(100)]
    text_duration = perf_counter() - text_start_time
    pretokenized_start_time = perf_counter()
    pretokenized_batch_toks = [
        pretokenized_dataset_store.get_batch_tokens(50) for _ in range(100)
    ]
    pretokenized_duration = perf_counter() - pretokenized_start_time
    print(f"get_batch_tokens() duration with text dataset: {text_duration}")
    print(
        f"get_batch_tokens() duration with pretokenized dataset: {pretokenized_duration}"
    )
    for text_toks, pretokenized_toks in zip(text_batch_toks, pretokenized_batch_toks):
        assert text_toks.tolist() == pretokenized_toks.tolist()
    assert pretokenized_duration < text_duration

================
File: benchmark/test_cache_activations_runner.py
================
import math
import os
import shutil
import time
from pathlib import Path
import torch
from safetensors.torch import save_file
from tqdm import trange
from sae_lens.cache_activations_runner import CacheActivationsRunner
from sae_lens.config import DTYPE_MAP, CacheActivationsRunnerConfig
os.environ["WANDB_MODE"] = "offline"  # turn this off if you want to see the output
# The way to run this with this command:
# poetry run py.test benchmark/test_cache_activations_runner.py --profile-svg -s
def test_cache_activations_runner():
    if torch.cuda.is_available():
        device = "cuda"
    elif torch.backends.mps.is_available():
        device = "mps"
    else:
        device = "cpu"
    print("Using device:", device)
    os.environ["TOKENIZERS_PARALLELISM"] = "false"
    activations_save_path = (
        os.path.dirname(os.path.realpath(__file__))
        + "/fixtures/test_activations/gelu_1l"
    )
    # If the directory exists, delete it.
    if os.path.exists(activations_save_path):
        shutil.rmtree(activations_save_path)
    torch.mps.empty_cache()
    cfg = CacheActivationsRunnerConfig(
        new_cached_activations_path=activations_save_path,
        training_tokens=16_000,
        # Pick a tiny model to make this easier.
        model_name="gelu-1l",
        model_batch_size=16,
        ## MLP Layer 0 ##
        hook_name="blocks.0.hook_mlp_out",
        hook_layer=0,
        d_in=512,
        ## Dataset ##
        dataset_path="NeelNanda/c4-tokenized-2b",
        context_size=1024,
        ## Misc ##
        device=device,
        seed=42,
        dtype="float32",
    )
    start_time = time.perf_counter()
    CacheActivationsRunner(cfg).run()
    end_time = time.perf_counter()
    elapsed_time = end_time - start_time
    print(f"Caching activations took: {elapsed_time:.4f}")
def test_hf_dataset_save_vs_safetensors(tmp_path: Path):
    niters = 10
    context_size = 32
    dataset_num_rows = 10_000
    total_training_tokens = dataset_num_rows * context_size
    model_batch_size = 8
    num_buffers = 4 * niters
    ###
    d_in = 512
    dtype = "float32"
    device = (
        "cuda"
        if torch.cuda.is_available()
        else "mps"
        if torch.backends.mps.is_available()
        else "cpu"
    )
    bytes_per_token = d_in * DTYPE_MAP[dtype].itemsize
    tokens_per_buffer = math.ceil(dataset_num_rows * context_size / num_buffers)
    buffer_size_gb = min((tokens_per_buffer * bytes_per_token) / 1_000_000_000, 2.0)
    cfg = CacheActivationsRunnerConfig(
        new_cached_activations_path=str(tmp_path),
        dataset_path="NeelNanda/c4-tokenized-2b",
        model_name="gelu-1l",
        hook_name="blocks.0.hook_mlp_out",
        hook_layer=0,
        d_in=d_in,
        context_size=context_size,
        training_tokens=total_training_tokens,
        model_batch_size=model_batch_size,
        buffer_size_gb=buffer_size_gb,
        prepend_bos=False,
        device=device,
        seed=42,
        dtype=dtype,
    )
    runner = CacheActivationsRunner(cfg)
    store = runner.activations_store
    ###
    safetensors_path = tmp_path / "saftensors"
    hf_path = tmp_path / "hf"
    safetensors_path.mkdir()
    hf_path.mkdir()
    print("Warmup")
    for i in trange(niters // 2, leave=False):
        buffer = store.get_buffer(cfg.n_batches_in_buffer)
    start_time = time.perf_counter()
    for i in trange(niters, leave=False):
        buffer = store.get_buffer(cfg.n_batches_in_buffer)
    end_time = time.perf_counter()
    print(f"No saving took: {end_time - start_time:.4f}")
    start_time = time.perf_counter()
    for i in trange(niters, leave=False):
        buffer = store.get_buffer(cfg.n_batches_in_buffer)
        shard = runner._create_shard(buffer)
        shard.save_to_disk(hf_path / str(i), num_shards=1)
    end_time = time.perf_counter()
    elapsed_time = end_time - start_time
    print(f"HF Dataset took: {elapsed_time:.4f}", flush=True)
    hf_size = sum(f.stat().st_size for f in hf_path.glob("**/*") if f.is_file())
    print(f"HF Dataset size: {hf_size / (1024 * 1024):.2f} MB")
    start_time = time.perf_counter()
    for i in trange(niters, leave=False):
        buffer = store.get_buffer(cfg.n_batches_in_buffer)[0]
        save_file({"activations": buffer}, safetensors_path / f"{i}.safetensors")
    end_time = time.perf_counter()
    elapsed_time = end_time - start_time
    print(f"Safetensors took: {elapsed_time:.4f}")
    safetensors_size = sum(
        f.stat().st_size for f in safetensors_path.glob("**/*") if f.is_file()
    )
    print(f"Safetensors size: {safetensors_size / (1024 * 1024):.2f} MB")

================
File: benchmark/test_eval_all_loadable_saes.py
================
# import pandas as pd
# import plotly.express as px
# import numpy as np
import argparse
import json
from pathlib import Path
import pytest
import torch
from sae_lens import SAE, ActivationsStore
from sae_lens.analysis.neuronpedia_integration import open_neuronpedia_feature_dashboard
from sae_lens.evals import (
    all_loadable_saes,
    get_eval_everything_config,
    process_results,
    run_evals,
    run_evaluations,
)
from sae_lens.toolkit.pretrained_sae_loaders import (
    SAEConfigLoadOptions,
    get_sae_config_from_hf,
)
from tests.helpers import load_model_cached
# from sae_lens.evals import run_evals
# from transformer_lens import HookedTransformer
torch.set_grad_enabled(False)
example_text = """
Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say
that they were perfectly normal, thank you very much. They were the last
people you'd expect to be involved in anything strange or mysterious,
because they just didn't hold with such nonsense.
"""
def test_get_sae_config():
    repo_id = "jbloom/GPT2-Small-SAEs-Reformatted"
    cfg = get_sae_config_from_hf(
        repo_id=repo_id,
        folder_name="blocks.0.hook_resid_pre",
        options=SAEConfigLoadOptions(),
    )
    assert cfg is not None
@pytest.mark.parametrize(
    "release, sae_name, expected_var_explained, expected_l0", all_loadable_saes()
)
def test_loading_pretrained_saes(
    release: str,
    sae_name: str,
    expected_var_explained: float,  # noqa: ARG001
    expected_l0: float,  # noqa: ARG001
):
    if torch.cuda.is_available():
        device = "cuda"
    elif torch.backends.mps.is_available():
        device = "mps"
    else:
        device = "cpu"
    sae, _, _ = SAE.from_pretrained(release, sae_name, device=device)
    assert isinstance(sae, SAE)
@pytest.mark.parametrize(
    "release, sae_name, expected_var_explained, expected_l0", all_loadable_saes()
)
def test_loading_pretrained_saes_open_neuronpedia(
    release: str,
    sae_name: str,
    expected_var_explained: float,  # noqa: ARG001
    expected_l0: float,  # noqa: ARG001
):
    if torch.cuda.is_available():
        device = "cuda"
    elif torch.backends.mps.is_available():
        device = "mps"
    else:
        device = "cpu"
    sae, _, _ = SAE.from_pretrained(release, sae_name, device=device)
    assert isinstance(sae, SAE)
    open_neuronpedia_feature_dashboard(sae, 0)
@pytest.mark.parametrize(
    "release, sae_name, expected_var_explained, expected_l0", all_loadable_saes()
)
def test_loading_pretrained_saes_do_forward_pass(
    release: str,
    sae_name: str,
    expected_var_explained: float,  # noqa: ARG001
    expected_l0: float,  # noqa: ARG001
):
    if torch.cuda.is_available():
        device = "cuda"
    elif torch.backends.mps.is_available():
        device = "mps"
    else:
        device = "cpu"
    sae, _, _ = SAE.from_pretrained(release, sae_name, device=device)
    assert isinstance(sae, SAE)
    # from transformer_lens import HookedTransformer
    # model = HookedTransformer.from_pretrained("gemma-2-9b")
    # sae_in = model.run_with_cache("test test test")[1][sae.cfg.hook_name]
    if "hook_z" in sae.cfg.hook_name:
        # check that reshaping works as intended
        from transformer_lens.loading_from_pretrained import get_pretrained_model_config
        model_cfg = get_pretrained_model_config(sae.cfg.model_name)
        sae_in = torch.randn(1, 4, model_cfg.n_heads, model_cfg.d_head).to(device)
        sae_out = sae(sae_in)
        assert sae_out.shape == sae_in.shape
    sae.turn_off_forward_pass_hook_z_reshaping()  # just in case
    sae_in = torch.randn(1, sae.cfg.d_in).to(device)
    sae_out = sae(sae_in)
    assert sae_out.shape == sae_in.shape
    assert True  # If we get here, we're good
@pytest.mark.parametrize(
    "release, sae_name, expected_var_explained, expected_l0", all_loadable_saes()
)
def test_eval_all_loadable_saes(
    release: str, sae_name: str, expected_var_explained: float, expected_l0: float
):
    """This test is currently only passing for a subset of SAEs because we need to
    have the normalization factors on hand to normalize the activations. We should
    really fold these into SAEs so this test is easy to do."""
    if torch.cuda.is_available():
        device = "cuda"
    elif torch.backends.mps.is_available():
        device = "mps"
    else:
        device = "cpu"
    sae, _, _ = SAE.from_pretrained(release, sae_name, device=device)
    sae.fold_W_dec_norm()
    model = load_model_cached(sae.cfg.model_name)
    model.to(device)
    activation_store = ActivationsStore.from_sae(
        model=model,
        sae=sae,
        streaming=True,
        # fairly conservative parameters here so can use same for larger
        # models without running out of memory.
        store_batch_size_prompts=8,
        train_batch_size_tokens=4096,
        n_batches_in_buffer=4,
        device=device,
    )
    eval_config = get_eval_everything_config(
        batch_size_prompts=8,
        n_eval_reconstruction_batches=3,
        n_eval_sparsity_variance_batches=100,
    )
    metrics, _ = run_evals(
        sae=sae,
        activation_store=activation_store,
        model=model,
        eval_config=eval_config,
        ignore_tokens={
            model.tokenizer.pad_token_id,  # type: ignore
            model.tokenizer.eos_token_id,  # type: ignore
            model.tokenizer.bos_token_id,  # type: ignore
        },
    )
    assert pytest.approx(metrics["l0"], abs=5) == expected_l0
    assert (
        pytest.approx(metrics["explained_variance"], abs=0.1) == expected_var_explained
    )
@pytest.fixture
def mock_evals_simple_args(tmp_path: Path):
    class Args:
        sae_regex_pattern = "gpt2-small-res-jb"
        sae_block_pattern = "blocks.0.hook_resid_pre"
        num_eval_batches = 1
        n_eval_reconstruction_batches = 1
        n_eval_sparsity_variance_batches = 1
        eval_batch_size_prompts = 2
        datasets = ["Skylion007/openwebtext"]
        ctx_lens = [128]
        output_dir = str(tmp_path)
        verbose = False
    return Args()
def test_run_evaluations_process_results(mock_evals_simple_args: argparse.Namespace):
    """
    This test is more like an acceptance test for the evals code than a benchmark.
    """
    eval_results = run_evaluations(mock_evals_simple_args)
    output_files = process_results(eval_results, mock_evals_simple_args.output_dir)
    print("Evaluation complete. Output files:")
    print(f"Individual JSONs: {len(output_files['individual_jsons'])}")  # type: ignore
    print(f"Combined JSON: {output_files['combined_json']}")
    print(f"CSV: {output_files['csv']}")
    # open and validate the files
    combined_json_path = output_files["combined_json"]
    assert isinstance(combined_json_path, Path)
    assert combined_json_path.exists()
    with open(combined_json_path) as f:
        data = json.load(f)[0]
        assert "metrics" in data
        assert "feature_metrics" in data

================
File: benchmark/test_language_model_sae_runner_multiple_devices.py
================
import torch
from sae_lens.config import LanguageModelSAERunnerConfig
from sae_lens.sae_training_runner import SAETrainingRunner
# os.environ["WANDB_MODE"] = "offline"  # turn this off if you want to see the output
if torch.cuda.is_available():
    device = "cuda"
elif torch.backends.mps.is_available():
    device = "mps"
else:
    device = "cpu"
# total_training_steps = 20_000
total_training_steps = 500
batch_size = 4096
total_training_tokens = total_training_steps * batch_size
print(f"Total Training Tokens: {total_training_tokens}")
lr_warm_up_steps = 0
lr_decay_steps = 40_000
print(f"lr_decay_steps: {lr_decay_steps}")
l1_warmup_steps = 10_000
print(f"l1_warmup_steps: {l1_warmup_steps}")
BASE_CFG = dict(
    # Pick a tiny model to make this easier.
    model_name="gelu-1l",
    ## MLP Layer 0 ##
    hook_name="blocks.0.hook_mlp_out",
    hook_layer=0,
    d_in=512,
    dataset_path="NeelNanda/c4-tokenized-2b",
    context_size=256,
    is_dataset_tokenized=True,
    prepend_bos=True,  # I used to train GPT2 SAEs with a prepended-bos but no longer think we should do this.
    # How big do we want our SAE to be?
    expansion_factor=16,
    # Dataset / Activation Store
    # When we do a proper test
    # training_tokens= 820_000_000, # 200k steps * 4096 batch size ~ 820M tokens (doable overnight on an A100)
    # For now.
    training_tokens=total_training_tokens,  # For initial testing I think this is a good number.
    train_batch_size_tokens=4096,
    # Loss Function
    ## Reconstruction Coefficient.
    mse_loss_normalization=None,  # MSE Loss Normalization is not mentioned (so we use stanrd MSE Loss). But not we take an average over the batch.
    ## Anthropic does not mention using an Lp norm other than L1.
    l1_coefficient=5,
    lp_norm=1.0,
    # Instead, they multiply the L1 loss contribution
    # from each feature of the activations by the decoder norm of the corresponding feature.
    scale_sparsity_penalty_by_decoder_norm=True,
    # Learning Rate
    lr_scheduler_name="constant",  # we set this independently of warmup and decay steps.
    l1_warm_up_steps=l1_warmup_steps,
    lr_warm_up_steps=lr_warm_up_steps,
    lr_decay_steps=lr_warm_up_steps,
    ## No ghost grad term.
    use_ghost_grads=False,
    # Initialization / Architecture
    apply_b_dec_to_input=False,
    # encoder bias zero's. (I'm not sure what it is by default now)
    # decoder bias zero's.
    b_dec_init_method="zeros",
    normalize_sae_decoder=False,
    decoder_heuristic_init=True,
    init_encoder_as_decoder_transpose=True,
    # Optimizer
    lr=4e-5,
    ## adam optimizer has no weight decay by default so worry about this.
    adam_beta1=0.9,
    adam_beta2=0.999,
    # Buffer details won't matter in we cache / shuffle our activations ahead of time.
    n_batches_in_buffer=16,
    store_batch_size_prompts=4,
    normalize_activations="constant_norm_rescale",
    n_eval_batches=3,
    eval_batch_size_prompts=4,
    # Feature Store
    feature_sampling_window=1000,
    dead_feature_window=1000,
    dead_feature_threshold=1e-4,
    # performance enhancement:
    compile_sae=False,
    # WANDB
    log_to_wandb=True,  # always use wandb unless you are just testing code.
    wandb_project="benchmark",
    wandb_log_frequency=100,
    # Misc
    device=device,
    seed=42,
    n_checkpoints=0,
    checkpoint_path="checkpoints",
    dtype="float32",
)
# The way to run this with this command:
def test_sae_runner_multiple_devices():
    cfg_dict = BASE_CFG
    cfg_dict["model_name"] = "gemma-2b"
    cfg_dict["d_in"] = 2048
    cfg_dict["device"] = "cuda:3"
    cfg_dict["model_from_pretrained_kwargs"] = {  # type: ignore
        "n_devices": torch.cuda.device_count() - 1
    }
    cfg_dict["act_store_device"] = "cpu"
    cfg_dict["dtype"] = "torch.bfloat16"
    cfg_dict["eval_every_n_wandb_logs"] = 3
    cfg = LanguageModelSAERunnerConfig(**cfg_dict)  # type: ignore
    # look at the next cell to see some instruction for what to do while this is running.
    sae = SAETrainingRunner(cfg).run()
    assert sae is not None
    # know whether or not this works by looking at the dashboard!
# genuinely a bit faster!
def test_sae_runner_multiple_devices_sae_act_store_on_gpus():
    cfg_dict = BASE_CFG
    cfg_dict["model_name"] = "gemma-2b"
    cfg_dict["d_in"] = 2048
    cfg_dict["device"] = "cuda:2"
    cfg_dict["model_from_pretrained_kwargs"] = {  # type: ignore
        "n_devices": torch.cuda.device_count() - 2
    }
    cfg_dict["act_store_device"] = "cuda:3"
    cfg_dict["dtype"] = "torch.bfloat16"
    cfg_dict["eval_every_n_wandb_logs"] = 3
    cfg = LanguageModelSAERunnerConfig(**cfg_dict)  # type: ignore
    # look at the next cell to see some instruction for what to do while this is running.
    sae = SAETrainingRunner(cfg).run()
    assert sae is not None
    # know whether or not this works by looking at the dashboard!

================
File: benchmark/test_language_model_sae_runner.py
================
import torch
from sae_lens.config import LanguageModelSAERunnerConfig
from sae_lens.sae_training_runner import SAETrainingRunner
# os.environ["WANDB_MODE"] = "offline"  # turn this off if you want to see the output
# The way to run this with this command:
# poetry run py.test tests/benchmark/test_language_model_sae_runner.py --profile-svg -s
def test_language_model_sae_runner():
    if torch.cuda.is_available():
        device = "cuda"
    elif torch.backends.mps.is_available():
        device = "mps"
    else:
        device = "cpu"
    # total_training_steps = 20_000
    total_training_steps = 500
    batch_size = 4096
    total_training_tokens = total_training_steps * batch_size
    print(f"Total Training Tokens: {total_training_tokens}")
    lr_warm_up_steps = 0
    lr_decay_steps = 40_000
    print(f"lr_decay_steps: {lr_decay_steps}")
    l1_warmup_steps = 10_000
    print(f"l1_warmup_steps: {l1_warmup_steps}")
    cfg = LanguageModelSAERunnerConfig(
        # Pick a tiny model to make this easier.
        model_name="gelu-1l",
        ## MLP Layer 0 ##
        hook_name="blocks.0.hook_mlp_out",
        hook_layer=0,
        d_in=512,
        dataset_path="NeelNanda/c4-tokenized-2b",
        context_size=256,
        is_dataset_tokenized=True,
        prepend_bos=True,  # I used to train GPT2 SAEs with a prepended-bos but no longer think we should do this.
        # How big do we want our SAE to be?
        expansion_factor=16,
        # Dataset / Activation Store
        # When we do a proper test
        # training_tokens= 820_000_000, # 200k steps * 4096 batch size ~ 820M tokens (doable overnight on an A100)
        # For now.
        training_tokens=total_training_tokens,  # For initial testing I think this is a good number.
        train_batch_size_tokens=4096,
        # Loss Function
        ## Reconstruction Coefficient.
        mse_loss_normalization=None,  # MSE Loss Normalization is not mentioned (so we use stanrd MSE Loss). But not we take an average over the batch.
        ## Anthropic does not mention using an Lp norm other than L1.
        l1_coefficient=5,
        lp_norm=1.0,
        # Instead, they multiply the L1 loss contribution
        # from each feature of the activations by the decoder norm of the corresponding feature.
        scale_sparsity_penalty_by_decoder_norm=True,
        # Learning Rate
        lr_scheduler_name="constant",  # we set this independently of warmup and decay steps.
        l1_warm_up_steps=l1_warmup_steps,
        lr_warm_up_steps=lr_warm_up_steps,
        lr_decay_steps=lr_warm_up_steps,
        ## No ghost grad term.
        use_ghost_grads=False,
        # Initialization / Architecture
        apply_b_dec_to_input=False,
        # encoder bias zero's. (I'm not sure what it is by default now)
        # decoder bias zero's.
        b_dec_init_method="zeros",
        normalize_sae_decoder=False,
        decoder_heuristic_init=True,
        init_encoder_as_decoder_transpose=True,
        # Optimizer
        lr=4e-5,
        ## adam optimizer has no weight decay by default so worry about this.
        adam_beta1=0.9,
        adam_beta2=0.999,
        # Buffer details won't matter in we cache / shuffle our activations ahead of time.
        n_batches_in_buffer=64,
        store_batch_size_prompts=16,
        normalize_activations="none",
        # Feature Store
        feature_sampling_window=1000,
        dead_feature_window=1000,
        dead_feature_threshold=1e-4,
        # performance enhancement:
        compile_sae=False,
        # WANDB
        log_to_wandb=True,  # always use wandb unless you are just testing code.
        wandb_project="benchmark",
        wandb_log_frequency=100,
        # Misc
        device=device,
        seed=42,
        n_checkpoints=0,
        checkpoint_path="checkpoints",
        dtype="float32",
    )
    # look at the next cell to see some instruction for what to do while this is running.
    sae = SAETrainingRunner(cfg).run()
    assert sae is not None
    # know whether or not this works by looking at the dashboard!
def test_language_model_sae_runner_gated():
    if torch.cuda.is_available():
        device = "cuda"
    elif torch.backends.mps.is_available():
        device = "mps"
    else:
        device = "cpu"
    # total_training_steps = 20_000
    total_training_steps = 500
    batch_size = 4096
    total_training_tokens = total_training_steps * batch_size
    print(f"Total Training Tokens: {total_training_tokens}")
    lr_warm_up_steps = 0
    lr_decay_steps = 40_000
    print(f"lr_decay_steps: {lr_decay_steps}")
    l1_warmup_steps = 10_000
    print(f"l1_warmup_steps: {l1_warmup_steps}")
    cfg = LanguageModelSAERunnerConfig(
        # Pick a tiny model to make this easier.
        model_name="gelu-1l",
        architecture="gated",
        ## MLP Layer 0 ##
        hook_name="blocks.0.hook_mlp_out",
        hook_layer=0,
        d_in=512,
        dataset_path="NeelNanda/c4-tokenized-2b",
        context_size=256,
        is_dataset_tokenized=True,
        prepend_bos=True,  # I used to train GPT2 SAEs with a prepended-bos but no longer think we should do this.
        # How big do we want our SAE to be?
        expansion_factor=16,
        # Dataset / Activation Store
        # When we do a proper test
        # training_tokens= 820_000_000, # 200k steps * 4096 batch size ~ 820M tokens (doable overnight on an A100)
        # For now.
        training_tokens=total_training_tokens,  # For initial testing I think this is a good number.
        train_batch_size_tokens=4096,
        # Loss Function
        ## Reconstruction Coefficient.
        mse_loss_normalization=None,  # MSE Loss Normalization is not mentioned (so we use stanrd MSE Loss). But not we take an average over the batch.
        ## Anthropic does not mention using an Lp norm other than L1.
        l1_coefficient=5,
        lp_norm=1.0,
        # Instead, they multiply the L1 loss contribution
        # from each feature of the activations by the decoder norm of the corresponding feature.
        scale_sparsity_penalty_by_decoder_norm=True,
        # Learning Rate
        lr_scheduler_name="constant",  # we set this independently of warmup and decay steps.
        l1_warm_up_steps=l1_warmup_steps,
        lr_warm_up_steps=lr_warm_up_steps,
        lr_decay_steps=lr_warm_up_steps,
        ## No ghost grad term.
        use_ghost_grads=False,
        # Initialization / Architecture
        apply_b_dec_to_input=False,
        # encoder bias zero's. (I'm not sure what it is by default now)
        # decoder bias zero's.
        b_dec_init_method="zeros",
        normalize_sae_decoder=False,
        decoder_heuristic_init=True,
        init_encoder_as_decoder_transpose=True,
        # Optimizer
        lr=4e-5,
        ## adam optimizer has no weight decay by default so worry about this.
        adam_beta1=0.9,
        adam_beta2=0.999,
        # Buffer details won't matter in we cache / shuffle our activations ahead of time.
        n_batches_in_buffer=64,
        store_batch_size_prompts=16,
        normalize_activations="none",
        # Feature Store
        feature_sampling_window=1000,
        dead_feature_window=1000,
        dead_feature_threshold=1e-4,
        # performance enhancement:
        compile_sae=False,
        # WANDB
        log_to_wandb=True,  # always use wandb unless you are just testing code.
        wandb_project="benchmark",
        wandb_log_frequency=100,
        # Misc
        device=device,
        seed=42,
        n_checkpoints=0,
        checkpoint_path="checkpoints",
        dtype="float32",
    )
    # look at the next cell to see some instruction for what to do while this is running.
    sae = SAETrainingRunner(cfg).run()
    assert sae is not None
    # know whether or not this works by looking at the dashboard!
def test_language_model_sae_runner_top_k():
    if torch.cuda.is_available():
        device = "cuda"
    elif torch.backends.mps.is_available():
        device = "mps"
    else:
        device = "cpu"
    # total_training_steps = 20_000
    total_training_steps = 500
    batch_size = 4096
    total_training_tokens = total_training_steps * batch_size
    print(f"Total Training Tokens: {total_training_tokens}")
    lr_warm_up_steps = 0
    lr_decay_steps = 40_000
    print(f"lr_decay_steps: {lr_decay_steps}")
    l1_warmup_steps = 10_000
    print(f"l1_warmup_steps: {l1_warmup_steps}")
    cfg = LanguageModelSAERunnerConfig(
        activation_fn="topk",
        activation_fn_kwargs={"k": 32},
        normalize_activations="layer_norm",
        # Pick a tiny model to make this easier.
        model_name="gelu-1l",
        ## MLP Layer 0 ##
        hook_name="blocks.0.hook_mlp_out",
        hook_layer=0,
        d_in=512,
        dataset_path="NeelNanda/c4-tokenized-2b",
        context_size=256,
        is_dataset_tokenized=True,
        prepend_bos=True,  # I used to train GPT2 SAEs with a prepended-bos but no longer think we should do this.
        # How big do we want our SAE to be?
        expansion_factor=16,
        training_tokens=total_training_tokens,  # For initial testing I think this is a good number.
        train_batch_size_tokens=4096,
        # Loss Function
        ## Reconstruction Coefficient.
        mse_loss_normalization=None,  # MSE Loss Normalization is not mentioned (so we use stanrd MSE Loss). But not we take an average over the batch.
        ## Anthropic does not mention using an Lp norm other than L1.
        l1_coefficient=5,
        lp_norm=1.0,
        # Instead, they multiply the L1 loss contribution
        # from each feature of the activations by the decoder norm of the corresponding feature.
        scale_sparsity_penalty_by_decoder_norm=False,
        # Learning Rate
        lr_scheduler_name="constant",  # we set this independently of warmup and decay steps.
        l1_warm_up_steps=l1_warmup_steps,
        lr_warm_up_steps=lr_warm_up_steps,
        lr_decay_steps=lr_warm_up_steps,
        ## No ghost grad term.
        apply_b_dec_to_input=True,
        b_dec_init_method="geometric_median",
        normalize_sae_decoder=True,
        decoder_heuristic_init=False,
        init_encoder_as_decoder_transpose=True,
        # Optimizer
        lr=4e-5,
        ## adam optimizer has no weight decay by default so worry about this.
        adam_beta1=0.9,
        adam_beta2=0.999,
        # Buffer details won't matter in we cache / shuffle our activations ahead of time.
        n_batches_in_buffer=64,
        store_batch_size_prompts=16,
        # Feature Store
        feature_sampling_window=1000,
        dead_feature_window=1000,
        dead_feature_threshold=1e-4,
        # performance enhancement:
        compile_sae=False,
        # WANDB
        log_to_wandb=True,  # always use wandb unless you are just testing code.
        wandb_project="benchmark",
        wandb_log_frequency=100,
        # Misc
        device=device,
        seed=42,
        n_checkpoints=0,
        checkpoint_path="checkpoints",
        dtype="float32",
    )
    # look at the next cell to see some instruction for what to do while this is running.
    sae = SAETrainingRunner(cfg).run()
    assert sae is not None
    # know whether or not this works by looking at the dashboard!
def test_language_model_sae_runner_othellogpt():
    if torch.cuda.is_available():
        device = "cuda"
    elif torch.backends.mps.is_available():
        device = "mps"
    else:
        device = "cpu"
    # total_training_steps = 20_000
    total_training_steps = 500
    batch_size = 4096
    total_training_tokens = total_training_steps * batch_size
    lr_warm_up_steps = 0
    lr_decay_steps = 40_000
    l1_warmup_steps = 10_000
    cfg = LanguageModelSAERunnerConfig(
        # Data Generating Function (Model + Training Distibuion)
        model_name="othello-gpt",  # othello-gpt model
        hook_name="blocks.6.hook_resid_pre",  # A valid hook point (see more details here: https://neelnanda-io.github.io/TransformerLens/generated/demos/Main_Demo.html#Hook-Points)
        hook_layer=6,  # Only one layer in the model.
        d_in=512,  # the width of the mlp output.
        dataset_path="taufeeque/othellogpt",  # this is a tokenized language dataset on Huggingface for OthelloGPT games.
        is_dataset_tokenized=True,
        streaming=True,  # we could pre-download the token dataset if it was small.
        # SAE Parameters
        mse_loss_normalization=None,  # We won't normalize the mse loss,
        expansion_factor=16,  # the width of the SAE. Larger will result in better stats but slower training.
        b_dec_init_method="geometric_median",  # The geometric median can be used to initialize the decoder weights.
        apply_b_dec_to_input=False,  # We won't apply the decoder weights to the input.
        normalize_sae_decoder=False,
        scale_sparsity_penalty_by_decoder_norm=True,
        decoder_heuristic_init=True,
        init_encoder_as_decoder_transpose=True,
        normalize_activations="expected_average_only_in",
        # Training Parameters
        lr=0.00003,  # lower the better, we'll go fairly high to speed up the tutorial.
        adam_beta1=0.9,  # adam params (default, but once upon a time we experimented with these.)
        adam_beta2=0.999,
        lr_scheduler_name="constant",  # constant learning rate with warmup. Could be better schedules out there.
        lr_warm_up_steps=lr_warm_up_steps,  # this can help avoid too many dead features initially.
        lr_decay_steps=lr_decay_steps,  # this will help us avoid overfitting.
        l1_coefficient=0.001,  # will control how sparse the feature activations are
        l1_warm_up_steps=l1_warmup_steps,  # this can help avoid too many dead features initially.
        lp_norm=1.0,  # the L1 penalty (and not a Lp for p < 1)
        train_batch_size_tokens=batch_size,
        context_size=59,  # will control the length of the prompts we feed to the model. Larger is better but slower. so for the tutorial we'll use a short one.
        seqpos_slice=(5, -5),
        # Activation Store Parameters
        n_batches_in_buffer=32,  # controls how many activations we store / shuffle.
        training_tokens=total_training_tokens,  # 100 million tokens is quite a few, but we want to see good stats. Get a coffee, come back.
        store_batch_size_prompts=32,
        # Resampling protocol
        use_ghost_grads=False,  # we don't use ghost grads anymore.
        feature_sampling_window=500,  # this controls our reporting of feature sparsity stats
        dead_feature_window=1000000,  # would effect resampling or ghost grads if we were using it.
        dead_feature_threshold=1e-4,  # would effect resampling or ghost grads if we were using it.
        # WANDB
        log_to_wandb=False,  # always use wandb unless you are just testing code.
        wandb_project="benchmark",
        wandb_log_frequency=100,
        eval_every_n_wandb_logs=20,
        # Misc
        device=device,
        seed=42,
        n_checkpoints=0,
        checkpoint_path="checkpoints",
        dtype="torch.float32",
    )
    # look at the next cell to see some instruction for what to do while this is running.
    sae = SAETrainingRunner(cfg).run()
    assert sae is not None
    # know whether or not this works by looking at the dashboard!

================
File: docs/api.md
================
# API

::: sae_lens

================
File: docs/citation.md
================
# Citation

```
@misc{bloom2024saetrainingcodebase,
   title = {SAELens},
   author = {Joseph Bloom, Curt Tigges and David Chanin},
   year = {2024},
   howpublished = {\url{https://github.com/jbloomAus/SAELens}},
}}
```

================
File: docs/contributing.md
================
# Contributing

Contributions are welcome! To get setup for development, follow the instructions below.

## Setup

Make sure you have [poetry](https://python-poetry.org/) installed, clone the repository, and install dependencies with:

```bash
git clone https://github.com/jbloomAus/SAELens.git # we recommend you make a fork for submitting PR's and clone that!
poetry lock # can take a while.
poetry install 
make check-ci # validate the install
```

## Testing, Linting, and Formatting

This project uses [pytest](https://docs.pytest.org/en/stable/) for testing, [pyright](https://github.com/microsoft/pyright) for type-checking, and [Ruff](https://docs.astral.sh/ruff/) for formatting and linting.

If you add new code, it would be greatly appreciated if you could add tests in the `tests` directory. You can run the tests with:

```bash
make test
```

Before commiting, make sure you format the code with:

```bash
make format
```

Finally, run all CI checks locally with:

```bash
make check-ci
```

If these pass, you're good to go! Open a pull request with your changes.

## Documentation

This project uses [mkdocs](https://www.mkdocs.org/) for documentation. You can see the docs locally with:

```bash
make docs-serve
```
If you make changes to code which requires updating documentation, it would be greatly appreciated if you could update the docs as well.

================
File: docs/feature_dashboards.md
================
## Example Output

Here's one feature we found in the residual stream of Layer 10 of GPT-2 Small:

![alt text](../../content/readme_screenshot_predict_pronoun_feature.png). Open `gpt2_resid_pre10_predict_pronoun_feature.html` in your browser to interact with the dashboard (WIP).

Note, probably this feature could split into more mono-semantic features in a larger SAE that had been trained for longer. (this was was only about 49152 features trained on 10M tokens from OpenWebText).

================
File: docs/generate_sae_table.py
================
# type: ignore
# ruff: noqa: T201
from pathlib import Path
from textwrap import dedent
import pandas as pd
import yaml
from tqdm import tqdm
from sae_lens import SAEConfig
from sae_lens.toolkit.pretrained_sae_loaders import (
    SAEConfigLoadOptions,
    get_sae_config,
    handle_config_defaulting,
)
INCLUDED_CFG = [
    "id",
    "architecture",
    "neuronpedia",
    "hook_name",
    "hook_layer",
    "d_sae",
    "context_size",
    "dataset_path",
    "normalize_activations",
]
def on_pre_build(config):  # noqa: ARG001
    print("Generating SAE table...")
    generate_sae_table()
    print("SAE table generation complete.")
def generate_sae_table():
    # Read the YAML file
    yaml_path = Path("sae_lens/pretrained_saes.yaml")
    with open(yaml_path) as file:
        data = yaml.safe_load(file)
    # Start the Markdown content
    markdown_content = "# Pretrained SAEs\n\n"
    markdown_content += "This is a list of SAEs importable from the SAELens package. Click on each link for more details.\n\n"  # Added newline
    markdown_content += "*This file contains the contents of `sae_lens/pretrained_saes.yaml` in Markdown*\n\n"
    # Generate content for each model
    for release, model_info in tqdm(data.items()):
        repo_link = f"https://huggingface.co/{model_info['repo_id']}"
        markdown_content += f"## [{release}]({repo_link})\n\n"
        markdown_content += f"- **Huggingface Repo**: {model_info['repo_id']}\n"
        markdown_content += f"- **model**: {model_info['model']}\n"
        if "links" in model_info:
            markdown_content += "- **Additional Links**:\n"
            for link_type, url in model_info["links"].items():
                markdown_content += f"    - [{link_type.capitalize()}]({url})\n"
        markdown_content += "\n"
        for info in tqdm(model_info["saes"]):
            # can remove this by explicitly overriding config in yaml. Do this later.
            sae_id = info["id"]
            cfg = get_sae_config(
                release,
                sae_id=sae_id,
                options=SAEConfigLoadOptions(),
            )
            cfg = handle_config_defaulting(cfg)
            cfg = SAEConfig.from_dict(cfg).to_dict()
            if "neuronpedia" not in info:
                info["neuronpedia"] = None
            info.update(cfg)
        df = pd.DataFrame(model_info["saes"])
        # Keep only 'id' and 'path' columns
        df = df[INCLUDED_CFG]
        def style_id(val):
            return f"<div>{val}</div><a class=\"saetable-loadSaeId\" onclick=\"SaeTable.showCode('{release}', '{val}')\">Load this SAE</a>"
        df["id"] = df["id"].apply(style_id)
        table = df.to_markdown(index=False)
        markdown_content += table + "\n\n"
    markdown_content += dedent(
        """
        <div id="codeModal" class="saetable-modal">
            <div class="saetable-modalContent">
                <span class="saetable-close" onclick="SaeTable.closeCode()">&times;</span>
                <pre><code id="codeContent" onclick="SaeTable.selectCode(this)"></code></pre>
                <button onclick="SaeTable.copyCode()" class="saetable-copyButton">Copy Code</button>
            </div>
        </div>
        """
    )
    # Write the content to a Markdown file
    output_path = Path("docs/sae_table.md")
    with open(output_path, "w") as file:
        file.write(markdown_content)
if __name__ == "__main__":
    generate_sae_table()

================
File: docs/index.md
================
<img width="1308" alt="Screenshot 2024-03-21 at 3 08 28 pm" src="https://github.com/jbloomAus/mats_sae_training/assets/69127271/209012ec-a779-4036-b4be-7b7739ea87f6">

# SAELens
[![PyPI](https://img.shields.io/pypi/v/sae-lens?color=blue)](https://pypi.org/project/sae-lens/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![build](https://github.com/jbloomAus/SAELens/actions/workflows/build.yml/badge.svg)](https://github.com/jbloomAus/SAELens/actions/workflows/build.yml)
[![Deploy Docs](https://github.com/jbloomAus/SAELens/actions/workflows/deploy_docs.yml/badge.svg)](https://github.com/jbloomAus/SAELens/actions/workflows/deploy_docs.yml)
[![codecov](https://codecov.io/gh/jbloomAus/SAELens/graph/badge.svg?token=N83NGH8CGE)](https://codecov.io/gh/jbloomAus/SAELens)

The SAELens training codebase exists to help researchers:

- Train sparse autoencoders.
- Analyse sparse autoencoders and neural network internals.
- Generate insights which make it easier to create safe and aligned AI systems.

**Please note these docs are in beta. We intend to make them cleaner and more comprehensive over time.**

## Quick Start

### Installation

```
pip install sae-lens
```

### Loading Sparse Autoencoders from Huggingface

To load a pretrained sparse autoencoder, you can use `SAE.from_pretrained()` as below. Note that we return the *original cfg dict* from the huggingface repo so that it's easy to debug older configs that are being handled when we import an SAe. We also return a sparsity tensor if it is present in the repo. For an example repo structure, see [here](https://huggingface.co/jbloom/Gemma-2b-Residual-Stream-SAEs). 

```python
from sae_lens import SAE

sae, cfg_dict, sparsity = SAE.from_pretrained(
    release = "gpt2-small-res-jb", # see other options in sae_lens/pretrained_saes.yaml
    sae_id = "blocks.8.hook_resid_pre", # won't always be a hook point
    device = device
)
```

You can see other importable SAEs on [this page](https://jbloomaus.github.io/SAELens/sae_table/).

### Background and further Readings

We highly recommend this [tutorial](https://www.lesswrong.com/posts/LnHowHgmrMbWtpkxx/intro-to-superposition-and-sparse-autoencoders-colab).

For recent progress in SAEs, we recommend the LessWrong forum's [Sparse Autoencoder tag](https://www.lesswrong.com/tag/sparse-autoencoders-saes)

## Tutorials

I wrote a tutorial to show users how to do some basic exploration of their SAE:

- Loading and Analysing Pre-Trained Sparse Autoencoders [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://githubtocolab.com/jbloomAus/SAELens/blob/main/tutorials/basic_loading_and_analysing.ipynb)
 - Understanding SAE Features with the Logit Lens [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://githubtocolab.com/jbloomAus/SAELens/blob/main/tutorials/logits_lens_with_features.ipynb)
  - Training a Sparse Autoencoder [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://githubtocolab.com/jbloomAus/SAELens/blob/main/tutorials/training_a_sparse_autoencoder.ipynb)


## Example WandB Dashboard

WandB Dashboards provide lots of useful insights while training SAE's. Here's a screenshot from one training run. 

![screenshot](dashboard_screenshot.png)

## Citation

```
@misc{bloom2024saetrainingcodebase,
   title = {SAELens},
   author = {Joseph Bloom, Curt Tigges and David Chanin},
   year = {2024},
   howpublished = {\url{https://github.com/jbloomAus/SAELens}},
}}
```

================
File: docs/roadmap.md
================
# Roadmap

### Motivation

- **Accelerate SAE Research**: Support fast experimentation to understand SAEs and improve SAE training so we can train SAEs on larger and more diverse models.
- **Make Research like Play**: Support research into language model internals via SAEs. Good tooling can make research tremendously exciting and enjoyable. Balancing modifiability and reliability with ease of understanding / access is the name of the game here.
- **Build an awesome community**: Mechanistic Interpretability already has an awesome community but as that community grows, it makes sense that there will be niches. I'd love to build a great community around Sparse Autoencoders.

### Goals

#### **SAE Training**
SAE Training features will fit into a number of categories including:

- **Making it easy to train SAEs**: Training SAEs is hard for a number of reasons and so making it easy for people to train SAEs with relatively little expertise seems like the main way this codebase will create value. 
- **Training SAEs on more models**: Supporting training of SAEs on more models, architectures, different activations within those models.
- **Being better at training SAEs**: Enabling methodological changes which may improve SAE performance as measured by reconstruction loss, Cross Entropy Loss when using reconstructed activation, L1 loss, L0 and interpretability of features as well as improving speed of training or reducing the compute resources required to train SAEs. 
- **Being better at measuring SAE Performance**: How do we know when SAEs are doing what we want them to? Improving training metrics should allow better decisions about which methods to use and which hyperparameters choices we make.
- **Training SAE variants**: People are already training “Transcoders” which map from one activation to another (such as before / after an MLP layer). These can be easily supported with a few changes. Other variants will come in time and 

#### **Analysis with SAEs**

Using SAEs to understand neural network internals is an exciting, but complicated task.

- **Feature-wise Interpretability**: This looks something like "for each feature, have as much knowledge about it as possible". Part of this will feature dashboard improvements, or supporting better integrations with Neuronpedia.
- **Mechanistic Interpretability**: This comprises the more traditional kinds of Mechanistic Interpretability which TransformerLens supports and should be supported by this codebase. Making it easy to patch, ablate or otherwise intervene on features so as to find circuits will likely speed up lots of researchers.

### Other Stuff

I think there are lots of other types of analysis that could be done in the future with SAE features. I've already explored many different types of statistical tests which can reveal interesting properties of features. There are also things like saliency mapping and attribution techniques which it would be nice to support.

- Accessibility and Code Quality: The codebase won’t be used if it doesn’t work and it also won’t get used if it’s too hard to understand, modify or read. 
Making the code accessible: This involves tasks like turning the code base into a python package.
- Knowing how the code is supposed to work: Is the code well-documented? This will require docstrings, tutorials and links to related work and publications. Getting aligned on what the code does is critical to sharing a resource like this. 
- Knowing the code works as intended: All code should be tested.
- Knowing the code is actually performant: This will ensure code works as intended. However deep learning introduces lots of complexity which makes actually running benchmarks essential to having confidence in the code.

================
File: docs/training_saes.md
================
# Training Sparse Autoencoders

Methods development for training SAEs is rapidly evolving, so these docs may change frequently. For all available training options, see [LanguageModelSAERunnerConfig][sae_lens.LanguageModelSAERunnerConfig].

However, we are attempting to maintain this [tutorial](https://github.com/jbloomAus/SAELens/blob/main/tutorials/training_a_sparse_autoencoder.ipynb)
 [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://githubtocolab.com/jbloomAus/SAELens/blob/main/tutorials/training_a_sparse_autoencoder.ipynb).

 We encourage readers to join the [Open Source Mechanistic Interpretability Slack](https://join.slack.com/t/opensourcemechanistic/shared_invite/zt-2k0id7mv8-CsIgPLmmHd03RPJmLUcapw) for support!

## Basic training setup

 Training a SAE is done using the [SAETrainingRunner][sae_lens.SAETrainingRunner] class. This class is configured using a [LanguageModelSAERunnerConfig][sae_lens.LanguageModelSAERunnerConfig], and has a single method, [run()][sae_lens.SAETrainingRunner.run], which performs training.

 Some of the core config options are below:

 - `architecture`: The architecture of the SAE to train. This can be `"standard"`, `"gated"`, or `"jumprelu"`. TopK training will be coming soon!
 - `model_name`: The base model name to train a SAE on. This must correspond to a [model from TransformerLens](https://neelnanda-io.github.io/TransformerLens/generated/model_properties_table.html).
 - `hook_name`: This is a TransformerLens hook in the model where our SAE will be trained from. More info on hooks can be found [here](https://neelnanda-io.github.io/TransformerLens/generated/demos/Main_Demo.html#Hook-Points).
 - `dataset_path`: The path to a dataset on Huggingface for training.
 - `hook_layer`: This is an int which corresponds to the layer specified in `hook_name`. This must match! e.g. if `hook_name` is `"blocks.3.hook_mlp_out"`, then `layer` must be `3`.
 - `d_in`: The input size of the SAE. This must match the size of the hook in the model where the SAE is trained.
 - `expansion_factor`: The hidden layer of the SAE will have size `expansion_factor * d_in`.
 - `l1_coefficient`: This controls how much sparsity the SAE will have after training.
 - `training_tokens`: The total tokens used for training.
 - `train_batch_size_tokens`: The batch size used for training. Adjust this to keep the GPU saturated.
 -  `model_from_pretrained_kwargs`: A dictionary of keyword arguments to pass to HookedTransformer.from_pretrained when loading the model. It's best to set "center_writing_weights" to False (this will be the default in the future).

A sample training run from the [tutorial](https://github.com/jbloomAus/SAELens/blob/main/tutorials/training_a_sparse_autoencoder.ipynb) is shown below:

```python
total_training_steps = 30_000
batch_size = 4096
total_training_tokens = total_training_steps * batch_size

lr_warm_up_steps = 0
lr_decay_steps = total_training_steps // 5  # 20% of training
l1_warm_up_steps = total_training_steps // 20  # 5% of training

cfg = LanguageModelSAERunnerConfig(
    # Data Generating Function (Model + Training Distibuion)
    model_name="tiny-stories-1L-21M",  # our model (more options here: https://neelnanda-io.github.io/TransformerLens/generated/model_properties_table.html)
    hook_name="blocks.0.hook_mlp_out",  # A valid hook point (see more details here: https://neelnanda-io.github.io/TransformerLens/generated/demos/Main_Demo.html#Hook-Points)
    hook_layer=0,  # Only one layer in the model.
    d_in=1024,  # the width of the mlp output.
    dataset_path="apollo-research/roneneldan-TinyStories-tokenizer-gpt2",  # this is a tokenized language dataset on Huggingface for the Tiny Stories corpus.
    is_dataset_tokenized=True,
    streaming=True,  # we could pre-download the token dataset if it was small.
    # SAE Parameters
    mse_loss_normalization=None,  # We won't normalize the mse loss,
    expansion_factor=16,  # the width of the SAE. Larger will result in better stats but slower training.
    b_dec_init_method="zeros",  # The geometric median can be used to initialize the decoder weights.
    apply_b_dec_to_input=False,  # We won't apply the decoder weights to the input.
    normalize_sae_decoder=False,
    scale_sparsity_penalty_by_decoder_norm=True,
    decoder_heuristic_init=True,
    init_encoder_as_decoder_transpose=True,
    normalize_activations="expected_average_only_in",
    # Training Parameters
    lr=5e-5,
    adam_beta1=0.9,  # adam params (default, but once upon a time we experimented with these.)
    adam_beta2=0.999,
    lr_scheduler_name="constant",  # constant learning rate with warmup.
    lr_warm_up_steps=lr_warm_up_steps,  # this can help avoid too many dead features initially.
    lr_decay_steps=lr_decay_steps,  # this will help us avoid overfitting.
    l1_coefficient=5,  # will control how sparse the feature activations are
    l1_warm_up_steps=l1_warm_up_steps,  # this can help avoid too many dead features initially.
    lp_norm=1.0,  # the L1 penalty (and not a Lp for p < 1)
    train_batch_size_tokens=batch_size,
    context_size=256,  # will control the lenght of the prompts we feed to the model. Larger is better but slower. so for the tutorial we'll use a short one.
    # Activation Store Parameters
    n_batches_in_buffer=64,  # controls how many activations we store / shuffle.
    training_tokens=total_training_tokens,  # 100 million tokens is quite a few, but we want to see good stats. Get a coffee, come back.
    store_batch_size_prompts=16,
    # Resampling protocol
    use_ghost_grads=False,  # we don't use ghost grads anymore.
    feature_sampling_window=1000,  # this controls our reporting of feature sparsity stats
    dead_feature_window=1000,  # would effect resampling or ghost grads if we were using it.
    dead_feature_threshold=1e-4,  # would effect resampling or ghost grads if we were using it.
    # WANDB
    log_to_wandb=True,  # always use wandb unless you are just testing code.
    wandb_project="sae_lens_tutorial",
    wandb_log_frequency=30,
    eval_every_n_wandb_logs=20,
    # Misc
    device=device,
    seed=42,
    n_checkpoints=0,
    checkpoint_path="checkpoints",
    dtype="float32"
)
sparse_autoencoder = SAETrainingRunner(cfg).run()
```

As you can see, the training setup provides a large number of options to explore. The full list of options can be found in the [LanguageModelSAERunnerConfig][sae_lens.LanguageModelSAERunnerConfig] class.

### Training Topk SAEs

By default, SAELens will train SAEs using a L1 loss term with ReLU activation. A popular alternative architecture is the [TopK](https://arxiv.org/abs/2406.04093) architecture, which fixes the L0 of the SAE using a TopK activation function. To train a TopK SAE, set the `architecture` parameter to `"topk"` in the config. You can set the `k` parameter via `activation_fn_kwargs`. If not set, the default is `k=100`. The TopK architecture ignores the `l1_coefficient` parameter.

```python
cfg = LanguageModelSAERunnerConfig(
    architecture="topk",
    activation_fn_kwargs={"k": 100},
    # ...
)
sparse_autoencoder = SAETrainingRunner(cfg).run()
```

### Training JumpReLU SAEs

[JumpReLU SAEs](https://arxiv.org/abs/2407.14435) are the current state-of-the-art SAE architecture, but are often more tricky to train than other architectures. To train a JumpReLU SAE, set the `architecture` parameter to `"jumprelu"` in the config. JumpReLU SAEs use an sparsity penalty that is controlled using the `l1_coefficient` parameter. This is technically a misnomer as the JumpReLU sparsity penalty is not a L1 penalty, but we keep the parameter name for consistency with the L1 penalty used by the standard architecture. The JumpReLU architecture also has two additional parameters: `jumprelu_bandwidth` and `jumprelu_init_threshold`. Both of these are likely fine at their default values, but may be worth experimenting with if JumpReLU training is too slow to converge.

```python
cfg = LanguageModelSAERunnerConfig(
    architecture="jumprelu",
    l1_coefficient=5.0,
    jumprelu_bandwidth=0.001,
    jumprelu_init_threshold=0.001,
    # ...
)
sparse_autoencoder = SAETrainingRunner(cfg).run()
```

### Training Gated SAEs

[Gated SAEs](https://arxiv.org/abs/2404.16014) are a precursor to JumpReLU SAEs, but using a simpler training procedure that should make them easier to train. To train a Gated SAE, set the `architecture` parameter to `"gated"` in the config. Gated SAEs use the `l1_coefficient` parameter to control the sparsity of the SAE, the same as standard SAEs. If JumpReLU training is too slow to converge, it may be worth trying a Gated SAE instead.

```python
cfg = LanguageModelSAERunnerConfig(
    architecture="gated",
    l1_coefficient=5.0,
    # ...
)
sparse_autoencoder = SAETrainingRunner(cfg).run()
```

## CLI Runner

The SAE training runner can also be run from the command line via the `sae_lens.sae_training_runner` module. This can be useful for quickly testing different hyperparameters or running training on a remote server. The command line interface is shown below. All options to the CLI are the same as the [LanguageModelSAERunnerConfig][sae_lens.LanguageModelSAERunnerConfig] with a `--` prefix. E.g., `--model_name` is the same as `model_name` in the config.

```bash
python -m sae_lens.sae_training_runner --help
```

## Logging to Weights and Biases

For any real training run, you should be logging to Weights and Biases (WandB). This will allow you to track your training progress and compare different runs. To enable WandB, set `log_to_wandb=True`. The `wandb_project` parameter in the config controls the project name in WandB. You can also control the logging frequency with `wandb_log_frequency` and `eval_every_n_wandb_logs`.

A number of helpful metrics are logged to WandB, including the sparsity of the SAE, the mean squared error (MSE) of the SAE, dead features, and explained variance. These metrics can be used to monitor the training progress and adjust the training parameters. Below is a screenshot from one training run.

![screenshot](dashboard_screenshot.png)


## Best practices for real SAEs

It may sound daunting to train a real SAE but nothing could be further from the truth! You can typically train a decent SAE for a real LLM on a single A100 GPU in a matter of hours.

SAE Training best practices are still rapidly evolving, so the default settings in SAELens may not be optimal for real SAEs. Fortunately, it's easy to see what any SAE trained using SAELens used for its training configuration and just copy its values as a starting point! If there's a SAE on Huggingface trained using SAELens, you can see all the training settings used by looking at the `cfg.json` file in the SAE's repo. For instance, here's the [cfg.json](https://huggingface.co/jbloom/Gemma-2b-Residual-Stream-SAEs/blob/main/gemma_2b_blocks.12.hook_resid_post_16384/cfg.json) for a Gemma 2B standard SAE trained by Joseph Bloom. You can also get the config in SAELens as the second return value from `SAE.from_pretrained()`. For instance, the same config mentioned above can be accessed as `cfg_dict = SAE.from_pretrained("jbloom/Gemma-2b-Residual-Stream-SAEs", "gemma_2b_blocks.12.hook_resid_post_16384")[1]`. You can browse all SAEs uploaded to Huggingface via SAELens to get some inspiration with the [SAELens library tag](https://huggingface.co/models?library=saelens).

Some general performance tips:

- If your GPU supports it (most modern nvidia-GPUs do), setting `autocast=True` and `autocast_lm=True` in the config will dramatically speed up training.
- We find that often SAEs struggle to train well with `dtype="bfloat16"`. We aren't sure why this is, but make sure to compare the SAE quality if you change the dtype.
- You can try turning on `compile_sae=True` and `compile_llm=True`in the config to see if it makes training faster. Your mileage may vary though, compilation can be finicky.

### JumpReLU SAEs

JumpReLU SAEs are a state-of-the-art SAE architecture from [DeepMind](https://arxiv.org/abs/2407.14435) which at present gives the best known sparsity vs reconstruction error trade-off, and is the architecture used for [Gemma Scope SAEs](https://deepmind.google/discover/blog/gemma-scope-helping-the-safety-community-shed-light-on-the-inner-workings-of-language-models/). However, JumpReLU SAEs are slightly trickier to train than standard SAEs due to how the threshold is learned. We recommend the following tips for training JumpReLU SAEs:

- Make sure to train on enough tokens. We've found that at least 2B tokens and ideally 4B tokens is needed for good performance with the default `jumprelu_bandwidth` setting. This may vary depending on the model and SAE size though, so make sure to monitor the training logs to ensure convergence.
- Set `normalize_activations="expected_average_only_in"` in the config. This helps with convergence and is generally a good idea for all SAEs.

You can find a sample config for a Gemma-2-2B JumpReLU SAE trained via SAELens here: [cfg.json](https://huggingface.co/chanind/sae-gemma-2-2b-layer-1-res-jumprelu/blob/main/blocks.1.hook_resid_post/cfg.json)

## Checkpoints

Checkpoints allow you to save a snapshot of the SAE and sparsitity statistics during training. To enable checkpointing, set `n_checkpoints` to a value larger than 0. If WandB logging is enabled, checkpoints will be uploaded as WandB artifacts. To save checkpoints locally, the `checkpoint_path` parameter can be set to a local directory.


## Optimizers and Schedulers

The SAE training runner uses the Adam optimizer with a constant learning rate by default. The optimizer betas can be controlled with the settings `adam_beta1` and `adam_beta2`.

The learning rate scheduler can be controlled with the `lr_scheduler_name` parameter. The available schedulers are: `constant` (default), `consineannealing`, and `cosineannealingwarmrestarts`. All schedulers can be used with linear warmup and linear decay, set via `lr_warm_up_steps` and `lr_decay_steps`.

To avoid dead features, it's often helpful to slowly increase the L1 penalty. This can be done by setting `l1_warm_up_steps` to a value larger than 0. This will linearly increase the L1 penalty over the first `l1_warm_up_steps` training steps.

## Training on Huggingface Models

While TransformerLens is the recommended way to use SAELens, it is also possible to use any Huggingface AutoModelForCausalLM as the model. This is useful if you want to use a model that is not supported by TransformerLens, or if you cannot use TransformerLens due to memory or performance reasons. To use a Huggingface AutoModelForCausalLM, you can specify `model_class_name = 'AutoModelForCausalLM'` in the SAE config. Your hook points will then need to correspond to the named parameters of the Huggingface model rather than the typical TransformerLens hook points. For instance, if you were using GPT2 from Huggingface, you would use `hook_name = 'transformer.h.1'` rather than `hook_name = 'blocks.1.hook_resid_post'`. Otherwise everything should work the same as with TransformerLens models.


## Datasets, streaming, and context size

SAELens works with datasets hosted on Huggingface. However, these datsets are often very large and take a long time and a lot of disk space to download. To speed this up, you can set `streaming=True` in the config. This will stream the dataset from Huggingface during training, which will allow training to start immediately and save disk space.

The `context_size` parameter controls the length of the prompts fed to the model. Larger context sizes will result in better SAE performance, but will also slow down training. Each training batch will be tokens of size `train_batch_size_tokens x context_size`.

It's also possible to use pre-tokenized datasets to speed up training, since tokenization can be a bottleneck. To use a pre-tokenized dataset on Huggingface, update the `dataset_path` parameter and set `is_dataset_tokenized=True` in the config.

## Pretokenizing datasets

We also provider a runner, [PretokenizeRunner][sae_lens.PretokenizeRunner], which can be used to pre-tokenize a dataset and upload it to Huggingface. See [PretokenizeRunnerConfig][sae_lens.PretokenizeRunnerConfig] for all available options. We also provide a [pretokenizing datasets tutorial](https://github.com/jbloomAus/SAELens/blob/main/tutorials/pretokenizing_datasets.ipynb) with more details.

A sample run from the tutorial for GPT2 and the NeelNanda/c4-10k dataset is shown below.

```python
from sae_lens import PretokenizeRunner, PretokenizeRunnerConfig

cfg = PretokenizeRunnerConfig(
    tokenizer_name="gpt2",
    dataset_path="NeelNanda/c4-10k", # this is just a tiny test dataset
    shuffle=True,
    num_proc=4, # increase this number depending on how many CPUs you have

    # tweak these settings depending on the model
    context_size=128,
    begin_batch_token="bos",
    begin_sequence_token=None,
    sequence_separator_token="eos",

    # uncomment to upload to huggingface
    # hf_repo_id="your-username/c4-10k-tokenized-gpt2"

    # uncomment to save the dataset locally
    # save_path="./c4-10k-tokenized-gpt2"
)

dataset = PretokenizeRunner(cfg).run()
```

## List of Pretokenized datasets

Below is a list of pre-tokenized datasets that can be used with SAELens. If you have a dataset you would like to add to this list, please open a PR!

| Huggingface ID | Tokenizer | Source Dataset | context size | Created with SAELens |
| --- | --- | --- | --- | --- |
| [chanind/openwebtext-gemma](https://huggingface.co/datasets/chanind/openwebtext-gemma) | gemma | [Skylion007/openwebtext](https://huggingface.co/datasets/Skylion007/openwebtext) | 8192 | [Yes](https://huggingface.co/datasets/chanind/openwebtext-gemma/blob/main/sae_lens.json) |
| [chanind/openwebtext-llama3](https://huggingface.co/datasets/chanind/openwebtext-llama3) | llama3 | [Skylion007/openwebtext](https://huggingface.co/datasets/Skylion007/openwebtext) | 8192 | [Yes](https://huggingface.co/datasets/chanind/openwebtext-llama3/blob/main/sae_lens.json) |
| [apollo-research/Skylion007-openwebtext-tokenizer-EleutherAI-gpt-neox-20b](https://huggingface.co/datasets/apollo-research/Skylion007-openwebtext-tokenizer-EleutherAI-gpt-neox-20b) | EleutherAI/gpt-neox-20b | [Skylion007/openwebtext](https://huggingface.co/datasets/Skylion007/openwebtext) | 2048 | [No](https://huggingface.co/datasets/apollo-research/Skylion007-openwebtext-tokenizer-EleutherAI-gpt-neox-20b/blob/main/upload_script.py) |
| [apollo-research/monology-pile-uncopyrighted-tokenizer-EleutherAI-gpt-neox-20b](https://huggingface.co/datasets/apollo-research/monology-pile-uncopyrighted-tokenizer-EleutherAI-gpt-neox-20b) | EleutherAI/gpt-neox-20b | [monology/pile-uncopyrighted](https://huggingface.co/datasets/monology/pile-uncopyrighted) | 2048 | [No](https://huggingface.co/datasets/apollo-research/monology-pile-uncopyrighted-tokenizer-EleutherAI-gpt-neox-20b/blob/main/upload_script.py) |
| [apollo-research/monology-pile-uncopyrighted-tokenizer-gpt2](https://huggingface.co/datasets/apollo-research/monology-pile-uncopyrighted-tokenizer-gpt2) | gpt2 | [monology/pile-uncopyrighted](https://huggingface.co/datasets/monology/pile-uncopyrighted) | 1024 | [No](https://huggingface.co/datasets/apollo-research/monology-pile-uncopyrighted-tokenizer-gpt2/blob/main/upload_script.py) |
| [apollo-research/Skylion007-openwebtext-tokenizer-gpt2](https://huggingface.co/datasets/apollo-research/Skylion007-openwebtext-tokenizer-gpt2) | gpt2 | [Skylion007/openwebtext](https://huggingface.co/datasets/Skylion007/openwebtext) | 1024 | [No](https://huggingface.co/datasets/apollo-research/Skylion007-openwebtext-tokenizer-gpt2/blob/main/upload_script.py) |

## Caching activations

The next step in improving performance beyond pre-tokenizing datasets is to cache model activations. This allows you to pre-calculate all the training activations for your SAE in advance so the model does not need to be run during training to generate activations. This allows rapid training of SAEs and is especially helpful for experimenting with training hyperparameters. However, pre-calculating activations can take a very large amount of disk space, so it may not always be possible.

SAELens provides a [CacheActivationsRunner][sae_lens.CacheActivationsRunner] class to help with pre-calculating activations. See [CacheActivationsRunnerConfig][sae_lens.CacheActivationsRunnerConfig] for all available options. This runner intentionally shares a lot of options with [LanguageModelSAERunnerConfig][sae_lens.LanguageModelSAERunnerConfig]. These options should be set identically when using the cached activations in training. The CacheActivationsRunner can be used as below:

```python
from sae_lens import CacheActivationsRunner, CacheActivationsRunnerConfig

cfg = CacheActivationsRunnerConfig(
    model_name="tiny-stories-1L-21M",
    hook_name="blocks.0.hook_mlp_out",
    dataset_path="apollo-research/roneneldan-TinyStories-tokenizer-gpt2",
    # ...
    new_cached_activations_path="./tiny-stories-1L-21M-cache",
    hf_repo_id="your-username/tiny-stories-1L-21M-cache", # To push to hub
)

CacheActivationsRunner(cfg).run()
```

To use the cached activations during training, set `use_cached_activations=True` and `cached_activations_path` to match the `new_cached_activations_path` above option in training configuration.

## Uploading SAEs to Huggingface

Once you have a set of SAEs that you're happy with, your next step is to share them with the world! SAELens has a `upload_saes_to_huggingface()` function which makes this easy to do. We also provide a [uploading saes to huggingface tutorial](https://github.com/jbloomAus/SAELens/blob/main/tutorials/uploading_saes_to_huggingface.ipynb) with more details.

You'll just need to pass a dictionary of SAEs to upload along with the huggingface repo id to upload to. The dictionary keys will become the folders in the repo where each SAE will be located. It's best practice to use the hook point that the SAE was trained on as the key to make it clear to users where in the model to apply the SAE. The values of this dictionary can be either an SAE object, or a path to a saved SAE object on disk from the `sae.save_model()` method.

A sample is shown below:

```python
from sae_lens import upload_saes_to_huggingface

saes_dict = {
    "blocks.0.hook_resid_pre": layer_0_sae,
    "blocks.1.hook_resid_pre": layer_1_sae,
    # ...
}

upload_saes_to_huggingface(
    saes_dict,
    hf_repo_id="your-username/your-sae-repo",
)
```

================
File: README.md
================
<img width="1308" alt="Screenshot 2024-03-21 at 3 08 28 pm" src="https://github.com/jbloomAus/mats_sae_training/assets/69127271/209012ec-a779-4036-b4be-7b7739ea87f6">

# SAE Lens 
[![PyPI](https://img.shields.io/pypi/v/sae-lens?color=blue)](https://pypi.org/project/sae-lens/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![build](https://github.com/jbloomAus/SAELens/actions/workflows/build.yml/badge.svg)](https://github.com/jbloomAus/SAELens/actions/workflows/build.yml)
[![Deploy Docs](https://github.com/jbloomAus/SAELens/actions/workflows/deploy_docs.yml/badge.svg)](https://github.com/jbloomAus/SAELens/actions/workflows/deploy_docs.yml)
[![codecov](https://codecov.io/gh/jbloomAus/SAELens/graph/badge.svg?token=N83NGH8CGE)](https://codecov.io/gh/jbloomAus/SAELens)

SAELens exists to help researchers:
- Train sparse autoencoders.
- Analyse sparse autoencoders / research mechanistic interpretability. 
- Generate insights which make it easier to create safe and aligned AI systems.

Please refer to the [documentation](https://jbloomaus.github.io/SAELens/) for information on how to:
- Download and Analyse pre-trained sparse autoencoders. 
- Train your own sparse autoencoders.
- Generate feature dashboards with the [SAE-Vis Library](https://github.com/callummcdougall/sae_vis/tree/main).

SAE Lens is the result of many contributors working collectively to improve humanity's understanding of neural networks, many of whom are motivated by a desire to [safeguard humanity from risks posed by artificial intelligence](https://80000hours.org/problem-profiles/artificial-intelligence/).

This library is maintained by [Joseph Bloom](https://www.jbloomaus.com/) and [David Chanin](https://github.com/chanind).

## Loading Pre-trained SAEs. 

Pre-trained SAEs for various models can be imported via SAE Lens. See this [page](https://jbloomaus.github.io/SAELens/sae_table/) in the readme for a list of all SAEs.
## Tutorials

- [SAE Lens + Neuronpedia](tutorials/tutorial_2_0.ipynb)[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://githubtocolab.com/jbloomAus/SAELens/blob/main/tutorials/tutorial_2_0.ipynb)
- [Loading and Analysing Pre-Trained Sparse Autoencoders](tutorials/basic_loading_and_analysing.ipynb)
 [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://githubtocolab.com/jbloomAus/SAELens/blob/main/tutorials/basic_loading_and_analysing.ipynb)
 - [Understanding SAE Features with the Logit Lens](tutorials/logits_lens_with_features.ipynb)
 [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://githubtocolab.com/jbloomAus/SAELens/blob/main/tutorials/logits_lens_with_features.ipynb)
  - [Training a Sparse Autoencoder](tutorials/training_a_sparse_autoencoder.ipynb)
 [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://githubtocolab.com/jbloomAus/SAELens/blob/main/tutorials/training_a_sparse_autoencoder.ipynb)


## Join the Slack!

Feel free to join the [Open Source Mechanistic Interpretability Slack](https://join.slack.com/t/opensourcemechanistic/shared_invite/zt-2k0id7mv8-CsIgPLmmHd03RPJmLUcapw) for support!


## Citation

Please cite the package as follows:

```
@misc{bloom2024saetrainingcodebase,
   title = {SAELens},
   author = {Joseph Bloom, Curt Tigges and David Chanin},
   year = {2024},
   howpublished = {\url{https://github.com/jbloomAus/SAELens}},
}
```

================
File: requirements.txt
================
transformer-lens==1.10.0
transformers==4.35.2
jupyter==1.0.0
plotly==5.18.0
plotly-express==0.4.1
nbformat==5.9.2
ipykernel==6.27.1
matplotlib==3.8.2
matplotlib-inline==0.1.6
flake8==7.0.0
isort==5.13.2
black==23.11.0
pytest==7.4.3
pytest-cov==4.1.0
pre-commit==3.6.0

================
File: sae_lens/__init__.py
================
# ruff: noqa: E402
__version__ = "5.6.0"
import logging
logger = logging.getLogger(__name__)
from .analysis.hooked_sae_transformer import HookedSAETransformer
from .cache_activations_runner import CacheActivationsRunner
from .config import (
    CacheActivationsRunnerConfig,
    LanguageModelSAERunnerConfig,
    PretokenizeRunnerConfig,
)
from .evals import run_evals
from .pretokenize_runner import PretokenizeRunner, pretokenize_runner
from .sae import SAE, SAEConfig
from .sae_training_runner import SAETrainingRunner
from .training.activations_store import ActivationsStore
from .training.training_sae import TrainingSAE, TrainingSAEConfig
from .training.upload_saes_to_huggingface import upload_saes_to_huggingface
__all__ = [
    "SAE",
    "SAEConfig",
    "TrainingSAE",
    "TrainingSAEConfig",
    "HookedSAETransformer",
    "ActivationsStore",
    "LanguageModelSAERunnerConfig",
    "SAETrainingRunner",
    "CacheActivationsRunnerConfig",
    "CacheActivationsRunner",
    "PretokenizeRunnerConfig",
    "PretokenizeRunner",
    "pretokenize_runner",
    "run_evals",
    "upload_saes_to_huggingface",
]

================
File: sae_lens/analysis/hooked_sae_transformer.py
================
import logging
from contextlib import contextmanager
from typing import Any, Callable, Dict, List, Optional, Tuple, Union
import torch
from jaxtyping import Float
from transformer_lens.ActivationCache import ActivationCache
from transformer_lens.hook_points import HookPoint  # Hooking utilities
from transformer_lens.HookedTransformer import HookedTransformer
from sae_lens.sae import SAE
SingleLoss = Float[torch.Tensor, ""]  # Type alias for a single element tensor
LossPerToken = Float[torch.Tensor, "batch pos-1"]
Loss = Union[SingleLoss, LossPerToken]
def get_deep_attr(obj: Any, path: str):
    """Helper function to get a nested attribute from a object.
    In practice used to access HookedTransformer HookPoints (eg model.blocks[0].attn.hook_z)
    Args:
        obj: Any object. In practice, this is a HookedTransformer (or subclass)
        path: str. The path to the attribute you want to access. (eg "blocks.0.attn.hook_z")
    returns:
        Any. The attribute at the end of the path
    """
    parts = path.split(".")
    # Navigate to the last component in the path
    for part in parts:
        obj = obj[int(part)] if part.isdigit() else getattr(obj, part)
    return obj
def set_deep_attr(obj: Any, path: str, value: Any):
    """Helper function to change the value of a nested attribute from a object.
    In practice used to swap HookedTransformer HookPoints (eg model.blocks[0].attn.hook_z) with HookedSAEs and vice versa
    Args:
        obj: Any object. In practice, this is a HookedTransformer (or subclass)
        path: str. The path to the attribute you want to access. (eg "blocks.0.attn.hook_z")
        value: Any. The value you want to set the attribute to (eg a HookedSAE object)
    """
    parts = path.split(".")
    # Navigate to the last component in the path
    for part in parts[:-1]:
        obj = obj[int(part)] if part.isdigit() else getattr(obj, part)
    # Set the value on the final attribute
    setattr(obj, parts[-1], value)
class HookedSAETransformer(HookedTransformer):
    def __init__(
        self,
        *model_args: Any,
        **model_kwargs: Any,
    ):
        """Model initialization. Just HookedTransformer init, but adds a dictionary to keep track of attached SAEs.
        Note that if you want to load the model from pretrained weights, you should use
        :meth:`from_pretrained` instead.
        Args:
            *model_args: Positional arguments for HookedTransformer initialization
            **model_kwargs: Keyword arguments for HookedTransformer initialization
        """
        super().__init__(*model_args, **model_kwargs)
        self.acts_to_saes: Dict[str, SAE] = {}  # type: ignore
    def add_sae(self, sae: SAE, use_error_term: Optional[bool] = None):
        """Attaches an SAE to the model
        WARNING: This sae will be permanantly attached until you remove it with reset_saes. This function will also overwrite any existing SAE attached to the same hook point.
        Args:
            sae: SparseAutoencoderBase. The SAE to attach to the model
            use_error_term: (Optional[bool]) If provided, will set the use_error_term attribute of the SAE to this value. Determines whether the SAE returns input or reconstruction. Defaults to None.
        """
        act_name = sae.cfg.hook_name
        if (act_name not in self.acts_to_saes) and (act_name not in self.hook_dict):
            logging.warning(
                f"No hook found for {act_name}. Skipping. Check model.hook_dict for available hooks."
            )
            return
        if use_error_term is not None:
            if not hasattr(sae, "_original_use_error_term"):
                sae._original_use_error_term = sae.use_error_term  # type: ignore
            sae.use_error_term = use_error_term
        self.acts_to_saes[act_name] = sae
        set_deep_attr(self, act_name, sae)
        self.setup()
    def _reset_sae(self, act_name: str, prev_sae: Optional[SAE] = None):
        """Resets an SAE that was attached to the model
        By default will remove the SAE from that hook_point.
        If prev_sae is provided, will replace the current SAE with the provided one.
        This is mainly used to restore previously attached SAEs after temporarily running with different SAEs (eg with run_with_saes)
        Args:
            act_name: str. The hook_name of the SAE to reset
            prev_sae: Optional[HookedSAE]. The SAE to replace the current one with. If None, will just remove the SAE from this hook point. Defaults to None
        """
        if act_name not in self.acts_to_saes:
            logging.warning(
                f"No SAE is attached to {act_name}. There's nothing to reset."
            )
            return
        current_sae = self.acts_to_saes[act_name]
        if hasattr(current_sae, "_original_use_error_term"):
            current_sae.use_error_term = current_sae._original_use_error_term  # type: ignore
            delattr(current_sae, "_original_use_error_term")
        if prev_sae:
            set_deep_attr(self, act_name, prev_sae)
            self.acts_to_saes[act_name] = prev_sae
        else:
            set_deep_attr(self, act_name, HookPoint())
            del self.acts_to_saes[act_name]
    def reset_saes(
        self,
        act_names: Optional[Union[str, List[str]]] = None,
        prev_saes: Optional[List[Union[SAE, None]]] = None,
    ):
        """Reset the SAEs attached to the model
        If act_names are provided will just reset SAEs attached to those hooks. Otherwise will reset all SAEs attached to the model.
        Optionally can provide a list of prev_saes to reset to. This is mainly used to restore previously attached SAEs after temporarily running with different SAEs (eg with run_with_saes).
        Args:
            act_names (Optional[Union[str, List[str]]): The act_names of the SAEs to reset. If None, will reset all SAEs attached to the model. Defaults to None.
            prev_saes (Optional[List[Union[HookedSAE, None]]]): List of SAEs to replace the current ones with. If None, will just remove the SAEs. Defaults to None.
        """
        if isinstance(act_names, str):
            act_names = [act_names]
        elif act_names is None:
            act_names = list(self.acts_to_saes.keys())
        if prev_saes:
            if len(act_names) != len(prev_saes):
                raise ValueError("act_names and prev_saes must have the same length")
        else:
            prev_saes = [None] * len(act_names)  # type: ignore
        for act_name, prev_sae in zip(act_names, prev_saes):  # type: ignore
            self._reset_sae(act_name, prev_sae)
        self.setup()
    def run_with_saes(
        self,
        *model_args: Any,
        saes: Union[SAE, List[SAE]] = [],
        reset_saes_end: bool = True,
        use_error_term: Optional[bool] = None,
        **model_kwargs: Any,
    ) -> Union[
        None,
        Float[torch.Tensor, "batch pos d_vocab"],
        Loss,
        Tuple[Float[torch.Tensor, "batch pos d_vocab"], Loss],
    ]:
        """Wrapper around HookedTransformer forward pass.
        Runs the model with the given SAEs attached for one forward pass, then removes them. By default, will reset all SAEs to original state after.
        Args:
            *model_args: Positional arguments for the model forward pass
            saes: (Union[HookedSAE, List[HookedSAE]]) The SAEs to be attached for this forward pass
            reset_saes_end (bool): If True, all SAEs added during this run are removed at the end, and previously attached SAEs are restored to their original state. Default is True.
            use_error_term: (Optional[bool]) If provided, will set the use_error_term attribute of all SAEs attached during this run to this value. Defaults to None.
            **model_kwargs: Keyword arguments for the model forward pass
        """
        with self.saes(
            saes=saes, reset_saes_end=reset_saes_end, use_error_term=use_error_term
        ):
            return self(*model_args, **model_kwargs)
    def run_with_cache_with_saes(
        self,
        *model_args: Any,
        saes: Union[SAE, List[SAE]] = [],
        reset_saes_end: bool = True,
        use_error_term: Optional[bool] = None,
        return_cache_object: bool = True,
        remove_batch_dim: bool = False,
        **kwargs: Any,
    ) -> Tuple[
        Union[
            None,
            Float[torch.Tensor, "batch pos d_vocab"],
            Loss,
            Tuple[Float[torch.Tensor, "batch pos d_vocab"], Loss],
        ],
        Union[ActivationCache, Dict[str, torch.Tensor]],
    ]:
        """Wrapper around 'run_with_cache' in HookedTransformer.
        Attaches given SAEs before running the model with cache and then removes them.
        By default, will reset all SAEs to original state after.
        Args:
            *model_args: Positional arguments for the model forward pass
            saes: (Union[HookedSAE, List[HookedSAE]]) The SAEs to be attached for this forward pass
            reset_saes_end: (bool) If True, all SAEs added during this run are removed at the end, and previously attached SAEs are restored to their original state. Default is True.
            use_error_term: (Optional[bool]) If provided, will set the use_error_term attribute of all SAEs attached during this run to this value. Determines whether the SAE returns input or reconstruction. Defaults to None.
            return_cache_object: (bool) if True, this will return an ActivationCache object, with a bunch of
                useful HookedTransformer specific methods, otherwise it will return a dictionary of
                activations as in HookedRootModule.
            remove_batch_dim: (bool) Whether to remove the batch dimension (only works for batch_size==1). Defaults to False.
            **kwargs: Keyword arguments for the model forward pass
        """
        with self.saes(
            saes=saes, reset_saes_end=reset_saes_end, use_error_term=use_error_term
        ):
            return self.run_with_cache(  # type: ignore
                *model_args,
                return_cache_object=return_cache_object,  # type: ignore
                remove_batch_dim=remove_batch_dim,
                **kwargs,
            )
    def run_with_hooks_with_saes(
        self,
        *model_args: Any,
        saes: Union[SAE, List[SAE]] = [],
        reset_saes_end: bool = True,
        fwd_hooks: List[Tuple[Union[str, Callable], Callable]] = [],  # type: ignore
        bwd_hooks: List[Tuple[Union[str, Callable], Callable]] = [],  # type: ignore
        reset_hooks_end: bool = True,
        clear_contexts: bool = False,
        **model_kwargs: Any,
    ):
        """Wrapper around 'run_with_hooks' in HookedTransformer.
        Attaches the given SAEs to the model before running the model with hooks and then removes them.
        By default, will reset all SAEs to original state after.
        Args:
            *model_args: Positional arguments for the model forward pass
            act_names: (Union[HookedSAE, List[HookedSAE]]) The SAEs to be attached for this forward pass
            reset_saes_end: (bool) If True, all SAEs added during this run are removed at the end, and previously attached SAEs are restored to their original state. (default: True)
            fwd_hooks: (List[Tuple[Union[str, Callable], Callable]]) List of forward hooks to apply
            bwd_hooks: (List[Tuple[Union[str, Callable], Callable]]) List of backward hooks to apply
            reset_hooks_end: (bool) Whether to reset the hooks at the end of the forward pass (default: True)
            clear_contexts: (bool) Whether to clear the contexts at the end of the forward pass (default: False)
            **model_kwargs: Keyword arguments for the model forward pass
        """
        with self.saes(saes=saes, reset_saes_end=reset_saes_end):
            return self.run_with_hooks(
                *model_args,
                fwd_hooks=fwd_hooks,
                bwd_hooks=bwd_hooks,
                reset_hooks_end=reset_hooks_end,
                clear_contexts=clear_contexts,
                **model_kwargs,
            )
    @contextmanager
    def saes(
        self,
        saes: Union[SAE, List[SAE]] = [],
        reset_saes_end: bool = True,
        use_error_term: Optional[bool] = None,
    ):
        """
        A context manager for adding temporary SAEs to the model.
        See HookedTransformer.hooks for a similar context manager for hooks.
        By default will keep track of previously attached SAEs, and restore them when the context manager exits.
        Example:
        .. code-block:: python
            from transformer_lens import HookedSAETransformer, HookedSAE, HookedSAEConfig
            model = HookedSAETransformer.from_pretrained('gpt2-small')
            sae_cfg = HookedSAEConfig(...)
            sae = HookedSAE(sae_cfg)
            with model.saes(saes=[sae]):
                spliced_logits = model(text)
        Args:
            saes (Union[HookedSAE, List[HookedSAE]]): SAEs to be attached.
            reset_saes_end (bool): If True, removes all SAEs added by this context manager when the context manager exits, returning previously attached SAEs to their original state.
            use_error_term (Optional[bool]): If provided, will set the use_error_term attribute of all SAEs attached during this run to this value. Defaults to None.
        """
        act_names_to_reset = []
        prev_saes = []
        if isinstance(saes, SAE):
            saes = [saes]
        try:
            for sae in saes:
                act_names_to_reset.append(sae.cfg.hook_name)
                prev_sae = self.acts_to_saes.get(sae.cfg.hook_name, None)
                prev_saes.append(prev_sae)
                self.add_sae(sae, use_error_term=use_error_term)
            yield self
        finally:
            if reset_saes_end:
                self.reset_saes(act_names_to_reset, prev_saes)

================
File: sae_lens/analysis/neuronpedia_integration.py
================
import asyncio
import json
import os
import urllib.parse
import webbrowser
from datetime import datetime
from typing import Any, Optional, TypeVar
import requests
from dotenv import load_dotenv
from neuron_explainer.activations.activation_records import calculate_max_activation
from neuron_explainer.activations.activations import ActivationRecord
from neuron_explainer.explanations.calibrated_simulator import (
    UncalibratedNeuronSimulator,
)
from neuron_explainer.explanations.explainer import (
    HARMONY_V4_MODELS,
    ContextSize,
    TokenActivationPairExplainer,
)
from neuron_explainer.explanations.explanations import ScoredSimulation
from neuron_explainer.explanations.few_shot_examples import FewShotExampleSet
from neuron_explainer.explanations.prompt_builder import PromptFormat
from neuron_explainer.explanations.scoring import (
    _simulate_and_score_sequence,
    aggregate_scored_sequence_simulations,
)
from neuron_explainer.explanations.simulator import (
    LogprobFreeExplanationTokenSimulator,
    NeuronSimulator,
)
from tenacity import retry, stop_after_attempt, wait_random_exponential
from sae_lens import SAE, logger
NEURONPEDIA_DOMAIN = "https://neuronpedia.org"
# Constants for replacing NaNs and Infs in outputs
POSITIVE_INF_REPLACEMENT = 9999
NEGATIVE_INF_REPLACEMENT = -9999
NAN_REPLACEMENT = 0
OTHER_INVALID_REPLACEMENT = -99999
# Pick up OPENAI_API_KEY from environment variable
load_dotenv()
def NanAndInfReplacer(value: str):
    """Replace NaNs and Infs in outputs."""
    replacements = {
        "-Infinity": NEGATIVE_INF_REPLACEMENT,
        "Infinity": POSITIVE_INF_REPLACEMENT,
        "NaN": NAN_REPLACEMENT,
    }
    if value in replacements:
        replaced_value = replacements[value]
        return float(replaced_value)
    return NAN_REPLACEMENT
def open_neuronpedia_feature_dashboard(sae: SAE, index: int):
    sae_id = sae.cfg.neuronpedia_id
    if sae_id is None:
        logger.warning(
            "SAE does not have a Neuronpedia ID. Either dashboards for this SAE do not exist (yet) on Neuronpedia, or the SAE was not loaded via the from_pretrained method"
        )
    else:
        url = f"{NEURONPEDIA_DOMAIN}/{sae_id}/{index}"
        webbrowser.open(url)
def get_neuronpedia_quick_list(
    sae: SAE,
    features: list[int],
    name: str = "temporary_list",
):
    sae_id = sae.cfg.neuronpedia_id
    if sae_id is None:
        logger.warning(
            "SAE does not have a Neuronpedia ID. Either dashboards for this SAE do not exist (yet) on Neuronpedia, or the SAE was not loaded via the from_pretrained method"
        )
    assert sae_id is not None
    url = NEURONPEDIA_DOMAIN + "/quick-list/"
    name = urllib.parse.quote(name)
    url = url + "?name=" + name
    list_feature = [
        {
            "modelId": sae.cfg.model_name,
            "layer": sae_id.split("/")[1],
            "index": str(feature),
        }
        for feature in features
    ]
    url = url + "&features=" + urllib.parse.quote(json.dumps(list_feature))
    webbrowser.open(url)
    return url
def get_neuronpedia_feature(
    feature: int, layer: int, model: str = "gpt2-small", dataset: str = "res-jb"
) -> dict[str, Any]:
    """Fetch a feature from Neuronpedia API."""
    url = f"{NEURONPEDIA_DOMAIN}/api/feature/{model}/{layer}-{dataset}/{feature}"
    result = requests.get(url).json()
    result["index"] = int(result["index"])
    return result
class NeuronpediaActivation:
    """Represents an activation from Neuronpedia."""
    def __init__(self, id: str, tokens: list[str], act_values: list[float]):
        self.id = id
        self.tokens = tokens
        self.act_values = act_values
class NeuronpediaFeature:
    """Represents a feature from Neuronpedia."""
    def __init__(
        self,
        modelId: str,
        layer: int,
        dataset: str,
        feature: int,
        description: str = "",
        activations: list[NeuronpediaActivation] | None = None,
        autointerp_explanation: str = "",
        autointerp_explanation_score: float = 0.0,
    ):
        self.modelId = modelId
        self.layer = layer
        self.dataset = dataset
        self.feature = feature
        self.description = description
        self.activations = activations
        self.autointerp_explanation = autointerp_explanation
        self.autointerp_explanation_score = autointerp_explanation_score
    def has_activating_text(self) -> bool:
        """Check if the feature has activating text."""
        if self.activations is None:
            return False
        return any(max(activation.act_values) > 0 for activation in self.activations)
T = TypeVar("T")
@retry(wait=wait_random_exponential(min=1, max=500), stop=stop_after_attempt(10))
def sleep_identity(x: T) -> T:
    """Dummy function for retrying."""
    return x
@retry(wait=wait_random_exponential(min=1, max=500), stop=stop_after_attempt(10))
async def simulate_and_score(
    simulator: NeuronSimulator, activation_records: list[ActivationRecord]
) -> ScoredSimulation:
    """Score an explanation of a neuron by how well it predicts activations on the given text sequences."""
    scored_sequence_simulations = await asyncio.gather(
        *[
            sleep_identity(
                _simulate_and_score_sequence(
                    simulator,
                    activation_record,
                )
            )
            for activation_record in activation_records
        ]
    )
    return aggregate_scored_sequence_simulations(scored_sequence_simulations)
def make_neuronpedia_list_with_features(
    api_key: str,
    list_name: str,
    features: list[NeuronpediaFeature],
    list_description: Optional[str] = None,
    open_browser: bool = True,
):
    url = NEURONPEDIA_DOMAIN + "/api/list/new-with-features"
    # make POST json request with body
    body = {
        "name": list_name,
        "description": list_description,
        "features": [
            {
                "modelId": feature.modelId,
                "layer": f"{feature.layer}-{feature.dataset}",
                "index": feature.feature,
                "description": feature.description,
            }
            for feature in features
        ],
    }
    response = requests.post(url, json=body, headers={"x-api-key": api_key})
    result = response.json()
    if "url" in result and open_browser:
        webbrowser.open(result["url"])
        return result["url"]
    raise Exception("Error in creating list: " + result["message"])
def test_key(api_key: str):
    """Test the validity of the Neuronpedia API key."""
    url = f"{NEURONPEDIA_DOMAIN}/api/test"
    body = {"apiKey": api_key}
    response = requests.post(url, json=body)
    if response.status_code != 200:
        raise Exception("Neuronpedia API key is not valid.")
async def autointerp_neuronpedia_features(  # noqa: C901
    features: list[NeuronpediaFeature],
    openai_api_key: str | None = None,
    autointerp_retry_attempts: int = 3,
    autointerp_score_max_concurrent: int = 20,
    neuronpedia_api_key: str | None = None,
    skip_neuronpedia_api_key_test: bool = False,
    do_score: bool = True,
    output_dir: str = "neuronpedia_outputs/autointerp",
    num_activations_to_use: int = 20,
    max_explanation_activation_records: int = 20,
    upload_to_neuronpedia: bool = True,
    autointerp_explainer_model_name: str = "gpt-4-1106-preview",
    autointerp_scorer_model_name: str | None = "gpt-3.5-turbo",
    save_to_disk: bool = True,
):
    """
    Autointerp Neuronpedia features.
    Args:
        features: List of NeuronpediaFeature objects.
        openai_api_key: OpenAI API key.
        autointerp_retry_attempts: Number of retry attempts for autointerp.
        autointerp_score_max_concurrent: Maximum number of concurrent requests for autointerp scoring.
        neuronpedia_api_key: Neuronpedia API key.
        do_score: Whether to score the features.
        output_dir: Output directory for saving the results.
        num_activations_to_use: Number of activations to use.
        max_explanation_activation_records: Maximum number of activation records for explanation.
        upload_to_neuronpedia: Whether to upload the results to Neuronpedia.
        autointerp_explainer_model_name: Model name for autointerp explainer.
        autointerp_scorer_model_name: Model name for autointerp scorer.
    Returns:
        None
    """
    logger.info("\n\n")
    if os.getenv("OPENAI_API_KEY") is None:
        if openai_api_key is None:
            raise Exception(
                "You need to provide an OpenAI API key either in environment variable OPENAI_API_KEY or as an argument."
            )
        os.environ["OPENAI_API_KEY"] = openai_api_key
    if autointerp_explainer_model_name not in HARMONY_V4_MODELS:
        raise Exception(
            f"Invalid explainer model name: {autointerp_explainer_model_name}. Must be one of: {HARMONY_V4_MODELS}"
        )
    if do_score and autointerp_scorer_model_name not in HARMONY_V4_MODELS:
        raise Exception(
            f"Invalid scorer model name: {autointerp_scorer_model_name}. Must be one of: {HARMONY_V4_MODELS}"
        )
    if upload_to_neuronpedia:
        if neuronpedia_api_key is None:
            raise Exception(
                "You need to provide a Neuronpedia API key to upload the results to Neuronpedia."
            )
        if not skip_neuronpedia_api_key_test:
            test_key(neuronpedia_api_key)
    logger.info("\n\n=== Step 1) Fetching features from Neuronpedia")
    for feature in features:
        feature_data = get_neuronpedia_feature(
            feature=feature.feature,
            layer=feature.layer,
            model=feature.modelId,
            dataset=feature.dataset,
        )
        if "modelId" not in feature_data:
            raise Exception(
                f"Feature {feature.feature} in layer {feature.layer} of model {feature.modelId} and dataset {feature.dataset} does not exist."
            )
        if "activations" not in feature_data or len(feature_data["activations"]) == 0:
            raise Exception(
                f"Feature {feature.feature} in layer {feature.layer} of model {feature.modelId} and dataset {feature.dataset} does not have activations."
            )
        activations = feature_data["activations"]
        activations_to_add = []
        for activation in activations:
            if len(activations_to_add) < num_activations_to_use:
                activations_to_add.append(
                    NeuronpediaActivation(
                        id=activation["id"],
                        tokens=activation["tokens"],
                        act_values=activation["values"],
                    )
                )
        feature.activations = activations_to_add
        if not feature.has_activating_text():
            raise Exception(
                f"Feature {feature.modelId}@{feature.layer}-{feature.dataset}:{feature.feature} appears dead - it does not have activating text."
            )
    for iteration_num, feature in enumerate(features):
        start_time = datetime.now()
        logger.info(
            f"\n========== Feature {feature.modelId}@{feature.layer}-{feature.dataset}:{feature.feature} ({iteration_num + 1} of {len(features)} Features) =========="
        )
        logger.info(
            f"\n=== Step 2) Explaining feature {feature.modelId}@{feature.layer}-{feature.dataset}:{feature.feature}"
        )
        if feature.activations is None:
            feature.activations = []
        activation_records = [
            ActivationRecord(
                tokens=activation.tokens, activations=activation.act_values
            )
            for activation in feature.activations
        ]
        activation_records_explaining = activation_records[
            :max_explanation_activation_records
        ]
        explainer = TokenActivationPairExplainer(
            model_name=autointerp_explainer_model_name,
            prompt_format=PromptFormat.HARMONY_V4,
            context_size=ContextSize.SIXTEEN_K,
            max_concurrent=1,
        )
        explanations = []
        for _ in range(autointerp_retry_attempts):
            try:
                explanations = await explainer.generate_explanations(
                    all_activation_records=activation_records_explaining,
                    max_activation=calculate_max_activation(
                        activation_records_explaining
                    ),
                    num_samples=1,
                )
            except Exception as e:
                logger.error(f"ERROR, RETRYING: {e}")
            else:
                break
        else:
            logger.error(
                f"ERROR: Failed to explain feature {feature.modelId}@{feature.layer}-{feature.dataset}:{feature.feature}"
            )
        if len(explanations) != 1:
            raise ValueError(
                f"Expected exactly one explanation but got {len(explanations)}. This may indicate an issue with the explainer's response."
            )
        explanation = explanations[0].rstrip(".")
        logger.info(
            f"===== {autointerp_explainer_model_name}'s explanation: {explanation}"
        )
        feature.autointerp_explanation = explanation
        scored_simulation = None
        if do_score and autointerp_scorer_model_name:
            logger.info(
                f"\n=== Step 3) Scoring feature {feature.modelId}@{feature.layer}-{feature.dataset}:{feature.feature}"
            )
            logger.info("=== This can take up to 30 seconds.")
            temp_activation_records = [
                ActivationRecord(
                    tokens=[
                        token.replace("<|endoftext|>", "<|not_endoftext|>")
                        .replace(" 55", "_55")
                        .encode("ascii", errors="backslashreplace")
                        .decode("ascii")
                        for token in activation_record.tokens
                    ],
                    activations=activation_record.activations,
                )
                for activation_record in activation_records
            ]
            score = None
            scored_simulation = None
            for _ in range(autointerp_retry_attempts):
                try:
                    simulator = UncalibratedNeuronSimulator(
                        LogprobFreeExplanationTokenSimulator(
                            autointerp_scorer_model_name,
                            explanation,
                            json_mode=True,
                            max_concurrent=autointerp_score_max_concurrent,
                            few_shot_example_set=FewShotExampleSet.JL_FINE_TUNED,
                            prompt_format=PromptFormat.HARMONY_V4,
                        )
                    )
                    scored_simulation = await simulate_and_score(
                        simulator, temp_activation_records
                    )
                    score = scored_simulation.get_preferred_score()
                except Exception as e:
                    logger.error(f"ERROR, RETRYING: {e}")
                else:
                    break
            if (
                score is None
                or scored_simulation is None
                or len(scored_simulation.scored_sequence_simulations)
                != num_activations_to_use
            ):
                logger.error(
                    f"ERROR: Failed to score feature {feature.modelId}@{feature.layer}-{feature.dataset}:{feature.feature}. Skipping it."
                )
                continue
            feature.autointerp_explanation_score = score
            logger.info(
                f"===== {autointerp_scorer_model_name}'s score: {(score * 100):.0f}"
            )
        else:
            logger.info("=== Step 3) Skipping scoring as instructed.")
        feature_data = {
            "modelId": feature.modelId,
            "layer": f"{feature.layer}-{feature.dataset}",
            "index": feature.feature,
            "explanation": feature.autointerp_explanation,
            "explanationScore": feature.autointerp_explanation_score,
            "explanationModel": autointerp_explainer_model_name,
        }
        if do_score and autointerp_scorer_model_name and scored_simulation:
            feature_data["activations"] = feature.activations
            feature_data["simulationModel"] = autointerp_scorer_model_name
            feature_data["simulationActivations"] = (
                scored_simulation.scored_sequence_simulations
            )  # type: ignore
            feature_data["simulationScore"] = feature.autointerp_explanation_score
        feature_data_str = json.dumps(feature_data, default=vars)
        if save_to_disk:
            output_file = f"{output_dir}/{feature.modelId}-{feature.layer}-{feature.dataset}_feature-{feature.feature}_time-{datetime.now().strftime('%Y%m%d-%H%M%S')}.jsonl"
            os.makedirs(output_dir, exist_ok=True)
            logger.info(f"\n=== Step 4) Saving feature to {output_file}")
            with open(output_file, "a") as f:
                f.write(feature_data_str)
                f.write("\n")
        else:
            logger.info("\n=== Step 4) Skipping saving to disk.")
        if upload_to_neuronpedia:
            logger.info("\n=== Step 5) Uploading feature to Neuronpedia")
            upload_data = json.dumps(
                {
                    "feature": feature_data,
                },
                default=vars,
            )
            upload_data_json = json.loads(upload_data, parse_constant=NanAndInfReplacer)
            url = f"{NEURONPEDIA_DOMAIN}/api/explanation/new"
            response = requests.post(
                url, json=upload_data_json, headers={"x-api-key": neuronpedia_api_key}
            )
            if response.status_code != 200:
                logger.error(
                    f"ERROR: Couldn't upload explanation to Neuronpedia: {response.text}"
                )
            else:
                logger.info(
                    f"===== Uploaded to Neuronpedia: {NEURONPEDIA_DOMAIN}/{feature.modelId}/{feature.layer}-{feature.dataset}/{feature.feature}"
                )
        end_time = datetime.now()
        logger.info(f"\n========== Time Spent for Feature: {end_time - start_time}\n")
    logger.info("\n\n========== Generation and Upload Complete ==========\n\n")

================
File: sae_lens/cache_activations_runner.py
================
import io
import json
import shutil
from dataclasses import asdict
from pathlib import Path
import einops
import torch
from datasets import Array2D, Dataset, Features, Sequence, Value
from datasets.fingerprint import generate_fingerprint
from huggingface_hub import HfApi
from jaxtyping import Float, Int
from tqdm import tqdm
from transformer_lens.HookedTransformer import HookedRootModule
from sae_lens import logger
from sae_lens.config import DTYPE_MAP, CacheActivationsRunnerConfig
from sae_lens.load_model import load_model
from sae_lens.training.activations_store import ActivationsStore
def _mk_activations_store(
    model: HookedRootModule,
    cfg: CacheActivationsRunnerConfig,
    override_dataset: Dataset | None = None,
) -> ActivationsStore:
    """
    Internal method used in CacheActivationsRunner. Used to create a cached dataset
    from a ActivationsStore.
    """
    return ActivationsStore(
        model=model,
        dataset=override_dataset or cfg.dataset_path,
        streaming=cfg.streaming,
        hook_name=cfg.hook_name,
        hook_layer=cfg.hook_layer,
        hook_head_index=None,
        context_size=cfg.context_size,
        d_in=cfg.d_in,
        n_batches_in_buffer=cfg.n_batches_in_buffer,
        total_training_tokens=cfg.training_tokens,
        store_batch_size_prompts=cfg.model_batch_size,
        train_batch_size_tokens=-1,
        prepend_bos=cfg.prepend_bos,
        normalize_activations="none",
        device=torch.device("cpu"),  # since we're saving to disk
        dtype=cfg.dtype,
        cached_activations_path=None,
        model_kwargs=cfg.model_kwargs,
        autocast_lm=cfg.autocast_lm,
        dataset_trust_remote_code=cfg.dataset_trust_remote_code,
        seqpos_slice=cfg.seqpos_slice,
    )
class CacheActivationsRunner:
    def __init__(
        self,
        cfg: CacheActivationsRunnerConfig,
        override_dataset: Dataset | None = None,
    ):
        self.cfg = cfg
        self.model: HookedRootModule = load_model(
            model_class_name=self.cfg.model_class_name,
            model_name=self.cfg.model_name,
            device=self.cfg.device,
            model_from_pretrained_kwargs=self.cfg.model_from_pretrained_kwargs,
        )
        if self.cfg.compile_llm:
            self.model = torch.compile(self.model, mode=self.cfg.llm_compilation_mode)  # type: ignore
        self.activations_store = _mk_activations_store(
            self.model,
            self.cfg,
            override_dataset=override_dataset,
        )
        self.context_size = self._get_sliced_context_size(
            self.cfg.context_size, self.cfg.seqpos_slice
        )
        features_dict: dict[str, Array2D | Sequence] = {
            hook_name: Array2D(
                shape=(self.context_size, self.cfg.d_in), dtype=self.cfg.dtype
            )
            for hook_name in [self.cfg.hook_name]
        }
        features_dict["token_ids"] = Sequence(
            Value(dtype="int32"), length=self.context_size
        )
        self.features = Features(features_dict)
    def __str__(self):
        """
        Print the number of tokens to be cached.
        Print the number of buffers, and the number of tokens per buffer.
        Print the disk space required to store the activations.
        """
        bytes_per_token = (
            self.cfg.d_in * self.cfg.dtype.itemsize
            if isinstance(self.cfg.dtype, torch.dtype)
            else DTYPE_MAP[self.cfg.dtype].itemsize
        )
        total_training_tokens = self.cfg.n_seq_in_dataset * self.context_size
        total_disk_space_gb = total_training_tokens * bytes_per_token / 10**9
        return (
            f"Activation Cache Runner:\n"
            f"Total training tokens: {total_training_tokens}\n"
            f"Number of buffers: {self.cfg.n_buffers}\n"
            f"Tokens per buffer: {self.cfg.n_tokens_in_buffer}\n"
            f"Disk space required: {total_disk_space_gb:.2f} GB\n"
            f"Configuration:\n"
            f"{self.cfg}"
        )
    @staticmethod
    def _consolidate_shards(
        source_dir: Path, output_dir: Path, copy_files: bool = True
    ) -> Dataset:
        """Consolidate sharded datasets into a single directory without rewriting data.
        Each of the shards must be of the same format, aka the full dataset must be able to
        be recreated like so:
        ```
        ds = concatenate_datasets(
            [Dataset.load_from_disk(str(shard_dir)) for shard_dir in sorted(source_dir.iterdir())]
        )
        ```
        Sharded dataset format:
        ```
        source_dir/
            shard_00000/
                dataset_info.json
                state.json
                data-00000-of-00002.arrow
                data-00001-of-00002.arrow
            shard_00001/
                dataset_info.json
                state.json
                data-00000-of-00001.arrow
        ```
        And flattens them into the format:
        ```
        output_dir/
            dataset_info.json
            state.json
            data-00000-of-00003.arrow
            data-00001-of-00003.arrow
            data-00002-of-00003.arrow
        ```
        allowing the dataset to be loaded like so:
        ```
        ds = datasets.load_from_disk(output_dir)
        ```
        Args:
            source_dir: Directory containing the sharded datasets
            output_dir: Directory to consolidate the shards into
            copy_files: If True, copy files; if False, move them and delete source_dir
        """
        first_shard_dir_name = "shard_00000"  # shard_{i:05d}
        if not source_dir.exists() or not source_dir.is_dir():
            raise NotADirectoryError(
                f"source_dir is not an existing directory: {source_dir}"
            )
        if not output_dir.exists() or not output_dir.is_dir():
            raise NotADirectoryError(
                f"output_dir is not an existing directory: {output_dir}"
            )
        other_items = [p for p in output_dir.iterdir() if p.name != ".tmp_shards"]
        if other_items:
            raise FileExistsError(
                f"output_dir must be empty (besides .tmp_shards). Found: {other_items}"
            )
        if not (source_dir / first_shard_dir_name).exists():
            raise Exception(f"No shards in {source_dir} exist!")
        transfer_fn = shutil.copy2 if copy_files else shutil.move
        # Move dataset_info.json from any shard (all the same)
        transfer_fn(
            source_dir / first_shard_dir_name / "dataset_info.json",
            output_dir / "dataset_info.json",
        )
        arrow_files = []
        file_count = 0
        for shard_dir in sorted(source_dir.iterdir()):
            if not shard_dir.name.startswith("shard_"):
                continue
            # state.json contains arrow filenames
            state = json.loads((shard_dir / "state.json").read_text())
            for data_file in state["_data_files"]:
                src = shard_dir / data_file["filename"]
                new_name = f"data-{file_count:05d}-of-{len(list(source_dir.iterdir())):05d}.arrow"
                dst = output_dir / new_name
                transfer_fn(src, dst)
                arrow_files.append({"filename": new_name})
                file_count += 1
        new_state = {
            "_data_files": arrow_files,
            "_fingerprint": None,  # temporary
            "_format_columns": None,
            "_format_kwargs": {},
            "_format_type": None,
            "_output_all_columns": False,
            "_split": None,
        }
        # fingerprint is generated from dataset.__getstate__ (not includeing _fingerprint)
        with open(output_dir / "state.json", "w") as f:
            json.dump(new_state, f, indent=2)
        ds = Dataset.load_from_disk(str(output_dir))
        fingerprint = generate_fingerprint(ds)
        del ds
        with open(output_dir / "state.json", "r+") as f:
            state = json.loads(f.read())
            state["_fingerprint"] = fingerprint
            f.seek(0)
            json.dump(state, f, indent=2)
            f.truncate()
        if not copy_files:  # cleanup source dir
            shutil.rmtree(source_dir)
        return Dataset.load_from_disk(output_dir)
    @torch.no_grad()
    def run(self) -> Dataset:
        activation_save_path = self.cfg.new_cached_activations_path
        assert activation_save_path is not None
        ### Paths setup
        final_cached_activation_path = Path(activation_save_path)
        final_cached_activation_path.mkdir(exist_ok=True, parents=True)
        if any(final_cached_activation_path.iterdir()):
            raise Exception(
                f"Activations directory ({final_cached_activation_path}) is not empty. Please delete it or specify a different path. Exiting the script to prevent accidental deletion of files."
            )
        tmp_cached_activation_path = final_cached_activation_path / ".tmp_shards/"
        tmp_cached_activation_path.mkdir(exist_ok=False, parents=False)
        ### Create temporary sharded datasets
        logger.info(f"Started caching activations for {self.cfg.dataset_path}")
        for i in tqdm(range(self.cfg.n_buffers), desc="Caching activations"):
            try:
                buffer = self.activations_store.get_buffer(
                    self.cfg.n_batches_in_buffer, shuffle=False
                )
                shard = self._create_shard(buffer)
                shard.save_to_disk(
                    f"{tmp_cached_activation_path}/shard_{i:05d}", num_shards=1
                )
                del buffer, shard
            except StopIteration:
                logger.warning(
                    f"Warning: Ran out of samples while filling the buffer at batch {i} before reaching {self.cfg.n_buffers} batches."
                )
                break
        ### Concatenate shards and push to Huggingface Hub
        dataset = self._consolidate_shards(
            tmp_cached_activation_path, final_cached_activation_path, copy_files=False
        )
        if self.cfg.shuffle:
            logger.info("Shuffling...")
            dataset = dataset.shuffle(seed=self.cfg.seed)
        if self.cfg.hf_repo_id:
            logger.info("Pushing to Huggingface Hub...")
            dataset.push_to_hub(
                repo_id=self.cfg.hf_repo_id,
                num_shards=self.cfg.hf_num_shards,
                private=self.cfg.hf_is_private_repo,
                revision=self.cfg.hf_revision,
            )
            meta_io = io.BytesIO()
            meta_contents = json.dumps(
                asdict(self.cfg), indent=2, ensure_ascii=False
            ).encode("utf-8")
            meta_io.write(meta_contents)
            meta_io.seek(0)
            api = HfApi()
            api.upload_file(
                path_or_fileobj=meta_io,
                path_in_repo="cache_activations_runner_cfg.json",
                repo_id=self.cfg.hf_repo_id,
                repo_type="dataset",
                commit_message="Add cache_activations_runner metadata",
            )
        return dataset
    def _create_shard(
        self,
        buffer: tuple[
            Float[torch.Tensor, "(bs context_size) num_layers d_in"],
            Int[torch.Tensor, "(bs context_size)"] | None,
        ],
    ) -> Dataset:
        hook_names = [self.cfg.hook_name]
        acts, token_ids = buffer
        acts = einops.rearrange(
            acts,
            "(bs context_size) num_layers d_in -> num_layers bs context_size d_in",
            bs=self.cfg.n_seq_in_buffer,
            context_size=self.context_size,
            d_in=self.cfg.d_in,
            num_layers=len(hook_names),
        )
        shard_dict = {hook_name: act for hook_name, act in zip(hook_names, acts)}
        if token_ids is not None:
            token_ids = einops.rearrange(
                token_ids,
                "(bs context_size) -> bs context_size",
                bs=self.cfg.n_seq_in_buffer,
                context_size=self.context_size,
            )
            shard_dict["token_ids"] = token_ids.to(torch.int32)
        return Dataset.from_dict(
            shard_dict,
            features=self.features,
        )
    @staticmethod
    def _get_sliced_context_size(
        context_size: int, seqpos_slice: tuple[int | None, ...] | None
    ) -> int:
        if seqpos_slice is not None:
            context_size = len(range(context_size)[slice(*seqpos_slice)])
        return context_size

================
File: sae_lens/config.py
================
import json
import math
import os
from dataclasses import dataclass, field
from typing import Any, Literal, Optional, cast
import simple_parsing
import torch
import wandb
from datasets import (
    Dataset,
    DatasetDict,
    IterableDataset,
    IterableDatasetDict,
    load_dataset,
)
from sae_lens import __version__, logger
DTYPE_MAP = {
    "float32": torch.float32,
    "float64": torch.float64,
    "float16": torch.float16,
    "bfloat16": torch.bfloat16,
    "torch.float32": torch.float32,
    "torch.float64": torch.float64,
    "torch.float16": torch.float16,
    "torch.bfloat16": torch.bfloat16,
}
HfDataset = DatasetDict | Dataset | IterableDatasetDict | IterableDataset
# calling this "json_dict" so error messages will reference "json_dict" being invalid
def json_dict(s: str) -> Any:
    res = json.loads(s)
    if res is not None and not isinstance(res, dict):
        raise ValueError(f"Expected a dictionary, got {type(res)}")
    return res
def dict_field(default: dict[str, Any] | None, **kwargs: Any) -> Any:  # type: ignore
    """
    Helper to wrap simple_parsing.helpers.dict_field so we can load JSON fields from the command line.
    """
    if default is None:
        return simple_parsing.helpers.field(default=None, type=json_dict, **kwargs)
    return simple_parsing.helpers.dict_field(default, type=json_dict, **kwargs)
@dataclass
class LanguageModelSAERunnerConfig:
    """
    Configuration for training a sparse autoencoder on a language model.
    Args:
        architecture (str): The architecture to use, either "standard", "gated", "topk", or "jumprelu".
        model_name (str): The name of the model to use. This should be the name of the model in the Hugging Face model hub.
        model_class_name (str): The name of the class of the model to use. This should be either `HookedTransformer` or `HookedMamba`.
        hook_name (str): The name of the hook to use. This should be a valid TransformerLens hook.
        hook_eval (str): NOT CURRENTLY IN USE. The name of the hook to use for evaluation.
        hook_layer (int): The index of the layer to hook. Used to stop forward passes early and speed up processing.
        hook_head_index (int, optional): When the hook if for an activatio with a head index, we can specify a specific head to use here.
        dataset_path (str): A Hugging Face dataset path.
        dataset_trust_remote_code (bool): Whether to trust remote code when loading datasets from Huggingface.
        streaming (bool): Whether to stream the dataset. Streaming large datasets is usually practical.
        is_dataset_tokenized (bool): NOT IN USE. We used to use this but now automatically detect if the dataset is tokenized.
        context_size (int): The context size to use when generating activations on which to train the SAE.
        use_cached_activations (bool): Whether to use cached activations. This is useful when doing sweeps over the same activations.
        cached_activations_path (str, optional): The path to the cached activations.
        d_in (int): The input dimension of the SAE.
        d_sae (int, optional): The output dimension of the SAE. If None, defaults to `d_in * expansion_factor`.
        b_dec_init_method (str): The method to use to initialize the decoder bias. Zeros is likely fine.
        expansion_factor (int): The expansion factor. Larger is better but more computationally expensive. Default is 4.
        activation_fn (str): The activation function to use. Relu is standard.
        normalize_sae_decoder (bool): Whether to normalize the SAE decoder. Unit normed decoder weights used to be preferred.
        noise_scale (float): Using noise to induce sparsity is supported but not recommended.
        from_pretrained_path (str, optional): The path to a pretrained SAE. We can finetune an existing SAE if needed.
        apply_b_dec_to_input (bool): Whether to apply the decoder bias to the input. Not currently advised.
        decoder_orthogonal_init (bool): Whether to use orthogonal initialization for the decoder. Not currently advised.
        decoder_heuristic_init (bool): Whether to use heuristic initialization for the decoder. See Anthropic April Update.
        init_encoder_as_decoder_transpose (bool): Whether to initialize the encoder as the transpose of the decoder. See Anthropic April Update.
        n_batches_in_buffer (int): The number of batches in the buffer. When not using cached activations, a buffer in ram is used. The larger it is, the better shuffled the activations will be.
        training_tokens (int): The number of training tokens.
        finetuning_tokens (int): The number of finetuning tokens. See [here](https://www.lesswrong.com/posts/3JuSjTZyMzaSeTxKk/addressing-feature-suppression-in-saes)
        store_batch_size_prompts (int): The batch size for storing activations. This controls how many prompts are in the batch of the language model when generating actiations.
        train_batch_size_tokens (int): The batch size for training. This controls the batch size of the SAE Training loop.
        normalize_activations (str): Activation Normalization Strategy. Either none, expected_average_only_in (estimate the average activation norm and divide activations by it following Antrhopic April update -> this can be folded post training and set to None), or constant_norm_rescale (at runtime set activation norm to sqrt(d_in) and then scale up the SAE output).
        seqpos_slice (tuple): Determines slicing of activations when constructing batches during training. The slice should be (start_pos, end_pos, optional[step_size]), e.g. for Othello we sometimes use (5, -5). Note, step_size > 0.
        device (str): The device to use. Usually cuda.
        act_store_device (str): The device to use for the activation store. CPU is advised in order to save vram.
        seed (int): The seed to use.
        dtype (str): The data type to use.
        prepend_bos (bool): Whether to prepend the beginning of sequence token. You should use whatever the model was trained with.
        jumprelu_init_threshold (float): The threshold to initialize for training JumpReLU SAEs.
        jumprelu_bandwidth (float): Bandwidth for training JumpReLU SAEs.
        autocast (bool): Whether to use autocast during training. Saves vram.
        autocast_lm (bool): Whether to use autocast during activation fetching.
        compile_llm (bool): Whether to compile the LLM.
        llm_compilation_mode (str): The compilation mode to use for the LLM.
        compile_sae (bool): Whether to compile the SAE.
        sae_compilation_mode (str): The compilation mode to use for the SAE.
        adam_beta1 (float): The beta1 parameter for Adam.
        adam_beta2 (float): The beta2 parameter for Adam.
        mse_loss_normalization (str): The normalization to use for the MSE loss.
        l1_coefficient (float): The L1 coefficient.
        lp_norm (float): The Lp norm.
        scale_sparsity_penalty_by_decoder_norm (bool): Whether to scale the sparsity penalty by the decoder norm.
        l1_warm_up_steps (int): The number of warm-up steps for the L1 loss.
        lr (float): The learning rate.
        lr_scheduler_name (str): The name of the learning rate scheduler to use.
        lr_warm_up_steps (int): The number of warm-up steps for the learning rate.
        lr_end (float): The end learning rate if lr_decay_steps is set. Default is lr / 10.
        lr_decay_steps (int): The number of decay steps for the learning rate.
        n_restart_cycles (int): The number of restart cycles for the cosine annealing warm restarts scheduler.
        finetuning_method (str): The method to use for finetuning.
        use_ghost_grads (bool): Whether to use ghost gradients.
        feature_sampling_window (int): The feature sampling window.
        dead_feature_window (int): The dead feature window.
        dead_feature_threshold (float): The dead feature threshold.
        n_eval_batches (int): The number of evaluation batches.
        eval_batch_size_prompts (int): The batch size for evaluation.
        log_to_wandb (bool): Whether to log to Weights & Biases.
        log_activations_store_to_wandb (bool): NOT CURRENTLY USED. Whether to log the activations store to Weights & Biases.
        log_optimizer_state_to_wandb (bool): NOT CURRENTLY USED. Whether to log the optimizer state to Weights & Biases.
        wandb_project (str): The Weights & Biases project to log to.
        wandb_id (str): The Weights & Biases ID.
        run_name (str): The name of the run.
        wandb_entity (str): The Weights & Biases entity.
        wandb_log_frequency (int): The frequency to log to Weights & Biases.
        eval_every_n_wandb_logs (int): The frequency to evaluate.
        resume (bool): Whether to resume training.
        n_checkpoints (int): The number of checkpoints.
        checkpoint_path (str): The path to save checkpoints.
        verbose (bool): Whether to print verbose output.
        model_kwargs (dict[str, Any]): Additional keyword arguments for the model.
        model_from_pretrained_kwargs (dict[str, Any]): Additional keyword arguments for the model from pretrained.
        exclude_special_tokens (bool | list[int]): Whether to exclude special tokens from the activations.
    """
    # Data Generating Function (Model + Training Distibuion)
    model_name: str = "gelu-2l"
    model_class_name: str = "HookedTransformer"
    hook_name: str = "blocks.0.hook_mlp_out"
    hook_eval: str = "NOT_IN_USE"
    hook_layer: int = 0
    hook_head_index: Optional[int] = None
    dataset_path: str = ""
    dataset_trust_remote_code: bool = True
    streaming: bool = True
    is_dataset_tokenized: bool = True
    context_size: int = 128
    use_cached_activations: bool = False
    cached_activations_path: Optional[str] = (
        None  # Defaults to "activations/{dataset}/{model}/{full_hook_name}_{hook_head_index}"
    )
    # SAE Parameters
    architecture: Literal["standard", "gated", "jumprelu", "topk"] = "standard"
    d_in: int = 512
    d_sae: Optional[int] = None
    b_dec_init_method: str = "geometric_median"
    expansion_factor: Optional[int] = (
        None  # defaults to 4 if d_sae and expansion_factor is None
    )
    activation_fn: str = None  # relu, tanh-relu, topk. Default is relu. # type: ignore
    activation_fn_kwargs: dict[str, int] = dict_field(default=None)  # for topk
    normalize_sae_decoder: bool = True
    noise_scale: float = 0.0
    from_pretrained_path: Optional[str] = None
    apply_b_dec_to_input: bool = True
    decoder_orthogonal_init: bool = False
    decoder_heuristic_init: bool = False
    init_encoder_as_decoder_transpose: bool = False
    # Activation Store Parameters
    n_batches_in_buffer: int = 20
    training_tokens: int = 2_000_000
    finetuning_tokens: int = 0
    store_batch_size_prompts: int = 32
    normalize_activations: str = "none"  # none, expected_average_only_in (Anthropic April Update), constant_norm_rescale (Anthropic Feb Update)
    seqpos_slice: tuple[int | None, ...] = (None,)
    # Misc
    device: str = "cpu"
    act_store_device: str = "with_model"  # will be set by post init if with_model
    seed: int = 42
    dtype: str = "float32"  # type: ignore #
    prepend_bos: bool = True
    # JumpReLU Parameters
    jumprelu_init_threshold: float = 0.001
    jumprelu_bandwidth: float = 0.001
    # Performance - see compilation section of lm_runner.py for info
    autocast: bool = False  # autocast to autocast_dtype during training
    autocast_lm: bool = False  # autocast lm during activation fetching
    compile_llm: bool = False  # use torch.compile on the LLM
    llm_compilation_mode: str | None = None  # which torch.compile mode to use
    compile_sae: bool = False  # use torch.compile on the SAE
    sae_compilation_mode: str | None = None
    # Training Parameters
    ## Batch size
    train_batch_size_tokens: int = 4096
    ## Adam
    adam_beta1: float = 0.0
    adam_beta2: float = 0.999
    ## Loss Function
    mse_loss_normalization: Optional[str] = None
    l1_coefficient: float = 1e-3
    lp_norm: float = 1
    scale_sparsity_penalty_by_decoder_norm: bool = False
    l1_warm_up_steps: int = 0
    ## Learning Rate Schedule
    lr: float = 3e-4
    lr_scheduler_name: str = (
        "constant"  # constant, cosineannealing, cosineannealingwarmrestarts
    )
    lr_warm_up_steps: int = 0
    lr_end: Optional[float] = None  # only used for cosine annealing, default is lr / 10
    lr_decay_steps: int = 0
    n_restart_cycles: int = 1  # used only for cosineannealingwarmrestarts
    ## FineTuning
    finetuning_method: Optional[str] = None  # scale, decoder or unrotated_decoder
    # Resampling protocol args
    use_ghost_grads: bool = False  # want to change this to true on some timeline.
    feature_sampling_window: int = 2000
    dead_feature_window: int = 1000  # unless this window is larger feature sampling,
    dead_feature_threshold: float = 1e-8
    # Evals
    n_eval_batches: int = 10
    eval_batch_size_prompts: int | None = None  # useful if evals cause OOM
    # WANDB
    log_to_wandb: bool = True
    log_activations_store_to_wandb: bool = False
    log_optimizer_state_to_wandb: bool = False
    wandb_project: str = "mats_sae_training_language_model"
    wandb_id: Optional[str] = None
    run_name: Optional[str] = None
    wandb_entity: Optional[str] = None
    wandb_log_frequency: int = 10
    eval_every_n_wandb_logs: int = 100  # logs every 1000 steps.
    # Misc
    resume: bool = False
    n_checkpoints: int = 0
    checkpoint_path: str = "checkpoints"
    verbose: bool = True
    model_kwargs: dict[str, Any] = dict_field(default={})
    model_from_pretrained_kwargs: dict[str, Any] | None = dict_field(default=None)
    sae_lens_version: str = field(default_factory=lambda: __version__)
    sae_lens_training_version: str = field(default_factory=lambda: __version__)
    exclude_special_tokens: bool | list[int] = False
    def __post_init__(self):
        if self.resume:
            raise ValueError(
                "Resuming is no longer supported. You can finetune a trained SAE using cfg.from_pretrained path."
                + "If you want to load an SAE with resume=True in the config, please manually set resume=False in that config."
            )
        if self.use_cached_activations and self.cached_activations_path is None:
            self.cached_activations_path = _default_cached_activations_path(
                self.dataset_path,
                self.model_name,
                self.hook_name,
                self.hook_head_index,
            )
        if self.activation_fn is None:
            self.activation_fn = "topk" if self.architecture == "topk" else "relu"
        if self.architecture == "topk" and self.activation_fn != "topk":
            raise ValueError("If using topk architecture, activation_fn must be topk.")
        if self.activation_fn_kwargs is None:
            self.activation_fn_kwargs = (
                {"k": 100} if self.activation_fn == "topk" else {}
            )
        if self.architecture == "topk" and self.activation_fn_kwargs.get("k") is None:
            raise ValueError(
                "activation_fn_kwargs.k must be provided for topk architecture."
            )
        if self.d_sae is not None and self.expansion_factor is not None:
            raise ValueError("You can't set both d_sae and expansion_factor.")
        if self.d_sae is None and self.expansion_factor is None:
            self.expansion_factor = 4
        if self.d_sae is None and self.expansion_factor is not None:
            self.d_sae = self.d_in * self.expansion_factor
        self.tokens_per_buffer = (
            self.train_batch_size_tokens * self.context_size * self.n_batches_in_buffer
        )
        if self.run_name is None:
            self.run_name = f"{self.d_sae}-L1-{self.l1_coefficient}-LR-{self.lr}-Tokens-{self.training_tokens:3.3e}"
        if self.model_from_pretrained_kwargs is None:
            if self.model_class_name == "HookedTransformer":
                self.model_from_pretrained_kwargs = {"center_writing_weights": False}
            else:
                self.model_from_pretrained_kwargs = {}
        if self.b_dec_init_method not in ["geometric_median", "mean", "zeros"]:
            raise ValueError(
                f"b_dec_init_method must be geometric_median, mean, or zeros. Got {self.b_dec_init_method}"
            )
        if self.normalize_sae_decoder and self.decoder_heuristic_init:
            raise ValueError(
                "You can't normalize the decoder and use heuristic initialization."
            )
        if self.normalize_sae_decoder and self.scale_sparsity_penalty_by_decoder_norm:
            raise ValueError(
                "Weighting loss by decoder norm makes no sense if you are normalizing the decoder weight norms to 1"
            )
        # if we use decoder fine tuning, we can't be applying b_dec to the input
        if (self.finetuning_method == "decoder") and (self.apply_b_dec_to_input):
            raise ValueError(
                "If we are fine tuning the decoder, we can't be applying b_dec to the input.\nSet apply_b_dec_to_input to False."
            )
        if self.normalize_activations not in [
            "none",
            "expected_average_only_in",
            "constant_norm_rescale",
            "layer_norm",
        ]:
            raise ValueError(
                f"normalize_activations must be none, layer_norm, expected_average_only_in, or constant_norm_rescale. Got {self.normalize_activations}"
            )
        if self.act_store_device == "with_model":
            self.act_store_device = self.device
        if self.lr_end is None:
            self.lr_end = self.lr / 10
        unique_id = self.wandb_id
        if unique_id is None:
            unique_id = cast(
                Any, wandb
            ).util.generate_id()  # not sure why this type is erroring
        self.checkpoint_path = f"{self.checkpoint_path}/{unique_id}"
        if self.verbose:
            logger.info(
                f"Run name: {self.d_sae}-L1-{self.l1_coefficient}-LR-{self.lr}-Tokens-{self.training_tokens:3.3e}"
            )
            # Print out some useful info:
            n_tokens_per_buffer = (
                self.store_batch_size_prompts
                * self.context_size
                * self.n_batches_in_buffer
            )
            logger.info(
                f"n_tokens_per_buffer (millions): {n_tokens_per_buffer / 10**6}"
            )
            n_contexts_per_buffer = (
                self.store_batch_size_prompts * self.n_batches_in_buffer
            )
            logger.info(
                f"Lower bound: n_contexts_per_buffer (millions): {n_contexts_per_buffer / 10**6}"
            )
            total_training_steps = (
                self.training_tokens + self.finetuning_tokens
            ) // self.train_batch_size_tokens
            logger.info(f"Total training steps: {total_training_steps}")
            total_wandb_updates = total_training_steps // self.wandb_log_frequency
            logger.info(f"Total wandb updates: {total_wandb_updates}")
            # how many times will we sample dead neurons?
            # assert self.dead_feature_window <= self.feature_sampling_window, "dead_feature_window must be smaller than feature_sampling_window"
            n_feature_window_samples = (
                total_training_steps // self.feature_sampling_window
            )
            logger.info(
                f"n_tokens_per_feature_sampling_window (millions): {(self.feature_sampling_window * self.context_size * self.train_batch_size_tokens) / 10**6}"
            )
            logger.info(
                f"n_tokens_per_dead_feature_window (millions): {(self.dead_feature_window * self.context_size * self.train_batch_size_tokens) / 10**6}"
            )
            logger.info(
                f"We will reset the sparsity calculation {n_feature_window_samples} times."
            )
            # logger.info("Number tokens in dead feature calculation window: ", self.dead_feature_window * self.train_batch_size_tokens)
            logger.info(
                f"Number tokens in sparsity calculation window: {self.feature_sampling_window * self.train_batch_size_tokens:.2e}"
            )
        if self.use_ghost_grads:
            logger.info("Using Ghost Grads.")
        if self.context_size < 0:
            raise ValueError(
                f"The provided context_size is {self.context_size} is negative. Expecting positive context_size."
            )
        _validate_seqpos(seqpos=self.seqpos_slice, context_size=self.context_size)
        if isinstance(self.exclude_special_tokens, list) and not all(
            isinstance(x, int) for x in self.exclude_special_tokens
        ):
            raise ValueError("exclude_special_tokens list must contain only integers")
    @property
    def total_training_tokens(self) -> int:
        return self.training_tokens + self.finetuning_tokens
    @property
    def total_training_steps(self) -> int:
        return self.total_training_tokens // self.train_batch_size_tokens
    def get_base_sae_cfg_dict(self) -> dict[str, Any]:
        return {
            # TEMP
            "architecture": self.architecture,
            "d_in": self.d_in,
            "d_sae": self.d_sae,
            "dtype": self.dtype,
            "device": self.device,
            "model_name": self.model_name,
            "hook_name": self.hook_name,
            "hook_layer": self.hook_layer,
            "hook_head_index": self.hook_head_index,
            "activation_fn_str": self.activation_fn,
            "apply_b_dec_to_input": self.apply_b_dec_to_input,
            "context_size": self.context_size,
            "prepend_bos": self.prepend_bos,
            "dataset_path": self.dataset_path,
            "dataset_trust_remote_code": self.dataset_trust_remote_code,
            "finetuning_scaling_factor": self.finetuning_method is not None,
            "sae_lens_training_version": self.sae_lens_training_version,
            "normalize_activations": self.normalize_activations,
            "activation_fn_kwargs": self.activation_fn_kwargs,
            "model_from_pretrained_kwargs": self.model_from_pretrained_kwargs,
            "seqpos_slice": self.seqpos_slice,
        }
    def get_training_sae_cfg_dict(self) -> dict[str, Any]:
        return {
            **self.get_base_sae_cfg_dict(),
            "l1_coefficient": self.l1_coefficient,
            "lp_norm": self.lp_norm,
            "use_ghost_grads": self.use_ghost_grads,
            "normalize_sae_decoder": self.normalize_sae_decoder,
            "noise_scale": self.noise_scale,
            "decoder_orthogonal_init": self.decoder_orthogonal_init,
            "mse_loss_normalization": self.mse_loss_normalization,
            "decoder_heuristic_init": self.decoder_heuristic_init,
            "init_encoder_as_decoder_transpose": self.init_encoder_as_decoder_transpose,
            "normalize_activations": self.normalize_activations,
            "jumprelu_init_threshold": self.jumprelu_init_threshold,
            "jumprelu_bandwidth": self.jumprelu_bandwidth,
            "scale_sparsity_penalty_by_decoder_norm": self.scale_sparsity_penalty_by_decoder_norm,
        }
    def to_dict(self) -> dict[str, Any]:
        return {
            **self.__dict__,
            # some args may not be serializable by default
            "dtype": str(self.dtype),
            "device": str(self.device),
            "act_store_device": str(self.act_store_device),
        }
    def to_json(self, path: str) -> None:
        if not os.path.exists(os.path.dirname(path)):
            os.makedirs(os.path.dirname(path))
        with open(path + "cfg.json", "w") as f:
            json.dump(self.to_dict(), f, indent=2)
    @classmethod
    def from_json(cls, path: str) -> "LanguageModelSAERunnerConfig":
        with open(path + "cfg.json") as f:
            cfg = json.load(f)
        # ensure that seqpos slices is a tuple
        # Ensure seqpos_slice is a tuple
        if "seqpos_slice" in cfg:
            if isinstance(cfg["seqpos_slice"], list):
                cfg["seqpos_slice"] = tuple(cfg["seqpos_slice"])
            elif not isinstance(cfg["seqpos_slice"], tuple):
                cfg["seqpos_slice"] = (cfg["seqpos_slice"],)
        return cls(**cfg)
@dataclass
class CacheActivationsRunnerConfig:
    """
    Configuration for creating and caching activations of an LLM.
    Args:
        dataset_path (str): The path to the Hugging Face dataset. This may be tokenized or not.
        model_name (str): The name of the model to use.
        model_batch_size (int): How many prompts are in the batch of the language model when generating activations.
        hook_name (str): The name of the hook to use.
        hook_layer (int): The layer of the final hook. Currently only support a single hook, so this should be the same as hook_name.
        d_in (int): Dimension of the model.
        total_training_tokens (int): Total number of tokens to process.
        context_size (int): Context size to process. Can be left as -1 if the dataset is tokenized.
        model_class_name (str): The name of the class of the model to use. This should be either `HookedTransformer` or `HookedMamba`.
        new_cached_activations_path (str, optional): The path to save the activations.
        shuffle (bool): Whether to shuffle the dataset.
        seed (int): The seed to use for shuffling.
        dtype (str): Datatype of activations to be stored.
        device (str): The device for the model.
        buffer_size_gb (float): The buffer size in GB. This should be < 2GB.
        hf_repo_id (str, optional): The Hugging Face repository id to save the activations to.
        hf_num_shards (int, optional): The number of shards to save the activations to.
        hf_revision (str): The revision to save the activations to.
        hf_is_private_repo (bool): Whether the Hugging Face repository is private.
        model_kwargs (dict): Keyword arguments for `model.run_with_cache`.
        model_from_pretrained_kwargs (dict): Keyword arguments for the `from_pretrained` method of the model.
        compile_llm (bool): Whether to compile the LLM.
        llm_compilation_mode (str): The torch.compile mode to use.
        prepend_bos (bool): Whether to prepend the beginning of sequence token. You should use whatever the model was trained with.
        seqpos_slice (tuple): Determines slicing of activations when constructing batches during training. The slice should be (start_pos, end_pos, optional[step_size]), e.g. for Othello we sometimes use (5, -5). Note, step_size > 0.
        streaming (bool): Whether to stream the dataset. Streaming large datasets is usually practical.
        autocast_lm (bool): Whether to use autocast during activation fetching.
        dataset_trust_remote_code (bool): Whether to trust remote code when loading datasets from Huggingface.
    """
    dataset_path: str
    model_name: str
    model_batch_size: int
    hook_name: str
    hook_layer: int
    d_in: int
    training_tokens: int
    context_size: int = -1  # Required if dataset is not tokenized
    model_class_name: str = "HookedTransformer"
    # defaults to "activations/{dataset}/{model}/{hook_name}
    new_cached_activations_path: str | None = None
    shuffle: bool = True
    seed: int = 42
    dtype: str = "float32"
    device: str = "cuda" if torch.cuda.is_available() else "cpu"
    buffer_size_gb: float = 2.0  # HF datasets writer have problems with shards > 2GB
    # Huggingface Integration
    hf_repo_id: str | None = None
    hf_num_shards: int | None = None
    hf_revision: str = "main"
    hf_is_private_repo: bool = False
    # Model
    model_kwargs: dict[str, Any] = field(default_factory=dict)
    model_from_pretrained_kwargs: dict[str, Any] = field(default_factory=dict)
    compile_llm: bool = False
    llm_compilation_mode: str | None = None  # which torch.compile mode to use
    # Activation Store
    prepend_bos: bool = True
    seqpos_slice: tuple[int | None, ...] = (None,)
    streaming: bool = True
    autocast_lm: bool = False
    dataset_trust_remote_code: bool | None = None
    def __post_init__(self):
        # Automatically determine context_size if dataset is tokenized
        if self.context_size == -1:
            ds = load_dataset(self.dataset_path, split="train", streaming=True)
            assert isinstance(ds, IterableDataset)
            first_sample = next(iter(ds))
            toks = first_sample.get("tokens") or first_sample.get("input_ids") or None
            if toks is None:
                raise ValueError(
                    "Dataset is not tokenized. Please specify context_size."
                )
            token_length = len(toks)
            self.context_size = token_length
        if self.context_size == -1:
            raise ValueError("context_size is still -1 after dataset inspection.")
        if self.seqpos_slice is not None:
            _validate_seqpos(
                seqpos=self.seqpos_slice,
                context_size=self.context_size,
            )
        if self.new_cached_activations_path is None:
            self.new_cached_activations_path = _default_cached_activations_path(  # type: ignore
                self.dataset_path, self.model_name, self.hook_name, None
            )
    @property
    def sliced_context_size(self) -> int:
        if self.seqpos_slice is not None:
            return len(range(self.context_size)[slice(*self.seqpos_slice)])
        return self.context_size
    @property
    def bytes_per_token(self) -> int:
        return self.d_in * DTYPE_MAP[self.dtype].itemsize
    @property
    def n_tokens_in_buffer(self) -> int:
        # Calculate raw tokens per buffer based on memory constraints
        _tokens_per_buffer = int(self.buffer_size_gb * 1e9) // self.bytes_per_token
        # Round down to nearest multiple of batch_token_size
        return _tokens_per_buffer - (_tokens_per_buffer % self.n_tokens_in_batch)
    @property
    def n_tokens_in_batch(self) -> int:
        return self.model_batch_size * self.sliced_context_size
    @property
    def n_batches_in_buffer(self) -> int:
        return self.n_tokens_in_buffer // self.n_tokens_in_batch
    @property
    def n_seq_in_dataset(self) -> int:
        return self.training_tokens // self.sliced_context_size
    @property
    def n_seq_in_buffer(self) -> int:
        return self.n_tokens_in_buffer // self.sliced_context_size
    @property
    def n_buffers(self) -> int:
        return math.ceil(self.training_tokens / self.n_tokens_in_buffer)
def _default_cached_activations_path(
    dataset_path: str,
    model_name: str,
    hook_name: str,
    hook_head_index: int | None,
) -> str:
    path = f"activations/{dataset_path.replace('/', '_')}/{model_name.replace('/', '_')}/{hook_name}"
    if hook_head_index is not None:
        path += f"_{hook_head_index}"
    return path
def _validate_seqpos(seqpos: tuple[int | None, ...], context_size: int) -> None:
    # Ensure that the step-size is larger or equal to 1
    if len(seqpos) == 3:
        step_size = seqpos[2] or 1
        if step_size <= 1:
            raise ValueError(
                f"Ensure the step_size={seqpos[2]} for sequence slicing is at least 1."
            )
    # Ensure that the choice of seqpos doesn't end up with an empty list
    if len(list(range(context_size))[slice(*seqpos)]) == 0:
        raise ValueError(
            f"The slice {seqpos} results in an empty range. Please adjust your seqpos or context_size."
        )
@dataclass
class PretokenizeRunnerConfig:
    tokenizer_name: str = "gpt2"
    dataset_path: str = ""
    dataset_name: str | None = None
    dataset_trust_remote_code: bool | None = None
    split: str | None = "train"
    data_files: list[str] | None = None
    data_dir: str | None = None
    num_proc: int = 4
    context_size: int = 128
    column_name: str = "text"
    shuffle: bool = True
    seed: int | None = None
    streaming: bool = False
    pretokenize_batch_size: int | None = 1000
    # special tokens
    begin_batch_token: int | Literal["bos", "eos", "sep"] | None = "bos"
    begin_sequence_token: int | Literal["bos", "eos", "sep"] | None = None
    sequence_separator_token: int | Literal["bos", "eos", "sep"] | None = "bos"
    # if saving locally, set save_path
    save_path: str | None = None
    # if saving to huggingface, set hf_repo_id
    hf_repo_id: str | None = None
    hf_num_shards: int = 64
    hf_revision: str = "main"
    hf_is_private_repo: bool = False

================
File: sae_lens/evals.py
================
# ruff: noqa: T201
import argparse
import json
import math
import re
import subprocess
from collections import defaultdict
from collections.abc import Mapping
from dataclasses import dataclass, field
from functools import partial
from importlib.metadata import PackageNotFoundError, version
from pathlib import Path
from typing import Any, Dict, List, Union
import einops
import pandas as pd
import torch
from tqdm import tqdm
from transformer_lens import HookedTransformer
from transformer_lens.hook_points import HookedRootModule
from sae_lens.sae import SAE
from sae_lens.toolkit.pretrained_saes_directory import get_pretrained_saes_directory
from sae_lens.training.activations_store import ActivationsStore
def get_library_version() -> str:
    try:
        return version("sae_lens")
    except PackageNotFoundError:
        return "unknown"
def get_git_hash() -> str:
    """
    Retrieves the current Git commit hash.
    Returns 'unknown' if the hash cannot be determined.
    """
    try:
        # Ensure the command is run in the directory where .git exists
        git_dir = Path(__file__).resolve().parent.parent  # Adjust if necessary
        result = subprocess.run(
            ["git", "rev-parse", "--short", "HEAD"],
            cwd=git_dir,
            capture_output=True,
            text=True,
            check=True,
        )
        return result.stdout.strip()
    except (subprocess.CalledProcessError, FileNotFoundError, OSError):
        return "unknown"
# Everything by default is false so the user can just set the ones they want to true
@dataclass
class EvalConfig:
    batch_size_prompts: int | None = None
    # Reconstruction metrics
    n_eval_reconstruction_batches: int = 10
    compute_kl: bool = False
    compute_ce_loss: bool = False
    # Sparsity and variance metrics
    n_eval_sparsity_variance_batches: int = 1
    compute_l2_norms: bool = False
    compute_sparsity_metrics: bool = False
    compute_variance_metrics: bool = False
    # compute featurewise density statistics
    compute_featurewise_density_statistics: bool = False
    # compute featurewise_weight_based_metrics
    compute_featurewise_weight_based_metrics: bool = False
    library_version: str = field(default_factory=get_library_version)
    git_hash: str = field(default_factory=get_git_hash)
def get_eval_everything_config(
    batch_size_prompts: int | None = None,
    n_eval_reconstruction_batches: int = 10,
    n_eval_sparsity_variance_batches: int = 1,
) -> EvalConfig:
    """
    Returns an EvalConfig object with all metrics set to True, so that when passed to run_evals all available metrics will be run.
    """
    return EvalConfig(
        batch_size_prompts=batch_size_prompts,
        n_eval_reconstruction_batches=n_eval_reconstruction_batches,
        compute_kl=True,
        compute_ce_loss=True,
        compute_l2_norms=True,
        n_eval_sparsity_variance_batches=n_eval_sparsity_variance_batches,
        compute_sparsity_metrics=True,
        compute_variance_metrics=True,
        compute_featurewise_density_statistics=True,
        compute_featurewise_weight_based_metrics=True,
    )
@torch.no_grad()
def run_evals(
    sae: SAE,
    activation_store: ActivationsStore,
    model: HookedRootModule,
    eval_config: EvalConfig = EvalConfig(),
    model_kwargs: Mapping[str, Any] = {},
    ignore_tokens: set[int | None] = set(),
    verbose: bool = False,
) -> tuple[dict[str, Any], dict[str, Any]]:
    hook_name = sae.cfg.hook_name
    actual_batch_size = (
        eval_config.batch_size_prompts or activation_store.store_batch_size_prompts
    )
    # TODO: Come up with a cleaner long term strategy here for SAEs that do reshaping.
    # turn off hook_z reshaping mode if it's on, and restore it after evals
    if "hook_z" in hook_name:
        previous_hook_z_reshaping_mode = sae.hook_z_reshaping_mode
        sae.turn_off_forward_pass_hook_z_reshaping()
    else:
        previous_hook_z_reshaping_mode = None
    all_metrics = {
        "model_behavior_preservation": {},
        "model_performance_preservation": {},
        "reconstruction_quality": {},
        "shrinkage": {},
        "sparsity": {},
        "token_stats": {},
    }
    if eval_config.compute_kl or eval_config.compute_ce_loss:
        if eval_config.n_eval_reconstruction_batches <= 0:
            raise ValueError(
                "eval_config.n_eval_reconstruction_batches must be > 0 when "
                "compute_kl or compute_ce_loss is True."
            )
        reconstruction_metrics = get_downstream_reconstruction_metrics(
            sae,
            model,
            activation_store,
            compute_kl=eval_config.compute_kl,
            compute_ce_loss=eval_config.compute_ce_loss,
            n_batches=eval_config.n_eval_reconstruction_batches,
            eval_batch_size_prompts=actual_batch_size,
            ignore_tokens=ignore_tokens,
            verbose=verbose,
        )
        if eval_config.compute_kl:
            all_metrics["model_behavior_preservation"].update(
                {
                    "kl_div_score": reconstruction_metrics["kl_div_score"],
                    "kl_div_with_ablation": reconstruction_metrics[
                        "kl_div_with_ablation"
                    ],
                    "kl_div_with_sae": reconstruction_metrics["kl_div_with_sae"],
                }
            )
        if eval_config.compute_ce_loss:
            all_metrics["model_performance_preservation"].update(
                {
                    "ce_loss_score": reconstruction_metrics["ce_loss_score"],
                    "ce_loss_with_ablation": reconstruction_metrics[
                        "ce_loss_with_ablation"
                    ],
                    "ce_loss_with_sae": reconstruction_metrics["ce_loss_with_sae"],
                    "ce_loss_without_sae": reconstruction_metrics[
                        "ce_loss_without_sae"
                    ],
                }
            )
        activation_store.reset_input_dataset()
    if (
        eval_config.compute_l2_norms
        or eval_config.compute_sparsity_metrics
        or eval_config.compute_variance_metrics
    ):
        if eval_config.n_eval_sparsity_variance_batches <= 0:
            raise ValueError(
                "eval_config.n_eval_sparsity_variance_batches must be > 0 when "
                "compute_l2_norms, compute_sparsity_metrics, or compute_variance_metrics is True."
            )
        sparsity_variance_metrics, feature_metrics = get_sparsity_and_variance_metrics(
            sae,
            model,
            activation_store,
            compute_l2_norms=eval_config.compute_l2_norms,
            compute_sparsity_metrics=eval_config.compute_sparsity_metrics,
            compute_variance_metrics=eval_config.compute_variance_metrics,
            compute_featurewise_density_statistics=eval_config.compute_featurewise_density_statistics,
            n_batches=eval_config.n_eval_sparsity_variance_batches,
            eval_batch_size_prompts=actual_batch_size,
            model_kwargs=model_kwargs,
            ignore_tokens=ignore_tokens,
            verbose=verbose,
        )
        if eval_config.compute_l2_norms:
            all_metrics["shrinkage"].update(
                {
                    "l2_norm_in": sparsity_variance_metrics["l2_norm_in"],
                    "l2_norm_out": sparsity_variance_metrics["l2_norm_out"],
                    "l2_ratio": sparsity_variance_metrics["l2_ratio"],
                    "relative_reconstruction_bias": sparsity_variance_metrics[
                        "relative_reconstruction_bias"
                    ],
                }
            )
        if eval_config.compute_sparsity_metrics:
            all_metrics["sparsity"].update(
                {
                    "l0": sparsity_variance_metrics["l0"],
                    "l1": sparsity_variance_metrics["l1"],
                }
            )
        if eval_config.compute_variance_metrics:
            all_metrics["reconstruction_quality"].update(
                {
                    "explained_variance": sparsity_variance_metrics[
                        "explained_variance"
                    ],
                    "mse": sparsity_variance_metrics["mse"],
                    "cossim": sparsity_variance_metrics["cossim"],
                }
            )
    else:
        feature_metrics = {}
    if eval_config.compute_featurewise_weight_based_metrics:
        feature_metrics |= get_featurewise_weight_based_metrics(sae)
    if len(all_metrics) == 0:
        raise ValueError(
            "No metrics were computed, please set at least one metric to True."
        )
    # restore previous hook z reshaping mode if necessary
    if "hook_z" in hook_name:
        if previous_hook_z_reshaping_mode and not sae.hook_z_reshaping_mode:
            sae.turn_on_forward_pass_hook_z_reshaping()
        elif not previous_hook_z_reshaping_mode and sae.hook_z_reshaping_mode:
            sae.turn_off_forward_pass_hook_z_reshaping()
    total_tokens_evaluated_eval_reconstruction = (
        activation_store.context_size
        * eval_config.n_eval_reconstruction_batches
        * actual_batch_size
    )
    total_tokens_evaluated_eval_sparsity_variance = (
        activation_store.context_size
        * eval_config.n_eval_sparsity_variance_batches
        * actual_batch_size
    )
    all_metrics["token_stats"] = {
        "total_tokens_eval_reconstruction": total_tokens_evaluated_eval_reconstruction,
        "total_tokens_eval_sparsity_variance": total_tokens_evaluated_eval_sparsity_variance,
    }
    # Remove empty metric groups
    all_metrics = {k: v for k, v in all_metrics.items() if v}
    return all_metrics, feature_metrics
def get_featurewise_weight_based_metrics(sae: SAE) -> dict[str, Any]:
    unit_norm_encoders = (sae.W_enc / sae.W_enc.norm(dim=0, keepdim=True)).cpu()
    unit_norm_decoder = (sae.W_dec.T / sae.W_dec.T.norm(dim=0, keepdim=True)).cpu()
    encoder_norms = sae.W_enc.norm(dim=-2).cpu().tolist()
    encoder_bias = sae.b_enc.cpu().tolist()
    encoder_decoder_cosine_sim = (
        torch.nn.functional.cosine_similarity(
            unit_norm_decoder.T,
            unit_norm_encoders.T,
        )
        .cpu()
        .tolist()
    )
    return {
        "encoder_bias": encoder_bias,
        "encoder_norm": encoder_norms,
        "encoder_decoder_cosine_sim": encoder_decoder_cosine_sim,
    }
def get_downstream_reconstruction_metrics(
    sae: SAE,
    model: HookedRootModule,
    activation_store: ActivationsStore,
    compute_kl: bool,
    compute_ce_loss: bool,
    n_batches: int,
    eval_batch_size_prompts: int,
    ignore_tokens: set[int | None] = set(),
    verbose: bool = False,
):
    metrics_dict = {}
    if compute_kl:
        metrics_dict["kl_div_with_sae"] = []
        metrics_dict["kl_div_with_ablation"] = []
    if compute_ce_loss:
        metrics_dict["ce_loss_with_sae"] = []
        metrics_dict["ce_loss_without_sae"] = []
        metrics_dict["ce_loss_with_ablation"] = []
    batch_iter = range(n_batches)
    if verbose:
        batch_iter = tqdm(batch_iter, desc="Reconstruction Batches")
    for _ in batch_iter:
        batch_tokens = activation_store.get_batch_tokens(eval_batch_size_prompts)
        for metric_name, metric_value in get_recons_loss(
            sae,
            model,
            batch_tokens,
            activation_store,
            compute_kl=compute_kl,
            compute_ce_loss=compute_ce_loss,
            ignore_tokens=ignore_tokens,
        ).items():
            if len(ignore_tokens) > 0:
                mask = torch.logical_not(
                    torch.any(
                        torch.stack(
                            [batch_tokens == token for token in ignore_tokens], dim=0
                        ),
                        dim=0,
                    )
                )
                if metric_value.shape[1] != mask.shape[1]:
                    # ce loss will be missing the last value
                    mask = mask[:, :-1]
                metric_value = metric_value[mask]
            metrics_dict[metric_name].append(metric_value)
    metrics: dict[str, float] = {}
    for metric_name, metric_values in metrics_dict.items():
        metrics[f"{metric_name}"] = torch.cat(metric_values).mean().item()
    if compute_kl:
        metrics["kl_div_score"] = (
            metrics["kl_div_with_ablation"] - metrics["kl_div_with_sae"]
        ) / metrics["kl_div_with_ablation"]
    if compute_ce_loss:
        metrics["ce_loss_score"] = (
            metrics["ce_loss_with_ablation"] - metrics["ce_loss_with_sae"]
        ) / (metrics["ce_loss_with_ablation"] - metrics["ce_loss_without_sae"])
    return metrics
def get_sparsity_and_variance_metrics(
    sae: SAE,
    model: HookedRootModule,
    activation_store: ActivationsStore,
    n_batches: int,
    compute_l2_norms: bool,
    compute_sparsity_metrics: bool,
    compute_variance_metrics: bool,
    compute_featurewise_density_statistics: bool,
    eval_batch_size_prompts: int,
    model_kwargs: Mapping[str, Any],
    ignore_tokens: set[int | None] = set(),
    verbose: bool = False,
) -> tuple[dict[str, Any], dict[str, Any]]:
    hook_name = sae.cfg.hook_name
    hook_head_index = sae.cfg.hook_head_index
    metric_dict = {}
    feature_metric_dict = {}
    if compute_l2_norms:
        metric_dict["l2_norm_in"] = []
        metric_dict["l2_norm_out"] = []
        metric_dict["l2_ratio"] = []
        metric_dict["relative_reconstruction_bias"] = []
    if compute_sparsity_metrics:
        metric_dict["l0"] = []
        metric_dict["l1"] = []
    if compute_variance_metrics:
        metric_dict["explained_variance"] = []
        metric_dict["mse"] = []
        metric_dict["cossim"] = []
    if compute_featurewise_density_statistics:
        feature_metric_dict["feature_density"] = []
        feature_metric_dict["consistent_activation_heuristic"] = []
    total_feature_acts = torch.zeros(sae.cfg.d_sae, device=sae.device)
    total_feature_prompts = torch.zeros(sae.cfg.d_sae, device=sae.device)
    total_tokens = 0
    batch_iter = range(n_batches)
    if verbose:
        batch_iter = tqdm(batch_iter, desc="Sparsity and Variance Batches")
    for _ in batch_iter:
        batch_tokens = activation_store.get_batch_tokens(eval_batch_size_prompts)
        if len(ignore_tokens) > 0:
            mask = torch.logical_not(
                torch.any(
                    torch.stack(
                        [batch_tokens == token for token in ignore_tokens], dim=0
                    ),
                    dim=0,
                )
            )
        else:
            mask = torch.ones_like(batch_tokens, dtype=torch.bool)
        flattened_mask = mask.flatten()
        # get cache
        _, cache = model.run_with_cache(
            batch_tokens,
            prepend_bos=False,
            names_filter=[hook_name],
            stop_at_layer=sae.cfg.hook_layer + 1,
            **model_kwargs,
        )
        # we would include hook z, except that we now have base SAE's
        # which will do their own reshaping for hook z.
        has_head_dim_key_substrings = ["hook_q", "hook_k", "hook_v", "hook_z"]
        if hook_head_index is not None:
            original_act = cache[hook_name][:, :, hook_head_index]
        elif any(substring in hook_name for substring in has_head_dim_key_substrings):
            original_act = cache[hook_name].flatten(-2, -1)
        else:
            original_act = cache[hook_name]
        # normalise if necessary (necessary in training only, otherwise we should fold the scaling in)
        if activation_store.normalize_activations == "expected_average_only_in":
            original_act = activation_store.apply_norm_scaling_factor(original_act)
        # send the (maybe normalised) activations into the SAE
        sae_feature_activations = sae.encode(original_act.to(sae.device))
        sae_out = sae.decode(sae_feature_activations).to(original_act.device)
        del cache
        if activation_store.normalize_activations == "expected_average_only_in":
            sae_out = activation_store.unscale(sae_out)
        flattened_sae_input = einops.rearrange(original_act, "b ctx d -> (b ctx) d")
        flattened_sae_feature_acts = einops.rearrange(
            sae_feature_activations, "b ctx d -> (b ctx) d"
        )
        flattened_sae_out = einops.rearrange(sae_out, "b ctx d -> (b ctx) d")
        # TODO: Clean this up.
        # apply mask
        masked_sae_feature_activations = sae_feature_activations * mask.unsqueeze(-1)
        flattened_sae_input = flattened_sae_input[
            flattened_mask.to(flattened_sae_input.device)
        ]
        flattened_sae_feature_acts = flattened_sae_feature_acts[
            flattened_mask.to(flattened_sae_feature_acts.device)
        ]
        flattened_sae_out = flattened_sae_out[
            flattened_mask.to(flattened_sae_out.device)
        ]
        if compute_l2_norms:
            l2_norm_in = torch.norm(flattened_sae_input, dim=-1)
            l2_norm_out = torch.norm(flattened_sae_out, dim=-1)
            l2_norm_in_for_div = l2_norm_in.clone()
            l2_norm_in_for_div[torch.abs(l2_norm_in_for_div) < 0.0001] = 1
            l2_norm_ratio = l2_norm_out / l2_norm_in_for_div
            # Equation 10 from https://arxiv.org/abs/2404.16014
            # https://github.com/saprmarks/dictionary_learning/blob/main/evaluation.py
            x_hat_norm_squared = torch.norm(flattened_sae_out, dim=-1) ** 2
            x_dot_x_hat = (flattened_sae_input * flattened_sae_out).sum(dim=-1)
            relative_reconstruction_bias = (
                x_hat_norm_squared.mean() / x_dot_x_hat.mean()
            ).unsqueeze(0)
            metric_dict["l2_norm_in"].append(l2_norm_in)
            metric_dict["l2_norm_out"].append(l2_norm_out)
            metric_dict["l2_ratio"].append(l2_norm_ratio)
            metric_dict["relative_reconstruction_bias"].append(
                relative_reconstruction_bias
            )
        if compute_sparsity_metrics:
            l0 = (flattened_sae_feature_acts > 0).sum(dim=-1).float()
            l1 = flattened_sae_feature_acts.sum(dim=-1)
            metric_dict["l0"].append(l0)
            metric_dict["l1"].append(l1)
        if compute_variance_metrics:
            resid_sum_of_squares = (
                (flattened_sae_input - flattened_sae_out).pow(2).sum(dim=-1)
            )
            total_sum_of_squares = (
                (flattened_sae_input - flattened_sae_input.mean(dim=0)).pow(2).sum(-1)
            )
            mse = resid_sum_of_squares / flattened_mask.sum()
            explained_variance = 1 - resid_sum_of_squares / total_sum_of_squares
            x_normed = flattened_sae_input / torch.norm(
                flattened_sae_input, dim=-1, keepdim=True
            )
            x_hat_normed = flattened_sae_out / torch.norm(
                flattened_sae_out, dim=-1, keepdim=True
            )
            cossim = (x_normed * x_hat_normed).sum(dim=-1)
            metric_dict["explained_variance"].append(explained_variance)
            metric_dict["mse"].append(mse)
            metric_dict["cossim"].append(cossim)
        if compute_featurewise_density_statistics:
            sae_feature_activations_bool = (masked_sae_feature_activations > 0).float()
            total_feature_acts += sae_feature_activations_bool.sum(dim=1).sum(dim=0)
            total_feature_prompts += (sae_feature_activations_bool.sum(dim=1) > 0).sum(
                dim=0
            )
            total_tokens += mask.sum()
    # Aggregate scalar metrics
    metrics: dict[str, float] = {}
    for metric_name, metric_values in metric_dict.items():
        metrics[f"{metric_name}"] = torch.cat(metric_values).mean().item()
    # Aggregate feature-wise metrics
    feature_metrics: dict[str, list[float]] = {}
    feature_metrics["feature_density"] = (total_feature_acts / total_tokens).tolist()
    feature_metrics["consistent_activation_heuristic"] = (
        total_feature_acts / total_feature_prompts
    ).tolist()
    return metrics, feature_metrics
@torch.no_grad()
def get_recons_loss(
    sae: SAE,
    model: HookedRootModule,
    batch_tokens: torch.Tensor,
    activation_store: ActivationsStore,
    compute_kl: bool,
    compute_ce_loss: bool,
    ignore_tokens: set[int | None] = set(),
    model_kwargs: Mapping[str, Any] = {},
) -> dict[str, Any]:
    hook_name = sae.cfg.hook_name
    head_index = sae.cfg.hook_head_index
    original_logits, original_ce_loss = model(
        batch_tokens, return_type="both", loss_per_token=True, **model_kwargs
    )
    if len(ignore_tokens) > 0:
        mask = torch.logical_not(
            torch.any(
                torch.stack([batch_tokens == token for token in ignore_tokens], dim=0),
                dim=0,
            )
        )
    else:
        mask = torch.ones_like(batch_tokens, dtype=torch.bool)
    metrics = {}
    # TODO(tomMcGrath): the rescaling below is a bit of a hack and could probably be tidied up
    def standard_replacement_hook(activations: torch.Tensor, hook: Any):  # noqa: ARG001
        original_device = activations.device
        activations = activations.to(sae.device)
        # Handle rescaling if SAE expects it
        if activation_store.normalize_activations == "expected_average_only_in":
            activations = activation_store.apply_norm_scaling_factor(activations)
        # SAE class agnost forward forward pass.
        new_activations = sae.decode(sae.encode(activations)).to(activations.dtype)
        # Unscale if activations were scaled prior to going into the SAE
        if activation_store.normalize_activations == "expected_average_only_in":
            new_activations = activation_store.unscale(new_activations)
        new_activations = torch.where(mask[..., None], new_activations, activations)
        return new_activations.to(original_device)
    def all_head_replacement_hook(activations: torch.Tensor, hook: Any):  # noqa: ARG001
        original_device = activations.device
        activations = activations.to(sae.device)
        # Handle rescaling if SAE expects it
        if activation_store.normalize_activations == "expected_average_only_in":
            activations = activation_store.apply_norm_scaling_factor(activations)
        # SAE class agnost forward forward pass.
        new_activations = sae.decode(sae.encode(activations.flatten(-2, -1))).to(
            activations.dtype
        )
        new_activations = new_activations.reshape(
            activations.shape
        )  # reshape to match original shape
        # Unscale if activations were scaled prior to going into the SAE
        if activation_store.normalize_activations == "expected_average_only_in":
            new_activations = activation_store.unscale(new_activations)
        return new_activations.to(original_device)
    def single_head_replacement_hook(activations: torch.Tensor, hook: Any):  # noqa: ARG001
        original_device = activations.device
        activations = activations.to(sae.device)
        # Handle rescaling if SAE expects it
        if activation_store.normalize_activations == "expected_average_only_in":
            activations = activation_store.apply_norm_scaling_factor(activations)
        new_activations = sae.decode(sae.encode(activations[:, :, head_index])).to(
            activations.dtype
        )
        activations[:, :, head_index] = new_activations
        # Unscale if activations were scaled prior to going into the SAE
        if activation_store.normalize_activations == "expected_average_only_in":
            activations = activation_store.unscale(activations)
        return activations.to(original_device)
    def standard_zero_ablate_hook(activations: torch.Tensor, hook: Any):  # noqa: ARG001
        original_device = activations.device
        activations = activations.to(sae.device)
        activations = torch.zeros_like(activations)
        return activations.to(original_device)
    def single_head_zero_ablate_hook(activations: torch.Tensor, hook: Any):  # noqa: ARG001
        original_device = activations.device
        activations = activations.to(sae.device)
        activations[:, :, head_index] = torch.zeros_like(activations[:, :, head_index])
        return activations.to(original_device)
    # we would include hook z, except that we now have base SAE's
    # which will do their own reshaping for hook z.
    has_head_dim_key_substrings = ["hook_q", "hook_k", "hook_v", "hook_z"]
    if any(substring in hook_name for substring in has_head_dim_key_substrings):
        if head_index is None:
            replacement_hook = all_head_replacement_hook
            zero_ablate_hook = standard_zero_ablate_hook
        else:
            replacement_hook = single_head_replacement_hook
            zero_ablate_hook = single_head_zero_ablate_hook
    else:
        replacement_hook = standard_replacement_hook
        zero_ablate_hook = standard_zero_ablate_hook
    recons_logits, recons_ce_loss = model.run_with_hooks(
        batch_tokens,
        return_type="both",
        fwd_hooks=[(hook_name, partial(replacement_hook))],
        loss_per_token=True,
        **model_kwargs,
    )
    zero_abl_logits, zero_abl_ce_loss = model.run_with_hooks(
        batch_tokens,
        return_type="both",
        fwd_hooks=[(hook_name, zero_ablate_hook)],
        loss_per_token=True,
        **model_kwargs,
    )
    def kl(original_logits: torch.Tensor, new_logits: torch.Tensor):
        original_probs = torch.nn.functional.softmax(original_logits, dim=-1)
        log_original_probs = torch.log(original_probs)
        new_probs = torch.nn.functional.softmax(new_logits, dim=-1)
        log_new_probs = torch.log(new_probs)
        kl_div = original_probs * (log_original_probs - log_new_probs)
        return kl_div.sum(dim=-1)
    if compute_kl:
        recons_kl_div = kl(original_logits, recons_logits)
        zero_abl_kl_div = kl(original_logits, zero_abl_logits)
        metrics["kl_div_with_sae"] = recons_kl_div
        metrics["kl_div_with_ablation"] = zero_abl_kl_div
    if compute_ce_loss:
        metrics["ce_loss_with_sae"] = recons_ce_loss
        metrics["ce_loss_without_sae"] = original_ce_loss
        metrics["ce_loss_with_ablation"] = zero_abl_ce_loss
    return metrics
def all_loadable_saes() -> list[tuple[str, str, float, float]]:
    all_loadable_saes = []
    saes_directory = get_pretrained_saes_directory()
    for release, lookup in tqdm(saes_directory.items()):
        for sae_name in lookup.saes_map:
            expected_var_explained = lookup.expected_var_explained[sae_name]
            expected_l0 = lookup.expected_l0[sae_name]
            all_loadable_saes.append(
                (release, sae_name, expected_var_explained, expected_l0)
            )
    return all_loadable_saes
def get_saes_from_regex(
    sae_regex_pattern: str, sae_block_pattern: str
) -> list[tuple[str, str, float, float]]:
    sae_regex_compiled = re.compile(sae_regex_pattern)
    sae_block_compiled = re.compile(sae_block_pattern)
    all_saes = all_loadable_saes()
    return [
        sae
        for sae in all_saes
        if sae_regex_compiled.fullmatch(sae[0]) and sae_block_compiled.fullmatch(sae[1])
    ]
def nested_dict() -> defaultdict[Any, Any]:
    return defaultdict(nested_dict)
def dict_to_nested(flat_dict: dict[str, Any]) -> defaultdict[Any, Any]:
    nested = nested_dict()
    for key, value in flat_dict.items():
        parts = key.split("/")
        d = nested
        for part in parts[:-1]:
            d = d[part]
        d[parts[-1]] = value
    return nested
def multiple_evals(
    sae_regex_pattern: str,
    sae_block_pattern: str,
    n_eval_reconstruction_batches: int,
    n_eval_sparsity_variance_batches: int,
    eval_batch_size_prompts: int = 8,
    datasets: list[str] = ["Skylion007/openwebtext", "lighteval/MATH"],
    ctx_lens: list[int] = [128],
    output_dir: str = "eval_results",
    verbose: bool = False,
) -> List[Dict[str, Any]]:
    device = "cuda" if torch.cuda.is_available() else "cpu"
    filtered_saes = get_saes_from_regex(sae_regex_pattern, sae_block_pattern)
    if len(filtered_saes) == 0:
        raise ValueError("No SAEs matched the given regex patterns")
    eval_results = []
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    eval_config = get_eval_everything_config(
        batch_size_prompts=eval_batch_size_prompts,
        n_eval_reconstruction_batches=n_eval_reconstruction_batches,
        n_eval_sparsity_variance_batches=n_eval_sparsity_variance_batches,
    )
    current_model = None
    current_model_str = None
    print(filtered_saes)
    for sae_release_name, sae_id, _, _ in tqdm(filtered_saes):
        sae = SAE.from_pretrained(
            release=sae_release_name,  # see other options in sae_lens/pretrained_saes.yaml
            sae_id=sae_id,  # won't always be a hook point
            device=device,
        )[0]
        # move SAE to device if not there already
        sae.to(device)
        if current_model_str != sae.cfg.model_name:
            del current_model  # potentially saves GPU memory
            current_model_str = sae.cfg.model_name
            current_model = HookedTransformer.from_pretrained_no_processing(
                current_model_str, device=device, **sae.cfg.model_from_pretrained_kwargs
            )
        assert current_model is not None
        for ctx_len in ctx_lens:
            for dataset in datasets:
                activation_store = ActivationsStore.from_sae(
                    current_model, sae, context_size=ctx_len, dataset=dataset
                )
                activation_store.shuffle_input_dataset(seed=42)
                eval_metrics = nested_dict()
                eval_metrics["unique_id"] = f"{sae_release_name}-{sae_id}"
                eval_metrics["sae_set"] = f"{sae_release_name}"
                eval_metrics["sae_id"] = f"{sae_id}"
                eval_metrics["eval_cfg"]["context_size"] = ctx_len
                eval_metrics["eval_cfg"]["dataset"] = dataset
                eval_metrics["eval_cfg"]["library_version"] = (
                    eval_config.library_version
                )
                eval_metrics["eval_cfg"]["git_hash"] = eval_config.git_hash
                scalar_metrics, feature_metrics = run_evals(
                    sae=sae,
                    activation_store=activation_store,
                    model=current_model,
                    eval_config=eval_config,
                    ignore_tokens={
                        current_model.tokenizer.pad_token_id,  # type: ignore
                        current_model.tokenizer.eos_token_id,  # type: ignore
                        current_model.tokenizer.bos_token_id,  # type: ignore
                    },
                    verbose=verbose,
                )
                eval_metrics["metrics"] = scalar_metrics
                eval_metrics["feature_metrics"] = feature_metrics
                # Add SAE config
                eval_metrics["sae_cfg"] = sae.cfg.to_dict()
                # Add eval config
                eval_metrics["eval_cfg"].update(eval_config.__dict__)
                eval_results.append(eval_metrics)
    return eval_results
def run_evaluations(args: argparse.Namespace) -> List[Dict[str, Any]]:
    # Filter SAEs based on regex patterns
    filtered_saes = get_saes_from_regex(args.sae_regex_pattern, args.sae_block_pattern)
    num_sae_sets = len(set(sae_set for sae_set, _, _, _ in filtered_saes))
    num_all_sae_ids = len(filtered_saes)
    print("Filtered SAEs based on provided patterns:")
    print(f"Number of SAE sets: {num_sae_sets}")
    print(f"Total number of SAE IDs: {num_all_sae_ids}")
    return multiple_evals(
        sae_regex_pattern=args.sae_regex_pattern,
        sae_block_pattern=args.sae_block_pattern,
        n_eval_reconstruction_batches=args.n_eval_reconstruction_batches,
        n_eval_sparsity_variance_batches=args.n_eval_sparsity_variance_batches,
        eval_batch_size_prompts=args.batch_size_prompts,
        datasets=args.datasets,
        ctx_lens=args.ctx_lens,
        output_dir=args.output_dir,
        verbose=args.verbose,
    )
def replace_nans_with_negative_one(obj: Any) -> Any:
    if isinstance(obj, dict):
        return {k: replace_nans_with_negative_one(v) for k, v in obj.items()}
    if isinstance(obj, list):
        return [replace_nans_with_negative_one(item) for item in obj]
    if isinstance(obj, float) and math.isnan(obj):
        return -1
    return obj
def process_results(
    eval_results: List[Dict[str, Any]], output_dir: str
) -> Dict[str, Union[List[Path], Path]]:
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    # Replace NaNs with -1 in each result
    cleaned_results = [
        replace_nans_with_negative_one(result) for result in eval_results
    ]
    # Save individual JSON files
    for result in cleaned_results:
        json_filename = f"{result['unique_id']}_{result['eval_cfg']['context_size']}_{result['eval_cfg']['dataset']}.json".replace(
            "/", "_"
        )
        json_path = output_path / json_filename
        with open(json_path, "w") as f:
            json.dump(result, f, indent=2)
    # Save all results in a single JSON file
    with open(output_path / "all_eval_results.json", "w") as f:
        json.dump(cleaned_results, f, indent=2)
    # Convert to DataFrame and save as CSV
    df = pd.json_normalize(cleaned_results)
    df.to_csv(output_path / "all_eval_results.csv", index=False)
    return {
        "individual_jsons": list(output_path.glob("*.json")),
        "combined_json": output_path / "all_eval_results.json",
        "csv": output_path / "all_eval_results.csv",
    }
if __name__ == "__main__":
    arg_parser = argparse.ArgumentParser(description="Run evaluations on SAEs")
    arg_parser.add_argument(
        "sae_regex_pattern",
        type=str,
        help="Regex pattern to match SAE names. Can be an entire SAE name to match a specific SAE.",
    )
    arg_parser.add_argument(
        "sae_block_pattern",
        type=str,
        help="Regex pattern to match SAE block names. Can be an entire block name to match a specific block.",
    )
    arg_parser.add_argument(
        "--batch_size_prompts",
        type=int,
        default=16,
        help="Batch size for evaluation prompts.",
    )
    arg_parser.add_argument(
        "--n_eval_reconstruction_batches",
        type=int,
        default=10,
        help="Number of evaluation batches for reconstruction metrics.",
    )
    arg_parser.add_argument(
        "--compute_kl",
        action="store_true",
        help="Compute KL divergence.",
    )
    arg_parser.add_argument(
        "--compute_ce_loss",
        action="store_true",
        help="Compute cross-entropy loss.",
    )
    arg_parser.add_argument(
        "--n_eval_sparsity_variance_batches",
        type=int,
        default=1,
        help="Number of evaluation batches for sparsity and variance metrics.",
    )
    arg_parser.add_argument(
        "--compute_l2_norms",
        action="store_true",
        help="Compute L2 norms.",
    )
    arg_parser.add_argument(
        "--compute_sparsity_metrics",
        action="store_true",
        help="Compute sparsity metrics.",
    )
    arg_parser.add_argument(
        "--compute_variance_metrics",
        action="store_true",
        help="Compute variance metrics.",
    )
    arg_parser.add_argument(
        "--compute_featurewise_density_statistics",
        action="store_true",
        help="Compute featurewise density statistics.",
    )
    arg_parser.add_argument(
        "--compute_featurewise_weight_based_metrics",
        action="store_true",
        help="Compute featurewise weight-based metrics.",
    )
    arg_parser.add_argument(
        "--datasets",
        nargs="+",
        default=["Skylion007/openwebtext"],
        help="Datasets to evaluate on, such as 'Skylion007/openwebtext' or 'lighteval/MATH'.",
    )
    arg_parser.add_argument(
        "--ctx_lens",
        nargs="+",
        type=int,
        default=[128],
        help="Context lengths to evaluate on.",
    )
    arg_parser.add_argument(
        "--output_dir",
        type=str,
        default="eval_results",
        help="Directory to save evaluation results",
    )
    arg_parser.add_argument(
        "--verbose",
        action="store_true",
        help="Enable verbose output with tqdm loaders.",
    )
    args = arg_parser.parse_args()
    eval_results = run_evaluations(args)
    output_files = process_results(eval_results, args.output_dir)
    print("Evaluation complete. Output files:")
    print(f"Individual JSONs: {len(output_files['individual_jsons'])}")  # type: ignore
    print(f"Combined JSON: {output_files['combined_json']}")
    print(f"CSV: {output_files['csv']}")

================
File: sae_lens/load_model.py
================
from typing import Any, Literal, cast
import torch
from transformer_lens import HookedTransformer
from transformer_lens.hook_points import HookedRootModule, HookPoint
from transformer_lens.HookedTransformer import Loss, Output
from transformer_lens.utils import (
    USE_DEFAULT_VALUE,
    get_tokens_with_bos_removed,
    lm_cross_entropy_loss,
)
from transformers import AutoModelForCausalLM, AutoTokenizer, PreTrainedTokenizerBase
from sae_lens import logger
def load_model(
    model_class_name: str,
    model_name: str,
    device: str | torch.device | None = None,
    model_from_pretrained_kwargs: dict[str, Any] | None = None,
) -> HookedRootModule:
    model_from_pretrained_kwargs = model_from_pretrained_kwargs or {}
    if "n_devices" in model_from_pretrained_kwargs:
        n_devices = model_from_pretrained_kwargs["n_devices"]
        if n_devices > 1:
            logger.info("MODEL LOADING:")
            logger.info("Setting model device to cuda for d_devices")
            logger.info(f"Will use cuda:0 to cuda:{n_devices-1}")
            device = "cuda"
            logger.info("-------------")
    if model_class_name == "HookedTransformer":
        return HookedTransformer.from_pretrained_no_processing(
            model_name=model_name, device=device, **model_from_pretrained_kwargs
        )
    if model_class_name == "HookedMamba":
        try:
            from mamba_lens import HookedMamba
        except ImportError:  # pragma: no cover
            raise ValueError(
                "mamba-lens must be installed to work with mamba models. This can be added with `pip install sae-lens[mamba]`"
            )
        # HookedMamba has incorrect typing information, so we need to cast the type here
        return cast(
            HookedRootModule,
            HookedMamba.from_pretrained(
                model_name, device=cast(Any, device), **model_from_pretrained_kwargs
            ),
        )
    if model_class_name == "AutoModelForCausalLM":
        hf_model = AutoModelForCausalLM.from_pretrained(
            model_name, **model_from_pretrained_kwargs
        ).to(device)
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        return HookedProxyLM(hf_model, tokenizer)
    # pragma: no cover
    raise ValueError(f"Unknown model class: {model_class_name}")
class HookedProxyLM(HookedRootModule):
    """
    A HookedRootModule that wraps a Huggingface AutoModelForCausalLM.
    """
    tokenizer: PreTrainedTokenizerBase
    model: torch.nn.Module
    def __init__(self, model: torch.nn.Module, tokenizer: PreTrainedTokenizerBase):
        super().__init__()
        self.model = model
        self.tokenizer = tokenizer
        self.setup()
    # copied and modified from base HookedRootModule
    def setup(self):
        self.mod_dict = {}
        self.hook_dict: dict[str, HookPoint] = {}
        for name, module in self.model.named_modules():
            if name == "":
                continue
            hook_point = HookPoint()
            hook_point.name = name  # type: ignore
            module.register_forward_hook(get_hook_fn(hook_point))
            self.hook_dict[name] = hook_point
            self.mod_dict[name] = hook_point
    def forward(
        self,
        tokens: torch.Tensor,
        return_type: Literal["both", "logits"] = "logits",
        loss_per_token: bool = False,
        # TODO: implement real support for stop_at_layer
        stop_at_layer: int | None = None,
        **kwargs: Any,
    ) -> Output | Loss:
        # This is just what's needed for evals, not everything that HookedTransformer has
        if return_type not in (
            "both",
            "logits",
        ):
            raise NotImplementedError(
                "Only return_type supported is 'both' or 'logits' to match what's in evals.py and ActivationsStore"
            )
        output = self.model(tokens)
        logits = _extract_logits_from_output(output)
        if return_type == "logits":
            return logits
        if tokens.device != logits.device:
            tokens = tokens.to(logits.device)
        loss = lm_cross_entropy_loss(logits, tokens, per_token=loss_per_token)
        return Output(logits, loss)
    def to_tokens(
        self,
        input: str | list[str],
        prepend_bos: bool | None = USE_DEFAULT_VALUE,
        padding_side: Literal["left", "right"] | None = USE_DEFAULT_VALUE,
        move_to_device: bool = True,
        truncate: bool = True,
    ) -> torch.Tensor:
        # Hackily modified version of HookedTransformer.to_tokens to work with ActivationsStore
        # Assumes that prepend_bos is always False, move_to_device is always False, and truncate is always False
        # copied from HookedTransformer.to_tokens
        if prepend_bos is not False:
            raise ValueError(
                "Only works with prepend_bos=False, to match ActivationsStore usage"
            )
        if padding_side is not None:
            raise ValueError(
                "Only works with padding_side=None, to match ActivationsStore usage"
            )
        if truncate is not False:
            raise ValueError(
                "Only works with truncate=False, to match ActivationsStore usage"
            )
        if move_to_device is not False:
            raise ValueError(
                "Only works with move_to_device=False, to match ActivationsStore usage"
            )
        tokens = self.tokenizer(
            input,
            return_tensors="pt",
            truncation=False,
            max_length=None,
        )["input_ids"]
        # We don't want to prepend bos but the tokenizer does it automatically, so we remove it manually
        if hasattr(self.tokenizer, "add_bos_token") and self.tokenizer.add_bos_token:  # type: ignore
            tokens = get_tokens_with_bos_removed(self.tokenizer, tokens)
        return tokens  # type: ignore
def _extract_logits_from_output(output: Any) -> torch.Tensor:
    if isinstance(output, torch.Tensor):
        return output
    if isinstance(output, tuple) and isinstance(output[0], torch.Tensor):
        return output[0]
    if isinstance(output, dict) and "logits" in output:
        return output["logits"]
    raise ValueError(f"Unknown output type: {type(output)}")
def get_hook_fn(hook_point: HookPoint):
    def hook_fn(module: Any, input: Any, output: Any) -> Any:  # noqa: ARG001
        if isinstance(output, torch.Tensor):
            return hook_point(output)
        if isinstance(output, tuple) and isinstance(output[0], torch.Tensor):
            return (hook_point(output[0]), *output[1:])
        # if this isn't a tensor, just skip the hook entirely as this will break otherwise
        return output
    return hook_fn

================
File: sae_lens/pretokenize_runner.py
================
import io
import json
import sys
from dataclasses import dataclass
from pathlib import Path
from typing import Iterator, Literal, cast
import torch
from datasets import Dataset, DatasetDict, load_dataset
from huggingface_hub import HfApi
from transformers import AutoTokenizer, PreTrainedTokenizerBase
from typing_extensions import deprecated
from sae_lens import __version__
from sae_lens.config import PretokenizeRunnerConfig
from sae_lens.tokenization_and_batching import concat_and_batch_sequences
@dataclass
class PretokenizedDatasetMetadata:
    """
    This metadata will be saved along with the pretokenized dataset as a JSON file.
    """
    sae_lens_version: str
    tokenizer_name: str
    original_dataset: str
    original_dataset_name: str | None
    original_split: str | None
    original_data_files: list[str] | None
    original_column_name: str | None
    context_size: int
    shuffled: bool
    seed: int | None
    begin_batch_token: int | Literal["bos", "eos", "sep"] | None
    begin_sequence_token: int | Literal["bos", "eos", "sep"] | None
    sequence_separator_token: int | Literal["bos", "eos", "sep"] | None
def metadata_from_config(cfg: PretokenizeRunnerConfig) -> PretokenizedDatasetMetadata:
    return PretokenizedDatasetMetadata(
        sae_lens_version=__version__,
        tokenizer_name=cfg.tokenizer_name,
        original_dataset=cfg.dataset_path,
        original_dataset_name=cfg.dataset_name,
        original_split=cfg.split,
        original_data_files=cfg.data_files,
        original_column_name=cfg.column_name,
        context_size=cfg.context_size,
        shuffled=cfg.shuffle,
        seed=cfg.seed,
        begin_batch_token=cfg.begin_batch_token,
        begin_sequence_token=cfg.begin_sequence_token,
        sequence_separator_token=cfg.sequence_separator_token,
    )
def get_special_token_from_cfg(
    cfg_token: int | Literal["bos", "eos", "sep"] | None,
    tokenizer: PreTrainedTokenizerBase,
) -> int | None:
    if cfg_token is None:
        return None
    if isinstance(cfg_token, int):
        return cfg_token
    if cfg_token == "bos":
        return tokenizer.bos_token_id  # type: ignore
    if cfg_token == "eos":
        return tokenizer.eos_token_id  # type: ignore
    if cfg_token == "sep":
        return tokenizer.sep_token_id  # type: ignore
    raise ValueError(f"Invalid token type: {cfg_token}")
def pretokenize_dataset(
    dataset: Dataset,
    tokenizer: PreTrainedTokenizerBase,
    cfg: PretokenizeRunnerConfig,
):
    def process_examples(examples: dict[str, list[str]]):
        tokens_iterator = cast(
            Iterator[torch.Tensor],
            (
                tokenizer.encode(text, return_tensors="pt")[0]
                for text in examples[cfg.column_name]
            ),
        )
        return {
            "input_ids": list(
                concat_and_batch_sequences(
                    tokens_iterator=tokens_iterator,
                    context_size=cfg.context_size,
                    begin_batch_token_id=get_special_token_from_cfg(
                        cfg.begin_batch_token, tokenizer
                    ),
                    begin_sequence_token_id=get_special_token_from_cfg(
                        cfg.begin_sequence_token, tokenizer
                    ),
                    sequence_separator_token_id=get_special_token_from_cfg(
                        cfg.sequence_separator_token, tokenizer
                    ),
                )
            )
        }
    if cfg.streaming:
        if cfg.num_proc > 1:
            raise ValueError("num_proc must be 1 when streaming is True")
        tokenized_dataset = dataset.map(
            process_examples,
            batched=True,
            batch_size=cfg.pretokenize_batch_size,
            remove_columns=dataset.column_names,
        )
    else:
        tokenized_dataset = dataset.map(
            process_examples,
            batched=True,
            batch_size=cfg.pretokenize_batch_size,
            num_proc=cfg.num_proc,
            remove_columns=dataset.column_names,
        )
    if cfg.shuffle:
        tokenized_dataset = tokenized_dataset.shuffle(seed=cfg.seed)
    if cfg.streaming:
        tokenized_dataset = tokenized_dataset.with_format(type="torch")
    else:
        tokenized_dataset.set_format(type="torch", columns=["input_ids"])
    return tokenized_dataset
def push_to_hugging_face_hub(
    dataset: Dataset,
    cfg: PretokenizeRunnerConfig,
):
    assert cfg.hf_repo_id is not None
    dataset.push_to_hub(
        repo_id=cfg.hf_repo_id,
        num_shards=cfg.hf_num_shards,
        private=cfg.hf_is_private_repo,
        revision=cfg.hf_revision,
    )
    # also upload metadata file
    metadata = metadata_from_config(cfg)
    meta_io = io.BytesIO()
    meta_contents = json.dumps(metadata.__dict__, indent=2, ensure_ascii=False).encode(
        "utf-8"
    )
    meta_io.write(meta_contents)
    meta_io.seek(0)
    api = HfApi()
    api.upload_file(
        path_or_fileobj=meta_io,
        path_in_repo="sae_lens.json",
        repo_id=cfg.hf_repo_id,
        repo_type="dataset",
        commit_message="Add sae_lens metadata",
    )
@deprecated("Use PretokenizeRunner instead")
def pretokenize_runner(
    cfg: PretokenizeRunnerConfig,
):
    runner = PretokenizeRunner(cfg)
    return runner.run()
class PretokenizeRunner:
    """
    Runner to pretokenize a dataset using a given tokenizer, and optionally upload to Huggingface.
    """
    def __init__(self, cfg: PretokenizeRunnerConfig):
        self.cfg = cfg
    def run(self):
        """
        Load the dataset, tokenize it, and save it to disk and/or upload to Huggingface.
        """
        dataset = load_dataset(
            self.cfg.dataset_path,
            name=self.cfg.dataset_name,
            data_dir=self.cfg.data_dir,
            data_files=self.cfg.data_files,
            split=self.cfg.split,
            streaming=self.cfg.streaming,
        )
        if isinstance(dataset, DatasetDict):
            raise ValueError(
                "Dataset has multiple splits. Must provide a 'split' param."
            )
        tokenizer = AutoTokenizer.from_pretrained(self.cfg.tokenizer_name)
        tokenizer.model_max_length = sys.maxsize
        tokenized_dataset = pretokenize_dataset(
            cast(Dataset, dataset), tokenizer, self.cfg
        )
        if self.cfg.save_path is not None:
            tokenized_dataset.save_to_disk(self.cfg.save_path)
            metadata = metadata_from_config(self.cfg)
            metadata_path = Path(self.cfg.save_path) / "sae_lens.json"
            with open(metadata_path, "w") as f:
                json.dump(metadata.__dict__, f, indent=2, ensure_ascii=False)
        if self.cfg.hf_repo_id is not None:
            push_to_hugging_face_hub(tokenized_dataset, self.cfg)
        return tokenized_dataset

================
File: sae_lens/sae_training_runner.py
================
import json
import signal
import sys
from collections.abc import Sequence
from pathlib import Path
from typing import Any, cast
import torch
import wandb
from simple_parsing import ArgumentParser
from transformer_lens.hook_points import HookedRootModule
from sae_lens import logger
from sae_lens.config import HfDataset, LanguageModelSAERunnerConfig
from sae_lens.load_model import load_model
from sae_lens.training.activations_store import ActivationsStore
from sae_lens.training.geometric_median import compute_geometric_median
from sae_lens.training.sae_trainer import SAETrainer
from sae_lens.training.training_sae import TrainingSAE, TrainingSAEConfig
class InterruptedException(Exception):
    pass
def interrupt_callback(sig_num: Any, stack_frame: Any):  # noqa: ARG001
    raise InterruptedException()
class SAETrainingRunner:
    """
    Class to run the training of a Sparse Autoencoder (SAE) on a TransformerLens model.
    """
    cfg: LanguageModelSAERunnerConfig
    model: HookedRootModule
    sae: TrainingSAE
    activations_store: ActivationsStore
    def __init__(
        self,
        cfg: LanguageModelSAERunnerConfig,
        override_dataset: HfDataset | None = None,
        override_model: HookedRootModule | None = None,
        override_sae: TrainingSAE | None = None,
    ):
        if override_dataset is not None:
            logger.warning(
                f"You just passed in a dataset which will override the one specified in your configuration: {cfg.dataset_path}. As a consequence this run will not be reproducible via configuration alone."
            )
        if override_model is not None:
            logger.warning(
                f"You just passed in a model which will override the one specified in your configuration: {cfg.model_name}. As a consequence this run will not be reproducible via configuration alone."
            )
        self.cfg = cfg
        if override_model is None:
            self.model = load_model(
                self.cfg.model_class_name,
                self.cfg.model_name,
                device=self.cfg.device,
                model_from_pretrained_kwargs=self.cfg.model_from_pretrained_kwargs,
            )
        else:
            self.model = override_model
        self.activations_store = ActivationsStore.from_config(
            self.model,
            self.cfg,
            override_dataset=override_dataset,
        )
        if override_sae is None:
            if self.cfg.from_pretrained_path is not None:
                self.sae = TrainingSAE.load_from_pretrained(
                    self.cfg.from_pretrained_path, self.cfg.device
                )
            else:
                self.sae = TrainingSAE(
                    TrainingSAEConfig.from_dict(
                        self.cfg.get_training_sae_cfg_dict(),
                    )
                )
                self._init_sae_group_b_decs()
        else:
            self.sae = override_sae
    def run(self):
        """
        Run the training of the SAE.
        """
        if self.cfg.log_to_wandb:
            wandb.init(
                project=self.cfg.wandb_project,
                entity=self.cfg.wandb_entity,
                config=cast(Any, self.cfg),
                name=self.cfg.run_name,
                id=self.cfg.wandb_id,
            )
        trainer = SAETrainer(
            model=self.model,
            sae=self.sae,
            activation_store=self.activations_store,
            save_checkpoint_fn=self.save_checkpoint,
            cfg=self.cfg,
        )
        self._compile_if_needed()
        sae = self.run_trainer_with_interruption_handling(trainer)
        if self.cfg.log_to_wandb:
            wandb.finish()
        return sae
    def _compile_if_needed(self):
        # Compile model and SAE
        #  torch.compile can provide significant speedups (10-20% in testing)
        # using max-autotune gives the best speedups but:
        # (a) increases VRAM usage,
        # (b) can't be used on both SAE and LM (some issue with cudagraphs), and
        # (c) takes some time to compile
        # optimal settings seem to be:
        # use max-autotune on SAE and max-autotune-no-cudagraphs on LM
        # (also pylance seems to really hate this)
        if self.cfg.compile_llm:
            self.model = torch.compile(
                self.model,
                mode=self.cfg.llm_compilation_mode,
            )  # type: ignore
        if self.cfg.compile_sae:
            backend = "aot_eager" if self.cfg.device == "mps" else "inductor"
            self.sae.training_forward_pass = torch.compile(  # type: ignore
                self.sae.training_forward_pass,
                mode=self.cfg.sae_compilation_mode,
                backend=backend,
            )  # type: ignore
    def run_trainer_with_interruption_handling(self, trainer: SAETrainer):
        try:
            # signal handlers (if preempted)
            signal.signal(signal.SIGINT, interrupt_callback)
            signal.signal(signal.SIGTERM, interrupt_callback)
            # train SAE
            sae = trainer.fit()
        except (KeyboardInterrupt, InterruptedException):
            logger.warning("interrupted, saving progress")
            checkpoint_name = str(trainer.n_training_tokens)
            self.save_checkpoint(trainer, checkpoint_name=checkpoint_name)
            logger.info("done saving")
            raise
        return sae
    # TODO: move this into the SAE trainer or Training SAE class
    def _init_sae_group_b_decs(
        self,
    ) -> None:
        """
        extract all activations at a certain layer and use for sae b_dec initialization
        """
        if self.cfg.b_dec_init_method == "geometric_median":
            layer_acts = self.activations_store.storage_buffer.detach()[:, 0, :]
            # get geometric median of the activations if we're using those.
            median = compute_geometric_median(
                layer_acts,
                maxiter=100,
            ).median
            self.sae.initialize_b_dec_with_precalculated(median)  # type: ignore
        elif self.cfg.b_dec_init_method == "mean":
            layer_acts = self.activations_store.storage_buffer.detach().cpu()[:, 0, :]
            self.sae.initialize_b_dec_with_mean(layer_acts)  # type: ignore
    @staticmethod
    def save_checkpoint(
        trainer: SAETrainer,
        checkpoint_name: str,
        wandb_aliases: list[str] | None = None,
    ) -> None:
        base_path = Path(trainer.cfg.checkpoint_path) / checkpoint_name
        base_path.mkdir(exist_ok=True, parents=True)
        trainer.activations_store.save(
            str(base_path / "activations_store_state.safetensors")
        )
        if trainer.sae.cfg.normalize_sae_decoder:
            trainer.sae.set_decoder_norm_to_unit_norm()
        weights_path, cfg_path, sparsity_path = trainer.sae.save_model(
            str(base_path),
            trainer.log_feature_sparsity,
        )
        # let's over write the cfg file with the trainer cfg, which is a super set of the original cfg.
        # and should not cause issues but give us more info about SAEs we trained in SAE Lens.
        config = trainer.cfg.to_dict()
        with open(cfg_path, "w") as f:
            json.dump(config, f)
        if trainer.cfg.log_to_wandb:
            # Avoid wandb saving errors such as:
            #   ValueError: Artifact name may only contain alphanumeric characters, dashes, underscores, and dots. Invalid name: sae_google/gemma-2b_etc
            sae_name = trainer.sae.get_name().replace("/", "__")
            # save model weights and cfg
            model_artifact = wandb.Artifact(
                sae_name,
                type="model",
                metadata=dict(trainer.cfg.__dict__),
            )
            model_artifact.add_file(str(weights_path))
            model_artifact.add_file(str(cfg_path))
            wandb.log_artifact(model_artifact, aliases=wandb_aliases)
            # save log feature sparsity
            sparsity_artifact = wandb.Artifact(
                f"{sae_name}_log_feature_sparsity",
                type="log_feature_sparsity",
                metadata=dict(trainer.cfg.__dict__),
            )
            sparsity_artifact.add_file(str(sparsity_path))
            wandb.log_artifact(sparsity_artifact)
def _parse_cfg_args(args: Sequence[str]) -> LanguageModelSAERunnerConfig:
    if len(args) == 0:
        args = ["--help"]
    parser = ArgumentParser(exit_on_error=False)
    parser.add_arguments(LanguageModelSAERunnerConfig, dest="cfg")
    return parser.parse_args(args).cfg
# moved into its own function to make it easier to test
def _run_cli(args: Sequence[str]):
    cfg = _parse_cfg_args(args)
    SAETrainingRunner(cfg=cfg).run()
if __name__ == "__main__":
    _run_cli(args=sys.argv[1:])

================
File: sae_lens/sae.py
================
"""Most of this is just copied over from Arthur's code and slightly simplified:
https://github.com/ArthurConmy/sae/blob/main/sae/model.py
"""
import json
import os
import warnings
from contextlib import contextmanager
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Callable, Literal, Optional, Tuple, TypeVar, Union, overload
import einops
import torch
from jaxtyping import Float
from safetensors.torch import save_file
from torch import nn
from transformer_lens.hook_points import HookedRootModule, HookPoint
from sae_lens.config import DTYPE_MAP
from sae_lens.toolkit.pretrained_sae_loaders import (
    NAMED_PRETRAINED_SAE_LOADERS,
    get_conversion_loader_name,
    handle_config_defaulting,
    read_sae_from_disk,
)
from sae_lens.toolkit.pretrained_saes_directory import (
    get_norm_scaling_factor,
    get_pretrained_saes_directory,
)
SPARSITY_FILENAME = "sparsity.safetensors"
SAE_WEIGHTS_FILENAME = "sae_weights.safetensors"
SAE_CFG_FILENAME = "cfg.json"
T = TypeVar("T", bound="SAE")
@dataclass
class SAEConfig:
    # architecture details
    architecture: Literal["standard", "gated", "jumprelu", "topk"]
    # forward pass details.
    d_in: int
    d_sae: int
    activation_fn_str: str
    apply_b_dec_to_input: bool
    finetuning_scaling_factor: bool
    # dataset it was trained on details.
    context_size: int
    model_name: str
    hook_name: str
    hook_layer: int
    hook_head_index: Optional[int]
    prepend_bos: bool
    dataset_path: str
    dataset_trust_remote_code: bool
    normalize_activations: str
    # misc
    dtype: str
    device: str
    sae_lens_training_version: Optional[str]
    activation_fn_kwargs: dict[str, Any] = field(default_factory=dict)
    neuronpedia_id: Optional[str] = None
    model_from_pretrained_kwargs: dict[str, Any] = field(default_factory=dict)
    seqpos_slice: tuple[int | None, ...] = (None,)
    @classmethod
    def from_dict(cls, config_dict: dict[str, Any]) -> "SAEConfig":
        # rename dict:
        rename_dict = {  # old : new
            "hook_point": "hook_name",
            "hook_point_head_index": "hook_head_index",
            "hook_point_layer": "hook_layer",
            "activation_fn": "activation_fn_str",
        }
        config_dict = {rename_dict.get(k, k): v for k, v in config_dict.items()}
        # use only config terms that are in the dataclass
        config_dict = {
            k: v
            for k, v in config_dict.items()
            if k in cls.__dataclass_fields__  # pylint: disable=no-member
        }
        if "seqpos_slice" in config_dict:
            config_dict["seqpos_slice"] = tuple(config_dict["seqpos_slice"])
        return cls(**config_dict)
    # def __post_init__(self):
    def to_dict(self) -> dict[str, Any]:
        return {
            "architecture": self.architecture,
            "d_in": self.d_in,
            "d_sae": self.d_sae,
            "dtype": self.dtype,
            "device": self.device,
            "model_name": self.model_name,
            "hook_name": self.hook_name,
            "hook_layer": self.hook_layer,
            "hook_head_index": self.hook_head_index,
            "activation_fn_str": self.activation_fn_str,  # use string for serialization
            "activation_fn_kwargs": self.activation_fn_kwargs or {},
            "apply_b_dec_to_input": self.apply_b_dec_to_input,
            "finetuning_scaling_factor": self.finetuning_scaling_factor,
            "sae_lens_training_version": self.sae_lens_training_version,
            "prepend_bos": self.prepend_bos,
            "dataset_path": self.dataset_path,
            "dataset_trust_remote_code": self.dataset_trust_remote_code,
            "context_size": self.context_size,
            "normalize_activations": self.normalize_activations,
            "neuronpedia_id": self.neuronpedia_id,
            "model_from_pretrained_kwargs": self.model_from_pretrained_kwargs,
            "seqpos_slice": self.seqpos_slice,
        }
class SAE(HookedRootModule):
    """
    Core Sparse Autoencoder (SAE) class used for inference. For training, see `TrainingSAE`.
    """
    cfg: SAEConfig
    dtype: torch.dtype
    device: torch.device
    x_norm_coeff: torch.Tensor
    # analysis
    use_error_term: bool
    def __init__(
        self,
        cfg: SAEConfig,
        use_error_term: bool = False,
    ):
        super().__init__()
        self.cfg = cfg
        if cfg.model_from_pretrained_kwargs:
            warnings.warn(
                "\nThis SAE has non-empty model_from_pretrained_kwargs. "
                "\nFor optimal performance, load the model like so:\n"
                "model = HookedSAETransformer.from_pretrained_no_processing(..., **cfg.model_from_pretrained_kwargs)",
                category=UserWarning,
                stacklevel=1,
            )
        self.activation_fn = get_activation_fn(
            cfg.activation_fn_str, **cfg.activation_fn_kwargs or {}
        )
        self.dtype = DTYPE_MAP[cfg.dtype]
        self.device = torch.device(cfg.device)
        self.use_error_term = use_error_term
        if self.cfg.architecture == "standard" or self.cfg.architecture == "topk":
            self.initialize_weights_basic()
            self.encode = self.encode_standard
        elif self.cfg.architecture == "gated":
            self.initialize_weights_gated()
            self.encode = self.encode_gated
        elif self.cfg.architecture == "jumprelu":
            self.initialize_weights_jumprelu()
            self.encode = self.encode_jumprelu
        else:
            raise ValueError(f"Invalid architecture: {self.cfg.architecture}")
        # handle presence / absence of scaling factor.
        if self.cfg.finetuning_scaling_factor:
            self.apply_finetuning_scaling_factor = (
                lambda x: x * self.finetuning_scaling_factor
            )
        else:
            self.apply_finetuning_scaling_factor = lambda x: x
        # set up hooks
        self.hook_sae_input = HookPoint()
        self.hook_sae_acts_pre = HookPoint()
        self.hook_sae_acts_post = HookPoint()
        self.hook_sae_output = HookPoint()
        self.hook_sae_recons = HookPoint()
        self.hook_sae_error = HookPoint()
        # handle hook_z reshaping if needed.
        # this is very cursed and should be refactored. it exists so that we can reshape out
        # the z activations for hook_z SAEs. but don't know d_head if we split up the forward pass
        # into a separate encode and decode function.
        # this will cause errors if we call decode before encode.
        if self.cfg.hook_name.endswith("_z"):
            self.turn_on_forward_pass_hook_z_reshaping()
        else:
            # need to default the reshape fns
            self.turn_off_forward_pass_hook_z_reshaping()
        # handle run time activation normalization if needed:
        if self.cfg.normalize_activations == "constant_norm_rescale":
            #  we need to scale the norm of the input and store the scaling factor
            def run_time_activation_norm_fn_in(x: torch.Tensor) -> torch.Tensor:
                self.x_norm_coeff = (self.cfg.d_in**0.5) / x.norm(dim=-1, keepdim=True)
                return x * self.x_norm_coeff
            def run_time_activation_norm_fn_out(x: torch.Tensor) -> torch.Tensor:  #
                x = x / self.x_norm_coeff
                del self.x_norm_coeff  # prevents reusing
                return x
            self.run_time_activation_norm_fn_in = run_time_activation_norm_fn_in
            self.run_time_activation_norm_fn_out = run_time_activation_norm_fn_out
        elif self.cfg.normalize_activations == "layer_norm":
            #  we need to scale the norm of the input and store the scaling factor
            def run_time_activation_ln_in(
                x: torch.Tensor, eps: float = 1e-5
            ) -> torch.Tensor:
                mu = x.mean(dim=-1, keepdim=True)
                x = x - mu
                std = x.std(dim=-1, keepdim=True)
                x = x / (std + eps)
                self.ln_mu = mu
                self.ln_std = std
                return x
            def run_time_activation_ln_out(
                x: torch.Tensor,
                eps: float = 1e-5,  # noqa: ARG001
            ) -> torch.Tensor:
                return x * self.ln_std + self.ln_mu  # type: ignore
            self.run_time_activation_norm_fn_in = run_time_activation_ln_in
            self.run_time_activation_norm_fn_out = run_time_activation_ln_out
        else:
            self.run_time_activation_norm_fn_in = lambda x: x
            self.run_time_activation_norm_fn_out = lambda x: x
        self.setup()  # Required for `HookedRootModule`s
    def initialize_weights_basic(self):
        # no config changes encoder bias init for now.
        self.b_enc = nn.Parameter(
            torch.zeros(self.cfg.d_sae, dtype=self.dtype, device=self.device)
        )
        # Start with the default init strategy:
        self.W_dec = nn.Parameter(
            torch.nn.init.kaiming_uniform_(
                torch.empty(
                    self.cfg.d_sae, self.cfg.d_in, dtype=self.dtype, device=self.device
                )
            )
        )
        self.W_enc = nn.Parameter(
            torch.nn.init.kaiming_uniform_(
                torch.empty(
                    self.cfg.d_in, self.cfg.d_sae, dtype=self.dtype, device=self.device
                )
            )
        )
        # methdods which change b_dec as a function of the dataset are implemented after init.
        self.b_dec = nn.Parameter(
            torch.zeros(self.cfg.d_in, dtype=self.dtype, device=self.device)
        )
        # scaling factor for fine-tuning (not to be used in initial training)
        # TODO: Make this optional and not included with all SAEs by default (but maintain backwards compatibility)
        if self.cfg.finetuning_scaling_factor:
            self.finetuning_scaling_factor = nn.Parameter(
                torch.ones(self.cfg.d_sae, dtype=self.dtype, device=self.device)
            )
    def initialize_weights_gated(self):
        # Initialize the weights and biases for the gated encoder
        self.W_enc = nn.Parameter(
            torch.nn.init.kaiming_uniform_(
                torch.empty(
                    self.cfg.d_in, self.cfg.d_sae, dtype=self.dtype, device=self.device
                )
            )
        )
        self.b_gate = nn.Parameter(
            torch.zeros(self.cfg.d_sae, dtype=self.dtype, device=self.device)
        )
        self.r_mag = nn.Parameter(
            torch.zeros(self.cfg.d_sae, dtype=self.dtype, device=self.device)
        )
        self.b_mag = nn.Parameter(
            torch.zeros(self.cfg.d_sae, dtype=self.dtype, device=self.device)
        )
        self.W_dec = nn.Parameter(
            torch.nn.init.kaiming_uniform_(
                torch.empty(
                    self.cfg.d_sae, self.cfg.d_in, dtype=self.dtype, device=self.device
                )
            )
        )
        self.b_dec = nn.Parameter(
            torch.zeros(self.cfg.d_in, dtype=self.dtype, device=self.device)
        )
    def initialize_weights_jumprelu(self):
        # The params are identical to the standard SAE
        # except we use a threshold parameter too
        self.threshold = nn.Parameter(
            torch.zeros(self.cfg.d_sae, dtype=self.dtype, device=self.device)
        )
        self.initialize_weights_basic()
    @overload
    def to(
        self: T,
        device: Optional[Union[torch.device, str]] = ...,
        dtype: Optional[torch.dtype] = ...,
        non_blocking: bool = ...,
    ) -> T: ...
    @overload
    def to(self: T, dtype: torch.dtype, non_blocking: bool = ...) -> T: ...
    @overload
    def to(self: T, tensor: torch.Tensor, non_blocking: bool = ...) -> T: ...
    def to(self, *args: Any, **kwargs: Any) -> "SAE":  # type: ignore
        device_arg = None
        dtype_arg = None
        # Check args
        for arg in args:
            if isinstance(arg, (torch.device, str)):
                device_arg = arg
            elif isinstance(arg, torch.dtype):
                dtype_arg = arg
            elif isinstance(arg, torch.Tensor):
                device_arg = arg.device
                dtype_arg = arg.dtype
        # Check kwargs
        device_arg = kwargs.get("device", device_arg)
        dtype_arg = kwargs.get("dtype", dtype_arg)
        if device_arg is not None:
            # Convert device to torch.device if it's a string
            device = (
                torch.device(device_arg) if isinstance(device_arg, str) else device_arg
            )
            # Update the cfg.device
            self.cfg.device = str(device)
            # Update the .device property
            self.device = device
        if dtype_arg is not None:
            # Update the cfg.dtype
            self.cfg.dtype = str(dtype_arg)
            # Update the .dtype property
            self.dtype = dtype_arg
        # Call the parent class's to() method to handle all cases (device, dtype, tensor)
        return super().to(*args, **kwargs)
    # Basic Forward Pass Functionality.
    def forward(
        self,
        x: torch.Tensor,
    ) -> torch.Tensor:
        feature_acts = self.encode(x)
        sae_out = self.decode(feature_acts)
        # TEMP
        if self.use_error_term:
            with torch.no_grad():
                # Recompute everything without hooks to get true error term
                # Otherwise, the output with error term will always equal input, even for causal interventions that affect x_reconstruct
                # This is in a no_grad context to detach the error, so we can compute SAE feature gradients (eg for attribution patching). See A.3 in https://arxiv.org/pdf/2403.19647.pdf for more detail
                # NOTE: we can't just use `sae_error = input - x_reconstruct.detach()` or something simpler, since this would mean intervening on features would mean ablating features still results in perfect reconstruction.
                with _disable_hooks(self):
                    feature_acts_clean = self.encode(x)
                    x_reconstruct_clean = self.decode(feature_acts_clean)
                sae_error = self.hook_sae_error(x - x_reconstruct_clean)
            sae_out = sae_out + sae_error
        return self.hook_sae_output(sae_out)
    def encode_gated(
        self, x: Float[torch.Tensor, "... d_in"]
    ) -> Float[torch.Tensor, "... d_sae"]:
        sae_in = self.process_sae_in(x)
        # Gating path
        gating_pre_activation = sae_in @ self.W_enc + self.b_gate
        active_features = (gating_pre_activation > 0).to(self.dtype)
        # Magnitude path with weight sharing
        magnitude_pre_activation = self.hook_sae_acts_pre(
            sae_in @ (self.W_enc * self.r_mag.exp()) + self.b_mag
        )
        feature_magnitudes = self.activation_fn(magnitude_pre_activation)
        return self.hook_sae_acts_post(active_features * feature_magnitudes)
    def encode_jumprelu(
        self, x: Float[torch.Tensor, "... d_in"]
    ) -> Float[torch.Tensor, "... d_sae"]:
        """
        Calculate SAE features from inputs
        """
        sae_in = self.process_sae_in(x)
        # "... d_in, d_in d_sae -> ... d_sae",
        hidden_pre = self.hook_sae_acts_pre(sae_in @ self.W_enc + self.b_enc)
        return self.hook_sae_acts_post(
            self.activation_fn(hidden_pre) * (hidden_pre > self.threshold)
        )
    def encode_standard(
        self, x: Float[torch.Tensor, "... d_in"]
    ) -> Float[torch.Tensor, "... d_sae"]:
        """
        Calculate SAE features from inputs
        """
        sae_in = self.process_sae_in(x)
        # "... d_in, d_in d_sae -> ... d_sae",
        hidden_pre = self.hook_sae_acts_pre(sae_in @ self.W_enc + self.b_enc)
        return self.hook_sae_acts_post(self.activation_fn(hidden_pre))
    def process_sae_in(
        self, sae_in: Float[torch.Tensor, "... d_in"]
    ) -> Float[torch.Tensor, "... d_sae"]:
        sae_in = sae_in.to(self.dtype)
        sae_in = self.reshape_fn_in(sae_in)
        sae_in = self.hook_sae_input(sae_in)
        sae_in = self.run_time_activation_norm_fn_in(sae_in)
        return sae_in - (self.b_dec * self.cfg.apply_b_dec_to_input)
    def decode(
        self, feature_acts: Float[torch.Tensor, "... d_sae"]
    ) -> Float[torch.Tensor, "... d_in"]:
        """Decodes SAE feature activation tensor into a reconstructed input activation tensor."""
        # "... d_sae, d_sae d_in -> ... d_in",
        sae_out = self.hook_sae_recons(
            self.apply_finetuning_scaling_factor(feature_acts) @ self.W_dec + self.b_dec
        )
        # handle run time activation normalization if needed
        # will fail if you call this twice without calling encode in between.
        sae_out = self.run_time_activation_norm_fn_out(sae_out)
        # handle hook z reshaping if needed.
        return self.reshape_fn_out(sae_out, self.d_head)  # type: ignore
    @torch.no_grad()
    def fold_W_dec_norm(self):
        W_dec_norms = self.W_dec.norm(dim=-1).unsqueeze(1)
        self.W_dec.data = self.W_dec.data / W_dec_norms
        self.W_enc.data = self.W_enc.data * W_dec_norms.T
        if self.cfg.architecture == "gated":
            self.r_mag.data = self.r_mag.data * W_dec_norms.squeeze()
            self.b_gate.data = self.b_gate.data * W_dec_norms.squeeze()
            self.b_mag.data = self.b_mag.data * W_dec_norms.squeeze()
        elif self.cfg.architecture == "jumprelu":
            self.threshold.data = self.threshold.data * W_dec_norms.squeeze()
            self.b_enc.data = self.b_enc.data * W_dec_norms.squeeze()
        else:
            self.b_enc.data = self.b_enc.data * W_dec_norms.squeeze()
    @torch.no_grad()
    def fold_activation_norm_scaling_factor(
        self, activation_norm_scaling_factor: float
    ):
        self.W_enc.data = self.W_enc.data * activation_norm_scaling_factor
        # previously weren't doing this.
        self.W_dec.data = self.W_dec.data / activation_norm_scaling_factor
        self.b_dec.data = self.b_dec.data / activation_norm_scaling_factor
        # once we normalize, we shouldn't need to scale activations.
        self.cfg.normalize_activations = "none"
    @overload
    def save_model(self, path: str | Path) -> Tuple[Path, Path]: ...
    @overload
    def save_model(
        self, path: str | Path, sparsity: torch.Tensor
    ) -> Tuple[Path, Path, Path]: ...
    def save_model(self, path: str | Path, sparsity: Optional[torch.Tensor] = None):
        path = Path(path)
        if not path.exists():
            path.mkdir(parents=True)
        # generate the weights
        state_dict = self.state_dict()
        self.process_state_dict_for_saving(state_dict)
        model_weights_path = path / SAE_WEIGHTS_FILENAME
        save_file(state_dict, model_weights_path)
        # save the config
        config = self.cfg.to_dict()
        cfg_path = path / SAE_CFG_FILENAME
        with open(cfg_path, "w") as f:
            json.dump(config, f)
        if sparsity is not None:
            sparsity_in_dict = {"sparsity": sparsity}
            sparsity_path = path / SPARSITY_FILENAME
            save_file(sparsity_in_dict, sparsity_path)
            return model_weights_path, cfg_path, sparsity_path
        return model_weights_path, cfg_path
    # overwrite this in subclasses to modify the state_dict in-place before saving
    def process_state_dict_for_saving(self, state_dict: dict[str, Any]) -> None:
        pass
    # overwrite this in subclasses to modify the state_dict in-place after loading
    def process_state_dict_for_loading(self, state_dict: dict[str, Any]) -> None:
        pass
    @classmethod
    def load_from_pretrained(
        cls, path: str, device: str = "cpu", dtype: str | None = None
    ) -> "SAE":
        # get the config
        config_path = os.path.join(path, SAE_CFG_FILENAME)
        with open(config_path) as f:
            cfg_dict = json.load(f)
        cfg_dict = handle_config_defaulting(cfg_dict)
        cfg_dict["device"] = device
        if dtype is not None:
            cfg_dict["dtype"] = dtype
        weight_path = os.path.join(path, SAE_WEIGHTS_FILENAME)
        cfg_dict, state_dict = read_sae_from_disk(
            cfg_dict=cfg_dict,
            weight_path=weight_path,
            device=device,
        )
        sae_cfg = SAEConfig.from_dict(cfg_dict)
        sae = cls(sae_cfg)
        sae.process_state_dict_for_loading(state_dict)
        sae.load_state_dict(state_dict)
        return sae
    @classmethod
    def from_pretrained(
        cls,
        release: str,
        sae_id: str,
        device: str = "cpu",
    ) -> Tuple["SAE", dict[str, Any], Optional[torch.Tensor]]:
        """
        Load a pretrained SAE from the Hugging Face model hub.
        Args:
            release: The release name. This will be mapped to a huggingface repo id based on the pretrained_saes.yaml file.
            id: The id of the SAE to load. This will be mapped to a path in the huggingface repo.
            device: The device to load the SAE on.
            return_sparsity_if_present: If True, will return the log sparsity tensor if it is present in the model directory in the Hugging Face model hub.
        """
        # get sae directory
        sae_directory = get_pretrained_saes_directory()
        # get the repo id and path to the SAE
        if release not in sae_directory:
            if "/" not in release:
                raise ValueError(
                    f"Release {release} not found in pretrained SAEs directory, and is not a valid huggingface repo."
                )
        elif sae_id not in sae_directory[release].saes_map:
            # If using Gemma Scope and not the canonical release, give a hint to use it
            if (
                "gemma-scope" in release
                and "canonical" not in release
                and f"{release}-canonical" in sae_directory
            ):
                canonical_ids = list(
                    sae_directory[release + "-canonical"].saes_map.keys()
                )
                # Shorten the lengthy string of valid IDs
                if len(canonical_ids) > 5:
                    str_canonical_ids = str(canonical_ids[:5])[:-1] + ", ...]"
                else:
                    str_canonical_ids = str(canonical_ids)
                value_suffix = f" If you don't want to specify an L0 value, consider using release {release}-canonical which has valid IDs {str_canonical_ids}"
            else:
                value_suffix = ""
            valid_ids = list(sae_directory[release].saes_map.keys())
            # Shorten the lengthy string of valid IDs
            if len(valid_ids) > 5:
                str_valid_ids = str(valid_ids[:5])[:-1] + ", ...]"
            else:
                str_valid_ids = str(valid_ids)
            raise ValueError(
                f"ID {sae_id} not found in release {release}. Valid IDs are {str_valid_ids}."
                + value_suffix
            )
        sae_info = sae_directory.get(release, None)
        config_overrides = sae_info.config_overrides if sae_info is not None else None
        conversion_loader_name = get_conversion_loader_name(sae_info)
        conversion_loader = NAMED_PRETRAINED_SAE_LOADERS[conversion_loader_name]
        cfg_dict, state_dict, log_sparsities = conversion_loader(
            release,
            sae_id=sae_id,
            device=device,
            force_download=False,
            cfg_overrides=config_overrides,
        )
        sae = cls(SAEConfig.from_dict(cfg_dict))
        sae.process_state_dict_for_loading(state_dict)
        sae.load_state_dict(state_dict)
        # Check if normalization is 'expected_average_only_in'
        if cfg_dict.get("normalize_activations") == "expected_average_only_in":
            norm_scaling_factor = get_norm_scaling_factor(release, sae_id)
            if norm_scaling_factor is not None:
                sae.fold_activation_norm_scaling_factor(norm_scaling_factor)
                cfg_dict["normalize_activations"] = "none"
            else:
                warnings.warn(
                    f"norm_scaling_factor not found for {release} and {sae_id}, but normalize_activations is 'expected_average_only_in'. Skipping normalization folding."
                )
        return sae, cfg_dict, log_sparsities
    def get_name(self):
        return f"sae_{self.cfg.model_name}_{self.cfg.hook_name}_{self.cfg.d_sae}"
    @classmethod
    def from_dict(cls, config_dict: dict[str, Any]) -> "SAE":
        return cls(SAEConfig.from_dict(config_dict))
    def turn_on_forward_pass_hook_z_reshaping(self):
        if not self.cfg.hook_name.endswith("_z"):
            raise ValueError("This method should only be called for hook_z SAEs.")
        def reshape_fn_in(x: torch.Tensor):
            self.d_head = x.shape[-1]  # type: ignore
            self.reshape_fn_in = lambda x: einops.rearrange(
                x, "... n_heads d_head -> ... (n_heads d_head)"
            )
            return einops.rearrange(x, "... n_heads d_head -> ... (n_heads d_head)")
        self.reshape_fn_in = reshape_fn_in
        self.reshape_fn_out = lambda x, d_head: einops.rearrange(
            x, "... (n_heads d_head) -> ... n_heads d_head", d_head=d_head
        )
        self.hook_z_reshaping_mode = True
    def turn_off_forward_pass_hook_z_reshaping(self):
        self.reshape_fn_in = lambda x: x
        self.reshape_fn_out = lambda x, d_head: x  # noqa: ARG005
        self.d_head = None
        self.hook_z_reshaping_mode = False
class TopK(nn.Module):
    def __init__(
        self, k: int, postact_fn: Callable[[torch.Tensor], torch.Tensor] = nn.ReLU()
    ):
        super().__init__()
        self.k = k
        self.postact_fn = postact_fn
    # TODO: Use a fused kernel to speed up topk decoding like https://github.com/EleutherAI/sae/blob/main/sae/kernels.py
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        topk = torch.topk(x, k=self.k, dim=-1)
        values = self.postact_fn(topk.values)
        result = torch.zeros_like(x)
        result.scatter_(-1, topk.indices, values)
        return result
def get_activation_fn(
    activation_fn: str, **kwargs: Any
) -> Callable[[torch.Tensor], torch.Tensor]:
    if activation_fn == "relu":
        return torch.nn.ReLU()
    if activation_fn == "tanh-relu":
        def tanh_relu(input: torch.Tensor) -> torch.Tensor:
            input = torch.relu(input)
            return torch.tanh(input)
        return tanh_relu
    if activation_fn == "topk":
        if "k" not in kwargs:
            raise ValueError("TopK activation function requires a k value.")
        k = kwargs.get("k", 1)  # Default k to 1 if not provided
        postact_fn = kwargs.get(
            "postact_fn", nn.ReLU()
        )  # Default post-activation to ReLU if not provided
        return TopK(k, postact_fn)
    raise ValueError(f"Unknown activation function: {activation_fn}")
_blank_hook = nn.Identity()
@contextmanager
def _disable_hooks(sae: SAE):
    """
    Temporarily disable hooks for the SAE. Swaps out all the hooks with a fake modules that does nothing.
    """
    try:
        for hook_name in sae.hook_dict:
            setattr(sae, hook_name, _blank_hook)
        yield
    finally:
        for hook_name, hook in sae.hook_dict.items():
            setattr(sae, hook_name, hook)

================
File: sae_lens/tokenization_and_batching.py
================
from typing import Generator, Iterator
import torch
def _add_tokens_to_batch(
    batch: torch.Tensor | None,
    tokens: torch.Tensor,
    offset: int,
    context_size: int,
    is_start_of_sequence: bool,
    begin_batch_token_id: int | None = None,
    begin_sequence_token_id: int | None = None,
    sequence_separator_token_id: int | None = None,
) -> tuple[torch.Tensor, int]:
    prefix_toks = []
    first_token = tokens[offset]
    # prepend the start of sequence token if needed
    if is_start_of_sequence and begin_sequence_token_id is not None:
        begin_sequence_token_id_tensor = torch.tensor(
            [begin_sequence_token_id], dtype=torch.long, device=tokens.device
        )
        if first_token != begin_sequence_token_id_tensor:
            prefix_toks.insert(0, begin_sequence_token_id_tensor)
            first_token = begin_sequence_token_id_tensor
    # We're at the start of a new batch
    if batch is None:
        # add the BOS token to the start if needed
        if begin_batch_token_id is not None:
            begin_batch_token_id_tensor = torch.tensor(
                [begin_batch_token_id], dtype=torch.long, device=tokens.device
            )
            if first_token != begin_batch_token_id_tensor:
                prefix_toks.insert(0, begin_batch_token_id_tensor)
                first_token = begin_batch_token_id_tensor
        tokens_needed = max(context_size - len(prefix_toks), 0)
        tokens_part = tokens[offset : offset + tokens_needed]
        batch = torch.cat([*prefix_toks[:context_size], tokens_part])
        return batch, offset + tokens_needed
    # if we're concatting batches, add the separator token as needed
    if sequence_separator_token_id is not None:
        sequence_separator_token_id_tensor = torch.tensor(
            [sequence_separator_token_id], dtype=torch.long, device=tokens.device
        )
        if first_token != sequence_separator_token_id_tensor:
            prefix_toks.insert(0, sequence_separator_token_id_tensor)
            first_token = sequence_separator_token_id_tensor
    tokens_needed = max(context_size - batch.shape[0] - len(prefix_toks), 0)
    prefix_toks_needed = max(context_size - batch.shape[0], 0)
    batch = torch.concat(
        [
            batch,
            *prefix_toks[:prefix_toks_needed],
            tokens[offset : offset + tokens_needed],
        ]
    )
    return batch, offset + tokens_needed
@torch.no_grad()
def concat_and_batch_sequences(
    tokens_iterator: Iterator[torch.Tensor],
    context_size: int,
    begin_batch_token_id: int | None = None,
    begin_sequence_token_id: int | None = None,
    sequence_separator_token_id: int | None = None,
) -> Generator[torch.Tensor, None, None]:
    """
    Generator to concat token sequences together from the tokens_interator, yielding
    batches of size `context_size`.
    Args:
        tokens_iterator: An iterator which returns a 1D tensors of tokens
        context_size: Each batch will have this many tokens
        begin_batch_token_id: If provided, this token will be at position 0 of each batch
        begin_sequence_token_id: If provided, this token will be the first token of each sequence
        sequence_separator_token_id: If provided, this token will be inserted between concatenated sequences
        max_batches: If not provided, the iterator will be run to completion.
    """
    batch: torch.Tensor | None = None
    for tokens in tokens_iterator:
        if len(tokens.shape) != 1:
            raise ValueError(f"tokens.shape should be 1D but was {tokens.shape}")
        offset = 0
        total_toks = tokens.shape[0]
        is_start_of_sequence = True
        while total_toks - offset > 0:
            batch, offset = _add_tokens_to_batch(
                batch=batch,
                tokens=tokens,
                offset=offset,
                context_size=context_size,
                is_start_of_sequence=is_start_of_sequence,
                begin_batch_token_id=begin_batch_token_id,
                begin_sequence_token_id=begin_sequence_token_id,
                sequence_separator_token_id=sequence_separator_token_id,
            )
            is_start_of_sequence = False
            if batch.shape[0] == context_size:
                yield batch
                batch = None

================
File: sae_lens/toolkit/pretrained_sae_loaders.py
================
import json
import re
from dataclasses import dataclass, field
from typing import Any, Dict, Optional, Protocol, Tuple
import numpy as np
import torch
from huggingface_hub import hf_hub_download
from huggingface_hub.utils import EntryNotFoundError
from safetensors import safe_open
from safetensors.torch import load_file
from sae_lens import logger
from sae_lens.config import DTYPE_MAP
from sae_lens.toolkit.pretrained_saes_directory import (
    PretrainedSAELookup,
    get_pretrained_saes_directory,
    get_repo_id_and_folder_name,
)
# loaders take in a release, sae_id, device, and whether to force download, and returns a tuple of config, state_dict, and log sparsity
class PretrainedSaeLoader(Protocol):
    def __call__(
        self,
        release: str,
        sae_id: str,
        device: str | torch.device | None = None,
        force_download: bool = False,
        cfg_overrides: dict[str, Any] | None = None,
    ) -> tuple[dict[str, Any], dict[str, torch.Tensor], Optional[torch.Tensor]]: ...
@dataclass
class SAEConfigLoadOptions:
    device: Optional[str] = None
    force_download: bool = False
    d_sae_override: Optional[int] = None
    layer_override: Optional[int] = None
    cfg_overrides: Optional[Dict[str, Any]] = field(default_factory=dict)
def sae_lens_loader(
    release: str,
    sae_id: str,
    device: str = "cpu",
    force_download: bool = False,
    cfg_overrides: Optional[dict[str, Any]] = None,
) -> tuple[dict[str, Any], dict[str, torch.Tensor], Optional[torch.Tensor]]:
    """
    Get's SAEs from HF, loads them.
    """
    options = SAEConfigLoadOptions(
        device=device,
        force_download=force_download,
        cfg_overrides=cfg_overrides,
    )
    cfg_dict = get_sae_config(release, sae_id=sae_id, options=options)
    repo_id, folder_name = get_repo_id_and_folder_name(release, sae_id=sae_id)
    weights_filename = f"{folder_name}/sae_weights.safetensors"
    sae_path = hf_hub_download(
        repo_id=repo_id, filename=weights_filename, force_download=force_download
    )
    # TODO: Make this cleaner. I hate try except statements.
    try:
        sparsity_filename = f"{folder_name}/sparsity.safetensors"
        log_sparsity_path = hf_hub_download(
            repo_id=repo_id, filename=sparsity_filename, force_download=force_download
        )
    except EntryNotFoundError:
        log_sparsity_path = None  # no sparsity file
    cfg_dict, state_dict = read_sae_from_disk(
        cfg_dict=cfg_dict,
        weight_path=sae_path,
        device=device,
    )
    # get sparsity tensor if it exists
    if log_sparsity_path is not None:
        with safe_open(log_sparsity_path, framework="pt", device=device) as f:  # type: ignore
            log_sparsity = f.get_tensor("sparsity")
    else:
        log_sparsity = None
    return cfg_dict, state_dict, log_sparsity
def get_sae_config_from_hf(
    repo_id: str,
    folder_name: str,
    options: SAEConfigLoadOptions,
) -> Dict[str, Any]:
    """
    Retrieve the configuration for a Sparse Autoencoder (SAE) from a Hugging Face repository.
    Args:
        repo_id (str): The repository ID on Hugging Face.
        folder_name (str): The folder name within the repository containing the config file.
        force_download (bool, optional): Whether to force download the config file. Defaults to False.
        cfg_overrides (Optional[Dict[str, Any]], optional): Overrides for the config. Defaults to None.
    Returns:
        Dict[str, Any]: The configuration dictionary for the SAE.
    """
    cfg_filename = f"{folder_name}/cfg.json"
    cfg_path = hf_hub_download(
        repo_id=repo_id, filename=cfg_filename, force_download=options.force_download
    )
    with open(cfg_path) as f:
        cfg_dict = json.load(f)
    if options.device is not None:
        cfg_dict["device"] = options.device
    return cfg_dict
def handle_config_defaulting(cfg_dict: dict[str, Any]) -> dict[str, Any]:
    # Set default values for backwards compatibility
    cfg_dict.setdefault("prepend_bos", True)
    cfg_dict.setdefault("dataset_trust_remote_code", True)
    cfg_dict.setdefault("apply_b_dec_to_input", True)
    cfg_dict.setdefault("finetuning_scaling_factor", False)
    cfg_dict.setdefault("sae_lens_training_version", None)
    cfg_dict.setdefault("activation_fn_str", cfg_dict.get("activation_fn", "relu"))
    cfg_dict.setdefault("architecture", "standard")
    cfg_dict.setdefault("neuronpedia_id", None)
    if "normalize_activations" in cfg_dict and isinstance(
        cfg_dict["normalize_activations"], bool
    ):
        # backwards compatibility
        cfg_dict["normalize_activations"] = (
            "none"
            if not cfg_dict["normalize_activations"]
            else "expected_average_only_in"
        )
    cfg_dict.setdefault("normalize_activations", "none")
    cfg_dict.setdefault("device", "cpu")
    return cfg_dict
def get_connor_rob_hook_z_config(
    repo_id: str, folder_name: str, options: SAEConfigLoadOptions
) -> dict[str, Any]:
    device = options.device
    config_path = folder_name.split(".pt")[0] + "_cfg.json"
    config_path = hf_hub_download(repo_id, config_path)
    with open(config_path) as config_file:
        old_cfg_dict = json.load(config_file)
    return {
        "architecture": "standard",
        "d_in": old_cfg_dict["act_size"],
        "d_sae": old_cfg_dict["dict_size"],
        "dtype": "float32",
        "device": device if device is not None else "cpu",
        "model_name": "gpt2-small",
        "hook_name": old_cfg_dict["act_name"],
        "hook_layer": old_cfg_dict["layer"],
        "hook_head_index": None,
        "activation_fn_str": "relu",
        "apply_b_dec_to_input": True,
        "finetuning_scaling_factor": False,
        "sae_lens_training_version": None,
        "prepend_bos": True,
        "dataset_path": "Skylion007/openwebtext",
        "context_size": 128,
        "normalize_activations": "none",
        "dataset_trust_remote_code": True,
    }
def connor_rob_hook_z_loader(
    release: str,
    sae_id: str,
    device: Optional[str] = None,
    force_download: bool = False,
    cfg_overrides: Optional[dict[str, Any]] = None,  # noqa: ARG001
) -> tuple[dict[str, Any], dict[str, torch.Tensor], None]:
    options = SAEConfigLoadOptions(
        device=device,
        force_download=force_download,
    )
    cfg_dict = get_sae_config(
        release,
        sae_id=sae_id,
        options=options,
    )
    repo_id, folder_name = get_repo_id_and_folder_name(release, sae_id=sae_id)
    file_path = hf_hub_download(
        repo_id=repo_id, filename=folder_name, force_download=force_download
    )
    weights = torch.load(file_path, map_location=device)
    return cfg_dict, weights, None
def read_sae_from_disk(
    cfg_dict: dict[str, Any],
    weight_path: str,
    device: str = "cpu",
    dtype: Optional[torch.dtype] = None,
) -> tuple[dict[str, Any], dict[str, torch.Tensor]]:
    """
    Given a loaded dictionary and a path to a weight file, load the weights and return the state_dict.
    """
    if dtype is None:
        dtype = DTYPE_MAP[cfg_dict["dtype"]]
    state_dict = {}
    with safe_open(weight_path, framework="pt", device=device) as f:  # type: ignore
        for k in f.keys():  # noqa: SIM118
            state_dict[k] = f.get_tensor(k).to(dtype=dtype)
    # if bool and True, then it's the April update method of normalizing activations and hasn't been folded in.
    if "scaling_factor" in state_dict:
        # we were adding it anyway for a period of time but are no longer doing so.
        # so we should delete it if
        if torch.allclose(
            state_dict["scaling_factor"],
            torch.ones_like(state_dict["scaling_factor"]),
        ):
            del state_dict["scaling_factor"]
            cfg_dict["finetuning_scaling_factor"] = False
        else:
            if not cfg_dict["finetuning_scaling_factor"]:
                raise ValueError(
                    "Scaling factor is present but finetuning_scaling_factor is False."
                )
            state_dict["finetuning_scaling_factor"] = state_dict["scaling_factor"]
            del state_dict["scaling_factor"]
    else:
        # it's there and it's not all 1's, we should use it.
        cfg_dict["finetuning_scaling_factor"] = False
    return cfg_dict, state_dict
def get_gemma_2_config(
    repo_id: str,
    folder_name: str,
    options: SAEConfigLoadOptions,
) -> Dict[str, Any]:
    # Detect width from folder_name
    width_map = {
        "width_4k": 4096,
        "width_16k": 16384,
        "width_32k": 32768,
        "width_65k": 65536,
        "width_131k": 131072,
        "width_262k": 262144,
        "width_524k": 524288,
        "width_1m": 1048576,
    }
    d_sae = next(
        (width for key, width in width_map.items() if key in folder_name), None
    )
    d_sae_override = options.d_sae_override
    if d_sae is None:
        if not d_sae_override:
            raise ValueError("Width not found in folder_name and no override provided.")
        d_sae = d_sae_override
    # Detect layer from folder_name
    match = re.search(r"layer_(\d+)", folder_name)
    layer = int(match.group(1)) if match else options.layer_override
    if layer is None:
        if "embedding" in folder_name:
            layer = 0
        else:
            raise ValueError("Layer not found in folder_name and no override provided.")
    # Model specific parameters
    model_params = {
        "2b-it": {"name": "gemma-2-2b-it", "d_in": 2304},
        "9b-it": {"name": "gemma-2-9b-it", "d_in": 3584},
        "27b-it": {"name": "gemma-2-27b-it", "d_in": 4608},
        "2b": {"name": "gemma-2-2b", "d_in": 2304},
        "9b": {"name": "gemma-2-9b", "d_in": 3584},
        "27b": {"name": "gemma-2-27b", "d_in": 4608},
    }
    model_info = next(
        (info for key, info in model_params.items() if key in repo_id), None
    )
    if not model_info:
        raise ValueError("Model name not found in repo_id.")
    model_name, d_in = model_info["name"], model_info["d_in"]
    # Hook specific parameters
    if "res" in repo_id and "embedding" not in folder_name:
        hook_name = f"blocks.{layer}.hook_resid_post"
    elif "res" in repo_id and "embedding" in folder_name:
        hook_name = "hook_embed"
    elif "mlp" in repo_id:
        hook_name = f"blocks.{layer}.hook_mlp_out"
    elif "att" in repo_id:
        hook_name = f"blocks.{layer}.attn.hook_z"
        d_in = {"2b": 2048, "9b": 4096, "27b": 4608}.get(
            next(key for key in model_params if key in repo_id), d_in
        )
    else:
        raise ValueError("Hook name not found in folder_name.")
    cfg = {
        "architecture": "jumprelu",
        "d_in": d_in,
        "d_sae": d_sae,
        "dtype": "float32",
        "model_name": model_name,
        "hook_name": hook_name,
        "hook_layer": layer,
        "hook_head_index": None,
        "activation_fn_str": "relu",
        "finetuning_scaling_factor": False,
        "sae_lens_training_version": None,
        "prepend_bos": True,
        "dataset_path": "monology/pile-uncopyrighted",
        "context_size": 1024,
        "dataset_trust_remote_code": True,
        "apply_b_dec_to_input": False,
        "normalize_activations": None,
    }
    if options.device is not None:
        cfg["device"] = options.device
    return cfg
def gemma_2_sae_loader(
    release: str,
    sae_id: str,
    device: str = "cpu",
    force_download: bool = False,
    cfg_overrides: Optional[Dict[str, Any]] = None,
    d_sae_override: Optional[int] = None,
    layer_override: Optional[int] = None,
) -> Tuple[Dict[str, Any], Dict[str, torch.Tensor], Optional[torch.Tensor]]:
    """
    Custom loader for Gemma 2 SAEs.
    """
    options = SAEConfigLoadOptions(
        device=device,
        d_sae_override=d_sae_override,
        layer_override=layer_override,
    )
    cfg_dict = get_sae_config(
        release,
        sae_id=sae_id,
        options=options,
    )
    cfg_dict["device"] = device
    # Apply overrides if provided
    if cfg_overrides is not None:
        cfg_dict.update(cfg_overrides)
    repo_id, folder_name = get_repo_id_and_folder_name(release, sae_id=sae_id)
    # Download the SAE weights
    sae_path = hf_hub_download(
        repo_id=repo_id,
        filename="params.npz",
        subfolder=folder_name,
        force_download=force_download,
    )
    # Load and convert the weights
    state_dict = {}
    with np.load(sae_path) as data:
        for key in data:
            state_dict_key = "W_" + key[2:] if key.startswith("w_") else key
            state_dict[state_dict_key] = (
                torch.tensor(data[key]).to(dtype=torch.float32).to(device)
            )
    # Handle scaling factor
    if "scaling_factor" in state_dict:
        if torch.allclose(
            state_dict["scaling_factor"], torch.ones_like(state_dict["scaling_factor"])
        ):
            del state_dict["scaling_factor"]
            cfg_dict["finetuning_scaling_factor"] = False
        else:
            if not cfg_dict["finetuning_scaling_factor"]:
                raise ValueError(
                    "Scaling factor is present but finetuning_scaling_factor is False."
                )
            state_dict["finetuning_scaling_factor"] = state_dict.pop("scaling_factor")
    else:
        cfg_dict["finetuning_scaling_factor"] = False
    # No sparsity tensor for Gemma 2 SAEs
    log_sparsity = None
    # if it is an embedding SAE, then we need to adjust for the scale of d_model because of how they trained it
    if "embedding" in folder_name:
        logger.debug("Adjusting for d_model in embedding SAE")
        state_dict["W_enc"].data = state_dict["W_enc"].data / np.sqrt(cfg_dict["d_in"])
        state_dict["W_dec"].data = state_dict["W_dec"].data * np.sqrt(cfg_dict["d_in"])
    return cfg_dict, state_dict, log_sparsity
def get_llama_scope_config(
    repo_id: str,
    folder_name: str,
    options: SAEConfigLoadOptions,  # noqa: ARG001
) -> Dict[str, Any]:
    # Llama Scope SAEs
    # repo_id: fnlp/Llama3_1-8B-Base-LX{sublayer}-{exp_factor}x
    # folder_name: Llama3_1-8B-Base-L{layer}{sublayer}-{exp_factor}x
    config_path = folder_name + "/hyperparams.json"
    config_path = hf_hub_download(repo_id, config_path)
    with open(config_path) as f:
        old_cfg_dict = json.load(f)
    # Model specific parameters
    model_name, d_in = "meta-llama/Llama-3.1-8B", old_cfg_dict["d_model"]
    return {
        "architecture": "jumprelu",
        "jump_relu_threshold": old_cfg_dict["jump_relu_threshold"],
        # We use a scalar jump_relu_threshold for all features
        # This is different from Gemma Scope JumpReLU SAEs.
        "d_in": d_in,
        "d_sae": old_cfg_dict["d_sae"],
        "dtype": "bfloat16",
        "model_name": model_name,
        "hook_name": old_cfg_dict["hook_point_in"],
        "hook_layer": int(old_cfg_dict["hook_point_in"].split(".")[1]),
        "hook_head_index": None,
        "activation_fn_str": "relu",
        "finetuning_scaling_factor": False,
        "sae_lens_training_version": None,
        "prepend_bos": True,
        "dataset_path": "cerebras/SlimPajama-627B",
        "context_size": 1024,
        "dataset_trust_remote_code": True,
        "apply_b_dec_to_input": False,
        "normalize_activations": "expected_average_only_in",
    }
def llama_scope_sae_loader(
    release: str,
    sae_id: str,
    device: str = "cpu",
    force_download: bool = False,
    cfg_overrides: Optional[Dict[str, Any]] = None,
    d_sae_override: Optional[int] = None,
    layer_override: Optional[int] = None,
) -> Tuple[Dict[str, Any], Dict[str, torch.Tensor], Optional[torch.Tensor]]:
    """
    Custom loader for Llama Scope SAEs.
    Args:
        release: Release identifier
        sae_id: SAE identifier
        device: Device to load tensors to
        force_download: Whether to force download even if files exist
        cfg_overrides: Optional configuration overrides
        d_sae_override: Optional override for SAE dimension
        layer_override: Optional override for layer number
    Returns:
        Tuple of (config dict, state dict, log sparsity tensor)
    """
    options = SAEConfigLoadOptions(
        device=device,
        d_sae_override=d_sae_override,
        layer_override=layer_override,
    )
    cfg_dict = get_sae_config(
        release,
        sae_id=sae_id,
        options=options,
    )
    cfg_dict["device"] = device
    # Apply overrides if provided
    if cfg_overrides is not None:
        cfg_dict.update(cfg_overrides)
    repo_id, folder_name = get_repo_id_and_folder_name(release, sae_id=sae_id)
    # Download the SAE weights
    sae_path = hf_hub_download(
        repo_id=repo_id,
        filename="final.safetensors",
        subfolder=folder_name + "/checkpoints",
        force_download=force_download,
    )
    # Load the weights using load_file instead of safe_open
    state_dict_loaded = load_file(sae_path, device=device)
    # Convert and organize the weights
    state_dict = {
        "W_enc": state_dict_loaded["encoder.weight"]
        .to(dtype=DTYPE_MAP[cfg_dict["dtype"]])
        .T,
        "W_dec": state_dict_loaded["decoder.weight"]
        .to(dtype=DTYPE_MAP[cfg_dict["dtype"]])
        .T,
        "b_enc": state_dict_loaded["encoder.bias"].to(
            dtype=DTYPE_MAP[cfg_dict["dtype"]]
        ),
        "b_dec": state_dict_loaded["decoder.bias"].to(
            dtype=DTYPE_MAP[cfg_dict["dtype"]]
        ),
        "threshold": torch.ones(
            cfg_dict["d_sae"],
            dtype=DTYPE_MAP[cfg_dict["dtype"]],
            device=cfg_dict["device"],
        )
        * cfg_dict["jump_relu_threshold"],
    }
    # No sparsity tensor for Llama Scope SAEs
    log_sparsity = None
    return cfg_dict, state_dict, log_sparsity
def get_llama_scope_r1_distill_config(
    repo_id: str,
    folder_name: str,
    options: SAEConfigLoadOptions,  # noqa: ARG001
) -> Dict[str, Any]:
    # Future Llama Scope series SAE by OpenMoss group use this config.
    # repo_id: [
    #   fnlp/Llama-Scope-R1-Distill
    # ]
    # folder_name: [
    #   800M-Slimpajama-0-OpenR1-Math-220k/L{layer}R,
    #   400M-Slimpajama-400M-OpenR1-Math-220k/L{layer}R,
    #   0-Slimpajama-800M-OpenR1-Math-220k/L{layer}R,
    # ]
    config_path = folder_name + "/config.json"
    config_path = hf_hub_download(repo_id, config_path)
    with open(config_path) as f:
        huggingface_cfg_dict = json.load(f)
    # Model specific parameters
    model_name, d_in = "meta-llama/Llama-3.1-8B", huggingface_cfg_dict["d_model"]
    return {
        "architecture": "jumprelu",
        "d_in": d_in,
        "d_sae": d_in * huggingface_cfg_dict["expansion_factor"],
        "dtype": "float32",
        "model_name": model_name,
        "hook_name": huggingface_cfg_dict["hook_point_in"],
        "hook_layer": int(huggingface_cfg_dict["hook_point_in"].split(".")[1]),
        "hook_head_index": None,
        "activation_fn_str": "relu",
        "finetuning_scaling_factor": False,
        "sae_lens_training_version": None,
        "prepend_bos": True,
        "dataset_path": "cerebras/SlimPajama-627B",
        "context_size": 1024,
        "dataset_trust_remote_code": True,
        "apply_b_dec_to_input": False,
        "normalize_activations": "expected_average_only_in",
    }
def llama_scope_r1_distill_sae_loader(
    release: str,
    sae_id: str,
    device: str = "cpu",
    force_download: bool = False,
    cfg_overrides: Optional[Dict[str, Any]] = None,
    d_sae_override: Optional[int] = None,
    layer_override: Optional[int] = None,
) -> Tuple[Dict[str, Any], Dict[str, torch.Tensor], Optional[torch.Tensor]]:
    """
    Custom loader for Llama Scope SAEs.
    Args:
        release: Release identifier
        sae_id: SAE identifier
        device: Device to load tensors to
        force_download: Whether to force download even if files exist
        cfg_overrides: Optional configuration overrides
        d_sae_override: Optional override for SAE dimension
        layer_override: Optional override for layer number
    Returns:
        Tuple of (config dict, state dict, log sparsity tensor)
    """
    options = SAEConfigLoadOptions(
        device=device,
        d_sae_override=d_sae_override,
        layer_override=layer_override,
    )
    cfg_dict = get_sae_config(
        release,
        sae_id=sae_id,
        options=options,
    )
    cfg_dict["device"] = device
    # Apply overrides if provided
    if cfg_overrides is not None:
        cfg_dict.update(cfg_overrides)
    repo_id, folder_name = get_repo_id_and_folder_name(release, sae_id=sae_id)
    # Download the SAE weights
    sae_path = hf_hub_download(
        repo_id=repo_id,
        filename="sae_weights.safetensors",
        subfolder=folder_name,
        force_download=force_download,
    )
    # Load the weights using load_file instead of safe_open
    state_dict_loaded = load_file(sae_path, device=device)
    # Convert and organize the weights
    state_dict = {
        "W_enc": state_dict_loaded["encoder.weight"]
        .to(dtype=DTYPE_MAP[cfg_dict["dtype"]])
        .T,
        "W_dec": state_dict_loaded["decoder.weight"]
        .to(dtype=DTYPE_MAP[cfg_dict["dtype"]])
        .T,
        "b_enc": state_dict_loaded["encoder.bias"].to(
            dtype=DTYPE_MAP[cfg_dict["dtype"]]
        ),
        "b_dec": state_dict_loaded["decoder.bias"].to(
            dtype=DTYPE_MAP[cfg_dict["dtype"]]
        ),
        "threshold": state_dict_loaded["log_jumprelu_threshold"]
        .to(dtype=DTYPE_MAP[cfg_dict["dtype"]])
        .exp(),
    }
    # No sparsity tensor for Llama Scope SAEs
    log_sparsity = None
    return cfg_dict, state_dict, log_sparsity
def get_dictionary_learning_config_1(
    repo_id: str, folder_name: str, options: SAEConfigLoadOptions
) -> dict[str, Any]:
    """
    Suitable for SAEs from https://huggingface.co/canrager/lm_sae.
    """
    config_path = hf_hub_download(
        repo_id=repo_id,
        filename=f"{folder_name}/config.json",
        force_download=options.force_download,
    )
    with open(config_path) as f:
        config = json.load(f)
    trainer = config["trainer"]
    buffer = config["buffer"]
    hook_point_name = f"blocks.{trainer['layer']}.hook_resid_post"
    activation_fn_str = "topk" if trainer["dict_class"] == "AutoEncoderTopK" else "relu"
    activation_fn_kwargs = {"k": trainer["k"]} if activation_fn_str == "topk" else {}
    return {
        "architecture": (
            "gated" if trainer["dict_class"] == "GatedAutoEncoder" else "standard"
        ),
        "d_in": trainer["activation_dim"],
        "d_sae": trainer["dict_size"],
        "dtype": "float32",
        "device": "cpu",
        "model_name": trainer["lm_name"].split("/")[-1],
        "hook_name": hook_point_name,
        "hook_layer": trainer["layer"],
        "hook_head_index": None,
        "activation_fn_str": activation_fn_str,
        "activation_fn_kwargs": activation_fn_kwargs,
        "apply_b_dec_to_input": True,
        "finetuning_scaling_factor": False,
        "sae_lens_training_version": None,
        "prepend_bos": True,
        "dataset_path": "monology/pile-uncopyrighted",
        "context_size": buffer["ctx_len"],
        "normalize_activations": "none",
        "neuronpedia_id": None,
        "dataset_trust_remote_code": True,
    }
def get_deepseek_r1_config(
    repo_id: str,  # noqa: ARG001
    folder_name: str,
    options: SAEConfigLoadOptions,
) -> dict[str, Any]:
    """Get config for DeepSeek R1 SAEs."""
    match = re.search(r"l(\d+)", folder_name)
    if match is None:
        raise ValueError(f"Could not find layer number in filename: {folder_name}")
    layer = int(match.group(1))
    return {
        "architecture": "standard",
        "d_in": 4096,  # LLaMA 8B hidden size
        "d_sae": 4096 * 16,  # Expansion factor 16
        "dtype": "bfloat16",
        "context_size": 1024,
        "model_name": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
        "hook_name": f"blocks.{layer}.hook_resid_post",
        "hook_layer": layer,
        "hook_head_index": None,
        "prepend_bos": True,
        "dataset_path": "lmsys/lmsys-chat-1m",
        "dataset_trust_remote_code": True,
        "sae_lens_training_version": None,
        "activation_fn_str": "relu",
        "normalize_activations": "none",
        "device": options.device,
        "apply_b_dec_to_input": False,
        "finetuning_scaling_factor": False,
    }
def deepseek_r1_sae_loader(
    release: str,
    sae_id: str,
    device: str = "cpu",
    force_download: bool = False,
    cfg_overrides: Optional[dict[str, Any]] = None,
) -> tuple[dict[str, Any], dict[str, torch.Tensor], Optional[torch.Tensor]]:
    """Load a DeepSeek R1 SAE."""
    repo_id, filename = get_repo_id_and_folder_name(release, sae_id=sae_id)
    # Download weights
    sae_path = hf_hub_download(
        repo_id=repo_id,
        filename=filename,
        force_download=force_download,
    )
    # Load state dict
    state_dict_loaded = torch.load(sae_path, map_location=device)
    # Create config
    options = SAEConfigLoadOptions(device=device, force_download=force_download)
    cfg_dict = get_deepseek_r1_config(repo_id, filename, options)
    # Convert weights
    state_dict = {
        "W_enc": state_dict_loaded["encoder.weight"].T,
        "W_dec": state_dict_loaded["decoder.weight"].T,
        "b_enc": state_dict_loaded["encoder.bias"],
        "b_dec": state_dict_loaded["decoder.bias"],
    }
    # Apply any config overrides
    if cfg_overrides:
        cfg_dict.update(cfg_overrides)
    return cfg_dict, state_dict, None
def get_conversion_loader_name(sae_info: Optional[PretrainedSAELookup]):
    conversion_loader_name = "sae_lens"
    if sae_info is not None and sae_info.conversion_func is not None:
        conversion_loader_name = sae_info.conversion_func
    if conversion_loader_name not in NAMED_PRETRAINED_SAE_LOADERS:
        raise ValueError(
            f"Conversion func '{conversion_loader_name}' not found in NAMED_PRETRAINED_SAE_LOADERS."
        )
    return conversion_loader_name
def get_sae_config(
    release: str, sae_id: str, options: SAEConfigLoadOptions
) -> dict[str, Any]:
    saes_directory = get_pretrained_saes_directory()
    sae_info = saes_directory.get(release, None)
    repo_id, folder_name = get_repo_id_and_folder_name(release, sae_id=sae_id)
    cfg_overrides = options.cfg_overrides or {}
    if sae_info is not None:
        # avoid modifying the original dict
        sae_info_overrides: dict[str, Any] = {**(sae_info.config_overrides or {})}
        if sae_info.neuronpedia_id is not None:
            sae_info_overrides["neuronpedia_id"] = sae_info.neuronpedia_id.get(sae_id)
        cfg_overrides = {**sae_info_overrides, **cfg_overrides}
    conversion_loader_name = get_conversion_loader_name(sae_info)
    config_getter = NAMED_PRETRAINED_SAE_CONFIG_GETTERS[conversion_loader_name]
    cfg = {
        **config_getter(repo_id, folder_name, options),
        **cfg_overrides,
    }
    return handle_config_defaulting(cfg)
def dictionary_learning_sae_loader_1(
    release: str,
    sae_id: str,
    device: str = "cpu",
    force_download: bool = False,
    cfg_overrides: Optional[dict[str, Any]] = None,
) -> tuple[dict[str, Any], dict[str, torch.Tensor], Optional[torch.Tensor]]:
    """
    Suitable for SAEs from https://huggingface.co/canrager/lm_sae.
    """
    options = SAEConfigLoadOptions(
        device=device,
        force_download=force_download,
    )
    cfg_dict = get_sae_config(release, sae_id=sae_id, options=options)
    cfg_dict["device"] = device
    if cfg_overrides:
        cfg_dict.update(cfg_overrides)
    repo_id, folder_name = get_repo_id_and_folder_name(release, sae_id=sae_id)
    encoder_path = hf_hub_download(
        repo_id=repo_id, filename=f"{folder_name}/ae.pt", force_download=force_download
    )
    encoder = torch.load(encoder_path, map_location="cpu")
    state_dict = {
        "W_enc": encoder["encoder.weight"].T,
        "W_dec": encoder["decoder.weight"].T,
        "b_dec": encoder.get(
            "b_dec", encoder.get("bias", encoder.get("decoder_bias", None))
        ),
    }
    if "encoder.bias" in encoder:
        state_dict["b_enc"] = encoder["encoder.bias"]
    if "mag_bias" in encoder:
        state_dict["b_mag"] = encoder["mag_bias"]
    if "gate_bias" in encoder:
        state_dict["b_gate"] = encoder["gate_bias"]
    if "r_mag" in encoder:
        state_dict["r_mag"] = encoder["r_mag"]
    return cfg_dict, state_dict, None
NAMED_PRETRAINED_SAE_LOADERS: dict[str, PretrainedSaeLoader] = {
    "sae_lens": sae_lens_loader,  # type: ignore
    "connor_rob_hook_z": connor_rob_hook_z_loader,  # type: ignore
    "gemma_2": gemma_2_sae_loader,
    "llama_scope": llama_scope_sae_loader,
    "llama_scope_r1_distill": llama_scope_r1_distill_sae_loader,
    "dictionary_learning_1": dictionary_learning_sae_loader_1,
    "deepseek_r1": deepseek_r1_sae_loader,
}
NAMED_PRETRAINED_SAE_CONFIG_GETTERS = {
    "sae_lens": get_sae_config_from_hf,
    "connor_rob_hook_z": get_connor_rob_hook_z_config,
    "gemma_2": get_gemma_2_config,
    "llama_scope": get_llama_scope_config,
    "llama_scope_r1_distill": get_llama_scope_r1_distill_config,
    "dictionary_learning_1": get_dictionary_learning_config_1,
    "deepseek_r1": get_deepseek_r1_config,
}

================
File: sae_lens/toolkit/pretrained_saes_directory.py
================
from dataclasses import dataclass
from functools import cache
from importlib import resources
from typing import Optional
import yaml
@dataclass
class PretrainedSAELookup:
    release: str
    repo_id: str
    model: str
    conversion_func: str | None
    saes_map: dict[str, str]  # id -> path
    expected_var_explained: dict[str, float]
    expected_l0: dict[str, float]
    neuronpedia_id: dict[str, str]
    config_overrides: dict[str, str] | dict[str, dict[str, str | bool | int]] | None
@cache
def get_pretrained_saes_directory() -> dict[str, PretrainedSAELookup]:
    package = "sae_lens"
    # Access the file within the package using importlib.resources
    directory: dict[str, PretrainedSAELookup] = {}
    with resources.open_text(package, "pretrained_saes.yaml") as file:
        # Load the YAML file content
        data = yaml.safe_load(file)
        for release, value in data.items():
            saes_map: dict[str, str] = {}
            var_explained_map: dict[str, float] = {}
            l0_map: dict[str, float] = {}
            neuronpedia_id_map: dict[str, str] = {}
            if "saes" not in value:
                raise KeyError(f"Missing 'saes' key in {release}")
            for hook_info in value["saes"]:
                saes_map[hook_info["id"]] = hook_info["path"]
                var_explained_map[hook_info["id"]] = hook_info.get(
                    "variance_explained", 1.00
                )
                l0_map[hook_info["id"]] = hook_info.get("l0", 0.00)
                neuronpedia_id_map[hook_info["id"]] = hook_info.get("neuronpedia")
            directory[release] = PretrainedSAELookup(
                release=release,
                repo_id=value["repo_id"],
                model=value["model"],
                conversion_func=value.get("conversion_func"),
                saes_map=saes_map,
                expected_var_explained=var_explained_map,
                expected_l0=l0_map,
                neuronpedia_id=neuronpedia_id_map,
                config_overrides=value.get("config_overrides"),
            )
    return directory
def get_norm_scaling_factor(release: str, sae_id: str) -> Optional[float]:
    """
    Retrieve the norm_scaling_factor for a specific SAE if it exists.
    Args:
        release (str): The release name of the SAE.
        sae_id (str): The ID of the specific SAE.
    Returns:
        Optional[float]: The norm_scaling_factor if it exists, None otherwise.
    """
    package = "sae_lens"
    with resources.open_text(package, "pretrained_saes.yaml") as file:
        data = yaml.safe_load(file)
        if release in data:
            for sae_info in data[release]["saes"]:
                if sae_info["id"] == sae_id:
                    return sae_info.get("norm_scaling_factor")
    return None
def get_repo_id_and_folder_name(release: str, sae_id: str) -> tuple[str, str]:
    saes_directory = get_pretrained_saes_directory()
    sae_info = saes_directory.get(release, None)
    if sae_info is None:
        return release, sae_id
    if sae_id not in sae_info.saes_map:
        raise ValueError(f"SAE ID '{sae_id}' not found in release '{release}'")
    repo_id = sae_info.repo_id
    folder_name = sae_info.saes_map[sae_id]
    return repo_id, folder_name

================
File: sae_lens/training/activations_store.py
================
from __future__ import annotations
import contextlib
import json
import os
import warnings
from collections.abc import Generator, Iterator, Sequence
from typing import Any, Literal, cast
import datasets
import numpy as np
import torch
from datasets import Dataset, DatasetDict, IterableDataset, load_dataset
from huggingface_hub import hf_hub_download
from huggingface_hub.utils import HfHubHTTPError
from jaxtyping import Float, Int
from requests import HTTPError
from safetensors.torch import save_file
from torch.utils.data import DataLoader
from tqdm import tqdm
from transformer_lens.hook_points import HookedRootModule
from transformers import AutoTokenizer, PreTrainedTokenizerBase
from sae_lens import logger
from sae_lens.config import (
    DTYPE_MAP,
    CacheActivationsRunnerConfig,
    HfDataset,
    LanguageModelSAERunnerConfig,
)
from sae_lens.sae import SAE
from sae_lens.tokenization_and_batching import concat_and_batch_sequences
# TODO: Make an activation store config class to be consistent with the rest of the code.
class ActivationsStore:
    """
    Class for streaming tokens and generating and storing activations
    while training SAEs.
    """
    model: HookedRootModule
    dataset: HfDataset
    cached_activations_path: str | None
    cached_activation_dataset: Dataset | None = None
    tokens_column: Literal["tokens", "input_ids", "text", "problem"]
    hook_name: str
    hook_layer: int
    hook_head_index: int | None
    _dataloader: Iterator[Any] | None = None
    _storage_buffer: torch.Tensor | None = None
    exclude_special_tokens: torch.Tensor | None = None
    device: torch.device
    @classmethod
    def from_cache_activations(
        cls,
        model: HookedRootModule,
        cfg: CacheActivationsRunnerConfig,
    ) -> ActivationsStore:
        """
        Public api to create an ActivationsStore from a cached activations dataset.
        """
        return cls(
            cached_activations_path=cfg.new_cached_activations_path,
            dtype=cfg.dtype,
            hook_name=cfg.hook_name,
            hook_layer=cfg.hook_layer,
            context_size=cfg.context_size,
            d_in=cfg.d_in,
            n_batches_in_buffer=cfg.n_batches_in_buffer,
            total_training_tokens=cfg.training_tokens,
            store_batch_size_prompts=cfg.model_batch_size,  # get_buffer
            train_batch_size_tokens=cfg.model_batch_size,  # dataloader
            seqpos_slice=(None,),
            device=torch.device(cfg.device),  # since we're sending these to SAE
            # NOOP
            prepend_bos=False,
            hook_head_index=None,
            dataset=cfg.dataset_path,
            streaming=False,
            model=model,
            normalize_activations="none",
            model_kwargs=None,
            autocast_lm=False,
            dataset_trust_remote_code=None,
            exclude_special_tokens=None,
        )
    @classmethod
    def from_config(
        cls,
        model: HookedRootModule,
        cfg: LanguageModelSAERunnerConfig | CacheActivationsRunnerConfig,
        override_dataset: HfDataset | None = None,
    ) -> ActivationsStore:
        if isinstance(cfg, CacheActivationsRunnerConfig):
            return cls.from_cache_activations(model, cfg)
        cached_activations_path = cfg.cached_activations_path
        # set cached_activations_path to None if we're not using cached activations
        if (
            isinstance(cfg, LanguageModelSAERunnerConfig)
            and not cfg.use_cached_activations
        ):
            cached_activations_path = None
        if override_dataset is None and cfg.dataset_path == "":
            raise ValueError(
                "You must either pass in a dataset or specify a dataset_path in your configutation."
            )
        device = torch.device(cfg.act_store_device)
        exclude_special_tokens = cfg.exclude_special_tokens
        if exclude_special_tokens is False:
            exclude_special_tokens = None
        if exclude_special_tokens is True:
            exclude_special_tokens = _get_special_token_ids(model.tokenizer)  # type: ignore
        if exclude_special_tokens is not None:
            exclude_special_tokens = torch.tensor(
                exclude_special_tokens, dtype=torch.long, device=device
            )
        return cls(
            model=model,
            dataset=override_dataset or cfg.dataset_path,
            streaming=cfg.streaming,
            hook_name=cfg.hook_name,
            hook_layer=cfg.hook_layer,
            hook_head_index=cfg.hook_head_index,
            context_size=cfg.context_size,
            d_in=cfg.d_in,
            n_batches_in_buffer=cfg.n_batches_in_buffer,
            total_training_tokens=cfg.training_tokens,
            store_batch_size_prompts=cfg.store_batch_size_prompts,
            train_batch_size_tokens=cfg.train_batch_size_tokens,
            prepend_bos=cfg.prepend_bos,
            normalize_activations=cfg.normalize_activations,
            device=device,
            dtype=cfg.dtype,
            cached_activations_path=cached_activations_path,
            model_kwargs=cfg.model_kwargs,
            autocast_lm=cfg.autocast_lm,
            dataset_trust_remote_code=cfg.dataset_trust_remote_code,
            seqpos_slice=cfg.seqpos_slice,
            exclude_special_tokens=exclude_special_tokens,
        )
    @classmethod
    def from_sae(
        cls,
        model: HookedRootModule,
        sae: SAE,
        context_size: int | None = None,
        dataset: HfDataset | str | None = None,
        streaming: bool = True,
        store_batch_size_prompts: int = 8,
        n_batches_in_buffer: int = 8,
        train_batch_size_tokens: int = 4096,
        total_tokens: int = 10**9,
        device: str = "cpu",
    ) -> ActivationsStore:
        return cls(
            model=model,
            dataset=sae.cfg.dataset_path if dataset is None else dataset,
            d_in=sae.cfg.d_in,
            hook_name=sae.cfg.hook_name,
            hook_layer=sae.cfg.hook_layer,
            hook_head_index=sae.cfg.hook_head_index,
            context_size=sae.cfg.context_size if context_size is None else context_size,
            prepend_bos=sae.cfg.prepend_bos,
            streaming=streaming,
            store_batch_size_prompts=store_batch_size_prompts,
            train_batch_size_tokens=train_batch_size_tokens,
            n_batches_in_buffer=n_batches_in_buffer,
            total_training_tokens=total_tokens,
            normalize_activations=sae.cfg.normalize_activations,
            dataset_trust_remote_code=sae.cfg.dataset_trust_remote_code,
            dtype=sae.cfg.dtype,
            device=torch.device(device),
            seqpos_slice=sae.cfg.seqpos_slice,
        )
    def __init__(
        self,
        model: HookedRootModule,
        dataset: HfDataset | str,
        streaming: bool,
        hook_name: str,
        hook_layer: int,
        hook_head_index: int | None,
        context_size: int,
        d_in: int,
        n_batches_in_buffer: int,
        total_training_tokens: int,
        store_batch_size_prompts: int,
        train_batch_size_tokens: int,
        prepend_bos: bool,
        normalize_activations: str,
        device: torch.device,
        dtype: str,
        cached_activations_path: str | None = None,
        model_kwargs: dict[str, Any] | None = None,
        autocast_lm: bool = False,
        dataset_trust_remote_code: bool | None = None,
        seqpos_slice: tuple[int | None, ...] = (None,),
        exclude_special_tokens: torch.Tensor | None = None,
    ):
        self.model = model
        if model_kwargs is None:
            model_kwargs = {}
        self.model_kwargs = model_kwargs
        self.dataset = (
            load_dataset(
                dataset,
                split="train",
                streaming=streaming,
                trust_remote_code=dataset_trust_remote_code,  # type: ignore
            )
            if isinstance(dataset, str)
            else dataset
        )
        if isinstance(dataset, (Dataset, DatasetDict)):
            self.dataset = cast(Dataset | DatasetDict, self.dataset)
            n_samples = len(self.dataset)
            if n_samples < total_training_tokens:
                warnings.warn(
                    f"The training dataset contains fewer samples ({n_samples}) than the number of samples required by your training configuration ({total_training_tokens}). This will result in multiple training epochs and some samples being used more than once."
                )
        self.hook_name = hook_name
        self.hook_layer = hook_layer
        self.hook_head_index = hook_head_index
        self.context_size = context_size
        self.d_in = d_in
        self.n_batches_in_buffer = n_batches_in_buffer
        self.half_buffer_size = n_batches_in_buffer // 2
        self.total_training_tokens = total_training_tokens
        self.store_batch_size_prompts = store_batch_size_prompts
        self.train_batch_size_tokens = train_batch_size_tokens
        self.prepend_bos = prepend_bos
        self.normalize_activations = normalize_activations
        self.device = torch.device(device)
        self.dtype = DTYPE_MAP[dtype]
        self.cached_activations_path = cached_activations_path
        self.autocast_lm = autocast_lm
        self.seqpos_slice = seqpos_slice
        self.exclude_special_tokens = exclude_special_tokens
        self.n_dataset_processed = 0
        self.estimated_norm_scaling_factor = None
        # Check if dataset is tokenized
        dataset_sample = next(iter(self.dataset))
        # check if it's tokenized
        if "tokens" in dataset_sample:
            self.is_dataset_tokenized = True
            self.tokens_column = "tokens"
        elif "input_ids" in dataset_sample:
            self.is_dataset_tokenized = True
            self.tokens_column = "input_ids"
        elif "text" in dataset_sample:
            self.is_dataset_tokenized = False
            self.tokens_column = "text"
        elif "problem" in dataset_sample:
            self.is_dataset_tokenized = False
            self.tokens_column = "problem"
        else:
            raise ValueError(
                "Dataset must have a 'tokens', 'input_ids', 'text', or 'problem' column."
            )
        if self.is_dataset_tokenized:
            ds_context_size = len(dataset_sample[self.tokens_column])
            if ds_context_size < self.context_size:
                raise ValueError(
                    f"""pretokenized dataset has context_size {ds_context_size}, but the provided context_size is {self.context_size}.
                    The context_size {ds_context_size} is expected to be larger than or equal to the provided context size {self.context_size}."""
                )
            if self.context_size != ds_context_size:
                warnings.warn(
                    f"""pretokenized dataset has context_size {ds_context_size}, but the provided context_size is {self.context_size}. Some data will be discarded in this case.""",
                    RuntimeWarning,
                )
            # TODO: investigate if this can work for iterable datasets, or if this is even worthwhile as a perf improvement
            if hasattr(self.dataset, "set_format"):
                self.dataset.set_format(type="torch", columns=[self.tokens_column])  # type: ignore
            if (
                isinstance(dataset, str)
                and hasattr(model, "tokenizer")
                and model.tokenizer is not None
            ):
                validate_pretokenized_dataset_tokenizer(
                    dataset_path=dataset,
                    model_tokenizer=model.tokenizer,  # type: ignore
                )
        else:
            warnings.warn(
                "Dataset is not tokenized. Pre-tokenizing will improve performance and allows for more control over special tokens. See https://jbloomaus.github.io/SAELens/training_saes/#pretokenizing-datasets for more info."
            )
        self.iterable_sequences = self._iterate_tokenized_sequences()
        self.cached_activation_dataset = self.load_cached_activation_dataset()
        # TODO add support for "mixed loading" (ie use cache until you run out, then switch over to streaming from HF)
    def _iterate_raw_dataset(
        self,
    ) -> Generator[torch.Tensor | list[int] | str, None, None]:
        """
        Helper to iterate over the dataset while incrementing n_dataset_processed
        """
        for row in self.dataset:
            # typing datasets is difficult
            yield row[self.tokens_column]  # type: ignore
            self.n_dataset_processed += 1
    def _iterate_raw_dataset_tokens(self) -> Generator[torch.Tensor, None, None]:
        """
        Helper to create an iterator which tokenizes raw text from the dataset on the fly
        """
        for row in self._iterate_raw_dataset():
            tokens = (
                self.model.to_tokens(
                    row,
                    truncate=False,
                    move_to_device=False,  # we move to device below
                    prepend_bos=False,
                )  # type: ignore
                .squeeze(0)
                .to(self.device)
            )
            if len(tokens.shape) != 1:
                raise ValueError(f"tokens.shape should be 1D but was {tokens.shape}")
            yield tokens
    def _iterate_tokenized_sequences(self) -> Generator[torch.Tensor, None, None]:
        """
        Generator which iterates over full sequence of context_size tokens
        """
        # If the datset is pretokenized, we will slice the dataset to the length of the context window if needed. Otherwise, no further processing is needed.
        # We assume that all necessary BOS/EOS/SEP tokens have been added during pretokenization.
        if self.is_dataset_tokenized:
            for row in self._iterate_raw_dataset():
                yield torch.tensor(
                    row[
                        : self.context_size
                    ],  # If self.context_size = None, this line simply returns the whole row
                    dtype=torch.long,
                    device=self.device,
                    requires_grad=False,
                )
        # If the dataset isn't tokenized, we'll tokenize, concat, and batch on the fly
        else:
            tokenizer = getattr(self.model, "tokenizer", None)
            bos_token_id = None if tokenizer is None else tokenizer.bos_token_id
            yield from concat_and_batch_sequences(
                tokens_iterator=self._iterate_raw_dataset_tokens(),
                context_size=self.context_size,
                begin_batch_token_id=(bos_token_id if self.prepend_bos else None),
                begin_sequence_token_id=None,
                sequence_separator_token_id=(
                    bos_token_id if self.prepend_bos else None
                ),
            )
    def load_cached_activation_dataset(self) -> Dataset | None:
        """
        Load the cached activation dataset from disk.
        - If cached_activations_path is set, returns Huggingface Dataset else None
        - Checks that the loaded dataset has current has activations for hooks in config and that shapes match.
        """
        if self.cached_activations_path is None:
            return None
        assert self.cached_activations_path is not None  # keep pyright happy
        # Sanity check: does the cache directory exist?
        if not os.path.exists(self.cached_activations_path):
            raise FileNotFoundError(
                f"Cache directory {self.cached_activations_path} does not exist. "
                "Consider double-checking your dataset, model, and hook names."
            )
        # ---
        # Actual code
        activations_dataset = datasets.load_from_disk(self.cached_activations_path)
        columns = [self.hook_name]
        if "token_ids" in activations_dataset.column_names:
            columns.append("token_ids")
        activations_dataset.set_format(
            type="torch", columns=columns, device=self.device, dtype=self.dtype
        )
        self.current_row_idx = 0  # idx to load next batch from
        # ---
        assert isinstance(activations_dataset, Dataset)
        # multiple in hooks future
        if not set([self.hook_name]).issubset(activations_dataset.column_names):
            raise ValueError(
                f"loaded dataset does not include hook activations, got {activations_dataset.column_names}"
            )
        if activations_dataset.features[self.hook_name].shape != (
            self.context_size,
            self.d_in,
        ):
            raise ValueError(
                f"Given dataset of shape {activations_dataset.features[self.hook_name].shape} does not match context_size ({self.context_size}) and d_in ({self.d_in})"
            )
        return activations_dataset
    def set_norm_scaling_factor_if_needed(self):
        if self.normalize_activations == "expected_average_only_in":
            self.estimated_norm_scaling_factor = self.estimate_norm_scaling_factor()
    def apply_norm_scaling_factor(self, activations: torch.Tensor) -> torch.Tensor:
        if self.estimated_norm_scaling_factor is None:
            raise ValueError(
                "estimated_norm_scaling_factor is not set, call set_norm_scaling_factor_if_needed() first"
            )
        return activations * self.estimated_norm_scaling_factor
    def unscale(self, activations: torch.Tensor) -> torch.Tensor:
        if self.estimated_norm_scaling_factor is None:
            raise ValueError(
                "estimated_norm_scaling_factor is not set, call set_norm_scaling_factor_if_needed() first"
            )
        return activations / self.estimated_norm_scaling_factor
    def get_norm_scaling_factor(self, activations: torch.Tensor) -> torch.Tensor:
        return (self.d_in**0.5) / activations.norm(dim=-1).mean()
    @torch.no_grad()
    def estimate_norm_scaling_factor(self, n_batches_for_norm_estimate: int = int(1e3)):
        norms_per_batch = []
        for _ in tqdm(
            range(n_batches_for_norm_estimate), desc="Estimating norm scaling factor"
        ):
            # temporalily set estimated_norm_scaling_factor to 1.0 so the dataloader works
            self.estimated_norm_scaling_factor = 1.0
            acts = self.next_batch()[0]
            self.estimated_norm_scaling_factor = None
            norms_per_batch.append(acts.norm(dim=-1).mean().item())
        mean_norm = np.mean(norms_per_batch)
        return np.sqrt(self.d_in) / mean_norm
    def shuffle_input_dataset(self, seed: int, buffer_size: int = 1):
        """
        This applies a shuffle to the huggingface dataset that is the input to the activations store. This
        also shuffles the shards of the dataset, which is especially useful for evaluating on different
        sections of very large streaming datasets. Buffer size is only relevant for streaming datasets.
        The default buffer_size of 1 means that only the shard will be shuffled; larger buffer sizes will
        additionally shuffle individual elements within the shard.
        """
        if isinstance(self.dataset, IterableDataset):
            self.dataset = self.dataset.shuffle(seed=seed, buffer_size=buffer_size)
        else:
            self.dataset = self.dataset.shuffle(seed=seed)
        self.iterable_dataset = iter(self.dataset)
    def reset_input_dataset(self):
        """
        Resets the input dataset iterator to the beginning.
        """
        self.iterable_dataset = iter(self.dataset)
    @property
    def storage_buffer(self) -> torch.Tensor:
        if self._storage_buffer is None:
            self._storage_buffer = _filter_buffer_acts(
                self.get_buffer(self.half_buffer_size), self.exclude_special_tokens
            )
        return self._storage_buffer
    @property
    def dataloader(self) -> Iterator[Any]:
        if self._dataloader is None:
            self._dataloader = self.get_data_loader()
        return self._dataloader
    def get_batch_tokens(
        self, batch_size: int | None = None, raise_at_epoch_end: bool = False
    ):
        """
        Streams a batch of tokens from a dataset.
        If raise_at_epoch_end is true we will reset the dataset at the end of each epoch and raise a StopIteration. Otherwise we will reset silently.
        """
        if not batch_size:
            batch_size = self.store_batch_size_prompts
        sequences = []
        # the sequences iterator yields fully formed tokens of size context_size, so we just need to cat these into a batch
        for _ in range(batch_size):
            try:
                sequences.append(next(self.iterable_sequences))
            except StopIteration:
                self.iterable_sequences = self._iterate_tokenized_sequences()
                if raise_at_epoch_end:
                    raise StopIteration(
                        f"Ran out of tokens in dataset after {self.n_dataset_processed} samples, beginning the next epoch."
                    )
                sequences.append(next(self.iterable_sequences))
        return torch.stack(sequences, dim=0).to(_get_model_device(self.model))
    @torch.no_grad()
    def get_activations(self, batch_tokens: torch.Tensor):
        """
        Returns activations of shape (batches, context, num_layers, d_in)
        d_in may result from a concatenated head dimension.
        """
        # Setup autocast if using
        if self.autocast_lm:
            autocast_if_enabled = torch.autocast(
                device_type="cuda",
                dtype=torch.bfloat16,
                enabled=self.autocast_lm,
            )
        else:
            autocast_if_enabled = contextlib.nullcontext()
        with autocast_if_enabled:
            layerwise_activations_cache = self.model.run_with_cache(
                batch_tokens,
                names_filter=[self.hook_name],
                stop_at_layer=self.hook_layer + 1,
                prepend_bos=False,
                **self.model_kwargs,
            )[1]
        layerwise_activations = layerwise_activations_cache[self.hook_name][
            :, slice(*self.seqpos_slice)
        ]
        n_batches, n_context = layerwise_activations.shape[:2]
        stacked_activations = torch.zeros((n_batches, n_context, 1, self.d_in))
        if self.hook_head_index is not None:
            stacked_activations[:, :, 0] = layerwise_activations[
                :, :, self.hook_head_index
            ]
        elif layerwise_activations.ndim > 3:  # if we have a head dimension
            try:
                stacked_activations[:, :, 0] = layerwise_activations.view(
                    n_batches, n_context, -1
                )
            except RuntimeError as e:
                logger.error(f"Error during view operation: {e}")
                logger.info("Attempting to use reshape instead...")
                stacked_activations[:, :, 0] = layerwise_activations.reshape(
                    n_batches, n_context, -1
                )
        else:
            stacked_activations[:, :, 0] = layerwise_activations
        return stacked_activations
    def _load_buffer_from_cached(
        self,
        total_size: int,
        context_size: int,
        num_layers: int,
        d_in: int,
        raise_on_epoch_end: bool,
    ) -> tuple[
        Float[torch.Tensor, "(total_size context_size) num_layers d_in"],
        Int[torch.Tensor, "(total_size context_size)"] | None,
    ]:
        """
        Loads `total_size` activations from `cached_activation_dataset`
        The dataset has columns for each hook_name,
        each containing activations of shape (context_size, d_in).
        raises StopIteration
        """
        assert self.cached_activation_dataset is not None
        # In future, could be a list of multiple hook names
        hook_names = [self.hook_name]
        if not set(hook_names).issubset(self.cached_activation_dataset.column_names):
            raise ValueError(
                f"Missing columns in dataset. Expected {hook_names}, "
                f"got {self.cached_activation_dataset.column_names}."
            )
        if self.current_row_idx > len(self.cached_activation_dataset) - total_size:
            self.current_row_idx = 0
            if raise_on_epoch_end:
                raise StopIteration
        new_buffer = []
        ds_slice = self.cached_activation_dataset[
            self.current_row_idx : self.current_row_idx + total_size
        ]
        for hook_name in hook_names:
            # Load activations for each hook.
            # Usually faster to first slice dataset then pick column
            _hook_buffer = ds_slice[hook_name]
            if _hook_buffer.shape != (total_size, context_size, d_in):
                raise ValueError(
                    f"_hook_buffer has shape {_hook_buffer.shape}, "
                    f"but expected ({total_size}, {context_size}, {d_in})."
                )
            new_buffer.append(_hook_buffer)
        # Stack across num_layers dimension
        # list of num_layers; shape: (total_size, context_size, d_in) -> (total_size, context_size, num_layers, d_in)
        new_buffer = torch.stack(new_buffer, dim=2)
        if new_buffer.shape != (total_size, context_size, num_layers, d_in):
            raise ValueError(
                f"new_buffer has shape {new_buffer.shape}, "
                f"but expected ({total_size}, {context_size}, {num_layers}, {d_in})."
            )
        self.current_row_idx += total_size
        acts_buffer = new_buffer.reshape(total_size * context_size, num_layers, d_in)
        if "token_ids" not in self.cached_activation_dataset.column_names:
            return acts_buffer, None
        token_ids_buffer = ds_slice["token_ids"]
        if token_ids_buffer.shape != (total_size, context_size):
            raise ValueError(
                f"token_ids_buffer has shape {token_ids_buffer.shape}, "
                f"but expected ({total_size}, {context_size})."
            )
        token_ids_buffer = token_ids_buffer.reshape(total_size * context_size)
        return acts_buffer, token_ids_buffer
    @torch.no_grad()
    def get_buffer(
        self,
        n_batches_in_buffer: int,
        raise_on_epoch_end: bool = False,
        shuffle: bool = True,
    ) -> tuple[torch.Tensor, torch.Tensor | None]:
        """
        Loads the next n_batches_in_buffer batches of activations into a tensor and returns it.
        The primary purpose here is maintaining a shuffling buffer.
        If raise_on_epoch_end is True, when the dataset it exhausted it will automatically refill the dataset and then raise a StopIteration so that the caller has a chance to react.
        """
        context_size = self.context_size
        training_context_size = len(range(context_size)[slice(*self.seqpos_slice)])
        batch_size = self.store_batch_size_prompts
        d_in = self.d_in
        total_size = batch_size * n_batches_in_buffer
        num_layers = 1
        if self.cached_activation_dataset is not None:
            return self._load_buffer_from_cached(
                total_size, context_size, num_layers, d_in, raise_on_epoch_end
            )
        refill_iterator = range(0, total_size, batch_size)
        # Initialize empty tensor buffer of the maximum required size with an additional dimension for layers
        new_buffer_activations = torch.zeros(
            (total_size, training_context_size, num_layers, d_in),
            dtype=self.dtype,  # type: ignore
            device=self.device,
        )
        new_buffer_token_ids = torch.zeros(
            (total_size, training_context_size),
            dtype=torch.long,
            device=self.device,
        )
        for refill_batch_idx_start in tqdm(
            refill_iterator, leave=False, desc="Refilling buffer"
        ):
            # move batch toks to gpu for model
            refill_batch_tokens = self.get_batch_tokens(
                raise_at_epoch_end=raise_on_epoch_end
            ).to(_get_model_device(self.model))
            refill_activations = self.get_activations(refill_batch_tokens)
            # move acts back to cpu
            refill_activations.to(self.device)
            new_buffer_activations[
                refill_batch_idx_start : refill_batch_idx_start + batch_size, ...
            ] = refill_activations
            # handle seqpos_slice, this is done for activations in get_activations
            refill_batch_tokens = refill_batch_tokens[:, slice(*self.seqpos_slice)]
            new_buffer_token_ids[
                refill_batch_idx_start : refill_batch_idx_start + batch_size, ...
            ] = refill_batch_tokens
        new_buffer_activations = new_buffer_activations.reshape(-1, num_layers, d_in)
        new_buffer_token_ids = new_buffer_token_ids.reshape(-1)
        if shuffle:
            new_buffer_activations, new_buffer_token_ids = permute_together(
                [new_buffer_activations, new_buffer_token_ids]
            )
        # every buffer should be normalized:
        if self.normalize_activations == "expected_average_only_in":
            new_buffer_activations = self.apply_norm_scaling_factor(
                new_buffer_activations
            )
        return (
            new_buffer_activations,
            new_buffer_token_ids,
        )
    def get_data_loader(
        self,
    ) -> Iterator[Any]:
        """
        Return a torch.utils.dataloader which you can get batches from.
        Should automatically refill the buffer when it gets to n % full.
        (better mixing if you refill and shuffle regularly).
        """
        batch_size = self.train_batch_size_tokens
        try:
            new_samples = _filter_buffer_acts(
                self.get_buffer(self.half_buffer_size, raise_on_epoch_end=True),
                self.exclude_special_tokens,
            )
        except StopIteration:
            warnings.warn(
                "All samples in the training dataset have been exhausted, we are now beginning a new epoch with the same samples."
            )
            self._storage_buffer = (
                None  # dump the current buffer so samples do not leak between epochs
            )
            try:
                new_samples = _filter_buffer_acts(
                    self.get_buffer(self.half_buffer_size),
                    self.exclude_special_tokens,
                )
            except StopIteration:
                raise ValueError(
                    "We were unable to fill up the buffer directly after starting a new epoch. This could indicate that there are less samples in the dataset than are required to fill up the buffer. Consider reducing batch_size or n_batches_in_buffer. "
                )
        # 1. # create new buffer by mixing stored and new buffer
        mixing_buffer = torch.cat(
            [new_samples, self.storage_buffer],
            dim=0,
        )
        mixing_buffer = mixing_buffer[torch.randperm(mixing_buffer.shape[0])]
        # 2.  put 50 % in storage
        self._storage_buffer = mixing_buffer[: mixing_buffer.shape[0] // 2]
        # 3. put other 50 % in a dataloader
        return iter(
            DataLoader(
                # TODO: seems like a typing bug?
                cast(Any, mixing_buffer[mixing_buffer.shape[0] // 2 :]),
                batch_size=batch_size,
                shuffle=True,
            )
        )
    def next_batch(self) -> torch.Tensor:
        """
        Get the next batch from the current DataLoader.
        If the DataLoader is exhausted, refill the buffer and create a new DataLoader.
        """
        try:
            # Try to get the next batch
            return next(self.dataloader)
        except StopIteration:
            # If the DataLoader is exhausted, create a new one
            self._dataloader = self.get_data_loader()
            return next(self.dataloader)
    def state_dict(self) -> dict[str, torch.Tensor]:
        result = {
            "n_dataset_processed": torch.tensor(self.n_dataset_processed),
        }
        if self._storage_buffer is not None:  # first time might be None
            result["storage_buffer_activations"] = self._storage_buffer[0]
            if self._storage_buffer[1] is not None:
                result["storage_buffer_tokens"] = self._storage_buffer[1]
        if self.estimated_norm_scaling_factor is not None:
            result["estimated_norm_scaling_factor"] = torch.tensor(
                self.estimated_norm_scaling_factor
            )
        return result
    def save(self, file_path: str):
        """save the state dict to a file in safetensors format"""
        save_file(self.state_dict(), file_path)
def validate_pretokenized_dataset_tokenizer(
    dataset_path: str, model_tokenizer: PreTrainedTokenizerBase
) -> None:
    """
    Helper to validate that the tokenizer used to pretokenize the dataset matches the model tokenizer.
    """
    try:
        tokenization_cfg_path = hf_hub_download(
            dataset_path, "sae_lens.json", repo_type="dataset"
        )
    except HfHubHTTPError:
        return
    if tokenization_cfg_path is None:
        return
    with open(tokenization_cfg_path) as f:
        tokenization_cfg = json.load(f)
    tokenizer_name = tokenization_cfg["tokenizer_name"]
    try:
        ds_tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
    # if we can't download the specified tokenizer to verify, just continue
    except HTTPError:
        return
    if ds_tokenizer.get_vocab() != model_tokenizer.get_vocab():
        raise ValueError(
            f"Dataset tokenizer {tokenizer_name} does not match model tokenizer {model_tokenizer}."
        )
def _get_model_device(model: HookedRootModule) -> torch.device:
    if hasattr(model, "W_E"):
        return model.W_E.device  # type: ignore
    if hasattr(model, "cfg") and hasattr(model.cfg, "device"):
        return model.cfg.device  # type: ignore
    return next(model.parameters()).device  # type: ignore
def _get_special_token_ids(tokenizer: PreTrainedTokenizerBase) -> list[int]:
    """Get all special token IDs from a tokenizer."""
    special_tokens = set()
    # Get special tokens from tokenizer attributes
    for attr in dir(tokenizer):
        if attr.endswith("_token_id"):
            token_id = getattr(tokenizer, attr)
            if token_id is not None:
                special_tokens.add(token_id)
    # Get any additional special tokens from the tokenizer's special tokens map
    if hasattr(tokenizer, "special_tokens_map"):
        for token in tokenizer.special_tokens_map.values():
            if isinstance(token, str):
                token_id = tokenizer.convert_tokens_to_ids(token)  # type: ignore
                special_tokens.add(token_id)
            elif isinstance(token, list):
                for t in token:
                    token_id = tokenizer.convert_tokens_to_ids(t)  # type: ignore
                    special_tokens.add(token_id)
    return list(special_tokens)
def _filter_buffer_acts(
    buffer: tuple[torch.Tensor, torch.Tensor | None],
    exclude_tokens: torch.Tensor | None,
) -> torch.Tensor:
    """
    Filter out activations for tokens that are in exclude_tokens.
    """
    activations, tokens = buffer
    if tokens is None or exclude_tokens is None:
        return activations
    mask = torch.isin(tokens, exclude_tokens)
    return activations[~mask]
def permute_together(tensors: Sequence[torch.Tensor]) -> tuple[torch.Tensor, ...]:
    """Permute tensors together."""
    permutation = torch.randperm(tensors[0].shape[0])
    return tuple(t[permutation] for t in tensors)

================
File: sae_lens/training/geometric_median.py
================
from types import SimpleNamespace
from typing import Optional
import torch
import tqdm
def weighted_average(points: torch.Tensor, weights: torch.Tensor):
    weights = weights / weights.sum()
    return (points * weights.view(-1, 1)).sum(dim=0)
@torch.no_grad()
def geometric_median_objective(
    median: torch.Tensor, points: torch.Tensor, weights: torch.Tensor
) -> torch.Tensor:
    norms = torch.linalg.norm(points - median.view(1, -1), dim=1)  # type: ignore
    return (norms * weights).sum()
def compute_geometric_median(
    points: torch.Tensor,
    weights: Optional[torch.Tensor] = None,
    eps: float = 1e-6,
    maxiter: int = 100,
    ftol: float = 1e-20,
    do_log: bool = False,
):
    """
    :param points: ``torch.Tensor`` of shape ``(n, d)``
    :param weights: Optional ``torch.Tensor`` of shape :math:``(n,)``.
    :param eps: Smallest allowed value of denominator, to avoid divide by zero.
        Equivalently, this is a smoothing parameter. Default 1e-6.
    :param maxiter: Maximum number of Weiszfeld iterations. Default 100
    :param ftol: If objective value does not improve by at least this `ftol` fraction, terminate the algorithm. Default 1e-20.
    :param do_log: If true will return a log of function values encountered through the course of the algorithm
    :return: SimpleNamespace object with fields
        - `median`: estimate of the geometric median, which is a ``torch.Tensor`` object of shape :math:``(d,)``
        - `termination`: string explaining how the algorithm terminated.
        - `logs`: function values encountered through the course of the algorithm in a list (None if do_log is false).
    """
    with torch.no_grad():
        if weights is None:
            weights = torch.ones((points.shape[0],), device=points.device)
        # initialize median estimate at mean
        new_weights = weights
        median = weighted_average(points, weights)
        objective_value = geometric_median_objective(median, points, weights)
        logs = [objective_value] if do_log else None
        # Weiszfeld iterations
        early_termination = False
        pbar = tqdm.tqdm(range(maxiter))
        for _ in pbar:
            prev_obj_value = objective_value
            norms = torch.linalg.norm(points - median.view(1, -1), dim=1)  # type: ignore
            new_weights = weights / torch.clamp(norms, min=eps)
            median = weighted_average(points, new_weights)
            objective_value = geometric_median_objective(median, points, weights)
            if logs is not None:
                logs.append(objective_value)
            if abs(prev_obj_value - objective_value) <= ftol * objective_value:
                early_termination = True
                break
            pbar.set_description(f"Objective value: {objective_value:.4f}")
    median = weighted_average(points, new_weights)  # allow autodiff to track it
    return SimpleNamespace(
        median=median,
        new_weights=new_weights,
        termination=(
            "function value converged within tolerance"
            if early_termination
            else "maximum iterations reached"
        ),
        logs=logs,
    )
if __name__ == "__main__":
    import time
    TOLERANCE = 1e-2
    dim1 = 10000
    dim2 = 768
    device = "cuda" if torch.cuda.is_available() else "cpu"
    sample = (
        torch.randn((dim1, dim2), device=device) * 100
    )  # seems to be the order of magnitude of the actual use case
    weights = torch.randn((dim1,), device=device)
    torch.tensor(weights, device=device)
    tic = time.perf_counter()
    new = compute_geometric_median(sample, weights=weights, maxiter=100)
    print(f"new code takes {time.perf_counter()-tic} seconds!")  # noqa: T201

================
File: sae_lens/training/optim.py
================
"""
Took the LR scheduler from my previous work: https://github.com/jbloomAus/DecisionTransformerInterpretability/blob/ee55df35cdb92e81d689c72fb9dd5a7252893363/src/decision_transformer/utils.py#L425
"""
from typing import Any
import torch.optim as optim
import torch.optim.lr_scheduler as lr_scheduler
#  Constant
#  Cosine Annealing with Warmup
#  Cosine Annealing with Warmup / Restarts
#  No default values specified so the type-checker can verify we don't forget any arguments.
def get_lr_scheduler(
    scheduler_name: str,
    optimizer: optim.Optimizer,
    training_steps: int,
    lr: float,
    warm_up_steps: int,
    decay_steps: int,
    lr_end: float,
    num_cycles: int,
) -> lr_scheduler.LRScheduler:
    """
    Loosely based on this, seemed simpler write this than import
    transformers: https://huggingface.co/docs/transformers/main_classes/optimizer_schedules
    Args:
        scheduler_name (str): Name of the scheduler to use, one of "constant", "cosineannealing", "cosineannealingwarmrestarts"
        optimizer (optim.Optimizer): Optimizer to use
        training_steps (int): Total number of training steps
        warm_up_steps (int, optional): Number of linear warm up steps. Defaults to 0.
        decay_steps (int, optional): Number of linear decay steps to 0. Defaults to 0.
        num_cycles (int, optional): Number of cycles for cosine annealing with warm restarts. Defaults to 1.
        lr_end (float, optional): Final learning rate multiplier before decay. Defaults to 0.0.
    """
    base_scheduler_steps = training_steps - warm_up_steps - decay_steps
    norm_scheduler_name = scheduler_name.lower()
    main_scheduler = _get_main_lr_scheduler(
        norm_scheduler_name,
        optimizer,
        steps=base_scheduler_steps,
        lr_end=lr_end,
        num_cycles=num_cycles,
    )
    if norm_scheduler_name == "constant":
        # constant scheduler ignores lr_end, so decay needs to start at lr
        lr_end = lr
    schedulers: list[lr_scheduler.LRScheduler] = []
    milestones: list[int] = []
    if warm_up_steps > 0:
        schedulers.append(
            lr_scheduler.LinearLR(
                optimizer,
                start_factor=1 / warm_up_steps,
                end_factor=1.0,
                total_iters=warm_up_steps - 1,
            ),
        )
        milestones.append(warm_up_steps)
    schedulers.append(main_scheduler)
    if decay_steps > 0:
        if lr_end == 0.0:
            raise ValueError(
                "Cannot have decay_steps with lr_end=0.0, this would decay from 0 to 0 and be a waste."
            )
        schedulers.append(
            lr_scheduler.LinearLR(
                optimizer,
                start_factor=lr_end / lr,
                end_factor=0.0,
                total_iters=decay_steps,
            ),
        )
        milestones.append(training_steps - decay_steps)
    return lr_scheduler.SequentialLR(
        schedulers=schedulers,
        optimizer=optimizer,
        milestones=milestones,
    )
def _get_main_lr_scheduler(
    scheduler_name: str,
    optimizer: optim.Optimizer,
    steps: int,
    lr_end: float,
    num_cycles: int,
) -> lr_scheduler.LRScheduler:
    if scheduler_name == "constant":
        return lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda steps: 1.0)  # noqa: ARG005
    if scheduler_name == "cosineannealing":
        return lr_scheduler.CosineAnnealingLR(optimizer, T_max=steps, eta_min=lr_end)  # type: ignore
    if scheduler_name == "cosineannealingwarmrestarts":
        return lr_scheduler.CosineAnnealingWarmRestarts(
            optimizer,
            T_0=steps // num_cycles,
            eta_min=lr_end,  # type: ignore
        )
    raise ValueError(f"Unsupported scheduler: {scheduler_name}")
class L1Scheduler:
    def __init__(
        self,
        l1_warm_up_steps: float,
        total_steps: int,
        final_l1_coefficient: float,
    ):
        self.l1_warmup_steps = l1_warm_up_steps
        # assume using warm-up
        if self.l1_warmup_steps != 0:
            self.current_l1_coefficient = 0.0
        else:
            self.current_l1_coefficient = final_l1_coefficient
        self.final_l1_coefficient = final_l1_coefficient
        self.current_step = 0
        self.total_steps = total_steps
        if not isinstance(self.final_l1_coefficient, (float, int)):
            raise TypeError(
                f"final_l1_coefficient must be float or int, got {type(self.final_l1_coefficient)}."
            )
    def __repr__(self) -> str:
        return (
            f"L1Scheduler(final_l1_value={self.final_l1_coefficient}, "
            f"l1_warmup_steps={self.l1_warmup_steps}, "
            f"total_steps={self.total_steps})"
        )
    def step(self):
        """
        Updates the l1 coefficient of the sparse autoencoder.
        """
        step = self.current_step
        if step < self.l1_warmup_steps:
            self.current_l1_coefficient = self.final_l1_coefficient * (
                (1 + step) / self.l1_warmup_steps
            )  # type: ignore
        else:
            self.current_l1_coefficient = self.final_l1_coefficient  # type: ignore
        self.current_step += 1
    def state_dict(self):
        """State dict for serializing as part of an SAETrainContext."""
        return {
            "l1_warmup_steps": self.l1_warmup_steps,
            "total_steps": self.total_steps,
            "current_l1_coefficient": self.current_l1_coefficient,
            "final_l1_coefficient": self.final_l1_coefficient,
            "current_step": self.current_step,
        }
    def load_state_dict(self, state_dict: dict[str, Any]):
        """Loads all state apart from attached SAE."""
        for k in state_dict:
            setattr(self, k, state_dict[k])

================
File: sae_lens/training/sae_trainer.py
================
import contextlib
from dataclasses import dataclass
from typing import Any, Optional, Protocol, cast
import torch
import wandb
from torch.optim import Adam
from tqdm import tqdm
from transformer_lens.hook_points import HookedRootModule
from sae_lens import __version__
from sae_lens.config import LanguageModelSAERunnerConfig
from sae_lens.evals import EvalConfig, run_evals
from sae_lens.training.activations_store import ActivationsStore
from sae_lens.training.optim import L1Scheduler, get_lr_scheduler
from sae_lens.training.training_sae import TrainingSAE, TrainStepOutput
# used to map between parameters which are updated during finetuning and the config str.
FINETUNING_PARAMETERS = {
    "scale": ["scaling_factor"],
    "decoder": ["scaling_factor", "W_dec", "b_dec"],
    "unrotated_decoder": ["scaling_factor", "b_dec"],
}
def _log_feature_sparsity(
    feature_sparsity: torch.Tensor, eps: float = 1e-10
) -> torch.Tensor:
    return torch.log10(feature_sparsity + eps).detach().cpu()
def _update_sae_lens_training_version(sae: TrainingSAE) -> None:
    """
    Make sure we record the version of SAELens used for the training run
    """
    sae.cfg.sae_lens_training_version = str(__version__)
@dataclass
class TrainSAEOutput:
    sae: TrainingSAE
    checkpoint_path: str
    log_feature_sparsities: torch.Tensor
class SaveCheckpointFn(Protocol):
    def __call__(
        self,
        trainer: "SAETrainer",
        checkpoint_name: str,
        wandb_aliases: Optional[list[str]] = None,
    ) -> None: ...
class SAETrainer:
    """
    Core SAE class used for inference. For training, see TrainingSAE.
    """
    def __init__(
        self,
        model: HookedRootModule,
        sae: TrainingSAE,
        activation_store: ActivationsStore,
        save_checkpoint_fn: SaveCheckpointFn,
        cfg: LanguageModelSAERunnerConfig,
    ) -> None:
        self.model = model
        self.sae = sae
        self.activations_store = activation_store
        self.save_checkpoint = save_checkpoint_fn
        self.cfg = cfg
        self.n_training_steps: int = 0
        self.n_training_tokens: int = 0
        self.started_fine_tuning: bool = False
        _update_sae_lens_training_version(self.sae)
        self.checkpoint_thresholds = []
        if self.cfg.n_checkpoints > 0:
            self.checkpoint_thresholds = list(
                range(
                    0,
                    cfg.total_training_tokens,
                    cfg.total_training_tokens // self.cfg.n_checkpoints,
                )
            )[1:]
        self.act_freq_scores = torch.zeros(
            cast(int, cfg.d_sae),
            device=cfg.device,
        )
        self.n_forward_passes_since_fired = torch.zeros(
            cast(int, cfg.d_sae),
            device=cfg.device,
        )
        self.n_frac_active_tokens = 0
        # we don't train the scaling factor (initially)
        # set requires grad to false for the scaling factor
        for name, param in self.sae.named_parameters():
            if "scaling_factor" in name:
                param.requires_grad = False
        self.optimizer = Adam(
            sae.parameters(),
            lr=cfg.lr,
            betas=(
                cfg.adam_beta1,
                cfg.adam_beta2,
            ),
        )
        assert cfg.lr_end is not None  # this is set in config post-init
        self.lr_scheduler = get_lr_scheduler(
            cfg.lr_scheduler_name,
            lr=cfg.lr,
            optimizer=self.optimizer,
            warm_up_steps=cfg.lr_warm_up_steps,
            decay_steps=cfg.lr_decay_steps,
            training_steps=self.cfg.total_training_steps,
            lr_end=cfg.lr_end,
            num_cycles=cfg.n_restart_cycles,
        )
        self.l1_scheduler = L1Scheduler(
            l1_warm_up_steps=cfg.l1_warm_up_steps,
            total_steps=cfg.total_training_steps,
            final_l1_coefficient=cfg.l1_coefficient,
        )
        # Setup autocast if using
        self.scaler = torch.amp.GradScaler(
            device=self.cfg.device, enabled=self.cfg.autocast
        )
        if self.cfg.autocast:
            self.autocast_if_enabled = torch.autocast(
                device_type=self.cfg.device,
                dtype=torch.bfloat16,
                enabled=self.cfg.autocast,
            )
        else:
            self.autocast_if_enabled = contextlib.nullcontext()
        # Set up eval config
        self.trainer_eval_config = EvalConfig(
            batch_size_prompts=self.cfg.eval_batch_size_prompts,
            n_eval_reconstruction_batches=self.cfg.n_eval_batches,
            n_eval_sparsity_variance_batches=self.cfg.n_eval_batches,
            compute_ce_loss=True,
            compute_l2_norms=True,
            compute_sparsity_metrics=True,
            compute_variance_metrics=True,
            compute_kl=False,
            compute_featurewise_weight_based_metrics=False,
        )
    @property
    def feature_sparsity(self) -> torch.Tensor:
        return self.act_freq_scores / self.n_frac_active_tokens
    @property
    def log_feature_sparsity(self) -> torch.Tensor:
        return _log_feature_sparsity(self.feature_sparsity)
    @property
    def current_l1_coefficient(self) -> float:
        return self.l1_scheduler.current_l1_coefficient
    @property
    def dead_neurons(self) -> torch.Tensor:
        return (self.n_forward_passes_since_fired > self.cfg.dead_feature_window).bool()
    def fit(self) -> TrainingSAE:
        pbar = tqdm(total=self.cfg.total_training_tokens, desc="Training SAE")
        self.activations_store.set_norm_scaling_factor_if_needed()
        # Train loop
        while self.n_training_tokens < self.cfg.total_training_tokens:
            # Do a training step.
            layer_acts = self.activations_store.next_batch()[:, 0, :].to(
                self.sae.device
            )
            self.n_training_tokens += self.cfg.train_batch_size_tokens
            step_output = self._train_step(sae=self.sae, sae_in=layer_acts)
            if self.cfg.log_to_wandb:
                self._log_train_step(step_output)
                self._run_and_log_evals()
            self._checkpoint_if_needed()
            self.n_training_steps += 1
            self._update_pbar(step_output, pbar)
            ### If n_training_tokens > sae_group.cfg.training_tokens, then we should switch to fine-tuning (if we haven't already)
            self._begin_finetuning_if_needed()
        # fold the estimated norm scaling factor into the sae weights
        if self.activations_store.estimated_norm_scaling_factor is not None:
            self.sae.fold_activation_norm_scaling_factor(
                self.activations_store.estimated_norm_scaling_factor
            )
            self.activations_store.estimated_norm_scaling_factor = None
        # save final sae group to checkpoints folder
        self.save_checkpoint(
            trainer=self,
            checkpoint_name=f"final_{self.n_training_tokens}",
            wandb_aliases=["final_model"],
        )
        pbar.close()
        return self.sae
    def _train_step(
        self,
        sae: TrainingSAE,
        sae_in: torch.Tensor,
    ) -> TrainStepOutput:
        sae.train()
        # Make sure the W_dec is still zero-norm
        if self.cfg.normalize_sae_decoder:
            sae.set_decoder_norm_to_unit_norm()
        # log and then reset the feature sparsity every feature_sampling_window steps
        if (self.n_training_steps + 1) % self.cfg.feature_sampling_window == 0:
            if self.cfg.log_to_wandb:
                sparsity_log_dict = self._build_sparsity_log_dict()
                wandb.log(sparsity_log_dict, step=self.n_training_steps)
            self._reset_running_sparsity_stats()
        # for documentation on autocasting see:
        # https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html
        with self.autocast_if_enabled:
            train_step_output = self.sae.training_forward_pass(
                sae_in=sae_in,
                dead_neuron_mask=self.dead_neurons,
                current_l1_coefficient=self.current_l1_coefficient,
            )
            with torch.no_grad():
                did_fire = (train_step_output.feature_acts > 0).float().sum(-2) > 0
                self.n_forward_passes_since_fired += 1
                self.n_forward_passes_since_fired[did_fire] = 0
                self.act_freq_scores += (
                    (train_step_output.feature_acts.abs() > 0).float().sum(0)
                )
                self.n_frac_active_tokens += self.cfg.train_batch_size_tokens
        # Scaler will rescale gradients if autocast is enabled
        self.scaler.scale(
            train_step_output.loss
        ).backward()  # loss.backward() if not autocasting
        self.scaler.unscale_(self.optimizer)  # needed to clip correctly
        # TODO: Work out if grad norm clipping should be in config / how to test it.
        torch.nn.utils.clip_grad_norm_(sae.parameters(), 1.0)
        self.scaler.step(self.optimizer)  # just ctx.optimizer.step() if not autocasting
        self.scaler.update()
        if self.cfg.normalize_sae_decoder:
            sae.remove_gradient_parallel_to_decoder_directions()
        self.optimizer.zero_grad()
        self.lr_scheduler.step()
        self.l1_scheduler.step()
        return train_step_output
    @torch.no_grad()
    def _log_train_step(self, step_output: TrainStepOutput):
        if (self.n_training_steps + 1) % self.cfg.wandb_log_frequency == 0:
            wandb.log(
                self._build_train_step_log_dict(
                    output=step_output,
                    n_training_tokens=self.n_training_tokens,
                ),
                step=self.n_training_steps,
            )
    @torch.no_grad()
    def _build_train_step_log_dict(
        self,
        output: TrainStepOutput,
        n_training_tokens: int,
    ) -> dict[str, Any]:
        sae_in = output.sae_in
        sae_out = output.sae_out
        feature_acts = output.feature_acts
        loss = output.loss.item()
        # metrics for currents acts
        l0 = (feature_acts > 0).float().sum(-1).mean()
        current_learning_rate = self.optimizer.param_groups[0]["lr"]
        per_token_l2_loss = (sae_out - sae_in).pow(2).sum(dim=-1).squeeze()
        total_variance = (sae_in - sae_in.mean(0)).pow(2).sum(-1)
        explained_variance = 1 - per_token_l2_loss / total_variance
        log_dict = {
            # losses
            "losses/overall_loss": loss,
            # variance explained
            "metrics/explained_variance": explained_variance.mean().item(),
            "metrics/explained_variance_std": explained_variance.std().item(),
            "metrics/l0": l0.item(),
            # sparsity
            "sparsity/mean_passes_since_fired": self.n_forward_passes_since_fired.mean().item(),
            "sparsity/dead_features": self.dead_neurons.sum().item(),
            "details/current_learning_rate": current_learning_rate,
            "details/current_l1_coefficient": self.current_l1_coefficient,
            "details/n_training_tokens": n_training_tokens,
        }
        for loss_name, loss_value in output.losses.items():
            loss_item = _unwrap_item(loss_value)
            # special case for l1 loss, which we normalize by the l1 coefficient
            if loss_name == "l1_loss":
                log_dict[f"losses/{loss_name}"] = (
                    loss_item / self.current_l1_coefficient
                )
                log_dict[f"losses/raw_{loss_name}"] = loss_item
            else:
                log_dict[f"losses/{loss_name}"] = loss_item
        return log_dict
    @torch.no_grad()
    def _run_and_log_evals(self):
        # record loss frequently, but not all the time.
        if (self.n_training_steps + 1) % (
            self.cfg.wandb_log_frequency * self.cfg.eval_every_n_wandb_logs
        ) == 0:
            self.sae.eval()
            ignore_tokens = set()
            if self.activations_store.exclude_special_tokens is not None:
                ignore_tokens = set(
                    self.activations_store.exclude_special_tokens.tolist()
                )
            eval_metrics, _ = run_evals(
                sae=self.sae,
                activation_store=self.activations_store,
                model=self.model,
                eval_config=self.trainer_eval_config,
                ignore_tokens=ignore_tokens,
                model_kwargs=self.cfg.model_kwargs,
            )  # not calculating featurwise metrics here.
            # Remove eval metrics that are already logged during training
            eval_metrics.pop("metrics/explained_variance", None)
            eval_metrics.pop("metrics/explained_variance_std", None)
            eval_metrics.pop("metrics/l0", None)
            eval_metrics.pop("metrics/l1", None)
            eval_metrics.pop("metrics/mse", None)
            # Remove metrics that are not useful for wandb logging
            eval_metrics.pop("metrics/total_tokens_evaluated", None)
            W_dec_norm_dist = self.sae.W_dec.detach().float().norm(dim=1).cpu().numpy()
            eval_metrics["weights/W_dec_norms"] = wandb.Histogram(W_dec_norm_dist)  # type: ignore
            if self.sae.cfg.architecture == "standard":
                b_e_dist = self.sae.b_enc.detach().float().cpu().numpy()
                eval_metrics["weights/b_e"] = wandb.Histogram(b_e_dist)  # type: ignore
            elif self.sae.cfg.architecture == "gated":
                b_gate_dist = self.sae.b_gate.detach().float().cpu().numpy()
                eval_metrics["weights/b_gate"] = wandb.Histogram(b_gate_dist)  # type: ignore
                b_mag_dist = self.sae.b_mag.detach().float().cpu().numpy()
                eval_metrics["weights/b_mag"] = wandb.Histogram(b_mag_dist)  # type: ignore
            wandb.log(
                eval_metrics,
                step=self.n_training_steps,
            )
            self.sae.train()
    @torch.no_grad()
    def _build_sparsity_log_dict(self) -> dict[str, Any]:
        log_feature_sparsity = _log_feature_sparsity(self.feature_sparsity)
        wandb_histogram = wandb.Histogram(log_feature_sparsity.numpy())  # type: ignore
        return {
            "metrics/mean_log10_feature_sparsity": log_feature_sparsity.mean().item(),
            "plots/feature_density_line_chart": wandb_histogram,
            "sparsity/below_1e-5": (self.feature_sparsity < 1e-5).sum().item(),
            "sparsity/below_1e-6": (self.feature_sparsity < 1e-6).sum().item(),
        }
    @torch.no_grad()
    def _reset_running_sparsity_stats(self) -> None:
        self.act_freq_scores = torch.zeros(
            self.cfg.d_sae,  # type: ignore
            device=self.cfg.device,
        )
        self.n_frac_active_tokens = 0
    @torch.no_grad()
    def _checkpoint_if_needed(self):
        if (
            self.checkpoint_thresholds
            and self.n_training_tokens > self.checkpoint_thresholds[0]
        ):
            self.save_checkpoint(
                trainer=self,
                checkpoint_name=str(self.n_training_tokens),
            )
            self.checkpoint_thresholds.pop(0)
    @torch.no_grad()
    def _update_pbar(
        self,
        step_output: TrainStepOutput,
        pbar: tqdm,  # type: ignore
        update_interval: int = 100,
    ):
        if self.n_training_steps % update_interval == 0:
            loss_strs = " | ".join(
                f"{loss_name}: {_unwrap_item(loss_value):.5f}"
                for loss_name, loss_value in step_output.losses.items()
            )
            pbar.set_description(f"{self.n_training_steps}| {loss_strs}")
            pbar.update(update_interval * self.cfg.train_batch_size_tokens)
    def _begin_finetuning_if_needed(self):
        if (not self.started_fine_tuning) and (
            self.n_training_tokens > self.cfg.training_tokens
        ):
            self.started_fine_tuning = True
            # finetuning method should be set in the config
            # if not, then we don't finetune
            if not isinstance(self.cfg.finetuning_method, str):
                return
            for name, param in self.sae.named_parameters():
                if name in FINETUNING_PARAMETERS[self.cfg.finetuning_method]:
                    param.requires_grad = True
                else:
                    param.requires_grad = False
            self.finetuning = True
def _unwrap_item(item: float | torch.Tensor) -> float:
    return item.item() if isinstance(item, torch.Tensor) else item

================
File: sae_lens/training/training_sae.py
================
"""Most of this is just copied over from Arthur's code and slightly simplified:
https://github.com/ArthurConmy/sae/blob/main/sae/model.py
"""
import json
import os
from dataclasses import dataclass, fields
from typing import Any, Optional
import einops
import numpy as np
import torch
from jaxtyping import Float
from torch import nn
from sae_lens import logger
from sae_lens.config import LanguageModelSAERunnerConfig
from sae_lens.sae import SAE, SAEConfig
from sae_lens.toolkit.pretrained_sae_loaders import (
    handle_config_defaulting,
    read_sae_from_disk,
)
SPARSITY_PATH = "sparsity.safetensors"
SAE_WEIGHTS_PATH = "sae_weights.safetensors"
SAE_CFG_PATH = "cfg.json"
def rectangle(x: torch.Tensor) -> torch.Tensor:
    return ((x > -0.5) & (x < 0.5)).to(x)
class Step(torch.autograd.Function):
    @staticmethod
    def forward(
        x: torch.Tensor,
        threshold: torch.Tensor,
        bandwidth: float,  # noqa: ARG004
    ) -> torch.Tensor:
        return (x > threshold).to(x)
    @staticmethod
    def setup_context(
        ctx: Any, inputs: tuple[torch.Tensor, torch.Tensor, float], output: torch.Tensor
    ) -> None:
        x, threshold, bandwidth = inputs
        del output
        ctx.save_for_backward(x, threshold)
        ctx.bandwidth = bandwidth
    @staticmethod
    def backward(  # type: ignore[override]
        ctx: Any, grad_output: torch.Tensor
    ) -> tuple[None, torch.Tensor, None]:
        x, threshold = ctx.saved_tensors
        bandwidth = ctx.bandwidth
        threshold_grad = torch.sum(
            -(1.0 / bandwidth) * rectangle((x - threshold) / bandwidth) * grad_output,
            dim=0,
        )
        return None, threshold_grad, None
class JumpReLU(torch.autograd.Function):
    @staticmethod
    def forward(
        x: torch.Tensor,
        threshold: torch.Tensor,
        bandwidth: float,  # noqa: ARG004
    ) -> torch.Tensor:
        return (x * (x > threshold)).to(x)
    @staticmethod
    def setup_context(
        ctx: Any, inputs: tuple[torch.Tensor, torch.Tensor, float], output: torch.Tensor
    ) -> None:
        x, threshold, bandwidth = inputs
        del output
        ctx.save_for_backward(x, threshold)
        ctx.bandwidth = bandwidth
    @staticmethod
    def backward(  # type: ignore[override]
        ctx: Any, grad_output: torch.Tensor
    ) -> tuple[torch.Tensor, torch.Tensor, None]:
        x, threshold = ctx.saved_tensors
        bandwidth = ctx.bandwidth
        x_grad = (x > threshold) * grad_output  # We don't apply STE to x input
        threshold_grad = torch.sum(
            -(threshold / bandwidth)
            * rectangle((x - threshold) / bandwidth)
            * grad_output,
            dim=0,
        )
        return x_grad, threshold_grad, None
@dataclass
class TrainStepOutput:
    sae_in: torch.Tensor
    sae_out: torch.Tensor
    feature_acts: torch.Tensor
    hidden_pre: torch.Tensor
    loss: torch.Tensor  # we need to call backwards on this
    losses: dict[str, float | torch.Tensor]
@dataclass(kw_only=True)
class TrainingSAEConfig(SAEConfig):
    # Sparsity Loss Calculations
    l1_coefficient: float
    lp_norm: float
    use_ghost_grads: bool
    normalize_sae_decoder: bool
    noise_scale: float
    decoder_orthogonal_init: bool
    mse_loss_normalization: Optional[str]
    jumprelu_init_threshold: float
    jumprelu_bandwidth: float
    decoder_heuristic_init: bool
    init_encoder_as_decoder_transpose: bool
    scale_sparsity_penalty_by_decoder_norm: bool
    @classmethod
    def from_sae_runner_config(
        cls, cfg: LanguageModelSAERunnerConfig
    ) -> "TrainingSAEConfig":
        return cls(
            # base config
            architecture=cfg.architecture,
            d_in=cfg.d_in,
            d_sae=cfg.d_sae,  # type: ignore
            dtype=cfg.dtype,
            device=cfg.device,
            model_name=cfg.model_name,
            hook_name=cfg.hook_name,
            hook_layer=cfg.hook_layer,
            hook_head_index=cfg.hook_head_index,
            activation_fn_str=cfg.activation_fn,
            activation_fn_kwargs=cfg.activation_fn_kwargs,
            apply_b_dec_to_input=cfg.apply_b_dec_to_input,
            finetuning_scaling_factor=cfg.finetuning_method is not None,
            sae_lens_training_version=cfg.sae_lens_training_version,
            context_size=cfg.context_size,
            dataset_path=cfg.dataset_path,
            prepend_bos=cfg.prepend_bos,
            seqpos_slice=cfg.seqpos_slice,
            # Training cfg
            l1_coefficient=cfg.l1_coefficient,
            lp_norm=cfg.lp_norm,
            use_ghost_grads=cfg.use_ghost_grads,
            normalize_sae_decoder=cfg.normalize_sae_decoder,
            noise_scale=cfg.noise_scale,
            decoder_orthogonal_init=cfg.decoder_orthogonal_init,
            mse_loss_normalization=cfg.mse_loss_normalization,
            decoder_heuristic_init=cfg.decoder_heuristic_init,
            init_encoder_as_decoder_transpose=cfg.init_encoder_as_decoder_transpose,
            scale_sparsity_penalty_by_decoder_norm=cfg.scale_sparsity_penalty_by_decoder_norm,
            normalize_activations=cfg.normalize_activations,
            dataset_trust_remote_code=cfg.dataset_trust_remote_code,
            model_from_pretrained_kwargs=cfg.model_from_pretrained_kwargs or {},
            jumprelu_init_threshold=cfg.jumprelu_init_threshold,
            jumprelu_bandwidth=cfg.jumprelu_bandwidth,
        )
    @classmethod
    def from_dict(cls, config_dict: dict[str, Any]) -> "TrainingSAEConfig":
        # remove any keys that are not in the dataclass
        # since we sometimes enhance the config with the whole LM runner config
        valid_field_names = {field.name for field in fields(cls)}
        valid_config_dict = {
            key: val for key, val in config_dict.items() if key in valid_field_names
        }
        # ensure seqpos slice is tuple
        # ensure that seqpos slices is a tuple
        # Ensure seqpos_slice is a tuple
        if "seqpos_slice" in valid_config_dict:
            if isinstance(valid_config_dict["seqpos_slice"], list):
                valid_config_dict["seqpos_slice"] = tuple(
                    valid_config_dict["seqpos_slice"]
                )
            elif not isinstance(valid_config_dict["seqpos_slice"], tuple):
                valid_config_dict["seqpos_slice"] = (valid_config_dict["seqpos_slice"],)
        return TrainingSAEConfig(**valid_config_dict)
    def to_dict(self) -> dict[str, Any]:
        return {
            **super().to_dict(),
            "l1_coefficient": self.l1_coefficient,
            "lp_norm": self.lp_norm,
            "use_ghost_grads": self.use_ghost_grads,
            "normalize_sae_decoder": self.normalize_sae_decoder,
            "noise_scale": self.noise_scale,
            "decoder_orthogonal_init": self.decoder_orthogonal_init,
            "init_encoder_as_decoder_transpose": self.init_encoder_as_decoder_transpose,
            "mse_loss_normalization": self.mse_loss_normalization,
            "decoder_heuristic_init": self.decoder_heuristic_init,
            "scale_sparsity_penalty_by_decoder_norm": self.scale_sparsity_penalty_by_decoder_norm,
            "normalize_activations": self.normalize_activations,
            "jumprelu_init_threshold": self.jumprelu_init_threshold,
            "jumprelu_bandwidth": self.jumprelu_bandwidth,
        }
    # this needs to exist so we can initialize the parent sae cfg without the training specific
    # parameters. Maybe there's a cleaner way to do this
    def get_base_sae_cfg_dict(self) -> dict[str, Any]:
        return {
            "architecture": self.architecture,
            "d_in": self.d_in,
            "d_sae": self.d_sae,
            "activation_fn_str": self.activation_fn_str,
            "activation_fn_kwargs": self.activation_fn_kwargs,
            "apply_b_dec_to_input": self.apply_b_dec_to_input,
            "dtype": self.dtype,
            "model_name": self.model_name,
            "hook_name": self.hook_name,
            "hook_layer": self.hook_layer,
            "hook_head_index": self.hook_head_index,
            "device": self.device,
            "context_size": self.context_size,
            "prepend_bos": self.prepend_bos,
            "finetuning_scaling_factor": self.finetuning_scaling_factor,
            "normalize_activations": self.normalize_activations,
            "dataset_path": self.dataset_path,
            "dataset_trust_remote_code": self.dataset_trust_remote_code,
            "sae_lens_training_version": self.sae_lens_training_version,
        }
class TrainingSAE(SAE):
    """
    A SAE used for training. This class provides a `training_forward_pass` method which calculates
    losses used for training.
    """
    cfg: TrainingSAEConfig
    use_error_term: bool
    dtype: torch.dtype
    device: torch.device
    def __init__(self, cfg: TrainingSAEConfig, use_error_term: bool = False):
        base_sae_cfg = SAEConfig.from_dict(cfg.get_base_sae_cfg_dict())
        super().__init__(base_sae_cfg)
        self.cfg = cfg  # type: ignore
        if cfg.architecture == "standard" or cfg.architecture == "topk":
            self.encode_with_hidden_pre_fn = self.encode_with_hidden_pre
        elif cfg.architecture == "gated":
            self.encode_with_hidden_pre_fn = self.encode_with_hidden_pre_gated
        elif cfg.architecture == "jumprelu":
            self.encode_with_hidden_pre_fn = self.encode_with_hidden_pre_jumprelu
            self.bandwidth = cfg.jumprelu_bandwidth
            self.log_threshold.data = torch.ones(
                self.cfg.d_sae, dtype=self.dtype, device=self.device
            ) * np.log(cfg.jumprelu_init_threshold)
        else:
            raise ValueError(f"Unknown architecture: {cfg.architecture}")
        self.check_cfg_compatibility()
        self.use_error_term = use_error_term
        self.initialize_weights_complex()
        # The training SAE will assume that the activation store handles
        # reshaping.
        self.turn_off_forward_pass_hook_z_reshaping()
        self.mse_loss_fn = self._get_mse_loss_fn()
    def initialize_weights_jumprelu(self):
        # same as the superclass, except we use a log_threshold parameter instead of threshold
        self.log_threshold = nn.Parameter(
            torch.empty(self.cfg.d_sae, dtype=self.dtype, device=self.device)
        )
        self.initialize_weights_basic()
    @property
    def threshold(self) -> torch.Tensor:
        if self.cfg.architecture != "jumprelu":
            raise ValueError("Threshold is only defined for Jumprelu SAEs")
        return torch.exp(self.log_threshold)
    @classmethod
    def from_dict(cls, config_dict: dict[str, Any]) -> "TrainingSAE":
        return cls(TrainingSAEConfig.from_dict(config_dict))
    def check_cfg_compatibility(self):
        if self.cfg.architecture != "standard" and self.cfg.use_ghost_grads:
            raise ValueError(f"{self.cfg.architecture} SAEs do not support ghost grads")
        if self.cfg.architecture == "gated" and self.use_error_term:
            raise ValueError("Gated SAEs do not support error terms")
    def encode_standard(
        self, x: Float[torch.Tensor, "... d_in"]
    ) -> Float[torch.Tensor, "... d_sae"]:
        """
        Calcuate SAE features from inputs
        """
        feature_acts, _ = self.encode_with_hidden_pre_fn(x)
        return feature_acts
    def encode_with_hidden_pre_jumprelu(
        self, x: Float[torch.Tensor, "... d_in"]
    ) -> tuple[Float[torch.Tensor, "... d_sae"], Float[torch.Tensor, "... d_sae"]]:
        sae_in = self.process_sae_in(x)
        hidden_pre = sae_in @ self.W_enc + self.b_enc
        if self.training:
            hidden_pre = (
                hidden_pre + torch.randn_like(hidden_pre) * self.cfg.noise_scale
            )
        threshold = torch.exp(self.log_threshold)
        feature_acts = JumpReLU.apply(hidden_pre, threshold, self.bandwidth)
        return feature_acts, hidden_pre  # type: ignore
    def encode_with_hidden_pre(
        self, x: Float[torch.Tensor, "... d_in"]
    ) -> tuple[Float[torch.Tensor, "... d_sae"], Float[torch.Tensor, "... d_sae"]]:
        sae_in = self.process_sae_in(x)
        # "... d_in, d_in d_sae -> ... d_sae",
        hidden_pre = self.hook_sae_acts_pre(sae_in @ self.W_enc + self.b_enc)
        hidden_pre_noised = hidden_pre + (
            torch.randn_like(hidden_pre) * self.cfg.noise_scale * self.training
        )
        feature_acts = self.hook_sae_acts_post(self.activation_fn(hidden_pre_noised))
        return feature_acts, hidden_pre_noised
    def encode_with_hidden_pre_gated(
        self, x: Float[torch.Tensor, "... d_in"]
    ) -> tuple[Float[torch.Tensor, "... d_sae"], Float[torch.Tensor, "... d_sae"]]:
        sae_in = self.process_sae_in(x)
        # Gating path with Heaviside step function
        gating_pre_activation = sae_in @ self.W_enc + self.b_gate
        active_features = (gating_pre_activation > 0).to(self.dtype)
        # Magnitude path with weight sharing
        magnitude_pre_activation = sae_in @ (self.W_enc * self.r_mag.exp()) + self.b_mag
        # magnitude_pre_activation_noised = magnitude_pre_activation + (
        #     torch.randn_like(magnitude_pre_activation) * self.cfg.noise_scale * self.training
        # )
        feature_magnitudes = self.activation_fn(
            magnitude_pre_activation
        )  # magnitude_pre_activation_noised)
        # Return both the gated feature activations and the magnitude pre-activations
        return (
            active_features * feature_magnitudes,
            magnitude_pre_activation,
        )  # magnitude_pre_activation_noised
    def forward(
        self,
        x: Float[torch.Tensor, "... d_in"],
    ) -> Float[torch.Tensor, "... d_in"]:
        feature_acts, _ = self.encode_with_hidden_pre_fn(x)
        return self.decode(feature_acts)
    def training_forward_pass(
        self,
        sae_in: torch.Tensor,
        current_l1_coefficient: float,
        dead_neuron_mask: Optional[torch.Tensor] = None,
    ) -> TrainStepOutput:
        # do a forward pass to get SAE out, but we also need the
        # hidden pre.
        feature_acts, hidden_pre = self.encode_with_hidden_pre_fn(sae_in)
        sae_out = self.decode(feature_acts)
        # MSE LOSS
        per_item_mse_loss = self.mse_loss_fn(sae_out, sae_in)
        mse_loss = per_item_mse_loss.sum(dim=-1).mean()
        losses: dict[str, float | torch.Tensor] = {}
        if self.cfg.architecture == "gated":
            # Gated SAE Loss Calculation
            # Shared variables
            sae_in_centered = (
                self.reshape_fn_in(sae_in) - self.b_dec * self.cfg.apply_b_dec_to_input
            )
            pi_gate = sae_in_centered @ self.W_enc + self.b_gate
            pi_gate_act = torch.relu(pi_gate)
            # SFN sparsity loss - summed over the feature dimension and averaged over the batch
            l1_loss = (
                current_l1_coefficient
                * torch.sum(pi_gate_act * self.W_dec.norm(dim=1), dim=-1).mean()
            )
            # Auxiliary reconstruction loss - summed over the feature dimension and averaged over the batch
            via_gate_reconstruction = pi_gate_act @ self.W_dec + self.b_dec
            aux_reconstruction_loss = torch.sum(
                (via_gate_reconstruction - sae_in) ** 2, dim=-1
            ).mean()
            loss = mse_loss + l1_loss + aux_reconstruction_loss
            losses["auxiliary_reconstruction_loss"] = aux_reconstruction_loss
            losses["l1_loss"] = l1_loss
        elif self.cfg.architecture == "jumprelu":
            threshold = torch.exp(self.log_threshold)
            l0 = torch.sum(Step.apply(hidden_pre, threshold, self.bandwidth), dim=-1)  # type: ignore
            l0_loss = (current_l1_coefficient * l0).mean()
            loss = mse_loss + l0_loss
            losses["l0_loss"] = l0_loss
        elif self.cfg.architecture == "topk":
            topk_loss = self.calculate_topk_aux_loss(
                sae_in=sae_in,
                sae_out=sae_out,
                hidden_pre=hidden_pre,
                dead_neuron_mask=dead_neuron_mask,
            )
            losses["auxiliary_reconstruction_loss"] = topk_loss
            loss = mse_loss + topk_loss
        else:
            # default SAE sparsity loss
            weighted_feature_acts = feature_acts
            if self.cfg.scale_sparsity_penalty_by_decoder_norm:
                weighted_feature_acts = feature_acts * self.W_dec.norm(dim=1)
            sparsity = weighted_feature_acts.norm(
                p=self.cfg.lp_norm, dim=-1
            )  # sum over the feature dimension
            l1_loss = (current_l1_coefficient * sparsity).mean()
            loss = mse_loss + l1_loss
            if (
                self.cfg.use_ghost_grads
                and self.training
                and dead_neuron_mask is not None
            ):
                ghost_grad_loss = self.calculate_ghost_grad_loss(
                    x=sae_in,
                    sae_out=sae_out,
                    per_item_mse_loss=per_item_mse_loss,
                    hidden_pre=hidden_pre,
                    dead_neuron_mask=dead_neuron_mask,
                )
                losses["ghost_grad_loss"] = ghost_grad_loss
                loss = loss + ghost_grad_loss
            losses["l1_loss"] = l1_loss
        losses["mse_loss"] = mse_loss
        return TrainStepOutput(
            sae_in=sae_in,
            sae_out=sae_out,
            feature_acts=feature_acts,
            hidden_pre=hidden_pre,
            loss=loss,
            losses=losses,
        )
    def calculate_topk_aux_loss(
        self,
        sae_in: torch.Tensor,
        sae_out: torch.Tensor,
        hidden_pre: torch.Tensor,
        dead_neuron_mask: torch.Tensor | None,
    ) -> torch.Tensor:
        # Mostly taken from https://github.com/EleutherAI/sae/blob/main/sae/sae.py, except without variance normalization
        # NOTE: checking the number of dead neurons will force a GPU sync, so performance can likely be improved here
        if (
            dead_neuron_mask is not None
            and (num_dead := int(dead_neuron_mask.sum())) > 0
        ):
            residual = sae_in - sae_out
            # Heuristic from Appendix B.1 in the paper
            k_aux = hidden_pre.shape[-1] // 2
            # Reduce the scale of the loss if there are a small number of dead latents
            scale = min(num_dead / k_aux, 1.0)
            k_aux = min(k_aux, num_dead)
            auxk_acts = _calculate_topk_aux_acts(
                k_aux=k_aux,
                hidden_pre=hidden_pre,
                dead_neuron_mask=dead_neuron_mask,
            )
            # Encourage the top ~50% of dead latents to predict the residual of the
            # top k living latents
            recons = self.decode(auxk_acts)
            auxk_loss = (recons - residual).pow(2).sum(dim=-1).mean()
            return scale * auxk_loss
        return sae_out.new_tensor(0.0)
    def calculate_ghost_grad_loss(
        self,
        x: torch.Tensor,
        sae_out: torch.Tensor,
        per_item_mse_loss: torch.Tensor,
        hidden_pre: torch.Tensor,
        dead_neuron_mask: torch.Tensor,
    ) -> torch.Tensor:
        # 1.
        residual = x - sae_out
        l2_norm_residual = torch.norm(residual, dim=-1)
        # 2.
        # ghost grads use an exponentional activation function, ignoring whatever
        # the activation function is in the SAE. The forward pass uses the dead neurons only.
        feature_acts_dead_neurons_only = torch.exp(hidden_pre[:, dead_neuron_mask])
        ghost_out = feature_acts_dead_neurons_only @ self.W_dec[dead_neuron_mask, :]
        l2_norm_ghost_out = torch.norm(ghost_out, dim=-1)
        norm_scaling_factor = l2_norm_residual / (1e-6 + l2_norm_ghost_out * 2)
        ghost_out = ghost_out * norm_scaling_factor[:, None].detach()
        # 3. There is some fairly complex rescaling here to make sure that the loss
        # is comparable to the original loss. This is because the ghost grads are
        # only calculated for the dead neurons, so we need to rescale the loss to
        # make sure that the loss is comparable to the original loss.
        # There have been methodological improvements that are not implemented here yet
        # see here: https://www.lesswrong.com/posts/C5KAZQib3bzzpeyrg/full-post-progress-update-1-from-the-gdm-mech-interp-team#Improving_ghost_grads
        per_item_mse_loss_ghost_resid = self.mse_loss_fn(ghost_out, residual.detach())
        mse_rescaling_factor = (
            per_item_mse_loss / (per_item_mse_loss_ghost_resid + 1e-6)
        ).detach()
        per_item_mse_loss_ghost_resid = (
            mse_rescaling_factor * per_item_mse_loss_ghost_resid
        )
        return per_item_mse_loss_ghost_resid.mean()
    @torch.no_grad()
    def _get_mse_loss_fn(self) -> Any:
        def standard_mse_loss_fn(
            preds: torch.Tensor, target: torch.Tensor
        ) -> torch.Tensor:
            return torch.nn.functional.mse_loss(preds, target, reduction="none")
        def batch_norm_mse_loss_fn(
            preds: torch.Tensor, target: torch.Tensor
        ) -> torch.Tensor:
            target_centered = target - target.mean(dim=0, keepdim=True)
            normalization = target_centered.norm(dim=-1, keepdim=True)
            return torch.nn.functional.mse_loss(preds, target, reduction="none") / (
                normalization + 1e-6
            )
        if self.cfg.mse_loss_normalization == "dense_batch":
            return batch_norm_mse_loss_fn
        return standard_mse_loss_fn
    def process_state_dict_for_saving(self, state_dict: dict[str, Any]) -> None:
        if self.cfg.architecture == "jumprelu" and "log_threshold" in state_dict:
            threshold = torch.exp(state_dict["log_threshold"]).detach().contiguous()
            del state_dict["log_threshold"]
            state_dict["threshold"] = threshold
    def process_state_dict_for_loading(self, state_dict: dict[str, Any]) -> None:
        if self.cfg.architecture == "jumprelu" and "threshold" in state_dict:
            threshold = state_dict["threshold"]
            del state_dict["threshold"]
            state_dict["log_threshold"] = torch.log(threshold).detach().contiguous()
    @classmethod
    def load_from_pretrained(
        cls,
        path: str,
        device: str = "cpu",
        dtype: str | None = None,
    ) -> "TrainingSAE":
        # get the config
        config_path = os.path.join(path, SAE_CFG_PATH)
        with open(config_path) as f:
            cfg_dict = json.load(f)
        cfg_dict = handle_config_defaulting(cfg_dict)
        cfg_dict["device"] = device
        if dtype is not None:
            cfg_dict["dtype"] = dtype
        weight_path = os.path.join(path, SAE_WEIGHTS_PATH)
        cfg_dict, state_dict = read_sae_from_disk(
            cfg_dict=cfg_dict,
            weight_path=weight_path,
            device=device,
        )
        sae_cfg = TrainingSAEConfig.from_dict(cfg_dict)
        sae = cls(sae_cfg)
        sae.process_state_dict_for_loading(state_dict)
        sae.load_state_dict(state_dict)
        return sae
    def initialize_weights_complex(self):
        """ """
        if self.cfg.decoder_orthogonal_init:
            self.W_dec.data = nn.init.orthogonal_(self.W_dec.data.T).T
        elif self.cfg.decoder_heuristic_init:
            self.W_dec = nn.Parameter(
                torch.rand(
                    self.cfg.d_sae, self.cfg.d_in, dtype=self.dtype, device=self.device
                )
            )
            self.initialize_decoder_norm_constant_norm()
        # Then we initialize the encoder weights (either as the transpose of decoder or not)
        if self.cfg.init_encoder_as_decoder_transpose:
            self.W_enc.data = self.W_dec.data.T.clone().contiguous()
        else:
            self.W_enc = nn.Parameter(
                torch.nn.init.kaiming_uniform_(
                    torch.empty(
                        self.cfg.d_in,
                        self.cfg.d_sae,
                        dtype=self.dtype,
                        device=self.device,
                    )
                )
            )
        if self.cfg.normalize_sae_decoder:
            with torch.no_grad():
                # Anthropic normalize this to have unit columns
                self.set_decoder_norm_to_unit_norm()
    @torch.no_grad()
    def fold_W_dec_norm(self):
        # need to deal with the jumprelu having a log_threshold in training
        if self.cfg.architecture == "jumprelu":
            cur_threshold = self.threshold.clone()
            W_dec_norms = self.W_dec.norm(dim=-1).unsqueeze(1)
            super().fold_W_dec_norm()
            self.log_threshold.data = torch.log(cur_threshold * W_dec_norms.squeeze())
        else:
            super().fold_W_dec_norm()
    ## Initialization Methods
    @torch.no_grad()
    def initialize_b_dec_with_precalculated(self, origin: torch.Tensor):
        out = torch.tensor(origin, dtype=self.dtype, device=self.device)
        self.b_dec.data = out
    @torch.no_grad()
    def initialize_b_dec_with_mean(self, all_activations: torch.Tensor):
        previous_b_dec = self.b_dec.clone().cpu()
        out = all_activations.mean(dim=0)
        previous_distances = torch.norm(all_activations - previous_b_dec, dim=-1)
        distances = torch.norm(all_activations - out, dim=-1)
        logger.info("Reinitializing b_dec with mean of activations")
        logger.debug(
            f"Previous distances: {previous_distances.median(0).values.mean().item()}"
        )
        logger.debug(f"New distances: {distances.median(0).values.mean().item()}")
        self.b_dec.data = out.to(self.dtype).to(self.device)
    ## Training Utils
    @torch.no_grad()
    def set_decoder_norm_to_unit_norm(self):
        self.W_dec.data /= torch.norm(self.W_dec.data, dim=1, keepdim=True)
    @torch.no_grad()
    def initialize_decoder_norm_constant_norm(self, norm: float = 0.1):
        """
        A heuristic proceedure inspired by:
        https://transformer-circuits.pub/2024/april-update/index.html#training-saes
        """
        # TODO: Parameterise this as a function of m and n
        # ensure W_dec norms at unit norm
        self.W_dec.data /= torch.norm(self.W_dec.data, dim=1, keepdim=True)
        self.W_dec.data *= norm  # will break tests but do this for now.
    @torch.no_grad()
    def remove_gradient_parallel_to_decoder_directions(self):
        """
        Update grads so that they remove the parallel component
            (d_sae, d_in) shape
        """
        assert self.W_dec.grad is not None  # keep pyright happy
        parallel_component = einops.einsum(
            self.W_dec.grad,
            self.W_dec.data,
            "d_sae d_in, d_sae d_in -> d_sae",
        )
        self.W_dec.grad -= einops.einsum(
            parallel_component,
            self.W_dec.data,
            "d_sae, d_sae d_in -> d_sae d_in",
        )
def _calculate_topk_aux_acts(
    k_aux: int,
    hidden_pre: torch.Tensor,
    dead_neuron_mask: torch.Tensor,
) -> torch.Tensor:
    # Don't include living latents in this loss
    auxk_latents = torch.where(dead_neuron_mask[None], hidden_pre, -torch.inf)
    # Top-k dead latents
    auxk_topk = auxk_latents.topk(k_aux, sorted=False)
    # Set the activations to zero for all but the top k_aux dead latents
    auxk_acts = torch.zeros_like(hidden_pre)
    auxk_acts.scatter_(-1, auxk_topk.indices, auxk_topk.values)
    # Set activations to zero for all but top k_aux dead latents
    return auxk_acts

================
File: sae_lens/training/upload_saes_to_huggingface.py
================
import io
from pathlib import Path
from tempfile import TemporaryDirectory
from textwrap import dedent
from typing import Iterable
from huggingface_hub import HfApi, create_repo, get_hf_file_metadata, hf_hub_url
from huggingface_hub.utils import EntryNotFoundError, RepositoryNotFoundError
from tqdm.autonotebook import tqdm
from sae_lens import logger
from sae_lens.sae import SAE, SAE_CFG_FILENAME, SAE_WEIGHTS_FILENAME, SPARSITY_FILENAME
def upload_saes_to_huggingface(
    saes_dict: dict[str, SAE | Path | str],
    hf_repo_id: str,
    hf_revision: str = "main",
    show_progress: bool = True,
    add_default_readme: bool = True,
):
    api = HfApi()
    if len(saes_dict) == 0:
        raise ValueError("No SAEs to upload")
    # pre-validate that everything is a SAE or a valid path before starting the upload
    for sae_ref in saes_dict.values():
        if isinstance(sae_ref, SAE):
            continue
        _validate_sae_path(Path(sae_ref))
    if not _repo_exists(api, hf_repo_id):
        create_repo(hf_repo_id)
    for sae_id, sae_ref in tqdm(
        saes_dict.items(), desc="Uploading SAEs", disable=not show_progress
    ):
        with TemporaryDirectory() as tmp_dir:
            sae_path = _build_sae_path(sae_ref, tmp_dir)
            _validate_sae_path(sae_path)
            _upload_sae(
                api,
                sae_path,
                repo_id=hf_repo_id,
                sae_id=sae_id,
                revision=hf_revision,
            )
        if add_default_readme:
            if _repo_file_exists(hf_repo_id, "README.md", hf_revision):
                logger.info("README.md already exists in the repo, skipping upload")
            else:
                readme = _create_default_readme(hf_repo_id, saes_dict)
                readme_io = io.BytesIO()
                readme_io.write(readme.encode("utf-8"))
                readme_io.seek(0)
                api.upload_file(
                    path_or_fileobj=readme_io,
                    path_in_repo="README.md",
                    repo_id=hf_repo_id,
                    revision=hf_revision,
                    commit_message="Add README.md",
                )
def _create_default_readme(repo_id: str, sae_ids: Iterable[str]) -> str:
    readme = dedent(
        """
        ---
        library_name: saelens
        ---
        # SAEs for use with the SAELens library
        This repository contains the following SAEs:
        """
    )
    for sae_id in sae_ids:
        readme += f"- {sae_id}\n"
    readme += dedent(
        f"""
        Load these SAEs using SAELens as below:
        ```python
        from sae_lens import SAE
        sae, cfg_dict, sparsity = SAE.from_pretrained("{repo_id}", "<sae_id>")
        ```
        """
    )
    return readme.strip()
def _repo_file_exists(repo_id: str, filename: str, revision: str) -> bool:
    try:
        url = hf_hub_url(repo_id=repo_id, filename=filename, revision=revision)
        get_hf_file_metadata(url)
        return True
    except EntryNotFoundError:
        return False
def _repo_exists(api: HfApi, repo_id: str) -> bool:
    try:
        api.repo_info(repo_id)
        return True
    except RepositoryNotFoundError:
        return False
def _upload_sae(api: HfApi, sae_path: Path, repo_id: str, sae_id: str, revision: str):
    api.upload_folder(
        folder_path=sae_path,
        path_in_repo=sae_id,
        repo_id=repo_id,
        revision=revision,
        repo_type="model",
        commit_message=f"Upload SAE {sae_id}",
        allow_patterns=[SAE_CFG_FILENAME, SAE_WEIGHTS_FILENAME, SPARSITY_FILENAME],
    )
def _build_sae_path(sae_ref: SAE | Path | str, tmp_dir: str) -> Path:
    if isinstance(sae_ref, SAE):
        sae_ref.save_model(tmp_dir)
        return Path(tmp_dir)
    if isinstance(sae_ref, Path):
        return sae_ref
    return Path(sae_ref)
def _validate_sae_path(sae_path: Path):
    "Validate that the model files exist in the given path."
    if not (sae_path / SAE_CFG_FILENAME).exists():
        raise FileNotFoundError(
            f"SAE config file not found: {sae_path / SAE_CFG_FILENAME}"
        )
    if not (sae_path / SAE_WEIGHTS_FILENAME).exists():
        raise FileNotFoundError(
            f"SAE weights file not found: {sae_path / SAE_WEIGHTS_FILENAME}"
        )

================
File: scripts/ansible/README.md
================
This is an Ansible playbook that runs `Cache Activations` and and `Train SAE` in AWS.

- The playbook looks in the `configs` directory for what jobs to run, and runs them.
- It makes a copy of previously run jobs in the `jobs` directory.
- Check out the `configs_example` directory and read the comments in the YAML files.

### Prerequisites
- AWS Account
- AWS ability to launch G instance types - you need to submit a request to enable this.
  - [Submit request for G. Click "Request increase at account level".](https://us-east-1.console.aws.amazon.com/servicequotas/home/services/ec2/quotas/L-DB2E81BA)
  - [Increase other quotas (like P instances) - Be sure to request On-Demand, not Spot"](https://us-east-1.console.aws.amazon.com/servicequotas/home/services/ec2/quotas)
  - G and P instances are not enabled by default [docs](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-resource-limits.html)
  - What GPUs/specs are G and P instance types? [docs](https://docs.aws.amazon.com/dlami/latest/devguide/gpu.html)
- Wandb Account (wandb.ai)

### Local setup

#### Save AWS Credentials locally
1) Generate a set of AWS access keys 
   1) Sign into AWS
   2) [Click here to generate keys](
	https://us-east-1.console.aws.amazon.com/iam/home?region=us-east-1#/security_credentials/access-key-wizard)

2) Save the following file into `~/.aws/credentials`, replacing the values with the ones you generated.
   - Don't change the region - keep it as `us-east-1`. Since all data transfer is in the same data center, it doesn't matter where you physically reside. If you change this, you will need to update `aws_ec2.yml` and the AMI ID in `cache_acts.yml`, and some services may not be available in other regions.

```
# ~/.aws/credentials
[default]
aws_access_key_id=AWS_ACCESS_KEY_ID
aws_secret_access_key=AWS_SECRET_ACCESS_KEY
region=us-east-1
```

#### Load Wandb API Key automatically
1) Get your Wandb API key here: https://wandb.ai/settings#api
2) Add the following to your `~/.bash_profile` (or your equivalent shell defaults file)
```
export WANDB_API_KEY=[Paste Wandb API Key]
```

#### Install Ansible

```
pip install ansible
ansible --version

cd scripts/ansible
ansible-galaxy collection install -r util/requirements.yml
```

#### Configure a Job to Run
```
cd scripts/ansible
cp -r configs_example configs
```
Modify `configs/shared.yml` and set `s3_bucket_name` to something unique. Bucket names are global so they must be unique (think of them as a username on a platform).

You don't need to modify anything else to run the example job.

Explanation of the config files under `configs_example`:
- `shared.yml` - Shared values for all jobs.
- `cache_acts.yml` - Cache Activations with a `training_steps` of 2000.
- `train_sae` - Contains `sweep_common.yml` which has the name of the sweep, plus all of the common config values between the sweep's jobs. There are two jobs in the `jobs` subdirectory, which defines only the values that are different, which in this case is the `l1_coefficient`.
- It's only 2000 training steps for Cache Activations and 500 training steps for Train SAEs so the jobs themselves in the example are fast - most of the time is spend launching instances, configuring them, etc.

#### Run the Example Job

You should have the [AWS EC2 Console](https://us-east-1.console.aws.amazon.com/ec2/home?region=us-east-1) open so you can watch instances be launched, and terminate them manually in case it has any problems. By default if you exit Ansible prematurely it will not stop your EC2 instance, so you'll keep getting billed for it.

```
cd scripts/ansible
export WANDB_API_KEY=[WANDB API key here]
ansible-playbook run-configs.yml
```

Briefly, this example job will (time estimates are for the example above):
1) Create your S3 bucket and other prerequisites. (~3 minutes)
2) Run the Cache Activations job (~15 minutes)
   1) Launch EC2 instance
   2) Run `util/cache_acts.py`, saving output to your S3 bucket
   3) Terminate the EC2 instance
3) Run the Train SAE jobs in parallel (~15 minutes)
   1) Launch EC2 instance
   2) Run `util/train_sae.py`, loading the cached activations from your S3 bucket.
   3) You can monitor progress of this by going to WANDB, where it should also have your artifacts.


#### Run Cache Acts or Train SAEs Job Separately

Cache Activations only
```
ansible-playbook playbooks/setup.yml
ansible-playbook playbooks/cache_acts.yml
```

Train SAE only
```
ansible-playbook playbooks/setup.yml
ansible-playbook playbooks/train_sae.yml
```

#### Run Instance for Development

This brings up an instance with SAELens on it that has everything configured to run a job, including mounted S3. You will need to shut down the instance yourself when you're done with it.
1) Make sure you've copied the latest `scripts/ansible/configs-example` to `scripts/ansible/config`.
2) Modify `scripts/ansible/config/dev.yml` with the instance type you wish to launch, then save.

```
cd scripts/ansible
ansible-playbook run-dev.yml

# wait for run-dev.yml to complete (~7 minutes)

# get the IP address to ssh into
ansible-inventory --list --yaml tag_service__dev | grep public_ip_address

ssh -i ~/.ssh/saelens_ansible ubuntu@[PASTE_PUBLIC_IP_ADDRESS]
```

Once you're SSH'ed into the instance, the directories are:
```
# s3 mounted bucket directory, which should contain your bucket as its sole subdirectory
cd /mnt/s3
ls

# SAELens git repo - main branch
cd /home/ubuntu/SAELens
```

**Remember to terminate your instance when you're done.**
You can terminate the instance from the EC2 console. Alternatively, the instance has been configured to terminate on shutdown, so from SSH you can just run:
```
sudo shutdown -h now
```
You should still double check that the instance does indeed terminate from EC2 Console, just in case shutdown failed for some reason.

### TODO
   - make config scripts that makes the config sweep files automatically
   - should do async so that canceling ansible doesnt cancel the job
   - document how to monitor running jobs
   - better integration with wandb ("sweep param")
     - should we just use/repurpose wandb stuff instead of manually doing all this?
   - use containers, possilby cloudformation to simplify instance configuration
   - use 'typer' on `cache_acts.py` and `train_sae.py` 
   - ansible "best practices", better use of ansible features
   - don't use 777 permissions
	- AWX server for GUI monitoring jobs
   - Automatically pull the latest AMI using Ansible

================
File: scripts/ansible/util/cache_acts.py
================
import inspect
import os
import sys
import time
import torch
import yaml
from sae_lens.cache_activations_runner import CacheActivationsRunner
from sae_lens.config import DTYPE_MAP, CacheActivationsRunnerConfig
if len(sys.argv) > 1:
    job_name = sys.argv[1]
    print(f"Cache Activations Job Name: {job_name}")
else:
    raise ValueError("Error: One argument required - the Cache Activations Job Name")
if torch.cuda.is_available():
    device = "cuda"
    torch.cuda.empty_cache()
elif torch.backends.mps.is_available():
    device = "mps"
    torch.mps.empty_cache()
else:
    device = "cpu"
print("Using device:", device)
os.environ["TOKENIZERS_PARALLELISM"] = "false"
# load the yaml file as config
# load only the keys that are in CacheActivationsRunnerConfig
# TODO: this is a hacky way of importing
with open(f"./jobs/cache_acts/{job_name}/cache_acts.yml") as file:
    config_yaml = yaml.load(file, Loader=yaml.FullLoader)
    config_params = inspect.signature(CacheActivationsRunnerConfig).parameters
    filtered_data = {k: v for k, v in config_yaml.items() if k in config_params}
    config = CacheActivationsRunnerConfig(**filtered_data)
    if type(config.dtype) != torch.dtype:
        config.dtype = DTYPE_MAP[config.dtype]  # type: ignore
    config.device = device
if config is None:
    raise ValueError("Error: The config is not loaded.")
print(f"Total Training Tokens: {config.training_tokens}")
# This is set by Ansible
new_cached_activations_path = config.new_cached_activations_path
if new_cached_activations_path is None:
    raise ValueError("Error: The new_cached_activations_path is not set.")
# Check directory to make sure we aren't overwriting unintentionally
if os.path.exists(new_cached_activations_path):
    raise ValueError(
        f"Error: The new_cached_activations_path ({new_cached_activations_path}) is not empty."
    )
start_time = time.time()
runner = CacheActivationsRunner(config)
print("-" * 50)
print(runner.__str__())
print("-" * 50)
runner.run()
end_time = time.time()
print(f"Total time taken: {end_time - start_time:.2f} seconds")
print(
    f"{config.training_tokens / ((end_time - start_time)*10**6):.2f} Million Tokens / Second"
)

================
File: scripts/ansible/util/train_sae.py
================
import inspect
import os
import sys
import torch
import yaml
from sae_lens.config import DTYPE_MAP, LanguageModelSAERunnerConfig
from sae_lens.sae_training_runner import SAETrainingRunner
# sys.path.append("..")
if len(sys.argv) > 1:
    job_config_path = sys.argv[1]
    print(f"Train SAE Job config path: {job_config_path}")
else:
    raise ValueError("Error: One argument required - the Train SAE Job Config path")
if torch.cuda.is_available():
    device = "cuda"
    torch.cuda.empty_cache()
elif torch.backends.mps.is_available():
    device = "mps"
    torch.mps.empty_cache()
else:
    device = "cpu"
print("Using device:", device)
os.environ["TOKENIZERS_PARALLELISM"] = "false"
# load the yaml file as config
# load only the keys that are in LanguageModelSAERunnerConfig
# TODO: this is a hacky way of importing
with open(job_config_path) as file:
    config_yaml = yaml.load(file, Loader=yaml.FullLoader)
    config_params = inspect.signature(LanguageModelSAERunnerConfig).parameters
    filtered_data = {k: v for k, v in config_yaml.items() if k in config_params}
    config = LanguageModelSAERunnerConfig(**filtered_data)
    if type(config.dtype) != torch.dtype:
        config.dtype = DTYPE_MAP[config.dtype]  # type: ignore
    config.device = device
if config is None:
    raise ValueError("Error: The config is not loaded.")
print(f"l1_warm_up_steps: {config.l1_warm_up_steps}")
print(f"lr_warm_up_steps: {config.lr_warm_up_steps}")
print(f"lr_decay_steps: {config.lr_decay_steps}")
cached_activations_path = config.cached_activations_path
if cached_activations_path is None:
    raise ValueError("Error: The cached_activations_path is not set.")
sparse_autoencoder_dictionary = SAETrainingRunner(config).run()

================
File: scripts/caching_replication_how_train_saes.py
================
import os
import time
import torch
from sae_lens.cache_activations_runner import CacheActivationsRunner
from sae_lens.config import CacheActivationsRunnerConfig
if torch.cuda.is_available():
    device = "cuda"
elif torch.backends.mps.is_available():
    device = "mps"
else:
    device = "cpu"
print("Using device:", device)
os.environ["TOKENIZERS_PARALLELISM"] = "false"
# change these configs
model_name = "gelu-1l"
model_batch_size = 16
dataset_path = "NeelNanda/c4-tokenized-2b"
total_training_tokens = 100_000
if device == "cuda":
    torch.cuda.empty_cache()
elif device == "mps":
    torch.mps.empty_cache()
cfg = CacheActivationsRunnerConfig(
    # Pick a tiny model to make this easier.
    model_name=model_name,
    dataset_path=dataset_path,
    ## MLP Layer 0 ##
    hook_name="blocks.0.hook_mlp_out",
    hook_layer=0,
    d_in=512,
    prepend_bos=True,
    training_tokens=total_training_tokens,
    model_batch_size=model_batch_size,
    # Misc
    device=device,
    seed=42,
    dtype="float16",
)
# look at the next cell to see some instruction for what to do while this is running.
start_time = time.time()
runner = CacheActivationsRunner(cfg)
print("-" * 50)
print(runner.__str__())
print("-" * 50)
runner.run()
end_time = time.time()
print(f"Total time taken: {end_time - start_time:.2f} seconds")
print(
    f"{cfg.training_tokens / ((end_time - start_time)*10**6):.2f} Million Tokens / Second"
)

================
File: scripts/replication_how_train_saes_control.py
================
import os
import sys
import torch
sys.path.append("..")
from sae_lens.config import LanguageModelSAERunnerConfig
from sae_lens.sae_training_runner import SAETrainingRunner
if torch.cuda.is_available():
    device = "cuda"
elif torch.backends.mps.is_available():
    device = "mps"
else:
    device = "cpu"
print("Using device:", device)
os.environ["TOKENIZERS_PARALLELISM"] = "false"
total_training_steps = 200_000
batch_size = 4096
total_training_tokens = total_training_steps * batch_size
print(f"Total Training Tokens: {total_training_tokens}")
# change these configs
model_name = "gelu-1l"
dataset_path = "NeelNanda/c4-tokenized-2b"
new_cached_activations_path = (
    f"./cached_activations/{model_name}/{dataset_path}/{total_training_steps}"
)
lr_warm_up_steps = total_training_steps // 20
lr_decay_steps = total_training_steps // 5  # 20% of training steps.
print(f"lr_decay_steps: {lr_decay_steps}")
l1_warmup_steps = 0  # total_training_steps // 20  # 5% of training steps.
print(f"l1_warmup_steps: {l1_warmup_steps}")
log_to_wandb = True
for l1_coefficient in [2, 5, 10]:
    cfg = LanguageModelSAERunnerConfig(
        # Pick a tiny model to make this easier.
        model_name="gelu-1l",
        ## MLP Layer 0 ##
        hook_name="blocks.0.hook_mlp_out",
        hook_layer=0,
        d_in=512,
        dataset_path="NeelNanda/c4-tokenized-2b",
        streaming=False,
        context_size=1024,
        is_dataset_tokenized=True,
        prepend_bos=True,
        # How big do we want our SAE to be?
        expansion_factor=64,
        # Dataset / Activation Store
        # When we do a proper test
        # training_tokens= 820_000_000, # 200k steps * 4096 batch size ~ 820M tokens (doable overnight on an A100)
        # For now.
        use_cached_activations=True,
        cached_activations_path="/home/paperspace/shared_volumes/activations_volume_1/gelu-1l",
        training_tokens=total_training_tokens,  # For initial testing I think this is a good number.
        train_batch_size_tokens=4096,
        # Loss Function
        ## Reconstruction Coefficient.
        mse_loss_normalization=None,  # MSE Loss Normalization is not mentioned (so we use stanrd MSE Loss). But not we take an average over the batch.
        ## Anthropic does not mention using an Lp norm other than L1.
        l1_coefficient=l1_coefficient,
        lp_norm=1.0,
        # Instead, they multiply the L1 loss contribution
        # from each feature of the activations by the decoder norm of the corresponding feature.
        scale_sparsity_penalty_by_decoder_norm=False,
        # Learning Rate
        lr_scheduler_name="constant",  # we set this independently of warmup and decay steps.
        l1_warm_up_steps=l1_warmup_steps,
        lr_warm_up_steps=lr_warm_up_steps,
        lr_decay_steps=lr_warm_up_steps,
        ## No ghost grad term.
        use_ghost_grads=False,
        # Initialization / Architecture
        apply_b_dec_to_input=False,
        # encoder bias zero's. (I'm not sure what it is by default now)
        # decoder bias zero's.
        b_dec_init_method="zeros",
        normalize_sae_decoder=True,
        decoder_heuristic_init=False,
        init_encoder_as_decoder_transpose=False,
        # Optimizer
        lr=3e-4,
        ## adam optimizer has no weight decay by default so worry about this.
        adam_beta1=0.9,
        adam_beta2=0.999,
        # Buffer details won't matter in we cache / shuffle our activations ahead of time.
        n_batches_in_buffer=64,
        store_batch_size_prompts=16,
        normalize_activations="none",
        # Feature Store
        feature_sampling_window=1000,
        dead_feature_window=1000,
        dead_feature_threshold=1e-4,
        # WANDB
        log_to_wandb=log_to_wandb,  # always use wandb unless you are just testing code.
        wandb_project="how_we_train_SAEs_replication_1",
        wandb_log_frequency=50,
        eval_every_n_wandb_logs=10,
        # Misc
        device=device,
        seed=42,
        n_checkpoints=0,
        checkpoint_path="checkpoints",
        dtype="float32",
    )
    # look at the next cell to see some instruction for what to do while this is running.
    sae = SAETrainingRunner(cfg).run()
    print("=" * 50)

================
File: scripts/replication_how_train_saes.py
================
import os
import sys
import torch
sys.path.append("..")
from sae_lens.config import LanguageModelSAERunnerConfig
from sae_lens.sae_training_runner import SAETrainingRunner
if torch.cuda.is_available():
    device = "cuda"
elif torch.backends.mps.is_available():
    device = "mps"
else:
    device = "cpu"
print("Using device:", device)
os.environ["TOKENIZERS_PARALLELISM"] = "false"
total_training_steps = 200_000
batch_size = 4096
total_training_tokens = total_training_steps * batch_size
print(f"Total Training Tokens: {total_training_tokens}")
# change these configs
model_name = "gelu-1l"
dataset_path = "NeelNanda/c4-tokenized-2b"
new_cached_activations_path = (
    f"./cached_activations/{model_name}/{dataset_path}/{total_training_steps}"
)
lr_warm_up_steps = 0
lr_decay_steps = total_training_steps // 5  # 20% of training steps.
print(f"lr_decay_steps: {lr_decay_steps}")
l1_warmup_steps = total_training_steps // 20  # 5% of training steps.
print(f"l1_warmup_steps: {l1_warmup_steps}")
log_to_wandb = True
for l1_coefficient in [2, 5, 10]:
    cfg = LanguageModelSAERunnerConfig(
        # Pick a tiny model to make this easier.
        model_name="gelu-1l",
        ## MLP Layer 0 ##
        hook_name="blocks.0.hook_mlp_out",
        hook_layer=0,
        d_in=512,
        dataset_path="NeelNanda/c4-tokenized-2b",
        streaming=False,
        context_size=1024,
        is_dataset_tokenized=True,
        prepend_bos=True,
        # How big do we want our SAE to be?
        expansion_factor=64,
        # Dataset / Activation Store
        # When we do a proper test
        # training_tokens= 820_000_000, # 200k steps * 4096 batch size ~ 820M tokens (doable overnight on an A100)
        # For now.
        use_cached_activations=False,
        # cached_activations_path="/home/paperspace/shared_volumes/activations_volume_1/gelu-1l",
        training_tokens=total_training_tokens,  # For initial testing I think this is a good number.
        train_batch_size_tokens=4096,
        # Loss Function
        ## Reconstruction Coefficient.
        mse_loss_normalization=None,  # MSE Loss Normalization is not mentioned (so we use stanrd MSE Loss). But not we take an average over the batch.
        ## Anthropic does not mention using an Lp norm other than L1.
        l1_coefficient=l1_coefficient,
        lp_norm=1.0,
        # Instead, they multiply the L1 loss contribution
        # from each feature of the activations by the decoder norm of the corresponding feature.
        scale_sparsity_penalty_by_decoder_norm=True,
        # Learning Rate
        lr_scheduler_name="constant",  # we set this independently of warmup and decay steps.
        l1_warm_up_steps=l1_warmup_steps,
        lr_warm_up_steps=lr_warm_up_steps,
        lr_decay_steps=lr_warm_up_steps,
        ## No ghost grad term.
        use_ghost_grads=False,
        # Initialization / Architecture
        apply_b_dec_to_input=False,
        # encoder bias zero's. (I'm not sure what it is by default now)
        # decoder bias zero's.
        b_dec_init_method="zeros",
        normalize_sae_decoder=False,
        decoder_heuristic_init=True,
        init_encoder_as_decoder_transpose=True,
        # Optimizer
        lr=5e-5,
        ## adam optimizer has no weight decay by default so worry about this.
        adam_beta1=0.9,
        adam_beta2=0.999,
        # Buffer details won't matter in we cache / shuffle our activations ahead of time.
        n_batches_in_buffer=64,
        store_batch_size_prompts=16,
        normalize_activations="none",
        # Feature Store
        feature_sampling_window=1000,
        dead_feature_window=1000,
        dead_feature_threshold=1e-4,
        # WANDB
        log_to_wandb=log_to_wandb,  # always use wandb unless you are just testing code.
        wandb_project="how_we_train_SAEs_replication_1",
        wandb_log_frequency=50,
        eval_every_n_wandb_logs=10,
        # Misc
        device=device,
        seed=42,
        n_checkpoints=0,
        checkpoint_path="checkpoints",
        dtype="float32",
    )
    # look at the next cell to see some instruction for what to do while this is running.
    sae = SAETrainingRunner(cfg).run()
    print("=" * 50)

================
File: scripts/sweep-gpt2-blocks.py
================
import os
import sys
import torch
sys.path.append("..")
from sae_lens.config import LanguageModelSAERunnerConfig
from sae_lens.sae_training_runner import SAETrainingRunner
if torch.cuda.is_available():
    device = "cuda"
elif torch.backends.mps.is_available():
    device = "mps"
else:
    device = "cpu"
print("Using device:", device)
os.environ["TOKENIZERS_PARALLELISM"] = "false"
total_training_steps = 20_000
batch_size = 4096
total_training_tokens = total_training_steps * batch_size
print(f"Total Training Tokens: {total_training_tokens}")
lr_warm_up_steps = 0
lr_decay_steps = total_training_steps // 5  # 20% of training steps.
print(f"lr_decay_steps: {lr_decay_steps}")
l1_warmup_steps = total_training_steps // 20  # 5% of training steps.
print(f"l1_warmup_steps: {l1_warmup_steps}")
log_to_wandb = True
for l1_coefficient in [3, 4, 5, 6, 7]:
    for block in [1, 3, 5, 6]:
        cfg = LanguageModelSAERunnerConfig(
            # Pick a tiny model to make this easier.
            model_name="gpt2",
            ## MLP ##
            hook_name=f"blocks.{block}.hook_mlp_out",
            hook_layer=block,
            d_in=768,
            dataset_path="apollo-research/Skylion007-openwebtext-tokenizer-gpt2",
            streaming=True,
            context_size=512,
            is_dataset_tokenized=True,
            prepend_bos=True,
            # How big do we want our SAE to be?
            expansion_factor=64,
            # Dataset / Activation Store
            use_cached_activations=False,
            training_tokens=total_training_tokens,
            train_batch_size_tokens=4096,
            # Loss Function
            ## Reconstruction Coefficient.
            mse_loss_normalization=None,  # MSE Loss Normalization is not mentioned (so we use stanrd MSE Loss). But not we take an average over the batch.
            ## Anthropic does not mention using an Lp norm other than L1.
            l1_coefficient=l1_coefficient,
            lp_norm=1.0,
            # Instead, they multiply the L1 loss contribution
            # from each feature of the activations by the decoder norm of the corresponding feature.
            scale_sparsity_penalty_by_decoder_norm=True,
            # Learning Rate
            lr_scheduler_name="constant",  # we set this independently of warmup and decay steps.
            l1_warm_up_steps=l1_warmup_steps,
            lr_warm_up_steps=lr_warm_up_steps,
            lr_decay_steps=lr_warm_up_steps,
            ## No ghost grad term.
            use_ghost_grads=False,
            # Initialization / Architecture
            apply_b_dec_to_input=False,
            # encoder bias zero's. (I'm not sure what it is by default now)
            # decoder bias zero's.
            b_dec_init_method="zeros",
            normalize_sae_decoder=False,
            decoder_heuristic_init=True,
            init_encoder_as_decoder_transpose=True,
            # Optimizer
            lr=1e-4,
            ## adam optimizer has no weight decay by default so worry about this.
            adam_beta1=0.9,
            adam_beta2=0.999,
            # Unsure if this is enough
            n_batches_in_buffer=64,
            store_batch_size_prompts=32,
            normalize_activations="none",
            # Feature Store
            feature_sampling_window=1000,
            dead_feature_window=1000,
            dead_feature_threshold=1e-4,
            # WANDB
            log_to_wandb=log_to_wandb,
            wandb_project="gpt-2-sweep-15may24-try-normalisation",
            wandb_log_frequency=50,
            eval_every_n_wandb_logs=10,
            # Misc
            device=device,
            seed=42,
            n_checkpoints=0,
            checkpoint_path="checkpoints",
            dtype="float32",
            eval_batch_size_prompts=2,
            n_eval_batches=40,
            autocast=True,
            compile_llm=True,
            compile_sae=True,
        )
        SAETrainingRunner(cfg).run()
        print("=" * 50)

================
File: scripts/sweep-gpt2.py
================
import os
import sys
import torch
sys.path.append("..")
from sae_lens.config import LanguageModelSAERunnerConfig
from sae_lens.sae_training_runner import SAETrainingRunner
if torch.cuda.is_available():
    device = "cuda"
elif torch.backends.mps.is_available():
    device = "mps"
else:
    device = "cpu"
print("Using device:", device)
os.environ["TOKENIZERS_PARALLELISM"] = "false"
block = 6
total_training_steps = 20_000
batch_size = 4096
total_training_tokens = total_training_steps * batch_size
print(f"Total Training Tokens: {total_training_tokens}")
lr_warm_up_steps = 0
lr_decay_steps = total_training_steps // 5  # 20% of training steps.
print(f"lr_decay_steps: {lr_decay_steps}")
l1_warmup_steps = total_training_steps // 20  # 5% of training steps.
print(f"l1_warmup_steps: {l1_warmup_steps}")
log_to_wandb = True
for l1_coefficient in [0.1, 1, 2, 4, 10]:
    for lr in [
        1e-5,
        5e-5,
        1e-4,
        4e-4,
    ]:
        cfg = LanguageModelSAERunnerConfig(
            # Pick a tiny model to make this easier.
            model_name="gpt2",
            ## MLP ##
            hook_name=f"blocks.{block}.hook_mlp_out",
            hook_layer=block,
            d_in=768,
            dataset_path="apollo-research/Skylion007-openwebtext-tokenizer-gpt2",
            streaming=True,
            context_size=512,
            is_dataset_tokenized=True,
            prepend_bos=True,
            # How big do we want our SAE to be?
            expansion_factor=64,
            # Dataset / Activation Store
            use_cached_activations=False,
            training_tokens=total_training_tokens,
            train_batch_size_tokens=4096,
            # Loss Function
            ## Reconstruction Coefficient.
            mse_loss_normalization=None,  # MSE Loss Normalization is not mentioned (so we use stanrd MSE Loss). But not we take an average over the batch.
            ## Anthropic does not mention using an Lp norm other than L1.
            l1_coefficient=l1_coefficient,
            lp_norm=1.0,
            # Instead, they multiply the L1 loss contribution
            # from each feature of the activations by the decoder norm of the corresponding feature.
            scale_sparsity_penalty_by_decoder_norm=True,
            # Learning Rate
            lr_scheduler_name="constant",  # we set this independently of warmup and decay steps.
            l1_warm_up_steps=l1_warmup_steps,
            lr_warm_up_steps=lr_warm_up_steps,
            lr_decay_steps=lr_warm_up_steps,
            ## No ghost grad term.
            use_ghost_grads=False,
            # Initialization / Architecture
            apply_b_dec_to_input=False,
            # encoder bias zero's. (I'm not sure what it is by default now)
            # decoder bias zero's.
            b_dec_init_method="zeros",
            normalize_sae_decoder=False,
            decoder_heuristic_init=True,
            init_encoder_as_decoder_transpose=True,
            # Optimizer
            lr=lr,
            ## adam optimizer has no weight decay by default so worry about this.
            adam_beta1=0.9,
            adam_beta2=0.999,
            # Unsure if this is enough
            n_batches_in_buffer=64,
            store_batch_size_prompts=16,
            normalize_activations="none",
            # Feature Store
            feature_sampling_window=1000,
            dead_feature_window=1000,
            dead_feature_threshold=1e-4,
            # WANDB
            log_to_wandb=log_to_wandb,
            wandb_project="gpt-2-sweep-14may24",
            wandb_log_frequency=50,
            eval_every_n_wandb_logs=10,
            # Misc
            device=device,
            seed=42,
            n_checkpoints=0,
            checkpoint_path="checkpoints",
            dtype="float32",
            eval_batch_size_prompts=2,
            autocast=True,
            compile_llm=True,
            compile_sae=True,
        )
        SAETrainingRunner(cfg).run()
        print("=" * 50)

================
File: scripts/test-autocast-lm.py
================
import os
import sys
import torch
sys.path.append("..")
from sae_lens.config import LanguageModelSAERunnerConfig
from sae_lens.sae_training_runner import SAETrainingRunner
if torch.cuda.is_available():
    device = "cuda"
elif torch.backends.mps.is_available():
    device = "mps"
else:
    device = "cpu"
print("Using device:", device)
os.environ["TOKENIZERS_PARALLELISM"] = "false"
total_training_steps = 2000
batch_size = 4096
total_training_tokens = total_training_steps * batch_size
print(f"Total Training Tokens: {total_training_tokens}")
lr_warm_up_steps = 0
lr_decay_steps = total_training_steps // 5  # 20% of training steps.
print(f"lr_decay_steps: {lr_decay_steps}")
l1_warmup_steps = total_training_steps // 20  # 5% of training steps.
print(f"l1_warmup_steps: {l1_warmup_steps}")
log_to_wandb = True
l1_coefficient = 5
for block in [11, 0]:
    for autocast_lm in [False, True]:
        cfg = LanguageModelSAERunnerConfig(
            # Pick a tiny model to make this easier.
            model_name="gpt2",
            ## MLP ##
            hook_name=f"blocks.{block}.hook_mlp_out",
            hook_layer=block,
            d_in=768,
            dataset_path="apollo-research/Skylion007-openwebtext-tokenizer-gpt2",
            streaming=True,
            context_size=512,
            is_dataset_tokenized=True,
            prepend_bos=True,
            # How big do we want our SAE to be?
            expansion_factor=64,
            # Dataset / Activation Store
            use_cached_activations=False,
            training_tokens=total_training_tokens,
            train_batch_size_tokens=4096,
            # Loss Function
            ## Reconstruction Coefficient.
            mse_loss_normalization=None,  # MSE Loss Normalization is not mentioned (so we use stanrd MSE Loss). But not we take an average over the batch.
            ## Anthropic does not mention using an Lp norm other than L1.
            l1_coefficient=l1_coefficient,
            lp_norm=1.0,
            # Instead, they multiply the L1 loss contribution
            # from each feature of the activations by the decoder norm of the corresponding feature.
            scale_sparsity_penalty_by_decoder_norm=True,
            # Learning Rate
            lr_scheduler_name="constant",  # we set this independently of warmup and decay steps.
            l1_warm_up_steps=l1_warmup_steps,
            lr_warm_up_steps=lr_warm_up_steps,
            lr_decay_steps=lr_warm_up_steps,
            ## No ghost grad term.
            use_ghost_grads=False,
            # Initialization / Architecture
            apply_b_dec_to_input=False,
            # encoder bias zero's. (I'm not sure what it is by default now)
            # decoder bias zero's.
            b_dec_init_method="zeros",
            normalize_sae_decoder=False,
            decoder_heuristic_init=True,
            init_encoder_as_decoder_transpose=True,
            # Optimizer
            lr=1e-4,
            ## adam optimizer has no weight decay by default so worry about this.
            adam_beta1=0.9,
            adam_beta2=0.999,
            # Unsure if this is enough
            n_batches_in_buffer=64,
            store_batch_size_prompts=16,
            normalize_activations="expected_average_only_in",
            # Feature Store
            feature_sampling_window=1000,
            dead_feature_window=1000,
            dead_feature_threshold=1e-4,
            # WANDB
            log_to_wandb=log_to_wandb,
            wandb_project="gpt-2-sweep-20may24-check-autocast-2",
            wandb_log_frequency=50,
            eval_every_n_wandb_logs=10,
            # Misc
            device=device,
            seed=42,
            n_checkpoints=0,
            checkpoint_path="checkpoints",
            dtype="float32",
            eval_batch_size_prompts=2,
            n_eval_batches=40,
            autocast=True,
            autocast_lm=autocast_lm,
            compile_llm=True,
            compile_sae=True,
        )
        SAETrainingRunner(cfg).run()
        print("=" * 50)

================
File: scripts/training_a_sparse_autoencoder_othelloGPT.py
================
import os
import torch
from sae_lens import (
    SAE,
    HookedSAETransformer,
    LanguageModelSAERunnerConfig,
    SAETrainingRunner,
    upload_saes_to_huggingface,
)
if torch.cuda.is_available():
    device = "cuda"
elif torch.backends.mps.is_available():
    device = "mps"
else:
    device = "cpu"
print("Using device:", device)
os.environ["TOKENIZERS_PARALLELISM"] = "false"
model_name = "othello-gpt"
model = HookedSAETransformer.from_pretrained(model_name)
dataset_path = "taufeeque/othellogpt"
context_size = 59
layer = 5
training_tokens = int(1e3)
train_batch_size_tokens = 2048
n_steps = int(training_tokens / train_batch_size_tokens)
print(LanguageModelSAERunnerConfig())
runner_cfg = LanguageModelSAERunnerConfig(
    #
    # Data generation
    model_name=model_name,
    hook_name=f"blocks.{layer}.mlp.hook_post",
    hook_layer=layer,
    d_in=model.cfg.d_mlp,
    dataset_path=dataset_path,
    is_dataset_tokenized=True,
    prepend_bos=False,
    streaming=True,
    train_batch_size_tokens=train_batch_size_tokens,
    context_size=context_size,
    seqpos_slice=(5, -5),
    #
    # SAE achitecture
    architecture="gated",
    expansion_factor=8,
    b_dec_init_method="zeros",
    apply_b_dec_to_input=True,
    normalize_sae_decoder=False,
    scale_sparsity_penalty_by_decoder_norm=True,
    decoder_heuristic_init=True,
    init_encoder_as_decoder_transpose=True,
    #
    # Activations store
    n_batches_in_buffer=32,
    store_batch_size_prompts=16,
    training_tokens=training_tokens,
    #
    # Training hyperparameters (standard)
    lr=2e-4,
    adam_beta1=0.9,
    adam_beta2=0.999,
    lr_scheduler_name="constant",
    lr_warm_up_steps=int(0.2 * n_steps),
    lr_decay_steps=int(0.2 * n_steps),
    #
    # Training hyperparameters (SAE-specific)
    l1_coefficient=5,
    l1_warm_up_steps=int(0.2 * n_steps),
    use_ghost_grads=False,
    feature_sampling_window=1000,
    dead_feature_window=500,
    dead_feature_threshold=1e-5,
    #
    # Logging / evals
    log_to_wandb=True,
    wandb_project=f"othello_gpt_sae_{layer=}",
    wandb_log_frequency=30,
    eval_every_n_wandb_logs=10,
    checkpoint_path="checkpoints",
    #
    # Misc.
    device=str(device),
    seed=42,
    n_checkpoints=5,
    dtype="float32",
)
# t.set_grad_enabled(True)
runner = SAETrainingRunner(runner_cfg)
sae = runner.run()
hf_repo_id = "callummcdougall/arena-demos-othellogpt"
sae_id = "blocks.5.mlp.hook_post-v1"
upload_saes_to_huggingface({sae_id: sae}, hf_repo_id=hf_repo_id)
othellogpt_sae = SAE.from_pretrained(
    release=hf_repo_id, sae_id=sae_id, device=str(device)
)[0]

================
File: tests/analysis/test_hooked_sae_transformer.py
================
# type: ignore
from typing import Tuple, Union
import pytest
import torch
from transformer_lens import HookedTransformer
from transformer_lens.ActivationCache import ActivationCache
from transformer_lens.hook_points import HookPoint  # Hooking utilities
from transformer_lens.HookedTransformer import Loss
from sae_lens.analysis.hooked_sae_transformer import HookedSAETransformer, get_deep_attr
from sae_lens.sae import SAE, SAEConfig
MODEL = "solu-1l"
prompt = "Hello World!"
Output = Union[torch.Tensor, Tuple[torch.Tensor, Loss], None]
def get_logits(output: Output) -> torch.Tensor:
    if output is None:
        raise ValueError("Model output is None")
    if isinstance(output, torch.Tensor):
        return output
    if isinstance(output, tuple) and len(output) == 2:
        return output[0]
    raise ValueError(f"Unexpected output type: {type(output)}")
class Counter:
    def __init__(self):
        self.count = 0
    def inc(self, *args, **kwargs):  # type: ignore
        self.count += 1
@pytest.fixture(scope="module")
def model():
    model = HookedSAETransformer.from_pretrained(MODEL, device="cpu")
    yield model
    model.reset_saes()  # type: ignore
@pytest.fixture(scope="module")
def original_logits(model: HookedTransformer):
    return model(prompt)
def get_hooked_sae(model: HookedTransformer, act_name: str) -> SAE:
    site_to_size = {
        "hook_z": model.cfg.d_head * model.cfg.n_heads,
        "hook_mlp_out": model.cfg.d_model,
        "hook_resid_pre": model.cfg.d_model,
        "hook_post": model.cfg.d_mlp,
    }
    site = act_name.split(".")[-1]
    d_in = site_to_size[site]
    sae_cfg = SAEConfig(
        architecture="standard",
        d_in=d_in,
        d_sae=d_in * 2,
        dtype="float32",
        device="cpu",
        model_name=MODEL,
        hook_name=act_name,
        hook_layer=0,
        hook_head_index=None,
        activation_fn_str="relu",
        prepend_bos=True,
        context_size=128,
        dataset_path="test",
        dataset_trust_remote_code=True,
        apply_b_dec_to_input=False,
        finetuning_scaling_factor=False,
        sae_lens_training_version=None,
        normalize_activations="none",
    )
    return SAE(sae_cfg)  # type: ignore
@pytest.fixture(
    scope="module",
    params=[
        "blocks.0.attn.hook_z",
        "blocks.0.hook_mlp_out",
        "blocks.0.mlp.hook_post",
        "blocks.0.hook_resid_pre",
    ],
    ids=[
        "blocks.0.attn.hook_z",
        "blocks.0.hook_mlp_out",
        "blocks.0.mlp.hook_post",
        "blocks.0.hook_resid_pre",
    ],
)
def hooked_sae(
    model: HookedTransformer,
    request: pytest.FixtureRequest,
) -> SAE:
    return get_hooked_sae(model, request.param)
@pytest.fixture(scope="module")
def list_of_hooked_saes(
    model: HookedTransformer,
):
    act_names = [
        "blocks.0.attn.hook_z",
        "blocks.0.hook_mlp_out",
        "blocks.0.mlp.hook_post",
        "blocks.0.hook_resid_pre",
    ]
    return [get_hooked_sae(model, act_name) for act_name in act_names]
def test_model_with_no_saes_matches_original_model(
    model: HookedTransformer, original_logits: torch.Tensor
):
    """Verifies that HookedSAETransformer behaves like a normal HookedTransformer model when no SAEs are attached."""
    assert len(model.acts_to_saes) == 0  # type: ignore
    logits = model(prompt)
    assert torch.allclose(original_logits, logits)
def test_model_with_saes_does_not_match_original_model(
    model: HookedTransformer,
    hooked_sae: SAE,
    original_logits: torch.Tensor,
):
    """Verifies that the attached (and turned on) SAEs actually affect the models output logits"""
    assert len(model.acts_to_saes) == 0  # type: ignore
    model.add_sae(hooked_sae)  # type: ignore
    assert len(model.acts_to_saes) == 1  # type: ignore
    logits_with_saes = model(prompt)
    assert not torch.allclose(original_logits, logits_with_saes)
    model.reset_saes()
def test_add_sae(model: HookedTransformer, hooked_sae: SAE):
    """Verifies that add_sae correctly updates the model's acts_to_saes dictionary and replaces the HookPoint."""
    act_name = hooked_sae.cfg.hook_name
    model.add_sae(hooked_sae)  # type: ignore
    assert len(model.acts_to_saes) == 1  # type: ignore
    assert model.acts_to_saes[act_name] == hooked_sae
    assert get_deep_attr(model, act_name) == hooked_sae
    model.reset_saes()
def test_add_sae_overwrites_prev_sae(model: HookedTransformer, hooked_sae: SAE):
    """Verifies that add_sae correctly updates the model's acts_to_saes dictionary and replaces the HookPoint."""
    act_name = hooked_sae.cfg.hook_name
    model.add_sae(hooked_sae)
    assert len(model.acts_to_saes) == 1
    assert model.acts_to_saes[act_name] == hooked_sae
    assert get_deep_attr(model, act_name) == hooked_sae
    second_hooked_sae = SAE.from_dict(hooked_sae.cfg.to_dict())  # type: ignore
    model.add_sae(second_hooked_sae)
    assert len(model.acts_to_saes) == 1
    assert model.acts_to_saes[act_name] == second_hooked_sae
    assert get_deep_attr(model, act_name) == second_hooked_sae
    model.reset_saes()
def test_reset_sae_removes_sae_by_default(model: HookedTransformer, hooked_sae: SAE):
    """Verifies that reset_sae correctly removes the SAE from the model's acts_to_saes dictionary and replaces the HookedSAE with a HookPoint."""
    act_name = hooked_sae.cfg.hook_name
    model.add_sae(hooked_sae)
    assert len(model.acts_to_saes) == 1
    assert model.acts_to_saes[act_name] == hooked_sae
    assert get_deep_attr(model, act_name) == hooked_sae
    model._reset_sae(act_name)
    assert len(model.acts_to_saes) == 0
    assert isinstance(get_deep_attr(model, act_name), HookPoint)
    model.reset_saes()
def test_reset_sae_replaces_sae(model: HookedTransformer, hooked_sae: SAE):
    """Verifies that reset_sae correctly removes the SAE from the model's acts_to_saes dictionary and replaces the HookedSAE with a HookPoint."""
    act_name = hooked_sae.cfg.hook_name
    second_hooked_sae = SAE.from_dict(hooked_sae.cfg.to_dict())  # type: ignore
    model.add_sae(hooked_sae)
    assert len(model.acts_to_saes) == 1
    assert model.acts_to_saes[act_name] == hooked_sae
    assert get_deep_attr(model, act_name) == hooked_sae
    model._reset_sae(act_name, second_hooked_sae)
    assert len(model.acts_to_saes) == 1
    assert get_deep_attr(model, act_name) == second_hooked_sae
    model.reset_saes()
def test_reset_saes_removes_all_saes_by_default(
    model: HookedTransformer, list_of_hooked_saes: list[SAE]
):
    """Verifies that reset_saes correctly removes all SAEs from the model's acts_to_saes dictionary and replaces the HookedSAEs with HookPoints."""
    act_names = [hooked_sae.cfg.hook_name for hooked_sae in list_of_hooked_saes]
    for hooked_sae in list_of_hooked_saes:
        model.add_sae(hooked_sae)
    assert len(model.acts_to_saes) == len(act_names)
    for act_name, hooked_sae in zip(act_names, list_of_hooked_saes):
        assert model.acts_to_saes[act_name] == hooked_sae
        assert get_deep_attr(model, act_name) == hooked_sae
    model.reset_saes()
    assert len(model.acts_to_saes) == 0
    for act_name in act_names:
        assert isinstance(get_deep_attr(model, act_name), HookPoint)
    model.reset_saes()
def test_reset_saes_replaces_saes(
    model: HookedTransformer, list_of_hooked_saes: list[SAE]
):
    """Verifies that reset_saes correctly removes all SAEs from the model's acts_to_saes dictionary and replaces the HookedSAEs with HookPoints."""
    act_names = [hooked_sae.cfg.hook_name for hooked_sae in list_of_hooked_saes]
    for hooked_sae in list_of_hooked_saes:
        model.add_sae(hooked_sae)
    prev_hooked_saes = [get_hooked_sae(model, act_name) for act_name in act_names]
    assert len(model.acts_to_saes) == len(act_names)
    for act_name, hooked_sae in zip(act_names, list_of_hooked_saes):
        assert model.acts_to_saes[act_name] == hooked_sae
        assert get_deep_attr(model, act_name) == hooked_sae
    model.reset_saes(act_names, prev_hooked_saes)
    assert len(model.acts_to_saes) == len(prev_hooked_saes)
    for act_name, prev_hooked_sae in zip(act_names, prev_hooked_saes):
        assert get_deep_attr(model, act_name) == prev_hooked_sae
    model.reset_saes()
def test_saes_context_manager_removes_saes_after(
    model: HookedTransformer, list_of_hooked_saes: list[SAE]
):
    """Verifies that the model.saes context manager successfully adds the SAEs for the specified activation name in the context manager and resets off after the context manager exits."""
    act_names = [hooked_sae.cfg.hook_name for hooked_sae in list_of_hooked_saes]
    assert len(model.acts_to_saes) == 0
    for act_name in act_names:
        assert isinstance(get_deep_attr(model, act_name), HookPoint)
    with model.saes(saes=list_of_hooked_saes):
        for act_name, hooked_sae in zip(act_names, list_of_hooked_saes):
            assert model.acts_to_saes[act_name] == hooked_sae
            assert isinstance(get_deep_attr(model, act_name), SAE)
            assert get_deep_attr(model, act_name) == hooked_sae
        model.forward(prompt)  # type: ignore
    assert len(model.acts_to_saes) == 0
    for act_name in act_names:
        assert isinstance(get_deep_attr(model, act_name), HookPoint)
    model.reset_saes()
def test_saes_context_manager_restores_previous_sae_state(
    model: HookedTransformer, list_of_hooked_saes: list[SAE]
):
    """Verifies that the model.saes context manager successfully adds the SAEs for the specified activation name in the context manager and resets off after the context manager exits."""
    act_names = [hooked_sae.cfg.hook_name for hooked_sae in list_of_hooked_saes]
    # First add SAEs statefully
    prev_hooked_saes = list_of_hooked_saes
    for act_name, prev_hooked_sae in zip(act_names, prev_hooked_saes):
        model.add_sae(prev_hooked_sae)
        assert get_deep_attr(model, act_name) == prev_hooked_sae
    assert len(model.acts_to_saes) == len(prev_hooked_saes)
    # Now temporarily run with new SAEs
    hooked_saes = [get_hooked_sae(model, act_name) for act_name in act_names]
    with model.saes(saes=hooked_saes):
        for act_name, hooked_sae in zip(act_names, hooked_saes):
            assert model.acts_to_saes[act_name] == hooked_sae
            assert isinstance(get_deep_attr(model, act_name), SAE)
            assert get_deep_attr(model, act_name) == hooked_sae
        model.forward(prompt)  # type: ignore
    # Check that the previously attached SAEs have been restored
    assert len(model.acts_to_saes) == len(prev_hooked_saes)
    for act_name, prev_hooked_sae in zip(act_names, prev_hooked_saes):
        assert isinstance(get_deep_attr(model, act_name), SAE)
        assert get_deep_attr(model, act_name) == prev_hooked_sae
    model.reset_saes()
def test_saes_context_manager_run_with_cache(
    model: HookedTransformer, list_of_hooked_saes: list[SAE]
):
    """Verifies that the model.run_with_cache method works correctly in the context manager."""
    act_names = [hooked_sae.cfg.hook_name for hooked_sae in list_of_hooked_saes]
    assert len(model.acts_to_saes) == 0
    for act_name in act_names:
        assert isinstance(get_deep_attr(model, act_name), HookPoint)
    with model.saes(saes=list_of_hooked_saes):
        for act_name, hooked_sae in zip(act_names, list_of_hooked_saes):
            assert model.acts_to_saes[act_name] == hooked_sae
            assert isinstance(get_deep_attr(model, act_name), SAE)
            assert get_deep_attr(model, act_name) == hooked_sae
        model.run_with_cache(prompt)
    assert len(model.acts_to_saes) == 0
    for act_name in act_names:
        assert isinstance(get_deep_attr(model, act_name), HookPoint)
    model.reset_saes()
def test_run_with_saes(
    model: HookedTransformer,
    list_of_hooked_saes: list[SAE],
    original_logits: torch.Tensor,
):
    """Verifies that the model.run_with_saes method works correctly. The logits with SAEs should be different from the original logits, but the SAE should be removed immediately after the forward pass."""
    act_names = [hooked_sae.cfg.hook_name for hooked_sae in list_of_hooked_saes]
    assert len(model.acts_to_saes) == 0
    logits_with_saes = model.run_with_saes(prompt, saes=list_of_hooked_saes)
    assert not torch.allclose(logits_with_saes, original_logits)
    assert len(model.acts_to_saes) == 0
    for act_name in act_names:
        assert isinstance(get_deep_attr(model, act_name), HookPoint)
    model.reset_saes()
def test_run_with_cache(
    model: HookedTransformer,
    list_of_hooked_saes: list[SAE],
    original_logits: torch.Tensor,
):
    """Verifies that the model.run_with_cache method works correctly. The logits with SAEs should be different from the original logits and the cache should contain SAE activations for the attached SAE."""
    act_names = [hooked_sae.cfg.hook_name for hooked_sae in list_of_hooked_saes]
    for hooked_sae in list_of_hooked_saes:
        model.add_sae(hooked_sae)
    assert len(model.acts_to_saes) == len(list_of_hooked_saes)
    logits_with_saes, cache = model.run_with_cache(prompt)
    assert not torch.allclose(logits_with_saes, original_logits)  # type: ignore
    assert isinstance(cache, ActivationCache)
    for act_name, hooked_sae in zip(act_names, list_of_hooked_saes):
        assert act_name + ".hook_sae_acts_post" in cache
        assert isinstance(get_deep_attr(model, act_name), SAE)
        assert get_deep_attr(model, act_name) == hooked_sae
    model.reset_saes()
def test_run_with_cache_with_saes(
    model: HookedTransformer,
    list_of_hooked_saes: list[SAE],
    original_logits: torch.Tensor,
):
    """Verifies that the model.run_with_cache_with_saes method works correctly. The logits with SAEs should be different from the original logits and the cache should contain SAE activations for the attached SAE."""
    act_names = [hooked_sae.cfg.hook_name for hooked_sae in list_of_hooked_saes]
    logits_with_saes, cache = model.run_with_cache_with_saes(
        prompt, saes=list_of_hooked_saes
    )
    assert not torch.allclose(logits_with_saes, original_logits)
    assert isinstance(cache, ActivationCache)
    assert len(model.acts_to_saes) == 0
    for act_name, _ in zip(act_names, list_of_hooked_saes):
        assert act_name + ".hook_sae_acts_post" in cache
        assert isinstance(get_deep_attr(model, act_name), HookPoint)
    model.reset_saes()
def test_run_with_hooks(
    model: HookedTransformer,
    list_of_hooked_saes: list[SAE],
    original_logits: torch.Tensor,
):
    """Verifies that the model.run_with_hooks method works correctly when SAEs are attached. The count should be incremented by 1 when the hooked SAE is called, and the SAE should stay attached after the forward pass"""
    act_names = [hooked_sae.cfg.hook_name for hooked_sae in list_of_hooked_saes]
    c = Counter()
    for hooked_sae in list_of_hooked_saes:
        model.add_sae(hooked_sae)
    logits_with_saes = model.run_with_hooks(
        prompt,
        fwd_hooks=[(act_name + ".hook_sae_acts_post", c.inc) for act_name in act_names],
    )
    assert not torch.allclose(logits_with_saes, original_logits)
    for act_name, hooked_sae in zip(act_names, list_of_hooked_saes):
        assert isinstance(get_deep_attr(model, act_name), SAE)
        assert get_deep_attr(model, act_name) == hooked_sae
    assert c.count == len(act_names)
    model.reset_saes()
    model.remove_all_hook_fns(including_permanent=True)
def test_run_with_hooks_with_saes(
    model: HookedTransformer,
    list_of_hooked_saes: list[SAE],
    original_logits: torch.Tensor,
):
    """Verifies that the model.run_with_hooks_with_saes method works correctly when SAEs are attached. The count should be incremented by 1 when the hooked SAE is called, but the SAE should be removed immediately after the forward pass."""
    act_names = [hooked_sae.cfg.hook_name for hooked_sae in list_of_hooked_saes]
    c = Counter()
    logits_with_saes = model.run_with_hooks_with_saes(
        prompt,
        saes=list_of_hooked_saes,
        fwd_hooks=[(act_name + ".hook_sae_acts_post", c.inc) for act_name in act_names],
    )
    assert not torch.allclose(logits_with_saes, original_logits)
    assert c.count == len(act_names)
    assert len(model.acts_to_saes) == 0
    for act_name in act_names:
        assert isinstance(get_deep_attr(model, act_name), HookPoint)
    model.reset_saes()
    model.remove_all_hook_fns(including_permanent=True)
def test_model_with_use_error_term_saes_matches_original_model(
    model: HookedTransformer,
    hooked_sae: SAE,
    original_logits: torch.Tensor,
):
    """Verifies that the attached (and turned on) SAEs actually affect the models output logits"""
    assert len(model.acts_to_saes) == 0
    model.add_sae(hooked_sae, use_error_term=True)
    assert len(model.acts_to_saes) == 1
    logits_with_saes = model(prompt)
    model.reset_saes()
    assert torch.allclose(original_logits, logits_with_saes, atol=1e-4)
def test_add_sae_with_use_error_term(model: HookedSAETransformer, hooked_sae: SAE):
    """Verifies that add_sae correctly sets the use_error_term when specified."""
    act_name = hooked_sae.cfg.hook_name
    original_use_error_term = hooked_sae.use_error_term
    model.add_sae(hooked_sae, use_error_term=True)
    assert model.acts_to_saes[act_name].use_error_term is True
    model.add_sae(hooked_sae, use_error_term=False)
    assert model.acts_to_saes[act_name].use_error_term is False
    model.add_sae(hooked_sae, use_error_term=None)
    assert model.acts_to_saes[act_name].use_error_term == original_use_error_term
    model.reset_saes()
def test_saes_context_manager_with_use_error_term(
    model: HookedSAETransformer, hooked_sae: SAE
):
    """Verifies that the saes context manager correctly handles use_error_term."""
    act_name = hooked_sae.cfg.hook_name
    original_use_error_term = hooked_sae.use_error_term
    with model.saes(saes=[hooked_sae], use_error_term=True):
        assert model.acts_to_saes[act_name].use_error_term is True
    assert hooked_sae.use_error_term == original_use_error_term
    assert len(model.acts_to_saes) == 0
def test_run_with_saes_with_use_error_term(
    model: HookedSAETransformer,
    hooked_sae: SAE,
):
    """Verifies that run_with_saes correctly handles use_error_term."""
    original_use_error_term = hooked_sae.use_error_term
    model.run_with_saes(prompt, saes=[hooked_sae], use_error_term=True)
    assert hooked_sae.use_error_term == original_use_error_term
    assert len(model.acts_to_saes) == 0
def test_run_with_cache_with_saes_with_use_error_term(
    model: HookedSAETransformer,
    hooked_sae: SAE,
):
    """Verifies that run_with_cache_with_saes correctly handles use_error_term."""
    act_name = hooked_sae.cfg.hook_name
    original_use_error_term = hooked_sae.use_error_term
    _, cache = model.run_with_cache_with_saes(
        prompt, saes=[hooked_sae], use_error_term=True
    )
    assert hooked_sae.use_error_term == original_use_error_term
    assert len(model.acts_to_saes) == 0
    assert act_name + ".hook_sae_acts_post" in cache
def test_use_error_term_restoration_after_exception(
    model: HookedSAETransformer,
    hooked_sae: SAE,
):
    """Verifies that use_error_term is restored even if an exception occurs."""
    original_use_error_term = hooked_sae.use_error_term
    try:
        with model.saes(saes=[hooked_sae], use_error_term=True):
            raise Exception("Test exception")
    except Exception:
        pass
    assert hooked_sae.use_error_term == original_use_error_term
    assert len(model.acts_to_saes) == 0
def test_add_sae_with_use_error_term_true(
    model: HookedSAETransformer,
    hooked_sae: SAE,
):
    """Verifies that add_sae with use_error_term=True doesn't change the model output."""
    # Get output without SAE
    output_without_sae = get_logits(model(prompt))
    # Add SAE with use_error_term=True
    model.add_sae(hooked_sae, use_error_term=True)
    output_with_sae = get_logits(model(prompt))
    # Compare outputs
    assert torch.allclose(output_without_sae, output_with_sae, atol=1e-4)
    # Clean up
    model.reset_saes()
def test_run_with_saes_use_error_term_true(
    model: HookedSAETransformer,
    hooked_sae: SAE,
):
    """Verifies that run_with_saes with use_error_term=True doesn't change the model output."""
    # Get output without SAE
    output_without_sae = get_logits(model(prompt))
    # Run with SAE and use_error_term=True
    output_with_sae = get_logits(
        model.run_with_saes(prompt, saes=[hooked_sae], use_error_term=True)
    )
    # Compare outputs
    assert torch.allclose(output_without_sae, output_with_sae, atol=1e-4)
def test_run_with_cache_with_saes_use_error_term_true(
    model: HookedSAETransformer,
    hooked_sae: SAE,
):
    """Verifies that run_with_cache_with_saes with use_error_term=True doesn't change the model output."""
    # Get output without SAE
    output_without_sae, cache_without_sae = model.run_with_cache(prompt)
    output_without_sae = get_logits(output_without_sae)
    # Run with SAE and use_error_term=True
    output_with_sae, cache_with_sae = model.run_with_cache_with_saes(
        prompt, saes=[hooked_sae], use_error_term=True
    )
    output_with_sae = get_logits(output_with_sae)
    # Compare outputs
    assert torch.allclose(output_without_sae, output_with_sae, atol=1e-4)
    # Verify that the cache contains the SAE activations
    assert hooked_sae.cfg.hook_name + ".hook_sae_acts_post" in cache_with_sae
    # Verify that the activations at the SAE hook point are the same in both caches
    assert torch.allclose(
        cache_without_sae[hooked_sae.cfg.hook_name],
        cache_with_sae[hooked_sae.cfg.hook_name + ".hook_sae_output"],
        atol=1e-5,
    )
def test_add_sae_with_use_error_term_false(
    model: HookedSAETransformer,
    hooked_sae: SAE,
):
    """Verifies that add_sae with use_error_term=False changes the model output."""
    # Get output without SAE
    output_without_sae = get_logits(model(prompt))
    # Add SAE with use_error_term=False
    model.add_sae(hooked_sae, use_error_term=False)
    output_with_sae = get_logits(model(prompt))
    # Compare outputs - they should be different
    assert not torch.allclose(output_without_sae, output_with_sae, atol=1e-5)
    # Clean up
    model.reset_saes()
def test_run_with_saes_use_error_term_false(
    model: HookedSAETransformer,
    hooked_sae: SAE,
):
    """Verifies that run_with_saes with use_error_term=False changes the model output."""
    # Get output without SAE
    output_without_sae = get_logits(model(prompt))
    # Run with SAE and use_error_term=False
    output_with_sae = get_logits(
        model.run_with_saes(prompt, saes=[hooked_sae], use_error_term=False)
    )
    # Compare outputs - they should be different
    assert not torch.allclose(output_without_sae, output_with_sae, atol=1e-4)
def test_run_with_cache_with_saes_use_error_term_false(
    model: HookedSAETransformer,
    hooked_sae: SAE,
):
    """Verifies that run_with_cache_with_saes with use_error_term=False changes the model output."""
    # Get output without SAE
    output_without_sae, cache_without_sae = model.run_with_cache(prompt)
    output_without_sae = get_logits(output_without_sae)
    # Run with SAE and use_error_term=False
    output_with_sae, cache_with_sae = model.run_with_cache_with_saes(
        prompt, saes=[hooked_sae], use_error_term=False
    )
    output_with_sae = get_logits(output_with_sae)
    # Compare outputs - they should be different
    assert not torch.allclose(output_without_sae, output_with_sae, atol=1e-4)
    # Verify that the cache contains the SAE activations
    assert hooked_sae.cfg.hook_name + ".hook_sae_acts_post" in cache_with_sae
    # Verify that the activations at the SAE hook point are different in both caches
    assert not torch.allclose(
        cache_without_sae[hooked_sae.cfg.hook_name],
        cache_with_sae[hooked_sae.cfg.hook_name + ".hook_sae_output"],
        atol=1e-5,
    )

================
File: tests/analysis/test_hooked_sae.py
================
# type: ignore
import einops
import pytest
import torch
from transformer_lens import HookedTransformer
from transformer_lens.hook_points import HookPoint
from sae_lens import HookedSAETransformer
from sae_lens.sae import SAE, SAEConfig
MODEL = "solu-1l"
prompt = "Hello World!"
class Counter:
    def __init__(self):
        self.count = 0
    def inc(self, *args, **kwargs):  # type: ignore
        self.count += 1
@pytest.fixture(scope="module")
def model():
    model = HookedSAETransformer.from_pretrained_no_processing(MODEL, device="cpu")
    yield model
    model.reset_saes()
@pytest.fixture(scope="module")
def original_logits(model: HookedTransformer):
    return model(prompt)
def get_hooked_sae(model: HookedTransformer, act_name: str) -> SAE:
    site_to_size = {
        "hook_z": model.cfg.d_head * model.cfg.n_heads,
        "hook_mlp_out": model.cfg.d_model,
        "hook_resid_pre": model.cfg.d_model,
        "hook_post": model.cfg.d_mlp,
    }
    site = act_name.split(".")[-1]
    d_in = site_to_size[site]
    sae_cfg = SAEConfig(
        architecture="standard",
        d_in=d_in,
        d_sae=d_in * 2,
        dtype="float32",
        device="cpu",
        model_name=MODEL,
        hook_name=act_name,
        hook_layer=0,
        hook_head_index=None,
        activation_fn_str="relu",
        prepend_bos=True,
        context_size=128,
        dataset_path="test",
        dataset_trust_remote_code=True,
        apply_b_dec_to_input=False,
        finetuning_scaling_factor=False,
        sae_lens_training_version=None,
        normalize_activations="none",
        model_from_pretrained_kwargs={},
    )
    return SAE(sae_cfg)
@pytest.fixture(
    scope="module",
    params=[
        "blocks.0.attn.hook_z",
        "blocks.0.hook_mlp_out",
        "blocks.0.mlp.hook_post",
        "blocks.0.hook_resid_pre",
    ],
    ids=[
        "blocks.0.attn.hook_z",
        "blocks.0.hook_mlp_out",
        "blocks.0.mlp.hook_post",
        "blocks.0.hook_resid_pre",
    ],
)
def hooked_sae(
    model: HookedTransformer,
    request: pytest.FixtureRequest,
) -> SAE:
    return get_hooked_sae(model, request.param)
def test_forward_reconstructs_input(model: HookedTransformer, hooked_sae: SAE):
    """Verfiy that the HookedSAE returns an output with the same shape as the input activations."""
    act_name = hooked_sae.cfg.hook_name
    _, cache = model.run_with_cache(prompt, names_filter=act_name)
    x = cache[act_name]
    sae_output = hooked_sae(x)
    assert sae_output.shape == x.shape
def test_run_with_cache(model: HookedTransformer, hooked_sae: SAE):
    """Verifies that run_with_cache caches SAE activations"""
    act_name = hooked_sae.cfg.hook_name
    _, cache = model.run_with_cache(prompt, names_filter=act_name)
    x = cache[act_name]
    sae_output, cache = hooked_sae.run_with_cache(x)
    assert sae_output.shape == x.shape
    assert "hook_sae_input" in cache
    assert "hook_sae_acts_pre" in cache
    assert "hook_sae_acts_post" in cache
    assert "hook_sae_recons" in cache
    assert "hook_sae_output" in cache
def test_run_with_hooks(model: HookedTransformer, hooked_sae: SAE):
    """Verifies that run_with_hooks works with SAE activations"""
    c = Counter()
    act_name = hooked_sae.cfg.hook_name
    _, cache = model.run_with_cache(prompt, names_filter=act_name)
    x = cache[act_name]
    sae_hooks = [
        "hook_sae_input",
        "hook_sae_acts_pre",
        "hook_sae_acts_post",
        "hook_sae_recons",
        "hook_sae_output",
    ]
    sae_output = hooked_sae.run_with_hooks(
        x, fwd_hooks=[(sae_hook_name, c.inc) for sae_hook_name in sae_hooks]
    )
    assert sae_output.shape == x.shape
    assert c.count == len(sae_hooks)
def test_error_term(model: HookedTransformer, hooked_sae: SAE):
    """Verifies that that if we use error_terms, HookedSAE returns an output that is equal tdef test_feature_grads_with_error_term(model: HookedTransformer, hooked_sae: SparseAutoencoderBase):
    o the input activations."""
    act_name = hooked_sae.cfg.hook_name
    hooked_sae.use_error_term = True
    _, cache = model.run_with_cache(prompt, names_filter=act_name)
    x = cache[act_name]
    sae_output = hooked_sae(x)
    assert sae_output.shape == x.shape
    assert torch.allclose(sae_output, x, atol=1e-6)
    """Verifies that pytorch backward computes the correct feature gradients when using error_terms. Motivated by the need to compute feature gradients for attribution patching."""
    act_name = hooked_sae.cfg.hook_name
    hooked_sae.use_error_term = True
    # Get input activations
    _, cache = model.run_with_cache(prompt, names_filter=act_name)
    x = cache[act_name]
    # Cache gradients with respect to feature acts
    hooked_sae.reset_hooks()
    grad_cache = {}
    def backward_cache_hook(act: torch.Tensor, hook: HookPoint):
        grad_cache[hook.name] = act.detach()
    hooked_sae.add_hook("hook_sae_acts_post", backward_cache_hook, "bwd")  # type: ignore
    hooked_sae.add_hook("hook_sae_output", backward_cache_hook, "bwd")  # type: ignore
    sae_output = hooked_sae(x)
    assert torch.allclose(sae_output, x, atol=1e-6)
    value = sae_output.sum()
    value.backward()
    hooked_sae.reset_hooks()
    # Compute gradient analytically
    if act_name.endswith("hook_z"):
        reshaped_output_grad = einops.rearrange(
            grad_cache["hook_sae_output"], "... n_heads d_head -> ... (n_heads d_head)"
        )
        analytic_grad = reshaped_output_grad @ hooked_sae.W_dec.T
    else:
        analytic_grad = grad_cache["hook_sae_output"] @ hooked_sae.W_dec.T
    # Compare analytic gradient with pytorch computed gradient
    assert torch.allclose(grad_cache["hook_sae_acts_post"], analytic_grad, atol=1e-6)

================
File: tests/analysis/test_neuronpedia_integration.py
================
import pytest
from sae_lens.analysis.neuronpedia_integration import (
    NeuronpediaFeature,
    autointerp_neuronpedia_features,
    get_neuronpedia_feature,
    make_neuronpedia_list_with_features,
)
def test_get_neuronpedia_feature():
    result = get_neuronpedia_feature(
        feature=0, layer=0, model="gpt2-small", dataset="res-jb"
    )
    assert result["modelId"] == "gpt2-small"
    assert result["layer"] == "0-res-jb"
    assert result["index"] == 0
@pytest.mark.skip(
    reason="Need a way to test with an API key - maybe test to dev environment?"
)
def test_make_neuronpedia_list_with_features():
    make_neuronpedia_list_with_features(
        api_key="test_api_key",
        list_name="test_api",
        list_description="List descriptions are optional",
        features=[
            NeuronpediaFeature(
                modelId="gpt2-small",
                layer=0,
                dataset="att-kk",
                feature=11,
                description="List feature descriptions are optional as well.",
            ),
            NeuronpediaFeature(
                modelId="gpt2-small",
                layer=6,
                dataset="res_scefr-ajt",
                feature=7,
                description="You can add features from any model or SAE in one list.",
            ),
        ],
    )
@pytest.mark.skip(
    reason="Need a way to test with an API key - maybe test to dev environment?"
)
@pytest.mark.anyio
async def test_neuronpedia_autointerp():
    features = [
        NeuronpediaFeature(
            modelId="example-model",
            layer=0,
            dataset="test-np",
            feature=0,
        )
    ]
    await autointerp_neuronpedia_features(
        features=features,
        openai_api_key="your-oai-key",
        neuronpedia_api_key="your-np-key",
        autointerp_explainer_model_name="gpt-4-turbo-2024-04-09",
        autointerp_scorer_model_name="gpt-3.5-turbo",
        num_activations_to_use=5,
        do_score=False,
        save_to_disk=False,
        upload_to_neuronpedia=True,
    )

================
File: tests/conftest.py
================
import random
import shutil
from pathlib import Path
import numpy as np
import pytest
import torch
from sae_lens.sae import SAE
from tests.helpers import TINYSTORIES_MODEL, load_model_cached
torch.set_grad_enabled(True)
@pytest.fixture(autouse=True)
def reproducibility():
    """Apply various mechanisms to try to prevent nondeterminism in test runs."""
    # I have not in general attempted to verify that the below are necessary
    # for reproducibility, only that they are likely to help and unlikely to
    # hurt.
    # https://pytorch.org/docs/stable/notes/randomness.html#reproducibility
    seed = 0x1234_5678_9ABC_DEF0
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.benchmark = False
    # Python native RNG; docs don't give any limitations on seed range
    random.seed(seed)
    # this is a "legacy" method that operates on a global RandomState
    # sounds like the argument must be in [0, 2**32)
    np.random.seed(seed & 0xFFFF_FFFF)
@pytest.fixture
def ts_model():
    return load_model_cached(TINYSTORIES_MODEL)
# we started running out of space in CI, try cleaing up tmp paths after each test
@pytest.fixture(autouse=True)
def cleanup_tmp_path(tmp_path: Path):
    yield  # This line allows the test to run and use tmp_path
    # After the test is done, clean up the directory
    for item in tmp_path.iterdir():
        if item.is_file():
            item.unlink()
        elif item.is_dir():
            shutil.rmtree(item)
@pytest.fixture
def gpt2_res_jb_l4_sae() -> SAE:
    return SAE.from_pretrained(
        release="gpt2-small-res-jb",
        sae_id="blocks.4.hook_resid_pre",
        device="cpu",
    )[0]

================
File: tests/helpers.py
================
import copy
from typing import Any, Optional, TypedDict
from transformer_lens import HookedTransformer
from sae_lens.config import LanguageModelSAERunnerConfig
TINYSTORIES_MODEL = "tiny-stories-1M"
TINYSTORIES_DATASET = "roneneldan/TinyStories"
ALL_ARCHITECTURES = ["standard", "gated", "jumprelu", "topk"]
class LanguageModelSAERunnerConfigDict(TypedDict, total=False):
    model_name: str
    hook_name: str
    hook_layer: int
    hook_head_index: Optional[int]
    dataset_path: str
    dataset_trust_remote_code: bool
    is_dataset_tokenized: bool
    use_cached_activations: bool
    d_in: int
    l1_coefficient: float
    lp_norm: float
    lr: float
    train_batch_size_tokens: int
    context_size: int
    feature_sampling_window: int
    dead_feature_threshold: float
    dead_feature_window: int
    n_batches_in_buffer: int
    training_tokens: int
    store_batch_size_prompts: int
    log_to_wandb: bool
    wandb_project: str
    wandb_entity: str
    wandb_log_frequency: int
    device: str
    seed: int
    checkpoint_path: str
    dtype: str
    prepend_bos: bool
    normalize_activations: str
def build_sae_cfg(**kwargs: Any) -> LanguageModelSAERunnerConfig:
    """
    Helper to create a mock instance of LanguageModelSAERunnerConfig.
    """
    mock_config_dict: LanguageModelSAERunnerConfigDict = {
        "model_name": TINYSTORIES_MODEL,
        "hook_name": "blocks.0.hook_mlp_out",
        "hook_layer": 0,
        "hook_head_index": None,
        "dataset_path": TINYSTORIES_DATASET,
        "dataset_trust_remote_code": True,
        "is_dataset_tokenized": False,
        "use_cached_activations": False,
        "d_in": 64,
        "l1_coefficient": 2e-3,
        "lp_norm": 1,
        "lr": 2e-4,
        "train_batch_size_tokens": 4,
        "context_size": 6,
        "feature_sampling_window": 50,
        "dead_feature_threshold": 1e-7,
        "dead_feature_window": 1000,
        "n_batches_in_buffer": 2,
        "training_tokens": 1_000_000,
        "store_batch_size_prompts": 4,
        "log_to_wandb": False,
        "wandb_project": "test_project",
        "wandb_entity": "test_entity",
        "wandb_log_frequency": 10,
        "device": "cpu",
        "seed": 24,
        "checkpoint_path": "test/checkpoints",
        "dtype": "float32",
        "prepend_bos": True,
        "normalize_activations": "none",
    }
    for key, value in kwargs.items():
        mock_config_dict[key] = value
    mock_config = LanguageModelSAERunnerConfig(**mock_config_dict)
    # reset checkpoint path (as we add an id to each each time)
    mock_config.checkpoint_path = kwargs.get("checkpoint_path", "test/checkpoints")
    return mock_config
MODEL_CACHE: dict[str, HookedTransformer] = {}
def load_model_cached(model_name: str) -> HookedTransformer:
    """
    helper to avoid unnecessarily loading the same model multiple times.
    NOTE: if the model gets modified in tests this will not work.
    """
    if model_name not in MODEL_CACHE:
        MODEL_CACHE[model_name] = HookedTransformer.from_pretrained(
            model_name, device="cpu"
        )
    # we copy here to prevent sharing state across tests
    return copy.deepcopy(MODEL_CACHE[model_name])

================
File: tests/test_evals.py
================
import argparse
import json
import math
from pathlib import Path
from unittest.mock import MagicMock, patch
import pytest
from datasets import Dataset
from transformer_lens import HookedTransformer
from sae_lens.config import LanguageModelSAERunnerConfig
from sae_lens.evals import (
    EvalConfig,
    all_loadable_saes,
    get_downstream_reconstruction_metrics,
    get_eval_everything_config,
    get_saes_from_regex,
    get_sparsity_and_variance_metrics,
    process_results,
    run_evals,
    run_evaluations,
)
from sae_lens.load_model import load_model
from sae_lens.sae import SAE
from sae_lens.toolkit.pretrained_saes_directory import PretrainedSAELookup
from sae_lens.training.activations_store import ActivationsStore
from sae_lens.training.training_sae import TrainingSAE
from tests.helpers import TINYSTORIES_MODEL, build_sae_cfg, load_model_cached
TRAINER_EVAL_CONFIG = EvalConfig(
    n_eval_reconstruction_batches=10,
    compute_ce_loss=True,
    n_eval_sparsity_variance_batches=1,
    compute_l2_norms=True,
)
@pytest.fixture
def example_dataset() -> Dataset:
    return Dataset.from_list(
        [
            {"text": "hello world1"},
            {"text": "hello world2"},
            {"text": "hello world3"},
        ]
        * 20
    )
# not sure why we have NaNs in the feature metrics, but this is a quick fix for tests
def _replace_nan(list: list[float]) -> list[float]:
    return [0 if math.isnan(x) else x for x in list]
@pytest.fixture(
    params=[
        {
            "model_name": "tiny-stories-1M",
            "dataset_path": "roneneldan/TinyStories",
            "hook_name": "blocks.1.hook_resid_pre",
            "hook_layer": 1,
            "d_in": 64,
        },
        {
            "model_name": "tiny-stories-1M",
            "dataset_path": "roneneldan/TinyStories",
            "hook_name": "blocks.1.hook_resid_pre",
            "hook_layer": 1,
            "d_in": 64,
            "normalize_sae_decoder": False,
            "scale_sparsity_penalty_by_decoder_norm": True,
        },
        {
            "model_name": "tiny-stories-1M",
            "dataset_path": "apollo-research/roneneldan-TinyStories-tokenizer-gpt2",
            "hook_name": "blocks.1.hook_resid_pre",
            "hook_layer": 1,
            "d_in": 64,
        },
        {
            "model_name": "tiny-stories-1M",
            "dataset_path": "roneneldan/TinyStories",
            "hook_name": "blocks.1.attn.hook_z",
            "hook_layer": 1,
            "d_in": 16 * 4,
        },
        {
            "model_name": "tiny-stories-1M",
            "dataset_path": "roneneldan/TinyStories",
            "hook_name": "blocks.1.attn.hook_q",
            "hook_layer": 1,
            "d_in": 16 * 4,
        },
        {
            "model_name": "tiny-stories-1M",
            "dataset_path": "roneneldan/TinyStories",
            "hook_name": "blocks.1.attn.hook_q",
            "hook_layer": 1,
            "d_in": 4,
            "hook_head_index": 2,
        },
    ],
    ids=[
        "tiny-stories-1M-resid-pre",
        "tiny-stories-1M-resid-pre-L1-W-dec-Norm",
        "tiny-stories-1M-resid-pre-pretokenized",
        "tiny-stories-1M-hook-z",
        "tiny-stories-1M-hook-q",
        "tiny-stories-1M-hook-q-head-index-2",
    ],
)
def cfg(request: pytest.FixtureRequest):
    """
    Pytest fixture to create a mock instance of LanguageModelSAERunnerConfig.
    """
    params = request.param
    return build_sae_cfg(**params)
@pytest.fixture
def model():
    return load_model_cached(TINYSTORIES_MODEL)
@pytest.fixture
def activation_store(model: HookedTransformer, cfg: LanguageModelSAERunnerConfig):
    return ActivationsStore.from_config(
        model, cfg, override_dataset=Dataset.from_list([{"text": "hello world"}] * 2000)
    )
@pytest.fixture
def base_sae(cfg: LanguageModelSAERunnerConfig):
    return SAE.from_dict(cfg.get_base_sae_cfg_dict())
@pytest.fixture
def training_sae(cfg: LanguageModelSAERunnerConfig):
    return TrainingSAE.from_dict(cfg.get_training_sae_cfg_dict())
all_possible_keys = [
    "model_behavior_preservation",
    "model_performance_preservation",
    "reconstruction_quality",
    "shrinkage",
    "sparsity",
    "token_stats",
]
all_featurewise_keys_expected = [
    "feature_density",
    "consistent_activation_heuristic",
    "encoder_bias",
    "encoder_decoder_cosine_sim",
    "encoder_norm",
]
def test_run_evals_base_sae(
    base_sae: SAE,
    activation_store: ActivationsStore,
    model: HookedTransformer,
):
    eval_metrics, _ = run_evals(
        sae=base_sae,
        activation_store=activation_store,
        model=model,
        eval_config=get_eval_everything_config(),
    )
    assert set(eval_metrics.keys()).issubset(set(all_possible_keys))
    assert len(eval_metrics) > 0
def test_run_evals_training_sae(
    training_sae: TrainingSAE,
    activation_store: ActivationsStore,
    model: HookedTransformer,
):
    eval_metrics, feature_metrics = run_evals(
        sae=training_sae,
        activation_store=activation_store,
        model=model,
        eval_config=get_eval_everything_config(),
    )
    assert set(eval_metrics.keys()).issubset(set(all_possible_keys))
    assert len(eval_metrics) > 0
    assert set(feature_metrics.keys()).issubset(set(all_featurewise_keys_expected))
def test_run_evals_training_sae_ignore_bos(
    training_sae: TrainingSAE,
    activation_store: ActivationsStore,
    model: HookedTransformer,
):
    eval_metrics, _ = run_evals(
        sae=training_sae,
        activation_store=activation_store,
        model=model,
        eval_config=get_eval_everything_config(),
        ignore_tokens={
            model.tokenizer.bos_token_id,  # type: ignore
            model.tokenizer.eos_token_id,  # type: ignore
            model.tokenizer.pad_token_id,  # type: ignore
        },
    )
    assert set(eval_metrics.keys()).issubset(set(all_possible_keys))
    assert len(eval_metrics) > 0
def test_training_eval_config(
    base_sae: SAE,
    activation_store: ActivationsStore,
    model: HookedTransformer,
):
    expected_keys = [
        "model_performance_preservation",
        "shrinkage",
        "token_stats",
    ]
    eval_config = TRAINER_EVAL_CONFIG
    eval_metrics, _ = run_evals(
        sae=base_sae,
        activation_store=activation_store,
        model=model,
        eval_config=eval_config,
    )
    assert set(eval_metrics.keys()) == set(expected_keys)
def test_training_eval_config_ignore_control_tokens(
    base_sae: SAE,
    activation_store: ActivationsStore,
    model: HookedTransformer,
):
    expected_keys = [
        "model_performance_preservation",
        "shrinkage",
        "token_stats",
    ]
    eval_config = TRAINER_EVAL_CONFIG
    eval_metrics, _ = run_evals(
        sae=base_sae,
        activation_store=activation_store,
        model=model,
        eval_config=eval_config,
        ignore_tokens={
            model.tokenizer.pad_token_id,  # type: ignore
            model.tokenizer.eos_token_id,  # type: ignore
            model.tokenizer.bos_token_id,  # type: ignore
        },
    )
    assert set(eval_metrics.keys()) == set(expected_keys)
def test_run_empty_evals(
    base_sae: SAE,
    activation_store: ActivationsStore,
    model: HookedTransformer,
):
    empty_config = EvalConfig(
        n_eval_reconstruction_batches=0,
        n_eval_sparsity_variance_batches=0,
        compute_ce_loss=False,
        compute_kl=False,
        compute_l2_norms=False,
        compute_sparsity_metrics=False,
        compute_variance_metrics=False,
        compute_featurewise_density_statistics=False,
    )
    eval_metrics, feature_metrics = run_evals(
        sae=base_sae,
        activation_store=activation_store,
        model=model,
        eval_config=empty_config,
    )
    assert len(eval_metrics) == 1, "Expected only token_stats in eval_metrics"
    assert "token_stats" in eval_metrics, "Expected token_stats in eval_metrics"
    assert len(feature_metrics) == 0, "Expected empty feature_metrics"
@pytest.fixture
def mock_args():
    args = argparse.Namespace()
    args.sae_regex_pattern = "test_pattern"
    args.sae_block_pattern = "test_block"
    args.num_eval_batches = 2
    args.batch_size_prompts = 4
    args.eval_batch_size_prompts = 4
    args.n_eval_reconstruction_batches = 1
    args.n_eval_sparsity_variance_batches = 1
    args.datasets = ["test_dataset"]
    args.ctx_lens = [64]
    args.output_dir = "test_output"
    args.verbose = False
    return args
@patch("sae_lens.evals.get_saes_from_regex")
@patch("sae_lens.evals.multiple_evals")
def test_run_evaluations(
    mock_multiple_evals: MagicMock,
    mock_get_saes_from_regex: MagicMock,
    mock_args: argparse.Namespace,
):
    mock_get_saes_from_regex.return_value = [
        ("release1", "sae1", 0.8, 10),
        ("release2", "sae2", 0.7, 8),
    ]
    mock_multiple_evals.return_value = [{"test": "result"}]
    result = run_evaluations(mock_args)
    mock_get_saes_from_regex.assert_called_once_with(
        mock_args.sae_regex_pattern, mock_args.sae_block_pattern
    )
    mock_multiple_evals.assert_called_once_with(
        sae_regex_pattern=mock_args.sae_regex_pattern,
        sae_block_pattern=mock_args.sae_block_pattern,
        eval_batch_size_prompts=mock_args.eval_batch_size_prompts,
        n_eval_reconstruction_batches=mock_args.n_eval_reconstruction_batches,
        n_eval_sparsity_variance_batches=mock_args.n_eval_sparsity_variance_batches,
        datasets=mock_args.datasets,
        ctx_lens=mock_args.ctx_lens,
        output_dir=mock_args.output_dir,
        verbose=mock_args.verbose,
    )
    assert result == [{"test": "result"}]
def test_process_results(tmp_path: Path):
    eval_results = [
        {
            "unique_id": "test-sae",
            "eval_cfg": {"context_size": 64, "dataset": "test/dataset"},
            "metrics": {"metric1": 0.5, "metric2": 0.7},
            "sae_cfg": {"config1": "value1"},
        }
    ]
    output_dir = tmp_path / "test_output"
    process_results(eval_results, str(output_dir))  # type: ignore
    # Check if individual JSON file is created
    individual_json_path = output_dir / "test-sae_64_test_dataset.json"
    assert individual_json_path.exists()
    with open(individual_json_path) as f:
        assert json.load(f) == eval_results[0]
    # Check if combined JSON file is created
    combined_json_path = output_dir / "all_eval_results.json"
    assert combined_json_path.exists()
    with open(combined_json_path) as f:
        assert json.load(f) == eval_results
    # Check if CSV file is created
    csv_path = output_dir / "all_eval_results.csv"
    assert csv_path.exists()
def test_get_downstream_reconstruction_metrics_with_hf_model_gives_same_results_as_tlens_model(
    gpt2_res_jb_l4_sae: SAE, example_dataset: Dataset
):
    hf_model = load_model(
        model_class_name="AutoModelForCausalLM",
        model_name="gpt2",
        device="cpu",
    )
    tlens_model = HookedTransformer.from_pretrained_no_processing("gpt2", device="cpu")
    cfg = build_sae_cfg(hook_name="transformer.h.3")
    gpt2_res_jb_l4_sae.cfg.hook_name = "transformer.h.3"
    hf_store = ActivationsStore.from_config(
        hf_model, cfg, override_dataset=example_dataset
    )
    hf_metrics = get_downstream_reconstruction_metrics(
        sae=gpt2_res_jb_l4_sae,
        model=hf_model,
        activation_store=hf_store,
        compute_kl=True,
        compute_ce_loss=True,
        n_batches=1,
        eval_batch_size_prompts=4,
    )
    cfg = build_sae_cfg(hook_name="blocks.4.hook_resid_pre")
    gpt2_res_jb_l4_sae.cfg.hook_name = "blocks.4.hook_resid_pre"
    tlens_store = ActivationsStore.from_config(
        tlens_model, cfg, override_dataset=example_dataset
    )
    tlens_metrics = get_downstream_reconstruction_metrics(
        sae=gpt2_res_jb_l4_sae,
        model=tlens_model,
        activation_store=tlens_store,
        compute_kl=True,
        compute_ce_loss=True,
        n_batches=1,
        eval_batch_size_prompts=4,
    )
    for key in hf_metrics:
        assert hf_metrics[key] == pytest.approx(tlens_metrics[key], abs=1e-3)
def test_get_sparsity_and_variance_metrics_with_hf_model_gives_same_results_as_tlens_model(
    gpt2_res_jb_l4_sae: SAE,
    example_dataset: Dataset,
):
    hf_model = load_model(
        model_class_name="AutoModelForCausalLM",
        model_name="gpt2",
        device="cpu",
    )
    tlens_model = HookedTransformer.from_pretrained_no_processing("gpt2", device="cpu")
    cfg = build_sae_cfg(hook_name="transformer.h.3")
    gpt2_res_jb_l4_sae.cfg.hook_name = "transformer.h.3"
    hf_store = ActivationsStore.from_config(
        hf_model, cfg, override_dataset=example_dataset
    )
    hf_metrics, hf_feat_metrics = get_sparsity_and_variance_metrics(
        sae=gpt2_res_jb_l4_sae,
        model=hf_model,
        activation_store=hf_store,
        n_batches=1,
        compute_l2_norms=True,
        compute_sparsity_metrics=True,
        compute_variance_metrics=True,
        compute_featurewise_density_statistics=True,
        eval_batch_size_prompts=4,
        model_kwargs={},
    )
    cfg = build_sae_cfg(hook_name="blocks.4.hook_resid_pre")
    gpt2_res_jb_l4_sae.cfg.hook_name = "blocks.4.hook_resid_pre"
    tlens_store = ActivationsStore.from_config(
        tlens_model, cfg, override_dataset=example_dataset
    )
    tlens_metrics, tlens_feat_metrics = get_sparsity_and_variance_metrics(
        sae=gpt2_res_jb_l4_sae,
        model=tlens_model,
        activation_store=tlens_store,
        n_batches=1,
        compute_l2_norms=True,
        compute_sparsity_metrics=True,
        compute_variance_metrics=True,
        compute_featurewise_density_statistics=True,
        eval_batch_size_prompts=4,
        model_kwargs={},
    )
    for key in hf_metrics:
        assert hf_metrics[key] == pytest.approx(tlens_metrics[key], rel=1e-4)
    for key in hf_feat_metrics:
        assert _replace_nan(hf_feat_metrics[key]) == pytest.approx(
            _replace_nan(tlens_feat_metrics[key]), rel=1e-4
        )
@patch("sae_lens.evals.get_pretrained_saes_directory")
def test_all_loadable_saes(mock_get_pretrained_saes_directory: MagicMock):
    mock_get_pretrained_saes_directory.return_value = {
        "release1": PretrainedSAELookup(
            release="release1",
            repo_id="repo1",
            model="model1",
            conversion_func=None,
            saes_map={"sae1": "path1", "sae2": "path2"},
            expected_var_explained={"sae1": 0.9, "sae2": 0.85},
            expected_l0={"sae1": 0.1, "sae2": 0.15},
            neuronpedia_id={},
            config_overrides=None,
        ),
        "release2": PretrainedSAELookup(
            release="release2",
            repo_id="repo2",
            model="model2",
            conversion_func=None,
            saes_map={"sae3": "path3"},
            expected_var_explained={"sae3": 0.8},
            expected_l0={"sae3": 0.2},
            neuronpedia_id={},
            config_overrides=None,
        ),
    }
    result = all_loadable_saes()
    expected = [
        ("release1", "sae1", 0.9, 0.1),
        ("release1", "sae2", 0.85, 0.15),
        ("release2", "sae3", 0.8, 0.2),
    ]
    assert result == expected
mock_all_saes = [
    ("release1", "sae1", 0.9, 0.1),
    ("release1", "sae2", 0.85, 0.15),
    ("release2", "sae3", 0.8, 0.2),
    ("release2", "block1", 0.95, 0.05),
]
@patch("sae_lens.evals.all_loadable_saes")
def test_get_saes_from_regex_no_match(mock_all_loadable_saes: MagicMock):
    mock_all_loadable_saes.return_value = mock_all_saes
    result = get_saes_from_regex("release1", "sae3")
    assert not result
@patch("sae_lens.evals.all_loadable_saes")
def test_get_saes_from_regex_single_match(mock_all_loadable_saes: MagicMock):
    mock_all_loadable_saes.return_value = mock_all_saes
    result = get_saes_from_regex("release1", "sae1")
    expected = [("release1", "sae1", 0.9, 0.1)]
    assert result == expected
@patch("sae_lens.evals.all_loadable_saes")
def test_get_saes_from_regex_multiple_matches(mock_all_loadable_saes: MagicMock):
    mock_all_loadable_saes.return_value = mock_all_saes
    result = get_saes_from_regex("release.*", "sae.*")
    expected = [
        ("release1", "sae1", 0.9, 0.1),
        ("release1", "sae2", 0.85, 0.15),
        ("release2", "sae3", 0.8, 0.2),
    ]
    assert result == expected

================
File: tests/toolkit/test_pretrained_sae_loaders.py
================
import pytest
from sae_lens.sae import SAE
from sae_lens.toolkit.pretrained_sae_loaders import (
    SAEConfigLoadOptions,
    get_deepseek_r1_config,
    get_sae_config,
)
def test_get_sae_config_sae_lens():
    cfg_dict = get_sae_config(
        "gpt2-small-res-jb",
        sae_id="blocks.0.hook_resid_pre",
        options=SAEConfigLoadOptions(),
    )
    expected_cfg_dict = {
        "activation_fn_str": "relu",
        "apply_b_dec_to_input": True,
        "architecture": "standard",
        "model_name": "gpt2-small",
        "hook_point": "blocks.0.hook_resid_pre",
        "hook_point_layer": 0,
        "hook_point_head_index": None,
        "dataset_path": "Skylion007/openwebtext",
        "dataset_trust_remote_code": True,
        "is_dataset_tokenized": False,
        "context_size": 128,
        "use_cached_activations": False,
        "cached_activations_path": "activations/Skylion007_openwebtext/gpt2-small/blocks.0.hook_resid_pre",
        "d_in": 768,
        "n_batches_in_buffer": 128,
        "total_training_tokens": 300000000,
        "store_batch_size": 32,
        "device": "mps",
        "seed": 42,
        "dtype": "torch.float32",
        "b_dec_init_method": "geometric_median",
        "expansion_factor": 32,
        "from_pretrained_path": None,
        "l1_coefficient": 8e-05,
        "lr": 0.0004,
        "lr_scheduler_name": None,
        "lr_warm_up_steps": 5000,
        "model_from_pretrained_kwargs": {
            "center_writing_weights": True,
        },
        "train_batch_size": 4096,
        "use_ghost_grads": False,
        "feature_sampling_window": 1000,
        "finetuning_scaling_factor": False,
        "feature_sampling_method": None,
        "resample_batches": 1028,
        "feature_reinit_scale": 0.2,
        "dead_feature_window": 5000,
        "dead_feature_estimation_method": "no_fire",
        "dead_feature_threshold": 1e-08,
        "log_to_wandb": True,
        "wandb_project": "mats_sae_training_gpt2_small_resid_pre_5",
        "wandb_entity": None,
        "wandb_log_frequency": 100,
        "n_checkpoints": 10,
        "checkpoint_path": "checkpoints/y1t51byy",
        "d_sae": 24576,
        "tokens_per_buffer": 67108864,
        "run_name": "24576-L1-8e-05-LR-0.0004-Tokens-3.000e+08",
        "neuronpedia_id": "gpt2-small/0-res-jb",
        "normalize_activations": "none",
        "prepend_bos": True,
        "sae_lens_training_version": None,
    }
    assert cfg_dict == expected_cfg_dict
def test_get_sae_config_connor_rob_hook_z():
    cfg_dict = get_sae_config(
        "gpt2-small-hook-z-kk",
        sae_id="blocks.0.hook_z",
        options=SAEConfigLoadOptions(),
    )
    expected_cfg_dict = {
        "architecture": "standard",
        "d_in": 768,
        "d_sae": 24576,
        "dtype": "float32",
        "device": "cpu",
        "model_name": "gpt2-small",
        "hook_name": "blocks.0.attn.hook_z",
        "hook_layer": 0,
        "hook_head_index": None,
        "activation_fn_str": "relu",
        "apply_b_dec_to_input": True,
        "finetuning_scaling_factor": False,
        "sae_lens_training_version": None,
        "prepend_bos": True,
        "dataset_path": "Skylion007/openwebtext",
        "context_size": 128,
        "normalize_activations": "none",
        "dataset_trust_remote_code": True,
        "neuronpedia_id": "gpt2-small/0-att-kk",
    }
    assert cfg_dict == expected_cfg_dict
def test_get_sae_config_gemma_2():
    cfg_dict = get_sae_config(
        "gemma-scope-2b-pt-res",
        sae_id="embedding/width_4k/average_l0_6",
        options=SAEConfigLoadOptions(),
    )
    expected_cfg_dict = {
        "architecture": "jumprelu",
        "d_in": 2304,
        "d_sae": 4096,
        "dtype": "float32",
        "model_name": "gemma-2-2b",
        "hook_name": "hook_embed",
        "hook_layer": 0,
        "hook_head_index": None,
        "activation_fn_str": "relu",
        "finetuning_scaling_factor": False,
        "sae_lens_training_version": None,
        "prepend_bos": True,
        "dataset_path": "monology/pile-uncopyrighted",
        "context_size": 1024,
        "dataset_trust_remote_code": True,
        "apply_b_dec_to_input": False,
        "normalize_activations": None,
        "device": "cpu",
        "neuronpedia_id": None,
    }
    assert cfg_dict == expected_cfg_dict
def test_get_sae_config_dictionary_learning_1():
    cfg_dict = get_sae_config(
        "sae_bench_gemma-2-2b_topk_width-2pow16_date-1109",
        sae_id="blocks.12.hook_resid_post__trainer_0",
        options=SAEConfigLoadOptions(),
    )
    expected_cfg_dict = {
        "architecture": "standard",
        "d_in": 2304,
        "d_sae": 65536,
        "dtype": "float32",
        "device": "cpu",
        "model_name": "gemma-2-2b",
        "hook_name": "blocks.12.hook_resid_post",
        "hook_layer": 12,
        "hook_head_index": None,
        "activation_fn_str": "topk",
        "activation_fn_kwargs": {"k": 20},
        "apply_b_dec_to_input": True,
        "finetuning_scaling_factor": False,
        "sae_lens_training_version": None,
        "prepend_bos": True,
        "dataset_path": "monology/pile-uncopyrighted",
        "dataset_trust_remote_code": True,
        "context_size": 128,
        "normalize_activations": "none",
        "neuronpedia_id": "gemma-2-2b/12-sae_bench-topk-res-65k__trainer_0_step_final",
    }
    assert cfg_dict == expected_cfg_dict
def test_get_sae_config_matches_from_pretrained():
    from_pretrained_cfg_dict = SAE.from_pretrained(
        "gpt2-small-res-jb",
        sae_id="blocks.0.hook_resid_pre",
        device="cpu",
    )[1]
    direct_sae_cfg = get_sae_config(
        "gpt2-small-res-jb",
        sae_id="blocks.0.hook_resid_pre",
        options=SAEConfigLoadOptions(device="cpu"),
    )
    assert direct_sae_cfg == from_pretrained_cfg_dict
def test_get_deepseek_r1_config():
    """Test that the DeepSeek R1 config is generated correctly."""
    options = SAEConfigLoadOptions(device="cpu")
    cfg = get_deepseek_r1_config(
        repo_id="some/repo",
        folder_name="DeepSeek-R1-Distill-Llama-8B-SAE-l19.pt",
        options=options,
    )
    expected_cfg = {
        "architecture": "standard",
        "d_in": 4096,  # LLaMA 8B hidden size
        "d_sae": 4096 * 16,  # Expansion factor 16
        "dtype": "bfloat16",
        "context_size": 1024,
        "model_name": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
        "hook_name": "blocks.19.hook_resid_post",
        "hook_layer": 19,
        "hook_head_index": None,
        "prepend_bos": True,
        "dataset_path": "lmsys/lmsys-chat-1m",
        "dataset_trust_remote_code": True,
        "sae_lens_training_version": None,
        "activation_fn_str": "relu",
        "normalize_activations": "none",
        "device": "cpu",
        "apply_b_dec_to_input": False,
        "finetuning_scaling_factor": False,
    }
    assert cfg == expected_cfg
def test_get_deepseek_r1_config_with_invalid_layer():
    """Test that get_deepseek_r1_config raises ValueError with invalid layer in filename."""
    options = SAEConfigLoadOptions(device="cpu")
    with pytest.raises(
        ValueError, match="Could not find layer number in filename: invalid_filename.pt"
    ):
        get_deepseek_r1_config(
            repo_id="some/repo", folder_name="invalid_filename.pt", options=options
        )

================
File: tests/toolkit/test_pretrained_saes_directory.py
================
import pandas as pd
import pytest
from sae_lens.toolkit.pretrained_saes_directory import (
    PretrainedSAELookup,
    get_pretrained_saes_directory,
    get_repo_id_and_folder_name,
)
def test_get_pretrained_saes_directory():
    sae_directory = get_pretrained_saes_directory()
    assert isinstance(sae_directory, dict)
    expected_result = PretrainedSAELookup(
        release="gpt2-small-res-jb",
        repo_id="jbloom/GPT2-Small-SAEs-Reformatted",
        model="gpt2-small",
        conversion_func=None,
        saes_map={
            "blocks.0.hook_resid_pre": "blocks.0.hook_resid_pre",
            "blocks.1.hook_resid_pre": "blocks.1.hook_resid_pre",
            "blocks.2.hook_resid_pre": "blocks.2.hook_resid_pre",
            "blocks.3.hook_resid_pre": "blocks.3.hook_resid_pre",
            "blocks.4.hook_resid_pre": "blocks.4.hook_resid_pre",
            "blocks.5.hook_resid_pre": "blocks.5.hook_resid_pre",
            "blocks.6.hook_resid_pre": "blocks.6.hook_resid_pre",
            "blocks.7.hook_resid_pre": "blocks.7.hook_resid_pre",
            "blocks.8.hook_resid_pre": "blocks.8.hook_resid_pre",
            "blocks.9.hook_resid_pre": "blocks.9.hook_resid_pre",
            "blocks.10.hook_resid_pre": "blocks.10.hook_resid_pre",
            "blocks.11.hook_resid_pre": "blocks.11.hook_resid_pre",
            "blocks.11.hook_resid_post": "blocks.11.hook_resid_post",
        },
        expected_var_explained={
            "blocks.0.hook_resid_pre": 0.999,
            "blocks.1.hook_resid_pre": 0.999,
            "blocks.2.hook_resid_pre": 0.999,
            "blocks.3.hook_resid_pre": 0.999,
            "blocks.4.hook_resid_pre": 0.9,
            "blocks.5.hook_resid_pre": 0.9,
            "blocks.6.hook_resid_pre": 0.9,
            "blocks.7.hook_resid_pre": 0.9,
            "blocks.8.hook_resid_pre": 0.9,
            "blocks.9.hook_resid_pre": 0.77,
            "blocks.10.hook_resid_pre": 0.77,
            "blocks.11.hook_resid_pre": 0.77,
            "blocks.11.hook_resid_post": 0.77,
        },
        expected_l0={
            "blocks.0.hook_resid_pre": 10.0,
            "blocks.1.hook_resid_pre": 10.0,
            "blocks.2.hook_resid_pre": 18.0,
            "blocks.3.hook_resid_pre": 23.0,
            "blocks.4.hook_resid_pre": 31.0,
            "blocks.5.hook_resid_pre": 41.0,
            "blocks.6.hook_resid_pre": 51.0,
            "blocks.7.hook_resid_pre": 54.0,
            "blocks.8.hook_resid_pre": 60.0,
            "blocks.9.hook_resid_pre": 70.0,
            "blocks.10.hook_resid_pre": 52.0,
            "blocks.11.hook_resid_pre": 56.0,
            "blocks.11.hook_resid_post": 70.0,
        },
        config_overrides={
            "model_from_pretrained_kwargs": {
                "center_writing_weights": True,
            }
        },
        neuronpedia_id={
            "blocks.0.hook_resid_pre": "gpt2-small/0-res-jb",
            "blocks.1.hook_resid_pre": "gpt2-small/1-res-jb",
            "blocks.2.hook_resid_pre": "gpt2-small/2-res-jb",
            "blocks.3.hook_resid_pre": "gpt2-small/3-res-jb",
            "blocks.4.hook_resid_pre": "gpt2-small/4-res-jb",
            "blocks.5.hook_resid_pre": "gpt2-small/5-res-jb",
            "blocks.6.hook_resid_pre": "gpt2-small/6-res-jb",
            "blocks.7.hook_resid_pre": "gpt2-small/7-res-jb",
            "blocks.8.hook_resid_pre": "gpt2-small/8-res-jb",
            "blocks.9.hook_resid_pre": "gpt2-small/9-res-jb",
            "blocks.10.hook_resid_pre": "gpt2-small/10-res-jb",
            "blocks.11.hook_resid_pre": "gpt2-small/11-res-jb",
            "blocks.11.hook_resid_post": "gpt2-small/12-res-jb",
        },
    )
    assert sae_directory["gpt2-small-res-jb"] == expected_result
def test_get_pretrained_saes_directory_unique_np_ids():
    # ideally this code should be elsewhere but as a stop-gap we'll leave it here.
    df = pd.DataFrame.from_records(
        {k: v.__dict__ for k, v in get_pretrained_saes_directory().items()}
    ).T
    df.drop(
        columns=[
            "repo_id",
            "saes_map",
            "expected_var_explained",
            "expected_l0",
            "config_overrides",
            "conversion_func",
        ],
        inplace=True,
    )
    df["neuronpedia_id_list"] = df["neuronpedia_id"].apply(lambda x: list(x.items()))
    df_exploded = df.explode("neuronpedia_id_list")
    df_exploded[["sae_lens_id", "neuronpedia_id"]] = pd.DataFrame(
        df_exploded["neuronpedia_id_list"].tolist(), index=df_exploded.index
    )
    df_exploded = df_exploded.drop(columns=["neuronpedia_id_list"])
    df_exploded = df_exploded.reset_index(drop=True)
    df_exploded["neuronpedia_set"] = df_exploded["neuronpedia_id"].apply(
        lambda x: "-".join(x.split("/")[-1].split("-")[1:]) if x is not None else None
    )
    duplicate_ids = df_exploded.groupby("neuronpedia_id").sae_lens_id.apply(
        lambda x: len(x)
    )
    assert (
        duplicate_ids.max() == 1
    ), f"Duplicate IDs found: {duplicate_ids[duplicate_ids > 1]}"
def test_get_repo_id_and_folder_name_release_found():
    repo_id, folder_name = get_repo_id_and_folder_name(
        "gpt2-small-res-jb", sae_id="blocks.0.hook_resid_pre"
    )
    assert repo_id == "jbloom/GPT2-Small-SAEs-Reformatted"
    assert folder_name == "blocks.0.hook_resid_pre"
def test_get_repo_id_and_folder_name_release_not_found():
    repo_id, folder_name = get_repo_id_and_folder_name("release1", "sae1")
    assert repo_id == "release1"
    assert folder_name == "sae1"
def test_get_repo_id_and_folder_name_raises_error_if_sae_id_not_found():
    with pytest.raises(ValueError):
        get_repo_id_and_folder_name("gpt2-small-res-jb", sae_id="sae1")

================
File: tests/training/test_activation_functions.py
================
import pytest
import torch
from sae_lens.sae import get_activation_fn
def test_get_activation_fn_tanh_relu():
    tanh_relu = get_activation_fn("tanh-relu")
    assert tanh_relu(torch.tensor([-1.0, 0.0])).tolist() == [0.0, 0.0]
    assert tanh_relu(torch.tensor(1e10)).item() == pytest.approx(1.0)
def test_get_activation_fn_relu():
    relu = get_activation_fn("relu")
    assert relu(torch.tensor([-1.0, 0.0])).tolist() == [0.0, 0.0]
    assert relu(torch.tensor(999.9)).item() == pytest.approx(999.9)
def test_get_activation_fn_error_for_unknown_values():
    with pytest.raises(ValueError):
        get_activation_fn("unknown")
def test_get_activation_fn_topk_32():
    topk = get_activation_fn("topk", k=32)
    example_activations = torch.randn(10, 4, 512)
    post_activations = topk(example_activations)
    assert post_activations.shape == example_activations.shape
    assert post_activations.nonzero().shape[0] == 32 * 10 * 4
    expected_activations = example_activations.flatten().topk(32).values
    assert torch.allclose(
        post_activations.flatten().sort(dim=0, descending=True).values[:32],
        expected_activations,
    )
def test_get_activation_fn_topk_16():
    topk = get_activation_fn("topk", k=16)
    example_activations = torch.randn(10, 4, 512)
    post_activations = topk(example_activations)
    assert post_activations.shape == example_activations.shape
    assert post_activations.nonzero().shape[0] == 16 * 10 * 4
    expected_activations = example_activations.flatten().topk(16).values
    assert torch.allclose(
        post_activations.flatten().sort(dim=0, descending=True).values[:16],
        expected_activations,
    )

================
File: tests/training/test_activations_store.py
================
import os
import tempfile
from collections.abc import Iterable
from math import ceil
from typing import Any, Optional
import numpy as np
import pytest
import torch
from datasets import Dataset, IterableDataset
from safetensors.torch import load_file
from transformer_lens import HookedTransformer
from sae_lens.config import LanguageModelSAERunnerConfig, PretokenizeRunnerConfig
from sae_lens.load_model import load_model
from sae_lens.pretokenize_runner import pretokenize_dataset
from sae_lens.training.activations_store import (
    ActivationsStore,
    _filter_buffer_acts,
    _get_special_token_ids,
    permute_together,
    validate_pretokenized_dataset_tokenizer,
)
from tests.helpers import build_sae_cfg, load_model_cached
def tokenize_with_bos(model: HookedTransformer, text: str) -> list[int]:
    assert model.tokenizer is not None
    assert model.tokenizer.bos_token_id is not None
    return [model.tokenizer.bos_token_id] + model.tokenizer.encode(text)  # type: ignore
# Define a new fixture for different configurations
@pytest.fixture(
    params=[
        {
            "model_name": "tiny-stories-1M",
            "dataset_path": "roneneldan/TinyStories",
            "hook_name": "blocks.1.hook_resid_pre",
            "hook_layer": 1,
            "d_in": 64,
            "normalize_activations": "expected_average_only_in",
        },
        {
            "model_name": "tiny-stories-1M",
            "dataset_path": "roneneldan/TinyStories",
            "hook_name": "blocks.1.attn.hook_z",
            "hook_layer": 1,
            "d_in": 64,
        },
        {
            "model_name": "gelu-2l",
            "dataset_path": "NeelNanda/c4-tokenized-2b",
            "hook_name": "blocks.1.hook_resid_pre",
            "hook_layer": 1,
            "d_in": 512,
            "context_size": 1024,
        },
        {
            "model_name": "gpt2",
            "dataset_path": "apollo-research/Skylion007-openwebtext-tokenizer-gpt2",
            "hook_name": "blocks.1.hook_resid_pre",
            "hook_layer": 1,
            "d_in": 768,
            "context_size": 1024,
        },
        {
            "model_name": "gpt2",
            "dataset_path": "Skylion007/openwebtext",
            "hook_name": "blocks.1.hook_resid_pre",
            "hook_layer": 1,
            "d_in": 768,
            "exclude_special_tokens": True,
        },
    ],
    ids=[
        "tiny-stories-1M-resid-pre",
        "tiny-stories-1M-attn-out",
        "gelu-2l-tokenized",
        "gpt2-tokenized",
        "gpt2",
    ],
)
def cfg(request: pytest.FixtureRequest) -> LanguageModelSAERunnerConfig:
    # This function will be called with each parameter set
    params = request.param
    return build_sae_cfg(**params)
@pytest.fixture
def model(cfg: LanguageModelSAERunnerConfig):
    return load_model_cached(cfg.model_name)
# tests involving loading real models / real datasets are very slow
# so do lots of stuff in this one test to make each load of model / data count
# poetry run py.test tests/training/test_activations_store.py -k 'test_activations_store__shapes_look_correct_with_real_models_and_datasets' --profile-svg -s
def test_activations_store__shapes_look_correct_with_real_models_and_datasets(
    cfg: LanguageModelSAERunnerConfig, model: HookedTransformer
):
    # --- first, test initialisation ---
    # config if you want to benchmark this:
    #
    # cfg.context_size = 1024
    # cfg.n_batches_in_buffer = 64
    # cfg.store_batch_size_prompts = 16
    store = ActivationsStore.from_config(model, cfg)
    if cfg.normalize_activations == "expected_average_only_in":
        store.estimated_norm_scaling_factor = 10.399
    assert store.model == model
    assert isinstance(store.dataset, IterableDataset)
    assert isinstance(store.iterable_sequences, Iterable)
    # the rest is in the dataloader.
    expected_size = (
        cfg.store_batch_size_prompts * cfg.context_size * cfg.n_batches_in_buffer // 2
    )
    assert store.storage_buffer.shape[1:] == (1, cfg.d_in)
    # if exluding special tokens, the buffer will be smaller
    assert store.storage_buffer.shape[0] <= expected_size
    # --- Next, get batch tokens and assert they look correct ---
    batch = store.get_batch_tokens()
    assert isinstance(batch, torch.Tensor)
    assert batch.shape == (
        store.store_batch_size_prompts,
        store.context_size,
    )
    assert batch.device == store.device
    # --- Next, get activations and assert they look correct ---
    activations = store.get_activations(batch)
    assert isinstance(activations, torch.Tensor)
    assert activations.shape == (
        store.store_batch_size_prompts,
        store.context_size,
        1,
        store.d_in,
    )
    assert activations.device == store.device
    # --- Next, get buffer and assert it looks correct ---
    n_batches_in_buffer = 3
    act_buffer, tok_buffer = store.get_buffer(n_batches_in_buffer)
    assert isinstance(act_buffer, torch.Tensor)
    assert isinstance(tok_buffer, torch.Tensor)
    buffer_size_expected = (
        store.store_batch_size_prompts * store.context_size * n_batches_in_buffer
    )
    assert act_buffer.shape == (buffer_size_expected, 1, store.d_in)
    assert tok_buffer.shape == (buffer_size_expected,)
    assert act_buffer.device == store.device
    assert tok_buffer.device == store.device
    # check the buffer norm
    if cfg.normalize_activations == "expected_average_only_in":
        assert torch.allclose(
            act_buffer.norm(dim=-1),
            np.sqrt(store.d_in) * torch.ones_like(act_buffer.norm(dim=-1)),
            atol=2,
        )
def test_activations_store__get_activations_head_hook(ts_model: HookedTransformer):
    cfg = build_sae_cfg(
        hook_name="blocks.0.attn.hook_q",
        hook_head_index=2,
        hook_layer=1,
        d_in=4,
    )
    activation_store_head_hook = ActivationsStore.from_config(ts_model, cfg)
    batch = activation_store_head_hook.get_batch_tokens()
    activations = activation_store_head_hook.get_activations(batch)
    assert isinstance(activations, torch.Tensor)
    assert activations.shape == (
        activation_store_head_hook.store_batch_size_prompts,
        activation_store_head_hook.context_size,
        1,
        activation_store_head_hook.d_in,
    )
    assert activations.device == activation_store_head_hook.device
def test_activations_store__get_activations__gives_same_results_with_hf_model_and_tlens_model():
    hf_model = load_model(
        model_class_name="AutoModelForCausalLM",
        model_name="gpt2",
        device="cpu",
    )
    tlens_model = HookedTransformer.from_pretrained_no_processing("gpt2", device="cpu")
    dataset = Dataset.from_list(
        [
            {"text": "hello world"},
        ]
        * 100
    )
    cfg = build_sae_cfg(hook_name="blocks.4.hook_resid_post", hook_layer=4, d_in=768)
    store_tlens = ActivationsStore.from_config(
        tlens_model, cfg, override_dataset=dataset
    )
    batch_tlens = store_tlens.get_batch_tokens()
    activations_tlens = store_tlens.get_activations(batch_tlens)
    cfg = build_sae_cfg(hook_name="transformer.h.4", hook_layer=4, d_in=768)
    store_hf = ActivationsStore.from_config(hf_model, cfg, override_dataset=dataset)
    batch_hf = store_hf.get_batch_tokens()
    activations_hf = store_hf.get_activations(batch_hf)
    assert torch.allclose(activations_hf, activations_tlens, atol=1e-3)
# 12 is divisible by the length of "hello world", 11 and 13 are not
@pytest.mark.parametrize("context_size", [11, 12, 13])
def test_activations_store__get_batch_tokens__fills_the_context_separated_by_bos(
    ts_model: HookedTransformer, context_size: int
):
    assert ts_model.tokenizer is not None
    dataset = Dataset.from_list(
        [
            {"text": "hello world"},
        ]
        * 100
    )
    cfg = build_sae_cfg(
        store_batch_size_prompts=2,
        context_size=context_size,
    )
    activation_store = ActivationsStore.from_config(
        ts_model, cfg, override_dataset=dataset
    )
    encoded_text = tokenize_with_bos(ts_model, "hello world")
    tokens = activation_store.get_batch_tokens()
    assert tokens.shape == (2, context_size)  # batch_size x context_size
    all_expected_tokens = (encoded_text * ceil(2 * context_size / len(encoded_text)))[
        : 2 * context_size
    ]
    expected_tokens1 = all_expected_tokens[:context_size]
    expected_tokens2 = all_expected_tokens[context_size:]
    if expected_tokens2[0] != ts_model.tokenizer.bos_token_id:
        expected_tokens2 = [ts_model.tokenizer.bos_token_id] + expected_tokens2[:-1]
    assert tokens[0].tolist() == expected_tokens1
    assert tokens[1].tolist() == expected_tokens2
def test_activations_store__iterate_raw_dataset_tokens__tokenizes_each_example_in_order(
    ts_model: HookedTransformer,
):
    tokenizer = ts_model.tokenizer
    assert tokenizer is not None
    cfg = build_sae_cfg()
    dataset = Dataset.from_list(
        [
            {"text": "hello world1"},
            {"text": "hello world2"},
            {"text": "hello world3"},
        ]
    )
    activation_store = ActivationsStore.from_config(
        ts_model, cfg, override_dataset=dataset
    )
    iterator = activation_store._iterate_raw_dataset_tokens()
    assert next(iterator).tolist() == tokenizer.encode("hello world1")
    assert next(iterator).tolist() == tokenizer.encode("hello world2")
    assert next(iterator).tolist() == tokenizer.encode("hello world3")
def test_activations_store__iterate_raw_dataset_tokens__can_handle_long_examples(
    ts_model: HookedTransformer,
):
    cfg = build_sae_cfg()
    dataset = Dataset.from_list(
        [
            {"text": " France" * 3000},
        ]
    )
    activation_store = ActivationsStore.from_config(
        ts_model, cfg, override_dataset=dataset
    )
    iterator = activation_store._iterate_raw_dataset_tokens()
    assert len(next(iterator).tolist()) == 3000
def test_activations_store_goes_to_cpu(ts_model: HookedTransformer):
    cfg = build_sae_cfg(act_store_device="cpu")
    activation_store = ActivationsStore.from_config(ts_model, cfg)
    activations = activation_store.next_batch()
    assert activations[0].device == torch.device("cpu")
    assert activations[1] is not None
    assert activations[1].device == torch.device("cpu")
@pytest.mark.skipif(not torch.cuda.is_available(), reason="No GPU to test on.")
def test_activations_store_with_model_on_gpu(ts_model: HookedTransformer):
    cfg = build_sae_cfg(act_store_device="cpu", device="cuda:0")
    activation_store = ActivationsStore.from_config(ts_model.to("cuda:0"), cfg)  # type: ignore
    activations = activation_store.next_batch()
    assert activations[0].device == torch.device("cpu")
    assert activations[1] is not None
    assert activations[1].device == torch.device("cpu")
@pytest.mark.skipif(not torch.cuda.is_available(), reason="No GPU to test on.")
def test_activations_store_moves_with_model(ts_model: HookedTransformer):
    # "with_model" resets to default so the second post_init in build_sae_cfg works
    cfg = build_sae_cfg(act_store_device="with_model", device="cuda:0")
    activation_store = ActivationsStore.from_config(ts_model.to("cuda:0"), cfg)  # type: ignore
    activations = activation_store.next_batch()
    assert activations[0].device == torch.device("cpu")
    assert activations[1] is not None
    assert activations[1].device == torch.device("cpu")
def test_activations_store_estimate_norm_scaling_factor(
    cfg: LanguageModelSAERunnerConfig, model: HookedTransformer
):
    # --- first, test initialisation ---
    # config if you want to benchmark this:
    #
    # cfg.context_size = 1024
    # cfg.n_batches_in_buffer = 64
    # cfg.store_batch_size_prompts = 16
    store = ActivationsStore.from_config(model, cfg)
    factor = store.estimate_norm_scaling_factor(n_batches_for_norm_estimate=10)
    assert isinstance(factor, float)
    assert store._storage_buffer is not None
    scaled_norm = store._storage_buffer[0].norm(dim=-1).mean() * factor
    assert scaled_norm == pytest.approx(np.sqrt(store.d_in), abs=5)
def test_activations_store___iterate_tokenized_sequences__yields_concat_and_batched_sequences(
    ts_model: HookedTransformer,
):
    tokenizer = ts_model.tokenizer
    assert tokenizer is not None
    cfg = build_sae_cfg(prepend_bos=True, context_size=5)
    dataset = Dataset.from_list(
        [
            {"text": "hello world1"},
            {"text": "hello world2"},
            {"text": "hello world3"},
        ]
    )
    activation_store = ActivationsStore.from_config(
        ts_model, cfg, override_dataset=dataset
    )
    iterator = activation_store._iterate_tokenized_sequences()
    expected = [
        tokenizer.bos_token_id,
        *tokenizer.encode("hello world1"),
        tokenizer.bos_token_id,
        *tokenizer.encode("hello world2"),
        tokenizer.bos_token_id,
        *tokenizer.encode("hello world3"),
    ]
    assert next(iterator).tolist() == expected[:5]
def test_activations_store___iterate_tokenized_sequences__yields_sequences_of_context_size(
    ts_model: HookedTransformer,
):
    tokenizer = ts_model.tokenizer
    assert tokenizer is not None
    cfg = build_sae_cfg(prepend_bos=True, context_size=5)
    dataset = Dataset.from_list(
        [
            {"text": "hello world1"},
            {"text": "hello world2"},
            {"text": "hello world3"},
        ]
        * 20
    )
    activation_store = ActivationsStore.from_config(
        ts_model, cfg, override_dataset=dataset
    )
    for toks in activation_store._iterate_tokenized_sequences():
        assert toks.shape == (5,)
def test_activations_store___iterate_tokenized_sequences__works_with_huggingface_models():
    hf_model = load_model(
        model_class_name="AutoModelForCausalLM",
        model_name="gpt2",
        device="cpu",
    )
    cfg = build_sae_cfg(prepend_bos=True, context_size=5)
    dataset = Dataset.from_list(
        [
            {"text": "hello world1"},
            {"text": "hello world2"},
            {"text": "hello world3"},
        ]
        * 20
    )
    activation_store = ActivationsStore.from_config(
        hf_model, cfg, override_dataset=dataset
    )
    for toks in activation_store._iterate_tokenized_sequences():
        assert toks.shape == (5,)
# We expect the code to work for context_size being less than or equal to the
# length of the dataset
@pytest.mark.parametrize(
    "context_size, expected_error",
    [(5, RuntimeWarning), (10, None), (15, ValueError)],
)
def test_activations_store__errors_on_context_size_mismatch(
    ts_model: HookedTransformer, context_size: int, expected_error: Optional[ValueError]
):
    tokenizer = ts_model.tokenizer
    assert tokenizer is not None
    cfg = build_sae_cfg(prepend_bos=True, context_size=context_size)
    dataset = Dataset.from_list(
        [
            {"text": "hello world1"},
            {"text": "hello world2"},
            {"text": "hello world3"},
        ]
        * 20
    )
    pretokenize_cfg = PretokenizeRunnerConfig(context_size=10)
    tokenized_dataset = pretokenize_dataset(dataset, tokenizer, cfg=pretokenize_cfg)
    # This context_size should raise an error or a warning if it mismatches the dataset size
    if expected_error is ValueError:
        with pytest.raises(expected_error):
            ActivationsStore.from_config(
                ts_model, cfg, override_dataset=tokenized_dataset
            )
    elif expected_error is RuntimeWarning:
        # If the context_size is smaller than the dataset size we should output a RuntimeWarning
        with pytest.warns(expected_error):
            ActivationsStore.from_config(
                ts_model, cfg, override_dataset=tokenized_dataset
            )
    else:
        # If the context_size is equal to the dataset size the function should pass
        ActivationsStore.from_config(ts_model, cfg, override_dataset=tokenized_dataset)
def test_activations_store__errors_on_negative_context_size():
    with pytest.raises(ValueError):
        # We should raise an error when the context_size is negative
        build_sae_cfg(prepend_bos=True, context_size=-1)
def test_activations_store___iterate_tokenized_sequences__yields_identical_results_with_and_without_pretokenizing(
    ts_model: HookedTransformer,
):
    tokenizer = ts_model.tokenizer
    assert tokenizer is not None
    cfg = build_sae_cfg(prepend_bos=True, context_size=5)
    dataset = Dataset.from_list(
        [
            {"text": "hello world1"},
            {"text": "hello world2"},
            {"text": "hello world3"},
        ]
        * 20
    )
    pretokenize_cfg = PretokenizeRunnerConfig(
        context_size=5,
        num_proc=1,
        shuffle=False,
        begin_batch_token="bos",
        sequence_separator_token="bos",
    )
    tokenized_dataset = pretokenize_dataset(dataset, tokenizer, cfg=pretokenize_cfg)
    activation_store = ActivationsStore.from_config(
        ts_model, cfg, override_dataset=dataset
    )
    tokenized_activation_store = ActivationsStore.from_config(
        ts_model, cfg, override_dataset=tokenized_dataset
    )
    seqs = [seq.tolist() for seq in activation_store._iterate_tokenized_sequences()]
    pretok_seqs = [
        seq.tolist()
        for seq in tokenized_activation_store._iterate_tokenized_sequences()
    ]
    assert seqs == pretok_seqs
def test_activation_store__errors_if_neither_dataset_nor_dataset_path(
    ts_model: HookedTransformer,
):
    cfg = build_sae_cfg(dataset_path="")
    example_ds = Dataset.from_list(
        [
            {"text": "hello world1"},
            {"text": "hello world2"},
            {"text": "hello world3"},
        ]
        * 20
    )
    ActivationsStore.from_config(ts_model, cfg, override_dataset=example_ds)
    with pytest.raises(ValueError):
        ActivationsStore.from_config(ts_model, cfg, override_dataset=None)
def test_validate_pretokenized_dataset_tokenizer_errors_if_the_tokenizer_doesnt_match_the_model():
    ds_path = "chanind/openwebtext-gpt2"
    model_tokenizer = HookedTransformer.from_pretrained("opt-125m").tokenizer
    assert model_tokenizer is not None
    with pytest.raises(ValueError):
        validate_pretokenized_dataset_tokenizer(ds_path, model_tokenizer)
def test_validate_pretokenized_dataset_tokenizer_runs_successfully_if_tokenizers_match(
    ts_model: HookedTransformer,
):
    ds_path = "chanind/openwebtext-gpt2"
    model_tokenizer = ts_model.tokenizer
    assert model_tokenizer is not None
    validate_pretokenized_dataset_tokenizer(ds_path, model_tokenizer)
def test_validate_pretokenized_dataset_tokenizer_does_nothing_if_the_dataset_is_not_created_by_sae_lens(
    ts_model: HookedTransformer,
):
    ds_path = "apollo-research/monology-pile-uncopyrighted-tokenizer-gpt2"
    model_tokenizer = ts_model.tokenizer
    assert model_tokenizer is not None
    validate_pretokenized_dataset_tokenizer(ds_path, model_tokenizer)
def test_validate_pretokenized_dataset_tokenizer_does_nothing_if_the_dataset_path_doesnt_exist(
    ts_model: HookedTransformer,
):
    ds_path = "blah/nonsense-1234"
    model_tokenizer = ts_model.tokenizer
    assert model_tokenizer is not None
    validate_pretokenized_dataset_tokenizer(ds_path, model_tokenizer)
def test_activations_store_respects_position_offsets(ts_model: HookedTransformer):
    cfg = build_sae_cfg(
        context_size=10,
        seqpos_slice=(2, 8),  # Only consider positions 2 to 7 (inclusive)
    )
    dataset = Dataset.from_list(
        [
            {"text": "This is a test sentence for slicing."},
        ]
        * 100
    )
    activation_store = ActivationsStore.from_config(
        ts_model, cfg, override_dataset=dataset
    )
    batch = activation_store.get_batch_tokens(1)
    activations = activation_store.get_activations(batch)
    assert batch.shape == (1, 10)  # Full context size
    assert activations.shape == (1, 6, 1, cfg.d_in)  # Only 6 positions (2 to 7)
@pytest.mark.parametrize(
    "params",
    [
        {
            "sae_kwargs": {
                "normalize_activations": "none",
            },
            "should_save": False,
        },
        {
            "sae_kwargs": {
                "normalize_activations": "expected_average_only_in",
            },
            "should_save": True,
        },
    ],
)
def test_activations_store_save_with_norm_scaling_factor(
    ts_model: HookedTransformer, params: dict[str, Any]
):
    cfg = build_sae_cfg(**params["sae_kwargs"])
    activation_store = ActivationsStore.from_config(ts_model, cfg)
    activation_store.set_norm_scaling_factor_if_needed()
    if params["sae_kwargs"]["normalize_activations"] == "expected_average_only_in":
        assert activation_store.estimated_norm_scaling_factor is not None
    with tempfile.NamedTemporaryFile() as temp_file:
        activation_store.save(temp_file.name)
        assert os.path.exists(temp_file.name)
        state_dict = load_file(temp_file.name)
        assert isinstance(state_dict, dict)
        if params["should_save"]:
            assert "estimated_norm_scaling_factor" in state_dict
            estimated_norm_scaling_factor = state_dict["estimated_norm_scaling_factor"]
            assert estimated_norm_scaling_factor.shape == ()
            assert (
                estimated_norm_scaling_factor.item()
                == activation_store.estimated_norm_scaling_factor
            )
        else:
            assert "estimated_norm_scaling_factor" not in state_dict
def test_get_special_token_ids():
    # Create a mock tokenizer with some special tokens
    class MockTokenizer:
        def __init__(self):
            self.bos_token_id = 1
            self.eos_token_id = 2
            self.pad_token_id = 3
            self.unk_token_id = None  # Test handling of None values
            self.special_tokens_map = {
                "additional_special_tokens": ["<extra_0>", "<extra_1>"],
                "mask_token": "<mask>",
            }
        def convert_tokens_to_ids(self, token: str) -> int:
            token_map = {"<extra_0>": 4, "<extra_1>": 5, "<mask>": 6}
            return token_map[token]
    tokenizer = MockTokenizer()
    special_tokens = _get_special_token_ids(tokenizer)  # type: ignore
    # Check that all expected token IDs are present
    assert set(special_tokens) == {1, 2, 3, 4, 5, 6}
    # Check that None values are properly handled
    assert None not in special_tokens
def test_get_special_token_ids_works_with_real_models(ts_model: HookedTransformer):
    special_tokens = _get_special_token_ids(ts_model.tokenizer)  # type: ignore
    assert special_tokens == [50256]
def test_activations_store_buffer_contains_token_ids(ts_model: HookedTransformer):
    """Test that the buffer contains both activations and token IDs."""
    cfg = build_sae_cfg(context_size=3, store_batch_size_prompts=5)
    dataset = Dataset.from_list([{"text": "hello world"}] * 100)
    store = ActivationsStore.from_config(ts_model, cfg, override_dataset=dataset)
    acts, token_ids = store.get_buffer(n_batches_in_buffer=2)
    assert acts.shape == (30, 1, 64)  # (batch_size x context_size x n_batches, 1, d_in)
    assert token_ids is not None
    assert token_ids.shape == (30,)  # (batch_size x context_size x n_batches,)
    expected_tokens = set(ts_model.to_tokens("hello world").squeeze().tolist())  # type: ignore
    assert set(token_ids.tolist()) == expected_tokens
def test_activations_store_buffer_shuffling(ts_model: HookedTransformer):
    """Test that buffer shuffling maintains alignment between acts and token_ids."""
    cfg = build_sae_cfg()
    dataset = Dataset.from_list([{"text": "hello world"}] * 100)
    # Get unshuffled buffer
    store = ActivationsStore.from_config(ts_model, cfg, override_dataset=dataset)
    acts_unshuffled_1, token_ids_unshuffled_1 = store.get_buffer(
        n_batches_in_buffer=2, shuffle=False
    )
    store = ActivationsStore.from_config(ts_model, cfg, override_dataset=dataset)
    acts_unshuffled_2, token_ids_unshuffled_2 = store.get_buffer(
        n_batches_in_buffer=2, shuffle=False
    )
    # Get shuffled buffer
    store = ActivationsStore.from_config(ts_model, cfg, override_dataset=dataset)
    acts_shuffled, token_ids_shuffled = store.get_buffer(
        n_batches_in_buffer=2, shuffle=True
    )
    assert token_ids_unshuffled_1 is not None
    assert token_ids_unshuffled_2 is not None
    assert token_ids_shuffled is not None
    assert torch.allclose(acts_unshuffled_1, acts_unshuffled_2)
    assert torch.allclose(token_ids_unshuffled_1, token_ids_unshuffled_2)
    assert not torch.allclose(acts_unshuffled_1, acts_shuffled)
    assert not torch.allclose(token_ids_unshuffled_1, token_ids_shuffled)
    assert set(token_ids_shuffled.tolist()) == set(token_ids_unshuffled_1.tolist())
@torch.no_grad()
def test_activations_store_storage_buffer_excludes_special_tokens(
    ts_model: HookedTransformer,
):
    hook_name = "blocks.0.hook_resid_post"
    base_cfg = build_sae_cfg(
        exclude_special_tokens=False,
        context_size=5,
        store_batch_size_prompts=2,
        hook_name=hook_name,
    )
    cfg = build_sae_cfg(
        exclude_special_tokens=True,
        context_size=5,
        store_batch_size_prompts=2,
        hook_name=hook_name,
    )
    dataset = Dataset.from_list([{"text": "hello world"}] * 100)
    _, cache = ts_model.run_with_cache(dataset[0]["text"])
    bos_act = cache[hook_name][0, 0]
    store_base = ActivationsStore.from_config(
        ts_model, base_cfg, override_dataset=dataset
    )
    store_exclude_special_tokens = ActivationsStore.from_config(
        ts_model, cfg, override_dataset=dataset
    )
    assert store_base.storage_buffer.shape[0] == 10
    assert store_exclude_special_tokens.storage_buffer.shape[0] < 10
    # bos act should be in the base buffer, but not in the exclude special tokens buffer
    assert (store_base.storage_buffer.squeeze() - bos_act).abs().sum(
        dim=-1
    ).min().item() == pytest.approx(0.0, abs=1e-5)
    assert (store_exclude_special_tokens.storage_buffer.squeeze() - bos_act).abs().sum(
        dim=-1
    ).min().item() != pytest.approx(0.0, abs=1e-5)
@torch.no_grad()
def test_activations_next_batch_excludes_special_tokens(
    ts_model: HookedTransformer,
):
    hook_name = "blocks.0.hook_resid_post"
    base_cfg = build_sae_cfg(
        exclude_special_tokens=False,
        context_size=5,
        store_batch_size_prompts=2,
        hook_name=hook_name,
        train_batch_size_tokens=5,
    )
    cfg = build_sae_cfg(
        exclude_special_tokens=True,
        context_size=5,
        store_batch_size_prompts=2,
        hook_name=hook_name,
        train_batch_size_tokens=5,
    )
    dataset = Dataset.from_list([{"text": "hello world"}] * 100)
    _, cache = ts_model.run_with_cache(dataset[0]["text"])
    bos_act = cache[hook_name][0, 0]
    store_base = ActivationsStore.from_config(
        ts_model, base_cfg, override_dataset=dataset
    )
    store_exclude_special_tokens = ActivationsStore.from_config(
        ts_model, cfg, override_dataset=dataset
    )
    batch_base = store_base.next_batch()
    batch_exclude_special_tokens = store_exclude_special_tokens.next_batch()
    assert batch_base.shape[0] == 5
    assert batch_exclude_special_tokens.shape[0] == 5
    # bos act should be in the base batch, but not in the exclude special tokens batch
    assert (batch_base.squeeze() - bos_act).abs().sum(
        dim=-1
    ).min().item() == pytest.approx(0.0, abs=1e-5)
    assert (batch_exclude_special_tokens.squeeze() - bos_act).abs().sum(
        dim=-1
    ).min().item() != pytest.approx(0.0, abs=1e-5)
def test_permute_together():
    """Test that permute_together correctly permutes tensors together."""
    # Create test tensors
    t1 = torch.tensor([[1, 2], [3, 4], [5, 6]])
    t2 = torch.tensor([10, 20, 30])
    t3 = torch.tensor([[100], [200], [300]])
    # Permute them together
    p1, p2, p3 = permute_together([t1, t2, t3])
    # Verify shapes are preserved
    assert p1.shape == t1.shape
    assert p2.shape == t2.shape
    assert p3.shape == t3.shape
    # Find the permutation that was applied by looking at t2
    perm = torch.zeros_like(t2, dtype=torch.long)
    for i in range(len(t2)):
        perm[i] = torch.where(p2 == t2[i])[0]
    # Verify all tensors used the same permutation
    for i in range(len(t2)):
        assert torch.allclose(p1[i], t1[perm[i]])
        assert torch.allclose(p2[i], t2[perm[i]])
        assert torch.allclose(p3[i], t3[perm[i]])
def test_permute_together_different_sizes_raises():
    """Test that permute_together raises an error if tensors have different first dimensions."""
    t1 = torch.tensor([[1, 2], [3, 4], [5, 6]])  # Shape (3, 2)
    t2 = torch.tensor([10, 20])  # Shape (2,)
    with pytest.raises(IndexError):
        permute_together([t1, t2])
def test_filter_buffer_acts_no_filtering():
    """Test that _filter_buffer_acts returns original activations when no filtering needed."""
    activations = torch.randn(10, 5)  # 10 tokens, 5 features
    tokens = None
    exclude_tokens = None
    filtered = _filter_buffer_acts((activations, tokens), exclude_tokens)
    assert torch.allclose(filtered, activations)
def test_filter_buffer_acts_with_filtering():
    """Test that _filter_buffer_acts correctly filters out specified tokens."""
    activations = torch.tensor(
        [
            [1.0, 2.0],  # token 0
            [3.0, 4.0],  # token 1
            [5.0, 6.0],  # token 2
            [7.0, 8.0],  # token 3
        ]
    )
    tokens = torch.tensor([0, 1, 0, 2])
    exclude_tokens = torch.tensor([0, 2])  # Filter out tokens 0 and 2
    filtered = _filter_buffer_acts((activations, tokens), exclude_tokens)
    expected = torch.tensor([[3.0, 4.0]])  # Only token 1 remains
    assert torch.allclose(filtered, expected)
def test_filter_buffer_acts_no_matches():
    """Test that _filter_buffer_acts handles case where no tokens match exclusion list."""
    activations = torch.tensor([[1.0, 2.0], [3.0, 4.0]])
    tokens = torch.tensor([0, 1])
    exclude_tokens = torch.tensor([2, 3])  # No matches
    filtered = _filter_buffer_acts((activations, tokens), exclude_tokens)
    assert torch.allclose(filtered, activations)  # All tokens kept
def test_filter_buffer_acts_all_filtered():
    """Test that _filter_buffer_acts handles case where all tokens are filtered."""
    activations = torch.tensor([[1.0, 2.0], [3.0, 4.0]])
    tokens = torch.tensor([0, 0])
    exclude_tokens = torch.tensor([0])  # All tokens filtered
    filtered = _filter_buffer_acts((activations, tokens), exclude_tokens)
    assert filtered.shape[0] == 0  # Empty tensor returned
    assert filtered.shape[1] == activations.shape[1]  # Feature dimension preserved

================
File: tests/training/test_cache_activations_runner.py
================
import dataclasses
import math
import os
from pathlib import Path
from typing import Any
import datasets
import pytest
import torch
from datasets import Dataset, load_dataset
from tqdm import trange
from transformer_lens import HookedTransformer
from sae_lens.cache_activations_runner import CacheActivationsRunner
from sae_lens.config import (
    DTYPE_MAP,
    CacheActivationsRunnerConfig,
    LanguageModelSAERunnerConfig,
)
from sae_lens.load_model import load_model
from sae_lens.training.activations_store import ActivationsStore
def _default_cfg(
    tmp_path: Path,
    batch_size: int = 16,
    context_size: int = 8,
    dataset_num_rows: int = 128,
    n_buffers: int = 4,
    shuffle: bool = False,
    **kwargs: Any,
) -> CacheActivationsRunnerConfig:
    d_in = 512
    dtype = "float32"
    device = (
        "cuda"
        if torch.cuda.is_available()
        else "mps"
        if torch.backends.mps.is_available()
        else "cpu"
    )
    sliced_context_size = kwargs.get("seqpos_slice")
    if sliced_context_size is not None:
        sliced_context_size = len(range(context_size)[slice(*sliced_context_size)])
    else:
        sliced_context_size = context_size
    # Calculate buffer_size_gb to achieve desired n_buffers
    bytes_per_token = d_in * DTYPE_MAP[dtype].itemsize
    tokens_per_buffer = math.ceil(dataset_num_rows * sliced_context_size / n_buffers)
    buffer_size_gb = (tokens_per_buffer * bytes_per_token) / 1_000_000_000
    total_training_tokens = dataset_num_rows * sliced_context_size
    cfg = CacheActivationsRunnerConfig(
        new_cached_activations_path=str(tmp_path),
        dataset_path="chanind/c4-10k-mini-tokenized-16-ctx-gelu-1l-tests",
        model_name="gelu-1l",
        hook_name="blocks.0.hook_mlp_out",
        hook_layer=0,
        ### Parameters
        training_tokens=total_training_tokens,
        model_batch_size=batch_size,
        buffer_size_gb=buffer_size_gb,
        context_size=context_size,
        ###
        d_in=d_in,
        shuffle=shuffle,
        prepend_bos=False,
        device=device,
        seed=42,
        dtype=dtype,
        **kwargs,
    )
    assert cfg.n_buffers == n_buffers
    assert cfg.n_seq_in_dataset == dataset_num_rows
    assert (
        cfg.n_tokens_in_buffer
        == cfg.n_batches_in_buffer * batch_size * sliced_context_size
    )
    return cfg
# The way to run this with this command:
# poetry run py.test tests/test_cache_activations_runner.py --profile-svg -s
def test_cache_activations_runner(tmp_path: Path):
    cfg = _default_cfg(tmp_path)
    runner = CacheActivationsRunner(cfg)
    dataset = runner.run()
    assert len(dataset) == cfg.n_buffers * (cfg.n_tokens_in_buffer // cfg.context_size)
    assert cfg.n_seq_in_dataset == len(dataset)
    assert dataset.column_names == [cfg.hook_name, "token_ids"]
    features = dataset.features
    assert isinstance(features[cfg.hook_name], datasets.Array2D)
    assert features[cfg.hook_name].shape == (cfg.context_size, cfg.d_in)
    assert isinstance(features["token_ids"], datasets.Sequence)
    assert features["token_ids"].length == cfg.context_size
def test_load_cached_activations(tmp_path: Path):
    cfg = _default_cfg(tmp_path)
    runner = CacheActivationsRunner(cfg)
    runner.run()
    model = HookedTransformer.from_pretrained(cfg.model_name)
    activations_store = ActivationsStore.from_config(model, cfg)
    for _ in range(cfg.n_buffers):
        buffer = activations_store.get_buffer(
            cfg.n_batches_in_buffer
        )  # Adjusted to use n_batches_in_buffer
        assert buffer[0].shape == (
            cfg.n_seq_in_buffer * cfg.context_size,
            1,
            cfg.d_in,
        )
        assert buffer[1] is not None
        assert buffer[1].shape == (cfg.n_seq_in_buffer * cfg.context_size,)
def test_activations_store_refreshes_dataset_when_it_runs_out(tmp_path: Path):
    context_size = 8
    n_batches_in_buffer = 4
    store_batch_size = 1
    total_training_steps = 4
    batch_size = 4
    total_training_tokens = total_training_steps * batch_size
    cache_cfg = _default_cfg(tmp_path)
    runner = CacheActivationsRunner(cache_cfg)
    runner.run()
    cfg = LanguageModelSAERunnerConfig(
        cached_activations_path=str(tmp_path),
        use_cached_activations=True,
        model_name="gelu-1l",
        hook_name="blocks.0.hook_mlp_out",
        hook_layer=0,
        d_in=512,
        dataset_path="",
        context_size=context_size,
        is_dataset_tokenized=True,
        prepend_bos=True,
        training_tokens=total_training_tokens // 2,
        train_batch_size_tokens=8,
        n_batches_in_buffer=n_batches_in_buffer,
        store_batch_size_prompts=store_batch_size,
        normalize_activations="none",
        device="cpu",
        seed=42,
        dtype="float16",
    )
    class MockModel:
        def to_tokens(self, *args: tuple[Any, ...], **kwargs: Any) -> torch.Tensor:
            return torch.ones(context_size)
        @property
        def W_E(self) -> torch.Tensor:
            return torch.ones(16, 16)
        @property
        def cfg(self) -> LanguageModelSAERunnerConfig:
            return cfg
    dataset = Dataset.from_list([{"text": "hello world1"}] * 64)
    model = MockModel()
    activations_store = ActivationsStore.from_config(
        model,  # type: ignore
        cfg,
        override_dataset=dataset,
    )
    for _ in range(16):
        _ = activations_store.get_batch_tokens(batch_size, raise_at_epoch_end=True)
    # assert a stop iteration is raised when we do one more get_batch_tokens
    pytest.raises(
        StopIteration,
        activations_store.get_batch_tokens,
        batch_size,
        raise_at_epoch_end=True,
    )
    # no errors are ever raised if we do not ask for raise_at_epoch_end
    for _ in range(32):
        _ = activations_store.get_batch_tokens(batch_size, raise_at_epoch_end=False)
def test_compare_cached_activations_end_to_end_with_ground_truth(tmp_path: Path):
    """
    Creates activations using CacheActivationsRunner and compares them with ground truth
    model.run_with_cache
    """
    torch.manual_seed(42)
    cfg = _default_cfg(tmp_path)
    runner = CacheActivationsRunner(cfg)
    activation_dataset = runner.run()
    activation_dataset.set_format("torch")
    dataset_acts: torch.Tensor = activation_dataset[cfg.hook_name]  # type: ignore
    model = HookedTransformer.from_pretrained(cfg.model_name, device=cfg.device)
    token_dataset: Dataset = load_dataset(
        cfg.dataset_path, split=f"train[:{cfg.n_seq_in_dataset}]"
    )  # type: ignore
    token_dataset.set_format("torch", device=cfg.device)
    ground_truth_acts = []
    for i in trange(0, cfg.n_seq_in_dataset, cfg.model_batch_size):
        tokens = token_dataset[i : i + cfg.model_batch_size]["input_ids"][
            :, : cfg.context_size
        ]
        _, layerwise_activations = model.run_with_cache(
            tokens,
            names_filter=[cfg.hook_name],
            stop_at_layer=cfg.hook_layer + 1,
        )
        acts = layerwise_activations[cfg.hook_name]
        ground_truth_acts.append(acts)
    ground_truth_acts = torch.cat(ground_truth_acts, dim=0).cpu()
    assert torch.allclose(ground_truth_acts, dataset_acts, rtol=1e-3, atol=5e-2)
def test_load_activations_store_with_nonexistent_dataset(tmp_path: Path):
    cfg = _default_cfg(tmp_path)
    model = load_model(
        model_class_name=cfg.model_class_name,
        model_name=cfg.model_name,
        device=cfg.device,
        model_from_pretrained_kwargs=cfg.model_from_pretrained_kwargs,
    )
    # Attempt to load from a non-existent dataset
    with pytest.raises(
        FileNotFoundError,
        match="is neither a `Dataset` directory nor a `DatasetDict` directory.",
    ):
        ActivationsStore.from_config(model, cfg)
def test_cache_activations_runner_with_nonempty_directory(tmp_path: Path):
    # Create a file to make the directory non-empty
    with open(tmp_path / "some_file.txt", "w") as f:
        f.write("test")
    with pytest.raises(
        Exception, match="is not empty. Please delete it or specify a different path."
    ):
        cfg = _default_cfg(tmp_path)
        runner = CacheActivationsRunner(cfg)
        runner.run()
def test_cache_activations_runner_with_incorrect_d_in(tmp_path: Path):
    correct_cfg = _default_cfg(tmp_path)
    # d_in different from hook
    wrong_d_in_cfg = CacheActivationsRunnerConfig(
        **dataclasses.asdict(correct_cfg),
    )
    wrong_d_in_cfg.d_in = 513
    runner = CacheActivationsRunner(wrong_d_in_cfg)
    with pytest.raises(
        RuntimeError,
        match=r"The expanded size of the tensor \(513\) must match the existing size \(512\) at non-singleton dimension 2.",
    ):
        runner.run()
def test_cache_activations_runner_load_dataset_with_incorrect_config(tmp_path: Path):
    correct_cfg = _default_cfg(tmp_path, context_size=16)
    runner = CacheActivationsRunner(correct_cfg)
    runner.run()
    model = runner.model
    # Context size different from dataset
    wrong_context_size_cfg = CacheActivationsRunnerConfig(
        **dataclasses.asdict(correct_cfg),
    )
    wrong_context_size_cfg.context_size = 13
    with pytest.raises(
        ValueError,
        match=r"Given dataset of shape \(16, 512\) does not match context_size \(13\) and d_in \(512\)",
    ):
        ActivationsStore.from_config(model, wrong_context_size_cfg)
    # d_in different from dataset
    wrong_d_in_cfg = CacheActivationsRunnerConfig(
        **dataclasses.asdict(correct_cfg),
    )
    wrong_d_in_cfg.d_in = 513
    with pytest.raises(
        ValueError,
        match=r"Given dataset of shape \(16, 512\) does not match context_size \(16\) and d_in \(513\)",
    ):
        ActivationsStore.from_config(model, wrong_d_in_cfg)
    # Incorrect hook_name
    wrong_hook_cfg = CacheActivationsRunnerConfig(
        **dataclasses.asdict(correct_cfg),
    )
    wrong_hook_cfg.hook_name = "blocks.1.hook_mlp_out"
    with pytest.raises(
        ValueError,
        match=r"Columns \['blocks.1.hook_mlp_out'\] not in the dataset. Current columns in the dataset: \['blocks.0.hook_mlp_out'\, 'token_ids'\]",
    ):
        ActivationsStore.from_config(model, wrong_hook_cfg)
def test_cache_activations_runner_with_valid_seqpos(tmp_path: Path):
    cfg = _default_cfg(
        tmp_path,
        batch_size=1,
        context_size=16,
        n_buffers=3,
        dataset_num_rows=12,
        seqpos_slice=(3, -3),
    )
    runner = CacheActivationsRunner(cfg)
    activation_dataset = runner.run()
    activation_dataset.set_format("torch", device=cfg.device)
    dataset_acts: torch.Tensor = activation_dataset[cfg.hook_name]  # type: ignore
    assert os.path.exists(tmp_path)
    # assert that there are n_buffer files in the directory.
    buffer_files = [
        f
        for f in os.listdir(tmp_path)
        if f.startswith("data-") and f.endswith(".arrow")
    ]
    assert len(buffer_files) == cfg.n_buffers
    for act in dataset_acts:
        # should be 16 - 3 - 3 = 10
        assert act.shape == (10, cfg.d_in)
def test_cache_activations_runner_stores_token_ids(tmp_path: Path):
    cfg = _default_cfg(tmp_path)
    runner = CacheActivationsRunner(cfg)
    dataset = runner.run()
    dataset.set_format("torch")
    assert "token_ids" in dataset.features
    assert dataset["token_ids"].shape[1] == cfg.context_size  # type: ignore
    assert dataset["blocks.0.hook_mlp_out"].shape[:2] == dataset["token_ids"].shape  # type: ignore
def test_cache_activations_runner_shuffling(tmp_path: Path):
    """Test that when shuffle=True, activations and token IDs remain aligned after shuffling."""
    # Create test dataset with arbitrary unique tokens
    tokenizer = HookedTransformer.from_pretrained("gelu-1l").tokenizer
    text = "".join(
        [
            " " + word[1:]
            for word in tokenizer.vocab  # type: ignore
            if word[0] == "Ġ" and word[1:].isascii() and word.isalnum()
        ]
    )
    dataset = Dataset.from_list([{"text": text}])
    # Create configs for unshuffled and shuffled versions
    base_cfg = _default_cfg(
        tmp_path / "base",
        context_size=3,
        batch_size=2,
        dataset_num_rows=8,
        shuffle=False,
    )
    shuffle_cfg = _default_cfg(
        tmp_path / "shuffled",
        context_size=3,
        batch_size=2,
        dataset_num_rows=8,
        shuffle=True,
    )
    # Get unshuffled dataset
    unshuffled_runner = CacheActivationsRunner(base_cfg, override_dataset=dataset)
    unshuffled_ds = unshuffled_runner.run()
    unshuffled_ds.set_format("torch")
    # Get shuffled dataset
    shuffled_runner = CacheActivationsRunner(shuffle_cfg, override_dataset=dataset)
    shuffled_ds = shuffled_runner.run()
    shuffled_ds.set_format("torch")
    # Get activations and tokens
    hook_name = base_cfg.hook_name
    unshuffled_acts: torch.Tensor = unshuffled_ds[hook_name]  # type: ignore
    unshuffled_tokens: torch.Tensor = unshuffled_ds["token_ids"]  # type: ignore
    shuffled_acts: torch.Tensor = shuffled_ds[hook_name]  # type: ignore
    shuffled_tokens: torch.Tensor = shuffled_ds["token_ids"]  # type: ignore
    # Verify shapes are preserved
    assert unshuffled_acts.shape == shuffled_acts.shape
    assert unshuffled_tokens.shape == shuffled_tokens.shape
    # Verify data is actually shuffled
    assert not (unshuffled_acts == shuffled_acts).all()
    assert not (unshuffled_tokens == shuffled_tokens).all()
    # For each token in unshuffled, find its position in shuffled
    # and verify the activations were moved together
    for i in range(len(unshuffled_tokens)):
        token = unshuffled_tokens[i]
        # Find where this token went in shuffled version
        shuffled_idx = torch.where(shuffled_tokens == token)[0][0]
        # Verify activations moved with it
        assert torch.allclose(unshuffled_acts[i], shuffled_acts[shuffled_idx])

================
File: tests/training/test_config.py
================
from dataclasses import fields
from typing import Type
import pytest
from sae_lens import __version__
from sae_lens.config import CacheActivationsRunnerConfig, LanguageModelSAERunnerConfig
from sae_lens.sae import SAEConfig
from sae_lens.training.training_sae import TrainingSAEConfig
TINYSTORIES_MODEL = "tiny-stories-1M"
TINYSTORIES_DATASET = "roneneldan/TinyStories"
def test_get_training_sae_cfg_dict_passes_scale_sparsity_penalty_by_decoder_norm():
    cfg = LanguageModelSAERunnerConfig(
        scale_sparsity_penalty_by_decoder_norm=True, normalize_sae_decoder=False
    )
    assert cfg.get_training_sae_cfg_dict()["scale_sparsity_penalty_by_decoder_norm"]
    cfg = LanguageModelSAERunnerConfig(
        scale_sparsity_penalty_by_decoder_norm=False, normalize_sae_decoder=False
    )
    assert not cfg.get_training_sae_cfg_dict()["scale_sparsity_penalty_by_decoder_norm"]
def test_get_training_sae_cfg_dict_has_all_relevant_options():
    cfg = LanguageModelSAERunnerConfig()
    cfg_dict = cfg.get_training_sae_cfg_dict()
    training_sae_opts = fields(TrainingSAEConfig)
    allowed_missing_fields = {"neuronpedia_id"}
    training_sae_field_names = {opt.name for opt in training_sae_opts}
    missing_fields = training_sae_field_names - allowed_missing_fields - cfg_dict.keys()
    assert missing_fields == set()
def test_get_base_sae_cfg_dict_has_all_relevant_options():
    cfg = LanguageModelSAERunnerConfig()
    cfg_dict = cfg.get_base_sae_cfg_dict()
    sae_opts = fields(SAEConfig)
    allowed_missing_fields = {"neuronpedia_id"}
    sae_field_names = {opt.name for opt in sae_opts}
    missing_fields = sae_field_names - allowed_missing_fields - cfg_dict.keys()
    assert missing_fields == set()
def test_sae_training_runner_config_runs_with_defaults():
    """
    Helper to create a mock instance of LanguageModelSAERunnerConfig.
    """
    # Create a mock object with the necessary attributes
    _ = LanguageModelSAERunnerConfig()
    assert True
def test_sae_training_runner_config_total_training_tokens():
    """
    Helper to create a mock instance of LanguageModelSAERunnerConfig.
    """
    # Create a mock object with the necessary attributes
    cfg = LanguageModelSAERunnerConfig()
    assert cfg.total_training_tokens == 2000000
def test_sae_training_runner_config_total_training_steps():
    """
    Helper to create a mock instance of LanguageModelSAERunnerConfig.
    """
    # Create a mock object with the necessary attributes
    cfg = LanguageModelSAERunnerConfig()
    assert cfg.total_training_steps == 488
def test_sae_training_runner_config_get_sae_base_parameters():
    """
    Helper to create a mock instance of LanguageModelSAERunnerConfig.
    """
    # Create a mock object with the necessary attributes
    cfg = LanguageModelSAERunnerConfig()
    expected_config = {
        "architecture": "standard",
        "d_in": 512,
        "d_sae": 2048,
        "activation_fn_str": "relu",
        "activation_fn_kwargs": {},
        "apply_b_dec_to_input": True,
        "dtype": "float32",
        "model_name": "gelu-2l",
        "hook_name": "blocks.0.hook_mlp_out",
        "hook_layer": 0,
        "hook_head_index": None,
        "device": "cpu",
        "context_size": 128,
        "prepend_bos": True,
        "finetuning_scaling_factor": False,
        "dataset_path": "",
        "dataset_trust_remote_code": True,
        "sae_lens_training_version": str(__version__),
        "normalize_activations": "none",
        "model_from_pretrained_kwargs": {
            "center_writing_weights": False,
        },
        "seqpos_slice": (None,),
    }
    assert expected_config == cfg.get_base_sae_cfg_dict()
def test_sae_training_runner_config_raises_error_if_resume_true():
    """
    Helper to create a mock instance of LanguageModelSAERunnerConfig.
    """
    # Create a mock object with the necessary attributes
    with pytest.raises(ValueError):
        _ = LanguageModelSAERunnerConfig(resume=True)
    assert True
def test_sae_training_runner_config_raises_error_if_d_sae_and_expansion_factor_not_none():
    with pytest.raises(ValueError):
        _ = LanguageModelSAERunnerConfig(d_sae=128, expansion_factor=4)
    assert True
def test_sae_training_runner_config_expansion_factor():
    cfg = LanguageModelSAERunnerConfig()
    assert cfg.expansion_factor == 4
test_cases_for_seqpos = [
    ((None, 10, -1), ValueError),
    ((None, 10, 0), ValueError),
    ((5, 5, None), ValueError),
    ((6, 3, None), ValueError),
]
@pytest.mark.parametrize("seqpos_slice, expected_error", test_cases_for_seqpos)
def test_sae_training_runner_config_seqpos(
    seqpos_slice: tuple[int, int], expected_error: Type[BaseException]
):
    context_size = 10
    with pytest.raises(expected_error):
        LanguageModelSAERunnerConfig(
            seqpos_slice=seqpos_slice,
            context_size=context_size,
        )
@pytest.mark.parametrize("seqpos_slice, expected_error", test_cases_for_seqpos)
def test_cache_activations_runner_config_seqpos(
    seqpos_slice: tuple[int, int],
    expected_error: Type[BaseException],
):
    with pytest.raises(expected_error):
        CacheActivationsRunnerConfig(
            dataset_path="",
            model_name="",
            model_batch_size=1,
            hook_name="",
            hook_layer=0,
            d_in=1,
            training_tokens=100,
            context_size=10,
            seqpos_slice=seqpos_slice,
        )
def test_topk_architecture_requires_topk_activation():
    with pytest.raises(
        ValueError, match="If using topk architecture, activation_fn must be topk."
    ):
        LanguageModelSAERunnerConfig(architecture="topk", activation_fn="relu")
def test_topk_architecture_requires_k_parameter():
    with pytest.raises(
        ValueError,
        match="activation_fn_kwargs.k must be provided for topk architecture.",
    ):
        LanguageModelSAERunnerConfig(
            architecture="topk", activation_fn="topk", activation_fn_kwargs={}
        )
def test_topk_architecture_sets_topk_defaults():
    cfg = LanguageModelSAERunnerConfig(architecture="topk")
    assert cfg.activation_fn == "topk"
    assert cfg.activation_fn_kwargs == {"k": 100}

================
File: tests/training/test_gated_sae.py
================
import pytest
import torch
from sae_lens.training.training_sae import TrainingSAE
from tests.helpers import build_sae_cfg
def test_gated_sae_initialization():
    cfg = build_sae_cfg()
    setattr(cfg, "architecture", "gated")
    sae = TrainingSAE.from_dict(cfg.get_training_sae_cfg_dict())
    assert sae.W_enc.shape == (cfg.d_in, cfg.d_sae)
    assert sae.W_dec.shape == (cfg.d_sae, cfg.d_in)
    # assert sae.b_enc.shape == (cfg.d_sae,)
    assert sae.b_mag.shape == (cfg.d_sae,)
    assert sae.b_gate.shape == (cfg.d_sae,)
    assert sae.r_mag.shape == (cfg.d_sae,)
    assert sae.b_dec.shape == (cfg.d_in,)
    assert isinstance(sae.activation_fn, torch.nn.ReLU)
    assert sae.device == torch.device("cpu")
    assert sae.dtype == torch.float32
    # biases
    assert torch.allclose(sae.b_dec, torch.zeros_like(sae.b_dec), atol=1e-6)
    assert torch.allclose(sae.b_mag, torch.zeros_like(sae.b_mag), atol=1e-6)
    assert torch.allclose(sae.b_gate, torch.zeros_like(sae.b_gate), atol=1e-6)
    # check if the decoder weight norm is 1 by default
    assert torch.allclose(
        sae.W_dec.norm(dim=1), torch.ones_like(sae.W_dec.norm(dim=1)), atol=1e-6
    )
def test_gated_sae_encoding():
    cfg = build_sae_cfg()
    setattr(cfg, "architecture", "gated")
    sae = TrainingSAE.from_dict(cfg.get_training_sae_cfg_dict())
    batch_size = 32
    d_in = sae.cfg.d_in
    d_sae = sae.cfg.d_sae
    x = torch.randn(batch_size, d_in)
    feature_acts, hidden_pre = sae.encode_with_hidden_pre_gated(x)
    assert feature_acts.shape == (batch_size, d_sae)
    assert hidden_pre.shape == (batch_size, d_sae)
    # Check the gating mechanism
    gating_pre_activation = x @ sae.W_enc + sae.b_gate
    active_features = (gating_pre_activation > 0).float()
    magnitude_pre_activation = x @ (sae.W_enc * sae.r_mag.exp()) + sae.b_mag
    feature_magnitudes = sae.activation_fn(magnitude_pre_activation)
    expected_feature_acts = active_features * feature_magnitudes
    assert torch.allclose(feature_acts, expected_feature_acts, atol=1e-6)
def test_gated_sae_loss():
    cfg = build_sae_cfg()
    setattr(cfg, "architecture", "gated")
    sae = TrainingSAE.from_dict(cfg.get_training_sae_cfg_dict())
    batch_size = 32
    d_in = sae.cfg.d_in
    x = torch.randn(batch_size, d_in)
    train_step_output = sae.training_forward_pass(
        sae_in=x,
        current_l1_coefficient=sae.cfg.l1_coefficient,
    )
    assert train_step_output.sae_out.shape == (batch_size, d_in)
    assert train_step_output.feature_acts.shape == (batch_size, sae.cfg.d_sae)
    sae_in_centered = x - sae.b_dec
    via_gate_feature_magnitudes = torch.relu(sae_in_centered @ sae.W_enc + sae.b_gate)
    preactivation_l1_loss = (
        sae.cfg.l1_coefficient * torch.sum(via_gate_feature_magnitudes, dim=-1).mean()
    )
    via_gate_reconstruction = (
        via_gate_feature_magnitudes @ sae.W_dec.detach() + sae.b_dec.detach()
    )
    aux_reconstruction_loss = torch.sum(
        (via_gate_reconstruction - x) ** 2, dim=-1
    ).mean()
    expected_loss = (
        train_step_output.losses["mse_loss"]
        + preactivation_l1_loss
        + aux_reconstruction_loss
    )
    assert (
        pytest.approx(train_step_output.loss.item(), rel=1e-3) == expected_loss.item()
    )
def test_gated_sae_forward_pass():
    cfg = build_sae_cfg()
    setattr(cfg, "architecture", "gated")
    sae = TrainingSAE.from_dict(cfg.get_training_sae_cfg_dict())
    batch_size = 32
    d_in = sae.cfg.d_in
    x = torch.randn(batch_size, d_in)
    sae_out = sae(x)
    assert sae_out.shape == (batch_size, d_in)
def test_gated_sae_training_forward_pass():
    cfg = build_sae_cfg()
    setattr(cfg, "architecture", "gated")
    sae = TrainingSAE.from_dict(cfg.get_training_sae_cfg_dict())
    batch_size = 32
    d_in = sae.cfg.d_in
    x = torch.randn(batch_size, d_in)
    train_step_output = sae.training_forward_pass(
        sae_in=x,
        current_l1_coefficient=sae.cfg.l1_coefficient,
    )
    assert train_step_output.sae_out.shape == (batch_size, d_in)
    assert train_step_output.feature_acts.shape == (batch_size, sae.cfg.d_sae)
    # Detach the loss tensor and convert to numpy for comparison
    detached_loss = train_step_output.loss.detach().cpu().numpy()
    expected_loss = (
        (
            train_step_output.losses["mse_loss"]
            + train_step_output.losses["l1_loss"]
            + train_step_output.losses["auxiliary_reconstruction_loss"]
        )
        .detach()  # type: ignore
        .cpu()
        .numpy()
    )
    assert pytest.approx(detached_loss, rel=1e-3) == expected_loss

================
File: tests/training/test_jumprelu_sae.py
================
import pytest
import torch
from sae_lens.training.training_sae import JumpReLU, TrainingSAE
from tests.helpers import build_sae_cfg
def test_jumprelu_sae_encoding():
    cfg = build_sae_cfg(architecture="jumprelu")
    sae = TrainingSAE.from_dict(cfg.get_training_sae_cfg_dict())
    batch_size = 32
    d_in = sae.cfg.d_in
    d_sae = sae.cfg.d_sae
    x = torch.randn(batch_size, d_in)
    feature_acts, hidden_pre = sae.encode_with_hidden_pre_jumprelu(x)
    assert feature_acts.shape == (batch_size, d_sae)
    assert hidden_pre.shape == (batch_size, d_sae)
    # Check the JumpReLU thresholding
    sae_in = sae.process_sae_in(x)
    expected_hidden_pre = sae_in @ sae.W_enc + sae.b_enc
    threshold = torch.exp(sae.log_threshold)
    expected_feature_acts = JumpReLU.apply(
        expected_hidden_pre, threshold, sae.bandwidth
    )
    assert torch.allclose(feature_acts, expected_feature_acts, atol=1e-6)  # type: ignore
def test_jumprelu_sae_training_forward_pass():
    cfg = build_sae_cfg(architecture="jumprelu")
    sae = TrainingSAE.from_dict(cfg.get_training_sae_cfg_dict())
    batch_size = 32
    d_in = sae.cfg.d_in
    x = torch.randn(batch_size, d_in)
    train_step_output = sae.training_forward_pass(
        sae_in=x,
        current_l1_coefficient=sae.cfg.l1_coefficient,
    )
    assert train_step_output.sae_out.shape == (batch_size, d_in)
    assert train_step_output.feature_acts.shape == (batch_size, sae.cfg.d_sae)
    assert (
        pytest.approx(train_step_output.loss.detach(), rel=1e-3)
        == (
            train_step_output.losses["mse_loss"] + train_step_output.losses["l0_loss"]
        ).item()  # type: ignore
    )
    expected_mse_loss = (
        (torch.pow((train_step_output.sae_out - x.float()), 2))
        .sum(dim=-1)
        .mean()
        .detach()
        .float()
    )
    assert (
        pytest.approx(train_step_output.losses["mse_loss"].item()) == expected_mse_loss  # type: ignore
    )

================
File: tests/training/test_l1_scheduler.py
================
from sae_lens.training.optim import L1Scheduler
from tests.helpers import build_sae_cfg
def test_l1_scheduler_initialization():
    cfg = build_sae_cfg(
        l1_coefficient=5,
        training_tokens=100 * 4,  # train batch size (so 100 steps)
        l1_warm_up_steps=10,
    )
    l1_scheduler = L1Scheduler(
        l1_warm_up_steps=cfg.l1_warm_up_steps,  # type: ignore
        total_steps=cfg.training_tokens // cfg.train_batch_size_tokens,
        final_l1_coefficient=cfg.l1_coefficient,
    )
    assert cfg.l1_coefficient == 5
    assert (
        l1_scheduler.current_l1_coefficient == 0
    )  # the l1 coefficient is set to 0, to begin warm up.
    # over 10 steps, we should get to the final value of 5
    for i in range(10):
        l1_scheduler.step()
        assert l1_scheduler.current_l1_coefficient == 5 * (1 + i) / 10
def test_l1_scheduler_initialization_no_warmup():
    cfg = build_sae_cfg(
        l1_coefficient=5,
        training_tokens=100 * 4,  # train batch size (so 100 steps)
        l1_warm_up_steps=0,
    )
    l1_scheduler = L1Scheduler(
        l1_warm_up_steps=cfg.l1_warm_up_steps,  # type: ignore
        total_steps=cfg.training_tokens // cfg.train_batch_size_tokens,
        final_l1_coefficient=cfg.l1_coefficient,
    )
    assert cfg.l1_coefficient == 5
    assert (
        l1_scheduler.current_l1_coefficient == 5
    )  # the l1 coefficient is set to 0, to begin warm up.
    # over 10 steps, we should get to the final value of 5
    for _ in range(10):
        l1_scheduler.step()
        assert l1_scheduler.current_l1_coefficient == l1_scheduler.final_l1_coefficient

================
File: tests/training/test_load_model.py
================
import pytest
import torch
from mamba_lens import HookedMamba
from transformer_lens import HookedTransformer
from transformers import AutoModelForCausalLM, AutoTokenizer
from sae_lens.load_model import HookedProxyLM, _extract_logits_from_output, load_model
@pytest.fixture
def gpt2_proxy_model():
    return load_model(
        model_class_name="AutoModelForCausalLM",
        model_name="gpt2",
        device="cpu",
    )
def test_load_model_works_with_mamba():
    model = load_model(
        model_class_name="HookedMamba",
        model_name="state-spaces/mamba-370m",
        device="cpu",
    )
    assert isinstance(model, HookedMamba)
def test_load_model_works_without_model_kwargs():
    model = load_model(
        model_class_name="HookedTransformer",
        model_name="pythia-14m",
        device="cpu",
    )
    assert isinstance(model, HookedTransformer)
    assert model.cfg.checkpoint_index is None
# TODO: debug why this is suddenly failing on CI. It may resolve itself in the future.
@pytest.mark.skip(
    reason="This is failing on CI but not locally due to huggingface headers."
)
def test_load_model_works_with_model_kwargs():
    model = load_model(
        model_class_name="HookedTransformer",
        model_name="pythia-14m",
        device="cpu",
        model_from_pretrained_kwargs={"checkpoint_index": 0},
    )
    assert isinstance(model, HookedTransformer)
    assert model.cfg.checkpoint_index == 0
def test_load_model_with_generic_huggingface_lm():
    model = load_model(
        model_class_name="AutoModelForCausalLM",
        model_name="gpt2",
        device="cpu",
    )
    assert isinstance(model, HookedProxyLM)
def test_HookedProxyLM_gives_same_cached_states_as_original_implementation():
    hf_model = AutoModelForCausalLM.from_pretrained("gpt2")
    tokenizer = AutoTokenizer.from_pretrained("gpt2")
    hooked_model = HookedProxyLM(hf_model, tokenizer)
    input_ids = tokenizer.encode("hi", return_tensors="pt")
    proxy_logits, cache = hooked_model.run_with_cache(input_ids)
    hf_output = hf_model(input_ids, output_hidden_states=True)
    assert torch.allclose(proxy_logits, hf_output.logits)
    for i in range(len(hf_output.hidden_states) - 2):
        assert torch.allclose(
            cache[f"transformer.h.{i}"], hf_output.hidden_states[i + 1]
        )
def test_HookedProxyLM_gives_same_cached_states_as_tlens_implementation(
    gpt2_proxy_model: HookedProxyLM,
):
    tlens_model = HookedTransformer.from_pretrained_no_processing("gpt2", device="cpu")
    input_ids = tlens_model.to_tokens("hi")
    hf_cache = gpt2_proxy_model.run_with_cache(input_ids)[1]
    tlens_cache = tlens_model.run_with_cache(input_ids)[1]
    for i in range(12):
        assert torch.allclose(
            hf_cache[f"transformer.h.{i}"],
            tlens_cache[f"blocks.{i}.hook_resid_post"],
            atol=1e-3,
        )
def test_HookedProxyLM_forward_gives_same_output_as_tlens(
    gpt2_proxy_model: HookedProxyLM,
):
    tlens_model = HookedTransformer.from_pretrained("gpt2", device="cpu")
    batch_tokens = tlens_model.to_tokens("hi there")
    tlens_output = tlens_model(batch_tokens, return_type="both", loss_per_token=True)
    hf_output = gpt2_proxy_model(batch_tokens, return_type="both", loss_per_token=True)
    # Seems like tlens removes the means before softmaxing
    hf_logits_normed = hf_output[0] - hf_output[0].mean(dim=-1, keepdim=True)
    assert torch.allclose(tlens_output[0], hf_logits_normed, atol=1e-3)
    assert torch.allclose(tlens_output[1], hf_output[1], atol=1e-3)
def test_extract_logits_from_output_works_with_multiple_return_types():
    model = AutoModelForCausalLM.from_pretrained("gpt2")
    tokenizer = AutoTokenizer.from_pretrained("gpt2")
    tokens = tokenizer.encode("hi there", return_tensors="pt")
    out_dict = model(tokens, return_dict=True)
    out_tuple = model(tokens, return_dict=False)
    logits_dict = _extract_logits_from_output(out_dict)
    logits_tuple = _extract_logits_from_output(out_tuple)
    assert torch.allclose(logits_dict, logits_tuple)
def test_HookedProxyLM_to_tokens_gives_same_output_as_tlens(
    gpt2_proxy_model: HookedProxyLM,
):
    tlens_model = HookedTransformer.from_pretrained("gpt2", device="cpu")
    tl_tokens = tlens_model.to_tokens(
        "hi there", prepend_bos=False, truncate=False, move_to_device=False
    )
    hf_tokens = gpt2_proxy_model.to_tokens(
        "hi there", prepend_bos=False, truncate=False, move_to_device=False
    )
    assert torch.allclose(tl_tokens, hf_tokens)

================
File: tests/training/test_optim.py
================
from typing import Any
import pytest
import torch
from torch.optim import Adam
from torch.optim.lr_scheduler import (
    CosineAnnealingLR,
    CosineAnnealingWarmRestarts,
    LRScheduler,
)
from sae_lens.training.optim import get_lr_scheduler
LR = 0.1
@pytest.fixture
def optimizer():
    return Adam([torch.tensor(1.0)], lr=LR)
def step_times(num: int, optimizer: Adam, scheduler: LRScheduler):
    for _ in range(num):
        step(optimizer, scheduler)
def step(optimizer: Adam, scheduler: LRScheduler):
    optimizer.step()
    scheduler.step()
def test_get_scheduler_errors_on_uknown_scheduler(optimizer: Adam):
    with pytest.raises(ValueError, match="Unsupported scheduler: unknown"):
        get_lr_scheduler(
            "unknown",
            optimizer,
            lr=LR,
            training_steps=10,
            warm_up_steps=0,
            decay_steps=0,
            lr_end=0.0,
            num_cycles=1,
        )
def test_get_scheduler_constant(optimizer: Adam):
    scheduler = get_lr_scheduler(
        "constant",
        optimizer,
        lr=LR,
        training_steps=4,
        warm_up_steps=0,
        decay_steps=0,
        lr_end=0.0,
        num_cycles=1,
    )
    assert scheduler.get_last_lr() == [0.1]
    step_times(3, optimizer, scheduler)
    assert scheduler.get_last_lr() == [0.1]
def test_get_scheduler_constantwithwarmup(optimizer: Adam):
    scheduler = get_lr_scheduler(
        "constant",
        optimizer,
        lr=LR,
        warm_up_steps=2,
        training_steps=4,
        decay_steps=0,
        lr_end=0.0,
        num_cycles=1,
    )
    assert scheduler.get_last_lr() == [pytest.approx(0.05)]
    step(optimizer, scheduler)
    assert scheduler.get_last_lr() == [0.1]
    step_times(3, optimizer, scheduler)
    assert scheduler.get_last_lr() == [0.1]
def test_get_scheduler_linearwarmupdecay(optimizer: Adam):
    scheduler = get_lr_scheduler(
        "constant",
        optimizer,
        lr=LR,
        warm_up_steps=2,
        decay_steps=4,
        training_steps=6,
        lr_end=0.0,
        num_cycles=1,
    )
    # first, ramp up for 2 steps
    assert scheduler.get_last_lr() == [0.05]
    step(optimizer, scheduler)
    assert scheduler.get_last_lr() == [0.1]
    step(optimizer, scheduler)
    # next, ramp down for 4 steps
    assert scheduler.get_last_lr() == [0.1]
    step(optimizer, scheduler)
    assert scheduler.get_last_lr() == [pytest.approx(0.075)]
    step(optimizer, scheduler)
    assert scheduler.get_last_lr() == [pytest.approx(0.05)]
    step(optimizer, scheduler)
    assert scheduler.get_last_lr() == [pytest.approx(0.025)]
    step(optimizer, scheduler)
    assert scheduler.get_last_lr() == [0.0]
def test_get_scheduler_errors_if_lr_end_is_0_and_decay_is_set(optimizer: Adam):
    with pytest.raises(ValueError, match="Cannot have decay_steps with lr_end=0.0"):
        get_lr_scheduler(
            "cosineannealing",
            optimizer,
            lr=LR,
            lr_end=0.0,
            warm_up_steps=0,
            decay_steps=2,
            training_steps=6,
            num_cycles=1,
        )
def test_get_scheduler_cosineannealing(optimizer: Adam):
    scheduler: Any = get_lr_scheduler(
        "cosineannealing",
        optimizer,
        lr=LR,
        training_steps=4,
        lr_end=0.05,
        warm_up_steps=0,
        decay_steps=0,
        num_cycles=1,
    )
    assert len(scheduler._schedulers) == 1
    main_scheduler = scheduler._schedulers[0]
    assert isinstance(main_scheduler, CosineAnnealingLR)
    assert main_scheduler.T_max == 4
    assert main_scheduler.eta_min == 0.05
def test_get_scheduler_cosineannealing_with_warmup_and_decay():
    lr_end = 0.01
    optimizer = Adam([torch.tensor(1.0)], lr=LR)
    scheduler = get_lr_scheduler(
        "cosineannealing",
        optimizer,
        lr=LR,
        warm_up_steps=2,
        training_steps=8,
        decay_steps=2,
        lr_end=lr_end,
        num_cycles=1,
    )
    # first, ramp up for 2 steps
    assert scheduler.get_last_lr() == [0.05]
    step(optimizer, scheduler)
    assert scheduler.get_last_lr() == [0.1]
    step(optimizer, scheduler)
    # From here on, it should match CosineAnnealingLR
    new_optimizer = Adam([torch.tensor(1.0)], lr=LR)
    cos_scheduler = CosineAnnealingLR(new_optimizer, T_max=4, eta_min=lr_end)  # type: ignore
    step(optimizer, scheduler)
    step(new_optimizer, cos_scheduler)
    assert scheduler.get_last_lr() == pytest.approx(cos_scheduler.get_last_lr())
    step(optimizer, scheduler)
    step(new_optimizer, cos_scheduler)
    assert scheduler.get_last_lr() == pytest.approx(cos_scheduler.get_last_lr())
    step(optimizer, scheduler)
    step(new_optimizer, cos_scheduler)
    assert scheduler.get_last_lr() == pytest.approx(cos_scheduler.get_last_lr())
    step(optimizer, scheduler)
    step(new_optimizer, cos_scheduler)
    assert scheduler.get_last_lr() == pytest.approx(cos_scheduler.get_last_lr())
    assert scheduler.get_last_lr() == [lr_end]
    # now, decay to 0 in 2 steps
    step(optimizer, scheduler)
    assert scheduler.get_last_lr() == [pytest.approx(0.005)]
    step(optimizer, scheduler)
    assert scheduler.get_last_lr() == [pytest.approx(0.0)]
def test_get_scheduler_cosineannealingwarmrestarts(optimizer: Adam):
    scheduler: Any = get_lr_scheduler(
        "cosineannealingwarmrestarts",
        optimizer,
        lr=LR,
        training_steps=8,
        lr_end=0.05,
        num_cycles=2,
        warm_up_steps=0,
        decay_steps=0,
    )
    assert len(scheduler._schedulers) == 1
    main_scheduler = scheduler._schedulers[0]
    assert isinstance(main_scheduler, CosineAnnealingWarmRestarts)
    assert main_scheduler.T_0 == 4
    assert main_scheduler.eta_min == 0.05

================
File: tests/training/test_pretokenize_runner.py
================
import json
from pathlib import Path
from typing import Any, cast
import pytest
from datasets import Dataset, IterableDataset
from transformers import AutoTokenizer, PreTrainedTokenizerBase
from sae_lens import __version__
from sae_lens.config import PretokenizeRunnerConfig
from sae_lens.pretokenize_runner import PretokenizeRunner, pretokenize_dataset
@pytest.fixture
def ts_tokenizer() -> PreTrainedTokenizerBase:
    return AutoTokenizer.from_pretrained("roneneldan/TinyStories-1M")
def test_pretokenize_dataset_concatenates_text_until_context_size(
    ts_tokenizer: PreTrainedTokenizerBase,
):
    dataset = Dataset.from_list([{"text": "hello world"}] * 30)
    cfg = PretokenizeRunnerConfig(
        context_size=10,
        num_proc=1,
        shuffle=False,
        begin_batch_token=None,
        sequence_separator_token=None,
        begin_sequence_token=None,
    )
    tokenized_dataset = cast(Any, pretokenize_dataset(dataset, ts_tokenizer, cfg))
    assert tokenized_dataset["input_ids"].shape[1] == cfg.context_size
    assert (
        ts_tokenizer.decode(tokenized_dataset["input_ids"][0])
        == "hello worldhello worldhello worldhello worldhello world"
    )
def test_pretokenize_dataset_can_add_bos_tokens_to_the_start_of_each_batch(
    ts_tokenizer: PreTrainedTokenizerBase,
):
    dataset = Dataset.from_list([{"text": "hello world"}] * 30)
    cfg = PretokenizeRunnerConfig(
        context_size=10,
        num_proc=1,
        shuffle=False,
        begin_batch_token="bos",
        sequence_separator_token=None,
        begin_sequence_token=None,
    )
    tokenized_dataset = cast(Any, pretokenize_dataset(dataset, ts_tokenizer, cfg))
    assert tokenized_dataset["input_ids"].shape[1] == cfg.context_size
    assert (
        ts_tokenizer.decode(tokenized_dataset["input_ids"][0])
        == "<|endoftext|>hello worldhello worldhello worldhello worldhello"
    )
    for batch in tokenized_dataset["input_ids"]:
        assert ts_tokenizer.decode(batch[0]) == "<|endoftext|>"
def test_pretokenize_dataset_can_separate_sequences_with_bos(
    ts_tokenizer: PreTrainedTokenizerBase,
):
    dataset = Dataset.from_list([{"text": "hello world"}] * 30)
    cfg = PretokenizeRunnerConfig(
        context_size=10,
        num_proc=1,
        shuffle=False,
        begin_batch_token=None,
        sequence_separator_token="bos",
        begin_sequence_token=None,
    )
    tokenized_dataset = cast(Any, pretokenize_dataset(dataset, ts_tokenizer, cfg))
    assert tokenized_dataset["input_ids"].shape[1] == cfg.context_size
    assert (
        ts_tokenizer.decode(tokenized_dataset["input_ids"][0])
        == "hello world<|endoftext|>hello world<|endoftext|>hello world<|endoftext|>hello"
    )
def test_pretokenize_dataset_can_begin_sequences_with_bos(
    ts_tokenizer: PreTrainedTokenizerBase,
):
    dataset = Dataset.from_list([{"text": "hello world"}] * 30)
    cfg = PretokenizeRunnerConfig(
        context_size=10,
        num_proc=1,
        shuffle=False,
        begin_batch_token=None,
        sequence_separator_token=None,
        begin_sequence_token="bos",
    )
    tokenized_dataset = cast(Any, pretokenize_dataset(dataset, ts_tokenizer, cfg))
    assert tokenized_dataset["input_ids"].shape[1] == cfg.context_size
    assert (
        ts_tokenizer.decode(tokenized_dataset["input_ids"][0])
        == "<|endoftext|>hello world<|endoftext|>hello world<|endoftext|>hello world<|endoftext|>"
    )
def test_pretokenize_dataset_dedupes_bos(
    ts_tokenizer: PreTrainedTokenizerBase,
):
    dataset = Dataset.from_list([{"text": "hello world"}] * 30)
    cfg = PretokenizeRunnerConfig(
        context_size=10,
        num_proc=1,
        shuffle=False,
        begin_batch_token="bos",
        sequence_separator_token="bos",
        begin_sequence_token="bos",
    )
    tokenized_dataset = cast(Any, pretokenize_dataset(dataset, ts_tokenizer, cfg))
    assert tokenized_dataset["input_ids"].shape[1] == cfg.context_size
    assert (
        ts_tokenizer.decode(tokenized_dataset["input_ids"][0])
        == "<|endoftext|>hello world<|endoftext|>hello world<|endoftext|>hello world<|endoftext|>"
    )
def test_pretokenize_dataset_can_shuffle(ts_tokenizer: PreTrainedTokenizerBase):
    dataset = Dataset.from_list(
        [
            {"text": "hello world1"},
            {"text": "hello world2"},
            {"text": "hello world3"},
        ]
        * 5000
    )
    cfg = PretokenizeRunnerConfig(context_size=10, num_proc=1, shuffle=True)
    # assert ts_model.tokenizer is not None
    tokenized_dataset1 = cast(Any, pretokenize_dataset(dataset, ts_tokenizer, cfg))
    tokenized_dataset2 = cast(Any, pretokenize_dataset(dataset, ts_tokenizer, cfg))
    assert len(tokenized_dataset1) == len(tokenized_dataset2)
    assert (
        tokenized_dataset1["input_ids"].tolist()
        != tokenized_dataset2["input_ids"].tolist()
    )
def test_pretokenize_runner_save_dataset_locally(tmp_path: Path):
    save_path = tmp_path / "ds"
    cfg = PretokenizeRunnerConfig(
        tokenizer_name="gpt2",
        context_size=10,
        num_proc=2,
        shuffle=True,
        save_path=str(save_path),
        dataset_path="NeelNanda/c4-10k",
        split="train[:20]",
        begin_batch_token="bos",
        sequence_separator_token="eos",
    )
    dataset = PretokenizeRunner(cfg).run()
    assert save_path.exists()
    loaded_dataset = Dataset.load_from_disk(str(save_path))
    assert len(dataset) == len(loaded_dataset)
    assert dataset["input_ids"].tolist() == loaded_dataset["input_ids"].tolist()  # type: ignore
    with open(save_path / "sae_lens.json") as f:
        metadata_dict = json.load(f)
    assert metadata_dict["original_dataset"] == "NeelNanda/c4-10k"
    assert metadata_dict["original_split"] == "train[:20]"
    assert metadata_dict["original_column_name"] == "text"
    assert metadata_dict["context_size"] == 10
    assert metadata_dict["shuffled"] is True
    assert metadata_dict["begin_batch_token"] == "bos"
    assert metadata_dict["begin_sequence_token"] is None
    assert metadata_dict["sequence_separator_token"] == "eos"
    assert metadata_dict["sae_lens_version"] == __version__
def test_pretokenize_runner_with_dataset_name(tmp_path: Path):
    save_path = tmp_path / "ds_with_dataset_name"
    cfg = PretokenizeRunnerConfig(
        tokenizer_name="gpt2",
        context_size=10,
        num_proc=2,
        shuffle=True,
        save_path=str(save_path),
        dataset_path="nyu-mll/glue",
        dataset_name="ax",
        split="test[:20]",
        column_name="premise",
        begin_batch_token="bos",
        sequence_separator_token="eos",
    )
    dataset = cast(Any, PretokenizeRunner(cfg).run())
    assert save_path.exists()
    loaded_dataset = Dataset.load_from_disk(str(save_path))
    assert len(dataset) == len(loaded_dataset)
    assert dataset["input_ids"].tolist() == loaded_dataset["input_ids"].tolist()  # type: ignore
    with open(save_path / "sae_lens.json") as f:
        metadata_dict = json.load(f)
    assert metadata_dict["original_dataset"] == "nyu-mll/glue"
    assert metadata_dict["original_dataset_name"] == "ax"
    assert metadata_dict["original_split"] == "test[:20]"
    assert metadata_dict["original_column_name"] == "premise"
    assert metadata_dict["context_size"] == 10
    assert metadata_dict["shuffled"] is True
    assert metadata_dict["begin_batch_token"] == "bos"
    assert metadata_dict["begin_sequence_token"] is None
    assert metadata_dict["sequence_separator_token"] == "eos"
    assert metadata_dict["sae_lens_version"] == __version__
def test_pretokenize_runner_streaming_dataset():
    cfg = PretokenizeRunnerConfig(
        tokenizer_name="gpt2",
        context_size=10,
        num_proc=1,
        dataset_path="NeelNanda/c4-10k",
        split="train",
        streaming=True,
    )
    dataset = PretokenizeRunner(cfg).run()
    assert isinstance(dataset, IterableDataset)
    cfg = PretokenizeRunnerConfig(
        tokenizer_name="gpt2",
        context_size=10,
        num_proc=2,
        dataset_path="NeelNanda/c4-10k",
        split="train",
        streaming=False,
    )
    dataset = PretokenizeRunner(cfg).run()
    assert not isinstance(dataset, IterableDataset)
def test_pretokenize_runner_raises_error_when_num_proc_is_greater_than_1_and_streaming_is_true():
    cfg = PretokenizeRunnerConfig(
        tokenizer_name="gpt2",
        context_size=10,
        num_proc=2,
        dataset_path="NeelNanda/c4-10k",
        split="train",
        streaming=True,
    )
    with pytest.raises(ValueError):
        PretokenizeRunner(cfg).run()

================
File: tests/training/test_sae_basic.py
================
import os
from copy import deepcopy
from pathlib import Path
import pytest
import torch
from torch import nn
from transformer_lens.hook_points import HookPoint
from sae_lens.config import LanguageModelSAERunnerConfig
from sae_lens.sae import SAE, _disable_hooks
from tests.helpers import ALL_ARCHITECTURES, build_sae_cfg
# Define a new fixture for different configurations
@pytest.fixture(
    params=[
        {
            "model_name": "tiny-stories-1M",
            "dataset_path": "roneneldan/TinyStories",
            "hook_name": "blocks.1.hook_resid_pre",
            "hook_layer": 1,
            "d_in": 64,
        },
        {
            "model_name": "tiny-stories-1M",
            "dataset_path": "roneneldan/TinyStories",
            "hook_name": "blocks.1.hook_resid_pre",
            "hook_layer": 1,
            "d_in": 64,
            "normalize_sae_decoder": False,
            "scale_sparsity_penalty_by_decoder_norm": True,
        },
        {
            "model_name": "tiny-stories-1M",
            "dataset_path": "apollo-research/roneneldan-TinyStories-tokenizer-gpt2",
            "hook_name": "blocks.1.hook_resid_pre",
            "hook_layer": 1,
            "d_in": 64,
        },
        {
            "model_name": "tiny-stories-1M",
            "dataset_path": "roneneldan/TinyStories",
            "hook_name": "blocks.1.attn.hook_z",
            "hook_layer": 1,
            "d_in": 64,
        },
    ],
    ids=[
        "tiny-stories-1M-resid-pre",
        "tiny-stories-1M-resid-pre-L1-W-dec-Norm",
        "tiny-stories-1M-resid-pre-pretokenized",
        "tiny-stories-1M-attn-out",
    ],
)
def cfg(request: pytest.FixtureRequest):
    """
    Pytest fixture to create a mock instance of LanguageModelSAERunnerConfig.
    """
    params = request.param
    return build_sae_cfg(**params)
def test_sae_init(cfg: LanguageModelSAERunnerConfig):
    sae = SAE.from_dict(cfg.get_base_sae_cfg_dict())
    assert isinstance(sae, SAE)
    assert sae.W_enc.shape == (cfg.d_in, cfg.d_sae)
    assert sae.W_dec.shape == (cfg.d_sae, cfg.d_in)
    assert sae.b_enc.shape == (cfg.d_sae,)
    assert sae.b_dec.shape == (cfg.d_in,)
def test_sae_fold_w_dec_norm(cfg: LanguageModelSAERunnerConfig):
    sae = SAE.from_dict(cfg.get_base_sae_cfg_dict())
    sae.turn_off_forward_pass_hook_z_reshaping()  # hook z reshaping not needed here.
    assert sae.W_dec.norm(dim=-1).mean().item() != pytest.approx(1.0, abs=1e-6)
    sae2 = deepcopy(sae)
    sae2.fold_W_dec_norm()
    W_dec_norms = sae.W_dec.norm(dim=-1).unsqueeze(1)
    assert torch.allclose(sae2.W_dec.data, sae.W_dec.data / W_dec_norms)
    assert torch.allclose(sae2.W_enc.data, sae.W_enc.data * W_dec_norms.T)
    assert torch.allclose(sae2.b_enc.data, sae.b_enc.data * W_dec_norms.squeeze())
    # fold_W_dec_norm should normalize W_dec to have unit norm.
    assert sae2.W_dec.norm(dim=-1).mean().item() == pytest.approx(1.0, abs=1e-6)
    # we expect activations of features to differ by W_dec norm weights.
    activations = torch.randn(10, 4, cfg.d_in, device=cfg.device)
    feature_activations_1 = sae.encode(activations)
    feature_activations_2 = sae2.encode(activations)
    assert torch.allclose(
        feature_activations_1.nonzero(),
        feature_activations_2.nonzero(),
    )
    expected_feature_activations_2 = feature_activations_1 * sae.W_dec.norm(dim=-1)
    torch.testing.assert_close(feature_activations_2, expected_feature_activations_2)
    sae_out_1 = sae.decode(feature_activations_1)
    sae_out_2 = sae2.decode(feature_activations_2)
    # but actual outputs should be the same
    torch.testing.assert_close(sae_out_1, sae_out_2)
@pytest.mark.parametrize("architecture", ALL_ARCHITECTURES)
@torch.no_grad()
def test_sae_fold_w_dec_norm_all_architectures(architecture: str):
    cfg = build_sae_cfg(architecture=architecture)
    sae = SAE.from_dict(cfg.get_base_sae_cfg_dict())
    sae.turn_off_forward_pass_hook_z_reshaping()  # hook z reshaping not needed here.
    # make sure all parameters are not 0s
    for param in sae.parameters():
        param.data = torch.rand_like(param)
    assert sae.W_dec.norm(dim=-1).mean().item() != pytest.approx(1.0, abs=1e-6)
    sae2 = deepcopy(sae)
    sae2.fold_W_dec_norm()
    # fold_W_dec_norm should normalize W_dec to have unit norm.
    assert sae2.W_dec.norm(dim=-1).mean().item() == pytest.approx(1.0, abs=1e-6)
    # we expect activations of features to differ by W_dec norm weights.
    activations = torch.randn(10, 4, cfg.d_in, device=cfg.device)
    feature_activations_1 = sae.encode(activations)
    feature_activations_2 = sae2.encode(activations)
    assert torch.allclose(
        feature_activations_1.nonzero(),
        feature_activations_2.nonzero(),
    )
    expected_feature_activations_2 = feature_activations_1 * sae.W_dec.norm(dim=-1)
    torch.testing.assert_close(feature_activations_2, expected_feature_activations_2)
    sae_out_1 = sae.decode(feature_activations_1)
    sae_out_2 = sae2.decode(feature_activations_2)
    # but actual outputs should be the same
    torch.testing.assert_close(sae_out_1, sae_out_2)
@torch.no_grad()
def test_sae_fold_norm_scaling_factor(cfg: LanguageModelSAERunnerConfig):
    norm_scaling_factor = 3.0
    sae = SAE.from_dict(cfg.get_base_sae_cfg_dict())
    # make sure b_dec and b_enc are not 0s
    sae.b_dec.data = torch.randn(cfg.d_in, device=cfg.device)
    sae.b_enc.data = torch.randn(cfg.d_sae, device=cfg.device)  # type: ignore
    sae.turn_off_forward_pass_hook_z_reshaping()  # hook z reshaping not needed here.
    sae2 = deepcopy(sae)
    sae2.fold_activation_norm_scaling_factor(norm_scaling_factor)
    assert sae2.cfg.normalize_activations == "none"
    assert torch.allclose(sae2.W_enc.data, sae.W_enc.data * norm_scaling_factor)
    # we expect activations of features to differ by W_dec norm weights.
    # assume activations are already scaled
    activations = torch.randn(10, 4, cfg.d_in, device=cfg.device)
    # we divide to get the unscale activations
    unscaled_activations = activations / norm_scaling_factor
    feature_activations_1 = sae.encode(activations)
    # with the scaling folded in, the unscaled activations should produce the same
    # result.
    feature_activations_2 = sae2.encode(unscaled_activations)
    assert torch.allclose(
        feature_activations_1.nonzero(),
        feature_activations_2.nonzero(),
    )
    torch.testing.assert_close(feature_activations_2, feature_activations_1)
    sae_out_1 = sae.decode(feature_activations_1)
    sae_out_2 = norm_scaling_factor * sae2.decode(feature_activations_2)
    # but actual outputs should be the same
    torch.testing.assert_close(sae_out_1, sae_out_2)
@pytest.mark.parametrize("architecture", ALL_ARCHITECTURES)
@torch.no_grad()
def test_sae_fold_norm_scaling_factor_all_architectures(architecture: str):
    cfg = build_sae_cfg(architecture=architecture)
    norm_scaling_factor = 3.0
    sae = SAE.from_dict(cfg.get_base_sae_cfg_dict())
    # make sure all parameters are not 0s
    for param in sae.parameters():
        param.data = torch.rand_like(param)
    sae2 = deepcopy(sae)
    sae2.fold_activation_norm_scaling_factor(norm_scaling_factor)
    assert sae2.cfg.normalize_activations == "none"
    assert torch.allclose(sae2.W_enc.data, sae.W_enc.data * norm_scaling_factor)
    # we expect activations of features to differ by W_dec norm weights.
    # assume activations are already scaled
    activations = torch.randn(10, 4, cfg.d_in, device=cfg.device)
    # we divide to get the unscale activations
    unscaled_activations = activations / norm_scaling_factor
    feature_activations_1 = sae.encode(activations)
    # with the scaling folded in, the unscaled activations should produce the same
    # result.
    feature_activations_2 = sae2.encode(unscaled_activations)
    assert torch.allclose(
        feature_activations_1.nonzero(),
        feature_activations_2.nonzero(),
    )
    torch.testing.assert_close(feature_activations_2, feature_activations_1)
    sae_out_1 = sae.decode(feature_activations_1)
    sae_out_2 = norm_scaling_factor * sae2.decode(feature_activations_2)
    # but actual outputs should be the same
    torch.testing.assert_close(sae_out_1, sae_out_2)
def test_sae_save_and_load_from_pretrained(tmp_path: Path) -> None:
    cfg = build_sae_cfg()
    model_path = str(tmp_path)
    sae = SAE.from_dict(cfg.get_base_sae_cfg_dict())
    sae_state_dict = sae.state_dict()
    sae.save_model(model_path)
    assert os.path.exists(model_path)
    sae_loaded = SAE.load_from_pretrained(model_path, device="cpu")
    sae_loaded_state_dict = sae_loaded.state_dict()
    # check state_dict matches the original
    for key in sae.state_dict():
        assert torch.allclose(
            sae_state_dict[key],
            sae_loaded_state_dict[key],
        )
    sae_in = torch.randn(10, cfg.d_in, device=cfg.device)
    sae_out_1 = sae(sae_in)
    sae_out_2 = sae_loaded(sae_in)
    assert torch.allclose(sae_out_1, sae_out_2)
def test_sae_save_and_load_from_pretrained_gated(tmp_path: Path) -> None:
    cfg = build_sae_cfg(architecture="gated")
    model_path = str(tmp_path)
    sae = SAE.from_dict(cfg.get_base_sae_cfg_dict())
    sae_state_dict = sae.state_dict()
    sae.save_model(model_path)
    assert os.path.exists(model_path)
    sae_loaded = SAE.load_from_pretrained(model_path, device="cpu")
    sae_loaded_state_dict = sae_loaded.state_dict()
    # check state_dict matches the original
    for key in sae.state_dict():
        assert torch.allclose(
            sae_state_dict[key],
            sae_loaded_state_dict[key],
        )
    sae_in = torch.randn(10, cfg.d_in, device=cfg.device)
    sae_out_1 = sae(sae_in)
    sae_out_2 = sae_loaded(sae_in)
    assert torch.allclose(sae_out_1, sae_out_2)
def test_sae_save_and_load_from_pretrained_topk(tmp_path: Path) -> None:
    cfg = build_sae_cfg(activation_fn_kwargs={"k": 30})
    model_path = str(tmp_path)
    sae = SAE.from_dict(cfg.get_base_sae_cfg_dict())
    sae_state_dict = sae.state_dict()
    sae.save_model(model_path)
    assert os.path.exists(model_path)
    sae_loaded = SAE.load_from_pretrained(model_path, device="cpu")
    sae_loaded_state_dict = sae_loaded.state_dict()
    # check state_dict matches the original
    for key in sae.state_dict():
        assert torch.allclose(
            sae_state_dict[key],
            sae_loaded_state_dict[key],
        )
    sae_in = torch.randn(10, cfg.d_in, device=cfg.device)
    sae_out_1 = sae(sae_in)
    sae_out_2 = sae_loaded(sae_in)
    assert torch.allclose(sae_out_1, sae_out_2)
def test_sae_seqpos(tmp_path: Path) -> None:
    cfg = build_sae_cfg(seqpos_slice=(1, 3))
    model_path = str(tmp_path)
    sae = SAE.from_dict(cfg.get_base_sae_cfg_dict())
    assert sae.cfg.seqpos_slice == (1, 3)
    sae.save_model(model_path)
    sae_loaded = SAE.load_from_pretrained(model_path, device="cpu")
    assert sae_loaded.cfg.seqpos_slice == (1, 3)
# TODO: Handle scaling factor in saeBase
# def test_sae_save_and_load_from_pretrained_lacks_scaling_factor(
#     tmp_path: Path,
# ) -> None:
#     cfg = build_sae_cfg()
#     model_path = str(tmp_path)
#     sparse_autoencoder = saeBase(**cfg.get_sae_base_parameters())
#     sparse_autoencoder_state_dict = sparse_autoencoder.state_dict()
#     sparse_autoencoder.save_model(model_path)
#     assert os.path.exists(model_path)
#     sparse_autoencoder_loaded = saeBase.load_from_pretrained(model_path)
#     sparse_autoencoder_loaded.cfg.verbose = True
#     sparse_autoencoder_loaded.cfg.checkpoint_path = cfg.checkpoint_path
#     sparse_autoencoder_loaded = sparse_autoencoder_loaded.to("cpu")
#     sparse_autoencoder_loaded_state_dict = sparse_autoencoder_loaded.state_dict()
#     # check cfg matches the original
#     assert sparse_autoencoder_loaded.cfg == cfg
#     # check state_dict matches the original
#     for key in sparse_autoencoder.state_dict().keys():
#         if key == "scaling_factor":
#             assert isinstance(cfg.d_sae, int)
#             assert torch.allclose(
#                 torch.ones(cfg.d_sae, dtype=cfg.dtype, device=cfg.device),
#                 sparse_autoencoder_loaded_state_dict[key],
#             )
#         else:
#             assert torch.allclose(
#                 sparse_autoencoder_state_dict[key],
#                 sparse_autoencoder_loaded_state_dict[key],
#             )
def test_sae_get_name_returns_correct_name_from_cfg_vals() -> None:
    cfg = build_sae_cfg(model_name="test_model", hook_name="test_hook_name", d_sae=128)
    sae = SAE.from_dict(cfg.get_base_sae_cfg_dict())
    assert sae.get_name() == "sae_test_model_test_hook_name_128"
def test_sae_move_between_devices() -> None:
    cfg = build_sae_cfg()
    sae = SAE.from_dict(cfg.get_base_sae_cfg_dict())
    sae.to("meta")
    assert sae.device == torch.device("meta")
    assert sae.cfg.device == "meta"
    assert sae.W_enc.device == torch.device("meta")
def test_sae_change_dtype() -> None:
    cfg = build_sae_cfg(dtype="float64")
    sae = SAE.from_dict(cfg.get_base_sae_cfg_dict())
    sae.to(dtype=torch.float16)
    assert sae.dtype == torch.float16
    assert sae.cfg.dtype == "torch.float16"
def test_sae_jumprelu_initialization():
    cfg = build_sae_cfg(architecture="jumprelu", device="cpu")
    sae = SAE.from_dict(cfg.get_base_sae_cfg_dict())
    assert isinstance(sae.W_enc, nn.Parameter)
    assert isinstance(sae.W_dec, nn.Parameter)
    assert isinstance(sae.b_enc, nn.Parameter)
    assert isinstance(sae.b_dec, nn.Parameter)
    assert isinstance(sae.threshold, nn.Parameter)
    assert sae.W_enc.shape == (cfg.d_in, cfg.d_sae)
    assert sae.W_dec.shape == (cfg.d_sae, cfg.d_in)
    assert sae.b_enc.shape == (cfg.d_sae,)
    assert sae.b_dec.shape == (cfg.d_in,)
    assert sae.threshold.shape == (cfg.d_sae,)
    # encoder/decoder should be initialized, everything else should be 0s
    assert not torch.allclose(sae.W_enc, torch.zeros_like(sae.W_enc))
    assert not torch.allclose(sae.W_dec, torch.zeros_like(sae.W_dec))
    assert torch.allclose(sae.b_dec, torch.zeros_like(sae.b_dec))
    assert torch.allclose(sae.b_enc, torch.zeros_like(sae.b_enc))
    assert torch.allclose(sae.threshold, torch.zeros_like(sae.threshold))
@pytest.mark.parametrize("use_error_term", [True, False])
def test_sae_jumprelu_forward(use_error_term: bool):
    cfg = build_sae_cfg(architecture="jumprelu", d_in=2, d_sae=3)
    sae = SAE.from_dict(cfg.get_base_sae_cfg_dict())
    sae.use_error_term = use_error_term
    sae.threshold.data = torch.tensor([1.0, 0.5, 0.25])
    sae.W_enc.data = torch.ones_like(sae.W_enc.data)
    sae.W_dec.data = torch.ones_like(sae.W_dec.data)
    sae.b_enc.data = torch.zeros_like(sae.b_enc.data)
    sae.b_dec.data = torch.zeros_like(sae.b_dec.data)
    sae_in = 0.3 * torch.ones(1, 2)
    expected_recons = torch.tensor([[1.2, 1.2]])
    # if we use error term, we should always get the same output as what we put in
    expected_output = sae_in if use_error_term else expected_recons
    out, cache = sae.run_with_cache(sae_in)
    assert torch.allclose(out, expected_output)
    assert torch.allclose(cache["hook_sae_input"], sae_in)
    assert torch.allclose(cache["hook_sae_output"], out)
    assert torch.allclose(cache["hook_sae_recons"], expected_recons)
    if use_error_term:
        assert torch.allclose(
            cache["hook_sae_error"], expected_output - expected_recons
        )
    assert torch.allclose(cache["hook_sae_acts_pre"], torch.tensor([[0.6, 0.6, 0.6]]))
    # the threshold of 1.0 should block the first latent from firing
    assert torch.allclose(cache["hook_sae_acts_post"], torch.tensor([[0.0, 0.6, 0.6]]))
def test_sae_gated_initialization():
    cfg = build_sae_cfg(architecture="gated")
    sae = SAE.from_dict(cfg.get_base_sae_cfg_dict())
    assert isinstance(sae.W_enc, nn.Parameter)
    assert isinstance(sae.W_dec, nn.Parameter)
    assert isinstance(sae.b_dec, nn.Parameter)
    assert isinstance(sae.b_gate, nn.Parameter)
    assert isinstance(sae.r_mag, nn.Parameter)
    assert isinstance(sae.b_mag, nn.Parameter)
    assert sae.W_enc.shape == (cfg.d_in, cfg.d_sae)
    assert sae.W_dec.shape == (cfg.d_sae, cfg.d_in)
    assert sae.b_dec.shape == (cfg.d_in,)
    assert sae.b_gate.shape == (cfg.d_sae,)
    assert sae.r_mag.shape == (cfg.d_sae,)
    assert sae.b_mag.shape == (cfg.d_sae,)
    assert not torch.allclose(sae.W_enc, torch.zeros_like(sae.W_enc))
    assert not torch.allclose(sae.W_dec, torch.zeros_like(sae.W_dec))
    assert torch.allclose(sae.b_dec, torch.zeros_like(sae.b_dec))
    assert torch.allclose(sae.b_gate, torch.zeros_like(sae.b_gate))
    assert torch.allclose(sae.r_mag, torch.zeros_like(sae.r_mag))
    assert torch.allclose(sae.b_mag, torch.zeros_like(sae.b_mag))
@pytest.mark.parametrize("use_error_term", [True, False])
def test_sae_gated_forward(use_error_term: bool):
    cfg = build_sae_cfg(architecture="gated", d_in=2, d_sae=3)
    sae = SAE.from_dict(cfg.get_base_sae_cfg_dict())
    sae.use_error_term = use_error_term
    sae.W_enc.data = torch.ones_like(sae.W_enc.data)
    sae.W_dec.data = torch.ones_like(sae.W_dec.data)
    sae.b_dec.data = torch.zeros_like(sae.b_dec.data)
    sae.b_gate.data = torch.tensor([-2.0, 0.0, 1.0])
    sae.r_mag.data = torch.tensor([1.0, 2.0, 3.0])
    sae.b_mag.data = torch.tensor([1.0, 1.0, 1.0])
    sae_in = torch.tensor([[0.3, 0.3]])
    # expected gating pre acts: [0.6 - 2 = -1.4, 0.6, 0.6 + 1 = 1.6]
    # so the first gate should be off
    # mags should be [0.6 * exp(1), 0.6 * exp(2), 0.6 * exp(3)] + b_mag => [2.6310,  5.4334, 13.0513]
    expected_recons = torch.tensor([[18.4848, 18.4848]])
    # if we use error term, we should always get the same output as what we put in
    expected_output = sae_in if use_error_term else expected_recons
    out, cache = sae.run_with_cache(sae_in)
    assert torch.allclose(out, expected_output, atol=1e-3)
    assert torch.allclose(cache["hook_sae_input"], sae_in, atol=1e-3)
    assert torch.allclose(cache["hook_sae_output"], out, atol=1e-3)
    assert torch.allclose(cache["hook_sae_recons"], expected_recons, atol=1e-3)
    assert torch.allclose(
        cache["hook_sae_acts_pre"], torch.tensor([[2.6310, 5.4334, 13.0513]]), atol=1e-3
    )
    # the threshold of 1.0 should block the first latent from firing
    assert torch.allclose(
        cache["hook_sae_acts_post"],
        torch.tensor([[0.0, 5.4334, 13.0513]]),
        atol=1e-3,
    )
    if use_error_term:
        assert torch.allclose(
            cache["hook_sae_error"], expected_output - expected_recons
        )
def test_disable_hooks_temporarily_stops_hooks_from_running():
    cfg = build_sae_cfg(d_in=2, d_sae=3)
    sae = SAE.from_dict(cfg.get_base_sae_cfg_dict())
    sae_in = torch.randn(10, cfg.d_in)
    orig_out, orig_cache = sae.run_with_cache(sae_in)
    with _disable_hooks(sae):
        disabled_out, disabled_cache = sae.run_with_cache(sae_in)
    subseq_out, subseq_cache = sae.run_with_cache(sae_in)
    assert torch.allclose(orig_out, disabled_out)
    assert torch.allclose(orig_out, subseq_out)
    assert disabled_cache.keys() == set()
    for key in orig_cache:
        assert torch.allclose(orig_cache[key], subseq_cache[key])
@pytest.mark.parametrize("architecture", ["standard", "gated", "jumprelu"])
def test_sae_forward_pass_works_with_error_term_and_hooks(architecture: str):
    cfg = build_sae_cfg(architecture=architecture, d_in=32, d_sae=64)
    sae = SAE.from_dict(cfg.get_base_sae_cfg_dict())
    sae.use_error_term = True
    sae_in = torch.randn(10, cfg.d_in)
    original_out, original_cache = sae.run_with_cache(sae_in)
    def ablate_hooked_sae(acts: torch.Tensor, hook: HookPoint):  # noqa: ARG001
        acts[:, :] = 20
        return acts
    with sae.hooks(fwd_hooks=[("hook_sae_acts_post", ablate_hooked_sae)]):
        ablated_out, ablated_cache = sae.run_with_cache(sae_in)
    assert not torch.allclose(original_out, ablated_out, rtol=1e-2)
    assert torch.all(ablated_cache["hook_sae_acts_post"] == 20)
    assert torch.allclose(
        original_cache["hook_sae_error"], ablated_cache["hook_sae_error"], rtol=1e-4
    )

================
File: tests/training/test_sae_from_pretrained.py
================
import pytest
import torch
from huggingface_hub import hf_hub_download
from safetensors import safe_open
from sae_lens.sae import SAE
def test_SparseAutoencoder_from_pretrained_loads_from_hugginface_using_shorthand():
    sae, original_cfg_dict, sparsity = SAE.from_pretrained(
        release="gpt2-small-res-jb",
        sae_id="blocks.0.hook_resid_pre",
        device="cpu",
    )
    assert (
        sae.cfg.neuronpedia_id == "gpt2-small/0-res-jb"
    )  # what we expect from the yml
    # it should match what we get when manually loading from hf
    repo_id = "jbloom/GPT2-Small-SAEs-Reformatted"
    hook_point = "blocks.0.hook_resid_pre"
    filename = f"{hook_point}/sae_weights.safetensors"
    weight_path = hf_hub_download(repo_id=repo_id, filename=filename)
    state_dict = {}
    with safe_open(weight_path, framework="pt", device="cpu") as f:  # type: ignore
        for k in f.keys():  # noqa: SIM118
            state_dict[k] = f.get_tensor(k)
    assert isinstance(sae, SAE)
    assert sae.cfg.model_name == "gpt2-small"
    assert sae.cfg.hook_name == "blocks.0.hook_resid_pre"
    assert isinstance(original_cfg_dict, dict)
    assert isinstance(sparsity, torch.Tensor)
    assert sparsity.shape == (sae.cfg.d_sae,)
    assert sparsity.max() < 0.0
    for k in sae.state_dict():
        if k == "finetuning_scaling_factor":
            continue
        assert torch.allclose(sae.state_dict()[k], state_dict[k])
def test_SparseAutoencoder_from_pretrained_can_load_arbitrary_saes_from_hugginface():
    sae, original_cfg_dict, sparsity = SAE.from_pretrained(
        release="jbloom/GPT2-Small-SAEs-Reformatted",
        sae_id="blocks.0.hook_resid_pre",
        device="cpu",
    )
    # it should match what we get when manually loading from hf
    repo_id = "jbloom/GPT2-Small-SAEs-Reformatted"
    hook_point = "blocks.0.hook_resid_pre"
    filename = f"{hook_point}/sae_weights.safetensors"
    weight_path = hf_hub_download(repo_id=repo_id, filename=filename)
    state_dict = {}
    with safe_open(weight_path, framework="pt", device="cpu") as f:  # type: ignore
        for k in f.keys():  # noqa: SIM118
            state_dict[k] = f.get_tensor(k)
    assert isinstance(sae, SAE)
    assert sae.cfg.model_name == "gpt2-small"
    assert sae.cfg.hook_name == "blocks.0.hook_resid_pre"
    assert isinstance(original_cfg_dict, dict)
    assert isinstance(sparsity, torch.Tensor)
    assert sparsity.shape == (sae.cfg.d_sae,)
    assert sparsity.max() < 0.0
    for k in sae.state_dict():
        if k == "finetuning_scaling_factor":
            continue
        assert torch.allclose(sae.state_dict()[k], state_dict[k])
def test_SparseAutoencoder_from_pretrained_errors_for_invalid_releases():
    with pytest.raises(ValueError):
        SAE.from_pretrained(
            release="wrong",
            sae_id="blocks.0.hook_resid_pre",
            device="cpu",
        )
def test_SparseAutoencoder_from_pretrained_errors_for_invalid_sae_ids():
    with pytest.raises(ValueError):
        SAE.from_pretrained(
            release="gpt2-small-res-jb",
            sae_id="wrong",
            device="cpu",
        )

================
File: tests/training/test_sae_initialization.py
================
import pytest
import torch
from sae_lens.training.training_sae import TrainingSAE
from tests.helpers import build_sae_cfg
def test_SparseAutoencoder_initialization_standard():
    cfg = build_sae_cfg()
    sae = TrainingSAE.from_dict(cfg.get_training_sae_cfg_dict())
    assert sae.W_enc.shape == (cfg.d_in, cfg.d_sae)
    assert sae.W_dec.shape == (cfg.d_sae, cfg.d_in)
    assert sae.b_enc.shape == (cfg.d_sae,)
    assert sae.b_dec.shape == (cfg.d_in,)
    assert isinstance(sae.activation_fn, torch.nn.ReLU)
    assert sae.device == torch.device("cpu")
    assert sae.dtype == torch.float32
    # biases
    assert torch.allclose(sae.b_dec, torch.zeros_like(sae.b_dec), atol=1e-6)
    assert torch.allclose(sae.b_enc, torch.zeros_like(sae.b_enc), atol=1e-6)
    # check if the decoder weight norm is 1 by default
    assert torch.allclose(
        sae.W_dec.norm(dim=1), torch.ones_like(sae.W_dec.norm(dim=1)), atol=1e-6
    )
    #  Default currently shouldn't be tranpose initialization
    unit_normed_W_enc = sae.W_enc / torch.norm(sae.W_enc, dim=0)
    unit_normed_W_dec = sae.W_dec.T
    assert not torch.allclose(unit_normed_W_enc, unit_normed_W_dec, atol=1e-6)
def test_SparseAutoencoder_initialization_gated():
    cfg = build_sae_cfg()
    setattr(cfg, "architecture", "gated")
    sae = TrainingSAE.from_dict(cfg.get_training_sae_cfg_dict())
    assert sae.W_enc.shape == (cfg.d_in, cfg.d_sae)
    assert sae.W_dec.shape == (cfg.d_sae, cfg.d_in)
    assert sae.b_mag.shape == (cfg.d_sae,)
    assert sae.b_gate.shape == (cfg.d_sae,)
    assert sae.r_mag.shape == (cfg.d_sae,)
    assert sae.b_dec.shape == (cfg.d_in,)
    assert isinstance(sae.activation_fn, torch.nn.ReLU)
    assert sae.device == torch.device("cpu")
    assert sae.dtype == torch.float32
    # biases
    assert torch.allclose(sae.b_dec, torch.zeros_like(sae.b_dec), atol=1e-6)
    assert torch.allclose(sae.b_mag, torch.zeros_like(sae.b_mag), atol=1e-6)
    assert torch.allclose(sae.b_gate, torch.zeros_like(sae.b_gate), atol=1e-6)
    # check if the decoder weight norm is 1 by default
    assert torch.allclose(
        sae.W_dec.norm(dim=1), torch.ones_like(sae.W_dec.norm(dim=1)), atol=1e-6
    )
    #  Default currently shouldn't be tranpose initialization
    unit_normed_W_enc = sae.W_enc / torch.norm(sae.W_enc, dim=0)
    unit_normed_W_dec = sae.W_dec.T
    assert not torch.allclose(unit_normed_W_enc, unit_normed_W_dec, atol=1e-6)
def test_SparseAutoencoder_initialization_jumprelu():
    cfg = build_sae_cfg(architecture="jumprelu")
    sae = TrainingSAE.from_dict(cfg.get_training_sae_cfg_dict())
    assert sae.W_enc.shape == (cfg.d_in, cfg.d_sae)
    assert sae.W_dec.shape == (cfg.d_sae, cfg.d_in)
    assert isinstance(sae.log_threshold, torch.nn.Parameter)
    assert sae.log_threshold.shape == (cfg.d_sae,)
    assert sae.b_enc.shape == (cfg.d_sae,)
    assert sae.b_dec.shape == (cfg.d_in,)
    assert isinstance(sae.activation_fn, torch.nn.ReLU)
    assert sae.device == torch.device("cpu")
    assert sae.dtype == torch.float32
    # biases
    assert torch.allclose(sae.b_dec, torch.zeros_like(sae.b_dec), atol=1e-6)
    assert torch.allclose(sae.b_enc, torch.zeros_like(sae.b_enc), atol=1e-6)
    # check if the decoder weight norm is 1 by default
    assert torch.allclose(
        sae.W_dec.norm(dim=1), torch.ones_like(sae.W_dec.norm(dim=1)), atol=1e-6
    )
    #  Default currently shouldn't be tranpose initialization
    unit_normed_W_enc = sae.W_enc / torch.norm(sae.W_enc, dim=0)
    unit_normed_W_dec = sae.W_dec.T
    assert not torch.allclose(unit_normed_W_enc, unit_normed_W_dec, atol=1e-6)
def test_SparseAutoencoder_initialization_orthogonal_enc_dec():
    cfg = build_sae_cfg(decoder_orthogonal_init=True, expansion_factor=2)
    sae = TrainingSAE.from_dict(cfg.get_training_sae_cfg_dict())
    projections = sae.W_dec.T @ sae.W_dec
    mask = ~torch.eye(projections.size(0), dtype=torch.bool)
    assert projections[mask].max() < 0.1
    # initialized weights of biases are 0
    assert torch.allclose(sae.b_dec, torch.zeros_like(sae.b_dec), atol=1e-6)
    assert torch.allclose(sae.b_enc, torch.zeros_like(sae.b_enc), atol=1e-6)
def test_SparseAutoencoder_initialization_normalize_decoder_norm():
    cfg = build_sae_cfg(normalize_sae_decoder=True)
    sae = TrainingSAE.from_dict(cfg.get_training_sae_cfg_dict())
    assert torch.allclose(
        sae.W_dec.norm(dim=1), torch.ones_like(sae.W_dec.norm(dim=1)), atol=1e-6
    )
    # initialized weights of biases are 0
    assert torch.allclose(sae.b_dec, torch.zeros_like(sae.b_dec), atol=1e-6)
    assert torch.allclose(sae.b_enc, torch.zeros_like(sae.b_enc), atol=1e-6)
def test_SparseAutoencoder_initialization_encoder_is_decoder_transpose():
    cfg = build_sae_cfg(init_encoder_as_decoder_transpose=True)
    sae = TrainingSAE.from_dict(cfg.get_training_sae_cfg_dict())
    # If we decoder norms are 1 we need to unit norm W_enc first.
    unit_normed_W_enc = sae.W_enc / torch.norm(sae.W_enc, dim=0)
    unit_normed_W_dec = sae.W_dec.T
    assert torch.allclose(unit_normed_W_enc, unit_normed_W_dec, atol=1e-6)
    # initialized weights of biases are 0
    assert torch.allclose(sae.b_dec, torch.zeros_like(sae.b_dec), atol=1e-6)
    assert torch.allclose(sae.b_enc, torch.zeros_like(sae.b_enc), atol=1e-6)
def test_SparseAutoencoder_initialization_enc_dec_T_no_unit_norm():
    cfg = build_sae_cfg(
        init_encoder_as_decoder_transpose=True,
        normalize_sae_decoder=False,
    )
    sae = TrainingSAE.from_dict(cfg.get_training_sae_cfg_dict())
    assert torch.allclose(sae.W_dec, sae.W_enc.T, atol=1e-6)
    # initialized weights of biases are 0
    assert torch.allclose(sae.b_dec, torch.zeros_like(sae.b_dec), atol=1e-6)
    assert torch.allclose(sae.b_enc, torch.zeros_like(sae.b_enc), atol=1e-6)
def test_SparseAutoencoder_initialization_heuristic_init_and_normalize_sae_decoder():
    # assert that an error is raised
    with pytest.raises(ValueError):
        _ = build_sae_cfg(
            decoder_heuristic_init=True,
            normalize_sae_decoder=True,
        )
def test_SparseAutoencoder_initialization_decoder_norm_in_loss_and_normalize_sae_decoder():
    # assert that an error is raised
    with pytest.raises(ValueError):
        _ = build_sae_cfg(
            scale_sparsity_penalty_by_decoder_norm=True,
            normalize_sae_decoder=True,
        )
def test_SparseAutoencoder_initialization_heuristic_init():
    cfg = build_sae_cfg(
        decoder_heuristic_init=True,
        normalize_sae_decoder=False,
    )
    sae = TrainingSAE.from_dict(cfg.get_training_sae_cfg_dict())
    decoder_norms = sae.W_dec.norm(dim=1)
    # not unit norms
    assert not torch.allclose(
        decoder_norms, torch.ones_like(sae.W_dec.norm(dim=1)), atol=1e-6
    )
    assert torch.allclose(
        decoder_norms, torch.ones_like(decoder_norms) * 0.1, atol=5e-2
    )

================
File: tests/training/test_sae_trainer.py
================
from pathlib import Path
from typing import Any, Callable
import pytest
import torch
from datasets import Dataset
from safetensors.torch import load_file
from transformer_lens import HookedTransformer
from sae_lens import __version__
from sae_lens.config import LanguageModelSAERunnerConfig
from sae_lens.sae_training_runner import SAETrainingRunner
from sae_lens.training.activations_store import ActivationsStore
from sae_lens.training.sae_trainer import (
    SAETrainer,
    TrainStepOutput,
    _log_feature_sparsity,
    _update_sae_lens_training_version,
)
from sae_lens.training.training_sae import TrainingSAE
from tests.helpers import TINYSTORIES_MODEL, build_sae_cfg, load_model_cached
@pytest.fixture
def cfg():
    return build_sae_cfg(d_in=64, d_sae=128, hook_layer=0)
@pytest.fixture
def model():
    return load_model_cached(TINYSTORIES_MODEL)
@pytest.fixture
def activation_store(model: HookedTransformer, cfg: LanguageModelSAERunnerConfig):
    return ActivationsStore.from_config(
        model, cfg, override_dataset=Dataset.from_list([{"text": "hello world"}] * 2000)
    )
@pytest.fixture
def training_sae(cfg: LanguageModelSAERunnerConfig):
    return TrainingSAE.from_dict(cfg.get_training_sae_cfg_dict())
@pytest.fixture
def trainer(
    cfg: LanguageModelSAERunnerConfig,
    training_sae: TrainingSAE,
    model: HookedTransformer,
    activation_store: ActivationsStore,
):
    return SAETrainer(
        model=model,
        sae=training_sae,
        activation_store=activation_store,
        save_checkpoint_fn=lambda *args, **kwargs: None,  # noqa: ARG005
        cfg=cfg,
    )
def modify_sae_output(sae: TrainingSAE, modifier: Callable[[torch.Tensor], Any]):
    """
    Helper to modify the output of the SAE forward pass for use in patching, for use in patch side_effect.
    We need real grads during training, so we can't just mock the whole forward pass directly.
    """
    def modified_forward(*args: Any, **kwargs: Any) -> torch.Tensor:
        output = TrainingSAE.forward(sae, *args, **kwargs)
        return modifier(output)
    return modified_forward
def test_train_step__reduces_loss_when_called_repeatedly_on_same_acts(
    trainer: SAETrainer,
) -> None:
    layer_acts = trainer.activations_store.next_batch()
    # intentionally train on the same activations 5 times to ensure loss decreases
    train_outputs = [
        trainer._train_step(
            sae=trainer.sae,
            sae_in=layer_acts[:, 0, :],
        )
        for _ in range(5)
    ]
    # ensure loss decreases with each training step
    for output, next_output in zip(train_outputs[:-1], train_outputs[1:]):
        assert output.loss > next_output.loss
    assert (
        trainer.n_frac_active_tokens == 20
    )  # should increment each step by batch_size (5*4)
def test_train_step__output_looks_reasonable(trainer: SAETrainer) -> None:
    layer_acts = trainer.activations_store.next_batch()
    output = trainer._train_step(
        sae=trainer.sae,
        sae_in=layer_acts[:, 0, :],
    )
    assert output.loss > 0
    # only hook_point_layer=0 acts should be passed to the SAE
    assert torch.allclose(output.sae_in, layer_acts[:, 0, :])
    assert output.sae_out.shape == output.sae_in.shape
    assert output.feature_acts.shape == (4, 128)  # batch_size, d_sae
    # ghots grads shouldn't trigger until dead_feature_window, which hasn't been reached yet
    assert output.losses.get("ghost_grad_loss", 0) == 0
    assert trainer.n_frac_active_tokens == 4
    assert trainer.act_freq_scores.sum() > 0  # at least SOME acts should have fired
    assert torch.allclose(
        trainer.act_freq_scores, (output.feature_acts.abs() > 0).float().sum(0)
    )
def test_train_step__sparsity_updates_based_on_feature_act_sparsity(
    trainer: SAETrainer,
) -> None:
    trainer._reset_running_sparsity_stats()
    layer_acts = trainer.activations_store.next_batch()
    train_output = trainer._train_step(
        sae=trainer.sae,
        sae_in=layer_acts[:, 0, :],
    )
    feature_acts = train_output.feature_acts
    # should increase by batch_size
    assert trainer.n_frac_active_tokens == 4
    # add freq scores for all non-zero feature acts
    assert torch.allclose(
        trainer.act_freq_scores, (feature_acts > 0).float().sum(dim=0)
    )
    # check that features that just fired have n_forward_passes_since_fired = 0
    assert (
        trainer.n_forward_passes_since_fired[
            ((feature_acts > 0).float()[-1] == 1)
        ].max()
        == 0
    )
    assert train_output.feature_acts is feature_acts
def test_log_feature_sparsity__handles_zeroes_by_default_fp32() -> None:
    fp32_zeroes = torch.tensor([0], dtype=torch.float32)
    assert _log_feature_sparsity(fp32_zeroes).item() != float("-inf")
# TODO: currently doesn't work for fp16, we should address this
@pytest.mark.skip(reason="Currently doesn't work for fp16")
def test_log_feature_sparsity__handles_zeroes_by_default_fp16() -> None:
    fp16_zeroes = torch.tensor([0], dtype=torch.float16)
    assert _log_feature_sparsity(fp16_zeroes).item() != float("-inf")
def test_build_train_step_log_dict(trainer: SAETrainer) -> None:
    train_output = TrainStepOutput(
        sae_in=torch.tensor([[-1, 0], [0, 2], [1, 1]]).float(),
        sae_out=torch.tensor([[0, 0], [0, 2], [0.5, 1]]).float(),
        feature_acts=torch.tensor([[0, 0, 0, 1], [1, 0, 0, 1], [1, 0, 1, 1]]).float(),
        hidden_pre=torch.tensor([[-1, 0, 0, 1], [1, -1, 0, 1], [1, -1, 1, 1]]).float(),
        loss=torch.tensor(0.5),
        losses={
            "mse_loss": 0.25,
            "l1_loss": 0.1,
            "ghost_grad_loss": 0.15,
        },
    )
    # we're relying on the trainer only for some of the metrics here
    # we should more / less try to break this and push
    # everything through the train step output if we can.
    log_dict = trainer._build_train_step_log_dict(
        output=train_output, n_training_tokens=123
    )
    assert log_dict == {
        "losses/mse_loss": 0.25,
        # l1 loss is scaled by l1_coefficient
        "losses/l1_loss": train_output.losses["l1_loss"] / trainer.cfg.l1_coefficient,
        "losses/raw_l1_loss": train_output.losses["l1_loss"],
        "losses/overall_loss": 0.5,
        "losses/ghost_grad_loss": 0.15,
        "metrics/explained_variance": 0.75,
        "metrics/explained_variance_std": 0.25,
        "metrics/l0": 2.0,
        "sparsity/mean_passes_since_fired": trainer.n_forward_passes_since_fired.mean().item(),
        "sparsity/dead_features": trainer.dead_neurons.sum().item(),
        "details/current_learning_rate": 2e-4,
        "details/current_l1_coefficient": trainer.cfg.l1_coefficient,
        "details/n_training_tokens": 123,
    }
def test_train_sae_group_on_language_model__runs(
    ts_model: HookedTransformer,
    tmp_path: Path,
) -> None:
    checkpoint_dir = tmp_path / "checkpoint"
    cfg = build_sae_cfg(
        checkpoint_path=str(checkpoint_dir),
        training_tokens=20,
        context_size=8,
    )
    # just a tiny datast which will run quickly
    dataset = Dataset.from_list([{"text": "hello world"}] * 100)
    activation_store = ActivationsStore.from_config(
        ts_model, cfg, override_dataset=dataset
    )
    sae = TrainingSAE.from_dict(cfg.get_training_sae_cfg_dict())
    sae = SAETrainer(
        model=ts_model,
        sae=sae,
        activation_store=activation_store,
        save_checkpoint_fn=lambda *args, **kwargs: None,  # noqa: ARG005
        cfg=cfg,
    ).fit()
    assert isinstance(sae, TrainingSAE)
def test_update_sae_lens_training_version_sets_the_current_version():
    cfg = build_sae_cfg(sae_lens_training_version="0.1.0")
    sae = TrainingSAE.from_dict(cfg.get_training_sae_cfg_dict())
    _update_sae_lens_training_version(sae)
    assert sae.cfg.sae_lens_training_version == str(__version__)
def test_estimated_norm_scaling_factor_persistence(
    ts_model: HookedTransformer,
    tmp_path: Path,
):
    """Test that estimated_norm_scaling_factor is correctly persisted in intermediate checkpoints
    but not in the final checkpoint."""
    checkpoint_dir = tmp_path / "checkpoints"
    checkpoint_dir.mkdir(exist_ok=True)
    cfg = build_sae_cfg(
        checkpoint_path=str(checkpoint_dir),
        training_tokens=100,  # Increased to ensure we hit checkpoints
        context_size=8,
        normalize_activations="expected_average_only_in",
        n_checkpoints=2,  # Explicitly request 2 checkpoints during training
    )
    # Create a small dataset
    dataset = Dataset.from_list([{"text": "hello world"}] * 100)
    activation_store = ActivationsStore.from_config(
        ts_model, cfg, override_dataset=dataset
    )
    sae = TrainingSAE.from_dict(cfg.get_training_sae_cfg_dict())
    trainer = SAETrainer(
        model=ts_model,
        sae=sae,
        activation_store=activation_store,
        save_checkpoint_fn=SAETrainingRunner.save_checkpoint,
        cfg=cfg,
    )
    # Train the model - this should create checkpoints
    trainer.fit()
    checkpoint_paths = list(
        checkpoint_dir.glob("**/activations_store_state.safetensors")
    )
    # We should have exactly 2 checkpoints:
    assert (
        len(checkpoint_paths) == 2
    ), f"Expected 2 checkpoints but got {len(checkpoint_paths)}"
    during_checkpoints = [
        load_file(path) for path in checkpoint_paths if "final" not in path.parent.name
    ]
    final_checkpoints = [
        load_file(path) for path in checkpoint_paths if "final" in path.parent.name
    ]
    assert (
        len(during_checkpoints) == 1
    ), f"Expected 1 other checkpoint but got {len(during_checkpoints)}"
    assert (
        len(final_checkpoints) == 1
    ), f"Expected 1 final checkpoint but got {len(final_checkpoints)}"
    during_checkpoint = during_checkpoints[0]
    final_checkpoint = final_checkpoints[0]
    # Check intermediate checkpoints have the scaling factor
    assert "estimated_norm_scaling_factor" in during_checkpoint
    assert during_checkpoint["estimated_norm_scaling_factor"] is not None
    # Final checkpoint should NOT have the scaling factor as it's been folded into the weights
    assert "estimated_norm_scaling_factor" not in final_checkpoint

================
File: tests/training/test_sae_training_runner.py
================
import argparse
import json
import os
from pathlib import Path
import pytest
import torch
from datasets import Dataset
from transformer_lens import HookedTransformer
from sae_lens.config import LanguageModelSAERunnerConfig
from sae_lens.sae import SAE
from sae_lens.sae_training_runner import SAETrainingRunner, _parse_cfg_args, _run_cli
from sae_lens.training.activations_store import ActivationsStore
from sae_lens.training.sae_trainer import SAETrainer
from sae_lens.training.training_sae import TrainingSAE
from tests.helpers import (
    TINYSTORIES_DATASET,
    TINYSTORIES_MODEL,
    build_sae_cfg,
    load_model_cached,
)
@pytest.fixture
def cfg(tmp_path: Path):
    return build_sae_cfg(
        d_in=64, d_sae=128, hook_layer=0, checkpoint_path=str(tmp_path)
    )
@pytest.fixture
def model():
    return load_model_cached(TINYSTORIES_MODEL)
@pytest.fixture
def activation_store(model: HookedTransformer, cfg: LanguageModelSAERunnerConfig):
    return ActivationsStore.from_config(
        model, cfg, override_dataset=Dataset.from_list([{"text": "hello world"}] * 2000)
    )
@pytest.fixture
def training_sae(cfg: LanguageModelSAERunnerConfig):
    return TrainingSAE.from_dict(cfg.get_training_sae_cfg_dict())
@pytest.fixture
def trainer(
    cfg: LanguageModelSAERunnerConfig,
    training_sae: TrainingSAE,
    model: HookedTransformer,
    activation_store: ActivationsStore,
):
    return SAETrainer(
        model=model,
        sae=training_sae,
        activation_store=activation_store,
        save_checkpoint_fn=lambda *args, **kwargs: None,  # noqa: ARG005
        cfg=cfg,
    )
@pytest.fixture
def training_runner(
    cfg: LanguageModelSAERunnerConfig,
):
    return SAETrainingRunner(cfg)
def test_save_checkpoint(training_runner: SAETrainingRunner, trainer: SAETrainer):
    training_runner.save_checkpoint(
        trainer=trainer,
        checkpoint_name="test",
    )
    contents = os.listdir(training_runner.cfg.checkpoint_path + "/test")
    assert "sae_weights.safetensors" in contents
    assert "sparsity.safetensors" in contents
    assert "cfg.json" in contents
    sae = SAE.load_from_pretrained(training_runner.cfg.checkpoint_path + "/test")
    assert isinstance(sae, SAE)
def test_training_runner_works_with_from_pretrained_path(
    trainer: SAETrainer,
    cfg: LanguageModelSAERunnerConfig,
):
    SAETrainingRunner.save_checkpoint(
        trainer=trainer,
        checkpoint_name="test",
    )
    cfg.from_pretrained_path = trainer.cfg.checkpoint_path + "/test"
    loaded_runner = SAETrainingRunner(cfg)
    # the loaded runner should load the pretrained SAE
    orig_sae = trainer.sae
    new_sae = loaded_runner.sae
    assert orig_sae.cfg.to_dict() == new_sae.cfg.to_dict()
    assert torch.allclose(orig_sae.W_dec, new_sae.W_dec)
    assert torch.allclose(orig_sae.W_enc, new_sae.W_enc)
    assert torch.allclose(orig_sae.b_enc, new_sae.b_enc)
    assert torch.allclose(orig_sae.b_dec, new_sae.b_dec)
def test_parse_cfg_args_prints_help_if_no_args():
    args = []
    with pytest.raises(SystemExit):
        _parse_cfg_args(args)
def test_parse_cfg_args_override():
    args = [
        "--model_name",
        "test-model",
        "--d_in",
        "1024",
        "--d_sae",
        "4096",
        "--activation_fn",
        "tanh-relu",
        "--normalize_sae_decoder",
        "False",
        "--dataset_path",
        "my/dataset",
    ]
    cfg = _parse_cfg_args(args)
    assert cfg.model_name == "test-model"
    assert cfg.d_in == 1024
    assert cfg.d_sae == 4096
    assert cfg.activation_fn == "tanh-relu"
    assert cfg.normalize_sae_decoder is False
    assert cfg.dataset_path == "my/dataset"
def test_parse_cfg_args_dict_args():
    # Test that we can pass dict args as json strings
    args = [
        "--model_kwargs",
        '{"foo": "bar", "baz": 123}',
        "--model_from_pretrained_kwargs",
        '{"center_writing_weights": false}',
        "--activation_fn_kwargs",
        '{"k": 100}',
    ]
    cfg = _parse_cfg_args(args)
    assert cfg.model_kwargs == {"foo": "bar", "baz": 123}
    assert cfg.model_from_pretrained_kwargs == {"center_writing_weights": False}
    assert cfg.activation_fn_kwargs == {"k": 100}
def test_parse_cfg_args_invalid_json():
    args = ["--model_kwargs", "{invalid json"]
    with pytest.raises(argparse.ArgumentError, match="invalid json_dict value"):
        _parse_cfg_args(args)
def test_parse_cfg_args_invalid_dict_type():
    # Test that we reject non-dict values for dict fields
    args = ["--model_kwargs", "[1, 2, 3]"]  # Array instead of dict
    with pytest.raises(argparse.ArgumentError, match="invalid json_dict value"):
        _parse_cfg_args(args)
    args = ["--model_from_pretrained_kwargs", '"not_a_dict"']  # String instead of dict
    with pytest.raises(argparse.ArgumentError, match="invalid json_dict value"):
        _parse_cfg_args(args)
    args = ["--activation_fn_kwargs", "123"]  # Number instead of dict
    with pytest.raises(argparse.ArgumentError, match="invalid json_dict value"):
        _parse_cfg_args(args)
def test_parse_cfg_args_expansion_factor():
    # Test that we can't set both d_sae and expansion_factor
    args = ["--d_sae", "1024", "--expansion_factor", "8"]
    with pytest.raises(ValueError):
        _parse_cfg_args(args)
def test_parse_cfg_args_b_dec_init_method():
    # Test validation of b_dec_init_method
    args = ["--b_dec_init_method", "invalid"]
    with pytest.raises(ValueError):
        cfg = _parse_cfg_args(args)
    valid_methods = ["geometric_median", "mean", "zeros"]
    for method in valid_methods:
        args = ["--b_dec_init_method", method]
        cfg = _parse_cfg_args(args)
        assert cfg.b_dec_init_method == method
def test_run_cli_saves_config(tmp_path: Path):
    # Set up args for a minimal training run
    args = [
        "--model_name",
        TINYSTORIES_MODEL,
        "--dataset_path",
        TINYSTORIES_DATASET,
        "--checkpoint_path",
        str(tmp_path),
        "--n_checkpoints",
        "1",  # Save one checkpoint
        "--training_tokens",
        "128",
        "--train_batch_size_tokens",
        "4",
        "--store_batch_size_prompts",
        "4",
        "--log_to_wandb",
        "False",  # Don't log to wandb in test
        "--d_in",
        "64",  # Match gelu-1l hidden size
        "--d_sae",
        "128",  # Small SAE for test
        "--activation_fn",
        "relu",
        "--normalize_sae_decoder",
        "False",
    ]
    # Run training
    _run_cli(args)
    # Check that checkpoint was saved
    run_dirs = list(tmp_path.glob("*"))  # run dirs
    assert len(run_dirs) == 1
    checkpoint_dirs = list(run_dirs[0].glob("*"))
    assert len(checkpoint_dirs) == 1
    # Load and verify saved config
    with open(checkpoint_dirs[0] / "cfg.json") as f:
        saved_cfg = json.load(f)
    # Verify key config values were saved correctly
    assert saved_cfg["model_name"] == TINYSTORIES_MODEL
    assert saved_cfg["d_in"] == 64
    assert saved_cfg["d_sae"] == 128
    assert saved_cfg["activation_fn"] == "relu"
    assert saved_cfg["normalize_sae_decoder"] is False
    assert saved_cfg["dataset_path"] == TINYSTORIES_DATASET
    assert saved_cfg["n_checkpoints"] == 1
    assert saved_cfg["training_tokens"] == 128
    assert saved_cfg["train_batch_size_tokens"] == 4
    assert saved_cfg["store_batch_size_prompts"] == 4
    assert saved_cfg["model_name"] == TINYSTORIES_MODEL
def test_sae_training_runner_works_with_huggingface_models(tmp_path: Path):
    cfg = build_sae_cfg(
        d_in=64,
        d_sae=128,
        hook_layer=0,
        hook_name="transformer.h.0",
        checkpoint_path=str(tmp_path),
        model_class_name="AutoModelForCausalLM",
        model_name="roneneldan/TinyStories-1M",
        training_tokens=128,
        train_batch_size_tokens=4,
        store_batch_size_prompts=4,
        n_checkpoints=1,
        log_to_wandb=False,
    )
    runner = SAETrainingRunner(cfg)
    runner.run()
    # Check that checkpoint was saved
    checkpoint_dirs = list(tmp_path.glob("*"))  # run dirs
    assert len(checkpoint_dirs) == 1
    # Load and verify saved config
    with open(checkpoint_dirs[0] / "cfg.json") as f:
        saved_cfg = json.load(f)
    assert saved_cfg["model_name"] == "roneneldan/TinyStories-1M"
    assert saved_cfg["training_tokens"] == 128
    assert saved_cfg["train_batch_size_tokens"] == 4
    assert saved_cfg["store_batch_size_prompts"] == 4
    assert saved_cfg["log_to_wandb"] is False
    assert saved_cfg["model_class_name"] == "AutoModelForCausalLM"
    sae = SAE.load_from_pretrained(str(checkpoint_dirs[0]))
    assert isinstance(sae, SAE)

================
File: tests/training/test_sae_training.py
================
from typing import Any
import einops
import pytest
import torch
from datasets import Dataset
from transformer_lens import HookedTransformer
from sae_lens.config import LanguageModelSAERunnerConfig
from sae_lens.training.activations_store import ActivationsStore
from sae_lens.training.sae_trainer import SAETrainer
from sae_lens.training.training_sae import JumpReLU, TrainingSAE
from tests.helpers import build_sae_cfg
# Define a new fixture for different configurations
@pytest.fixture(
    params=[
        {
            "model_name": "tiny-stories-1M",
            "dataset_path": "roneneldan/TinyStories",
            "hook_name": "blocks.1.hook_resid_pre",
            "hook_layer": 1,
            "d_in": 64,
        },
        {
            "model_name": "tiny-stories-1M",
            "dataset_path": "roneneldan/TinyStories",
            "hook_name": "blocks.1.hook_resid_pre",
            "hook_layer": 1,
            "d_in": 64,
            "normalize_sae_decoder": False,
            "scale_sparsity_penalty_by_decoder_norm": True,
        },
        {
            "model_name": "tiny-stories-1M",
            "dataset_path": "apollo-research/roneneldan-TinyStories-tokenizer-gpt2",
            "hook_name": "blocks.1.hook_resid_pre",
            "hook_layer": 1,
            "d_in": 64,
        },
        {
            "model_name": "tiny-stories-1M",
            "dataset_path": "apollo-research/roneneldan-TinyStories-tokenizer-gpt2",
            "hook_name": "blocks.1.hook_resid_pre",
            "hook_layer": 1,
            "d_in": 64,
            "normalize_activations": "constant_norm_rescale",
        },
        {
            "model_name": "tiny-stories-1M",
            "dataset_path": "roneneldan/TinyStories",
            "hook_name": "blocks.1.attn.hook_z",
            "hook_layer": 1,
            "d_in": 64,
        },
    ],
    ids=[
        "tiny-stories-1M-resid-pre",
        "tiny-stories-1M-resid-pre-L1-W-dec-Norm",
        "tiny-stories-1M-resid-pre-pretokenized",
        "tiny-stories-1M-resid-pre-pretokenized-norm-rescale",
        "tiny-stories-1M-hook-z",
    ],
)
def cfg(request: pytest.FixtureRequest):
    """
    Pytest fixture to create a mock instance of LanguageModelSAERunnerConfig.
    """
    params = request.param
    return build_sae_cfg(**params)
@pytest.fixture
def training_sae(cfg: Any):
    """
    Pytest fixture to create a mock instance of SparseAutoencoder.
    """
    return TrainingSAE(cfg)
@pytest.fixture
def activation_store(model: HookedTransformer, cfg: LanguageModelSAERunnerConfig):
    return ActivationsStore.from_config(
        model, cfg, override_dataset=Dataset.from_list([{"text": "hello world"}] * 2000)
    )
@pytest.fixture
def model(cfg: LanguageModelSAERunnerConfig):
    return HookedTransformer.from_pretrained(cfg.model_name, device="cpu")
# todo: remove the need for this fixture
@pytest.fixture
def trainer(
    cfg: LanguageModelSAERunnerConfig,
    training_sae: TrainingSAE,
    model: HookedTransformer,
    activation_store: ActivationsStore,
):
    return SAETrainer(
        model=model,
        sae=training_sae,
        activation_store=activation_store,
        save_checkpoint_fn=lambda *args, **kwargs: None,  # noqa: ARG005
        cfg=cfg,
    )
# TODO: DECIDE IF WE ARE KEEPING ENCODE AND DECODE METHODS
# def test_sparse_autoencoder_encode(training_sae: TrainingSparseAutoencoder):
#     batch_size = 32
#     d_in = training_sae.d_in
#     d_sae = training_sae.d_sae
#     x = torch.randn(batch_size, d_in)
#     feature_acts1 = training_sae.encode(x)
#     _, cache = training_sae.run_with_cache(x, names_filter="hook_sae_acts_post")
#     feature_acts2 = cache["hook_sae_acts_post"]
#     # Check shape
#     assert feature_acts2.shape == (batch_size, d_sae)
#     # Check values
#     assert torch.allclose(feature_acts1, feature_acts2)
# def test_sparse_autoencoder_decode(training_sae: TrainingSparseAutoencoder):
#     batch_size = 32
#     d_in = training_sae.d_in
#     x = torch.randn(batch_size, d_in)
#     sae_out1 = training_sae(x)
#     assert sae_out1.shape == x.shape
#     assert torch.allclose(sae_out1, sae_out2)
def test_sae_forward(training_sae: TrainingSAE):
    batch_size = 32
    d_in = training_sae.cfg.d_in
    d_sae = training_sae.cfg.d_sae
    x = torch.randn(batch_size, d_in)
    train_step_output = training_sae.training_forward_pass(
        sae_in=x,
        current_l1_coefficient=training_sae.cfg.l1_coefficient,
    )
    assert train_step_output.sae_out.shape == (batch_size, d_in)
    assert train_step_output.feature_acts.shape == (batch_size, d_sae)
    assert (
        pytest.approx(train_step_output.loss.detach(), rel=1e-3)
        == (
            train_step_output.losses["mse_loss"]
            + train_step_output.losses["l1_loss"]
            + train_step_output.losses.get("ghost_grad_loss", 0.0)
        )
        .detach()  # type: ignore
        .cpu()
        .numpy()
    )
    expected_mse_loss = (
        (torch.pow((train_step_output.sae_out - x.float()), 2))
        .sum(dim=-1)
        .mean()
        .detach()
        .float()
    )
    assert (
        pytest.approx(train_step_output.losses["mse_loss"].item()) == expected_mse_loss  # type: ignore
    )
    if not training_sae.cfg.scale_sparsity_penalty_by_decoder_norm:
        expected_l1_loss = train_step_output.feature_acts.sum(dim=1).mean(dim=(0,))
    else:
        expected_l1_loss = (
            (train_step_output.feature_acts * training_sae.W_dec.norm(dim=1))
            .norm(dim=1, p=1)
            .mean()
        )
    assert (
        pytest.approx(train_step_output.losses["l1_loss"].item(), rel=1e-3)  # type: ignore
        == training_sae.cfg.l1_coefficient * expected_l1_loss.detach().float()
    )
def test_sae_forward_with_mse_loss_norm(
    training_sae: TrainingSAE,
):
    # change the confgi and ensure the mse loss is calculated correctly
    training_sae.cfg.mse_loss_normalization = "dense_batch"
    training_sae.mse_loss_fn = training_sae._get_mse_loss_fn()
    batch_size = 32
    d_in = training_sae.cfg.d_in
    d_sae = training_sae.cfg.d_sae
    x = torch.randn(batch_size, d_in)
    train_step_output = training_sae.training_forward_pass(
        sae_in=x,
        current_l1_coefficient=training_sae.cfg.l1_coefficient,
    )
    assert train_step_output.sae_out.shape == (batch_size, d_in)
    assert train_step_output.feature_acts.shape == (batch_size, d_sae)
    assert "ghost_grad_loss" not in train_step_output.losses
    x_centred = x - x.mean(dim=0, keepdim=True)
    expected_mse_loss = (
        (
            torch.nn.functional.mse_loss(train_step_output.sae_out, x, reduction="none")
            / (1e-6 + x_centred.norm(dim=-1, keepdim=True))
        )
        .sum(dim=-1)
        .mean()
        .detach()
        .item()
    )
    assert (
        pytest.approx(train_step_output.losses["mse_loss"].item()) == expected_mse_loss  # type: ignore
    )
    assert (
        pytest.approx(train_step_output.loss.detach(), rel=1e-3)
        == (
            train_step_output.losses["mse_loss"]
            + train_step_output.losses["l1_loss"]
            + train_step_output.losses.get("ghost_grad_loss", 0.0)
        )
        .detach()  # type: ignore
        .numpy()
    )
    if not training_sae.cfg.scale_sparsity_penalty_by_decoder_norm:
        expected_l1_loss = train_step_output.feature_acts.sum(dim=1).mean(dim=(0,))
    else:
        expected_l1_loss = (
            (train_step_output.feature_acts * training_sae.W_dec.norm(dim=1))
            .norm(dim=1, p=1)
            .mean()
        )
    assert (
        pytest.approx(train_step_output.losses["l1_loss"].item(), rel=1e-3)  # type: ignore
        == training_sae.cfg.l1_coefficient * expected_l1_loss.detach().float()
    )
def test_SparseAutoencoder_forward_ghost_grad_loss_non_zero(
    training_sae: TrainingSAE,
):
    training_sae.cfg.use_ghost_grads = True
    batch_size = 32
    d_in = training_sae.cfg.d_in
    x = torch.randn(batch_size, d_in)
    train_step_output = training_sae.training_forward_pass(
        sae_in=x,
        current_l1_coefficient=training_sae.cfg.l1_coefficient,
        dead_neuron_mask=torch.ones_like(
            training_sae.b_enc
        ).bool(),  # all neurons are dead.
    )
    assert train_step_output.losses["ghost_grad_loss"] != 0.0
def test_calculate_ghost_grad_loss(
    trainer: SAETrainer,
):
    training_sae = trainer.sae
    trainer.cfg.use_ghost_grads = True
    batch_size = 32
    d_in = trainer.cfg.d_in
    x = torch.randn(batch_size, d_in)
    trainer.sae.train()
    # set n_forward passes since fired to < dead feature window for all neurons
    trainer.n_forward_passes_since_fired = (
        torch.ones_like(trainer.n_forward_passes_since_fired)
        * 3
        * trainer.cfg.dead_feature_window
    )  # type: ignore
    # then set the first 10 neurons to have fired recently
    trainer.n_forward_passes_since_fired[:10] = 0
    feature_acts = training_sae.encode(x)
    sae_out = training_sae.decode(feature_acts)
    _, hidden_pre = training_sae.encode_with_hidden_pre(x)
    ghost_grad_loss = training_sae.calculate_ghost_grad_loss(
        x=x,
        sae_out=sae_out,
        per_item_mse_loss=training_sae.mse_loss_fn(sae_out, x),
        hidden_pre=hidden_pre,
        dead_neuron_mask=trainer.dead_neurons,
    )
    ghost_grad_loss.backward()  # type: ignore
    # W_enc grad
    assert trainer.sae.W_enc.grad is not None
    assert torch.allclose(
        trainer.sae.W_enc.grad[:, :10], torch.zeros_like(trainer.sae.W_enc[:, :10])
    )
    assert trainer.sae.W_enc.grad[:, 10:].abs().sum() > 0.001
    # only features 1 and 3 should have non-zero gradients on the decoder weights
    assert trainer.sae.W_dec.grad is not None
    assert torch.allclose(
        trainer.sae.W_dec.grad[:10, :], torch.zeros_like(trainer.sae.W_dec[:10, :])
    )
    assert trainer.sae.W_dec.grad[10:, :].abs().sum() > 0.001
def test_per_item_mse_loss_with_norm_matches_original_implementation(
    training_sae: TrainingSAE,
) -> None:
    training_sae.cfg.mse_loss_normalization = "dense_batch"
    training_sae.mse_loss_fn = training_sae._get_mse_loss_fn()
    input = torch.randn(3, 2)
    target = torch.randn(3, 2)
    target_centered = target - target.mean(dim=0, keepdim=True)
    orig_impl_res = (
        torch.pow((input - target.float()), 2)
        / (target_centered**2).sum(dim=-1, keepdim=True).sqrt()
    )
    sae_res = training_sae.mse_loss_fn(
        input,
        target,
    )
    assert torch.allclose(orig_impl_res, sae_res, atol=1e-5)
def test_SparseAutoencoder_forward_can_add_noise_to_hidden_pre() -> None:
    clean_cfg = build_sae_cfg(d_in=2, d_sae=4, noise_scale=0)
    noisy_cfg = build_sae_cfg(d_in=2, d_sae=4, noise_scale=100)
    clean_sae = TrainingSAE.from_dict(clean_cfg.get_training_sae_cfg_dict())
    noisy_sae = TrainingSAE.from_dict(noisy_cfg.get_training_sae_cfg_dict())
    input = torch.randn(3, 2)
    clean_output1 = clean_sae.forward(input)
    clean_output2 = clean_sae.forward(input)
    noisy_output1 = noisy_sae.forward(input)
    noisy_output2 = noisy_sae.forward(input)
    # with no noise, the outputs should be identical
    assert torch.allclose(clean_output1, clean_output2)
    # noisy outputs should be different
    assert not torch.allclose(noisy_output1, noisy_output2)
    assert not torch.allclose(clean_output1, noisy_output1)
def test_SparseAutoencoder_remove_gradient_parallel_to_decoder_directions(
    training_sae: TrainingSAE,
) -> None:
    if not training_sae.cfg.normalize_sae_decoder:
        pytest.skip("Test only applies when decoder is not normalized")
    sae = training_sae
    orig_grad = torch.randn_like(sae.W_dec)
    orig_W_dec = sae.W_dec.clone()
    sae.W_dec.grad = orig_grad.clone()
    sae.remove_gradient_parallel_to_decoder_directions()
    # check that the gradient is orthogonal to the decoder directions
    parallel_component = einops.einsum(
        sae.W_dec.grad,
        sae.W_dec.data,
        "d_sae d_in, d_sae d_in -> d_sae",
    )
    assert torch.allclose(
        parallel_component, torch.zeros_like(parallel_component), atol=1e-5
    )
    # the decoder weights should not have changed
    assert torch.allclose(sae.W_dec, orig_W_dec)
    # the gradient delta should align with the decoder directions
    grad_delta = orig_grad - sae.W_dec.grad
    assert torch.nn.functional.cosine_similarity(
        sae.W_dec.detach(), grad_delta, dim=1
    ).abs() == pytest.approx(1.0, abs=1e-3)
def test_SparseAutoencoder_set_decoder_norm_to_unit_norm(
    trainer: SAETrainer,
) -> None:
    if not trainer.cfg.normalize_sae_decoder:
        pytest.skip("Test only applies when decoder is not normalized")
    sae = trainer.sae
    sae.W_dec.data = 20 * torch.randn_like(sae.W_dec)
    sae.set_decoder_norm_to_unit_norm()
    assert torch.allclose(
        torch.norm(sae.W_dec, dim=1), torch.ones_like(sae.W_dec[:, 0])
    )
def test_jumprelu_forward():
    x = torch.tensor([-1.0, 0.5, 1.5, 2.5])
    threshold = torch.tensor(1.0)
    expected_output = torch.tensor([0.0, 0.0, 1.5, 2.5])
    output = JumpReLU.apply(x, threshold, 0.001)
    assert torch.allclose(output, expected_output)  # type: ignore
def test_jumprelu_backward():
    x = torch.tensor([-1.0, 0.5, 1.5, 2.5], requires_grad=True)
    threshold = torch.tensor(1.0)
    output = JumpReLU.apply(x, threshold, 0.001)
    output.sum().backward()  # type: ignore
    expected_grad_x = torch.tensor([0.0, 0.0, 1.0, 1.0])
    assert torch.allclose(x.grad, expected_grad_x)  # type: ignore
def test_jumprelu_backward_with_threshold_grad():
    bandwidth = 1e-3
    threshold_value = 1.0
    threshold = torch.tensor(threshold_value, requires_grad=True)
    epsilon = 1e-8
    x = torch.tensor(
        [
            threshold_value - bandwidth / 2 + epsilon,
            threshold_value + bandwidth / 2 - epsilon,
        ],
        requires_grad=True,
    )
    output = JumpReLU.apply(x, threshold, bandwidth)
    output.sum().backward()  # type: ignore
    expected_grad_x = torch.tensor([0.0, 1.0])
    expected_grad_threshold = torch.tensor(-2000.0)
    atol = 1e-2
    assert torch.allclose(x.grad, expected_grad_x, atol=atol)  # type: ignore
    assert torch.isclose(threshold.grad, expected_grad_threshold, atol=atol)  # type: ignore

================
File: tests/training/test_tokenization_and_batching.py
================
from itertools import islice
import pytest
import torch
from sae_lens.tokenization_and_batching import (
    _add_tokens_to_batch,
    concat_and_batch_sequences,
)
def test_add_tokens_to_batch_can_start_a_new_batch():
    tokens = torch.arange(10)
    new_batch, new_offset = _add_tokens_to_batch(
        batch=None, tokens=tokens, offset=1, context_size=5, is_start_of_sequence=True
    )
    assert torch.all(new_batch == tokens[1:6])
    assert new_offset == 6
def test_add_tokens_to_batch_adds_bos_if_new_batch():
    tokens = torch.arange(10)
    new_batch, new_offset = _add_tokens_to_batch(
        batch=None,
        tokens=tokens,
        offset=0,
        context_size=5,
        is_start_of_sequence=True,
        begin_batch_token_id=999,
        sequence_separator_token_id=998,
    )
    remaining_tokens = tokens[new_offset:]
    assert new_batch.tolist() == [999] + tokens[:4].tolist()
    assert torch.all(remaining_tokens == tokens[4:])
def test_add_tokens_respects_token_offset_when_adding_bos_if_new_batch():
    tokens = torch.arange(10)
    new_batch, new_offset = _add_tokens_to_batch(
        batch=None,
        tokens=tokens,
        offset=2,
        context_size=5,
        is_start_of_sequence=True,
        begin_batch_token_id=999,
        sequence_separator_token_id=998,
    )
    assert new_batch.tolist() == [999] + tokens[2:6].tolist()
    assert new_offset == 6
def test_add_tokens_to_batch_does_not_adds_bos_if_the_bos_is_already_there():
    tokens = torch.tensor([999, 1, 2, 3])
    new_batch, new_offset = _add_tokens_to_batch(
        batch=None,
        tokens=tokens,
        offset=0,
        context_size=5,
        is_start_of_sequence=True,
        begin_batch_token_id=999,
    )
    remaining_tokens = tokens[new_offset:]
    assert new_batch.tolist() == tokens.tolist()
    assert len(remaining_tokens) == 0
def test_add_tokens_to_batch_uses_all_tokens_if_less_than_context_size():
    tokens = torch.arange(3)
    new_batch, new_offset = _add_tokens_to_batch(
        batch=None,
        tokens=tokens,
        offset=0,
        context_size=5,
        is_start_of_sequence=False,
    )
    remaining_tokens = tokens[new_offset:]
    assert torch.all(new_batch == tokens)
    assert remaining_tokens.shape == (0,)
def test_add_tokens_to_batch_can_append_both_the_bos_and_start_of_sequence_token():
    tokens = torch.arange(10)
    new_batch, new_offset = _add_tokens_to_batch(
        batch=None,
        tokens=tokens,
        offset=0,
        context_size=5,
        is_start_of_sequence=True,
        begin_batch_token_id=999,
        begin_sequence_token_id=998,
    )
    remaining_tokens = tokens[new_offset:]
    assert new_batch.tolist() == [999, 998] + tokens[:3].tolist()
    assert torch.all(remaining_tokens == tokens[3:])
def test_add_tokens_to_batch_appends_to_the_existing_batch():
    batch = torch.arange(4)
    tokens = torch.arange(10)
    new_batch, new_offset = _add_tokens_to_batch(
        batch=batch,
        tokens=tokens,
        offset=0,
        context_size=5,
        is_start_of_sequence=True,
    )
    remaining_tokens = tokens[new_offset:]
    assert new_batch.tolist() == batch.tolist() + tokens[:1].tolist()
    assert torch.all(remaining_tokens == tokens[1:])
def test_add_tokens_to_batch_can_separate_sequences():
    batch = torch.arange(3)
    tokens = torch.arange(10)
    new_batch, new_offset = _add_tokens_to_batch(
        batch=batch,
        tokens=tokens,
        offset=0,
        context_size=5,
        is_start_of_sequence=True,
        sequence_separator_token_id=997,
        begin_batch_token_id=999,
    )
    remaining_tokens = tokens[new_offset:]
    assert new_batch.tolist() == batch.tolist() + [997] + tokens[:1].tolist()
    assert torch.all(remaining_tokens == tokens[1:])
def test_add_tokens_to_batch_can_both_separate_sequences_and_add_seq_start_token():
    batch = torch.arange(2)
    tokens = torch.arange(10)
    new_batch, new_offset = _add_tokens_to_batch(
        batch=batch,
        tokens=tokens,
        offset=0,
        context_size=5,
        is_start_of_sequence=True,
        sequence_separator_token_id=997,
        begin_sequence_token_id=998,
        begin_batch_token_id=999,
    )
    remaining_tokens = tokens[new_offset:]
    assert new_batch.tolist() == batch.tolist() + [997, 998] + tokens[:1].tolist()
    assert torch.all(remaining_tokens == tokens[1:])
@pytest.mark.parametrize("offset", [0, 1, 3])
def test_add_tokens_to_batch_wont_return_more_remaining_tokens_than_the_original(
    offset: int,
):
    batch = torch.arange(4)
    tokens = torch.arange(10)
    new_batch, new_offset = _add_tokens_to_batch(
        batch=batch,
        tokens=tokens,
        offset=offset,
        context_size=5,
        is_start_of_sequence=True,
        sequence_separator_token_id=997,
        begin_sequence_token_id=998,
        begin_batch_token_id=999,
    )
    assert new_batch.tolist() == batch.tolist() + [997]
    assert new_offset == offset
def test_add_tokens_to_batch_collapses_separate_sequences_and_add_seq_start_token_if_identical():
    batch = torch.arange(2)
    tokens = torch.arange(10)
    new_batch, new_offset = _add_tokens_to_batch(
        batch=batch,
        tokens=tokens,
        offset=0,
        context_size=5,
        is_start_of_sequence=True,
        sequence_separator_token_id=998,
        begin_sequence_token_id=998,
        begin_batch_token_id=999,
    )
    remaining_tokens = tokens[new_offset:]
    assert new_batch.tolist() == batch.tolist() + [998] + tokens[:2].tolist()
    assert torch.all(remaining_tokens == tokens[2:])
def test_add_tokens_to_batch_skips_add_seq_start_token_if_not_start_of_seq():
    batch = torch.arange(2)
    tokens = torch.arange(10)
    new_batch, new_offset = _add_tokens_to_batch(
        batch=batch,
        tokens=tokens,
        offset=0,
        context_size=5,
        is_start_of_sequence=False,
        sequence_separator_token_id=997,
        begin_sequence_token_id=998,
    )
    remaining_tokens = tokens[new_offset:]
    assert new_batch.tolist() == batch.tolist() + [997] + tokens[:2].tolist()
    assert torch.all(remaining_tokens == tokens[2:])
def test_concat_and_batch_sequences_generates_context_size_sequences():
    all_toks = torch.arange(20)
    seqs = [all_toks[:3], all_toks[3:10], all_toks[10:17], all_toks[17:]]
    batches_list = list(
        concat_and_batch_sequences(
            tokens_iterator=iter(seqs),
            context_size=5,
        )
    )
    batches = torch.stack(batches_list)
    assert batches.shape == (4, 5)
    assert torch.all(batches == all_toks.reshape(4, 5))
def test_concat_and_batch_sequences_drops_the_final_batch_if_too_small():
    all_toks = torch.arange(19)
    seqs = [all_toks[:3], all_toks[3:10], all_toks[10:17], all_toks[17:]]
    batches_list = list(
        concat_and_batch_sequences(
            tokens_iterator=iter(seqs),
            context_size=5,
        )
    )
    batches = torch.stack(batches_list)
    assert batches.shape == (3, 5)
    assert torch.all(batches == all_toks[:15].reshape(3, 5))
def test_concat_and_batch_sequences_can_ensure_everything_starts_with_bos():
    all_toks = torch.arange(19)
    seqs = [all_toks[:3], all_toks[3:10], all_toks[10:17], all_toks[17:]]
    batches_list = list(
        concat_and_batch_sequences(
            tokens_iterator=iter(seqs),
            context_size=5,
            begin_batch_token_id=999,
        )
    )
    batches = torch.stack(batches_list)
    expected = [
        [999, 0, 1, 2, 3],
        [999, 4, 5, 6, 7],
        [999, 8, 9, 10, 11],
        [999, 12, 13, 14, 15],
    ]
    assert batches.tolist() == expected
def test_concat_and_batch_sequences_can_ensure_each_seq_starts_with_a_token():
    all_toks = torch.arange(19)
    seqs = [all_toks[:3], all_toks[3:10], all_toks[10:17], all_toks[17:]]
    batches_list = list(
        concat_and_batch_sequences(
            tokens_iterator=iter(seqs),
            context_size=5,
            begin_sequence_token_id=998,
        )
    )
    batches = torch.stack(batches_list)
    expected = [
        [998, 0, 1, 2, 998],
        [3, 4, 5, 6, 7],
        [8, 9, 998, 10, 11],
        [12, 13, 14, 15, 16],
    ]
    assert batches.tolist() == expected
def test_concat_and_batch_sequences_can_ensure_each_seq_is_separated_with_a_token():
    all_toks = torch.arange(19)
    seqs = [all_toks[:3], all_toks[3:10], all_toks[10:17], all_toks[17:]]
    batches_list = list(
        concat_and_batch_sequences(
            tokens_iterator=iter(seqs),
            context_size=5,
            sequence_separator_token_id=997,
        )
    )
    batches = torch.stack(batches_list)
    expected = [
        [0, 1, 2, 997, 3],
        [4, 5, 6, 7, 8],
        [9, 997, 10, 11, 12],
        [13, 14, 15, 16, 997],
    ]
    assert batches.tolist() == expected
def test_concat_and_batch_sequences_can_use_all_token_types():
    all_toks = torch.arange(19)
    seqs = [all_toks[:3], all_toks[3:8], all_toks[8:11], all_toks[11:17], all_toks[17:]]
    batches_list = list(
        concat_and_batch_sequences(
            tokens_iterator=iter(seqs),
            context_size=5,
            sequence_separator_token_id=997,
            begin_sequence_token_id=998,
            begin_batch_token_id=999,
        )
    )
    batches = torch.stack(batches_list)
    expected = [
        [999, 998, 0, 1, 2],
        [999, 998, 3, 4, 5],
        [999, 6, 7, 997, 998],
        [999, 8, 9, 10, 997],
        [999, 11, 12, 13, 14],
        [999, 15, 16, 997, 998],
    ]
    assert batches.tolist() == expected
def test_concat_and_batch_collapses_identical_special_tokens():
    all_toks = torch.arange(19)
    seqs = [all_toks[:3], all_toks[3:8], all_toks[8:11], all_toks[11:17], all_toks[17:]]
    batches_list = list(
        concat_and_batch_sequences(
            tokens_iterator=iter(seqs),
            context_size=5,
            sequence_separator_token_id=999,
            begin_sequence_token_id=999,
            begin_batch_token_id=999,
        )
    )
    batches = torch.stack(batches_list)
    expected = [
        [999, 0, 1, 2, 999],
        [999, 3, 4, 5, 6],
        [999, 7, 999, 8, 9],
        [999, 10, 999, 11, 12],
        [999, 13, 14, 15, 16],
    ]
    assert batches.tolist() == expected
def test_concat_and_batch_sequences_works_with_extremely_long_samples():
    seq = torch.arange(10_000_000)
    batches_list = list(
        islice(
            concat_and_batch_sequences(
                tokens_iterator=iter([seq]),
                context_size=5,
                begin_sequence_token_id=999,
                begin_batch_token_id=999,
            ),
            50_000,  # cut off after 50k batchs so test still runs fast
        )
    )
    assert len(batches_list) == 50_000
    for batch in batches_list:
        assert batch.shape == (5,)
        assert batch[0] == 999

================
File: tests/training/test_training_sae.py
================
from copy import deepcopy
from pathlib import Path
import pytest
import torch
from sae_lens.sae import SAE
from sae_lens.training.training_sae import (
    TrainingSAE,
    TrainingSAEConfig,
    _calculate_topk_aux_acts,
)
from tests.helpers import build_sae_cfg
@pytest.mark.parametrize("scale_sparsity_penalty_by_decoder_norm", [True, False])
def test_TrainingSAE_training_forward_pass_can_scale_sparsity_penalty_by_decoder_norm(
    scale_sparsity_penalty_by_decoder_norm: bool,
):
    cfg = build_sae_cfg(
        d_in=3,
        d_sae=5,
        scale_sparsity_penalty_by_decoder_norm=scale_sparsity_penalty_by_decoder_norm,
        normalize_sae_decoder=False,
    )
    training_sae = TrainingSAE(TrainingSAEConfig.from_sae_runner_config(cfg))
    x = torch.randn(32, 3)
    train_step_output = training_sae.training_forward_pass(
        sae_in=x,
        current_l1_coefficient=2.0,
    )
    feature_acts = train_step_output.feature_acts
    decoder_norm = training_sae.W_dec.norm(dim=1)
    # double-check decoder norm is not all ones, or this test is pointless
    assert not torch.allclose(decoder_norm, torch.ones_like(decoder_norm), atol=1e-2)
    scaled_feature_acts = feature_acts * decoder_norm
    if scale_sparsity_penalty_by_decoder_norm:
        assert (
            pytest.approx(train_step_output.losses["l1_loss"].detach().item())  # type: ignore
            == 2.0 * scaled_feature_acts.norm(p=1, dim=1).mean().detach().item()
        )
    else:
        assert (
            pytest.approx(train_step_output.losses["l1_loss"].detach().item())  # type: ignore
            == 2.0 * feature_acts.norm(p=1, dim=1).mean().detach().item()
        )
def test_calculate_topk_aux_acts():
    # Create test inputs
    k_aux = 3
    hidden_pre = torch.tensor(
        [
            [1.0, 2.0, -3.0, 4.0, -5.0, 6.0],
            [-1.0, -2.0, 3.0, -4.0, 5.0, -6.0],
            [0.1, 0.2, 0.3, 0.4, 0.5, 0.6],
            [-0.6, -0.5, -0.4, -0.3, -0.2, -0.1],
        ]
    )
    # Create dead neuron mask where neurons 1,3,5 are dead
    dead_neuron_mask = torch.tensor([False, True, False, True, False, True])
    # Calculate expected result
    # For each row, should select top k_aux=3 values from dead neurons (indices 1,3,5)
    # and zero out all other values
    expected = torch.zeros_like(hidden_pre)
    expected[0, [1, 3, 5]] = torch.tensor([2.0, 4.0, 6.0])
    expected[1, [1, 3, 5]] = torch.tensor([-2.0, -4.0, -6.0])
    expected[2, [1, 3, 5]] = torch.tensor([0.2, 0.4, 0.6])
    expected[3, [1, 3, 5]] = torch.tensor([-0.5, -0.3, -0.1])
    result = _calculate_topk_aux_acts(k_aux, hidden_pre, dead_neuron_mask)
    assert torch.allclose(result, expected)
def test_calculate_topk_aux_acts_k_less_than_dead():
    # Create test inputs with k_aux less than number of dead neurons
    k_aux = 1  # Only select top 1 dead neuron
    hidden_pre = torch.tensor(
        [
            [1.0, 2.0, -3.0, 4.0],  # 2 items in batch
            [-1.0, -2.0, 3.0, -4.0],
        ]
    )
    # Create dead neuron mask where neurons 1,3 are dead (2 dead neurons)
    dead_neuron_mask = torch.tensor([False, True, False, True])
    # Calculate expected result
    # For each row, should select only top k_aux=1 value from dead neurons (indices 1,3)
    # and zero out all other values
    expected = torch.zeros_like(hidden_pre)
    expected[0, 3] = 4.0  # Only highest value among dead neurons for first item
    expected[1, 1] = -2.0  # Only highest value among dead neurons for second item
    result = _calculate_topk_aux_acts(k_aux, hidden_pre, dead_neuron_mask)
    assert torch.allclose(result, expected)
def test_TrainingSAE_calculate_topk_aux_loss():
    # Create a small test SAE with d_sae=4, d_in=3
    cfg = build_sae_cfg(
        d_in=3,
        d_sae=4,
        architecture="topk",
        normalize_sae_decoder=False,
    )
    sae = TrainingSAE(TrainingSAEConfig.from_sae_runner_config(cfg))
    # Set up test inputs
    hidden_pre = torch.tensor(
        [[1.0, -2.0, 3.0, -4.0], [1.0, 0.0, -3.0, -4.0]]  # batch size 2
    )
    sae.W_dec.data = torch.tensor(2 * torch.ones((4, 3)))
    sae.b_dec.data = torch.tensor(torch.zeros(3))
    sae_out = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])
    sae_in = torch.tensor([[2.0, 1.0, 3.0], [5.0, 4.0, 6.0]])
    # Mark neurons 1 and 3 as dead
    dead_neuron_mask = torch.tensor([False, True, False, True])
    # Calculate loss
    loss = sae.calculate_topk_aux_loss(
        sae_in=sae_in,
        hidden_pre=hidden_pre,
        sae_out=sae_out,
        dead_neuron_mask=dead_neuron_mask,
    )
    # The loss should:
    # 1. Select top k_aux=2 (half of d_sae) dead neurons
    # 2. Decode their activations (should be 2x the sum of the activations of the dead neurons)
    # thus, (-12, -12, -12), (-8, -8, -8)
    # and the residual is (1, -1, 0), (1, -1, 0)
    # Thus, squared errors are (169, 121, 144), (81, 49, 64)
    # and the sums are (434, 194)
    # and the mean of these is 314
    assert loss == 314
def test_TrainingSAE_forward_includes_topk_loss_with_topk_architecture():
    cfg = build_sae_cfg(
        d_in=3,
        d_sae=4,
        architecture="topk",
        activation_fn_kwargs={"k": 2},
        normalize_sae_decoder=False,
    )
    sae = TrainingSAE(TrainingSAEConfig.from_sae_runner_config(cfg))
    x = torch.randn(32, 3)
    train_step_output = sae.training_forward_pass(
        sae_in=x,
        current_l1_coefficient=2.0,
        dead_neuron_mask=None,
    )
    assert "auxiliary_reconstruction_loss" in train_step_output.losses
    assert train_step_output.losses["auxiliary_reconstruction_loss"] == 0.0
def test_TrainingSAE_forward_includes_topk_loss_is_nonzero_if_dead_neurons_present():
    cfg = build_sae_cfg(
        d_in=3,
        d_sae=4,
        architecture="topk",
        activation_fn_kwargs={"k": 2},
        normalize_sae_decoder=False,
    )
    sae = TrainingSAE(TrainingSAEConfig.from_sae_runner_config(cfg))
    x = torch.randn(32, 3)
    train_step_output = sae.training_forward_pass(
        sae_in=x,
        current_l1_coefficient=2.0,
        dead_neuron_mask=torch.tensor([False, True, False, True]),
    )
    assert "auxiliary_reconstruction_loss" in train_step_output.losses
    assert train_step_output.losses["auxiliary_reconstruction_loss"] > 0.0
@pytest.mark.parametrize("architecture", ["standard", "gated", "jumprelu", "topk"])
def test_TrainingSAE_encode_returns_same_value_as_encode_with_hidden_pre(
    architecture: str,
):
    cfg = build_sae_cfg(architecture=architecture)
    sae = TrainingSAE(TrainingSAEConfig.from_sae_runner_config(cfg))
    x = torch.randn(32, cfg.d_in)
    encode_out = sae.encode(x)
    encode_with_hidden_pre_out = sae.encode_with_hidden_pre_fn(x)[0]
    assert torch.allclose(encode_out, encode_with_hidden_pre_out)
def test_TrainingSAE_initializes_only_with_log_threshold_if_jumprelu():
    cfg = build_sae_cfg(architecture="jumprelu", jumprelu_init_threshold=0.01)
    sae = TrainingSAE(TrainingSAEConfig.from_sae_runner_config(cfg))
    param_names = dict(sae.named_parameters()).keys()
    assert "log_threshold" in param_names
    assert "threshold" not in param_names
    assert torch.allclose(
        sae.threshold,
        torch.ones_like(sae.log_threshold.data) * cfg.jumprelu_init_threshold,
    )
def test_TrainingSAE_jumprelu_save_and_load(tmp_path: Path):
    cfg = build_sae_cfg(architecture="jumprelu")
    training_sae = TrainingSAE.from_dict(cfg.get_training_sae_cfg_dict())
    training_sae.save_model(str(tmp_path))
    loaded_training_sae = TrainingSAE.load_from_pretrained(str(tmp_path))
    loaded_sae = SAE.load_from_pretrained(str(tmp_path))
    assert training_sae.cfg.to_dict() == loaded_training_sae.cfg.to_dict()
    for param_name, param in training_sae.named_parameters():
        assert torch.allclose(param, loaded_training_sae.state_dict()[param_name])
    test_input = torch.randn(32, cfg.d_in)
    training_sae_out = training_sae.encode_with_hidden_pre_fn(test_input)[0]
    loaded_training_sae_out = loaded_training_sae.encode_with_hidden_pre_fn(test_input)[
        0
    ]
    loaded_sae_out = loaded_sae.encode(test_input)
    assert torch.allclose(training_sae_out, loaded_training_sae_out)
    assert torch.allclose(training_sae_out, loaded_sae_out)
@torch.no_grad()
def test_TrainingSAE_fold_w_dec_norm_jumprelu():
    cfg = build_sae_cfg(architecture="jumprelu")
    sae = TrainingSAE.from_dict(cfg.get_training_sae_cfg_dict())
    # make sure all parameters are not 0s
    for param in sae.parameters():
        param.data = torch.rand_like(param)
    assert sae.W_dec.norm(dim=-1).mean().item() != pytest.approx(1.0, abs=1e-6)
    sae2 = deepcopy(sae)
    sae2.fold_W_dec_norm()
    # fold_W_dec_norm should normalize W_dec to have unit norm.
    assert sae2.W_dec.norm(dim=-1).mean().item() == pytest.approx(1.0, abs=1e-6)
    W_dec_norms = sae.W_dec.norm(dim=-1).unsqueeze(1)
    assert torch.allclose(sae2.b_enc, sae.b_enc * W_dec_norms.squeeze())
    assert torch.allclose(sae2.threshold, sae.threshold * W_dec_norms.squeeze())
    # we expect activations of features to differ by W_dec norm weights.
    activations = torch.randn(10, 4, cfg.d_in, device=cfg.device)
    feature_activations_1 = sae.encode(activations)
    feature_activations_2 = sae2.encode(activations)
    assert torch.allclose(
        feature_activations_1.nonzero(),
        feature_activations_2.nonzero(),
    )
    expected_feature_activations_2 = feature_activations_1 * sae.W_dec.norm(dim=-1)
    torch.testing.assert_close(feature_activations_2, expected_feature_activations_2)
    sae_out_1 = sae.decode(feature_activations_1)
    sae_out_2 = sae2.decode(feature_activations_2)
    # but actual outputs should be the same
    torch.testing.assert_close(sae_out_1, sae_out_2)

================
File: tests/training/test_upload_saes_to_huggingface.py
================
from pathlib import Path
from textwrap import dedent
import pytest
from huggingface_hub import HfApi
from sae_lens.sae import SAE
from sae_lens.training.upload_saes_to_huggingface import (
    _build_sae_path,
    _create_default_readme,
    _repo_exists,
    _repo_file_exists,
    _validate_sae_path,
)
from tests.helpers import build_sae_cfg
def test_create_default_readme():
    saes_dict = ["sae1", "sae2"]
    expected_readme = dedent(
        """
        ---
        library_name: saelens
        ---
        # SAEs for use with the SAELens library
        This repository contains the following SAEs:
        - sae1
        - sae2
        Load these SAEs using SAELens as below:
        ```python
        from sae_lens import SAE
        sae, cfg_dict, sparsity = SAE.from_pretrained("jimi/hendrix", "<sae_id>")
        ```
        """
    ).strip()
    assert _create_default_readme("jimi/hendrix", saes_dict) == expected_readme
def test_build_sae_path_saves_live_saes_to_tmpdir(tmp_path: Path):
    cfg = build_sae_cfg(device="cpu")
    sae = SAE.from_dict(cfg.get_base_sae_cfg_dict())
    sae_path = _build_sae_path(sae, str(tmp_path))
    assert sae_path == tmp_path
    assert (tmp_path / "sae_weights.safetensors").exists()
    assert (tmp_path / "cfg.json").exists()
def test_build_sae_path_directly_passes_through_existing_dirs(tmp_path: Path):
    assert _build_sae_path("/sae/path", str(tmp_path)) == Path("/sae/path")
    assert _build_sae_path(Path("/sae/path"), str(tmp_path)) == Path("/sae/path")
def test_repo_exists():
    api = HfApi()
    assert not _repo_exists(api, "fake/repo")
    assert _repo_exists(api, "jbloom/Gemma-2b-Residual-Stream-SAEs")
def test_repo_file_exists():
    assert _repo_file_exists(
        "jbloom/Gemma-2b-Residual-Stream-SAEs", "README.md", "main"
    )
    assert not _repo_file_exists(
        "jbloom/Gemma-2b-Residual-Stream-SAEs", "fake_file.md", "main"
    )
def test_validate_sae_path_errors_if_files_are_missing(tmp_path: Path):
    with pytest.raises(FileNotFoundError):
        _validate_sae_path(tmp_path)
    (tmp_path / "cfg.json").touch()
    with pytest.raises(FileNotFoundError):
        _validate_sae_path(tmp_path)
    (tmp_path / "sae_weights.safetensors").touch()
    _validate_sae_path(tmp_path)

================
File: tutorials/mamba_train_example.py
================
# install from https://github.com/Phylliida/MambaLens
import os
import sys
sys.path.append(os.path.join(os.path.dirname(os.path.abspath(__file__)), ".."))
# run this as python3 tutorials/mamba_train_example.py
# i.e. from the root directory
from sae_lens.config import LanguageModelSAERunnerConfig
from sae_lens.sae_training_runner import SAETrainingRunner
cfg = LanguageModelSAERunnerConfig(
    # Data Generating Function (Model + Training Distibuion)
    model_name="state-spaces/mamba-370m",
    model_class_name="HookedMamba",
    hook_name="blocks.39.hook_ssm_input",
    hook_layer=39,
    hook_eval="blocks.39.hook_ssm_output",  # we compare this when replace hook_point activations with autoencode.decode(autoencoder.encode( hook_point activations))
    d_in=2048,
    dataset_path="NeelNanda/openwebtext-tokenized-9b",
    is_dataset_tokenized=True,
    # SAE Parameters
    expansion_factor=64,
    b_dec_init_method="geometric_median",
    # Training Parameters
    lr=0.0004,
    l1_coefficient=0.00006 * 0.2,
    lr_scheduler_name="cosineannealingwarmrestarts",
    train_batch_size_tokens=4096,
    context_size=128,
    lr_warm_up_steps=5000,
    # Activation Store Parameters
    n_batches_in_buffer=128,
    training_tokens=1_000_000 * 300,
    store_batch_size_prompts=32,
    # Dead Neurons and Sparsity
    use_ghost_grads=True,
    feature_sampling_window=1000,
    dead_feature_window=5000,
    dead_feature_threshold=1e-6,
    # WANDB
    log_to_wandb=True,
    wandb_project="sae_training_mamba",
    wandb_entity=None,
    wandb_log_frequency=100,
    # Misc
    device="cuda",
    seed=42,
    checkpoint_path="checkpoints",
    dtype="float32",
    model_kwargs={
        "fast_ssm": True,
        "fast_conv": True,
    },
)
SAETrainingRunner(cfg).run()

================
File: tutorials/tsea.py
================
import os
import re
import string
from typing import Optional
import nltk
import numpy as np
import pandas as pd
import plotly_express as px
import torch
from babe import UsNames
from transformer_lens import HookedTransformer
from sae_lens import logger
def get_enrichment_df(
    projections: torch.Tensor,
    features: list[int],
    gene_sets_selected: dict[str, set[int]],
):
    gene_sets_token_ids_padded = pad_gene_sets(gene_sets_selected)
    gene_sets_token_ids_tensor = torch.tensor(list(gene_sets_token_ids_padded.values()))
    enrichment_scores = calculate_batch_enrichment_scores(
        projections[features], gene_sets_token_ids_tensor
    )
    return pd.DataFrame(
        enrichment_scores.numpy(),
        index=gene_sets_selected.keys(),  # type: ignore
        columns=features,  # type: ignore
    )
def calculate_batch_enrichment_scores(scores: torch.Tensor, index_lists: torch.Tensor):
    """
    # features with large skew
    features_top_800_by_prediction_skew = W_U_stats_df_dec["skewness"].sort_values(ascending=False).head(12000).index
    gene_sets_index = ["starts_with_space", "starts_with_capi", "all_digits", "is_punctuation"]
    gene_sets_temp = {k:v for k,v in gene_sets_token_ids_padded.items() if k in gene_sets_index}
    gene_sets_token_ids_tensor = torch.tensor([value for value in gene_sets_temp.values()])
    gene_sets_token_ids_tensor.shape
    gene_sets = gene_sets_token_ids_tensor
    enrichment_scores = calculate_batch_enrichment_scores(dec_projection_onto_W_U[features_top_800_by_prediction_skew], gene_sets_token_ids_tensor)
    df_enrichment_scores = pd.DataFrame(enrichment_scores.numpy(), index=gene_sets_index, columns=features_top_800_by_prediction_skew)
    """
    n_sets, _ = index_lists.shape
    n_scores, vocab_size = scores.shape
    # Ensure scores and index_lists are on the same device
    scores = scores.to(index_lists.device)
    # Create a mask for valid indices (ignore padding)
    valid_mask = index_lists != -1  # Assuming -1 is used for padding
    # Initialize a mask for all scores
    score_mask = torch.zeros(
        n_sets, vocab_size, device=index_lists.device, dtype=torch.bool
    )
    # Set true for valid indices in score_mask
    for i in range(n_sets):
        score_mask[i, index_lists[i][valid_mask[i]]] = True
    # Sort scores along each row
    _, sorted_indices = scores.sort(dim=1, descending=True)
    # Create a mask to identify hits within the sorted indices
    hits = (
        score_mask.unsqueeze(1)
        .expand(-1, n_scores, -1)
        .gather(2, sorted_indices.unsqueeze(0).expand(n_sets, -1, -1))
    )
    # Calculate hit increment and miss decrement dynamically for each list
    list_sizes = valid_mask.sum(dim=1).float()  # Actual sizes of each list
    hit_increment = (1.0 / list_sizes).view(-1, 1, 1)  # Reshape for broadcasting
    miss_decrement = (1.0 / (vocab_size - list_sizes)).view(
        -1, 1, 1
    )  # Reshape for broadcasting
    # Ensure hit_increment and miss_decrement are broadcastable to the shape of hits
    # Apply hit increment or miss decrement based on hits
    running_sums = torch.where(hits, hit_increment, -miss_decrement).cumsum(dim=2)
    return running_sums.abs().max(dim=2).values
def manhattan_plot_enrichment_scores(
    df_enrichment_scores: pd.DataFrame, label_threshold: float = 1.0, top_n: int = 3
):
    tmp_df = df_enrichment_scores.apply(lambda x: -1 * np.log(1 - x))
    # wide to long format
    tmp_df = tmp_df.reset_index().melt(
        id_vars="index", var_name="Feature", value_name="Enrichment Score"
    )
    tmp_df.rename(columns={"index": "gene_set"}, inplace=True)
    tmp_df.reset_index(drop=True, inplace=True)
    fig = px.scatter(
        tmp_df,
        x="Feature",
        y="Enrichment Score",
        color=tmp_df.gene_set,
        facet_col=tmp_df.gene_set,
        labels={"index": "", "value": "Enrichment Score", "variable": "Token Set"},
        width=1400,
        height=500,
    )
    fig.update_traces(marker={"size": 3})
    #  only annotate the top n points in each gene set
    annotation_df = (
        tmp_df.groupby("gene_set")
        .apply(lambda x: x.nlargest(top_n, "Enrichment Score"))
        .reset_index(drop=True)
    )
    gene_set_to_subplot = {
        gene_set: i + 1 for i, gene_set in enumerate(tmp_df["gene_set"].unique())
    }
    # Annotate all points above the label_threshold
    for _, row in annotation_df.iterrows():
        if row["Enrichment Score"] > label_threshold:
            # Find the subplot index
            subplot_index = gene_set_to_subplot[row["gene_set"]]
            # Add annotation at the position of the point that exceeds the threshold
            fig.add_annotation(
                x=row["Feature"],
                y=row["Enrichment Score"],
                text=row["Feature"],  # Or any other text you want to display
                showarrow=False,
                arrowhead=1,
                xref=f"x{subplot_index}",  # Refer to the correct x-axis
                yref=f"y{subplot_index}",  # Refer to the correct y-axis
                ax=20,  # Adjusts the x position of the arrow (try changing this if needed)
                ay=-30,  # Adjusts the y position of the arrow (try changing this if needed)
                yshift=15,
            )
    # relabel facet cols to remove gene_set
    fig.for_each_annotation(lambda a: a.update(text=a.text.split("=")[-1]))
    # hide legend
    fig.update_layout(showlegend=False)
    # increase font size
    fig.update_layout(font={"size": 16})
    return fig
def plot_top_k_feature_projections_by_token_and_category(
    gene_sets_selected: dict[str, set[int]],
    df_enrichment_scores: pd.DataFrame,
    category: str,
    model: HookedTransformer,
    dec_projection_onto_W_U: torch.Tensor,
    k: int = 5,
    projection_onto: str = "W_U",
    features: Optional[list[int]] = None,
    log_y: bool = True,
    histnorm: Optional[str] = None,
):
    if not os.path.exists("es_plots"):
        os.makedirs("es_plots")
    enrichment_scores = df_enrichment_scores.filter(like=category, axis=0).T
    if features is None:
        features = (
            enrichment_scores.sort_values(category, ascending=False).head(k).index
        ).to_list()
    # scores = enrichment_scores[category][features]
    scores = enrichment_scores[category].loc[features]
    logger.debug(scores)
    tokens_list = [model.to_single_str_token(i) for i in list(range(model.cfg.d_vocab))]
    logger.debug(features)
    feature_logit_scores = pd.DataFrame(
        dec_projection_onto_W_U[features].numpy(),
        index=features,  # type: ignore
    ).T
    feature_logit_scores["token"] = tokens_list
    feature_logit_scores[category] = [
        i in gene_sets_selected[category] for i in list(range(model.cfg.d_vocab))
    ]
    # display(feature_)
    logger.debug(category)
    for feature, score in zip(features, scores):  # type: ignore
        logger.debug(feature)
        score = -1 * np.log(1 - score)  # convert to enrichment score
        fig = px.histogram(
            feature_logit_scores,
            x=feature,
            color=category,
            title=f"W_dec_{feature}, {projection_onto}, {category}: {score:2.2f}",
            barmode="overlay",
            histnorm=histnorm,
            log_y=log_y,
            hover_name="token",
            marginal="box",
            width=800,
            height=400,
            labels={f"{feature}": f"W_U W_dec[{feature}]"},
        )
        # increase the font size
        fig.update_layout(font={"size": 16})
        fig.show()
        fig.write_html(
            f"es_plots/{feature}_projection_onto_{projection_onto}_by_{category}.html"
        )
def pad_gene_sets(gene_sets_token_ids: dict[str, set[int]]) -> dict[str, list[int]]:
    for k, v in gene_sets_token_ids.items():
        gene_sets_token_ids[k] = list(v)  # type: ignore
    max_len = max([len(v) for v in gene_sets_token_ids.values()])
    # pad with -1's to max length
    return {
        key: value + [-1] * (max_len - len(value))  # type: ignore
        for key, value in gene_sets_token_ids.items()
    }
def get_baby_name_sets(vocab: dict[str, int], k: int = 300) -> dict[str, list[int]]:
    d = UsNames()
    baby_df = d.data
    boy_names = baby_df[baby_df.gender == "M"].name.value_counts().head(k).index
    girl_names = baby_df[baby_df.gender == "F"].name.value_counts().head(k).index
    # prepend spaces
    boy_names = [f"Ġ{name}" for name in boy_names]
    girl_names = [f"Ġ{name}" for name in girl_names]
    # get all the tokens in the tokenizer that are in each of thes
    names = {"boy_names": [], "girl_names": []}
    for token, id in vocab.items():
        if token in boy_names:
            names["boy_names"].append(id)
        elif token in girl_names:
            names["girl_names"].append(id)
    return names
def get_letter_gene_sets(vocab: dict[str, int]) -> dict[str, set[int]]:
    letters = string.ascii_lowercase
    gene_sets = {letter: set() for letter in letters}
    for token, id in vocab.items():
        clean_token = token.strip("Ġ")  # Remove leading 'Ġ'
        if (
            clean_token.isalpha() and clean_token[0].lower() in letters
        ):  # Check if the first character is in letters
            gene_sets[clean_token[0].lower()].add(id)
    return gene_sets
def generate_pos_sets(vocab: dict[str, int]) -> dict[str, set[int]]:
    # tagged_tokens = nltk.pos_tag([i.strip("Ġ") for i in list(vocab.keys())])
    # tagged_vocab = {word: tag for word, tag in tagged_tokens}
    pos_sets = {}
    for token, id in vocab.items():
        clean_token = token.strip("Ġ")  # Remove leading 'Ġ'
        tagged_token = nltk.pos_tag([clean_token])
        tag = tagged_token[0][1]
        if f"nltk_pos_{tag}" not in pos_sets:
            pos_sets[f"nltk_pos_{tag}"] = set()
        pos_sets[f"nltk_pos_{tag}"].add(id)
    return pos_sets
def get_gene_set_from_regex(vocab: dict[str, int], pattern: str) -> set[int]:
    gene_set = set()
    for token, id in vocab.items():
        if re.match(pattern, token):
            gene_set.add(id)
    return gene_set
def get_test_gene_sets(model: HookedTransformer) -> dict[str, set[int]]:
    colors = [
        "red",
        "blue",
        "yellow",  # Primary colors
        "green",
        "orange",
        "purple",  # Secondary colors
        "pink",
        "teal",
        "lavender",
        "maroon",
        "olive",
        "navy",
        "grey",  # Tertiary and common colors
        "black",
        "white",
        "brown",  # Basics
    ]
    negative_words = [
        "terrible",
        "awful",
        "horrible",
        "dreadful",
        "abysmal",
        "wretched",
        "dire",
        "appalling",
        "horrific",
        "disastrous",
        "ghastly",
        "hideous",
        "gruesome",
        "vile",
        "foul",
        "atrocious",
        "heinous",
        "abhorrent",
        "detestable",
        "loathsome",
        "repulsive",
        "repugnant",
        "disgusting",
        "revolting",
        "noxious",
        "offensive",
        "nauseating",
        "sickening",
        "distasteful",
        "unpleasant",
        "obnoxious",
        "odious",
        "unsavory",
        "unpalatable",
        "grim",
        "gloomy",
        "deplorable",
        "depressing",
        "despicable",
        "miserable",
        "pathetic",
        "pitiful",
        "lamentable",
        "direful",
        "tragic",
        "woeful",
        "painful",
        "harsh",
        "bitter",
    ]
    positive_words = [
        "wonderful",
        "amazing",
        "fabulous",
        "excellent",
        "fantastic",
        "brilliant",
        "awesome",
        "spectacular",
        "marvelous",
        "incredible",
        "superb",
        "magical",
        "delightful",
        "charming",
        "beautiful",
        "astonishing",
        "impressive",
        "stunning",
        "breathtaking",
        "admirable",
        "lovely",
        "pleasing",
        "enchanting",
        "exquisite",
        "radiant",
        "splendid",
        "glorious",
        "divine",
        "sublime",
        "heavenly",
        "idyllic",
        "blissful",
        "serene",
        "tranquil",
        "peaceful",
        "joyful",
        "ecstatic",
        "jubilant",
        "elated",
        "uplifting",
        "inspiring",
        "revitalizing",
        "refreshing",
        "invigorating",
        "energizing",
        "thrilling",
        "captivating",
        "enthralling",
        "enlightening",
    ]
    emotions = [
        "anger",
        "fear",
        "joy",
        "sadness",  # Basic emotions
        "trust",
        "disgust",
        "anticipation",
        "surprise",  # Complex emotions
        "love",
        "hate",
        "envy",
        "compassion",
        "pride",
        "shame",
        "guilt",
        "hope",
        "despair",  # Complex emotions
    ]
    boys_names = [
        "Michael",
        "James",
        "John",
        "Robert",
        "David",
        "William",
        "Joseph",
        "Charles",
        "Thomas",
        "Christopher",
    ]
    girls_names = [
        "Mary",
        "Patricia",
        "Jennifer",
        "Linda",
        "Elizabeth",
        "Barbara",
        "Susan",
        "Jessica",
        "Sarah",
        "Karen",
    ]
    capital_cities = [
        "Washington, D.C.",
        "Ottawa",
        "London",
        "Paris",
        "Berlin",
        "Tokyo",
        "Moscow",
        "Beijing",
        "Canberra",
        "New Delhi",
    ]
    countries = [
        "United States",
        "Canada",
        "United Kingdom",
        "France",
        "Germany",
        "Japan",
        "Russia",
        "China",
        "Australia",
        "India",
    ]
    neuroscience_terms = [
        "Neuron",
        "Synapse",
        "Axon",
        "Dendrite",
        "Neuroplasticity",
        "Cerebral cortex",
        "Neurotransmitter",
        "Myelin sheath",
        "Action potential",
        "Grey matter",
        "White matter",
        "Neurogenesis",
        "Neurotransmission",
        "Neurodegeneration",
        "Neuroinflammation",
        "Neurodevelopment",
        "Neuroimaging",
        "Neuropharmacology",
        "Neurophysiology",
        "Neuropsychology",
    ]
    neuroscience_terms = [i.lower() for i in neuroscience_terms]
    economics_terms = [
        "Supply and Demand",
        "Elasticity",
        "Gross Domestic Product (GDP)",
        "Inflation",
        "Monetary policy",
        "Fiscal policy",
        "Marginal utility",
        "Opportunity cost",
        "Equilibrium price",
        "Market efficiency",
        "Monopoly",
        "Oligopoly",
        "Monopolistic competition",
        "Perfect competition",
        "Economic surplus",
        "Consumer surplus",
        "Producer surplus",
        "Deadweight loss",
        "Economic rent",
        "Externality",
    ]
    economics_terms = [i.lower() for i in economics_terms]
    spanish_words = [
        "hola",
        "amor",
        "feliz",
        "casa",
        "familia",
        "gracias",
        "libro",
        "mañana",
        "noche",
        "amigo",
    ]
    french_words = [
        "bonjour",
        "amour",
        "heureux",
        "maison",
        "famille",
        "merci",
        "livre",
        "matin",
        "nuit",
        "ami",
    ]
    jewish_last_names = [
        "Bloom",
        "Levine",
        "Goldstein",
        "Cohen",
        "Katz",
        "Kaplan",
        "Adler",
        "Stein",
        "Weiss",
        "Stern",
        "Cohen",
        "Levi",
        "Katz",
        "Kahan",
        "Weiss",
        "Gross",
        "Friedman",
        "Kramer",
        "Grossman",
        "Zimmerman",
    ]
    ologies = [
        "Biology",
        "Ecology",
        "Psychology",
        "Sociology",
        "Geology",
        "Meteorology",
        "Zoology",
        "Botany",
        "Anthropology",
        "Astrology",
        "Astronomy",
        "Theology",
        "Philology",
        "Pharmacology",
        "Pathology",
        "Oceanology",
        "Toxicology",
        "Volcanology",
        "Entomology",
        "Paleontology",
        "Neurology",
        "Ethnology",
        "Criminology",
        "Seismology",
        "Cytology",
    ]
    gene_sets = {
        "1910's": [str(i) for i in range(1910, 1920)],
        "1920's": [str(i) for i in range(1920, 1930)],
        "1930's": [str(i) for i in range(1930, 1940)],
        "1940's": [str(i) for i in range(1940, 1950)],
        "1950's": [str(i) for i in range(1950, 1960)],
        "1960's": [str(i) for i in range(1960, 1970)],
        "1970's": [str(i) for i in range(1970, 1980)],
        "1980's": [str(i) for i in range(1980, 1990)],
        "1990's": [str(i) for i in range(1990, 2000)],
        "2000's": [str(i) for i in range(2000, 2010)],
        "2010's": [str(i) for i in range(2010, 2020)],
        "colors": colors,
        "positive_words": positive_words,
        "negative_words": negative_words,
        "emotions": emotions,
        "boys_names": boys_names,
        "girls_names": girls_names,
        "spanish_words": spanish_words,
        "french_words": french_words,
        "neuroscience_terms": neuroscience_terms,
        "economics_terms": economics_terms,
        "capital_cities": capital_cities,
        "countries": countries,
        "jewish_last_names": jewish_last_names,
        "ologies": ologies,
    }
    def convert_tokens_to_ids(
        list_of_strings: list[str], model: HookedTransformer
    ) -> set[int]:
        token_ids = [
            model.tokenizer.encode(f" {word}", add_special_tokens=False)  # type: ignore
            for word in list_of_strings
        ]
        token_ids = [item for sublist in token_ids for item in sublist]
        return set(token_ids)
    return {
        key: convert_tokens_to_ids(value, model) for key, value in gene_sets.items()
    }  # type: ignore



================================================================
End of Codebase
================================================================
