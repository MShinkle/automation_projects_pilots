This file is a merged representation of a subset of the codebase, containing specifically included files and files not matching ignore patterns, combined into a single document by Repomix.
The content has been processed where empty lines have been removed.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
- Pay special attention to the Repository Description. These contain important context and guidelines specific to this project.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: **/*.py, **/*.md, **/*.txt
- Files matching these patterns are excluded: **/.git/**, **/.github/**, CHANGELOG.md
- Empty lines have been removed from all files
- Files are sorted by Git change count (files with more changes are at the bottom)

Additional Info:
----------------
User Provided Header:
-----------------------
This file is a consolidated single-file compilation of all code in the repository generated by Repomix. Note that .ipynb files have been converted to .py files.

================================================================
Directory Structure
================================================================
docs/api/analysis.md
docs/api/ecco.md
docs/api/language-model.md
docs/api/nmf.md
docs/api/output.md
docs/architecture.md
docs/contribution_guide.md
docs/eccoJS/overview.md
docs/index.md
docs/models.md
docs/requirements.txt
readme.md
requirements.txt
setup.py
src/ecco/__init__.py
src/ecco/__main__.py
src/ecco/activations.py
src/ecco/analysis.py
src/ecco/attribution.py
src/ecco/cli.py
src/ecco/lm_plots.py
src/ecco/lm.py
src/ecco/output.py
src/ecco/svcca_lib/cca_core.py
src/ecco/svcca_lib/cka_lib.py
src/ecco/svcca_lib/pwcca.py
src/ecco/util.py
tests/activations_test.py
tests/analysis_tests.py
tests/lm_plots_test.py
tests/lm_test.py
tests/output_test.py
tests/test_ecco.py
tests/tokenizers_test.py

================================================================
Files
================================================================

================
File: docs/api/analysis.md
================
::: ecco.analysis

================
File: docs/api/ecco.md
================
**ecco.from__pretrained()**
::: ecco.from_pretrained

================
File: docs/api/language-model.md
================
::: ecco.lm.LM
    handler: python

================
File: docs/api/nmf.md
================
One of the ways in which Ecco tries to make Transformer language models more transparent is by making it easier to [examine the neuron activations](https://jalammar.github.io/explaining-transformers/) in the feed-forward neural network sublayer of [Transformer blocks](https://jalammar.github.io/illustrated-transformer/). 
Large language models can have up to billions of neurons. Direct examination of these neurons is not always insightful because their firing is sparse, there's a lot of redundancy, and their number makes it hard to extract a signal.

[Matrix decomposition](https://scikit-learn.org/stable/modules/decomposition.html) methods can give us a glimpse into the underlying patterns in neuron firing. From these methods, Ecco currently provides easy access to Non-negative Matrix Factorization (NMF).

## NMF

::: ecco.output.NMF

================
File: docs/api/output.md
================
::: ecco.output.OutputSeq

================
File: docs/architecture.md
================
Ecco is made up of two components:

- [Ecco](https://github.com/jalammar/ecco), a python component. Wraps around language models and collects relevant data. 
- [EccoJS](https://github.com/jalammar/eccojs), a Javascript component used to create interactive explorables from the outputs of Ecco.

All the machine learning happens in the Ecco. The results can be plotted by python, or interactive explorables are created using eccoJS.

================
File: docs/contribution_guide.md
================
## Building docs site locally
Make sure mkdocs is installed

`pip install mkdocs`

Install the requirements of the docs:

`pip install -r docs/requirements.txt`

Run mkdoc:

`mkdocs serve`

## Setting EccoJS development environment

1. Clone the eccoJS repository 
2. Install eccoJS dependencies by running `npm install`
3. Run rollup so it bundles a new version of eccoJS whenevver we change the files
```
rollup --config .\rollup.config.js --watch
```
4. eccoJS is bundled into a file (dist/ecco-bundle.min.js). Due to CORS protections in browsers, we'll have to run a python web server to serve the file locally.
```
python3 -m http.server
```
5.In ecco/html/setup.html, point the ecco_url towards a local path:
```
    //var ecco_url = 'https://storage.googleapis.com/ml-intro/ecco/'
    var ecco_url = 'http://localhost:8000/'
```
6. Whenever we build, copy ecco-bundle.min.js to wherever the server is pointing and ecco_url describes. (for convience, consider changing the file: in eccojs/rollup.config.js so the file is exported directly to where the python web server is serving from)

## Developing for EccoJS
EccoJS is mostly used inside Jupyter notebooks. The development process, however, is better done in a javascript test file to begin with. Only when that functionality is ready should we proceed to integrate it with python and Jupyter. 


The Javascript test files are in `eccoJS/test`. Some make assertions on Javascript code functionality, and some tests actually build a visualization in an HTML page for visual inspection. The automated tests run against the node.js bundle of Ecco.

Tests are run using the `tape` command. You can run a specific test file using the command `tape <file name>'.

================
File: docs/eccoJS/overview.md
================
Ecco loads EccoJS by running html/setup.html in the browser. This loads the javascript package and other required files.

setup.html
basic.html

================
File: docs/index.md
================
# Welcome to Ecco
Ecco is a python library for explaining Natural Language Processing models using interactive visualizations.

Language models are some of the most fascinating technologies. They are programs that can speak and <i>understand</i> language better than any technology we've had before. For the general audience, Ecco provides an easy way to start interacting with language models. For people closer to NLP, Ecco provides methods to visualize and interact with underlying mechanics of the language models.

Ecco runs inside Jupyter notebooks. It is built on top of [pytorch](https://pytorch.org/) and [transformers](https://github.com/huggingface/transformers).

Ecco is not concerned with training or fine-tuning models. Only exploring and understanding existing pre-trained models.

## Installation

You can install `ecco` either with `pip` or with `conda`.

**with pip**

```sh
pip install ecco
```

**with conda**

```sh
conda install -c conda-forge ecco
```

## Tutorials
- Video: [Take A Look Inside Language Models With Ecco](https://www.youtube.com/watch?v=rHrItfNeuh0). \[<a href="https://colab.research.google.com/github/jalammar/ecco/blob/main/notebooks/Language_Models_and_Ecco_PyData_Khobar.ipynb">Colab Notebook</a>]


## How-to Guides
- [Interfaces for Explaining Transformer Language Models](https://jalammar.github.io/explaining-transformers/)
- [Finding the Words to Say: Hidden State Visualizations for Language Models](https://jalammar.github.io/hidden-states/)


## API Reference
The [API reference](api/ecco) and the [architecture](architecture) page explain Ecco's components and how they work together.

## Gallery

<div class="container gallery" markdown="1">

<p><strong>Predicted Tokens:</strong> View the model's prediction for the next token (with probability scores). See how the predictions evolved through the model's layers. [<a href="https://github.com/jalammar/ecco/blob/main/notebooks/Ecco_Output_Token_Scores.ipynb">Notebook</a>] [<a href="https://colab.research.google.com/github/jalammar/ecco/blob/main/notebooks/Ecco_Output_Token_Scores.ipynb">Colab</a>]</p>
<img src="img/layer_predictions_ex_london.png" />
<hr />
<p><strong>Rankings across layers:</strong> After the model picks an output token, Look back at how each layer ranked that token.  [<a href="https://github.com/jalammar/ecco/blob/main/notebooks/Ecco_Evolution_of_Selected_Token.ipynb">Notebook</a>] [<a href="https://colab.research.google.com/github/jalammar/ecco/blob/main/notebooks/Ecco_Evolution_of_Selected_Token.ipynb">Colab</a>]</p>
<img src="img/rankings_ex_eu_1_widethumb.png" />
<hr />
<p><strong>Layer Predictions:</strong>Compare the rankings of multiple tokens as candidates for a certain position in the sequence.  [<a href="https://github.com/jalammar/ecco/blob/main/notebooks/Ecco_Comparing_Token_Rankings.ipynb">Notebook</a>] [<a href="https://colab.research.google.com/github/jalammar/ecco/blob/main/notebooks/Ecco_Comparing_Token_Rankings.ipynb">Colab</a>]</p>
<img src="img/rankings_watch_ex_is_are_widethumb.png" />
<hr />
<p><strong>Input Saliency:</strong> How much did each input token contribute to producing the output token?   [<a href="https://github.com/jalammar/ecco/blob/main/notebooks/Ecco_Input_Saliency.ipynb">Notebook</a>] [<a href="https://colab.research.google.com/github/jalammar/ecco/blob/main/notebooks/Ecco_Input_Saliency.ipynb">Colab</a>]
</p>
<img src="img/saliency_ex_1_thumbwide.png" />

<hr />
<p><strong>Detailed Saliency:</strong> See more precise input saliency values using the detailed view. [<a href="https://github.com/jalammar/ecco/blob/main/notebooks/Ecco_Input_Saliency.ipynb">Notebook</a>] [<a href="https://colab.research.google.com/github/jalammar/ecco/blob/main/notebooks/Ecco_Input_Saliency.ipynb">Colab</a>]
</p>
<img src="img/saliency_ex_2_thumbwide.png" />

<hr />
<p><strong>Neuron Activation Analysis:</strong> Examine underlying patterns in neuron activations using non-negative matrix factorization. [<a href="https://github.com/jalammar/ecco/blob/main/notebooks/Ecco_Neuron_Factors.ipynb">Notebook</a>] [<a href="https://colab.research.google.com/github/jalammar/ecco/blob/main/notebooks/Ecco_Neuron_Factors.ipynb">Colab</a>]</p>
<img src="img/nmf_ex_1_widethumb.png" />

</div>

## Getting Help
Having trouble?

- The [Discussion](https://github.com/jalammar/ecco/discussions) board might have some relevant information. If not, you can post your questions there.
- Report bugs at Ecco's [issue tracker](https://github.com/jalammar/ecco/issues)

================
File: docs/models.md
================
Ecco works as a wrapper around HuggingFace models. You can load a model from the Model Hub, as well load a local model. model-config.yaml contains configurations for a number of models that Eecco now supports, these include GPT2, BERT, RoBERTa, Albert, Electra, GPTNeo, and T5.

## Loading a local model
To load a local model, prepare its configuration file and pass that along with the model's path to `ecco.from_pretrained()`.

 ```python
    import ecco

    model_config = {
        'embedding': "transformer.wte.weight",
        'type': 'causal',
        'activations': ['mlp\.c_proj'],
        'token_prefix': ' ',
        'partial_token_prefix': ''
    }
    lm = ecco.from_pretrained('gpt2', model_config=model_config)
 ```

Make sure that both the model and its tokenizer are saved in the same path you're passing (the model will typically be a `pytorch_model.bin` file while the tokenizer will have files like `tokenizer.json`, `tokenizer_config.json`, and `vocab.json`).

### Configuration parameters
Ecco uses configuration parameters for its functionality. If trying to load a local model that builds on an existing model (a BERT finetune, for example), you can use the same configuration as BERT. If the model has custom layer names or a custom tokenizer, you'll need to pass the appropriate config parameters.

**'embedding'**: The name of the embeddings layer of the model. This is used to calculate gradient saliency. To get the name of the embedding layer of your model, Running `print(model)` shows the layers of the model. 

**'type'**: the options are 'causal' for GPT-like decoder-based models. 'mlm' for BERT-like encoder-based models. 'enc-dec' for encoder-decoder models like T5.

**'activations'**: The name of the Feed-forward neural network layer inside transformer blocks. This is used for capturing neuron activations.

**token_prefix**: Here we specify the characters that the tokenizer places in the beginning of tokens, if any. GPT2's tokenizer, for example, has a space ' '. BERT does not have any characers.

**partial_token_prefix**: The characters that the tokenizer places in the beginning of partial tokens. BERT, for example, places '##' at the beginning of a partial token.

================
File: docs/requirements.txt
================
mkdocs==1.1.2
mkdocs-material==6.2.8
mkdocstrings==0.14.0
-f https://download.pytorch.org/whl/torch_stable.html
#torch~=1.9.0
transformers~=4.2.2
matplotlib~=3.3.1
numpy~=1.19.1
ipython~=7.16.1
scikit-learn~=0.23.2
seaborn~=0.11.0
PyYAML==5.4.1

================
File: readme.md
================
<img src="https://ar.pegg.io/img/ecco-logo-w800.png" width="400" style="background-color: white" />

<br />
<br />

<!--- BADGES: START --->
[![GitHub - License](https://img.shields.io/github/license/jalammar/ecco?logo=github&style=flat&color=green)][#github-license]
[![PyPI - Latest Package Version](https://img.shields.io/pypi/v/ecco?logo=pypi&style=flat&color=orange)][#pypi-package]
[![PyPI - Supported Python Versions](https://img.shields.io/pypi/pyversions/ecco?logo=pypi&style=flat&color=blue)][#pypi-package]
[![Conda - Platform](https://img.shields.io/conda/pn/conda-forge/ecco?logo=anaconda&style=flat)][#conda-forge-package]
[![Conda (channel only)](https://img.shields.io/conda/vn/conda-forge/ecco?logo=anaconda&style=flat&color=orange)][#conda-forge-package]
[![Docs - GitHub.io](https://img.shields.io/static/v1?logo=readthedocs&style=flat&color=pink&label=docs&message=ecco)][#docs-package]


[#github-license]: https://github.com/jalammar/ecco/blob/main/LICENSE
[#pypi-package]: https://pypi.org/project/ecco/
[#conda-forge-package]: https://anaconda.org/conda-forge/ecco
[#docs-package]: https://ecco.readthedocs.io/
<!--- BADGES: END --->


Ecco is a python library for exploring and explaining Natural Language Processing models using interactive visualizations. 


Ecco provides multiple interfaces to aid the explanation and intuition of [Transformer](https//jalammar.github.io/illustrated-transformer/)-based language models. Read: [Interfaces for Explaining Transformer Language Models](https://jalammar.github.io/explaining-transformers/).

Ecco runs inside Jupyter notebooks. It is built on top of [pytorch](https://pytorch.org/) and [transformers](https://github.com/huggingface/transformers).


Ecco is not concerned with training or fine-tuning models. Only exploring and understanding existing pre-trained models. The library is currently an alpha release of a research project. You're welcome to contribute to make it better!


Documentation: [ecco.readthedocs.io](https://ecco.readthedocs.io/)

## Features
- Support for a wide variety of language models (GPT2, BERT, RoBERTA, T5, T0, and others) [[notebook & instructions for adding more models](https://github.com/jalammar/ecco/blob/main/notebooks/Identifying%20model%20configuration.ipynb)].
- Ability to add your own **local models** (if they're based on Hugging Face pytorch models).
- **Feature attribution** ([IntegratedGradients](https://arxiv.org/abs/1703.01365), [Saliency](https://arxiv.org/abs/1312.6034), [InputXGradient](https://arxiv.org/abs/1412.6815), [DeepLift](https://arxiv.org/abs/1704.02685), [DeepLiftShap](https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html), [GuidedBackprop](https://arxiv.org/abs/1412.6806), [GuidedGradCam](https://arxiv.org/abs/1610.02391), [Deconvolution](https://arxiv.org/abs/1311.2901), and [LRP](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0130140) via [Captum](https://captum.ai/))
- **Capture neuron activations** in the FFNN layer in the Transformer block
- Identify and **visualize neuron activation patterns**  (via Non-negative Matrix Factorization)
- Examine neuron activations via comparisons of activations spaces using [SVCCA](https://arxiv.org/abs/1706.05806), [PWCCA](https://arxiv.org/abs/1806.05759), and [CKA](https://arxiv.org/abs/1905.00414) (See [this video on inspecting neural networks with CCA](https://www.youtube.com/watch?v=u7Dvb_a1D-0))
- Visualizations for:
    - Evolution of processing a token through the layers of the model ([Logit lens](https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens))
    - Candidate output tokens and their probabilities (at each layer in the model)

## Installation

You can install `ecco` either with `pip` or with `conda`.

**with pip**

```sh
pip install ecco
```

**with conda**

```sh
conda install -c conda-forge ecco
```

## Examples:
You can run all these examples from this [[notebook](https://github.com/jalammar/ecco/blob/main/notebooks/readme.md%20examples.ipynb)] | [[colab](https://colab.research.google.com/github/jalammar/ecco/blob/main/notebooks/readme.md%20examples.ipynb)].
### What is the sentiment of this film review?
<img src="https://ar.pegg.io/img/ecco/ecco-sentiment-1.png" width="500px" />

Use a large language model (T5 in this case) to detect text sentiment. In addition to the sentiment, see the tokens the model broke the text into (which can help debug some edge cases).

### Which words in this review lead the model to classify its sentiment as "negative"?
<img src="https://ar.pegg.io/img/ecco/ecco-attrib-ig-1.png" width="500px" />

Feature attribution using Integrated Gradients helps you explore model decisions. In this case, switching "weakness" to "inclination" allows the model to correctly switch the prediction to *positive*.

### Explore the world knowledge of GPT models by posing fill-in-the blank questions.
<img src="https://ar.pegg.io/img/ecco/gpt2-heathrow-1.png" width="500px" alt="Asking GPT2 where heathrow airport is" />

Does GPT2 know where Heathrow Airport is? Yes. It does.

### What other cities/words did the model consider in addition to London?
<img src="https://ar.pegg.io/img/ecco/gpt-candidate-logits.png" width="500px" alt="The model also considered Birmingham and Manchester"/>

Visualize the candidate output tokens and their probability scores.

### Which input words lead it to think of London?
<img src="https://ar.pegg.io/img/ecco/heathrow-attribution.png" width="400px" alt="Asking GPT2 where heathrow airport is"/>

### At which layers did the model gather confidence that London is the right answer?
<img src="https://ar.pegg.io/img/ecco/token-evolution.png" width="200px" alt="The order of the token in each layer, layer 11 makes it number 1"/>

The model chose London by making the highest probability token (ranking it #1) after the last layer in the model. How much did each layer contribute to increasing the ranking of *London*? This is a [logit lens](https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens) visualizations that helps explore the activity of different model layers.

### What are the patterns in BERT neuron activation when it processes a piece of text? 
<img src="https://ar.pegg.io/img/ecco/neuron-bert.png" width="500px" alt="Colored line graphs on the left, a piece of text on the right. The line graphs indicate the activation of BERT neuron groups in response to the text"/>

A group of neurons in BERT tend to fire in response to commas and other punctuation. Other groups of neurons tend to fire in response to pronouns. Use this visualization to factorize neuron activity in individual FFNN layers or in the entire model.


Read the paper: 
>[Ecco: An Open Source Library for the Explainability of Transformer Language Models](https://aclanthology.org/2021.acl-demo.30/)
> Association for Computational Linguistics (ACL) System Demonstrations, 2021


## Tutorials
- Video: [Take A Look Inside Language Models With Ecco](https://www.youtube.com/watch?v=rHrItfNeuh0). \[<a href="https://colab.research.google.com/github/jalammar/ecco/blob/main/notebooks/Language_Models_and_Ecco_PyData_Khobar.ipynb">Colab Notebook</a>]


## How-to Guides
- [Interfaces for Explaining Transformer Language Models](https://jalammar.github.io/explaining-transformers/)
- [Finding the Words to Say: Hidden State Visualizations for Language Models](https://jalammar.github.io/hidden-states/)


## API Reference
The [API reference](https://ecco.readthedocs.io/en/main/api/ecco/) and the [architecture](https://ecco.readthedocs.io/en/main/architecture/) page explain Ecco's components and how they work together.

## Gallery & Examples

<div class="container gallery" markdown="1">

<p><strong>Predicted Tokens:</strong> View the model's prediction for the next token (with probability scores). See how the predictions evolved through the model's layers. [<a href="https://github.com/jalammar/ecco/blob/main/notebooks/Ecco_Output_Token_Scores.ipynb">Notebook</a>] [<a href="https://colab.research.google.com/github/jalammar/ecco/blob/main/notebooks/Ecco_Output_Token_Scores.ipynb">Colab</a>]</p>
<img src="docs/img/layer_predictions_ex_london.png" width="400" />
<hr />
<p><strong>Rankings across layers:</strong> After the model picks an output token, Look back at how each layer ranked that token.  [<a href="https://github.com/jalammar/ecco/blob/main/notebooks/Ecco_Evolution_of_Selected_Token.ipynb">Notebook</a>] [<a href="https://colab.research.google.com/github/jalammar/ecco/blob/main/notebooks/Ecco_Evolution_of_Selected_Token.ipynb">Colab</a>]</p>
<img src="docs/img/rankings_ex_eu_1_widethumb.png" width="400"/>
<hr />
<p><strong>Layer Predictions:</strong>Compare the rankings of multiple tokens as candidates for a certain position in the sequence.  [<a href="https://github.com/jalammar/ecco/blob/main/notebooks/Ecco_Comparing_Token_Rankings.ipynb">Notebook</a>] [<a href="https://colab.research.google.com/github/jalammar/ecco/blob/main/notebooks/Ecco_Comparing_Token_Rankings.ipynb">Colab</a>]</p>
<img src="docs/img/rankings_watch_ex_is_are_widethumb.png" width="400" />
<hr />
<p><strong>Primary Attributions:</strong> How much did each input token contribute to producing the output token?   [<a href="https://github.com/jalammar/ecco/blob/main/notebooks/Ecco_Primary_Attributions.ipynb">Notebook</a>] [<a href="https://colab.research.google.com/github/jalammar/ecco/blob/main/notebooks/Ecco_Primary_Attributions.ipynb">Colab</a>]
</p>
<img src="docs/img/saliency_ex_1_thumbwide.png" width="400"/>

<hr />
<p><strong>Detailed Primary Attributions:</strong> See more precise input attributions values using the detailed view. [<a href="https://github.com/jalammar/ecco/blob/main/notebooks/Ecco_Primary_Attributions.ipynb">Notebook</a>] [<a href="https://colab.research.google.com/github/jalammar/ecco/blob/main/notebooks/Ecco_Primary_Attributions.ipynb">Colab</a>]
</p>
<img src="docs/img/saliency_ex_2_thumbwide.png" width="400"/>

<hr />
<p><strong>Neuron Activation Analysis:</strong> Examine underlying patterns in neuron activations using non-negative matrix factorization. [<a href="https://github.com/jalammar/ecco/blob/main/notebooks/Ecco_Neuron_Factors.ipynb">Notebook</a>] [<a href="https://colab.research.google.com/github/jalammar/ecco/blob/main/notebooks/Ecco_Neuron_Factors.ipynb">Colab</a>]</p>
<img src="docs/img/nmf_ex_1_widethumb.png" width="400"/>

</div>

## Getting Help
Having trouble?

- The [Discussion](https://github.com/jalammar/ecco/discussions) board might have some relevant information. If not, you can post your questions there.
- Report bugs at Ecco's [issue tracker](https://github.com/jalammar/ecco/issues)



Bibtex for citations:
```bibtex
@inproceedings{alammar-2021-ecco,
    title = "Ecco: An Open Source Library for the Explainability of Transformer Language Models",
    author = "Alammar, J",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations",
    year = "2021",
    publisher = "Association for Computational Linguistics",
}
```

================
File: requirements.txt
================
matplotlib>=3.3
numpy>=1.19
ipython>=7.16
scikit-learn>=0.24.2,<2
seaborn>=0.11
transformers~=4.6
pytest>=6.1.2
setuptools>=49.6.0
torch>=1.9.0,<3
PyYAML>=6.0
captum~=0.4.1

================
File: setup.py
================
#!/usr/bin/env python
# -*- encoding: utf-8 -*-
from __future__ import absolute_import
from __future__ import print_function
import io
import re
from glob import glob
from os.path import basename
from os.path import dirname
from os.path import join
from os.path import splitext
from setuptools import find_packages
from setuptools import setup
def read(*names, **kwargs):
    with io.open(
        join(dirname(__file__), *names),
        encoding=kwargs.get('encoding', 'utf8')
    ) as fh:
        return fh.read()
setup(
    name='ecco',
    version='0.1.2',
    license='BSD-3-Clause',
    description='Visualization tools for NLP machine learning models.',
    long_description='%s\n%s' % (
        re.compile('^.. start-badges.*^.. end-badges', re.M | re.S).sub('', read('README.rst')),
        re.sub(':[a-z]+:`~?(.*?)`', r'``\1``', read('CHANGELOG.rst'))
    ),
    author='Jay Alammar',
    author_email='alammar@gmail.com',
    url='https://github.com/jalammar/ecco',
    packages=find_packages('src'),
    package_dir={'': 'src'},
    py_modules=[splitext(basename(path))[0] for path in glob('src/*.py')],
    include_package_data=True,
    zip_safe=False,
    classifiers=[
        # complete classifier list: http://pypi.python.org/pypi?%3Aaction=list_classifiers
        'Development Status :: 5 - Production/Stable',
        'Intended Audience :: Developers',
        'License :: OSI Approved :: BSD License',
        'Operating System :: Unix',
        'Operating System :: POSIX',
        'Operating System :: Microsoft :: Windows',
        'Programming Language :: Python',
        'Programming Language :: Python :: 3.7',
        'Programming Language :: Python :: 3.8',
        'Programming Language :: Python :: 3.9',
        'Programming Language :: Python :: 3.10',
        'Programming Language :: Python :: 3.11',
        'Programming Language :: Python :: Implementation :: CPython',
        'Programming Language :: Python :: Implementation :: PyPy',
        'Topic :: Utilities',
    ],
    project_urls={
        'Changelog': 'https://github.com/jalammar/ecco/blob/master/CHANGELOG.rst',
        'Issue Tracker': 'https://github.com/jalammar/ecco/issues',
    },
    keywords=[
        'Natural Language Processing', 'Explainable AI', 'keyword3',
    ],
    python_requires='!=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*, !=3.5.*, !=3.6.*',
    install_requires=[
        "transformers ~= 4.2",
        "seaborn >= 0.11",
        "scikit-learn>=0.23,<2",
        "PyYAML>=6.0",
        "captum ~= 0.4"
    ],
    extras_require={
        "dev": [
            "pytest>=6.1",
        ],
        # eg:
        #   'rst': ['docutils>=0.11'],
        #   ':python_version=="2.6"': ['argparse'],
    },
    entry_points={
        'console_scripts': [
            'ecco = ecco.cli:main',
        ]
    },
    dependency_links=[
        "https://download.pytorch.org/whl/torch_stable.html"
    ]
)

================
File: src/ecco/__init__.py
================
"""
This is main entry point to Ecco. `from_pretrained()` is used to initialize an [LM][ecco.lm.LM]
object which then we use as a language model like GPT2 (or masked language model like BERT).
Usage:
```
    import ecco
    lm = ecco.from_pretrained('distilgpt2')
```
"""
__version__ = '0.1.2'
from ecco.lm import LM
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel, AutoModelForSeq2SeqLM
from typing import Any, Dict, Optional, List
from ecco.util import load_config, pack_tokenizer_config
def from_pretrained(hf_model_id: str,
                    model_config: Optional[Dict[str, Any]] = None,
                    activations: Optional[bool] = False,
                    attention: Optional[bool] = False,
                    hidden_states: Optional[bool] = True,
                    activations_layer_nums: Optional[List[int]] = None,
                    verbose: Optional[bool] = True,
                    gpu: Optional[bool] = True
                    ):
    """
    Constructs a [LM][ecco.lm.LM] object based on a string identifier from HuggingFace Transformers. This is
    the main entry point to Ecco.
    Usage:
    ```python
    import ecco
    lm = ecco.from_pretrained('gpt2')
    ```
    You can also use a custom model and specify its configurations:
    ```python
    import ecco
    model_config = {
        'embedding': "transformer.wte.weight",
        'type': 'causal',
        'activations': ['mlp\.c_proj'],
        'token_prefix': ' ',
        'partial_token_prefix': ''
    }
    lm = ecco.from_pretrained('gpt2', model_config=model_config)
    ```
    Args:
        hf_model_id (str): Name of the model identifying it in the HuggingFace model hub. e.g. 'distilgpt2', 'bert-base-uncased'.
        model_config (Optional[Dict[str, Any]]): Custom model configuration. If the value is None the config file will be
                                                 searched in the model-config.yaml. Defaults to None.
        activations (Optional[bool]): If True, collect activations when this model runs inference. Option saved in LM. Defaults to False.
        attention (Optional[bool]): If True, collect attention. Option passed to the model. Defaults to False.
        hidden_states (Optional[bool]): If True, collect hidden states. Needed for layer_predictions and rankings(). Defaults to True.
        activations_layer_nums (Optional[List[int]]): If we are collecting activations, we can specify which layers to track. This is None by
                                                      default and all layer are collected if 'activations' is set to True. Defaults to None.
        verbose (Optional[bool]): If True, model.generate() displays output tokens in HTML as they're generated. Defaults to True.
        gpu (Optional[bool]): Set to False to force using the CPU even if a GPU exists. Defaults to True.
    """
    if model_config:
        config = pack_tokenizer_config(model_config)
    else:
        config = load_config(hf_model_id)
    tokenizer = AutoTokenizer.from_pretrained(hf_model_id)
    if config['type'] == 'enc-dec':
        model_cls = AutoModelForSeq2SeqLM
    elif config['type'] == 'causal':
        model_cls = AutoModelForCausalLM
    else:
        model_cls = AutoModel
    model = model_cls.from_pretrained(hf_model_id, output_hidden_states=hidden_states, output_attentions=attention)
    lm_kwargs = {
        'model_name': hf_model_id,
        'config': config,
        'collect_activations_flag': activations,
        'collect_activations_layer_nums': activations_layer_nums,
        'verbose': verbose,
        'gpu': gpu}
    lm = LM(model, tokenizer, **lm_kwargs)
    return lm

================
File: src/ecco/__main__.py
================
"""
Entrypoint module, in case you use `python -m ecco`.
Why does this file exist, and why __main__? For more info, read:
- https://www.python.org/dev/peps/pep-0338/
- https://docs.python.org/2/using/cmdline.html#cmdoption-m
- https://docs.python.org/3/using/cmdline.html#cmdoption-m
"""
import sys
from ecco.cli import main
if __name__ == "__main__":
    sys.exit(main())

================
File: src/ecco/activations.py
================
import torch
import numpy as np
def reshape_hidden_states_to_3d(hidden_states):
    """
    Turn hidden_states from (layer, batch, position, d_model)
    to a tensor  (layer, d_model, batch + position).
    Args:
        hidden_states: the hidden states return by the language model. A list of tensors. Its shape:
            (layer, batch, position, d_model)
    returns:
        hidden_states: tensor in the shape (layer, d_model, batch + position)
    """
    hs = hidden_states
    # Turn from a list of tensors into a tensor
    if isinstance(hs, tuple):
        hs = torch.stack(hs)
    # Merge the batch and position dimensions
    hs = hs.reshape((hs.shape[0], -1, hs.shape[-1]))
    return hs
def reshape_activations_to_3d(activations):
    """
    Reshape the activations tensors into a shape where it's easier to compare
    activation vectors.
    Args:
        activations: activations tensor of LM. Shape:
            (batch, layer, neuron, position)
    returns:
        activations: activations tensor reshaped into:
            (layer, neuron, batch + position)
    """
    # Swap axes from (0 batch, 1 layer, 2 neuron, 3 position)
    # to (0 layer, 1 neuron, 2 batch, 3 position)
    activations = np.moveaxis(activations, [0, 1, 2], [2, 0, 1])
    s = activations.shape
    acts = activations.reshape(s[0], s[1], -1)
    return acts

================
File: src/ecco/analysis.py
================
from .svcca_lib import cca_core, pwcca as pwcca_lib, cka_lib
import numpy as np
def cca(acts1, acts2):
    """
    Calculate a similarity score for two activation matrices using Canonical Correlation Analysis (CCA). Returns the
    average of all the correlation coefficients.
    Args:
        acts1: Activations matrix #1. 2D numPy array. Dimensions: (neurons, token position)
        acts2: Activations matrix #2. 2D numPy array. Dimensions: (neurons, token position)
    Returns:
        score: Float between 0 and 1, where 0 means not correlated, 1 means the two activation matrices are linear transformations of each other.
    """
    result = cca_core.get_cca_similarity(acts1, acts2, epsilon=1e-10, verbose=False)
    return result['mean'][0]
# More details at https://github.com/google/svcca/blob/master/tutorials/001_Introduction.ipynb
def svcca(acts1, acts2, dims: int = 20):
    """
    Calculate a similarity score for two activation matrices using Singular Value Canonical Correlation Analysis
    (SVCCA). A meaningful score requires setting an appropriate value for 'dims', see SVCCA tutorial for how to do
    that.
    Args:
        acts1: Activations matrix #1. 2D numPy array. Dimensions: (neurons, token position)
        acts2: Activations matrix #2. 2D numPy array. Dimensions: (neurons, token position)
        dims: The number of dimensions to consider for SVCCA calculation. See the SVCCA tutorial to see how to
                determine this in a way
    Returns:
        score: between 0 and 1, where 0 means not correlated, 1 means the two activation matrices are linear
        transformations of each other.
    """
    # Center activations by subtracting the mean
    centered_acts_1 = acts1 - np.mean(acts1, axis=1, keepdims=True)
    centered_acts_2 = acts2 - np.mean(acts2, axis=1, keepdims=True)
    # Perform SVD
    _, s1, v1 = np.linalg.svd(centered_acts_1, full_matrices=False)
    _, s2, v2 = np.linalg.svd(centered_acts_2, full_matrices=False)
    # Reconstruct by multiplying s and v but only for the top <dims> number of dimensions
    sv_acts1 = np.dot(s1[:dims] * np.eye(dims), v1[:dims])
    sv_acts2 = np.dot(s2[:dims] * np.eye(dims), v2[:dims])
    # NOW do cca to get SVCCA values
    results = cca_core.get_cca_similarity(sv_acts1, sv_acts2, epsilon=1e-10, verbose=False)
    return np.mean(results["cca_coef1"])
def pwcca(acts1, acts2, epsilon=1e-10):
    """
    Calculate a similarity score for two activation matrices using Projection Weighted Canonical Correlation Analysis.
    It's more convenient as it does not require setting a specific number of dims like SVCCA.
    Args:
        acts1: Activations matrix #1. 2D numPy array. Dimensions: (neurons, token position)
        acts2: Activations matrix #2. 2D numPy array. Dimensions: (neurons, token position)
    Returns:
        score: between 0 and 1, where 0 means not correlated, 1 means the two activation matrices are
        linear transformations of each other.
    """
    results = pwcca_lib.compute_pwcca(acts1, acts2, epsilon=epsilon)
    return results[0]
def cka(acts1, acts2):
    """
    Calculates a similarity score for two activation matrices using center kernel alignment (CKA). CKA
    has the benefit of not requiring the number of tokens to be larger than the number of neurons.
    Args:
        acts1: Activations matrix #1. 2D numPy array. Dimensions: (neurons, token position)
        acts2: Activations matrix #2. 2D numPy array. Dimensions: (neurons, token position)
    Returns:
        score: between 0 and 1, where 0 means not correlated, 1 means the two activation matrices are
        linear transformations of each other.
    """
    return cka_lib.feature_space_linear_cka(acts1.T, acts2.T)

================
File: src/ecco/attribution.py
================
from functools import partial
import torch
from typing import Any, Dict
from captum.attr import (
    IntegratedGradients,
    Saliency,
    InputXGradient,
    DeepLift,
    DeepLiftShap,
    GuidedBackprop,
    GuidedGradCam,
    Deconvolution,
    LRP
)
from torch.nn import functional as F
import transformers
ATTR_NAME_ALIASES = {
    'ig': 'integrated_gradients',
    'saliency': 'gradient',
    'dl': 'deep_lift',
    'dls': 'deep_lift_shap',
    'gb': 'guided_backprop',
    'gg': 'guided_gradcam',
    'deconv': 'deconvolution',
    'lrp': 'layer_relevance_propagation'
}
ATTR_NAME_TO_CLASS = { # TODO: Add more Captum Primary attributions with needed computed arguments
    'integrated_gradients': IntegratedGradients,
    'gradient': Saliency,
    'grad_x_input': InputXGradient,
    'deep_lift': DeepLift,
    'deep_lift_shap': DeepLiftShap,
    'guided_backprop': GuidedBackprop,
    'guided_gradcam': GuidedGradCam,
    'deconvolution': Deconvolution,
    'layer_relevance_propagation': LRP
}
def compute_primary_attributions_scores(attr_method : str, model: transformers.PreTrainedModel,
                                        forward_kwargs: Dict[str, Any], prediction_id: torch.Tensor,
                                        aggregation: str = "L2") -> torch.Tensor:
    """
    Computes the primary attributions with respect to the specified `prediction_id`.
    Args:
        attr_method: Name of the primary attribution method to compute
        model: HuggingFace Transformers Pytorch language model.
        forward_kwargs: contains all the inputs that are passed to `model` in the forward pass
        prediction_id: Target Id. The Integrated Gradients will be computed with respect to it.
        aggregation: Aggregation/normalzation method to perform to the Integrated Gradients attributions.
         Currently only "L2" is implemented
    Returns: a tensor of the normalized attributions with shape (input sequence size,)
    """
    def model_forward(input_: torch.Tensor, decoder_: torch.Tensor, model, extra_forward_args: Dict[str, Any]) \
            -> torch.Tensor:
        if decoder_ is not None:
            output = model(inputs_embeds=input_, decoder_inputs_embeds=decoder_, **extra_forward_args)
        else:
            output = model(inputs_embeds=input_, **extra_forward_args)
        return F.softmax(output.logits[:, -1, :], dim=-1)
    def normalize_attributes(attributes: torch.Tensor) -> torch.Tensor:
        # attributes has shape (batch, sequence size, embedding dim)
        attributes = attributes.squeeze(0)
        if aggregation == "L2":  # norm calculates a scalar value (L2 Norm)
            norm = torch.norm(attributes, dim=1)
            attributes = norm / torch.sum(norm)  # Normalize the values so they add up to 1
        else:
            raise NotImplemented
        return attributes
    extra_forward_args = {k: v for k, v in forward_kwargs.items() if
                          k not in ['inputs_embeds', 'decoder_inputs_embeds']}
    input_ = forward_kwargs.get('inputs_embeds')
    decoder_ = forward_kwargs.get('decoder_inputs_embeds')
    if decoder_ is None:
        forward_func = partial(model_forward, decoder_=decoder_, model=model, extra_forward_args=extra_forward_args)
        inputs = input_
    else:
        forward_func = partial(model_forward, model=model, extra_forward_args=extra_forward_args)
        inputs = tuple([input_, decoder_])
    attr_method_class = ATTR_NAME_TO_CLASS.get(ATTR_NAME_ALIASES.get(attr_method, attr_method), None)
    if attr_method_class is None:
        raise NotImplementedError(
            f"No implementation found for primary attribution method '{attr_method}'. "
            f"Please choose one of the methods: {list(ATTR_NAME_TO_CLASS.keys())}"
        )
    ig = attr_method_class(forward_func=forward_func)
    attributions = ig.attribute(inputs, target=prediction_id)
    if decoder_ is not None:
        # Does it make sense to concatenate encoder and decoder attributions before normalization?
        # We assume that the encoder/decoder embeddings are the same
        return normalize_attributes(torch.cat(attributions, dim=1))
    else:
        return normalize_attributes(attributions)

================
File: src/ecco/cli.py
================
"""
Module that contains the command line app.
Why does this file exist, and why not put this in __main__?
  You might be tempted to import things from __main__ later, but that will cause
  problems: the code will get executed twice:
  - When you run `python -mecco` python will execute
    ``__main__.py`` as a script. That means there won't be any
    ``ecco.__main__`` in ``sys.modules``.
  - When you import __main__ it will get executed again (as a module) because
    there's no ``ecco.__main__`` in ``sys.modules``.
  Also see (1) from http://click.pocoo.org/5/setuptools/#setuptools-integration
"""
import sys
import ecco
def main(argv=sys.argv):
    """
    Args:
        argv (list): List of arguments
    Returns:
        int: A return code
    Does stuff.
    """
    # lm = ecco.from_pretrained("mockGPT")
    print("Loaded lm")
    # lm.generate('', generate=1)
    print(argv)
    return 0

================
File: src/ecco/lm_plots.py
================
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from matplotlib.patches import Rectangle
from matplotlib.cm import get_cmap
import copy
import warnings
from matplotlib.colors import ListedColormap
import matplotlib as mpl
from matplotlib.ticker import FuncFormatter
from typing import Optional, List, Dict
import sys
def plot_activations(tokens, activations, vmin=0, vmax=2, height=60,
                     width_scale_per_item=1,
                     file_prefix='neuron_activation_plot'):
    """ Plots a heatmap showing how active each neuron (row) was with each token
    (columns). Neurons with activation less then masking_threashold are masked.
    Args:
      tokens: list of the tokens. Note if you're examining activations
      associated with the token as input or as output.
    """
    n_tokens = activations.shape[-1]
    # Activations lower than this threshold will show up as blank
    masking_threshold = 0.01
    masked = np.ma.masked_less(activations, masking_threshold)
    # mask = tensor.shape()
    fig = plt.figure(figsize=(activations.shape[1], height))  # Width adjusts to number of tokens
    sns.set()
    ax = plt.gca()
    v = copy.copy(get_cmap('viridis_r'))
    v.set_bad('white')
    g = sns.heatmap(activations,
                    mask=masked.mask,
                    cmap=v,
                    ax=ax,
                    vmin=vmin,
                    vmax=vmax,
                    cbar=False)
    if tokens:
        ax.set_xticklabels(tokens[-n_tokens:], rotation=0)
    ax.tick_params(axis='x', which='major', labelsize=18)
    ax.set_xlabel('\nOutput Token', fontsize=14)
    plt.title('FFNN Activations', fontsize=28)
    plt.tick_params(axis='x',  # changes apply to the x-axis
                    which='both',  # both major and minor ticks are affected
                    left=False,  # ticks along the bottom edge are off
                    bottom=True,  # ticks along the bottom edge are off
                    top=True,  # ticks along thx top edge are off
                    labeltop=True,
                    labelbottom=True)  # labels along the bottom edge are off
    plt.xticks(rotation=-45)
    # # Save Figure to file & download
    fig.set_facecolor("w")
    # time_str = int(datetime.datetime.now().strftime("%s")) * 1000
    # file_name = 'distilgpt2_layer_' + str(layer) + '_' + str(time_str) + '.png'
    # plt.savefig(file_name)
    # plt.close(fig)
    plt.show()
def plot_clustered_activations(tokens, activations, clusters, cluster_ids, file_prefix='neuron_activation_plot'):
    """ Plots a heat mapshowing how active each neuron (row) was with each token
    (columns). Neurons with activation less then masking_threashold are masked.
    Args:
      tokens: list of the tokens. Note if you're examining activations
      associated with the token as input or as output.
    """
    n_tokens = activations.shape[-1]
    # Activations lower than this threshold will show up as blank
    masking_threshold = 0.01
    masked = np.ma.masked_less(activations, masking_threshold)
    # mask = tensor.shape()
    fig = plt.figure(figsize=(activations.shape[1], 60))  # Width adjusts to number of tokens
    # sns.set()
    ax = plt.gca()
    v = copy.copy(get_cmap('viridis_r'))
    v.set_bad('white')
    g = sns.heatmap(activations,
                    mask=masked.mask,
                    cmap=v,
                    ax=ax,
                    vmax=2,
                    vmin=0,
                    cbar=False)
    colors = get_cmap("cool", len(clusters.keys()))
    colors = get_cmap("tab20", len(clusters.keys()))
    colors_2 = get_cmap("hot", len(clusters.keys()))
    colors_2 = get_cmap("prism", len(clusters.keys()))
    row = 0
    for idx, (cluster_id, neurons) in enumerate(clusters.items()):
        n_neurons = len(neurons)
        # print(idx, 'cluster #', cluster_id, "number of neurons: ",
        #       len(neurons), n_neurons, 'row', row)
        opacity = 0.00
        edge_color = colors(idx, 0.5)
        fill_color = edge_color  # colors(idx, opacity)
        # if idx % 2 == 0:
        #     edge_color = colors(idx)
        #     fill_color = colors(idx, opacity)  # Color + Opacity
        # else:
        #     edge_color = colors_2(idx)
        #     fill_color = colors_2(idx, opacity)  # Color + Opacity
        # First colored column to the leftmost of the figure identifying the cluster
        g.add_patch(Rectangle((-1, row),
                              1,  # width
                              n_neurons,  # height
                              fill=True,
                              facecolor=edge_color,
                              edgecolor=edge_color,
                              lw=5,
                              label="cluster {}".format(cluster_ids[idx])))
        # Border surrounding the cluster
        g.add_patch(Rectangle((-1, row),
                              activations.shape[1] + 1,  # width - span all columns
                              n_neurons,  # height
                              fill=False,
                              facecolor=fill_color,
                              edgecolor=edge_color,
                              lw=5))
        row += n_neurons
    # ax_ = g.ax_heatmap
    # print(ax_)
    # From https://github.com/mwaskom/seaborn/issues/1773
    # fix for mpl bug that cuts off top/bottom of seaborn viz
    left, right = plt.xlim()  # discover the values for bottom and top
    # right += 0.5  # Add 0.5 to the bottom
    left -= 0.5  # Subtract 0.5 from the top
    plt.xlim(left, right)  # update the ylim(bottom, top) values
    # print(ax_)
    if tokens:
        ax.set_xticklabels(tokens[-n_tokens:], rotation=0)
    ax.tick_params(axis='x', which='major', labelsize=28)
    plt.tick_params(axis='x',  # changes apply to the x-axis
                    which='both',  # both major and minor ticks are affected
                    left=False,  # ticks along the bottom edge are off
                    bottom=True,  # ticks along the bottom edge are off
                    top=True,  # ticks along thx top edge are off
                    labeltop=True,
                    labelbottom=True)  # labels along the bottom edge are off
    plt.xticks(rotation=-45)
    # # Save Figure to file & download
    fig.set_facecolor("w")
    plt.legend(loc='lower left', bbox_to_anchor=(1.05, 0))
    plt.show()
def token_barplot(tokens, values):
    fig, ax = plt.subplots(figsize=(len(values) / 2, 1))
    # fig = plt.figure(figsize=(len(values) / 2, 6))
    fig.set_facecolor("w")
    cm = get_cmap('viridis_r')
    colors = [cm(v / 0.5) for v in values]
    x = np.arange(len(values))
    bars = ax.bar(x, values, color=colors)
    # ax = sns.barplot(x=np.arange(len(values)), y=values, hue=values, palette='viridis')  # self.tokens[:len(importance)]
    ax.set_xticks(x)
    ax.set_xticklabels(tokens[:len(values)])
    ax.set_title('Feature importance when the model was generating the token: {}'
                 .format(tokens[len(values)]))  # repr(
    plt.xticks(rotation=-90)
# See: https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens
def plot_logit_lens(tokens,
                    softmax_scores,
                    predicted_tokens,
                    vmin=0,
                    vmax=1,
                    token_found_mask=None,
                    show_input_tokens: bool = False,
                    n_input_tokens: int = 0):
    # print(tokens, softmax_scores, predicted_tokens, token_found_mask)
    start_token = 0
    if not show_input_tokens:
        start_token = n_input_tokens - 1
        if n_input_tokens == 0:
            warnings.warn(
                'Setting show_input_tokens to True requires supplying n_input_tokens to exlucde inputs from the plot. Defaulting to 0 and showing the input.')
    fig = plt.figure(figsize=(20, 8))
    fig.set_facecolor("w")
    # Activations lower than this threshold will show up as blank
    # masking_threshold = 0.01
    # masked = np.ma.masked_less(softmax_scores, masking_threshold)
    ax = plt.gca()
    v = copy.copy(get_cmap('viridis_r'))
    v.set_bad('white')
    g = sns.heatmap(softmax_scores[:, start_token:],
                    mask=token_found_mask[:, start_token:],
                    cmap=v,
                    fmt="",
                    ax=ax,
                    annot=predicted_tokens[:, start_token:],
                    vmin=vmin,
                    vmax=vmax,
                    linewidths=0.5,
                    linecolor="#f0f0f0",
                    annot_kws={"size": 22},
                    cbar_kws={'label': 'Probability of token (softmax score)'}
                    )
    ax.tick_params(axis='x', which='major', labelsize=18)
    plt.tick_params(axis='x',  # changes apply to the x-axis
                    which='both',  # both major and minor ticks are affected
                    left=False,  # ticks along the bottom edge are off
                    bottom=True,  # ticks along the bottom edge are off
                    top=False,  # ticks along thx top edge are off
                    labeltop=False,
                    labelbottom=True)  # labels along the bottom edge are off
    # Output token labels at the bottom
    ax.set_xticklabels(tokens[start_token + 1:], rotation=-90)
    # Layer names label at the left
    ylabels = ["Layer {}".format(n) for n in range(softmax_scores.shape[0])]
    ax.set_yticklabels(ylabels, fontsize=18, rotation=0)
    # Input token labels at the top
    ax2 = ax.twiny()
    ax2.set_xlim([0, ax.get_xlim()[1]])
    ax2.set_xticks(ax.get_xticks())
    ax2.set_xticklabels(tokens[start_token:-1], fontsize=18, rotation=-90)
def plot_inner_token_rankings_watch(input_tokens,
                                    output_tokens,
                                    rankings: np.ndarray,
                                    position: int,
                                    vmin: Optional[int] = 2,  # Good range for topk 50
                                    vmax: Optional[int] = 5000,
                                    show_inputs: Optional[bool] = False,
                                    save_file_path: Optional[str] = None
                                    ):
    n_columns = len(output_tokens)
    n_rows = rankings.shape[0]
    fsize = (1 + 0.9 * n_columns,  # Make figure wider if more columns
             1 + 0.4 * n_rows)  # Make taller if more layers
    fig, (ax, cax) = plt.subplots(nrows=1, ncols=2,
                                  figsize=fsize,
                                  gridspec_kw={"width_ratios": [n_columns, 0.4]})
    fig.subplots_adjust(wspace=0.2)
    fig.set_facecolor("w")
    cmap_big = get_cmap('GnBu_r', 512)
    newcmp = ListedColormap(cmap_big(np.linspace(0.40, 0.90, 256)))
    v = copy.copy(newcmp)
    v.set_under('#1a7bb5')
    v.set_over('white')
    v.set_bad('white')
    comma_fmt = FuncFormatter(lambda x, p: format(int(x), ','))
    norm = mpl.colors.LogNorm(vmin=vmin, vmax=vmax)
    g = sns.heatmap(rankings,
                    # mask=token_found_mask[:,start_token:],
                    cmap=v,
                    fmt="d",
                    ax=ax,
                    annot=rankings,
                    cbar=False,
                    norm=norm,
                    linewidths=0.5,
                    linecolor="#f0f0f0",
                    annot_kws={"size": 12})
    fig.colorbar(ax.get_children()[0],
                 cax=cax,
                 format=comma_fmt,
                 extend='both',
                 orientation="vertical",
                 label='Ranking of token (by score)')
    ax.tick_params(axis='x', which='major', labelsize=20)
    plt.tick_params(axis='x',  # changes apply to the x-axis
                    which='both',  # both major and minor ticks are affected
                    left=False,  # ticks along the bottom edge are off
                    bottom=True,  # ticks along the bottom edge are off
                    top=False,  # ticks along thx top edge are off
                    labeltop=False,
                    labelbottom=True)  # labels along the bottom edge are off
    # Output token labels at the bottom
    ax.set_xticklabels(output_tokens, rotation=-90)
    ax.set_xlabel('Output Token', fontsize=14)
    # Layer names label at the left
    ylabels = ["Decoder Layer {}".format(n) for n in range(rankings.shape[0])]
    ax.set_yticklabels(ylabels, fontsize=14, rotation=0)
    # ax.set_ylabel('Output Token')
    ax2 = ax.twiny()
    ax2.set_xlim([0, ax.get_xlim()[1]])
    if show_inputs:
        # Input token labels at the top
        ax2.set_xticks(ax.get_xticks())
        ax2.set_xticklabels(input_tokens, fontsize=14, rotation=-90)
        ax2.set_xlabel('\nWatched Token', fontsize=14)
    else:
        ax2.set_xticks([])
    plt.title(' '.join(input_tokens[:position + 1]) + ' ____\n', fontsize=14)
    if save_file_path is not None:
        try:
            plt.savefig(save_file_path)
        except:
            e = sys.exc_info()[0]
            print("<p>Error: (likely ./tmp/ folder does not exist or can't be created). %s</p>" % e)
            raise
def plot_inner_token_rankings(input_tokens,
                              output_tokens,
                              rankings,
                              vmin: int = 2,
                              vmax: int = 5000,
                              show_inputs: Optional[bool] = False,
                              save_file_path: Optional[str] = None,
                              **kwargs
                              ):
    n_columns = len(input_tokens)
    n_rows = rankings.shape[0]
    fsize = (1 + 0.9 * n_columns,  # Make figure wider if more columns
             1 + 0.4 * n_rows)  # Make taller if more layers
    fig, (ax, cax) = plt.subplots(nrows=1, ncols=2,
                                  figsize=fsize,
                                  gridspec_kw={"width_ratios": [n_columns, 0.5]}
                                  )
    plt.subplots_adjust(wspace=0.1)
    fig.set_facecolor("w")
    # ax = plt.gca()
    cmap_big = get_cmap('RdPu_r', 512)
    newcmp = ListedColormap(cmap_big(np.linspace(0.40, 0.90, 256)))
    v = copy.copy(newcmp)
    v.set_under('#9a017b')
    v.set_over('white')
    v.set_bad('white')
    comma_fmt = FuncFormatter(lambda x, p: format(int(x), ','))
    # print(vmin, vmax)
    norm = mpl.colors.LogNorm(vmin=vmin, vmax=vmax)
    g = sns.heatmap(rankings,
                    # mask=token_found_mask[:,start_token:],
                    cmap=v,
                    fmt="d",
                    ax=ax,
                    annot=rankings,
                    cbar=False,
                    norm=norm,
                    linewidths=0.5,
                    linecolor="#f0f0f0",
                    annot_kws={"size": 12}
                    )
    fig.colorbar(ax.get_children()[0],
                 cax=cax,
                 format=comma_fmt,
                 extend='both',
                 orientation="vertical",
                 label='Ranking of token (by score)')
    ax.tick_params(axis='x', which='major', labelsize=18)
    plt.tick_params(axis='x',  # changes apply to the x-axis
                    which='both',  # both major and minor ticks are affected
                    left=False,  # ticks along the bottom edge are off
                    bottom=True,  # ticks along the bottom edge are off
                    top=False,  # ticks along thx top edge are off
                    labeltop=False,
                    labelbottom=True)  # labels along the bottom edge are off
    # Output token labels at the bottom
    ax.set_xticklabels(output_tokens, rotation=-90)
    ax.set_xlabel('Output Token', fontsize=14)
    # Layer names label at the left
    ylabels = ["Layer {}".format(n) for n in range(rankings.shape[0])]
    ax.set_yticklabels(ylabels, fontsize=14, rotation=0)
    # ax.set_ylabel('Output Token')
    ax2 = ax.twiny()
    ax2.set_xlim([0, ax.get_xlim()[1]])
    if show_inputs:
        # Input token labels at the top
        ax2.set_xticks(ax.get_xticks())
        ax2.set_xticklabels(input_tokens, fontsize=14, rotation=-90)
        ax2.set_xlabel('\nInput Token', fontsize=14)
    else:
        ax2.set_xticks([])
    if save_file_path is not None:
        try:
            plt.savefig(save_file_path)
        except:
            e = sys.exc_info()[0]
            print("<p>Error: (likely ./tmp/ folder does not exist or can't be created). %s</p>" % e)
            raise

================
File: src/ecco/lm.py
================
from collections import defaultdict
import inspect
import json
import os
import random
import torch
import transformers
from transformers import BatchEncoding
import ecco
import numpy as np
from IPython import display as d
from torch.nn import functional as F
from ecco.attribution import compute_primary_attributions_scores
from ecco.output import OutputSeq
from typing import Optional, Any, List, Tuple, Dict, Union
from operator import attrgetter
import re
from ecco.util import is_partial_token, strip_tokenizer_prefix
from packaging import version
class LM(object):
    """
    Ecco's central class. A wrapper around language models. We use it to run the language models
    and collect important data like input saliency and neuron activations.
    A LM object is typically not created directly by users,
    it is returned by `ecco.from_pretrained()`.
    Usage:
    ```python
    import ecco
    lm = ecco.from_pretrained('distilgpt2')
    output = lm.generate("Hello computer")
    ```
    """
    def __init__(self,
                 model: transformers.PreTrainedModel,
                 tokenizer: transformers.PreTrainedTokenizerFast,
                 model_name: str,
                 config: Dict[str, Any],
                 collect_activations_flag: Optional[bool] = False,
                 collect_activations_layer_nums: Optional[List[int]] = None,  # None --> collect for all layers
                 verbose: Optional[bool] = True,
                 gpu: Optional[bool] = True
                 ):
        """
        Creates an LM object given a model and tokenizer.
        Args:
            model: HuggingFace Transformers Pytorch language model.
            tokenizer: The tokenizer associated with the model
            model_name: The name of the model. Used to retrieve required settings (like what the embedding layer is called)
            config: Configuration that has the information about the layer whose activations we will collect
            collect_activations_flag: True if we want to collect activations
            collect_activations_layer_nums: If collecting activations, we can use this parameter to indicate which layers
                to track. By default this would be None and we'd collect activations for all layers.
            verbose: If True, model.generate() displays output tokens in HTML as they're generated.
            gpu: Set to False to force using the CPU even if a GPU exists.
        """
        self.model_name = model_name
        self.model = model
        if torch.cuda.is_available() and gpu:
            self.model = model.to('cuda')
        self.device = 'cuda' if torch.cuda.is_available() \
                                and self.model.device.type == 'cuda' \
            else 'cpu'
        self.tokenizer = tokenizer
        self.verbose = verbose
        self._path = os.path.dirname(ecco.__file__)
        # Neuron Activation
        self.collect_activations_flag = collect_activations_flag
        self.collect_activations_layer_nums = collect_activations_layer_nums
        # For each model, this indicates the layer whose activations
        # we will collect
        self.model_config = config
        try:
            self.model_type = self.model_config['type']
            embeddings_layer_name = self.model_config['embedding']
            embed_retriever = attrgetter(embeddings_layer_name)
            if type(embed_retriever(self.model)) == torch.nn.Embedding:
                self.model_embeddings = embed_retriever(self.model).weight
            else:
                self.model_embeddings = embed_retriever(self.model)
            self.collect_activations_layer_name_sig = self.model_config['activations'][0]
        except KeyError:
            raise ValueError(
                   f"The model '{self.model_name}' is not correctly configured in Ecco's 'model-config.yaml' file"
            ) from KeyError()
        assert self.model_type in ['causal', 'mlm', 'enc-dec'], f"model type {self.model_type} not found"
        self._reset()
        # If running in Jupyer, outputting setup this in one cell is enough. But for colab
        # we're running it before every d.HTML cell
        # d.display(d.HTML(filename=os.path.join(self._path, "html", "setup.html")))
    def _reset(self):
        self._all_activations_dict = defaultdict(dict)
        self.activations = defaultdict(dict)
        self.all_activations = []
        self.generation_activations = []
        self.neurons_to_inhibit = {}
        self.neurons_to_induce = {}
        self._hooks = {}
    def to(self, tensor: Union[torch.Tensor, BatchEncoding]):
        if self.device == 'cuda':
            return tensor.to('cuda')
        return tensor
    def _analyze_token(self,
                       encoder_input_embeds: torch.Tensor,
                       encoder_attention_mask: Optional, # TODO: use encoder mask and also decoder mask
                       decoder_input_embeds: Optional[torch.Tensor],
                       prediction_id: torch.Tensor,
                       attribution_flags: Optional[List[str]] = []) -> None:
        """
        Analyzes a predicted token.
        Currently this methods computes the primary attribution explainability scores for each given token.
        """
        for attr_method in attribution_flags:
            # deactivate hooks: attr method can perform multiple forward steps
            self._remove_hooks()
            # Add attribution scores to self.attributions
            self.attributions[attr_method].append(
                compute_primary_attributions_scores(
                    attr_method=attr_method,
                    model=self.model,
                    forward_kwargs={
                        'inputs_embeds': encoder_input_embeds,
                        'decoder_inputs_embeds': decoder_input_embeds
                    },
                    prediction_id=prediction_id
                ).cpu().detach().numpy()
            )
    def generate(self, input_str: str,
                 max_length: Optional[int] = 8,
                 temperature: Optional[float] = None,
                 top_k: Optional[int] = None,
                 top_p: Optional[float] = None,
                 do_sample: Optional[bool] = False,
                 attribution: Optional[List[str]] = [],
                 generate: Optional[int] = None,
                 beam_size: int = 1,
                 **generate_kwargs: Any):
        """
        Generate tokens in response to an input prompt.
        Works with Language models like GPT2, not masked language models like BERT.
        Args:
            input_str: Input prompt. # TODO: accept batch of input strings
            generate: Number of tokens to generate.
            max_length: max length of sequence (input + output tokens)
            temperature: Adjust the probability distibution of output candidate tokens.
            top_k: Specify top-k tokens to consider in decoding. Only used when do_sample is True.
            top_p: Specify top-p to consider in decoding. Only used when do_sample is True.
            do_sample: Decoding parameter. If set to False, the model always always
                chooses the highest scoring candidate output
                token. This may lead to repetitive text. If set to True, the model considers
                consults top_k and/or top_p to generate more interesting output.
            attribution: List of attribution methods to be calculated. By default, it does not calculate anything.
            beam_size: Beam size to consider while generating
            generate_kwargs: Other arguments to be passed directly to self.model.generate
        """
        assert self.model_type in ['enc-dec', 'causal'], f"generate method not supported for model type '{self.model_type}'"
        top_k = top_k if top_k is not None else self.model.config.top_k
        top_p = top_p if top_p is not None else self.model.config.top_p
        temperature = temperature if temperature is not None else self.model.config.temperature
        do_sample = do_sample if do_sample is not None else self.model.config.task_specific_params.get('text-generation', {}).get('do_sample', False)
        pad_token_id = self.model.config.pad_token_id
        eos_token_id = self.model.config.eos_token_id
        # We need this as a batch in order to collect activations.
        input_tokenized_info = self.tokenizer(input_str, return_tensors="pt")
        input_tokenized_info = self.to(input_tokenized_info)
        input_ids, attention_mask = input_tokenized_info['input_ids'], input_tokenized_info['attention_mask']
        n_input_tokens = len(input_ids[0])
        cur_len = n_input_tokens
        if generate is not None:
            max_length = n_input_tokens + generate
        if cur_len >= max_length:
            raise ValueError(
                "max_length set to {} while input token has more tokens ({}). Consider increasing max_length" \
                    .format(max_length, cur_len))
        # Get decoder input ids
        if self.model_type == 'enc-dec': # FIXME: only done because causal LMs like GPT-2 have the _prepare_decoder_input_ids_for_generation method but do not use it
            assert len(input_ids.size()) == 2 # will break otherwise
            if version.parse(transformers.__version__) >= version.parse('4.13'):
                # following the code in https://github.com/huggingface/transformers/blob/d0c1aebea467af499331234e7b285a6bf91ea073/tests/generation/test_utils.py#L2099
                model_kwargs = self.model._prepare_encoder_decoder_kwargs_for_generation(input_ids, {})
                decoder_input_ids, model_kwargs = self.model._prepare_decoder_input_ids_for_generation(
                    batch_size=input_ids.shape[0],
                    model_input_name=self.model.main_input_name,
                    model_kwargs=model_kwargs,
                    decoder_start_token_id=self.model.config.decoder_start_token_id,
                    bos_token_id=self.model.config.bos_token_id,
                )
            else:
                decoder_input_ids = self.model._prepare_decoder_input_ids_for_generation(input_ids, None, None)
        else:
            decoder_input_ids = None
        # Print output
        n_printed_tokens = n_input_tokens
        if self.verbose:
            viz_id = self.display_input_sequence(input_ids[0])
        # Get model output
        self._remove_hooks() # deactivate hooks: we will run them for the last model forward only
        output = self.model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            num_beams=beam_size,
            # FIXME: +1 in max_length to account for first start token in decoder, find a better way to do this
            max_length=(generate or max_length - cur_len) + 1 if self.model_type == 'enc-dec' else max_length,
            do_sample=do_sample,
            top_p=top_p,
            top_k=top_k,
            temperature=temperature,
            return_dict_in_generate=True,
            output_scores=True,
            **generate_kwargs
        )
        # Get prediction logits for each chosen prediction id
        prediction_logits, prediction_ids = [], []
        if output.__class__.__name__.endswith("EncoderDecoderOutput"):
            prediction_ids, prediction_scores = output.sequences[0][1:], output.scores
        elif output.__class__.__name__.endswith("DecoderOnlyOutput"):
            prediction_ids, prediction_scores = output.sequences[0][n_input_tokens:], output.scores
        else:
            raise NotImplementedError(f"Unexpected output type: {type(output)}")
        assert prediction_ids != []
        assert len(prediction_ids) == len(prediction_scores)
        for pred_id, scores in zip(prediction_ids, prediction_scores):
            prediction_logits.append(scores[0][pred_id])
        # Analyze each generated token
        self.attributions = defaultdict(list) # reset attributions dict
        for pred_index, prediction_id in enumerate(prediction_ids):
            # First get encoder/decoder input embeddings
            encoder_input_embeds, _ = self._get_embeddings(input_ids)
            # TODO: This is only okay as long as encoder and decoder share the embeddings
            # Should make separate ones for more flexibility
            if decoder_input_ids is not None:
                decoder_input_embeds, _ = self._get_embeddings(decoder_input_ids)
            else:
                decoder_input_embeds= None
            if pred_index == len(prediction_ids) - 1: # -1 because we want to catch the inputs for the last generated token
                # attach hooks and run last forward step
                # TODO: collect activation for more than 1 step
                self._attach_hooks(self.model)
                extra_forward_kwargs = {'attention_mask': attention_mask, 'decoder_inputs_embeds': decoder_input_embeds}
                forward_kwargs = {
                    'inputs_embeds': encoder_input_embeds,
                    'use_cache': False,
                    'return_dict': True,
                    **{k: v for k, v in extra_forward_kwargs.items() if k in inspect.signature(self.model.forward).parameters}
                }
                _ = self.model(**forward_kwargs)
            # Get primary attributions for produced token
            self._analyze_token(
                encoder_input_embeds=encoder_input_embeds,
                encoder_attention_mask=attention_mask,
                decoder_input_embeds=decoder_input_embeds,
                attribution_flags=attribution,
                prediction_id=prediction_id
            )
            # Recomputing inputs ids, attention mask and decoder input ids
            if decoder_input_ids is not None:
                assert len(decoder_input_ids.size()) == 2 # will break otherwise
                decoder_input_ids = torch.cat(
                    [decoder_input_ids, torch.tensor([[prediction_id]], device=decoder_input_ids.device)],
                    dim=-1
                )
            else:
                input_ids = torch.cat(
                    [input_ids, torch.tensor([[prediction_id]], device=input_ids.device)],
                    dim=-1
                )
                # Recomputing Attention Mask
                if getattr(self.model, '_prepare_attention_mask_for_generation'):
                    assert len(input_ids.size()) == 2 # will break otherwise
                    attention_mask = self.model._prepare_attention_mask_for_generation(input_ids, pad_token_id, eos_token_id)
                    attention_mask = self.to(attention_mask)
            offset = n_input_tokens if decoder_input_ids is not None else 0
            generated_token_ids = decoder_input_ids if decoder_input_ids is not None else input_ids
            # More than one token can be generated at once (e.g., automatic split/pad tokens)
            while len(generated_token_ids[0]) + offset != n_printed_tokens:
                # Display token
                if self.verbose:
                    self.display_token(
                        viz_id,
                        generated_token_ids[0][n_printed_tokens - offset].cpu().numpy(),
                        cur_len
                    )
                n_printed_tokens += 1
                # Add a zero vector to the attributions vector, if we did not reach the last predicted token
                if len(generated_token_ids[0]) + offset != n_printed_tokens:
                    for k in self.attributions:
                        self.attributions[k].insert(-1, np.zeros_like(self.attributions[k][-1]))
            cur_len += 1
        # Get encoder/decoder hidden states
        embedding_states = None
        for attributes in ["hidden_states", "encoder_hidden_states", "decoder_hidden_states"]:
            out_attr = getattr(output, attributes, None)
            if out_attr is not None:
                tokens_hs_list = []
                for token_out_attr in out_attr:
                    hs_list = []
                    for idx, layer_hs in enumerate(token_out_attr):
                        # in Hugging Face Transformers v4, there's an extra index for batch
                        if len(layer_hs.shape) == 3:  # If there's a batch dimension, pick the first oen
                            hs = layer_hs.cpu().detach()[0].unsqueeze(0)  # Adding a dimension to concat to later
                        # Earlier versions are only 2 dimensional
                        # But also, in v4, for GPT2, all except the last one would have 3 dims, the last layer
                        # would only have two dims
                        else:
                            hs = layer_hs.cpu().detach().unsqueeze(0)
                        hs_list.append(hs)
                    # First hidden state is the embedding layer, skip it
                    # FIXME: do this in a cleaner way
                    hs_list = torch.cat(hs_list, dim=0)
                    embedding_states = hs_list[0]
                    hidden_states = hs_list[1:]
                    tokens_hs_list.append(hidden_states)
                setattr(output, attributes, tokens_hs_list)
        # Pass 'hidden_states' to 'decoder_hidden_states'
        if getattr(output, "hidden_states", None) is not None:
            assert getattr(output, "encoder_hidden_states", None) is None \
                   and getattr(output, "decoder_hidden_states", None) is None, \
                "Not expected to have encoder_hidden_states/decoder_hidden_states with 'hidden_states'"
            setattr(output, "decoder_hidden_states", output.hidden_states)
        encoder_hidden_states = getattr(output, "encoder_hidden_states", None)
        decoder_hidden_states = getattr(output, "hidden_states", getattr(output, "decoder_hidden_states", None))
        # Turn activations from dict to a proper array
        activations_dict = self._all_activations_dict
        for layer_type, activations in activations_dict.items():
            self.activations[layer_type] = activations_dict_to_array(activations)
        if decoder_input_ids is not None:
            assert len(decoder_input_ids.size()) == 2
            all_token_ids = torch.cat([input_ids, decoder_input_ids], dim=-1)[0]
        else:
            all_token_ids = input_ids[0]
        tokens = self.tokenizer.convert_ids_to_tokens(all_token_ids)
        # tokens = []
        # for i in all_token_ids:
        #     token = self.tokenizer.decode([i])
        #     tokens.append(token)
        attributions = self.attributions
        attn = getattr(output, "attentions", None)
        return OutputSeq(**{'tokenizer': self.tokenizer,
                            'token_ids': all_token_ids.unsqueeze(0),  # Add a batch dimension
                            'n_input_tokens': n_input_tokens,
                            'output_text': self.tokenizer.decode(all_token_ids),
                            'tokens': [tokens],  # Add a batch dimension
                            'encoder_hidden_states': encoder_hidden_states,
                            'decoder_hidden_states': decoder_hidden_states,
                            'embedding_states': embedding_states,
                            'attention': attn,
                            'attribution': attributions,
                            'activations': self.activations,
                            'collect_activations_layer_nums': self.collect_activations_layer_nums,
                            'lm_head': self.model.lm_head,
                            'model_type': self.model_type,
                            'device': self.device,
                            'config': self.model_config})
    def __call__(self, input_tokens: torch.Tensor):
        """
        Run a forward pass through the model. For when we don't care about output tokens.
        Currently only support activations collection. No attribution/saliency.
        Usage:
        ```python
        inputs = lm.tokenizer("Hello computer", return_tensors="pt")
        output = lm(inputs)
        ```
        Args:
            input_tokens: tuple returned by tokenizer( TEXT, return_tensors="pt").
                contains key 'input_ids', its value tensor with input token ids.
                Shape is (batch_size, sequence_length).
                Also a key for masked tokens
        """
        if 'input_ids' not in input_tokens:
            raise ValueError("Parameter 'input_tokens' needs to have the attribute 'input_ids'."
                             "Verify it was produced by the appropriate tokenizer with the "
                             "parameter return_tensors=\"pt\".")
        # Move inputs to GPU if the model is on GPU
        if self.model.device.type == "cuda" and input_tokens['input_ids'].device.type == "cpu":
            input_tokens = self.to(input_tokens)
        # Remove downstream. For now setting to batch length
        n_input_tokens = len(input_tokens['input_ids'][0])
        # attach hooks
        self._attach_hooks(self.model)
        # model
        if self.model_type == 'mlm':
            output = self.model(**input_tokens, return_dict=True)
            lm_head = None
        elif self.model_type == 'causal':
            output = self.model(**input_tokens, return_dict=True, use_cache=False)
            lm_head = self.model.lm_head
        elif self.model_type == 'enc-dec':
            decoder_input_ids = self.model._prepare_decoder_input_ids_for_generation(input_tokens['input_ids'], None, None)
            output = self.model(**input_tokens, decoder_input_ids=decoder_input_ids, return_dict=True, use_cache=False)
            lm_head = self.model.lm_head
        else:
            raise NotImplemented(f"model type {self.model_type} not found")
        # Turn activations from dict to a proper array
        activations_dict = self._all_activations_dict
        for layer_type, activations in activations_dict.items():
            self.activations[layer_type] = activations_dict_to_array(activations)
        encoder_hidden_states = getattr(output, "encoder_hidden_states", None)
        decoder_hidden_states = getattr(output, "hidden_states", getattr(output, "decoder_hidden_states", None))
        if self.model_type in ['causal', 'mlm']:
            # First hidden state of the causal model is the embedding layer, skip it
            # FIXME: do this in a cleaner way
            embedding_states = decoder_hidden_states[0]
            decoder_hidden_states = decoder_hidden_states[1:]
        elif self.model_type == 'enc-dec':
            embedding_states = encoder_hidden_states[0]
            encoder_hidden_states = encoder_hidden_states[1:]
        else:
            raise NotImplemented(f"model type {self.model_type} not found")
        tokens = []
        for i in input_tokens['input_ids']:
            token = self.tokenizer.convert_ids_to_tokens(i)
            tokens.append(token)
        attn = getattr(output, "attentions", None)
        return OutputSeq(**{'tokenizer': self.tokenizer,
                            'token_ids': input_tokens['input_ids'],
                            'n_input_tokens': n_input_tokens,
                            'tokens': tokens,
                            'encoder_hidden_states': encoder_hidden_states,
                            'decoder_hidden_states': decoder_hidden_states,
                            'embedding_states': embedding_states,
                            'attention': attn,
                            'activations': self.activations,
                            'collect_activations_layer_nums': self.collect_activations_layer_nums,
                            'lm_head': lm_head,
                            'model_type': self.model_type,
                            'device': self.device,
                            'config': self.model_config})
    def _get_embeddings(self, input_ids) -> Tuple[torch.FloatTensor, torch.FloatTensor]:
        """
        Get token embeddings and one-hot vector into vocab. It's done via matrix multiplication
        so that gradient attribution is available when needed.
        Args:
            input_ids: Int tensor containing token ids. Of length (sequence length).
            Generally returned from the the tokenizer such as
            lm.tokenizer(text, return_tensors="pt")['input_ids'][0]
        Returns:
            inputs_embeds: Embeddings of the tokens. Dimensions are (sequence_len, d_embed)
            token_ids_tensor_one_hot: Dimensions are (sequence_len, vocab_size)
        """
        embedding_matrix = self.model_embeddings
        vocab_size = embedding_matrix.shape[0]
        one_hot_tensor = self.to(_one_hot_batched(input_ids, vocab_size))
        token_ids_tensor_one_hot = one_hot_tensor.clone().requires_grad_(True)
        inputs_embeds = torch.matmul(token_ids_tensor_one_hot, embedding_matrix)
        return inputs_embeds, token_ids_tensor_one_hot
    def _attach_hooks(self, model):
        # TODO: Collect activations for more than 1 step
        if self._hooks:
            # skip if hooks are already attached
            return
        for name, module in model.named_modules():
            # Add hooks to capture activations in every FFNN
            if re.search(self.collect_activations_layer_name_sig, name):
                # print("mlp.c_proj", self.collect_activations_flag , name)
                if self.collect_activations_flag:
                    self._hooks[name] = module.register_forward_hook(
                        lambda self_, input_, output,
                               name=name: self._get_activations_hook(name, input_))
                # Register neuron inhibition hook
                self._hooks[name + '_inhibit'] = module.register_forward_pre_hook(
                    lambda self_, input_, name=name: \
                        self._inhibit_neurons_hook(name, input_)
                )
    def _remove_hooks(self):
        for handle in self._hooks.values():
            handle.remove()
        self._hooks = {}
    def _get_activations_hook(self, name: str, input_):
        """
        Collects the activation for all tokens (input and output).
        The default activations collection method.
        Args:
            input_: activation tuple to capture. A tuple containing one tensor of
            dimensions (batch_size, sequence_length, neurons)
        """
        # print('_get_activations_hook', name)
        # pprint(input_)
        # print(type(input_), len(input_), type(input_[0]), input_[0].shape, len(input_[0]), input_[0][0].shape)
        # in distilGPT and GPT2, the layer name is 'transformer.h.0.mlp.c_fc'
        # Extract the number of the layer from the name
        # TODO: it will not always be 2 for other models. Move to model-config
        # layer_number = int(name.split('.')[2])
        # Get the layer number. This will be an int with periods before aand after it.
        # (?<=\.) means look for a period before the int
        # \d+ means look for one or multiple digits
        # (?=\.) means look for a period after the int
        layer_number = re.search("(?<=\.)\d+(?=\.)", name).group(0)
        layer_type = 'encoder' if name.startswith('encoder.') else 'decoder'
        # print("layer number: ", layer_number)
        collecting_this_layer = (self.collect_activations_layer_nums is None) or (
                layer_number in self.collect_activations_layer_nums)
        if collecting_this_layer:
            # Initialize the layer's key the first time we encounter it
            if layer_number not in self._all_activations_dict:
                self._all_activations_dict[layer_type][layer_number] = [0]
            # For MLM, we only run one inference step. We save it.
            # For Causal LM, we could be running multiple inference steps with generate(). In that case,
            # overwrite the previous step activations. This collects all activations in the last step
            # Assuming all input tokens are presented as input, no "past"
            # The inputs to c_proj already pass through the gelu activation function
            self._all_activations_dict[layer_type][layer_number] = input_[0].detach().cpu().numpy()
    def _inhibit_neurons_hook(self, name: str, input_tensor):
        """
        After being attached as a pre-forward hook, it sets to zero the activation value
        of the neurons indicated in self.neurons_to_inhibit
        """
        layer_number = re.search("(?<=\.)\d+(?=\.)", name).group(0)
        if layer_number in self.neurons_to_inhibit.keys():
            # print('layer_number', layer_number, input_tensor[0].shape)
            for n in self.neurons_to_inhibit[layer_number]:
                # print('inhibiting', layer_number, n)
                input_tensor[0][0][-1][n] = 0  # tuple, batch, position
        if layer_number in self.neurons_to_induce.keys():
            # print('layer_number', layer_number, input_tensor[0].shape)
            for n in self.neurons_to_induce[layer_number]:
                # print('inhibiting', layer_number, n)
                input_tensor[0][0][-1][n] = input_tensor[0][0][-1][n] * 10  # tuple, batch, position
        return input_tensor
    def display_input_sequence(self, input_ids):
        tokens = []
        for idx, token_id in enumerate(input_ids):
            type = "input"
            raw_token = self.tokenizer.convert_ids_to_tokens([token_id])[0]
            clean_token = self.tokenizer.decode(token_id)
            # Strip prefixes because bert decode still has ## for partials even after decode()
            clean_token = strip_tokenizer_prefix(self.model_config, clean_token)
            tokens.append({
                # 'token': self.tokenizer.decode([token_id]),
                           'token': clean_token,
                           'is_partial': is_partial_token(self.model_config, raw_token),
                           'position': idx,
                           'token_id': int(token_id),
                           'type': type})
        data = {'tokens': tokens}
        d.display(d.HTML(filename=os.path.join(self._path, "html", "setup.html")))
        viz_id = f'viz_{round(random.random() * 1000000)}'
        # TODO: Stop passing tokenization_config to JS now that
        # it's handled with the is_partial parameter
        js = f"""
         requirejs( ['basic', 'ecco'], function(basic, ecco){{
            basic.init('{viz_id}') // Python needs to know the viz id. Used for each output token.
            window.ecco['{viz_id}'] = new ecco.renderOutputSequence({{
                    parentDiv: '{viz_id}',
                    data: {json.dumps(data)},
                    tokenization_config: {json.dumps(self.model_config['tokenizer_config'])}
            }})
         }}, function (err) {{
            console.log(err);
        }})
        """
        d.display(d.Javascript(js))
        return viz_id
    def display_token(self, viz_id, token_id, position):
        raw_token = self.tokenizer.convert_ids_to_tokens([token_id])[0]
        clean_token = self.tokenizer.decode(token_id)
        # Strip prefixes because bert decode still has ## for partials even after decode()
        clean_token = strip_tokenizer_prefix(self.model_config, clean_token)
        token = {
            # 'token': self.tokenizer.decode([token_id]),
            'token': clean_token,
            'is_partial': is_partial_token(self.model_config, raw_token),
            'token_id': int(token_id),
            'position': position,
            'type': 'output'
        }
        js = f"""
        // We don't really need these require scripts. But this is to avert
        //this code from running before display_input_sequence which DOES require external files
        requirejs(['basic', 'ecco'], function(basic, ecco){{
                console.log('addToken viz_id', '{viz_id}');
                window.ecco['{viz_id}'].addToken({json.dumps(token)})
                window.ecco['{viz_id}'].redraw()
        }})
        """
        # print(js)
        d.display(d.Javascript(js))
    def predict_token(self, inputs, topk=50, temperature=1.0):
        output = self.model(**inputs)
        scores = output[0][0][-1] / temperature
        s = scores.detach().numpy()
        sorted_predictions = s.argsort()[::-1]
        sm = F.softmax(scores, dim=-1).detach().numpy()
        tokens = [self.tokenizer.decode([t]) for t in sorted_predictions[:topk]]
        probs = sm[sorted_predictions[:topk]]
        prediction_data = []
        for idx, (token, prob) in enumerate(zip(tokens, probs)):
            # print(idx, token, prob)
            prediction_data.append({'token': token,
                                    'prob': str(prob),
                                    'ranking': idx + 1,
                                    'token_id': str(sorted_predictions[idx])
                                    })
        params = prediction_data
        viz_id = 'viz_{}'.format(round(random.random() * 1000000))
        d.display(d.HTML(filename=os.path.join(self._path, "html", "predict_token.html")))
        js = """
        requirejs(['predict_token'], function(predict_token){{
        if (window.predict === undefined)
            window.predict = {{}}
        window.predict["{}"] = new predict_token.predictToken("{}", {})
        }}
        )
        """.format(viz_id, viz_id, json.dumps(params))
        d.display(d.Javascript(js))
def sample_output_token(scores, do_sample, temperature, top_k, top_p):
    # TODO: Add beam search in here
    if do_sample:
        # Temperature (higher temperature => more likely to sample low probability tokens)
        if temperature != 1.0:
            scores = scores / temperature
        # Top-p/top-k filtering
        if version.parse(transformers.__version__) >= version.parse('4.25.1'):
            top_k_top_p_filtering_fn = transformers.generation.utils.top_k_top_p_filtering
        else:
            top_k_top_p_filtering_fn = transformers.generation_utils.top_k_top_p_filtering
        next_token_logscores = top_k_top_p_filtering_fn(scores, top_k=top_k, top_p=top_p)
        # Sample
        probs = F.softmax(next_token_logscores, dim=-1)
        prediction_id = torch.multinomial(probs, num_samples=1)
    else:
        # Greedy decoding
        prediction_id = torch.argmax(scores, dim=-1)
    prediction_id = prediction_id.squeeze()
    return prediction_id
def _one_hot(token_ids: torch.Tensor, vocab_size: int) -> torch.Tensor:
    return torch.zeros(len(token_ids), vocab_size, device=token_ids.device).scatter_(1, token_ids.unsqueeze(1), 1.)
def _one_hot_batched(token_ids: torch.Tensor, vocab_size: int) -> torch.Tensor:
    batch_size, num_tokens = token_ids.shape
    return torch.zeros(batch_size, num_tokens, vocab_size, device=token_ids.device).scatter_(-1, token_ids.unsqueeze(-1), 1.)
def activations_dict_to_array(activations_dict):
    """
    Converts the dict used to collect activations into an array of the
    shape (batch, layers, neurons, token position).
    Args:
        activations_dict: python dictionary. Contains a key/value for each layer
        in the model whose activations were collected. Key is the layer id ('0', '1').
        Value is a tensor of shape (batch, position, neurons).
    """
    activations = []
    for i in sorted(activations_dict.keys()):
        activations.append(activations_dict[i])
    activations = np.array(activations)
    # 'activations' now is in the shape (layer, batch, position, neurons)
    activations = np.swapaxes(activations, 2, 3)
    activations = np.swapaxes(activations, 0, 1)
    # print('after swapping: ', activations.shape)
    return activations

================
File: src/ecco/output.py
================
import os
import json
import ecco
from IPython import display as d
from ecco import util, lm_plots
import random
import matplotlib.pyplot as plt
import numpy as np
import torch
from torch.nn import functional as F
from sklearn import decomposition
from typing import Dict, Optional, List, Tuple, Union
from ecco.util import strip_tokenizer_prefix, is_partial_token
class OutputSeq:
    """An OutputSeq object is the result of running a language model on some input data. It contains not only the output
    sequence of words generated by the model, but also other data collecting during the generation process
    that is useful to analyze the model.
    In addition to the data, the object has methods to create plots
    and visualizations of that collected data. These include:
    - [layer_predictions()](./#ecco.output.OutputSeq.layer_predictions) <br/>
    Which tokens did the model consider as the best outputs for a specific position in the sequence?
    - [rankings()](./#ecco.output.OutputSeq.rankings) <br/>
    After the model chooses an output token for a specific position, this visual looks back at the ranking
    of this token at each layer of the model when it was generated (layers assign scores to candidate output tokens,
    the higher the "probability" score, the higher the ranking of the token).
    - [rankings_watch()](./#ecco.output.OutputSeq.rankings_watch) <br />
    Shows the rankings of multiple tokens as the model scored them for a single position. For example, if the input is
    "The cat \_\_\_", we use this method to observe how the model ranked the words "is", "are", "was" as candidates
    to fill in the blank.
    - [primary_attributions()](./#ecco.output.OutputSeq.primary_attributions) <br />
    How important was each input token in the selection of calculating the output token?
    To process neuron activations, OutputSeq has methods to reduce the dimensionality and reveal underlying patterns in
    neuron firings. These are:
    - [run_nmf()](./#ecco.output.OutputSeq.run_nmf)
    """
    def __init__(self,
                 token_ids=None,
                 n_input_tokens=None,
                 tokenizer=None,
                 output_text=None,
                 tokens=None,
                 encoder_hidden_states=None,
                 decoder_hidden_states=None,
                 embedding_states=None,
                 attribution=None,
                 activations=None,
                 collect_activations_layer_nums=None,
                 attention=None,
                 model_type: str= 'mlm',
                 lm_head=None,
                 device='cpu',
                 config=None):
        """
        Args:
            token_ids: The input token ids. Dimensions: (batch, position)
            n_input_tokens: Int. The number of input tokens in the sequence.
            tokenizer: huggingface tokenizer associated with the model generating this output
            output_text: The output text generated by the model (if processed with generate())
            tokens: A list of token text. Shorthand to passing the token ids by the tokenizer.
                dimensions are (batch, position)
            hidden_states: A tensor of  dimensions (layer, position, hidden_dimension).
                In layer, index 0 is for embedding hidden_state.
            attribution: A list of attributions. One element per generated token.
                Each element is a list giving a value for tokens from 0 to right before the generated token.
            activations: The activations collected from model processing.
                Shape is (batch, layer, neurons, position)
            collect_activations_layer_nums:
            attention: The attention tensor retrieved from the language model
            model_outputs: Raw return object returned by the model
            lm_head: The trained language model head from a language model projecting a
                hidden state to an output vocabulary associated with teh tokenizer.
            device: "cuda" or "cpu"
            config: The configuration dict of the language model
        """
        self.token_ids = token_ids
        self.tokenizer = tokenizer
        self.n_input_tokens = n_input_tokens
        self.output_text = output_text
        self.tokens = tokens
        self.encoder_hidden_states = encoder_hidden_states
        self.decoder_hidden_states = decoder_hidden_states
        self.embedding_states = embedding_states
        self.attribution = attribution
        self.activations = activations
        self.collect_activations_layer_nums = collect_activations_layer_nums
        self.attention_values = attention
        self.lm_head = lm_head
        self.device = device
        self.config = config
        self.model_type = model_type
        self._path = os.path.dirname(ecco.__file__)
    def _get_encoder_hidden_states(self):
        return self.encoder_hidden_states if self.encoder_hidden_states is not None else self.decoder_hidden_states
    def _get_hidden_states(self) -> Tuple[Union[torch.Tensor, None], Union[torch.Tensor, None]]:
        """
        Returns a tuple with (encoder hidden states, decoder hidden states)
        """
        return (self.encoder_hidden_states, self.decoder_hidden_states)
    def __str__(self):
        return "<LMOutput '{}' # of lm outputs: {}>".format(self.output_text, len(self._get_hidden_states()[1][-1]))
    def to(self, tensor: torch.Tensor):
        if self.device == 'cuda':
            return tensor.to('cuda')
        return tensor
    def explorable(self, printJson: Optional[bool] = False):
        tokens = []
        for idx, token in enumerate(self.tokens[0]):
            type = "input" if idx < self.n_input_tokens else 'output'
            tokens.append({'token': token,
                           'token_id': int(self.token_ids[0][idx]),
                           'type': type
                           })
        data = {
            'tokens': tokens
        }
        d.display(d.HTML(filename=os.path.join(self._path, "html", "setup.html")))
        js = f"""
         requirejs(['basic', 'ecco'], function(basic, ecco){{
            const viz_id = basic.init()
            ecco.renderOutputSequence({{
                parentDiv: viz_id,
                data: {data},
                tokenization_config: {json.dumps(self.config['tokenizer_config'])}
            }})
         }}, function (err) {{
            console.log(err);
        }})"""
        d.display(d.Javascript(js))
        if printJson:
            print(data)
    def __call__(self, position=None, **kwargs):
        if position is not None:
            self.position(position, **kwargs)
        else:
            self.primary_attributions(**kwargs)
    def position(self, position, attr_method='grad_x_input'):
        if (position < self.n_input_tokens) or (position > len(self.tokens) - 1):
            raise ValueError("'position' should indicate a position of a generated token. "
                             "Accepted values for this sequence are between {} and {}."
                             .format(self.n_input_tokens, len(self.tokens) - 1))
        importance_id = position - self.n_input_tokens
        tokens = []
        assert attr_method in self.attribution, \
            f"attr_method={attr_method} not found. Choose one of the following: {list(self.attribution.keys())}"
        attribution = self.attribution[attr_method]
        for idx, token in enumerate(self.tokens):
            type = "input" if idx < self.n_input_tokens else 'output'
            if idx < len(attribution[importance_id]):
                imp = attribution[importance_id][idx]
            else:
                imp = -1
            tokens.append({'token': token,
                           'token_id': int(self.token_ids[idx]),
                           'type': type,
                           'value': str(imp)  # because json complains of floats
                           })
        data = {
            'tokens': tokens
        }
        d.display(d.HTML(filename=os.path.join(self._path, "html", "setup.html")))
        # d.display(d.HTML(filename=os.path.join(self._path, "html", "basic.html")))
        viz_id = 'viz_{}'.format(round(random.random() * 1000000))
        js = """
         requirejs(['basic', 'ecco'], function(basic, ecco){{
            const viz_id = basic.init()
            ecco.renderSeqHighlightPosition(viz_id, {}, {})
         }}, function (err) {{
            console.log(err);
        }})""".format(position, data)
        d.display(d.Javascript(js))
    def primary_attributions(self,
                             attr_method: Optional[str] = 'grad_x_input',
                             style="minimal",
                             ignore_tokens: Optional[List[int]] = [],
                             **kwargs):
        """
            Explorable showing primary attributions of each token generation step.
            Hovering-over or tapping an output token imposes a saliency map on other tokens
            showing their importance as features to that prediction.
            Examples:
            ```python
            import ecco
            lm = ecco.from_pretrained('distilgpt2')
            text= "The countries of the European Union are:\n1. Austria\n2. Belgium\n3. Bulgaria\n4."
            output = lm.generate(text, generate=20, do_sample=True)
            # Show primary attributions explorable
            output.primary_attributions()
            ```
            Which creates the following interactive explorable:
            ![input saliency example 1](../../img/saliency_ex_1.png)
            If we want more details on the saliency values, we can use the detailed view:
            ```python
            # Show detailed explorable
            output.primary_attributions(style="detailed")
            ```
            Which creates the following interactive explorable:
            ![input saliency example 2 - detailed](../../img/saliency_ex_2.png)
            Details:
            This view shows the Gradient * Inputs method of input saliency. The attribution values are calculated across the
            embedding dimensions, then we use the L2 norm to calculate a score for each token (from the values of its embeddings dimension)
            To get a percentage value, we normalize the scores by dividing by the sum of the attribution scores for all
            the tokens in the sequence.
        """
        position = self.n_input_tokens
        importance_id = position - self.n_input_tokens
        tokens = []
        assert attr_method in self.attribution, \
            f"attr_method={attr_method} not found. Choose one of the following: {list(self.attribution.keys())}"
        attribution = self.attribution[attr_method]
        for idx, token in enumerate(self.tokens[0]):
            token_id = self.token_ids[0][idx]
            raw_token = self.tokenizer.convert_ids_to_tokens([token_id])[0]
            clean_token = self.tokenizer.decode(token_id)
            # Strip prefixes because bert decode still has ## for partials even after decode()
            clean_token = strip_tokenizer_prefix(self.config, clean_token)
            type = "input" if idx < self.n_input_tokens else 'output'
            if idx < len(attribution[importance_id]):
                imp = attribution[importance_id][idx]
            else:
                imp = 0
            tokens.append({'token': clean_token,
                           'token_id': int(self.token_ids[0][idx]),
                           'is_partial': is_partial_token(self.config, raw_token),
                           'type': type,
                           'value': str(imp),  # because json complains of floats. Probably not used?
                           'position': idx
                           })
        if len(ignore_tokens) > 0:
            for output_token_index, _ in enumerate(attribution):
                for idx in ignore_tokens:
                    attribution[output_token_index][idx] = 0
        data = {
            'tokens': tokens,
            'attributions': [att.tolist() for att in attribution]
        }
        d.display(d.HTML(filename=os.path.join(self._path, "html", "setup.html")))
        if (style == "minimal"):
            js = f"""
             requirejs(['basic', 'ecco'], function(basic, ecco){{
                const viz_id = basic.init()
                console.log(viz_id)
                // ecco.interactiveTokens(viz_id, {{}})
                window.ecco[viz_id] = new ecco.MinimalHighlighter({{
                    parentDiv: viz_id,
                    data: {json.dumps(data)},
                    preset: 'viridis',
                    tokenization_config: {json.dumps(self.config['tokenizer_config'])}
             }})
             window.ecco[viz_id].init();
             window.ecco[viz_id].selectFirstToken();
             }}, function (err) {{
                console.log(err);
            }})"""
        elif (style == "detailed"):
            js = f"""
             requirejs(['basic', 'ecco'], function(basic, ecco){{
                const viz_id = basic.init()
                console.log(viz_id)
                window.ecco[viz_id] = ecco.interactiveTokens({{
                    parentDiv: viz_id,
                    data: {json.dumps(data)},
                    tokenization_config: {json.dumps(self.config['tokenizer_config'])}
             }})
             }}, function (err) {{
                console.log(err);
            }})"""
        d.display(d.Javascript(js))
        if 'printJson' in kwargs and kwargs['printJson']:
            print(data)
            return data
    def _repr_html_(self, **kwargs):
        # if util.type_of_script() == "jupyter":
        self.explorable(**kwargs)
        return '<OutputSeq>'
    def layer_predictions(self, position: int = 1, topk: Optional[int] = 10, layer: Optional[int] = None, **kwargs):
        """
            Visualization plotting the topk predicted tokens after each layer (using its hidden state).
            Example:
            ![prediction scores](../../img/layer_predictions_ex_london.png)
            Args:
                position: The index of the output token to trace
                topk: Number of tokens to show for each layer
                layer: None shows all layers. Can also pass an int with the layer id to show only that layer
        """
        assert self.model_type != 'mlm', "method not supported for Masked-LMs"
        _, dec_hidden_states = self._get_hidden_states()
        assert dec_hidden_states is not None, "decoder hidden states not found"
        if position == 0:
            raise ValueError(f"'position' is set to 0. There is never a hidden state associated with this position."
                             f"Possible values are 1 and above -- the position of the token of interest in the sequence")
        if self.model_type in ['enc-dec', 'causal']:
            # The position is relative. By that means, position self.n_input_tokens + 1 is the first generated token
            offset = 1 if self.model_type == 'enc-dec' else 0
            new_position = position - offset - self.n_input_tokens
            assert new_position >= 0, f"position={position} not supported, minimum is " \
                                      f"position={self.n_input_tokens + offset} for the first generated token"
            assert new_position < len(dec_hidden_states), f"position={position} not supported, maximum is " \
                                                          f"position={len(dec_hidden_states) - 1 + self.n_input_tokens + offset} " \
                                                          f"for the last generated token."
            position = new_position
        else:
            raise NotImplemented(f"model_type={self.model_type} not supported")
        dec_hidden_states = dec_hidden_states[position][:, -1, :] # only focus on the hidden states for that particular position
        if layer is not None:
            # If a layer is specified, choose it only.
            assert dec_hidden_states is not None
            dec_hidden_states = dec_hidden_states[layer].unsqueeze(0)
        k = topk
        top_tokens = []
        probs = []
        data = []
        # loop through layer levels
        for layer_no, h in enumerate(dec_hidden_states):
            # Use lm_head to project the layer's hidden state to output vocabulary
            logits = self.lm_head(self.to(h))
            softmax = F.softmax(logits, dim=-1)
            # softmax dims are (number of words in vocab) - 50257 in GPT2
            sorted_softmax = self.to(torch.argsort(softmax))
            # Not currently used. If we're "watching" a specific token, this gets its ranking
            # idx = sorted_softmax.shape[0] - torch.nonzero((sorted_softmax == watch)).flatten()
            layer_top_tokens = [self.tokenizer.decode(t) for t in sorted_softmax[-k:]][::-1]
            top_tokens.append(layer_top_tokens)
            layer_probs = softmax[sorted_softmax[-k:]].cpu().detach().numpy()[::-1]
            probs.append(layer_probs.tolist())
            # Package in output format
            layer_data = []
            for idx, (token, prob) in enumerate(zip(layer_top_tokens, layer_probs)):
                layer_num = layer if layer is not None else layer_no
                layer_data.append({'token': token,
                                   'prob': str(prob),
                                   'ranking': idx + 1,
                                   'layer': layer_num
                                   })
            data.append(layer_data)
        d.display(d.HTML(filename=os.path.join(self._path, "html", "setup.html")))
        # d.display(d.HTML(filename=os.path.join(self._path, "html", "basic.html")))
        js = f"""
         requirejs(['basic', 'ecco'], function(basic, ecco){{
            const viz_id = basic.init()
            let pred = new ecco.LayerPredictions({{
                parentDiv: viz_id,
                data:{json.dumps(data)}
            }})
            pred.init()
         }}, function (err) {{
            console.log(viz_id, err);
        }})"""
        d.display(d.Javascript(js))
        if 'printJson' in kwargs and kwargs['printJson']:
            print(data)
            return data
    def rankings(self, **kwargs):
        """
        Plots the rankings (across layers) of the tokens the model selected.
        Each column is a position in the sequence. Each row is a layer.
        ![Rankings watch](../../img/rankings_ex_eu_1.png)
        """
        assert self.model_type != 'mlm', "method not supported for Masked-LMs"
        _, dec_hidden_states = self._get_hidden_states()
        assert dec_hidden_states is not None, "decoder hidden states not found"
        n_layers_dec = dec_hidden_states[0].shape[0]
        position = len(dec_hidden_states)
        rankings = np.zeros((n_layers_dec, position), dtype=np.int32)
        predicted_tokens = np.empty((n_layers_dec, position), dtype='U25')
        token_found_mask = np.ones((n_layers_dec, position))
        # loop through tokens hidden states
        for j, token_hidden_states in enumerate(dec_hidden_states):
            # Loop through generated/output positions
            for i, hidden_state in enumerate(token_hidden_states[:, -1, :]):
                # Project hidden state to vocabulary
                # (after debugging pain: ensure input is on GPU, if appropriate)
                logits = self.lm_head(self.to(hidden_state))
                # Sort by score (ascending)
                sorted = torch.argsort(logits)
                # What token was sampled in this position?
                offset = self.n_input_tokens + 1 if self.model_type == 'enc-dec' else self.n_input_tokens
                token_id = torch.tensor(self.token_ids[0][offset + j])
                # token_id = self.token_ids.clone().detach()[self.n_input_tokens + j]
                # What's the index of the sampled token in the sorted list?
                r = torch.nonzero((sorted == token_id)).flatten()
                # subtract to get ranking (where 1 is the top scoring, because sorting was in ascending order)
                ranking = sorted.shape[0] - r
                token = self.tokenizer.decode([token_id])
                predicted_tokens[i, j] = token
                rankings[i, j] = int(ranking)
                if token_id == self.token_ids[0][j + 1]:
                    token_found_mask[i, j] = 0
        input_tokens = [repr(strip_tokenizer_prefix(self.config, t)) for t in self.tokens[0][self.n_input_tokens - 1:-1]]
        offset = self.n_input_tokens + 1 if self.model_type == 'enc-dec' else self.n_input_tokens
        output_tokens = [repr(strip_tokenizer_prefix(self.config, t)) for t in self.tokens[0][offset:]]
        lm_plots.plot_inner_token_rankings(input_tokens,
                                           output_tokens,
                                           rankings,
                                           **kwargs)
        if 'printJson' in kwargs and kwargs['printJson']:
            data = {
                'input_tokens': input_tokens,
                'output_tokens': output_tokens,
                'rankings': rankings,
                'predicted_tokens': predicted_tokens,
                'token_found_mask': token_found_mask
            }
            print(data)
            return data
    def rankings_watch(self, watch: List[int] = None, position: int = -1, **kwargs):
        """
        Plots the rankings of the tokens whose ids are supplied in the watch list.
        Only considers one position.
        ![Rankings plot](../../img/ranking_watch_ex_is_are_1.png)
        """
        assert self.model_type != 'mlm', "method not supported for Masked-LMs"
        _, dec_hidden_states = self._get_hidden_states()
        assert dec_hidden_states is not None, "decoder hidden states not found"
        if position != -1:
            if self.model_type in ['enc-dec', 'causal']:
                # The position is relative. By that means, position self.n_input_tokens + 1 is the first generated token
                offset = 1 if self.model_type == 'enc-dec' else 0
                new_position = position - offset - self.n_input_tokens
                assert new_position >= 0, f"position={position} not supported, minimum is " \
                                          f"position={self.n_input_tokens + offset} for the first generated token"
                assert new_position < len(dec_hidden_states), f"position={position} not supported, maximum is " \
                                                              f"position={len(dec_hidden_states) - 1 + self.n_input_tokens + offset} " \
                                                              f"for the last generated token."
                position = new_position
            else:
                raise NotImplemented(f"model_type={self.model_type} not supported")
        dec_hidden_states = dec_hidden_states[position][:, -1, :]
        n_layers_dec = len(dec_hidden_states) if dec_hidden_states is not None else 0
        n_tokens_to_watch = len(watch)
        rankings = np.zeros((n_layers_dec, n_tokens_to_watch), dtype=np.int32)
        # loop through layer levels
        for i, level in enumerate(dec_hidden_states):
            # Loop through generated/output positions
            for j, token_id in enumerate(watch):
                # Project hidden state to vocabulary
                # (after debugging pain: ensure input is on GPU, if appropriate)
                logits = self.lm_head(self.to(level))
                # Sort by score (ascending)
                sorted = torch.argsort(logits)
                # What token was sampled in this position?
                token_id = torch.tensor(token_id)
                # What's the index of the sampled token in the sorted list?
                r = torch.nonzero((sorted == token_id)).flatten()
                # subtract to get ranking (where 1 is the top scoring, because sorting was in ascending order)
                ranking = sorted.shape[0] - r
                rankings[i, j] = int(ranking)
        input_tokens = [strip_tokenizer_prefix(self.config,t) for t in self.tokens[0]]
        output_tokens = [repr(self.tokenizer.decode(t)) for t in watch]
        lm_plots.plot_inner_token_rankings_watch(input_tokens, output_tokens, rankings,
                                                 position + self.n_input_tokens if self.model_type == 'enc-dec' else position)
        if 'printJson' in kwargs and kwargs['printJson']:
            data = {'input_tokens': input_tokens,
                    'output_tokens': output_tokens,
                    'rankings': rankings}
            print(data)
            return data
    def attention(self, attention_values=None, layer=0, **kwargs):
        position = self.n_input_tokens
        # importance_id = position - self.n_input_tokens
        importance_id = self.n_input_tokens - 1  # Sete first values to first output token
        tokens = []
        if attention_values:
            attn = attention_values
        else:
            attn = self.attention_values[layer]
            # normalize attention heads
            attn = attn.sum(axis=1) / attn.shape[1]
        for idx, token in enumerate(self.tokens):
            # print(idx, attn.shape)
            type = "input" if idx < self.n_input_tokens else 'output'
            if idx < len(attn[0][importance_id]):
                attention_value = attn[0][importance_id][idx].cpu().detach().numpy()
            else:
                attention_value = 0
            tokens.append({'token': token,
                           'token_id': int(self.token_ids[idx]),
                           'type': type,
                           'value': str(attention_value),  # because json complains of floats
                           'position': idx
                           })
        data = {
            'tokens': tokens,
            'attributions': [att.tolist() for att in attn[0].cpu().detach().numpy()]
        }
        d.display(d.HTML(filename=os.path.join(self._path, "html", "setup.html")))
        # d.display(d.HTML(filename=os.path.join(self._path, "html", "basic.html")))
        viz_id = 'viz_{}'.format(round(random.random() * 1000000))
        js = """
         requirejs(['basic', 'ecco'], function(basic, ecco){{
            const viz_id = basic.init()
            ecco.interactiveTokens(viz_id, {})
         }}, function (err) {{
            console.log(err);
        }})""".format(data)
        d.display(d.Javascript(js))
        if 'printJson' in kwargs and kwargs['printJson']:
            print(data)
    def run_nmf(self, **kwargs):
        """
        Run Non-negative Matrix Factorization on network activations of FFNN. Returns an [NMF]() object which holds
        the factorization model and data and methods to visualize them.
        """
        return NMF(self.activations,
                   n_input_tokens=self.n_input_tokens,
                   token_ids=self.token_ids,
                   _path=self._path,
                   tokens=self.tokens,
                   config=self.config,
                   collect_activations_layer_nums=self.collect_activations_layer_nums,
                   **kwargs)
class NMF:
    """ Conducts NMF and holds the models and components """
    def __init__(self, activations: Dict[str, np.ndarray],
                 n_input_tokens: int = 0,
                 token_ids: torch.Tensor = torch.Tensor(0),
                 _path: str = '',
                 n_components: int = 10,
                 from_layer: Optional[int] = None,
                 to_layer: Optional[int] = None,
                 tokens: Optional[List[str]] = None,
                 collect_activations_layer_nums: Optional[List[int]] = None,
                 config=None,
                 **kwargs):
        """
        Receives a neuron activations tensor from OutputSeq and decomposes it using NMF into the number
        of components specified by `n_components`. For example, a model like `distilgpt2` has 18,000+
        neurons. Using NMF to reduce them to 32 components can reveal interesting underlying firing
        patterns.
        Args:
            activations: Activations tensor. Dimensions: (batch, layer, neuron, position)
            n_input_tokens: Number of input tokens.
            token_ids: List of tokens ids.
            _path: Disk path to find javascript that create interactive explorables
            n_components: Number of components/factors to reduce the neuron factors to.
            tokens: The text of each token.
            collect_activations_layer_nums: The list of layer ids whose activtions were collected. If
            None, then all layers were collected.
            """
        if activations == []:
            raise ValueError(f"No activation data found. Make sure 'activations=True' was passed to "
                             f"ecco.from_pretrained().")
        self._path = _path
        self.token_ids = token_ids
        self.n_input_tokens = n_input_tokens
        self.config = config
        # Joining Encoder and Decoder (if exists) together
        activations = np.concatenate(list(activations.values()), axis=-1)
        merged_act = self.reshape_activations(activations,
                                              from_layer,
                                              to_layer,
                                              collect_activations_layer_nums)
        # 'merged_act' is now ( neuron (and layer), position (and batch) )
        activations = merged_act
        self.tokens = tokens
        # Run NMF. 'activations' is neuron activations shaped (neurons (and layers), positions (and batches))
        n_output_tokens = activations.shape[-1]
        n_layers = activations.shape[0]
        n_components = min([n_components, n_output_tokens])
        components = np.zeros((n_layers, n_components, n_output_tokens))
        models = []
        # Get rid of negative activation values
        # (There are some, because GPT2 uses GELU, which allow small negative values)
        self.activations = np.maximum(activations, 0).T
        self.model = decomposition.NMF(n_components=n_components,
                                  init='random',
                                  random_state=0,
                                  max_iter=500)
        self.components = self.model.fit_transform(self.activations).T
    @staticmethod
    def reshape_activations(activations,
                            from_layer: Optional[int] = None,
                            to_layer: Optional[int] = None,
                            collect_activations_layer_nums: Optional[List[int]] = None):
        """Prepares the activations tensor for NMF by reshaping it from four dimensions
        (batch, layer, neuron, position) down to two:
        ( neuron (and layer), position (and batch) ).
        Args:
            activations (tensor): activations tensors of shape (batch, layers, neurons, positions) and float values
            from_layer (int or None): Start value. Used to indicate a range of layers whose activations are to
                be processed
            to_layer (int or None): End value. Used to indicate a range of layers
            collect_activations_layer_nums (list of ints or None): A list of layer IDs. Used to indicate specific
                layers whose activations are to be processed
        """
        if len(activations.shape) != 4:
            raise ValueError(f"The 'activations' parameter should have four dimensions: "
                             f"(batch, layers, neurons, positions). "
                             f"Supplied dimensions: {activations.shape}", 'activations')
        if collect_activations_layer_nums is None:
            collect_activations_layer_nums = list(range(activations.shape[1]))
        layer_nums_to_row_ixs = {layer_num: i
                                 for i, layer_num in enumerate(collect_activations_layer_nums)}
        if from_layer is not None or to_layer is not None:
            from_layer = from_layer if from_layer is not None else 0
            to_layer = to_layer if to_layer is not None else activations.shape[0]
            if from_layer == to_layer:
                raise ValueError(f"from_layer ({from_layer}) and to_layer ({to_layer}) cannot be the same value. "
                                 "They must be apart by at least one to allow for a layer of activations.")
            if from_layer > to_layer:
                raise ValueError(f"from_layer ({from_layer}) cannot be larger than to_layer ({to_layer}).")
            layer_nums = list(range(from_layer, to_layer))
        else:
            layer_nums = sorted(layer_nums_to_row_ixs.keys())
        if any([num not in layer_nums_to_row_ixs for num in layer_nums]):
            available = sorted(layer_nums_to_row_ixs.keys())
            raise ValueError(f"Not all layers between from_layer ({from_layer}) and to_layer ({to_layer}) "
                             f"have recorded activations. Layers with recorded activations are: {available}")
        row_ixs = [layer_nums_to_row_ixs[layer_num] for layer_num in layer_nums]
        activation_rows = [activations[:, row_ix] for row_ix in row_ixs]
        # Merge 'layers' and 'neuron' dimensions. Sending activations down from
        # (batch, layer, neuron, position) to (batch, neuron, position)
        merged_act = np.concatenate(activation_rows, axis=1)
        # merged_act = np.stack(activation_rows, axis=1)
        # 'merged_act' is now (batch, neuron (and layer), position)
        merged_act = merged_act.swapaxes(0, 1)
        # 'merged_act' is now (neuron (and layer), batch, position)
        merged_act = merged_act.reshape(merged_act.shape[0], -1)
        return merged_act
    def explore(self, input_sequence: int = 0, **kwargs):
        """
        Show interactive explorable for a single sequence with sparklines to isolate factors.
        Example:
            ![NMF Example](../../img/nmf_ex_1.png)
        Args:
            input_sequence: Which sequence in the batch to show.
        """
        tokens = []
        for idx, token in enumerate(self.tokens[input_sequence]):  # self.tokens[:-1]
            type = "input" if idx < self.n_input_tokens else 'output'
            tokens.append({'token': token,
                           'token_id': int(self.token_ids[input_sequence][idx]),
                           # 'token_id': int(self.token_ids[idx]),
                           'type': type,
                           # 'value': str(components[0][comp_num][idx]),  # because json complains of floats
                           'position': idx
                           })
        # If the sequence contains both input and generated tokens:
        # Duplicate the factor at index 'n_input_tokens'. THis way
        # each token has an activation value (instead of having one activation less than tokens)
        # But with different meanings: For inputs, the activation is a response
        # For outputs, the activation is a cause
        if len(self.token_ids[input_sequence]) != self.n_input_tokens:
            # Case: Generation. Duplicate value of last input token.
            factors = np.array(
                [np.concatenate([comp[:self.n_input_tokens], comp[self.n_input_tokens - 1:]]) for comp in
                  self.components])
            factors = [comp.tolist() for comp in factors]  # the json conversion needs this
        else:
            # Case: no generation
            factors = [comp.tolist() for comp in self.components]  # the json conversion needs this
        data = {
            # A list of dicts. Each in the shape {
            # Example: [{'token': 'by', 'token_id': 2011, 'type': 'input', 'position': 235}]
            'tokens': tokens,
            # Three-dimensional list. Shape: (1, factors, sequence length)
            'factors': [factors]
        }
        d.display(d.HTML(filename=os.path.join(self._path, "html", "setup.html")))
        js = f"""
         requirejs(['basic', 'ecco'], function(basic, ecco){{
            const viz_id = basic.init()
            ecco.interactiveTokensAndFactorSparklines(viz_id, {data},
            {{
            'hltrCFG': {{'tokenization_config': {json.dumps(self.config['tokenizer_config'])}
                }}
            }})
         }}, function (err) {{
            console.log(err);
        }})"""
        d.display(d.Javascript(js))
        if 'printJson' in kwargs and kwargs['printJson']:
            print(data)
            return data
    def plot(self, n_components=3):
        for idx, comp in enumerate(self.components):
            #     print('Layer {} components'.format(idx), 'Variance: {}'.format(lm.variances[idx][:n_components]))
            print('Layer {} components'.format(idx))
            comp = comp[:n_components, :].T
            #     plt.figure(figsize=(16,2))
            fig, ax1 = plt.subplots(1)
            plt.subplots_adjust(wspace=.4)
            fig.set_figheight(2)
            fig.set_figwidth(17)
            #     fig.tight_layout()
            # PCA Line plot
            ax1.plot(comp)
            ax1.set_xticks(range(len(self.tokens)))
            ax1.set_xticklabels(self.tokens, rotation=-90)
            ax1.legend(['Component {}'.format(i + 1) for i in range(n_components)], loc='center left',
                       bbox_to_anchor=(1.01, 0.5))
            plt.show()

================
File: src/ecco/svcca_lib/cca_core.py
================
# Copyright 2018 Google Inc.
# 
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
#     http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""
The core code for applying Canonical Correlation Analysis to deep networks.
This module contains the core functions to apply canonical correlation analysis
to deep neural networks. The main function is get_cca_similarity, which takes in
two sets of activations, typically the neurons in two layers and their outputs
on all of the datapoints D = [d_1,...,d_m] that have been passed through.
Inputs have shape (num_neurons1, m), (num_neurons2, m). This can be directly
applied used on fully connected networks. For convolutional layers, the 3d block
of neurons can either be flattened entirely, along channels, or alternatively,
the dft_ccas (Discrete Fourier Transform) module can be used.
See:
https://arxiv.org/abs/1706.05806
https://arxiv.org/abs/1806.05759
for full details.
"""
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
import numpy as np
num_cca_trials = 5
def positivedef_matrix_sqrt(array):
  """Stable method for computing matrix square roots, supports complex matrices.
  Args:
            array: A numpy 2d array, can be complex valued that is a positive
                   definite symmetric (or hermitian) matrix
  Returns:
            sqrtarray: The matrix square root of array
  """
  w, v = np.linalg.eigh(array)
  #  A - np.dot(v, np.dot(np.diag(w), v.T))
  wsqrt = np.sqrt(w)
  sqrtarray = np.dot(v, np.dot(np.diag(wsqrt), np.conj(v).T))
  return sqrtarray
def remove_small(sigma_xx, sigma_xy, sigma_yx, sigma_yy, epsilon):
  """Takes covariance between X, Y, and removes values of small magnitude.
  Args:
            sigma_xx: 2d numpy array, variance matrix for x
            sigma_xy: 2d numpy array, crossvariance matrix for x,y
            sigma_yx: 2d numpy array, crossvariance matrixy for x,y,
                      (conjugate) transpose of sigma_xy
            sigma_yy: 2d numpy array, variance matrix for y
            epsilon : cutoff value for norm below which directions are thrown
                       away
  Returns:
            sigma_xx_crop: 2d array with low x norm directions removed
            sigma_xy_crop: 2d array with low x and y norm directions removed
            sigma_yx_crop: 2d array with low x and y norm directiosn removed
            sigma_yy_crop: 2d array with low y norm directions removed
            x_idxs: indexes of sigma_xx that were removed
            y_idxs: indexes of sigma_yy that were removed
  """
  x_diag = np.abs(np.diagonal(sigma_xx))
  y_diag = np.abs(np.diagonal(sigma_yy))
  x_idxs = (x_diag >= epsilon)
  y_idxs = (y_diag >= epsilon)
  sigma_xx_crop = sigma_xx[x_idxs][:, x_idxs]
  sigma_xy_crop = sigma_xy[x_idxs][:, y_idxs]
  sigma_yx_crop = sigma_yx[y_idxs][:, x_idxs]
  sigma_yy_crop = sigma_yy[y_idxs][:, y_idxs]
  return (sigma_xx_crop, sigma_xy_crop, sigma_yx_crop, sigma_yy_crop,
          x_idxs, y_idxs)
def compute_ccas(sigma_xx, sigma_xy, sigma_yx, sigma_yy, epsilon,
                 verbose=True):
  """Main cca computation function, takes in variances and crossvariances.
  This function takes in the covariances and cross covariances of X, Y,
  preprocesses them (removing small magnitudes) and outputs the raw results of
  the cca computation, including cca directions in a rotated space, and the
  cca correlation coefficient values.
  Args:
            sigma_xx: 2d numpy array, (num_neurons_x, num_neurons_x)
                      variance matrix for x
            sigma_xy: 2d numpy array, (num_neurons_x, num_neurons_y)
                      crossvariance matrix for x,y
            sigma_yx: 2d numpy array, (num_neurons_y, num_neurons_x)
                      crossvariance matrix for x,y (conj) transpose of sigma_xy
            sigma_yy: 2d numpy array, (num_neurons_y, num_neurons_y)
                      variance matrix for y
            epsilon:  small float to help with stabilizing computations
            verbose:  boolean on whether to print intermediate outputs
  Returns:
            [ux, sx, vx]: [numpy 2d array, numpy 1d array, numpy 2d array]
                          ux and vx are (conj) transposes of each other, being
                          the canonical directions in the X subspace.
                          sx is the set of canonical correlation coefficients-
                          how well corresponding directions in vx, Vy correlate
                          with each other.
            [uy, sy, vy]: Same as above, but for Y space
            invsqrt_xx:   Inverse square root of sigma_xx to transform canonical
                          directions back to original space
            invsqrt_yy:   Same as above but for sigma_yy
            x_idxs:       The indexes of the input sigma_xx that were pruned
                          by remove_small
            y_idxs:       Same as above but for sigma_yy
  """
  (sigma_xx, sigma_xy, sigma_yx, sigma_yy,
   x_idxs, y_idxs) = remove_small(sigma_xx, sigma_xy, sigma_yx, sigma_yy, epsilon)
  numx = sigma_xx.shape[0]
  numy = sigma_yy.shape[0]
  if numx == 0 or numy == 0:
    return ([0, 0, 0], [0, 0, 0], np.zeros_like(sigma_xx),
            np.zeros_like(sigma_yy), x_idxs, y_idxs)
  if verbose:
    print("adding eps to diagonal and taking inverse")
  sigma_xx += epsilon * np.eye(numx)
  sigma_yy += epsilon * np.eye(numy)
  inv_xx = np.linalg.pinv(sigma_xx)
  inv_yy = np.linalg.pinv(sigma_yy)
  if verbose:
    print("taking square root")
  invsqrt_xx = positivedef_matrix_sqrt(inv_xx)
  invsqrt_yy = positivedef_matrix_sqrt(inv_yy)
  if verbose:
    print("dot products...")
  arr = np.dot(invsqrt_xx, np.dot(sigma_xy, invsqrt_yy))
  if verbose:
    print("trying to take final svd")
  u, s, v = np.linalg.svd(arr)
  if verbose:
    print("computed everything!")
  return [u, np.abs(s), v], invsqrt_xx, invsqrt_yy, x_idxs, y_idxs
def sum_threshold(array, threshold):
  """Computes threshold index of decreasing nonnegative array by summing.
  This function takes in a decreasing array nonnegative floats, and a
  threshold between 0 and 1. It returns the index i at which the sum of the
  array up to i is threshold*total mass of the array.
  Args:
            array: a 1d numpy array of decreasing, nonnegative floats
            threshold: a number between 0 and 1
  Returns:
            i: index at which np.sum(array[:i]) >= threshold
  """
  assert (threshold >= 0) and (threshold <= 1), "print incorrect threshold"
  for i in range(len(array)):
    if np.sum(array[:i])/np.sum(array) >= threshold:
      return i
def create_zero_dict(compute_dirns, dimension):
  """Outputs a zero dict when neuron activation norms too small.
  This function creates a return_dict with appropriately shaped zero entries
  when all neuron activations are very small.
  Args:
            compute_dirns: boolean, whether to have zero vectors for directions
            dimension: int, defines shape of directions
  Returns:
            return_dict: a dict of appropriately shaped zero entries
  """
  return_dict = {}
  return_dict["mean"] = (np.asarray(0), np.asarray(0))
  return_dict["sum"] = (np.asarray(0), np.asarray(0))
  return_dict["cca_coef1"] = np.asarray(0)
  return_dict["cca_coef2"] = np.asarray(0)
  return_dict["idx1"] = 0
  return_dict["idx2"] = 0
  if compute_dirns:
    return_dict["cca_dirns1"] = np.zeros((1, dimension))
    return_dict["cca_dirns2"] = np.zeros((1, dimension))
  return return_dict
def get_cca_similarity(acts1, acts2, epsilon=0., threshold=0.98,
                       compute_coefs=True,
                       compute_dirns=False,
                       verbose=True):
  """The main function for computing cca similarities.
  This function computes the cca similarity between two sets of activations,
  returning a dict with the cca coefficients, a few statistics of the cca
  coefficients, and (optionally) the actual directions.
  Args:
            acts1: (num_neurons1, data_points) a 2d numpy array of neurons by
                   datapoints where entry (i,j) is the output of neuron i on
                   datapoint j.
            acts2: (num_neurons2, data_points) same as above, but (potentially)
                   for a different set of neurons. Note that acts1 and acts2
                   can have different numbers of neurons, but must agree on the
                   number of datapoints
            epsilon: small float to help stabilize computations
            threshold: float between 0, 1 used to get rid of trailing zeros in
                       the cca correlation coefficients to output more accurate
                       summary statistics of correlations.
            compute_coefs: boolean value determining whether coefficients
                           over neurons are computed. Needed for computing
                           directions
            compute_dirns: boolean value determining whether actual cca
                           directions are computed. (For very large neurons and
                           datasets, may be better to compute these on the fly
                           instead of store in memory.)
            verbose: Boolean, whether intermediate outputs are printed
  Returns:
            return_dict: A dictionary with outputs from the cca computations.
                         Contains neuron coefficients (combinations of neurons
                         that correspond to cca directions), the cca correlation
                         coefficients (how well aligned directions correlate),
                         x and y idxs (for computing cca directions on the fly
                         if compute_dirns=False), and summary statistics. If
                         compute_dirns=True, the cca directions are also
                         computed.
  """
  # assert dimensionality equal
  assert acts1.shape[1] == acts2.shape[1], "dimensions don't match"
  # check that acts1, acts2 are transposition
  assert acts1.shape[0] < acts1.shape[1], ("input must be number of neurons"
                                           "by datapoints")
  return_dict = {}
  # compute covariance with numpy function for extra stability
  numx = acts1.shape[0]
  numy = acts2.shape[0]
  covariance = np.cov(acts1, acts2)
  sigmaxx = covariance[:numx, :numx]
  sigmaxy = covariance[:numx, numx:]
  sigmayx = covariance[numx:, :numx]
  sigmayy = covariance[numx:, numx:]
  # rescale covariance to make cca computation more stable
  xmax = np.max(np.abs(sigmaxx))
  ymax = np.max(np.abs(sigmayy))
  sigmaxx /= xmax
  sigmayy /= ymax
  sigmaxy /= np.sqrt(xmax * ymax)
  sigmayx /= np.sqrt(xmax * ymax)
  ([u, s, v], invsqrt_xx, invsqrt_yy,
   x_idxs, y_idxs) = compute_ccas(sigmaxx, sigmaxy, sigmayx, sigmayy,
                                  epsilon=epsilon,
                                  verbose=verbose)
  # if x_idxs or y_idxs is all false, return_dict has zero entries
  if (not np.any(x_idxs)) or (not np.any(y_idxs)):
    return create_zero_dict(compute_dirns, acts1.shape[1])
  if compute_coefs:
    # also compute full coefficients over all neurons
    x_mask = np.dot(x_idxs.reshape((-1, 1)), x_idxs.reshape((1, -1)))
    y_mask = np.dot(y_idxs.reshape((-1, 1)), y_idxs.reshape((1, -1)))
    return_dict["coef_x"] = u.T
    return_dict["invsqrt_xx"] = invsqrt_xx
    return_dict["full_coef_x"] = np.zeros((numx, numx))
    np.place(return_dict["full_coef_x"], x_mask,
             return_dict["coef_x"])
    return_dict["full_invsqrt_xx"] = np.zeros((numx, numx))
    np.place(return_dict["full_invsqrt_xx"], x_mask,
             return_dict["invsqrt_xx"])
    return_dict["coef_y"] = v
    return_dict["invsqrt_yy"] = invsqrt_yy
    return_dict["full_coef_y"] = np.zeros((numy, numy))
    np.place(return_dict["full_coef_y"], y_mask,
             return_dict["coef_y"])
    return_dict["full_invsqrt_yy"] = np.zeros((numy, numy))
    np.place(return_dict["full_invsqrt_yy"], y_mask,
             return_dict["invsqrt_yy"])
    # compute means
    neuron_means1 = np.mean(acts1, axis=1, keepdims=True)
    neuron_means2 = np.mean(acts2, axis=1, keepdims=True)
    return_dict["neuron_means1"] = neuron_means1
    return_dict["neuron_means2"] = neuron_means2
  if compute_dirns:
    # orthonormal directions that are CCA directions
    cca_dirns1 = np.dot(np.dot(return_dict["full_coef_x"],
                               return_dict["full_invsqrt_xx"]),
                        (acts1 - neuron_means1)) + neuron_means1
    cca_dirns2 = np.dot(np.dot(return_dict["full_coef_y"],
                               return_dict["full_invsqrt_yy"]),
                        (acts2 - neuron_means2)) + neuron_means2
  # get rid of trailing zeros in the cca coefficients
  idx1 = sum_threshold(s, threshold)
  idx2 = sum_threshold(s, threshold)
  return_dict["cca_coef1"] = s
  return_dict["cca_coef2"] = s
  return_dict["x_idxs"] = x_idxs
  return_dict["y_idxs"] = y_idxs
  # summary statistics
  return_dict["mean"] = (np.mean(s[:idx1]), np.mean(s[:idx2]))
  return_dict["sum"] = (np.sum(s), np.sum(s))
  if compute_dirns:
    return_dict["cca_dirns1"] = cca_dirns1
    return_dict["cca_dirns2"] = cca_dirns2
  return return_dict
def robust_cca_similarity(acts1, acts2, threshold=0.98, epsilon=1e-6,
                          compute_dirns=True):
  """Calls get_cca_similarity multiple times while adding noise.
  This function is very similar to get_cca_similarity, and can be used if
  get_cca_similarity doesn't converge for some pair of inputs. This function
  adds some noise to the activations to help convergence.
  Args:
            acts1: (num_neurons1, data_points) a 2d numpy array of neurons by
                   datapoints where entry (i,j) is the output of neuron i on
                   datapoint j.
            acts2: (num_neurons2, data_points) same as above, but (potentially)
                   for a different set of neurons. Note that acts1 and acts2
                   can have different numbers of neurons, but must agree on the
                   number of datapoints
            threshold: float between 0, 1 used to get rid of trailing zeros in
                       the cca correlation coefficients to output more accurate
                       summary statistics of correlations.
            epsilon: small float to help stabilize computations
            compute_dirns: boolean value determining whether actual cca
                           directions are computed. (For very large neurons and
                           datasets, may be better to compute these on the fly
                           instead of store in memory.)
  Returns:
            return_dict: A dictionary with outputs from the cca computations.
                         Contains neuron coefficients (combinations of neurons
                         that correspond to cca directions), the cca correlation
                         coefficients (how well aligned directions correlate),
                         x and y idxs (for computing cca directions on the fly
                         if compute_dirns=False), and summary statistics. If
                         compute_dirns=True, the cca directions are also
                         computed.
  """
  for trial in range(num_cca_trials):
    try:
      return_dict = get_cca_similarity(acts1, acts2, threshold, compute_dirns)
    except np.LinAlgError:
      acts1 = acts1*1e-1 + np.random.normal(size=acts1.shape)*epsilon
      acts2 = acts2*1e-1 + np.random.normal(size=acts1.shape)*epsilon
      if trial + 1 == num_cca_trials:
        raise
  return return_dict

================
File: src/ecco/svcca_lib/cka_lib.py
================
# Copyright 2019 Google Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import numpy as np
def gram_linear(x):
  """Compute Gram (kernel) matrix for a linear kernel.
  Args:
    x: A num_examples x num_features matrix of features.
  Returns:
    A num_examples x num_examples Gram matrix of examples.
  """
  return x.dot(x.T)
def gram_rbf(x, threshold=1.0):
  """Compute Gram (kernel) matrix for an RBF kernel.
  Args:
    x: A num_examples x num_features matrix of features.
    threshold: Fraction of median Euclidean distance to use as RBF kernel
      bandwidth. (This is the heuristic we use in the paper. There are other
      possible ways to set the bandwidth; we didn't try them.)
  Returns:
    A num_examples x num_examples Gram matrix of examples.
  """
  dot_products = x.dot(x.T)
  sq_norms = np.diag(dot_products)
  sq_distances = -2 * dot_products + sq_norms[:, None] + sq_norms[None, :]
  sq_median_distance = np.median(sq_distances)
  return np.exp(-sq_distances / (2 * threshold ** 2 * sq_median_distance))
def center_gram(gram, unbiased=False):
  """Center a symmetric Gram matrix.
  This is equvialent to centering the (possibly infinite-dimensional) features
  induced by the kernel before computing the Gram matrix.
  Args:
    gram: A num_examples x num_examples symmetric matrix.
    unbiased: Whether to adjust the Gram matrix in order to compute an unbiased
      estimate of HSIC. Note that this estimator may be negative.
  Returns:
    A symmetric matrix with centered columns and rows.
  """
  if not np.allclose(gram, gram.T):
    raise ValueError('Input must be a symmetric matrix.')
  gram = gram.copy()
  if unbiased:
    # This formulation of the U-statistic, from Szekely, G. J., & Rizzo, M.
    # L. (2014). Partial distance correlation with methods for dissimilarities.
    # The Annals of Statistics, 42(6), 2382-2412, seems to be more numerically
    # stable than the alternative from Song et al. (2007).
    n = gram.shape[0]
    np.fill_diagonal(gram, 0)
    means = np.sum(gram, 0, dtype=np.float64) / (n - 2)
    means -= np.sum(means) / (2 * (n - 1))
    gram -= means[:, None]
    gram -= means[None, :]
    np.fill_diagonal(gram, 0)
  else:
    means = np.mean(gram, 0, dtype=np.float64)
    means -= np.mean(means) / 2
    gram -= means[:, None]
    gram -= means[None, :]
  return gram
def cka(gram_x, gram_y, debiased=False):
  """Compute CKA.
  Args:
    gram_x: A num_examples x num_examples Gram matrix.
    gram_y: A num_examples x num_examples Gram matrix.
    debiased: Use unbiased estimator of HSIC. CKA may still be biased.
  Returns:
    The value of CKA between X and Y.
  """
  gram_x = center_gram(gram_x, unbiased=debiased)
  gram_y = center_gram(gram_y, unbiased=debiased)
  # Note: To obtain HSIC, this should be divided by (n-1)**2 (biased variant) or
  # n*(n-3) (unbiased variant), but this cancels for CKA.
  scaled_hsic = gram_x.ravel().dot(gram_y.ravel())
  normalization_x = np.linalg.norm(gram_x)
  normalization_y = np.linalg.norm(gram_y)
  return scaled_hsic / (normalization_x * normalization_y)
def _debiased_dot_product_similarity_helper(
    xty, sum_squared_rows_x, sum_squared_rows_y, squared_norm_x, squared_norm_y,
    n):
  """Helper for computing debiased dot product similarity (i.e. linear HSIC)."""
  # This formula can be derived by manipulating the unbiased estimator from
  # Song et al. (2007).
  return (
      xty - n / (n - 2.) * sum_squared_rows_x.dot(sum_squared_rows_y)
      + squared_norm_x * squared_norm_y / ((n - 1) * (n - 2)))
def feature_space_linear_cka(features_x, features_y, debiased=False):
  """Compute CKA with a linear kernel, in feature space.
  This is typically faster than computing the Gram matrix when there are fewer
  features than examples.
  Args:
    features_x: A num_examples x num_features matrix of features.
    features_y: A num_examples x num_features matrix of features.
    debiased: Use unbiased estimator of dot product similarity. CKA may still be
      biased. Note that this estimator may be negative.
  Returns:
    The value of CKA between X and Y.
  """
  features_x = features_x - np.mean(features_x, 0, keepdims=True)
  features_y = features_y - np.mean(features_y, 0, keepdims=True)
  dot_product_similarity = np.linalg.norm(features_x.T.dot(features_y)) ** 2
  normalization_x = np.linalg.norm(features_x.T.dot(features_x))
  normalization_y = np.linalg.norm(features_y.T.dot(features_y))
  if debiased:
    n = features_x.shape[0]
    # Equivalent to np.sum(features_x ** 2, 1) but avoids an intermediate array.
    sum_squared_rows_x = np.einsum('ij,ij->i', features_x, features_x)
    sum_squared_rows_y = np.einsum('ij,ij->i', features_y, features_y)
    squared_norm_x = np.sum(sum_squared_rows_x)
    squared_norm_y = np.sum(sum_squared_rows_y)
    dot_product_similarity = _debiased_dot_product_similarity_helper(
        dot_product_similarity, sum_squared_rows_x, sum_squared_rows_y,
        squared_norm_x, squared_norm_y, n)
    normalization_x = np.sqrt(_debiased_dot_product_similarity_helper(
        normalization_x ** 2, sum_squared_rows_x, sum_squared_rows_x,
        squared_norm_x, squared_norm_x, n))
    normalization_y = np.sqrt(_debiased_dot_product_similarity_helper(
        normalization_y ** 2, sum_squared_rows_y, sum_squared_rows_y,
        squared_norm_y, squared_norm_y, n))
  return dot_product_similarity / (normalization_x * normalization_y)

================
File: src/ecco/svcca_lib/pwcca.py
================
# Copyright 2018 Google Inc.
# 
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
#     http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""
The function for computing projection weightings.
See:
https://arxiv.org/abs/1806.05759
for full details.
"""
import numpy as np
from ecco.svcca_lib import cca_core
def compute_pwcca(acts1, acts2, epsilon=0.):
    """ Computes projection weighting for weighting CCA coefficients 
    Args:
        acts1: 2d numpy array, shaped (neurons, num_datapoints)
	    acts2: 2d numpy array, shaped (neurons, num_datapoints)
    Returns:
	 Original cca coefficient mean and weighted mean
    """
    sresults = cca_core.get_cca_similarity(acts1, acts2, epsilon=epsilon,
                                           compute_dirns=False, compute_coefs=True, verbose=False)
    if np.sum(sresults["x_idxs"]) <= np.sum(sresults["y_idxs"]):
        dirns = np.dot(sresults["coef_x"],
                       (acts1[sresults["x_idxs"]] - \
                        sresults["neuron_means1"][sresults["x_idxs"]])) + sresults["neuron_means1"][sresults["x_idxs"]]
        coefs = sresults["cca_coef1"]
        acts = acts1
        idxs = sresults["x_idxs"]
    else:
        dirns = np.dot(sresults["coef_y"],
                       (acts1[sresults["y_idxs"]] - \
                        sresults["neuron_means2"][sresults["y_idxs"]])) + sresults["neuron_means2"][sresults["y_idxs"]]
        coefs = sresults["cca_coef2"]
        acts = acts2
        idxs = sresults["y_idxs"]
    P, _ = np.linalg.qr(dirns.T)
    weights = np.sum(np.abs(np.dot(P.T, acts[idxs].T)), axis=1)
    weights = weights / np.sum(weights)
    return np.sum(weights * coefs), weights, coefs

================
File: src/ecco/util.py
================
import yaml
import os
# CHeck if running from inside jupyter
# From https://stackoverflow.com/questions/47211324/check-if-module-is-running-in-jupyter-or-not
def type_of_script():
    try:
        ipy_str = str(type(get_ipython()))
        if 'zmqshell' in ipy_str:
            return 'jupyter'
        if 'terminal' in ipy_str:
            return 'ipython'
    except:
        return 'terminal'
def load_config(model_name):
    path = os.path.dirname(__file__) 
    configs = yaml.safe_load(open(os.path.join(path, "model-config.yaml"),
                                  encoding="utf8"))
    try:
        model_config = configs[model_name]
        model_config = pack_tokenizer_config(model_config)
    except KeyError:
        raise ValueError(
                f"The model '{model_name}' is not defined in Ecco's 'model-config.yaml' file and"
                f" so is not explicitly supported yet. Supported models are:",
                list(configs.keys())) from KeyError()
    return model_config
def pack_tokenizer_config(model_config):
    """
    Convenience method to package tokenizer configs into one element to more easily pass it to
    JavaScript rendering code.
    Args:
        model_config: dict of model configuration options used for model-config or in __init__.py
    Returns:
        model_config dict with 'tokenizer_config' elements
    """
    tokenizer_config = {'token_prefix': model_config['token_prefix'],
                        'partial_token_prefix': model_config['partial_token_prefix']}
    model_config['tokenizer_config'] = tokenizer_config
    return model_config
def strip_tokenizer_prefix(model_config,
                           token,
                           ellipsis_partial_tokens=False):
    token = token.lstrip(model_config['token_prefix'])
    token = token.lstrip(model_config['partial_token_prefix'])
    token= token.lstrip(' ')
    return token
def is_partial_token(model_config,
                              token):
    if (token[0: len(model_config['partial_token_prefix'])] == model_config['partial_token_prefix']) and \
            ((len(model_config['token_prefix']) == 0) or \
             token[0:len(model_config['token_prefix'])] != model_config['token_prefix']):
        return True
    else:
        return False

================
File: tests/activations_test.py
================
import pytest
import torch
import numpy as np
from ecco import activations
class TestActivations:
    def test_reshape_hidden_states(self):
        # Create a tensor of shape (1,2,3,1)
        t = torch.stack([torch.ones(3), torch.zeros(3)]).unsqueeze(0).unsqueeze(-1)
        result = activations.reshape_hidden_states_to_3d(t)
        assert result.shape == (1, 6, 1)

================
File: tests/analysis_tests.py
================
from ecco import analysis
import pytest
import numpy as np
shape = (100, 1000)
np.random.seed(seed=1)
@pytest.fixture
def acts():
    acts1 = np.random.randn(*shape)
    acts2 = np.random.randn(*shape)
    yield acts1, acts2
class TestAnalysis:
    def test_cca_smoke(self, acts):
        actual = analysis.cca(acts[0], acts[1])
        assert isinstance(actual, float)
        assert actual >= 0
        assert actual <= 1
    def test_svcca_smoke(self, acts):
        actual = analysis.svcca(acts[0], acts[1])
        assert isinstance(actual, float)
        assert actual >= 0
        assert actual <= 1
    def test_pwcca_smoke(self, acts):
        actual = analysis.pwcca(acts[0], acts[1])
        assert isinstance(actual, float)
        assert actual >= 0
        assert actual <= 1
    def test_cka_smoke(self, acts):
        actual = analysis.cka(acts[0], acts[1])
        assert isinstance(actual, float)
        assert actual >= 0
        assert actual <= 1
    def test_linear_transformation(self, acts):
        acts_1 = acts[0]
        acts_2 = acts_1 * 10
        assert pytest.approx(analysis.cca(acts_1, acts_2), 1.0), "CCA of linear transformation is approx 1.0"
        assert pytest.approx(analysis.svcca(acts_1, acts_2), 1.0), "SVCCA of linear transformation is approx 1.0"
        assert pytest.approx(analysis.pwcca(acts_1, acts_2), 1.0), "PWCCA of linear transformation is approx 1.0"
        assert pytest.approx(analysis.cka(acts_1, acts_2), 1.0), "CKA of linear transformation is approx 1.0"

================
File: tests/lm_plots_test.py
================
import pytest
import numpy as np
from ecco import lm_plots
import os
class TestLMPlots:
    def test_save_rankings_plot(self, rankings_plot_data_1):
        lm_plots.plot_inner_token_rankings(**rankings_plot_data_1, save_file_path='./tmp/ranking_1.png')
    def test_save_ranking_watch_plot(self, ranking_watch_data_1):
        lm_plots.plot_inner_token_rankings_watch(**ranking_watch_data_1, save_file_path='./tmp/ranking_watch_1.png')
@pytest.fixture
def rankings_plot_data_1():
    yield {'input_tokens': ["'.'",
                            "' Denmark'",
                            "'\\n'",
                            "'5'",
                            "'.'",
                            "' Estonia'",
                            "'\\n'",
                            "'6'",
                            "'.'",
                            "' Hungary'"],
           'output_tokens': ["' Denmark'",
                             "'\\n'",
                             "'5'",
                             "'.'",
                             "' Estonia'",
                             "'\\n'",
                             "'6'",
                             "'.'",
                             "' Hungary'",
                             "'\\n'"],
           'rankings': np.array([[32716, 7, 162, 3, 35167, 10, 233, 3, 25890,
                                  7],
                                 [29856, 6, 169, 3, 32155, 6, 209, 3, 21194,
                                  4],
                                 [17906, 9, 163, 4, 22282, 9, 207, 4, 19788,
                                  7],
                                 [113, 1, 6, 1, 259, 1, 8, 1, 373,
                                  1],
                                 [14, 1, 1, 1, 29, 1, 1, 1, 60,
                                  1],
                                 [3, 1, 1, 1, 2, 1, 1, 1, 2,
                                  1]]),
           'predicted_tokens': np.array([[' Denmark', '\n', '5', '.', ' Estonia', '\n', '6', '.',
                                          ' Hungary', '\n'],
                                         [' Denmark', '\n', '5', '.', ' Estonia', '\n', '6', '.',
                                          ' Hungary', '\n'],
                                         [' Denmark', '\n', '5', '.', ' Estonia', '\n', '6', '.',
                                          ' Hungary', '\n'],
                                         [' Denmark', '\n', '5', '.', ' Estonia', '\n', '6', '.',
                                          ' Hungary', '\n'],
                                         [' Denmark', '\n', '5', '.', ' Estonia', '\n', '6', '.',
                                          ' Hungary', '\n'],
                                         [' Denmark', '\n', '5', '.', ' Estonia', '\n', '6', '.',
                                          ' Hungary', '\n']], dtype='<U25')}
@pytest.fixture
def ranking_watch_data_1():
    yield {'input_tokens': ['The', ' keys', ' to', ' the', ' cabinet', ';'],
           'output_tokens': ["' is'", "' are'"],
           'rankings': np.array([[19, 121],
                                 [15, 109],
                                 [16, 64],
                                 [14, 24],
                                 [9, 16],
                                 [12, 1]]),
           'position': 7}
@pytest.fixture(scope="session", autouse=True)
def tmp_dir():
    if not os.path.exists('tmp'):
        os.makedirs('tmp')

================
File: tests/lm_test.py
================
from ecco.lm import LM, _one_hot, sample_output_token, activations_dict_to_array
import ecco
import torch
import numpy as np
from transformers import PreTrainedModel 
class TestLM:
    def test_one_hot(self):
        expected = torch.tensor([[1., 0., 0.], [0., 1., 0.]])
        actual = _one_hot(torch.tensor([0, 1]), 3)
        assert torch.all(torch.eq(expected, actual))
    def test_select_output_token_argmax(self):
        result = sample_output_token(torch.tensor([0., 1.]), False, 0, 0, 0)
        assert result == torch.tensor(1)
    def test_select_output_token_sample(self):
        result = sample_output_token(torch.tensor([[0., 0.5, 1.]]), True, 1, 1, 1.0)
        assert result == torch.tensor(2)
    def test_activations_dict_to_array(self):
        batch, position, neurons = 1, 3, 4
        actual_dict = {0: np.zeros((batch, position, neurons)),
                       1: np.zeros((batch, position, neurons))}
        activations = activations_dict_to_array(actual_dict)
        assert activations.shape == (batch, 2, neurons, position)
    def test_init(self):
        lm = ecco.from_pretrained('sshleifer/tiny-gpt2', activations=True)
        assert isinstance(lm.model, PreTrainedModel), "Model downloaded and LM was initialized successfully."
    def test_generate(self):
        lm = ecco.from_pretrained('sshleifer/tiny-gpt2',
                                  activations=True,
                                  verbose=False)
        output = lm.generate('test', generate=1, attribution=['grad_x_input'])
        assert output.token_ids.shape == (1, 2), "Generated one token successfully"
        assert output.attribution['grad_x_input'][0] == 1, "Successfully got an attribution value"
        # Confirm activations is dimensions:
        # (batch 1, layer 2, h_dimension 8, position 1)
        assert output.activations['decoder'].shape == (1, 2, 8, 1)
    def test_call_dummy_bert(self):
        lm = ecco.from_pretrained('julien-c/bert-xsmall-dummy',
                                  activations=True,
                                  verbose=False)
        inputs = lm.to(lm.tokenizer(['test', 'hi'],
                                    padding=True,
                                    truncation=True,
                                    return_tensors="pt",
                                    max_length=512))
        output = lm(inputs)
        # Confirm it's (batch 2, layer 1, h_dimension 40, position 3)
        # position is 3 because of [CLS] and [SEP]
        # If we do require padding, this CUDA compains with this model for some reason.
        assert output.activations['encoder'].shape == (2, 1, 40, 3)
    # TODO: Test LM Generate with Activation. Tweak to support batch dimension.
    # def test_generate_token_no_attribution(self, mocker):
    #     pass
    #
    # def test_generate_token_with_attribution(self, mocker):
    #     pass

================
File: tests/output_test.py
================
from ecco import output
import pytest
import torch
import numpy as np
from ecco.output import NMF
import ecco
class TestOutput:
    def test_position_raises_value_error_more(self):
        output_seq = output.OutputSeq(tokens=[0, 0], n_input_tokens=1)
        with pytest.raises(ValueError):
            output_seq.position(position=4)
    def test_position_raises_value_error_less(self):
        output_seq = output.OutputSeq(tokens=[0, 0], n_input_tokens=1)
        with pytest.raises(ValueError):
            output_seq.position(position=0)
    def test_saliency(self, output_seq_1):
        actual = output_seq_1.primary_attributions(printJson=True)
        # print(actual) # This is how to get the expected value. Validated manually then pasted below
        expected = {'tokens': [{'token': '', 'token_id': 352, 'is_partial': True, 'type': 'input',
                                'value': '0.31678662', 'position': 0},
                               {'token': '', 'token_id': 11, 'is_partial': True, 'type': 'input', 'value': '0.18056837',
                                'position': 1},
                               {'token': '', 'token_id': 352, 'is_partial': True, 'type': 'input',
                                'value': '0.37555906',
                                'position': 2},
                               {'token': '', 'token_id': 11, 'is_partial': True, 'type': 'input', 'value': '0.12708597',
                                'position': 3},
                               {'token': '', 'token_id': 362, 'is_partial': True, 'type': 'output', 'value': '0',
                                'position': 4}], 'attributions': [
            [0.31678661704063416, 0.1805683672428131, 0.3755590617656708, 0.12708596885204315]]}
        assert actual == expected
    def test_layer_position_zero_raises_valueerror(self, output_seq_1):
        with pytest.raises(ValueError, match=r".* set to 0*") as ex:
            actual = output_seq_1.layer_predictions(position=0)
    def test_layer_predictions_all_layers(self, output_seq_1):
        actual = output_seq_1.layer_predictions(printJson=True, position=4)
        assert len(actual) == 6  # an array for each layer
        assert actual[0][0]['ranking'] == 1
        assert actual[0][0]['layer'] == 0
    def test_layer_predictions_one_layer(self, output_seq_1):
        actual = output_seq_1.layer_predictions(layer=2, printJson=True, position=4)
        assert len(actual) == 1  # an array for each layer
        assert actual[0][0]['ranking'] == 1
        assert actual[0][0]['layer'] == 2
    def test_layer_predictions_topk(self, output_seq_1):
        actual = output_seq_1.layer_predictions(layer=2, printJson=True, topk=15, position=4)
        assert len(actual) == 1  # an array for each layer
        assert len(actual[0]) == 15
    def test_rankings(self, output_seq_1):
        actual = output_seq_1.rankings(printJson=True)
        assert len(actual['output_tokens']) == 1
        assert actual['rankings'].shape == (6, 1)
        assert isinstance(int(actual['rankings'][0][0]), int)
    def test_rankings_watch(self, output_seq_1):
        actual = output_seq_1.rankings_watch(printJson=True, watch=[0, 0])
        assert len(actual['output_tokens']) == 2
        assert actual['rankings'].shape == (6, 2)
        assert isinstance(int(actual['rankings'][0][0]), int)
    def test_nmf_raises_activations_dimension_value_error(self):
        with pytest.raises(ValueError, match=r".* four dimensions.*") as ex:
            NMF({'layer_0': np.zeros(0)},
                n_components=2)
    def test_nmf_raises_value_error_same_layer(self):
        with pytest.raises(ValueError, match=r".* same value.*") as ex:
            NMF({'layer_0':np.zeros((1, 1, 1, 1))},
                n_components=2,
                from_layer=0,
                to_layer=0)
    def test_nmf_raises_value_error_layer_bounds(self):
        with pytest.raises(ValueError, match=r".* larger.*"):
            NMF({'layer_0':np.zeros((1, 1, 1, 1))},
                n_components=2,
                from_layer=1,
                to_layer=0)
    # NMF properly deals with collect_activations_layer_nums
    def test_nmf_reshape_activations_1(self):
        batch, layers, neurons, position = 1, 6, 128, 10
        activations = np.ones((batch, layers, neurons, position))
        merged_activations = NMF.reshape_activations(activations,
                                                     None, None, None)
        assert merged_activations.shape == (layers*neurons, batch*position)
    # NMF properly deals with collect_activations_layer_nums
    def test_nmf_reshape_activations_2(self):
        batch, layers, neurons, position = 2, 6, 128, 10
        activations = np.ones((batch, layers, neurons, position))
        merged_activations = NMF.reshape_activations(activations,
                                                     None, None, None)
        assert merged_activations.shape == (layers*neurons, batch*position)
    def test_nmf_explore_on_dummy_gpt(self):
        lm = ecco.from_pretrained('sshleifer/tiny-gpt2',
                                  activations=True,
                                  verbose=False)
        output = lm.generate('test', generate=1)
        nmf = output.run_nmf()
        exp = nmf.explore(printJson=True)
        assert len(exp['tokens']) == 2 # input & output tokens
        # 1 redundant dimension, 1 generation /factor, 2 tokens.
        assert np.array(exp['factors']).shape == (1, 1, 2)
    def test_nmf_explore_on_dummy_bert(self):
        lm = ecco.from_pretrained('julien-c/bert-xsmall-dummy',
                                  activations=True,
                                  verbose=False)
        inputs = lm.to(lm.tokenizer(['test', 'hi'],
                                    padding=True,
                                    truncation=True,
                                    return_tensors="pt",
                                    max_length=512))
        output = lm(inputs)
        nmf = output.run_nmf()
        exp = nmf.explore(printJson=True)
        assert len(exp['tokens']) == 3  # CLS UNK SEP
        # 1 redundant dimension,6 factors, 6 tokens (a batch of two examples, 3 tokens each)
        assert np.array(exp['factors']).shape == (1, 6, 6)
    def test_nmf_output_dims(self):
        pass
    # 4d activations to 2d activations: one batch
    # multiple batches
    # one batch collect_activations_layer_nums
@pytest.fixture
def output_seq_1():
    class MockTokenizer:
        def decode(self, i=None):
            return ''
        def convert_ids_to_tokens(self,i=None):
            return ['']
    output_1 = output.OutputSeq(**{'model_type': 'causal',
                                   'tokenizer': MockTokenizer(),
                                   'token_ids': [[352, 11, 352, 11, 362]],
                                   'n_input_tokens': 4,
                                   'output_text': ' 1, 1, 2',
                                   'tokens': [[' 1', ',', ' 1', ',', ' 2']],
                                   'decoder_hidden_states': [torch.rand(6, 1, 768)],
                                   'attention': None,
                                   'attribution': {'gradient': [
                                       np.array([0.41861308, 0.13054065, 0.23851791, 0.21232839], dtype=np.float32)],
                                       'grad_x_input': [
                                           np.array([0.31678662, 0.18056837, 0.37555906, 0.12708597],
                                                    dtype=np.float32)]},
                                   'activations': [{
                                       'decoder': {
                                           'layer_0': torch.rand(1, 768),
                                           'layer_1': torch.rand(1, 768),
                                           'layer_2': torch.rand(1, 768),
                                       }
                                   }],
                                   'lm_head': torch.nn.Linear(768, 50257, bias=False),
                                   'config': {
                                            'embedding': "embeddings.word_embeddings",
                                            'type': 'mlm',
                                            'activations': ['intermediate\.dense'], #This is a regex
                                            'token_prefix': '',
                                            'partial_token_prefix': '',
                                            'tokenizer_config': {
                                                'token_prefix': '',
                                                'partial_token_prefix': ''}
                                        },
                                   'device': 'cpu'})
    yield output_1

================
File: tests/test_ecco.py
================
from ecco.cli import main
def test_main():
    assert main([]) == 0

================
File: tests/tokenizers_test.py
================
from transformers import AutoTokenizer
from ecco import util
class TestTokenizers:
    def test_gpt_tokenizer(self):
        tokenizers = ['gpt2', 'bert-base-uncased']
        model_name = 'distilgpt2'
        config = util.load_config(model_name)
        tokenizer = AutoTokenizer.from_pretrained(tokenizers[0])
        token_ids = tokenizer(' tokenization')['input_ids']
        is_partial_1 = util.is_partial_token(config,
                                             tokenizer.convert_ids_to_tokens(token_ids[0]))
        is_partial_2 = util.is_partial_token(config,
                                             tokenizer.convert_ids_to_tokens(token_ids[1]))
        assert not is_partial_1
        assert is_partial_2
    def test_bert_tokenizer(self):
        model_name = 'bert-base-uncased'
        config = util.load_config(model_name)
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        token_ids = tokenizer(' tokenization')['input_ids']
        is_partial_1 = util.is_partial_token(config,
                                             tokenizer.convert_ids_to_tokens(token_ids[1])) # skip CLS
        is_partial_2 = util.is_partial_token(config,
                                             tokenizer.convert_ids_to_tokens(token_ids[2]))
        assert not is_partial_1
        assert is_partial_2
    def test_t5_tokenizer(self):
        model_name = 't5-small'
        config = util.load_config(model_name)
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        token_ids = tokenizer(' tokenization')['input_ids']
        is_partial_1 = util.is_partial_token(config,
                                             tokenizer.convert_ids_to_tokens(token_ids[0]))
        is_partial_2 = util.is_partial_token(config,
                                             tokenizer.convert_ids_to_tokens(token_ids[1]))
        assert not is_partial_1
        assert is_partial_2



================================================================
End of Codebase
================================================================
