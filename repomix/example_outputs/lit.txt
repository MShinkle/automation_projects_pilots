This file is a merged representation of a subset of the codebase, containing specifically included files and files not matching ignore patterns, combined into a single document by Repomix.
The content has been processed where empty lines have been removed.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
- Pay special attention to the Repository Description. These contain important context and guidelines specific to this project.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: **/*.py, **/*.md, **/*.txt
- Files matching these patterns are excluded: **/.git/**, **/.github/**, CHANGELOG.md
- Empty lines have been removed from all files
- Files are sorted by Git change count (files with more changes are at the bottom)

Additional Info:
----------------
User Provided Header:
-----------------------
This file is a consolidated single-file compilation of all code in the repository generated by Repomix. Note that .ipynb files have been converted to .py files.

================================================================
Directory Structure
================================================================
CONTRIBUTING.md
docs/documentation/_sources/api.md.txt
docs/documentation/_sources/components.md.txt
docs/documentation/_sources/demos.md.txt
docs/documentation/_sources/docker.md.txt
docs/documentation/_sources/faq.md.txt
docs/documentation/_sources/frontend_development.md.txt
docs/documentation/_sources/getting_started.md.txt
docs/documentation/_sources/glossary.md.txt
docs/documentation/_sources/index.md.txt
docs/documentation/_sources/ui_guide.md.txt
docs/documentation/_static/scripts/furo.js.LICENSE.txt
lit_nlp/__init__.py
lit_nlp/api/__init__.py
lit_nlp/api/components.py
lit_nlp/api/dataset_test.py
lit_nlp/api/dataset.py
lit_nlp/api/dtypes.py
lit_nlp/api/layout.py
lit_nlp/api/model_test.py
lit_nlp/api/model.py
lit_nlp/api/types_test.py
lit_nlp/api/types.py
lit_nlp/app.py
lit_nlp/components/__init__.py
lit_nlp/components/ablation_flip_int_test.py
lit_nlp/components/ablation_flip_test.py
lit_nlp/components/ablation_flip.py
lit_nlp/components/annotators.py
lit_nlp/components/backtranslator.py
lit_nlp/components/cf_utils_test.py
lit_nlp/components/cf_utils.py
lit_nlp/components/citrus/__init__.py
lit_nlp/components/citrus/helpers.py
lit_nlp/components/citrus/lime_test.py
lit_nlp/components/citrus/lime.py
lit_nlp/components/citrus/utils_test.py
lit_nlp/components/citrus/utils.py
lit_nlp/components/classification_results_test.py
lit_nlp/components/classification_results.py
lit_nlp/components/core.py
lit_nlp/components/curves_test.py
lit_nlp/components/curves.py
lit_nlp/components/gradient_maps_test.py
lit_nlp/components/gradient_maps.py
lit_nlp/components/hotflip_int_test.py
lit_nlp/components/hotflip_test.py
lit_nlp/components/hotflip.py
lit_nlp/components/image_gradient_maps_test.py
lit_nlp/components/image_gradient_maps.py
lit_nlp/components/index.py
lit_nlp/components/lime_explainer_test.py
lit_nlp/components/lime_explainer.py
lit_nlp/components/metrics_test.py
lit_nlp/components/metrics.py
lit_nlp/components/minimal_targeted_counterfactuals_test.py
lit_nlp/components/minimal_targeted_counterfactuals.py
lit_nlp/components/model_salience.py
lit_nlp/components/nearest_neighbors_test.py
lit_nlp/components/nearest_neighbors.py
lit_nlp/components/pca_test.py
lit_nlp/components/pca.py
lit_nlp/components/pdp_test.py
lit_nlp/components/pdp.py
lit_nlp/components/projection.py
lit_nlp/components/regression_results_test.py
lit_nlp/components/regression_results.py
lit_nlp/components/remote_model.py
lit_nlp/components/salience_clustering_test.py
lit_nlp/components/salience_clustering.py
lit_nlp/components/scrambler_test.py
lit_nlp/components/scrambler.py
lit_nlp/components/sequence_salience.py
lit_nlp/components/shap_explainer_test.py
lit_nlp/components/shap_explainer.py
lit_nlp/components/static_preds_test.py
lit_nlp/components/static_preds.py
lit_nlp/components/tcav_int_test.py
lit_nlp/components/tcav_test.py
lit_nlp/components/tcav.py
lit_nlp/components/thresholder_int_test.py
lit_nlp/components/thresholder.py
lit_nlp/components/umap_test.py
lit_nlp/components/umap.py
lit_nlp/components/word_replacer_test.py
lit_nlp/components/word_replacer.py
lit_nlp/dev_server.py
lit_nlp/examples/__init__.py
lit_nlp/examples/blank_slate_demo.py
lit_nlp/examples/custom_module/__init__.py
lit_nlp/examples/custom_module/potato_demo.py
lit_nlp/examples/gcp/constants.py
lit_nlp/examples/gcp/model_server_gunicorn_config.py
lit_nlp/examples/gcp/model_server_test.py
lit_nlp/examples/gcp/model_server.py
lit_nlp/examples/gcp/model.py
lit_nlp/examples/gcp/README.md
lit_nlp/examples/gcp/server_gunicorn_config.py
lit_nlp/examples/gcp/server.py
lit_nlp/examples/gcp/vertexai_models_test.py
lit_nlp/examples/gcp/vertexai_models.py
lit_nlp/examples/glue/data.py
lit_nlp/examples/glue/demo.py
lit_nlp/examples/glue/model_int_test.py
lit_nlp/examples/glue/model_utils_test.py
lit_nlp/examples/glue/model_utils.py
lit_nlp/examples/glue/models_test.py
lit_nlp/examples/glue/models.py
lit_nlp/examples/glue/README.md
lit_nlp/examples/glue/testdata/bert_tokenizer/vocab.txt
lit_nlp/examples/gunicorn_config.py
lit_nlp/examples/penguin/data.py
lit_nlp/examples/penguin/demo.py
lit_nlp/examples/penguin/model_int_test.py
lit_nlp/examples/penguin/model.py
lit_nlp/examples/penguin/README.md
lit_nlp/examples/prompt_debugging/constants.py
lit_nlp/examples/prompt_debugging/datasets.py
lit_nlp/examples/prompt_debugging/keras_lms.py
lit_nlp/examples/prompt_debugging/layouts.py
lit_nlp/examples/prompt_debugging/models.py
lit_nlp/examples/prompt_debugging/notebook.py
lit_nlp/examples/prompt_debugging/server.py
lit_nlp/examples/prompt_debugging/transformers_lms_int_test.py
lit_nlp/examples/prompt_debugging/transformers_lms.py
lit_nlp/examples/prompt_debugging/utils_test.py
lit_nlp/examples/prompt_debugging/utils.py
lit_nlp/examples/tools/__init__.py
lit_nlp/examples/tools/glue_trainer.py
lit_nlp/examples/tydi/data.py
lit_nlp/examples/tydi/demo.py
lit_nlp/examples/tydi/model.py
lit_nlp/examples/tydi/README.md
lit_nlp/examples/tydi/requirements.txt
lit_nlp/lib/__init__.py
lit_nlp/lib/caching_test.py
lit_nlp/lib/caching.py
lit_nlp/lib/file_cache_test.py
lit_nlp/lib/file_cache.py
lit_nlp/lib/flag_helpers.py
lit_nlp/lib/image_utils_test.py
lit_nlp/lib/image_utils.py
lit_nlp/lib/serialize_test.py
lit_nlp/lib/serialize.py
lit_nlp/lib/testing_utils.py
lit_nlp/lib/ui_state.py
lit_nlp/lib/utils_test.py
lit_nlp/lib/utils.py
lit_nlp/lib/validation_test.py
lit_nlp/lib/validation.py
lit_nlp/lib/wsgi_app.py
lit_nlp/lib/wsgi_serving.py
lit_nlp/notebook.py
lit_nlp/server_config.py
lit_nlp/server_flags_test.py
lit_nlp/server_flags.py
README.md
RELEASE.md
requirements_examples_common.txt
requirements_examples_discriminative_ai.txt
requirements_examples_generative_ai.txt
requirements_test.txt
requirements.txt
website/README.md
website/sphinx_src/api.md
website/sphinx_src/components.md
website/sphinx_src/conf.py
website/sphinx_src/demos.md
website/sphinx_src/docker.md
website/sphinx_src/faq.md
website/sphinx_src/frontend_development.md
website/sphinx_src/getting_started.md
website/sphinx_src/glossary.md
website/sphinx_src/index.md
website/sphinx_src/ui_guide.md
website/src/demos.md
website/src/demos/glue.md
website/src/demos/penguins.md
website/src/index.md
website/src/tutorials.md
website/src/tutorials/sentiment.md
website/src/tutorials/sequence-salience.md
website/src/tutorials/tab-feat-attr.md
website/src/tutorials/tcav.md
website/src/tutorials/text-salience.md
website/src/tutorials/tour.md

================================================================
Files
================================================================

================
File: CONTRIBUTING.md
================
# How to Contribute

We'd love to accept your patches and contributions to this project. There are
just a few small guidelines you need to follow.

## Contributor License Agreement

Contributions to this project must be accompanied by a Contributor License
Agreement. You (or your employer) retain the copyright to your contribution;
this simply gives us permission to use and redistribute your contributions as
part of the project. Head over to <https://cla.developers.google.com/> to see
your current agreements on file or to sign a new one.

You generally only need to submit a CLA once, so if you've already submitted one
(even if it was for a different project), you probably don't need to do it
again.

## Code reviews

All submissions, including submissions by project members, require review. We
use GitHub pull requests for this purpose. Consult
[GitHub Help](https://help.github.com/articles/about-pull-requests/) for more
information on using pull requests.

## Community Guidelines

This project follows [Google's Open Source Community
Guidelines](https://opensource.google/conduct/).

================
File: docs/documentation/_sources/api.md.txt
================
# LIT Python API

<!--* freshness: { owner: 'lit-dev' reviewed: '2024-06-24' } *-->

<!-- [TOC] placeholder - DO NOT REMOVE -->

## Design Overview

LIT is a modular system, comprising a collection of backend components (written
in Python) and frontend modules (written in TypeScript). Most users will develop
against the Python API, which is documented below and allows LIT to be extended
with custom models, datasets, metrics, counterfactual generators, and more. The
LIT server and components are provided as a library which users can use through
their own demo binaries or via Colab.

The components can also be used as regular Python classes without starting a
server; see [below](#using-lit-components-outside-of-lit) for details.

![LIT system overview](./images/lit-system-diagram.svg)

The LIT backend serves models, data, and interpretability components, each of
which is a Python class implementing a minimal API and relying on the
[spec system](#type-system) to detect fields and verify compatibility. The
server is stateless, but implements a caching layer for model predictions - this
simplifies component design and allows interactive use of large models like BERT
or T5.

The frontend is a stateful single-page app, built using
[Lit](https://lit.dev/)[^1] for modularity and [MobX](https://mobx.js.org/) for
state management. It consists of a core UI framework, a set of shared "services"
which manage persistent state, and a set of independent modules which render
visualizations and support user interaction. For more details, see the
[UI guide](./ui_guide.md) and the
[frontend developer guide](./frontend_development.md).

[^1]: Naming is just a happy coincidence; the Learning Interpretability Tool is
      not related to the Lit projects.


## Adding Models and Data

To run LIT with your own models and data, you can create a custom `demo.py`
script that passes these to the LIT server. For example:

```py
def main(_):
  # MulitiNLIData implements the Dataset API
  datasets = {
      'mnli_matched': MultiNLIData('/path/to/dev_matched.tsv'),
      'mnli_mismatched': MultiNLIData('/path/to/dev_mismatched.tsv'),
  }

  # NLIModel implements the Model API
  models = {
      'model_foo': NLIModel('/path/to/model/foo/files'),
      'model_bar': NLIModel('/path/to/model/bar/files'),
  }

  lit_demo = lit_nlp.dev_server.Server(models, datasets, port=4321)
  lit_demo.serve()

if __name__ == '__main__':
  main()
```

Conceptually, a dataset is just a list of examples and a model is just a
function that takes examples and returns predictions. The [`Dataset`](#datasets)
and [`Model`](#models) classes implement this, and provide metadata (see the
[type system](#type-system)) to describe themselves to other components.

For pre-built `demo.py` examples, check out
https://github.com/PAIR-code/lit/tree/main/lit_nlp/examples

### Validating Models and Data

Datasets and models can optionally be validated by LIT to ensure that dataset
examples match their spec and that model output values match their spec.
This can be very helpful during development of new model and dataset wrappers
to ensure correct behavior in LIT.

At LIT server startup, the `validate` flag can be used to enable validation.
There are three modes:

*   `--validate=first` will check the first example in each dataset.
*   `--validate=sample` will validate a sample of 5% of each dataset.
*   `--validate=all` will run validation on all examples from all datasets.

Additionally, if using LIT datasets and models outside of the LIT server,
validation can be called directly through the
[`validation`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/lib/validation.py) module.

## Datasets

Datasets ([`Dataset`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/api/dataset.py)) are
just a list of examples, with associated type information following LIT's
[type system](#type-system).

*   `spec()` should return a flat dict that describes the fields in each example
*   `self._examples` should be a list of flat dicts

LIT operates on all examples loaded in the datasets you include in your LIT
server, therefore you should take care to use dataset sizes that can fit into
memory on your backend server and can be displayed in the browser.

NOTE: See the [FAQ](./faq.md) for more details on dataset size limitations.

Implementations should subclass
[`Dataset`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/api/dataset.py). Usually this
is just a few lines of code - for example, the following is a complete
implementation for the [MultiNLI](https://cims.nyu.edu/~sbowman/multinli/)
dataset:

```py
class MultiNLIData(Dataset):
  """Loader for MultiNLI development set."""

  NLI_LABELS = ['entailment', 'neutral', 'contradiction']

  def __init__(self, path: str):
    # Read the eval set from a .tsv file as distributed with the GLUE benchmark.
    df = pandas.read_csv(path, sep='\t')
    # Store as a list of dicts, conforming to self.spec()
    self._examples = [{
      'premise': row['sentence1'],
      'hypothesis': row['sentence2'],
      'label': row['gold_label'],
      'genre': row['genre'],
    } for _, row in df.iterrows()]

  def spec(self) -> types.Spec:
    return {
      'premise': lit_types.TextSegment(),
      'hypothesis': lit_types.TextSegment(),
      'label': lit_types.CategoryLabel(vocab=self.NLI_LABELS),
      # We can include additional fields, which don't have to be used by the model.
      'genre': lit_types.CategoryLabel(),
    }
```

In this example, all four fields (premise, hypothesis, label, and genre) have
string values, but the [semantic types](#type-system) tell LIT a bit more about
how to interpret them:

*   `premise` and `hypothesis` should be treated as natural-language text
    (`TextSegment`)
*   `label` should be treated as a categorical feature (`CategoryLabel`) with a
    fixed, known set of possible values (`vocab=self.NLI_LABELS`)
*   `genre` should be treated as a categorical feature, but with an unknown or
    open set of values.

This implementation uses Pandas to read a TSV file, but you can also use
services like [TensorFlow Datasets](https://www.tensorflow.org/datasets) -
simply wrap them in your `__init__()` function.

Note that you can freely add additional features - such as `genre` in the
example above - which the model may not be aware of. The LIT UI will recognize
these features for slicing, binning, etc., and they will also be available to
interpretation components such as custom metrics.

### Transformations

The `Dataset` class also supports a limited set of transformations, similar to
TensorFlow's
[tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)
but more limited in scope and aimed at supporting quick iteration:

*   `Dataset.slice[start:step:end]` will return a new `Dataset` with the same
    spec and a slice of the datapoints.
*   `Dataset.sample(n, seed=42)` will return a new `Dataset` with the same spec
    and a random sample of the datapoints.
*   `Dataset.remap(field_map: dict[str, str])` will return a new `Dataset` with
    renamed fields in both the examples and spec.

The latter is a shortcut to use datasets matching one model with another; for
example, a dataset with a `"document"` field can be used with a model expecting
a `"text"` input via `Dataset.remap({"document":
"text"})`.[^why-not-standardize-names]

[^why-not-standardize-names]: We could solve this particular case by
    standardizing names, but one still needs to be
    explicit if there are multiple segments available,
    such as `"question"` and `"document"` for a QA
    task.

## Models

Models ([`Model`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/api/model.py)) are
functions which take inputs and produce outputs, with associated type
information following LIT's [type system](#type-system). The core API consists
of three methods:

*   `input_spec()` should return a flat dict that describes necessary input
    fields
*   `output_spec()` should return a flat dict that describes the model's
    predictions and any additional outputs
*   `predict()` should take a sequence of inputs (satisfying `input_spec()`) and
    yields a parallel sequence of outputs matching `output_spec()`.

Implementations should subclass
[`Model`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/api/model.py). An example for
[MultiNLI](https://cims.nyu.edu/~sbowman/multinli/) might look something like:

```py
class NLIModel(Model):
  """Wrapper for a Natural Language Inference model."""

  NLI_LABELS = ['entailment', 'neutral', 'contradiction']

  def __init__(self, model_path: str, **kw):
    # Load the model into memory so we're ready for interactive use.
    self._model = _load_my_model(model_path, **kw)

  ##
  # LIT API implementations
  def predict(self, inputs: Iterable[Input]) -> Iterable[Preds]:
    """Predict on a stream of examples."""
    examples = [self._model.convert_dict_input(d) for d in inputs]  # any custom preprocessing
    return self._model.predict_examples(examples)  # returns a dict for each input

  def input_spec(self) -> types.Spec:
    """Describe the inputs to the model."""
    return {
        'premise': lit_types.TextSegment(),
        'hypothesis': lit_types.TextSegment(),
    }

  def output_spec(self) -> types.Spec:
    """Describe the model outputs."""
    return {
      # The 'parent' keyword tells LIT where to look for gold labels when computing metrics.
      'probas': lit_types.MulticlassPreds(vocab=NLI_LABELS, parent='label'),
    }
```

Unlike the dataset example, this model implementation is incomplete - you'll
need to customize `predict()` accordingly with any pre- or post-processing
needed, such as tokenization.

Many deep learning models support a batched prediction behavior. Thus, we
provide the `BatchedModel` class that implements simple batching. Users of this
class must implement the `predict_minibatch()` function, which should convert
a `Sequence` of `JsonDict` objects to the appropriate batch representation
(typically, a `Mapping` of strings to aligned `Sequences` or `Tensors`) before
calling the model. Optionally, you may want to override the
`max_minibatch_size()` function, which determines the batch size.

Note: there are a few additional methods in the model API - see
[`Model`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/api/model.py) for details.

If your model is on a remote server, consider using the `BatchedRemoteModel`
base class, which implements parallel batched requests using a thread pool.

### Adding more outputs

The above example defined a black-box model, with predictions but no access to
internals. If we want a richer view into the model's behavior, we can add
additional return fields corresponding to hidden-state activations, gradients,
word embeddings, attention, or more. For example, a BERT-based model with
several such features might have the following `output_spec()`:

```py
  def output_spec(self) -> types.Spec:
    """Describe the model outputs."""
    return {
      # The 'parent' keyword tells LIT where to look for gold labels when computing metrics.
      'probas': lit_types.MulticlassPreds(vocab=NLI_LABELS, parent='label'),
      # This model returns two different embeddings (activation vectors), but you can easily add more.
      'output_embs': lit_types.Embeddings(),      # from [CLS] token at top layer
      'mean_word_embs':  lit_types.Embeddings(),  # mean of input word embeddings
      # In LIT, we treat tokens as another model output. There can be more than one,
      # and the 'parent' field describes which input segment they correspond to.
      'premise_tokens': lit_types.Tokens(parent='premise'),
      'hypothesis_tokens': lit_types.Tokens(parent='hypothesis'),
      # Gradients are also returned by the model; 'align' here references a Tokens field.
      'premise_grad': lit_types.TokenGradients(align='premise_tokens'),
      'hypothesis_grad': lit_types.TokenGradients(align='hypothesis_tokens'),
      # Similarly, attention references a token field, but here we want the model's full "internal"
      # tokenization, which might be something like: [START] foo bar baz [SEP] spam eggs [END]
      'tokens': lit_types.Tokens(),
      'attention_layer0': lit_types.AttentionHeads(align=['tokens', 'tokens']),
      'attention_layer1': lit_types.AttentionHeads(align=['tokens', 'tokens']),
      'attention_layer2': lit_types.AttentionHeads(align=['tokens', 'tokens']),
      # ...and so on. Since the spec is just a dictionary of dataclasses, you can populate it
      # in a loop if you have many similar fields.
    }
```

The `predict()` function would return, for each example, additional dict entries
corresponding to each of these fields.

Note: Because tokenization is often tightly coupled with the model code, we
treat it as an intermediate state on the same level as embeddings or attention,
and thus return `Tokens` as a field in the model *output*. This also allows
models to expose different tokenizations for different inputs, such as
`premise_tokens` and `hypothesis_tokens` above.

LIT components and frontend modules will automatically detect these spec fields
and use them to support additional interpretation methods, such as the embedding
projector or gradient-based salience maps.

You can also implement multi-headed models this way: simply add additional
output fields for each prediction (such as another `MulticlassPreds`), and
they'll be automatically detected.

See the [type system documentation](#type-system) for more details on available
types and their semantics.

### Optional inputs

By default, LIT treats `input_spec` fields as required. However, this can be set
to false if you wish to define optional model inputs. For example, a model that
can accept pre-tokenized inputs might have the following spec:

```python
    def input_spec(self) -> types.Spec:
      return {
          "text": lit_types.TextSegment(),
          "tokens": lit_types.Tokens(parent='text', required=False),
      }
```

And in the model's `predict()`, you would have logic to use these and bypass the
tokenizer:

```python
    def predict(self, inputs: Iterable[Input]) -> Iterable[Preds]:
      input_tokens = [ex.get('tokens') or self.tokenizer.tokenize(ex['text'])
                      for ex in inputs]
      # ...rest of your predict logic...
```

`required=False` can also be used for label fields (such as `"label":
lit_types.CategoryLabel(required=False)`), though these can also be omitted from
the input spec entirely if they are not needed to compute model outputs.

## Interpretation Components

Backend interpretation components include metrics, salience maps, visualization
aids like [UMAP](https://umap-learn.readthedocs.io/en/latest/), and
counterfactual generator plug-ins.

Most such components implement the
[`Interpreter`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/api/components.py) API.
Conceptually, this is any function that takes a set of datapoints and a model,
and produces some output.[^identity-component] For example,
[local gradient-based salience (GradientNorm)](https://github.com/PAIR-code/lit/blob/main/lit_nlp/components/gradient_maps.py)
processes the `TokenGradients` and `Tokens` returned by a model and produces a
list of scores for each token. The Integrated Gradients saliency method
additionally requires a `TokenEmbeddings` input and corresponding output, as
well as a label field `Target` to pin the gradient target to the same class as
an input and corresponding output. See the
[GLUE models class](https://github.com/PAIR-code/lit/blob/main/lit_nlp/examples/glue/models.py)
for an example of these spec requirements.

The core API involves implementing the `run()` method:

```python
  def run(self,
          inputs: list[JsonDict],
          model: lit_model.Model,
          dataset: lit_dataset.Dataset,
          model_outputs: Optional[list[JsonDict]] = None,
          config: Optional[JsonDict] = None):
    # config is any runtime options to this component, such as a threshold for
    # (binary) classification metrics.
```

Output from an interpreter component is unconstrained; it's up to the frontend
component requesting it to process the output correctly. In particular, some
components (such as salience maps) may operate on each example independently,
similar to model predictions, while others (such as metrics) may produce
aggregate summaries of the input set.

Interpreters are also responsible for verifying compatibility by reading the
model and dataset specs; these are also used to determine what fields to operate
on. A typical implementation just loops over the relevant specs. For example,
for
[simple gradient-based salience](https://github.com/PAIR-code/lit/blob/main/lit_nlp/components/gradient_maps.py)
we might have:

```python
  def find_fields(self, output_spec: Spec) -> list[str]:
    # Find TokenGradients fields
    grad_fields = utils.find_spec_keys(output_spec, types.TokenGradients)

    # Check that these are aligned to Tokens fields
    for f in grad_fields:
      tokens_field = output_spec[f].align  # pytype: disable=attribute-error
      assert tokens_field in output_spec
      assert isinstance(output_spec[tokens_field], types.Tokens)
    return grad_fields

  def run(self,
          inputs: list[JsonDict],
          model: lit_model.Model,
          dataset: lit_dataset.Dataset,
          model_outputs: Optional[list[JsonDict]] = None,
          config: Optional[JsonDict] = None) -> Optional[list[JsonDict]]:
    """Run this component, given a model and input(s)."""
    # Find gradient fields to interpret
    output_spec = model.output_spec()
    grad_fields = self.find_fields(output_spec)
    logging.info('Found fields for gradient attribution: %s', str(grad_fields))
    if len(grad_fields) == 0:  # pylint: disable=g-explicit-length-test
      return None

    # do rest of the work to create the salience maps for each available field

    # return a dtypes.TokenSalience for each input, which has a list of
    # tokens (from the model) and their associated scores.
```

This design adds some code overhead to interpretation components, but the
benefit is flexibility - Python can be used to specify complex dependencies
between fields, and multiple outputs can be easily supported in a loop.

[^identity-component]: A trivial one might just run the model and return
    predictions, though in practice we have a separate
    endpoint for that.

### Metrics

For metrics, the
[`SimpleMetrics`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/components/metrics.py)
class implements the spec-matching and input-unpacking logic to satisfy the
general `Interpreter` API. A subclass of `SimpleMetrics` should implement an
`is_compatible()` method and a `compute()` method, which is called on compatible
(prediction, label) pairs and returns a dict of named score fields. For example:

```python
class RegressionMetrics(SimpleMetrics):
  """Standard regression metrics."""

  def is_compatible(self, field_spec: types.LitType) -> bool:
    """Return true if compatible with this field."""
    return isinstance(field_spec, types.RegressionScore)

  def compute(self,
              labels: Sequence[float],
              preds: Sequence[float],
              label_spec: types.Scalar,
              pred_spec: types.RegressionScore,
              config: Optional[JsonDict] = None) -> dict[str, float]:
    """Compute metric(s) between labels and predictions."""
    del config
    mse = sklearn_metrics.mean_squared_error(labels, preds)
    pearsonr = scipy_stats.pearsonr(labels, preds)[0]
    spearmanr = scipy_stats.spearmanr(labels, preds)[0]
    return {'mse': mse, 'pearsonr': pearsonr, 'spearmanr': spearmanr}
```

The implementation of `SimpleMetrics.run()` uses the `parent` key (see
[type system](#type-system)) in fields of the model's output spec to find the
appropriate input fields to compare against, and calls `compute()` accordingly
on the unpacked values.

### Generators

Conceptually, a generator is just an interpreter that returns new input
examples. These may depend on the input only, as for techniques such as back-
translation, or can involve feedback from the model, such as for adversarial
attacks.

The core generator API is:

```python
class Generator(Interpreter):
  """Base class for LIT generators."""

  def generate_all(self,
                   inputs: list[JsonDict],
                   model: lit_model.Model,
                   dataset: lit_dataset.Dataset,
                   config: Optional[JsonDict] = None) -> list[list[JsonDict]]:
    """Run generation on a set of inputs.

    Args:
      inputs: sequence of inputs, following model.input_spec()
      model: optional model to use to generate new examples.
      dataset: optional dataset which the current examples belong to.
      config: optional runtime config.

    Returns:
      list of list of new generated inputs, following model.input_spec()
    """
```

Where the output is a list of lists: a set of generated examples for each input.
For convenience, there is also a `generate()` method which takes a single
example and returns a single list; we provide the more general `generate_all()`
API to support model-based generators (such as back-translation) which benefit
from batched requests.

As with other interpreter components, a generator can take custom arguments
through `config`, such as the list of substitutions for the
[word replacer](https://github.com/PAIR-code/lit/blob/main/lit_nlp/components/word_replacer.py).

#### Backtranslator Generator

The [backtranslator](https://github.com/PAIR-code/lit/blob/main/lit_nlp/components/backtranslator.py)
generator translates text segment inputs into foreign languages and back to the
source language in order to create paraphrases.
It relies on the Google Cloud Translate API to perform those translations.
To use it, you must have a Google Cloud project and set up Cloud Translation
as described at https://cloud.google.com/translate/docs/setup.
Then, download  your application credentials file locally and set the
GOOGLE_APPLICATION_CREDENTIALS environment variable to point to that file.
With that environment variable set to the correct path, LIT can make use of the
backtranlator generator if you pass it as a generator in the Server constructor.

### Configuration UI

Interpreter components support an optional `config` option to specify run-time
options, such as the number of samples for LIME or the pivot languages for
back-translation. LIT provides a simple DSL to define these options, which will
auto-generate a form on the frontend. The DSL uses the same
[type system](#type-system) as used to define data and model outputs, and the
`config` argument will be passed a dict with the form values.

For example, the following spec:

```python
  def config_spec(self) -> types.Spec:
    return {
        "Pivot languages": types.SparseMultilabel(
            vocab=['ar', 'bg', 'de', 'el', 'en', 'es', 'fr', 'hi', 'ru', 'sw',
                   'th', 'tr', 'ur', 'vi', 'zh'],
            default=['de', 'fr']),
        "Source language": types.TextSegment(default='en'),
    }
```

will give this form to configure back-translation:

![Back-translation Config Form](./images/api/backtranslation-form-example.png){w=400px align=center}

Currently `config_spec()` is supported only for generators and salience methods,
though any component can support the `config` argument to its `run()` method,
which can be useful if
[running outside of the LIT UI](#using-lit-components-outside-of-lit).

The following [types](#available-types) are supported (see
[interpreter_controls.ts](https://github.com/PAIR-code/lit/blob/main/lit_nlp/client/elements/interpreter_controls.ts)):

*   `Scalar`, which creates a slider for setting a numeric option. You can
    specify the `min_val`, `max_val`, `default`, and `step`, values for the
    slider through arguments to the `Scalar` constructor.
*   `Boolean` (`BooleanLitType` in TypeScript), which creates a checkbox, with
    a `default` value to be set in the constructor.
*   `CategoryLabel`, which creates a dropdown with options specified in the
    `vocab` argument.
*   `SparseMultilabel`, which creates a series of checkboxes for each option
    specified in the `vocab` argument.
*   `TextSegment`, which creates an input text box for string entry, with an
    optional default value from the `default` argument.
*   `Tokens`, which creates an input text box for entry of multiple,
    comma-separated strings which are parsed into a list of strings to be
    supplied to the interpreter.
*   `SingleFieldMatcher`, which acts like a `CategoryLabel` but where the vocab
    is automatically populated by the names of fields from the data or model
    spec. For example, `SingleFieldMatcher(spec='dataset',
    types=['TextSegment'])` will give a dropdown with the names of all
    `TextSegment` fields in the dataset.
*   `MultiFieldMatcher` is similar to `SingleFieldMatcher` except it gives a set
    of checkboxes to select one or more matching field names. The returned value
    in `config` will be a list of string values.

The field matching controls can be useful for selecting one or more fields to
operate on. For example,to choose which input fields to perturb, or which output
field of a multi-head model to run an adversarial attack (such as HotFlip)
against.

## Type System

LIT passes data around (e.g., between the server and the web app) as flat
records with `string` keys. In Python types these are `Mapping[str, ...]` and in
TypeScript types these are `{[key: string]: unknown}`. LIT serializes these
records to JSON when communicating between the server and the web app client. It
is because of this serialization that we introduced LIT's type system; LIT needs
a way to communicate how to process and understand the semantics of the _shape_
and (allowable) _values_ for the records being passed around in
[JSON's more limited type system][json].

<!--
  TODO(b/290782213): Update the serialization discussion above to reflect any
  changes to LIT's wrire format for HTTP APIs.
-->

<!--
  TODO(b/258531316): Update Spec type once converted to a readonly type
-->
The _shape_ of a record &ndash; its specific keys and the types of their values
&ndash; is defined by a `Spec`; a `dict[str, LitType]`. Each `LitType` class has
a `default` property whose type annotation describes the type of the _value_ for
that field in a JSON record. `LitType`s are implemented using hierarchical
inheritance; the canonical types can be found in [types.py][types_py], with
parallel implementations in [lit_types.ts][types_ts].

### Conventions

LIT supports several different "kinds" of `Spec`s (input vs output vs meta,
etc.), and their use in context has specific implications, described
per base class below.

* [`lit_nlp.api.dataset.Dataset`][dataset-py]
    * **`.spec() -> Spec`** describes the shape of every record in the
      `Sequence` returned by `Dataset.examples()`.
    * **`.init_spec() -> Optional[Spec]`** describes the user-configurable
      arguments for loading a new instance of this `Dataset` class via the web
      app's UI. Returning `None` or an empty `Spec` means that there is nothing
      configurable about how this `Dataset` is loaded, and it will not show up
      in the dataset loading section of the web app's Global Settings.
* [`lit_nlp.api.model.Model`][model-py]
    * **`.input_spec() -> Spec`** describes the shape required of all records
      passed into the `Model.predict()` function via the `inputs` argument. LIT
      checks for compatibility between a `Dataset` and a `Model` by ensuring
      that `Model.input_spec()` is a subset of `Dataset.spec()`.
    * **`.output_spec() -> Spec`** describes the shape of all records returned
      by the `Model.predict()` function.
    * **`.init_spec() -> Optional[Spec]`** describes the user-configurable
      arguments for loading a new instance of this `Model` class via the web
      app's UI. Returning `None` or an empty `Spec` means that there is nothing
      configurable about how this `model` is loaded, and it will not show up in
      the model loading section of the web app's Global Settings.
* [`lit_nlp.api.components.[Interpreter | Generator]`][components-py]
    * **`.config_spec() -> Spec`** describes the user-configurable parameters
      for running this component. Returning an empty `Spec` means that this
      component always processes inputs in the same way.
    * **`.meta_spec() -> Spec`** is essentially unconstrained, but ideally
      describes the shape of the records returned by this component's `.run()`
      method. Note that this `Spec` has different semantics depending on the
      component type. `Interpreter.run()` typically returns an
      `Iterable[Mapping[str, ...]]` of records (i.e., the `Mapping`) with this
      shape, because each input corresponds to one interpretation. Whereas
      `Generator.run()` typically returns an
      `Iterable[Iterable[Mapping[str, ...]]]` of records with this shape,
      because each input may enable the generation of one or more new examples.
* [`lit_nlp.api.components.Metrics`][components-py]
    * **`.config_spec() -> Spec`** describes the user-configurable parameters
      for running this component. Returning an empty `Spec` means that this
      component always processes inputs in the same way.
    * **`.meta_spec() -> Spec`** is a slight variation on the tradition `Spec`;
      it will always be a `Mapping[str, MetricResult]` describing the single
      record returned by the `Metrics.run()` method for each pair of compatible
      keys in the `Model.output_spec()` and `Dataset.spec()`. The `MetricResult`
      type also describes how to interpret the values in each record, e.g., if
      higher, lower, or numbers closer to zero are better.

Each `LitType` subclass encapsulates its own semantics (see
[types.py][types_py]), but there are a few conventions all subclasses follow:

*   The **`align=` attribute** references another field _in the same spec_ and
    implies that both fields have index-aligned elements. For
    example, `Model.output_spec()` may contain `'tokens': lit_types.Tokens(...)`
    and `'pos': lit_types.SequenceTags(align='tokens')`, which references the
    "tokens" field. This implies that the "pos" field contains a corresponding
    value for every item in "tokens" and that you can access them with numeric
    indices. Transitively, this means that using `zip(..., strict=True)` (in
    Python 3.10 and above) will act as a pseudo-validator of this expectation.

*   The **`parent=` attribute** is _typically_ used by `LitType`s in a
    `Model.output_spec()`, and must be a field in the _input spec_ (i.e. the
    `Dataset.spec()`) against which this field's value will be compared. For
    example, the `Model.output_spec()` may contain
    `'probas': lit_types.MulticlassPreds(parent='label', ...)` and the
    `Dataset.spec()` may contain `'label': lit_types.CategoryLabel()`, which
    means that the `Dataset`'s "label" field contains the ground truth values
    for that example, and the class prediction in the "probas" field can be
    compared to this label, e.g., by multi-class metrics.

*   The **`vocab=` attribute** is used to represent the allowable values for
    that field, such as a set of classes for a `MulticlassPreds` field, or the
    set of labels for a `CategoryLabel` field.

*   A field that appears in _both_ the model's input and output specs is assumed
    to represent the same value. This pattern is used for model-based input
    manipulation. For example, a
    [language model](https://github.com/PAIR-code/lit/blob/main/lit_nlp/examples/glue/models.py)
    might output `'tokens': lit_types.Tokens(...)`, and accept as (optional)
    input `'tokens': lit_types.Tokens(required=False, ...)`. An interpretability
    component could take output from the former, swap one or more tokens (e.g.
    with `[MASK]`), and feed them in the corresponding input field to compute
    masked fills.

### Compatibility Checks

LIT's type system plays a critical role in ensuring reliability of and
interoperability between the `Model`, `Dataset`, `Interpreter`, `Generator`, and
`Metrics` classes:

*   The **Model-Dataset compatibility check** ensures that the
    `Model.input_spec()` is a subset of the `Dataset.spec()`. The base
    [`Model` class][model-py] provides a robust and universal implementation of
    this check in the `is_compatible_with_dataset()` API, but you can override
    this method in your `Model` subclass if you so choose.
*   All [`lit_nlp.api.components` classes][components-py] provide an
    `is_compatible` API to check their compatibility against `Model`s and
    `Dataset`s, as appropriate. For example, the
    [`WordReplacer` generator][word-replacer] only checks against the `Dataset`
    spec because it does not depend on model outputs, whereas the
    [`Curves` interpreter][curves-interp] checks the `Model` and `Dataset`
    because it needs labeled predictions, and the
    [`GradientDotInput` interpreter][grad-maps] only checks against the
    `Model.output_spec()` because it needs data that only the model can provide.

The LIT web app also uses `Spec` based compatibility checks. Each TypeScript
module defines a [`shouldDisplayModule` function][should_display_module] that
returns `true` if any active model-dataset pair provides sufficient information
to support the visualization methods encapsulated by that module. If this
function returns `false`, the module is not displayed in the layout. Note that
this can cause jitter (UI modules appearing, disappearing, reordering, resizing,
etc.) when switching between models or datasets with heterogeneous `Spec`s.

When implementing your own LIT components and modules, you can use
[`utils.find_spec_keys()`][utils-lib-py] (Python) and
[`findSpecKeys()`][utils-lib] (TypeScript) to identify fields of interest in a
`Spec`. These methods recognize and respect subclasses. For example,
`utils.find_spec_keys(spec, Scalar)` will also match any `RegressionScore`
fields, but `utils.find_spec_keys(spec, RegressionScore)` will not return all
`Scalar` fields in the `Spec`.

Important: Compatibility checks are performed automatically when
[building the `LitMetadata`][build-metadata] for an instance of `LitApp`,
typically by calling `dev_server.Serve()`. **These checks are not performed when
using components in a raw Python context** (e.g., Colab, Jupyter, a REPL), as
[described below](#using-lit-components-outside-of-lit), and it is encouraged
that you call these explicitly to ensure compatibility and avoid chasing red
herrings.

### An In-Depth Example

Consider the following example from the [MNLI demo][mnli-demo]. The
[MultiNLI][mnli-dataset] dataset might define the following `Spec`.

```python
# Dataset.spec()
{
  "premise": lit_types.TextSegment(),
  "hypothesis": lit_types.TextSegment(),
  "label": lit_types.CategoryLabel(
      vocab=["entailment", "neutral", "contradiction"]
  ),
  "genre": lit_types.CategoryLabel(),
}
```

An example record in this `Dataset` might be:

```python
# dataset.examples[0]
{
  "premise": "Buffet and a la carte available.",
  "hypothesis": "It has a buffet."
  "label": "entailment",
  "genre": "travel",
}
```

A classification model for this task might have the following `input_spec()` and
`output_spec()`. Notice that the input spec is a subset of the `Dataset.spec()`,
thus LIT considers these to be compatible.

```python
# model.input_spec()
{
  "premise": lit_types.TextSegment(),
  "hypothesis": lit_types.TextSegment(),
}

# model.output_spec()
{
  "probas": lit_types.MulticlassPreds(
      parent="label",
      vocab=["entailment", "neutral", "contradiction"]
  ),
}
```

Running this model over the input might yield the following prediction.

```python
# model.predict([dataset.examples[0]])[0]
{
  "probas": [0.967, 0.024, 0.009],
}
```

Passing this input and the prediction to the `ClassificationResults` interpreter
would yield additional human-readable information as follows.

```python
# classification_results.run(
#     dataset.examples[:1], model, dataset, [prediction]
# )[0]
{
  "probas": {
      "scores": [0.967, 0.024, 0.009],
      "predicted_class": "entailment",
      "correct": True,
  },
}
```

_See the [examples](https://github.com/PAIR-code/lit/blob/main/lit_nlp/examples) for more._

### Available types

The full set of `LitType`s is defined in
[types.py](https://github.com/PAIR-code/lit/blob/main/lit_nlp/api/types.py). Numeric types
such as `Integer` and `Scalar` have predefined ranges that can be overridden
using corresponding `min_val` and `max_val` attributes as seen in
[penguin data](https://github.com/PAIR-code/lit/blob/main/lit_nlp/examples/penguin/data.py)
`INPUT_SPEC`. The different types available in LIT are summarized in the table
below.

Note: Bracket syntax, such as `<float>[num_tokens]`, refers to the shapes of
NumPy arrays where each element inside the brackets is an integer.

Name                      | Description                                                                                                                                                           | Value Type
------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------
`TextSegment`             | Natural language text, untokenized.                                                                                                                                   | `str`
`GeneratedText`           | Untokenized text, generated from a model (such as seq2seq).                                                                                                           | `str`
`URL`                     | TextSegment, but interpreted as a URL.                                                                                                                                | `str`
`GeneratedURL`            | Generated TextSegment, but interpreted as a URL (i.e., it maye not be real/is inappropriate as a label).                                                              | `str`
`SearchQuery`             | TextSegment, but interpreted as a search query.                                                                                                                       | `str`
`String`                  | Opaque string data; ignored by components such as perturbation methods that operate on natural language.                                                              | `str`
`ReferenceTexts`          | Multiple texts, such as a set of references for summarization or MT.                                                                                                  | `list[tuple[str, float]]`
`GeneratedTextCandidates` | Multiple generation candidates, such as beam search output from a seq2seq model.                                                                                      | `list[tuple[str, float]]`
`Tokens`                  | Tokenized text.                                                                                                                                                       | `list[str]`
`TokenTopKPreds`          | Predicted tokens and their scores, as from a language model or seq2seq model.                                                                                         | `list[list[tuple[str, float]]]`
`Boolean`                 | Boolean value.                                                                                                                                                        | `bool`
`Scalar`                  | Scalar numeric value.                                                                                                                                                 | `float`
`Integer`                 | Integer, with a default range from -32768 to +32767. value.                                                                                                                                                        | `int`
`ImageBytes`              | Image, represented by a base64 encoded string. LIT also provides `JPEGBytes` and `PNGBytes` types for those specific encodings.                                       | `str`
`RegressionScore`         | Scalar value, treated as a regression target or prediction.                                                                                                           | `float`
`ReferenceScores`         | Scores for one or more reference texts.                                                                                                                               | `list[float]`
`CategoryLabel`           | Categorical label, from open or fixed vocabulary.                                                                                                                     | `str`
`MulticlassPreds`         | Multiclass predicted probabilities.                                                                                                                                   | `<float>[num_labels]`
`SparseMultilabel`        | Multiple non-exclusive labels, such as a set of attributes.                                                                                                           | `list[str]`
`SparseMultilabelPreds`   | Sparse multi-label predictions, represented as scored candidates.                                                                                                     | `list[tuple[str, float]]`
`SequenceTags`            | Sequence tags, aligned to tokens.                                                                                                                                     | `list[str]`
`SpanLabels`              | Span labels, aligned to tokens. Each label is (i,j,label).                                                                                                            | `list[SpanLabel]`
`EdgeLabels`              | Edge labels, aligned to tokens. This is a general way to represent many structured prediction tasks, such as coreference or SRL. See https://arxiv.org/abs/1905.06316 | `list[EdgeLabel]`
`MultiSegmentAnnotations` | In-line byte-span annotations, which can span multiple text segments.                                                                                                 | `list[AnnotationCluster]`
`Embeddings`              | Fixed-length embeddings or model activations.                                                                                                                         | `<float>[emb_dim]`
`Gradients`               | Gradients with respect to embeddings or model activations.                                                                                                            | `<float>[emb_dim]`
`TokenEmbeddings`         | Per-token embeddings or model activations.                                                                                                                            | `<float>[num_tokens, emb_dim]`
`TokenGradients`          | Gradients with respect to per-token embeddings or model activations.                                                                                                  | `<float>[num_tokens, emb_dim]`
`ImageGradients`          | Gradients with respect to image pixels.                                                                                                                               | `<float>[image_height, image_width, color_channels]`
`AttentionHeads`          | Attention heads, grouped by layer.                                                                                                                                    | `<float>[num_heads, num_tokens, num_tokens]`

Values can be plain data, NumPy arrays, or custom dataclasses - see
[dtypes.py](https://github.com/PAIR-code/lit/blob/main/lit_nlp/api/dtypes.py) for further
detail.

*Note: Note that `String`, `Boolean` and `URL` types in Python are represented
as `StringLitType`, `BooleanLitType` and `URLLitType` in TypeScript to avoid
naming collisions with protected TypeScript keywords.*

## Server Configuration

Some properties of the LIT frontend can be configured from Python as
**arguments to `dev_server.Server()`**. These include:

*   `page_title`: set a custom page title.
*   `canonical_url`: set a "canonical" URL (such as a shortlink) that will be
    used as the base when copying links from the LIT UI.
*   `default_layout`: set the default UI layout, by name. See `layout.ts` and
    the section below for available layouts.
*   `demo_mode`: demo / kiosk mode, which disables some functionality (such as
    save/load datapoints) which you may not want to expose to untrusted users.
*   `inline_doc`: a markdown string that will be rendered in a documentation
    module in the main LIT panel.
*   `onboard_start_doc`: a markdown string that will be rendered as the first
    panel of the LIT onboarding splash-screen.
*   `onboard_end_doc`: a markdown string that will be rendered as the last
    panel of the LIT onboarding splash-screen.

For detailed documentation, see
[server_flags.py](https://github.com/PAIR-code/lit/blob/main/lit_nlp/server_flags.py).

Most Python components (such as `Model`, `Dataset`, and `Interpreter`) also have
a `description()` method which can be used to specify a human-readable
description or help text that will appear in the UI.

### Customizing the Layout

You can specify custom web app layouts from Python via the `layouts=` attribute.
The value should be a `Mapping[str, LitCanonicalLayout]`, such as:

```python
PENGUIN_LAYOUT = layout.LitCanonicalLayout(
    upper={
        'Main': [
            modules.DiveModule,
            modules.DataTableModule,
            modules.DatapointEditorModule,
        ]
    },
    lower=layout.STANDARD_LAYOUT.lower,
    description='Custom layout for the Palmer Penguins demo.',
)
```

You can pass this to the server as:

```python
lit_demo = dev_server.Server(
    models,
    datasets,
    # other args...
    layouts=layout.DEFAULT_LAYOUTS | {'penguins': PENGUIN_LAYOUT},
    default_layout='penguins',
    **server_flags.get_flags())
return lit_demo.serve()
```

You can see the pre-configured layouts provided by LIT, as well as the list of
modules that can be included in your custom layout in
[`layout.py`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/api/layout.py). A
`LitCanonicalLayout` can be defined to achieve four different configurations of
the major content areas:

* Single-panel: Define only the `upper=` parameter.
* Two-panel, upper/lower: Define the `upper=` and `lower=` parameters.
* Two-panel, left/right: Define the `left=` and `upper=` parameters; the
  `upper=` section will be shown on the right.
* Three-panel: Define the `left=`, `upper=`, and `lower=` parameters; the
  `upper=` and `lower=` sections will be shown on the right.

To use a specific layout by default for a given LIT instance, pass the key
(e.g., "simple", "default", or the name of a custom layout) as a server flag
when initializing LIT (`--default_layout=<layout>`) or by setting the default
value for that flag in you `server.py` file, e.g.,
`flags.FLAGS.set_default('default_layout', 'my_layout_name')`. The layout can
also be set on-the-fly with the `layout=` URL param, which will take precedence.

Note: The pre-configured layouts are added to every `LitApp` instance using
[dictionary comprehension](https://docs.python.org/3/library/stdtypes.html#dict)
where the Mapping passed to the `LitApp` constructor overrides the
pre-configured layouts `Mapping`. Thus, you can remove or change these
pre-configured layouts as you like by passing a `Mapping` where the values of
`simple`, `default`, and/or `experimental` is `None` (to remove) or a
`LitCanonicalLayout` instance (to override) as you desire.

## Accessing the LIT UI in Notebooks

As an alternative to running a LIT server and connecting to it through a web
browser, LIT can be used directly inside of python notebook environments, such
as [Colab](https://colab.research.google.com/) and
[Jupyter](https://jupyter.org/).

After installing LIT through pip, create a `lit_nlp.notebook.LitWidget` object,
passing in a dict of models and a dict of datasets, similar to the
`lit_nlp.dev_server.Server` constructor. You can optionally provide a height
parameter that specifies the height in pixels to render the LIT UI.

Then, in its own output cell, call the `render` method on the widget object to
render the LIT UI. The LIT UI can be rendered in multiple cells if desired. The
LIT UI can also be rendered in its own browser tab, outside of the notebook, by
passing the parameter `open_in_new_tab=True` to the `render` method. The
`render` method can optionally take in a configuration object to specify
certain options to render the LIT UI using, such as the selected layout,
current display tab, dataset, and models. See
[notebook.py](https://github.com/PAIR-code/lit/blob/main/lit_nlp/notebook.py) for details.

The widget has a `stop` method which shuts down the widget's server. This can be
important for freeing up resources if you plan to create multiple LIT widget
instances in a single notebook. Stopping the server doesn't disable the model
and dataset instances used by the server; they can still be used in the notebook
and take up the resources they require.

Check out an
[example notebook](https://colab.research.google.com/github/pair-code/lit/blob/main/lit_nlp/examples/notebooks/LIT_sentiment_classifier.ipynb).

## Using LIT components outside of LIT

All LIT Python components (models, datasets, interpreters, metrics, generators,
etc.) are standalone classes that do not depend on the serving framework. You
can easily use them from Colab, in scripts, or in your libraries. This can
also be handy for development, as you can test new models or components without
needing to reload the server or click the UI.

For example, to view examples in a dataset:

```python
from lit_nlp.examples.glue import data as glue_data
dataset = glue_data.SST2Data('validation')
print(dataset.examples)  # list of records {"sentence": ..., "label": ...}
```

And to run inference on a few of them:

```python
from lit_nlp.examples.glue import models as glue_models

model = glue_models.SST2Model("/path/to/model/files")
preds = list(model.predict(dataset.examples[:5]))
# will return records {"probas": ..., "cls_emb": ..., ...} for each input
```

Or to compute input salience using
[LIME](https://homes.cs.washington.edu/~marcotcr/blog/lime/):

```python
from lit_nlp.components import lime_explainer

lime = lime_explainer.LIME()
lime.run([dataset.examples[0]], model, dataset)
# will return {"tokens": ..., "salience": ...} for each example given
```

For a full working example in Colab, see [LIT_components_example.ipynb](https://colab.research.google.com/github/pair-code/lit/blob/dev/lit_nlp/examples/notebooks/LIT_components_example.ipynb).


<!-- Links -->

[build-metadata]: https://github.com/PAIR-code/lit/blob/main/lit_nlp/app.py
[components-py]: https://github.com/PAIR-code/lit/blob/main/lit_nlp/api/components.py
[curves-interp]: https://github.com/PAIR-code/lit/blob/main/lit_nlp/components/curves.py
[dataset-py]: https://github.com/PAIR-code/lit/blob/main/lit_nlp/api/dataset.py
[grad-maps]: https://github.com/PAIR-code/lit/blob/main/lit_nlp/components/gradient_maps.py
[json]: https://www.json.org
[mnli-dataset]: https://cims.nyu.edu/~sbowman/multinli/

[mnli-demo]: https://pair-code.github.io/lit/demos/glue.html

[model-py]: https://github.com/PAIR-code/lit/blob/main/lit_nlp/api/model.py
[should_display_module]: https://github.com/PAIR-code/lit/blob/main/lit_nlp/client/core/lit_module.ts
[types_py]: https://github.com/PAIR-code/lit/blob/main/lit_nlp/api/types.py
[types_ts]: https://github.com/PAIR-code/lit/blob/main/lit_nlp/client/lib/lit_types.ts
[utils-lib]: https://github.com/PAIR-code/lit/blob/main/lit_nlp/client/lib/utils.ts
[utils-lib-py]: https://github.com/PAIR-code/lit/blob/main/lit_nlp/lib/utils.py
[word-replacer]: https://github.com/PAIR-code/lit/blob/main/lit_nlp/components/word_replacer.py

================
File: docs/documentation/_sources/components.md.txt
================
# Components and Features

<!--* freshness: { owner: 'lit-dev' reviewed: '2024-06-24' } *-->

<!-- [TOC] placeholder - DO NOT REMOVE -->

## Framework and Model Support

LIT is framework-agnostic and is compatible with any model that can be wrapped
in a Python class for inference. In particular, we've tested with TF1.x, TF2,
JAX, and PyTorch, as well as models that use custom C++ inference code (wrapped
via CLIF) and with remote models over RPC. In general, there aren't any
constraints beyond those imposed by the modeling platform (for example, TF1.x
and TF2 models can't coexist in the same process) or underlying hardware (such
as GPU memory). For working with very large models, also see the
[Scale section of the FAQ](./faq.md#scale).

Many LIT users implement their own
[model and dataset classes](./api.md#adding-models-and-data), but we also have
out-of-the-box support for a few modeling frameworks, described below.

### HuggingFace Transformers

Many of the
[open-source LIT examples](https://github.com/PAIR-code/lit/blob/main/lit_nlp/examples/) use
HuggingFace Transformers via their TF2/Keras model classes. These give easy
access to model internals such as embeddings, attention, and gradients, and the
LIT wrappers for these support many interpretability methods - such as
[integrated gradients](https://arxiv.org/abs/1703.01365) out-of-the-box.

These models are a great place to start for small-scale experiments or for
working on academic projects.

### TF1.x Estimator

LIT supports Estimator and other TF1.x models, but the model wrappers can be
more involved due to the need to explicitly manage the graph and sessions. (In
particular: `Estimator.predict()` cannot be used because it reloads the model on
every invocation.) Generally, you'll need to:

*   In your model's `__init__()`, build the graph, create a persistent TF
    session, and load the model weights.
*   In your `predict()` function, build a feed dict and call `session.run`
    directly.

Alternatively, you can export to a `SavedModel` and load this in an eager mode
runtime. This leads to much simpler code, but may require changes to your
`SavedModel` exporter in order to access model internals like embeddings,
gradients, or attention.

### Remote or hosted models

LIT can easily interact with models hosted via an RPC or HTTP endpoint,
including Servomatic. In this usage, the model weights and computation remain on
the server, while the LIT `Model` implementation simply manages the RPC stub and
handles format conversion and any additional pre- or post-processing.

*   For a general-purpose interface to connect to another LIT server over HTTP,
    see [components/remote_model.py](https://github.com/PAIR-code/lit/blob/main/lit_nlp/components/remote_model.py).

### Static predictions

LIT works best when the model can be queried interactively, but this isn't
always possible for all situations. The
[`StaticPredictions`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/components/static_preds.py)
class allows LIT to serve a set of pre-computed predictions by creating a
"model" backed by a lookup table. This can be handy for quickly browsing data,
while still retaining access to LIT's rich visualizations.

Note: `StaticPredictions` does not support counterfactuals or any methods such
as LIME which involve querying the model on new examples.

### Data loading

LIT can load data from almost any format, including TFRecord, Capacitor,
SSTable, or even SQL queries, via a custom Python class that implements the
[Dataset API](./api.md#datasets). Many of our demos use TFDS, and the LIT loader
code is only a small wrapper class which performs minimal post-processing. See
the [demos page](./demos.md) for specific examples.

Datasets can also be loaded or saved from the UI, if the appropriate methods are
implemented on the dataset class. See the
[Workflow and Integrations FAQ](./faq.md#workflow-and-integrations) for more
details.

## Input and Output Types

LIT uses an extensible system of semantic types to describe data and models.
This allows for flexible support of a number of different input and output
modalities common to NLP and other domains. For a full reference, see the
documentation of the [Python API](api.md), in particular:

*   [Adding models and data](api.md#adding-models-and-data)
*   [Type system](api.md#type-system)

Below, we describe a few common model types and usage patterns. Note that while
some features (such as metrics and output visualizations) are specific to an
output type, many LIT features - such as datapoint exploration, side-by-side
functionality, and counterfactual generation - are available for any model.

### Classification

LIT supports many features for classification tasks, including common metrics,
confusion matrices, and custom thresholding via the UI. Classification is
implemented with the `MulticlassPreds` and `CategoryLabel` types.

*   Models should define a `MulticlassPreds` field in their output spec with the
    `vocab=` attribute as the set of class labels, and for each example should
    return a vector of probabilities for each class.
*   To provide labels for evaluation, the data should define a `CategoryLabel`
    field which contains string-valued labels. The model's `MulticlassPreds`
    field should set the `parent=` attribute to the name of this field.
*   A negative class can be designated using the `null_idx` attribute of
    `MulticlassPreds` (most commonly, `null_idx=0`), and metrics such as
    precision, recall, F1 will be computed for the remaining classes. AUC and
    AUCPR will be computed for binary classification tasks.
*   If `null_idx` is set and there is only one other class, the other class
    (often, class `1`) is treated as a positive class, and the LIT UI can be
    used to change the classification threshold. If `null_idx` is set and there
    are more than two classes, a "margin" can be set which acts as a bias (in
    log space) for the negative class.

![Classification Results Module](images/components/classification-results.png){w=600px align=center}

### Regression / Scoring

Regression or scoring models also are well-supported with metrics, bucketed
faceting, and scatterplots of scalar output. Regression is implemented with the
`Scalar` (input) and `RegressionScore` (output) types.

*   Models should define a `RegressionScore` field in their output spec, and for
    each example should return a numerical score.
*   To provide labels for evaluation, the data should define a `Scalar` field
    which contains numerical targets, and the model's `RegressionScore` field
    should set `parent=` to the name of this field.
*   For an example, see the STS-B textual similarity task in
    [examples/glue/demo.py](https://github.com/PAIR-code/lit/blob/main/lit_nlp/examples/glue/demo.py).

### Multi-label classification

LIT supports multi-label tasks, when a model can label a single example with
more than one label. Multi-label classification is implemented with the
`SparseMultilabelPreds` and `SparseMultilabel` types.

*   Models should define a `SparseMultilabelPreds` field in their output spec
    with the`vocab=` attribute as the set of class labels, and for each example
    should return a list of class score tuples. Each tuple contains two
    elements: a string class label and a non-negative numeric score for that
    class.
*   To provide labels for evaluation, the data should define a
    `SparseMultilabel` field which contains a list of string-valued labels. The
    model's `SparseMultilabelPreds` field should set the `parent=` attribute to
    the name of this field.

### Seq2Seq / Generation

LIT has a number of features for sequence generation models, though support is
not quite as mature as for classification or regression. In particular, LIT can
display single generations as well as scored candidates from beam search, and
can highlight diffs against one or more reference texts. If supported by the
model, LIT can also render per-token output probabilities from a language model
or decoder.

*   Models should define a `GeneratedText` field (for single generation) and
    emit a single string per example, or a `GeneratedTextCandidates` field and
    emit multiple candidates and their scores.
*   To provide target sequences for evaluation, the data should include a
    `TextSegment` field (for a single reference) or a `ReferenceTexts` field
    (for multiple references), and the model's output field should set `parent=`
    accordingly.
*   To use a model in scoring mode over one or more predefined target sequences,
    the model can also output a `ReferenceScores` field (with values as
    `list[float]`) with `parent=` set to reference a `TextSegment` or
    `ReferenceTexts` field from the input.

![Generated Text Module](images/components/generation-results.png){w=600px align=center}

### Span Labeling and Structured Prediction

LIT can support a variety of structured prediction types, and provides rich,
interactive visualizations.

*   For token-aligned output, models should define a `Tokens` field in their
    output, and return a list of tokens for each example.
*   For part-of-speech and other per-token tags, models should define a
    `SequenceTags` type with the `align=` attribute set to the name of the
    appropriate `Tokens` field. For each example, they should return a list of
    tags, one per token.
*   For span labels such as named entities (NER), models can use the
    `SpanLabels` type and return tuples (as `dtypes.SpanLabel`) of
    `(i,j,label)`. Similarly, an `EdgeLabel` type is available for tasks such as
    SRL and dependency parsing that consist of relations between two spans.
*   Experimentally, byte-based annotations are supported via the
    `MultiSegmentAnnotations` type.

![Structured Predictions Module](images/components/structured-preds.png){w=400px align=center}

### Multiple input segments

LIT can easily handle multiple text fields, or a mix of text, categorical,
scalar, and other input features. LIT does not explicitly "privilege" one input
field, and metadata in the model spec can be used to align gradients, attention,
and otherwise to different parts of the input.

*   For an example with two-sentence input, see the
    [Dataset class documentation](./api.md#datasets) and the corresponding
    [Model](./api.md#models).
*   For a more involved code example including per-token gradients, see
    [examples/glue/demo.py](https://github.com/PAIR-code/lit/blob/main/lit_nlp/examples/glue/demo.py).

### Tabular data

LIT can be used as a replacement for the [What-If Tool](https://whatif-tool.dev)
but with more extensibility, when working with predictions over tabular data.

Some interpreters, such as Kernel SHAP, require models that use tabular data. In
these cases, LIT validates model compatibility by checking that:

*   The model inputs (`input_spec()`) are exclusively categorical
    (`CategoryLabel`) or numeric (`Scalar`), and none of these are marked as
    optional (`required=False`).
*   The model outputs include at least one classification (`MulticlassPreds`),
    regression (`RegressionScore` or `Scalar`), or multilabel
    (`SparseMultilabel`) field.

For a demo using a penguin stats dataset/binary classification task, see
[examples/penguin/demo.py](https://github.com/PAIR-code/lit/blob/main/lit_nlp/examples/penguin/demo.py).

### Images

LIT also contains support for models with images as input features or generated
images as model output. The LIT type `ImageBytes` can be used as a feature in
datasets and as part of an input spec or output spec for a model. That feature's
value must be a base64 encoded string for an image.

NOTE: We may transition images away from encoded strings, moving to individual
pixel color values. We will ensure we don't break existing checked-in code with
such a change.

## Token-based Salience

LIT supports several methods for token-based input salience, including
gradient-based methods as well as black-box techniques like LIME that don't
require any access to model internals. Output is rendered in the Salience Maps
module in the LIT UI, which allows for comparison of multiple methods at once:

![Salience Map Module](./images/components/salience-map.png){w=600px align=center}

For a demo with a BERT-based classifier, see https://pair-code.github.io/lit/demos/glue.html and navigate to the
"Explanations" tab.

Currently, salience is supported for classification ( `MulticlassPreds`),
regression (`RegressionScore`) and generation (`GeneratedText` or
`GeneratedTextCandidates`) outputs.

### Gradient Norm

This is a simple method, in which salience scores are proportional to the L2
norm of the gradient, i.e. the score for token $i$ is:

$$S(i) \propto ||\nabla_{x_i} \hat{y}||_2$$

To enable this method, your model should, as part of the
[output spec and `predict()` implementation](./api.md#models):

*   Return a `Tokens` field with values (as `list[str]`) containing the
    tokenized input.
*   Return a `TokenGradients` field with the `align` attribute pointing to the
    name of the `Tokens` field (i.e. `align="tokens"`). Values should be arrays
    of shape `<float>[num_tokens, emb_dim]` representing the gradient
    $\nabla_{x} \hat{y}$ of the embeddings with respect to the prediction
    $\hat{y}$.

Because LIT is framework-agnostic, the model code is responsible for performing
the gradient computation and returning the result as a NumPy array. The choice
of $\hat{y}$ is up to the developer; typically for regression/scoring this is
the raw score and for classification this is the score of the predicted (argmax)
class.

### Gradient-dot-Input

In this method, salience scores are proportional to the dot product of the input
embeddings and their gradients, i.e. for token $i$ we compute:

$$S(i) \propto x_i \cdot \nabla_{x_i} \hat{y}$$

Compared to grad-norm, this gives directional scores: a positive score is can be
interpreted as that token having a positive influence on the prediction
$\hat{y}$, while a negative score suggests that the prediction would be
stronger if that token was removed.

To enable this method, your model should, as part of the
[output spec and `predict()` implementation](./api.md#models):

*   Return a `Tokens` field with values (as `list[str]`) containing the
    tokenized input.
*   Return a `TokenEmbeddings` field with values as arrays of shape
    `<float>[num_tokens, emb_dim]` containing the input embeddings $x$.
*   Return a `TokenGradients` field with the `align` attribute pointing to the
    name of the `Tokens` field (i.e. `align="tokens"`), and the `grad_for`
    attribute pointing to the name of the `TokenEmbeddings` field. Values should
    be arrays of shape `<float>[num_tokens, emb_dim]` representing the gradient
    $\nabla_{x} \hat{y}$ of the embeddings with respect to the prediction
    $\hat{y}$.

As with grad-norm, the model should return embeddings and gradients as NumPy
arrays. The LIT `GradientDotInput` component will compute the dot products and
appropriate normalization.

### Integrated Gradients

Integrated gradients is a more robust method for estimating feature
contribution, based on integrating a gradients along a path in embedding space.
See [Sundararajan et al. 2017](https://arxiv.org/abs/1703.01365) for additional
detail on the algorithm. This method may give better results than grad-norm and
grad-dot-input, but also requires more involved instrumentation of the model.

To support this method, your model needs to return the gradients and embeddings
needed for grad-dot-input, and also to *accept* modified embeddings as input.

*   The model output should be as for grad-dot-input, plus
    `grad_target_field_key` must be set to the name of a label field from the
    input.
*   The model should have an [optional input](./api.md#optional-inputs) of type
    `TokenEmbeddings` with the same name as the output `TokenEmbeddings` field
    (see [type system conventions](./api.md#conventions)), which will be used to
    feed in the interpolated inputs as arrays of shape `<float>[num_tokens,
    emb_dim]`.

An example spec would look like:

```python
   def input_spec(self) -> types.Spec:
     return {
         # ...
         "token_embs": lit_types.TokenEmbeddings(align='tokens', required=False),
         # ...
     }

   def output_spec(self) -> types.Spec:
     return {
         # ...
         "tokens": lit_types.Tokens(parent="input_text"),
         "token_embs": lit_types.TokenEmbeddings(align='tokens'),
         "token_grads": lit_types.TokenGradients(align='tokens',
                                                 grad_for="token_embs",
                                                 grad_target_field_key="label"),
         # ...
     }
```

For a more concrete example that also supports multiple segments with separate
gradients, see our
[BERT classifier demo model](https://github.com/PAIR-code/lit/blob/main/lit_nlp/examples/glue/models.py),
or contact the LIT team for assistance.

### LIME

[LIME](https://arxiv.org/abs/1602.04938) is a black-box salience method that
does not require access to any model internals. It works by generating a set of
perturbed inputs - generally, by dropping out or masking tokens - and training a
local linear model to reconstruct the original model's predictions. The weights
of this linear model are treated as the salience values.

The trade-off, compared to gradient-based methods, is that computing LIME can be
slow as it requires many evaluations of the model. Additionally, LIME can be
noisy on longer inputs, as there are more tokens to ablate. To compensate, you
can increase the number of samples:

![LIME configuration options](./images/components/lime-options.png){w=600px align=center}

LIME works out-of-the-box with any classification (`MulticlassPreds`) or
regression/scoring (`RegressionScore`) model.

### Target Selection on Classification Output

For all salience methods, we require that the class to explain is given as a
label field in the input. For example, if the input example is:

```
{"text": "this movie was terrible!", "label": "0"}
```

Our model should return gradients with respect to the class 0. Conversely, we
might want to ask what features would encourage the model to predict a different
class. If we select class 1 from the UI:

![Target Selection](./images/components/salience-target-select.png){w=400px align=center}

Then the model will receive a modified input with this target:

```
{"text": "this movie was terrible!", "label": "1"}
```

To support this, the model should have the label field in the `input_spec`:

```
def input_spec(self) -> types.Spec:
  return {
    'text': lit_types.TextSegment(),
    'label': lit_types.CategoryLabel(..., required=False),
    ...
  }
```

and have an output field which references this using `parent=`:

```
def output_spec(self) -> types.Spec:
  return {
    'probas': lit_types.MulticlassPreds(..., parent="label"),
    ...
  }
```

You don't have to call the field "label", and it's okay if this field isn't
present in the *dataset* - as long as it's something that the model will
recognize and use as the target to derive gradients.

## Sequence Salience

Sequence salience generalizes token-based salience to text-to-text models,
allowing you to explain the impact of the prompt tokens on parts of the model
output.

LIT has a general-purpose sequence salience visualization designed for
left-to-right ("causal") language models:

![Sequence salience - sequence selection](./images/components/sequence-salience-1.png){w=650px align=center}

![Sequence salience - visualization](./images/components/sequence-salience-2.png){w=650px align=center}

The UI supports multiple options for analysis, including:

*   Select from predefined target sequences, or explain generations from the
    model.
*   Different salience methods, including [Gradient Norm](#gradient-norm) and
    [Gradient-dot-Input](#gradient-dot-input).
*   Multiple granularity levels for analysis, from individual sub-word tokens up
    to words, sentences, lines, or paragraphs. Quickly switch between different
    views to refine your analysis to different parts of a prompt.
*   Display density options to enable working with longer sequences, such as
    document text, few-shot examples, or chain-of-thought prompts.

For a walkthrough of how to use sequence salience to debug LLMs, check out the
Responsible Generative AI Toolkit at
https://ai.google.dev/responsible/model_behavior and for more on design of the
system see our paper at https://arxiv.org/abs/2404.07498.

If you find this useful in your work, please cite Sequence Salience as:

```
@article{tenney2024interactive,
  title={Interactive Prompt Debugging with Sequence Salience},
  author={Tenney, Ian and Mullins, Ryan and Du, Bin and Pandya, Shree and Kahng, Minsuk and Dixon, Lucas},
  journal={arXiv preprint arXiv:2404.07498},
  year={2024}
}
```

**Code:**

Currently, this works out-of-the-box with Gemma, Llama 2, Mistral, and GPT-2,
using either KerasNLP or Transformers.

*   LIT-for-Gemma Colab:
    [`lit_gemma.ipynb`](https://colab.research.google.com/github/google/generative-ai-docs/blob/main/site/en/gemma/docs/lit_gemma.ipynb)
*   Demo binary:
    https://github.com/PAIR-code/lit/blob/main/lit_nlp/examples/prompt_debugging/server.py
*   KerasNLP model wrappers:
    https://github.com/PAIR-code/lit/blob/main/lit_nlp/examples/prompt_debugging/keras_lms.py
*   Transformers model wrappers:
    https://github.com/PAIR-code/lit/blob/main/lit_nlp/examples/prompt_debugging/transformers_lms.py

## Salience Clustering

LIT includes a basic implementation of the salience clustering method from
[Ebert et al. 2022](https://arxiv.org/abs/2211.05485), which uses k-means on a
salience-weighted bag-of-words representation to find patterns in model
behavior. This method is available using any of the token-based salience methods
above, and if enabled will appear in the "Salience Clustering" tab:

![Salience clustering UI](./images/components/salience-clustering.png)

To run clustering, select a group of examples or the entire dataset, choose a
salience method, and run using the "Apply" button. The result will be a set of
top tokens for each cluster, as in Table 6 of
[the paper](https://arxiv.org/pdf/2211.05485.pdf).

## Tabular Feature Attribution

Tabular feature attribution seeks to understand the importance of a column of
data on a model's predictions. LIT's tabular feature attribution module supports
this analysis using the [SHAP interpreter](https://github.com/slundberg/shap).
Please check out
[our tutorial](https://pair-code.github.io/lit/tutorials/tab-feat-attr/) to
learn more about how to use this module to analyze feature importance in the
[Penguins demo](https://pair-code.github.io/lit/demos/penguins.html).

![Tabular feature attribution module module](./images/components/tabular-feature-attribution.png){w=500px align=center}

## Pixel-based Salience

LIT also supports pixel-based salience methods, for models that take images as
inputs. Output is rendered in the Salience Maps module in the LIT UI, which
allows for comparison of multiple methods at once.

To enable pixel-based salience methods for models that take images as inputs,
your model should, as part of the
[output spec and `predict()` implementation](./api.md#models):

*   Return a `ImageGradients` field with the `align` attribute pointing to the
    name of the `ImageBytes` field and, optionally, the `grad_target_field_key`
    attribute pointing to the `CategoryLabel` field in input spec that specifies
    the target class for which to take gradients, if the model can process that
    as an input. Without this gradient target field key, the model should return
    gradients with respect to the argmax class for classification models. The
    model should also return the actual class for which the gradients have been
    computed in the `grad_target` output field. The values returned in this
    field (as `<float>[image_height, image_width, color_channels]`) should be
    the gradients with respect to each pixel in each color channel in the 2D
    input image.

    The model should be able to accept input images as numpy arrays in addition
    to accepting base64 URL encoded format.

A variety of image saliency techniques are implemented for models that return
image gradients, through use of the
[PAIR-code saliency library](https://github.com/PAIR-code/saliency), including
integrated gradients, guided integrated gradients, blurred integrated gradients,
and XRAI.

Each of these techniques returns a saliency map image as a base64-encoded string
through the `ImageSalience` type.

## Embedding Projector

LIT includes a version of the
[embedding projector](https://projector.tensorflow.org/) which can be used to
visualize the latent space of your model, in order to find clusters or patterns
in the data. [UMAP](https://umap-learn.readthedocs.io/en/latest/) and PCA are
both supported as projection techniques.

![Embedding Projector](./images/components/embeddings.png){w=500px align=center}

The plot can be panned, zoomed, and rotated, and you can click a point to select
an example, or shift-click to select a group. You can also use LIT's global
colormap (the "Color By" menu) to color points by features such as the original
label, the model's predictions, or another categorical feature from the input.

To enable the embedding projector, your model should return one or more
`Embeddings` fields, with corresponding values as fixed-length vectors
`<float>[emb_dim]` for each example.

## Aggregate Analysis

### Metrics

LIT includes common metrics for classification, regression, and seq2seq (BLEU)
by default, which will appear in the table when the appropriate types are
present in the model output and input data. Metrics can be computed on the whole
dataset, a selected subset, or on facets defined by particular features. For
example, we could facet by class label:

![Metrics Table](./images/components/metrics-table.png)

To try this out, see https://pair-code.github.io/lit/demos/glue.html and navigate to the "Metrics" tab.

To enable metrics, your model should set the `parent` attribute on one or more
output fields, pointing to the name of the input field that it should be
evaluated against. For example, for classification, the data spec might have:

```python
    def spec(self) -> types.Spec:
      return {
          # ...
          "label": lit_types.CategoryLabel(vocab=self.LABELS),
          # ...
      }
```

and the model would include:

```python
    def output_spec(self) -> types.Spec:
      return {
          # ...
          "probas": lit_types.MulticlassPreds(vocab=self.LABELS, parent='label'),
          # ...
      }
```

Custom metrics can be easily defined in Python; see the
[API documentation](./api.md#metrics) for more.

### Confusion Matrix

LIT includes a powerful and flexible confusion matrix, which can be used to
compare predictions to gold labels as well as to compare between two models or
between different categorical features. You can click cells or row/column
headers to select a subset of examples, which is useful for intersectional
analysis.

![Confusion Matrix](./images/components/confusion-matrix.png){w=600px align=center}

To try this out, see https://pair-code.github.io/lit/demos/glue.html and navigate to the "Metrics" tab.

The confusion matrix is supported for classification models, or if the input
data includes any categorical features (`CategoryLabel`).

### Scalar Plots

LIT includes scatterplots for scalar features, including plain scalars (`Scalar`
or `RegressionScore`) as well as per-class probabilities from classification
output (`MulticlassPreds`).

![Scalar Plot Module](./images/components/scalars-sst2.png)

You can click individual points to select them, or drag to select all examples
in a range of scores - for example, to find examples near the decision boundary.
The plots also respond to LIT's global colormap (the "Color By" menu), which can
color points by categorical features or the model's predicted class.

To try this out, see https://pair-code.github.io/lit/demos/glue.html and navigate to the "Predictions" tab.

### Binary Classification Thresholds

For binary classification models, LIT contains a module for setting
classification thresholds, which determine at what score for the positive class
the model determines that an example should be classified as belonging to the
positive class.

This threshold can be set either on the entire dataset, or can be set separately
on faceted subsets of the dataset. Checkboxes in this module are used to select
which features in the dataset will be used to create the faceted subsets.
Multiple features can be selected, which leads to intersectional subsets.

Additionally, if the dataset has ground truth labels for the value being
predicted, then the module can calculate the optimal value to set these
thresholds. The cost ratio input box allows setting of the relative penalty of
the model producing a false positive, compared to a false negative. By default,
this is set to 1, meaning that false positives and false negatives are equally
costly, in terms of how we should calculate the optimal threshold(s). Setting it
to 2 would mean that false positives are twice as costly as false negatives, and
setting it to .5 would mean that false negatives are twice as costly as false
positives.

The "Get optimal threshold" button will calculate optimal thresholds for each
subset specified by the checkboxes, or the entire dataset if no subsets are
created. These are displayed in the thresholds table along with the slider to
manually change the thresholds. The buttons in the table header allow easy
setting of those optimal thresholds.

When the dataset is faceted into subsets, along with calculating optimal
individual thresholds per subset, and an optimal overall threshold for the
entire dataset, a number of other threshold sets are calculated. These are based
on different fairness constraints that may be of interest to the user.

One such constraint is demographic parity, which attempts to have an equal
percentage of positive classifications for each subset. Another is equal
accuracy, which attempts to have an equal accuracy score for each subset. There
is also equal opportunity, which attempts to equalize for each subset the
percentage of positive predictions among those datapoints with a positive ground
truth label.

![Binary Classifier Thresholds Module](./images/components/lit-thresholder.png)

### Partial Dependence Plots

For classification or regression models with `CategoryLabel` or `Scalar` input
features, the Partial Dependence Plots module shows plots indicating the effect
that changing those individual features has on model output.

If a single datapoint is selected, then a feature's plot shows the effect of
changing that one value has on model output. For numeric features, the model
output is calculated for 10 different values, ranging from the minimum value of
that feature in the dataset to its maximum value in the dataset, and the results
are shown in the line chart. For categorical features, the model output is
calculated for all values of that feature from the `vocab` specified in the
`CategoryLabel` for that feature, and the results are shown in a column chart.

If multiple datapoints are selected, then the model outputs are calculated using
the same logic for each datapoint, and the outputs are averaged to create the
points on the line or column charts. In this way, the charts show the average
effect of that feature on model output, given the datapoints chosen.

If no datapoints are selected, then the calculations are done across all
datapoints, giving a global view of feature effects.

![Partial Dependence Plots Module](./images/components/lit-pdps.png){w=400px align=center}

To try this out, see https://pair-code.github.io/lit/demos/penguins.html and navigate to the "Predictions" tab.

### Dive

Dive is a visualization module, inspired by our prior work on
[Facets Dive](https://pair-code.github.io/facets/) and its use in the
[What-If Tool](https://pair-code.github.io/what-if-tool/), that enables
exploration of data subsets grouped by feature values.

![Dive module](./images/components/dive.png){w=500px align=center}

Data are displayed in a matrix of groups based on feature values, with each
group containing the datapoints at the intersection of the feature values for
that column and row. Use the drop-downs at the top to select the feature to use
for the rows and columns in the matrix. You can use the "Color By" drop-down in
the main toolbar to change the feature by which datapoints are colored in the
matrix.

This visualization is powered by
[Megaplot](https://github.com/PAIR-code/megaplot), which allows it to support up
to 100k datapoints. Dive support mouse-based zoom (scroll) and pan (drag)
interactions to help you navigate these very large datasets. You can also use
the "zoom in", "zoom out", and "reset view" buttons in the module toolbar to
help navigate with more precision.

Dive is currently integrated in the
[Penguins demo](https://pair-code.github.io/lit/demos/penguins.html), and will
be supported in other demos in future releases.

## TCAV

Many interpretability methods provide importance values per input feature (e.g,
token). By contrast, [TCAV](https://arxiv.org/abs/1711.11279) shows the
importance of high-level concepts (e.g., color, gender, race) for a prediction
class, which is more akin to how humans communicate.

From those examples, TCAV learns a vector representing those concepts in a model
layer, which we call a concept activation vector (CAV). A CAV is learned using a
linear classifier. Intuitively, CAV measures how sensitive prediction is to the
concept (i.e., directional derivative of the prediction with respect to the
CAV). Unlike many local attribution methods mentioned above, TCAV is a global
method. This means that TCAV explains "a class" rather than "a data point". TCAV
does this by aggregating concepts' influence on data points in a class (i.e.,
ratio of data points with positive directional derivatives).

The TCAV method can be applied to models with any input modality. To enable
TCAV, your model should return one or more example-level Embeddings fields for a
layer, the predicted class, and the corresponding gradient values for that layer
and class.

### Example

1.) To use TCAV, begin by creating one or more 'concept' slices.

Every dataset/model is different, but for images, as low as 15 data points are
shown to be sufficient. Start by adding at least 3 data points and add more as
needed.

For this example, we select all examples related to acting in the data table
using the selector `acting|actor|actress`.

![Data table - select examples](./images/components/tcav-search-examples.png){w=400px align=center}

2.) Next, name the slice `acting` and click 'Create slice'.

![Slice](./images/components/tcav-create-slice.png){w=280px align=center}

3.) Finally, navigate to the TCAV tab, select the newly created slice, and click
'Run TCAV'.

This initiates standard TCAV, which compares the selected examples against
random splits of examples in the rest of the dataset. Alternatively, selecting a
second 'negative' slice would initiate relative TCAV, which compares the
selected slice's examples against those in the negative slice.

![TCAV1](./images/components/tcav-select-slice.png){w=800px align=center}

When the run is complete (usually after a few seconds), the results are
displayed in the table. In this example, the TCAV score is ~0.9 (shown by the
blue bar in the score bar), which is higher than the baseline (shown as the
black bar in the score bar ), indicating that the acting concept positively
influences the prediction class 1, or positive sentiment. (Technically, the
baseline represents 'null hypothesis', calculated with random concepts.)

![TCAV2](./images/components/tcav-results-table.png){w=800px align=center}

### Statistical Significance

One of the pitfalls with the TCAV method is the potential generating meaningless
CAVs, since any randomly chosen set of images will still produce a CAV (even if
it is not meaningful).

To guard against this, we use statistical testing to verify whether CAVs are
statistically significant. For standard TCAV, we generate 15 possibly meaningful
CAVs using the selected concept slice and random splits of the same size from
the remainder of the dataset. We also generate 15 random CAVs using random
splits against random splits. We then do a t-test to check if these two sets of
scores are from the same distribution and reject CAVs as insignificant if the
p-value is greater than 0.05. (If this happens, a warning is displayed in place
of the TCAV score in the UI.)

For relative TCAV, users would ideally test concepts with at least ~100 examples
each so we can perform ~15 runs on unique subsets. In practice, users may not
pass in this many examples.

To accommodate this, we use a cross-validation approach, where we will try
different subset split sizes, and return one with a statistically significant
result (when compared against random CAVs). We set the minimum number of
examples to run TCAV at 3 examples, and need at least 2 runs for statistical
testing. If there are too few examples for this, we will perform 1 run of size
min(concept set length, negative set length), and return the result without
statistical testing (which is indicated in the UI).

### Sorting by Cosine Similarity

The option to sort examples by cosine similarity to a CAV will be available in
an upcoming release.

## Counterfactual Analysis

While aggregate metrics can give a picture of overall behavior, and salience
maps can give quick insight into a model's local behavior, many questions about
model behavior are best answered in a counterfactual setting: "How does my model
behave under a controlled change in inputs?"

For example, you might want to see what happens if a single token is deleted, a
word is substituted, or some systematic transformation - like paraphrasing or an
adversarial attack - is applied to the whole dataset. LIT includes features to
explore this, both through manual edits and through automatic "generator"
components.

### Manual Editing

Examples can be edited manually in the Datapoint Editor module:

![Manual Edit in the Datapoint Editor](./images/components/manual-edit.png){w=400px align=center}

The "Add and Compare" button can be used to enter comparison mode, which will
automatically "pin" the original example as a reference selection. Many LIT
modules will automatically duplicate to show the predictions on this example
side-by-side with the original. For example:

![Side-by-Side](./images/components/side-by-side-salience.png)

You can also use the toolbar controls to enter comparison mode. LIT also keeps
track of the relationship between examples, and you can use the pair selection
controls to cycle through the available (original, edited) examples:

![Pair Selection Controls](./images/components/pair-selection.png){w=700px align=center}

### Generators

The **Generator Module** supports automatic generation of counterfactuals
through a variety of plug-in components:

![Generator Module](./images/components/generator-module.png)

Semantically, generators are Python classes which take one or more input
examples and return a new set of transformed examples. This can include simple
transformations such as scrambling word order or making regex substitutions, or
more complex methods such as back-translation or adversarial methods such as
[HotFlip](https://arxiv.org/abs/1712.06751).

Generators can be easily defined using the [Python API](./api.md#generators) and
customized for particular applications or domains.

We also include a handful of off-the-shelf methods:

*   The
    [**scrambler**](https://github.com/PAIR-code/lit/blob/main/lit_nlp/components/scrambler.py)
    simply randomizes word order of the input.
*   The
    [**word replacer**](https://github.com/PAIR-code/lit/blob/main/lit_nlp/components/word_replacer.py)
    makes simple substitutions, such as `great -> terrible`.
*   [**HotFlip**](https://github.com/PAIR-code/lit/blob/main/lit_nlp/components/hotflip.py)
    ([Ebrahimi et al. 2017](https://arxiv.org/abs/1712.06751)) tries to find
    minimal token substitutions to change the model's prediction. Compatible
    with classification models (`MulticlassPreds`) or regression models
    (`RegressionScore`) via thresholding, and requires access to
    `TokenGradients` as well as a special `get_embedding_table()` method on the
    model class.
*   [**Ablation Flip**](https://github.com/PAIR-code/lit/blob/main/lit_nlp/components/ablation_flip.py)
    is similar to HotFlip, but tries to change the prediction by selectively
    dropping tokens from the input. Unlike HotFlip, this does not require
    gradients or access to the embedding table and can work with any
    classification or regression model.

================
File: docs/documentation/_sources/demos.md.txt
================
# Demos

<!-- freshness: { owner: 'lit-dev' reviewed: '2024-06-24' } -->

<!-- [TOC] placeholder - DO NOT REMOVE -->

The LIT team maintains a number of hosted demos, as well as pre-built launchers
for some common tasks and model types.

For publicly-visible demos hosted on Google Cloud, see
https://pair-code.github.io/lit/demos/.

--------------------------------------------------------------------------------

## Classification <!-- DO NOT REMOVE {#classification .demo-section-header} -->

### Sentiment and NLI <!-- DO NOT REMOVE {#glue .demo-header} -->

**Hosted instance:** https://pair-code.github.io/lit/demos/glue.html \
**Code:** [examples/glue/demo.py](https://github.com/PAIR-code/lit/blob/main/lit_nlp/examples/glue/demo.py)

*   Multi-task demo:
    *   Sentiment analysis as a binary classification task
        ([SST-2](https://nlp.stanford.edu/sentiment/treebank.html)) on single
        sentences.
    *   Natural Language Inference (NLI) using
        [MultiNLI](https://cims.nyu.edu/~sbowman/multinli/), as a three-way
        classification task with two-segment input (premise, hypothesis).
    *   STS-B textual similarity task (see
        [Regression / Scoring](#regression-scoring) below).
    *   Switch tasks using the Settings () menu.
*   BERT models of different sizes, built on HuggingFace TF2 (Keras).
*   Supports the widest range of LIT interpretability features:
    *   Model output probabilities, custom thresholds, and multiclass metrics.
    *   Jitter plot of output scores, to find confident examples or ones near
        the margin.
    *   Embedding projector to find clusters in representation space.
    *   Integrated Gradients, LIME, and other salience methods.
    *   Counterfactual generators, including HotFlip for targeted adversarial
        perturbations.

Tip: check out a case study for this demo on the public LIT website:
https://pair-code.github.io/lit/tutorials/sentiment

--------------------------------------------------------------------------------

## Regression / Scoring <!-- DO NOT REMOVE {#regression-scoring .demo-section-header} -->

### Textual Similarity (STS-B) <!-- DO NOT REMOVE {#stsb .demo-header} -->

**Hosted instance:** https://pair-code.github.io/lit/demos/glue.html?models=stsb&dataset=stsb_dev \
**Code:** [examples/glue/demo.py](https://github.com/PAIR-code/lit/blob/main/lit_nlp/examples/glue/demo.py)

*   STS-B textual similarity task, predicting scores on a range from 0
    (unrelated) to 5 (very similar).
*   BERT models built on HuggingFace TF2 (Keras).
*   Supports a wide range of LIT interpretability features:
    *   Model output scores and metrics.
    *   Scatter plot of scores and error, and jitter plot of true labels for
        quick filtering.
    *   Embedding projector to find clusters in representation space.
    *   Integrated Gradients, LIME, and other salience methods.

--------------------------------------------------------------------------------

## Sequence-to-Sequence <!-- DO NOT REMOVE {#seq2seq .demo-section-header} -->

### Gemma <!-- DO NOT REMOVE {#gemma .demo-header} -->

**Code:**
[examples/prompt_debugging/server.py](https://github.com/PAIR-code/lit/blob/main/lit_nlp/examples/prompt_debugging/server.py)

*   Supports Gemma 2B and 7B models using KerasNLP (with TensorFlow or PyTorch)
    and Transformers (with PyTorch).
*   Interactively debug LLM prompts using
    [sequence salience](./components.md#sequence-salience).
*   Multiple salience methods (grad-l2 and grad-dot-input), at multiple
    granularities: token-, word-, line-, sentence-, and paragraph-level.

Tip: check out the in-depth walkthrough at
https://ai.google.dev/responsible/model_behavior, part of the Responsible
Generative AI Toolkit.

--------------------------------------------------------------------------------

## Multimodal <!-- DO NOT REMOVE {#multimodal .demo-section-header} -->

### Tabular Data: Penguin Classification <!-- DO NOT REMOVE {#penguin .demo-header} -->

**Hosted instance:** https://pair-code.github.io/lit/demos/penguins.html \
**Code:** [examples/penguin/demo.py](https://github.com/PAIR-code/lit/blob/main/lit_nlp/examples/penguin/demo.py)

*   Binary classification on
    [penguin dataset](https://www.tensorflow.org/datasets/catalog/penguins).
*   Showing using of LIT on non-text data (numeric and categorical features).
*   Use partial-dependence plots to understand feature importance on individual
    examples, selections, or the entire evaluation dataset.
*   Use binary classifier threshold setters to find best thresholds for slices
    of examples to achieve specific fairness constraints, such as demographic
    parity.

================
File: docs/documentation/_sources/docker.md.txt
================
# Running LIT in a Docker container

<!--* freshness: { owner: 'lit-dev' reviewed: '2024-06-04' } *-->

Users might want to deploy LIT onto servers for public-facing, long-running
instances. This is how we host the LIT demos found on
https://pair-code.github.io/lit/demos/. This doc describes the basic usage of
LIT's built-in demos, how to integrate your custom demo into this

## Basic Usage

LIT can be run as a containerized app using [Docker](https://www.docker.com/) or
your preferred engine. This is how we run our
[hosted demos](https://pair-code.github.io/lit/demos/).

We provide a basic Dockerfile https://github.com/PAIR-code/lit/blob/main/Dockerfile that you can use to build and run any of the demos in the `lit_nlp/examples` directory.
The `Dockerfile` installs all necessary dependencies for LIT and builds the
front-end code from source. Then it runs [gunicorn](https://gunicorn.org/) as
the HTTP server, invoking the `get_wsgi_app()` method from our demo file to get
the WSGI app to serve. The options provided to gunicorn for our use-case can be
found in
[`gunicorn_config.py`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/examples/gunicorn_config.py).
You can find a reference implementation in
[`glue/demo.py`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/examples/glue/demo.py).

Use the following shell
https://github.com/PAIR-code/lit/blob/main/.github/workflows/ci.yml commands to build the
default Docker image for LIT from the provided `Dockerfile`, and then run a
container from that image. Comments are provided in-line to help explain what
each step does.

```shell
# Build the docker image using the -t argument to name the image. Remember to
# include the trailing . so Docker knows where to look for the Dockerfile.
docker build --file Dockerfile --tag lit-nlp .

# Now you can run LIT as a containerized app using the following command. Note
# that the last parameter to the run command is the value you passed to the -t
# argument in the build command above.
docker run --rm -p 5432:5432 lit-nlp
```

The image above defaults to launching the GLUE demo on port 5432, but you can
override this using the DEMO_NAME and DEMO_PORT environment variables, as shown
below.

```shell
# DEMO_NAME is used to complete the Python module path
#
#     "lit_nlp.examples.$DEMO_NAME.demo:get_wsgi_app()"
#
# Therefore, valid values for DEMO_NAME are Python module paths in the
# lit_nlp/examples directory, such as glue, penguin, tydi, etc.
docker run --rm -p 5432:5432 -e DEMO_NAME=penguin lit-nlp

# Use the DEMO_PORT environment variable as to change the port that LIT uses in
# the container. Be sure to also change the -p option to map the container's
# DEMO_PORT to a port on the host system.
docker run --rm -p 2345:2345 -e DEMO_PORT=2345 lit-nlp

# Bringing this all together, you can run multiple LIT apps in separate
# containers on your machine using the combination of the DEMO_NAME and
# DEMO_PORT arguments, and docker run with the -d flag to run the container in
# the background.
docker run -d -p 5432:5432 -e DEMO_NAME=penguin lit-nlp
docker run -d -p 2345:2345 -e DEMO_NAME=tydi -e DEMO_PORT=2345 lit-nlp
```

## Integrating Custom LIT Instances with the Default Docker Image

Many LIT users create their own custom LIT server script to demo or serve, which
involves creating an executable Python module with a `main()` method, as
described in the [Python API docs](api.md#adding-models-and-data).

These custom server scripts can be easily integrated with LIT's default image as
long as your server meets two requirements:

1.  Ensure your server script is located in the `lit_nlp/examples` directory (or
    in a nested directory under `lit_nlp/examples`).
2.  Ensure that your server script defines a `get_wsgi_app()` function similar
    to the minimal example shown below.

```python
def get_wsgi_app() -> Optional[dev_server.LitServerType]:
  """Return WSGI app for container-hosted demos."""
  # Set any flag defaults for this LIT instance
  FLAGS.set_default("server_type", "external")
  FLAGS.set_default("demo_mode", True)
  # Parse flags before calling main()
  unused = flags.FLAGS(sys.argv, known_only=True)
  if unused:
    logging.info("get_wsgi_app() called with unused args: %s", unused)
  return main([])
```

Assuming your custom script meets the two requirements above, you can simply
rebuild the default Docker image and run a container using the steps above,
ensuring that you pass the `-e DEMO_NAME=your_server_script_path_here` to the
run command.

A more detailed description of the `get_wsgi_app()` code can be found below.

```python
def get_wsgi_app() -> Optional[dev_server.LitServerType]:
  """Returns a WSGI app for gunicorn to consume in container-hosted demos."""
  # Start by setting any default values for flags your LIT instance requires.
  # Here we set:
  #
  #     server_type to "external" (required), and
  #     demo_mode to "True" (optional)
  #
  # You can add additional defaults as required for your use case.
  FLAGS.set_default("server_type", "external")
  FLAGS.set_default("demo_mode", True)

  # Parse any parameters from flags before calling main(). All flags should
  # defined using one of absl's flags.DEFINE methods.
  #
  # Note the use of the known_only=True parameter here. This ensures that only
  # those flags that have been define using one of absl's flags.DEFINE methods
  # will be parsed from the command line arguments in sys.argv. All unused
  # arguments will be returned as a Sequence[str].
  unused = flags.FLAGS(sys.argv, known_only=True)

  # Running a LIT instance in a container based on the default Dockerfile and
  # image will always produce unused arguments, because sys.argv contains the
  # command and parameters used to run the gunicorn sever. While not stricly
  # required, we recommend logging these to the console, e.g., in case you need
  # to verify the value of an environment variable.
  if unused:
    logging.info("get_wsgi_app() called with unused args: %s", unused)

  # Always pass an empty list to main() inside of get_wsgi_app() functions, as
  # absl apps are supposed to use absl.flags to define any and all flags
  # required to run the app.
  return main([])
```

## Building Your Own Image

Coming soon.

================
File: docs/documentation/_sources/faq.md.txt
================
# Frequently Asked Questions

<!--* freshness: { owner: 'lit-dev' reviewed: '2024-06-03' } *-->

<!-- [TOC] placeholder - DO NOT REMOVE -->

Looking for help? Submit bugs, ask questions, suggest content, and request
features on our
[Github issues list](https://github.com/pair-code/lit/issues/).

## Model and Data Types

LIT can handle a variety of models with different input and output types, and
works with any modern ML framework. For more information, see
[Framework & Model Support](components.md#framework-and-model-support).

In addition to text, LIT has good support for different modalities, including
images and tabular data. For examples, see:

*   [Tabular demo](https://github.com/PAIR-code/lit/blob/main/lit_nlp/examples/penguin/demo.py) -
    multi-class classification on tabular (numeric and categorical string) data,
    using the
    [Palmer Penguins](https://www.tensorflow.org/datasets/catalog/penguins)
    dataset.

For more details, see
[the features guide to input and output types](api.md#type-system).

## Languages

All strings in LIT are unicode and most components use model-provided
tokenization if available, so in most cases non-English languages and non-Latin
scripts should work without any modifications.

## Scale

### Dataset Size

LIT can comfortably handle 10k-100k datapoints, depending on the speed of the
server (for hosting the model) and your local machine (for viewing the UI). When
working with large datasets, there are a couple caveats:

*   LIT expects predictions to be available on the whole dataset when the UI
    loads. This can take a while if you have a lot of examples or a larger model
    like BERT. In this case, we recommend adding the flag `--warm_start=1` (or
    pass `warm_start=1` to the `Server` constructor in Python) to pre-compute
    predictions before starting the server.

*   Datasets containing images may take a while to load. If full "native"
    resolution is not needed (such as if the model operates on a smaller size
    anyway, such as 256x256), then you can speed things up by resizing images in
    your `Dataset` loading code.

*   LIT uses WebGL for the embedding projector (via
    [ScatterGL](https://github.com/PAIR-code/scatter-gl)) and for the Scalars
    and Dive modules (via [Megaplot](https://github.com/PAIR-code/megaplot)),
    which may be slow on older machines if you have more than a few thousand
    points.

If you have more data, you can use `Dataset.sample` or `Dataset.slice` to select
a smaller number of examples to load. You can also pass individual examples to
LIT [through URL params](#sending-examples-from-another-tool), or load custom
data files at runtime using the settings () menu.

### Large Models

LIT can work with large or slow models, as long as you can wrap them into the
model API. If you have more than a few preloaded datapoints, however, you'll
probably want to use `warm_start=1` (or pass `--warm_start=1` as a flag) to
pre-compute predictions when the server loads, so you don't have to wait when
you first visit the UI.

Also, beware of memory usage: since LIT keeps the models in memory to support
new queries, only so many models can fit on a single node or GPU. If you want to
load more or larger models than can fit in local memory, you can host your model
with your favorite serving framework and connect to it using a custom
[`Model`](api.md#models) class.

We also have experimental support for using LIT as a lightweight model server;
this can be useful, e.g., for comparing an experimental model running locally
against a production model already running in an existing LIT demo. See
[`remote_model.py`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/components/remote_model.py)
for more details.

## Privacy and Security

LIT allows users to query the model, as well as to view the loaded evaluation
data. The LIT UI state is ephemeral and exists only in the browser window;
however, model predictions and any newly-generated examples (including as
manually entered in the web UI) are stored in server memory, and if `--data_dir`
is specified, may be cached to disk.

LIT has the ability to create or edit datapoints in the UI and then save them to
disk. If you do not want the tool to be able to write edited datapoints to disk,
then pass the `--demo_mode` runtime flag to the LIT server.

### I have proprietary data. Is LIT secure for my team to use?

We don't store, collect or share datasets, models or any other information
loaded into LIT. When you run a LIT server, anyone with access to the web
address of the server will be able to see data from the loaded datasets and
interact with the loaded models. If you need to restrict access to a LIT
server, then make sure to configure the hosting of your LIT server to do so.

The default LIT development server does not implement any explicit access
controls. However, this server is just a thin convenience wrapper, and the
underlying WSGI App can be easily exported and used with additional middleware
layers or external serving frameworks. See
[Running LIT in a Docker container](./docker.md) for an example.

## Workflow and Integrations

### Sending examples from another tool

LIT can read input fields directly from the URL; they should be encoded as
`data_<fieldname>=<value>`, and field names should match those in the (default)
dataset.

There is also (experimental) support for sending more than one example, as long
as the total URL length is within the browser's size limit. Specify as above,
but using `data0`, `data1`, `data2`, e.g. `data0_<fieldname>=<value>`.

### Downloading or exporting data

Currently, there are three ways to export data from the LIT UI:

-   In the Data Table, you can copy or download the current view in CSV format -
    see [the UI guide](./ui_guide.md#data-table) for more details.
-   In the "Dataset" tab of the settings () menu, you can enter a path to save
    data to. Data is pushed to the server and written by the server backend, so
    be sure the path is writable.

-   If using LIT in a Colab or other notebook environment, you can access the
    current selection from another cell using `widget.ui_state.primary`,
    `widget.ui_state.selection`, and `widget.ui_state.pinned`.

### Loading data from the UI

There is limited support for this via the settings () menu. Select a dataset,
and enter a path to load from:

![Load data from the UI](./images/lit-load-data.png)

The supported format(s) depend on the dataset class; in most cases, the user
should implement the `load()` method on their dataset class to handle the
appropriate format.

### Using components outside the LIT UI

Python components such as models, datasets, and generators are designed to
support standalone use. These don't depend on the LIT serving framework, and you
can treat them as any other Python class and use from Colab, regular scripts,
bulk inference pipelines, etc. For an example, see
[the API documentation](./api.md#using-lit-components-outside-of-lit).

For the frontend, it's a little more difficult. In order to respond to and
interact with the shared UI state, there's a lot more "framework" code involved
(see the [frontend development guide](./frontend_development.md) for more).
We're working on refactoring the LIT
[`modules`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/client/modules) to separate
framework and API code from the visualizations (e.g.
[`elements`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/client/elements)), which can
then be re-used in other environments.

### Training models with LIT

LIT is primarily an evaluation/inference-time tool, so we don't provide any
official training APIs. However, to facilitate code reuse you can easily add
training methods to your model class. In fact, several of our demos do exactly
this, using LIT's `Dataset` objects to manage training data along with standard
training APIs (such as Keras' `model.fit()`). See
[`glue/models.py`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/examples/glue/models.py)
for examples.

### Debug LIT UI in Colab

The LIT instance launched from CLI typically has helpful error messages in the
UI. However, this is not the case for the LIT UI in Colab and the error message
does not report any stacktrace, which makes debugging very difficult.

![LIT UI error in colab](./images/lit-ui-error-in-colab.png "LIT UI error in colab")

While in
[Chrome developer tools](https://support.google.com/campaignmanager/answer/2828688?hl=en),
you will be able to debug issues solely related to the frontend, but not so for
issues related to the backend or on the HTTP request path.

Thus, to show the full stacktrace, you would need to find the HTTP request sent
from the frontend to the backend, compose the same request in colab and send it
to the server.

1.  When rendering the UI, display it in a separate tab to make things a bit
    easier to work with, e.g. `lit_widget.render(open_in_new_tab=True)`.
2.  Open
    [Chrome developer tools](https://support.google.com/campaignmanager/answer/2828688?hl=en),
    go to "Sources" tab and find the file
    [client/services/api_service.ts](https://github.com/PAIR-code/lit/blob/main/lit_nlp/client/services/api_service.ts) and set a
    breakpoint right after where the HTTP request is set up in the `queryServer`
    method, e.g. after this line `const res = await fetch(url, {method: 'POST',
    body});`.
    *   Note it is possible that the whole frontend source code is compiled into
        a `main.js` file, and the code is not exactly the same as that in LIT
        frontend source code. You might have to do a bit digging to find the
        right line.
3.  Go to the UI and trigger the behavior that causes the error. Now in Chrome
    developer tools you will be able to see the variables and their values in
    the `queryServer` method. Copy the values of the `url` and `body` variables
    in the method.
4.  Go back to Colab, compose your HTTP request method. Look for the main server
    address printed out from `lit_widget.render(open_in_new_tab=True)`.

![LIT colab server address](./images/lit-colab-server-address.png "LIT colab server address")

Let's say the server address is "https://localhost:32943/?" as shown above, the
`body` variable obtained earlier has value "request_body_text" and the `url`
variable has value "./get_preds?param1=value1". Then your HTTP request will be
like this:

```sh
! curl -H "Content-Type: application/json" \
       -d "request_body_text" \
       -X POST "http://localhost:32943/get_preds?param1=value1"
```

Run this in Colab and you should be able to retrieve the full stacktrace of the
error.

================
File: docs/documentation/_sources/frontend_development.md.txt
================
# Frontend Developer Guide

<!--* freshness: { owner: 'lit-dev' reviewed: '2024-06-24' } *-->

<!-- [TOC] placeholder - DO NOT REMOVE -->

This document aims to describe the current LIT frontend system, including
conventions, best practices, and gotchas.

## High Level Overview

LIT is powered by two central pieces of tech -
[lit-element](https://lit-element.polymer-project.org/) for components and HTML
rendering, and [mobx](https://mobx.js.org/README.html) for observable-oriented
state management.

Lit-element is a simple, web-component based library for building small,
self-contained pieces of web functionality. It uses a template-string based
output to declaratively render small, isolated pieces of UI.

Mobx is a tool centered around observable data, and it makes managing state
simple and scalable.

We highly recommend reading the docs for both projects - they both have fairly
simple APIs and are easy to digest in comparison to some heavier-weight toolkits
like Angular.

## Application Architecture

The LIT client frontend is roughly divided into three conceptual groups -
**Modules** (which render visualizations), **Services** (which manage data), and
the **App** itself (which coordinates initialization of services and determines
which modules to render).

### Bootstrapping

The LIT app bootstrapping takes place in two steps: First, the served
[`index.html`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/client/static/index.html)
page contains a single web component for the
[`<lit-app>`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/client/core/lit_app.ts).
This component is responsible for the overall layout of the app, including the
toolbar, footer, and the
[`<lit-modules>`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/client/core/modules.ts)
component. The `<lit-modules>` component is responsible for actually laying out
and rendering the various `LitModule` components, a process about which we'll go
into greater detail later.

The JS bundle entry point is
[`main.ts`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/client/main.ts), which first
imports the loaded, the `<lit-app>` web component is declared, and attaches
itself to the DOM, waiting for the app to be initialized.

The second step is kicking off app initialization. The
[`LitApp`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/client/core/app.ts) singleton
class is provided with a layout declaring which `LitModule` components to use,
then builds the app services and kicks off app initialization and loading data.

### Layout

A layout defines the arraignment of `LitModule` classes in the UI. Layouts are
specified in Python as `LitCanonicalLayout` instances, and LIT includes three
pre-configured layouts in
[`layout.py`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/api/layout.py):

*   `simple`: A minimalist layout with the examples on top (either individually
    (selected by default) or in a table) and predictions on the bottom;
*   `default`: The original LIT layout with a single group of modules on top for
    exploring and selecting data, and a collection of tabs supporting different
    analytical tasks on the bottom; and
*   `three_panel`: A three-panel layout that puts exploratory data
    visualizations at full-page height on the left, tools for inspecting and
    manipulating examples and their associated predictions in the upper right,
    and a collection of tabs supporting different analytical tasks in the lower
    left. Note that this was introduced in v1.0 as an experimental feature, your
    feedback is appreciated.

You can also add [custom layouts](./api.md#customizing-the-layout) to your LIT
instance by defining one or more `LitCanonicalLayout` instances and passing them
to the server. For an example, see
[`prompt_debugging/layouts.py`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/examples/prompt_debugging/layouts.py).

Note: The pre-configured layouts are added to every `LitApp` instance using
[dictionary updates](https://docs.python.org/3/library/stdtypes.html#dict) where
the Mapping passed to the `LitApp` constructor overrides the pre-configured
layouts `Mapping`. Thus, you can remove or change these pre-configured layouts
as you like by passing a `Mapping` where the values of `simple`, `default`,
and/or `three_panel` is `None` (to remove) or a `LitCanonicalLayout` instance
(to override) as you desire.

The actual layout of components in the LIT UI, see
[`<lit-modules>`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/client/core/modules.ts),
can be different than the declared layout, since the visibility of modules
depends on a number of factors, including the user-chosen visibility, the
compatibility of the configured modules with the selected model and dataset, and
whether or not specific modules show multiple copies per selected model. The
actual layout is computed in
[`modules_service`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/client/services/modules_service.ts).

### Initialization

Finally, the LIT App initializes by building the various service classes and
starting the initial load of data from the server. This process consists of:

1.  Parsing the URL query params to get the url configuration
1.  Fetching the app metadata, which includes what models/datasets are available
    to use.
1.  Determining which models/datasets to load and then loading them.

## Modules (LitModule)

The
[`LitModule`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/client/core/lit_module.ts)
is the base class from which all module components derive. It provides a number
of convenience methods for handling common update / data loading patterns. Each
LIT Module also requires a few static methods by convention, responsible for
specifying Module display and behavior. These helpers and conventions are
outlined below:

```typescript
/**
 * A dummy module that responds to changes in selected data by making a request
 * to an API service to get the pig latin translation.
 */
@customElement('demo-module')                                                   // (0)
export class DemoTextModule extends LitModule {
  static override title = 'Demo Module';                                        // (1)
  static override template =
      (model: string, selectionServiceIndex: number, shouldReact: number) =>    // (2)
          html`<demo-module model=${model} .shouldReact=${shouldReact}
                selectionServiceIndex=${selectionServiceIndex}></demo-module>`;
  static override duplicateForModelComparison = true;                           // (3)

  static override get styles() {
    return [styles];                                                            // (4)
  }

  private readonly colorService = app.getService(ColorService);                 // (5)

  @observable private pigLatin: string = '';                                    // (6)

  override firstUpdated() {
    this.reactImmediately(() => this.selectionService.primarySelectedInputData, // (7)
      primarySelectedInputData => {
        this.getTranslation(primarySelectedInputData);
      });
  }

  private async getTranslation(primarySelectedInputData: IndexedInput) {
    if (primarySelectedInputData === null) return;

    const promise = this.apiService.getPigLatin(primarySelectedInputData);      // (8)
    const results = await this.loadLatest('pigLatin', promise);                 // (9)
    if (results === null) return;

    this.pigLatin = results;
  }

  override renderImpl() {                                                       // (10)
    const color = this.colorService.getDatapointColor(
        this.selectionService.primarySelectedInputData);
    return html`
      <div class="results" style=${styleMap({'color': color})}>
        ${this.pigLatin}
      </div>
    `;
  }

  static checkModule(modelSpecs: ModelsMap, datasetSpec: Spec): boolean {       // (11)
    return true;
  }
}

declare global {                                                                // (12)
  interface HTMLElementTagNameMap {
    'demo-module': DemoTextModule;
  }
}
```

The above LitModule, while just a dummy example, illustrates all of the
necessary static properties and many of the most common patterns found in the
LIT app.

### Setup

First, a `LitModule` must declare a static `title` string (1) and `template`
function (2). The `template` function determines how the modules layout renders
the component template and passes in module properties, such as the name of the
`model` this should respond to. (3) specified behavior in model comparison mode;
if duplicate is set to true, the layout engine will create two (or more)
instances of this module, each responsible for a different model.

*Note: there are additional static attributes which control module behavior; see
the
[`LitModule`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/client/core/lit_module.ts)
base class for full definitions.*

Styles are also declared with a static get method (4), following the lit-element
convention. These styles can be built using the lit-element `css` template
function, or by importing a separate .css file. Styles can be shared between
components by importing a shared styles .css file (for instance,
[`shared_styles.css`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/client/lib/shared_styles.css))

Services are used by requesting them from the LitApp `app` singleton class (5).
This can be thought of as a super-simple dependency injection system, and allows
for much easier stubbing / mocking of services in testing. We request the
[`colorService`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/client/services/color_service.ts)
here, but the base `LitModule` class initializes the most common services
([`apiService`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/client/services/api_service.ts),
[`appState`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/client/services/state_service.ts),
and
[`selectionService`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/client/services/selection_service.ts))
for us automatically.

The `LitModule` must also provide a static `checkModule` (11) method, which
determines if this module should display for the given model(s) and dataset.

Finally, the `@customElement('demo-module')` decorator (0) defines this class as
a custom HTML element `<demo-module>`, and (12) ensures this is accessible to
other TypeScript files in different build units.

### Functionality

The above module has a very simple task - When the user selects input data, it
makes a request to an API service to fetch and display a pig latin translation
of the data. Since we're using mobx observables to store and compute our state,
we do this all in a reactive way.

First, since the `LitModule` base class derives from `MobxLitElement`, any
observable data that we use in the `renderImpl` method automatically triggers a
re-render when updated. This is excellent for simple use cases, but what about
when we want to trigger more complex behavior, such as the asynchronous request
outlined above?

The pattern that we leverage across the app is as follows: The `renderImpl`
method (10) accesses a private observable `pigLatin` property (6) that, when
updated, will re-render the template and show the results of the translation
automatically. In order to update the `pigLatin` observable, we need to set up a
bit of machinery. In the lit-element lifecycle method `firstUpdated`, we use a
helper method `reactImmediately` (7) to set up an explicit reaction to the user
selecting data. Whatever is returned by the first function (in this case
`this.selectionService.primarySelectedInputData`) is observed and passed to the
second function immediately **and** whenever it changes, allowing us to do
something whenever the selection changes. Note, another helper method `react` is
used in the same way as `reactImmediately`, in instances where you don't want to
immediately invoke the reaction. Also note that modules should override
`renderImpl` and not the base `render` method as our `LitModule` base class
overrides `render` with custom logic which calls our `renderImpl` method for
modules to perform their rendering in.

We pass the selection to the `getTranslation` method to fetch the data from our
API service. However rather than awaiting our API request directly, we pass the
request promise (8) to another helper method `loadLatest` (9). This ensures that
we won't have any race conditions if, for instance, the user selects different
data rapidly - the function returns `null` when the request being fetched has
been superseded by a more recent call to the same endpoint. Finally, we set the
private `pigLatin` observable with the results of our API request and the
template is automatically rerendered, displaying our data.

This may seem like a bit of work for a simple module, but the pattern of using
purely observable data to declaratively specify what gets rendered is very
powerful for simplifying the logic around building larger, more complex
components.

### Escape Hatches

Finally, it's worth noting that the declarative template-based rendering setup,
while effective for handling most component render logic, is sometimes
inadequate for more advanced visualizations. In particular, the template
approach is not well suited for animations, rapidly changing data, or things
that MUST be done imperatively (such as drawing to a canvas). Fortunately, it's
very easy to "bridge" from declarative to imperative code by leveraging the
lit-element lifecycle methods.

In particular, the `updated` and `firstUpdated` methods are useful for
explicitly doing work after the component has rendered. You can use normal
`querySelector` methods to select elements and update their properties
imperatively (note that you must make selections using the shadow root, not the
document, since we're using isolated web components).

One important caveat is that messing with the actual structure of the rendered
DOM output (such as removing/reordering DOM nodes) **will** cause issues with
lit-element, since it relies on a consistent template output to do its
reconciliation of what needs to be updated per render.

```typescript
// An example of a LITModule imperative "escape hatch"
  updated() {
    const canvas = this.shadowRoot!.querySelector('canvas');
    this.drawCanvas(canvas);
  }

  override renderImpl() {
    return html`<canvas></canvas>`;
  }
```

### Stateful Child Elements

Some modules may contain stateful child elements, where the element has some
internal state that can have an effect on the module that contains it. Examples
of this include any modules that contain the
[core/faceting_control.ts](https://github.com/PAIR-code/lit/blob/main/lit_nlp/client/core/faceting_control.ts)
element.

With these types of child elements, it's important for the containing module to
construct them programmatically and store them in a class member variable, as
opposed to only constructing them in the module's html template string returned
by the `renderImpl` method. Otherwise they will be destroyed and recreated when
a module is hidden off-screen and then brought back on-screen, leading them to
lose whatever state they previously held. Below is a snippet of example code to
handle these types of elements.

```typescript
// An example of a LITModule using a stateful child element.
@customElement('example-module')
export class ExampleModule extends LitModule {
  private readonly facetingControl = document.createElement('faceting-control');

  constructor() {
    super();

    const facetsChange = (event: CustomEvent<FacetsChange>) => {
      // Do something with the information from the event.
    };
    // Set the necessary properties on the faceting-control element.
    this.facetingControl.contextName = ExampleModule.title;
    this.facetingControl.addEventListener(
        'facets-change', facetsChange as EventListener)
  }

  override renderImpl() {
    // Render the faceting-control element.
    return html`${this.facetingControl}`;
  }
```

## Style Guide

*   Please disable clang-format on `lit-html` templates and format these
    manually instead:

    ```ts
    // clang-format off
    return html`
      <div class=...>
        <button id=... @click=${doSomething}>Foo</button>
      </div>
    `;
    // clang format on
    ```

*   For new modules, in most cases you should implement two classes: one module
    (subclassing
    [`LitModule`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/client/core/lit_module.ts))
    that interfaces with the LIT framework, and another element which subclasses
    `LitElement`, `MobxLitElement`, or preferably,
    [`ReactiveElement`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/client/lib/elements.ts?),
    and implements self-contained visualization code. For an example, see
    [modules/annotated_text_module.ts](https://github.com/PAIR-code/lit/blob/main/lit_nlp/client/modules/annotated_text_module.ts)
    and
    [elements/annotated_text_vis.ts](https://github.com/PAIR-code/lit/blob/main/lit_nlp/client/elements/annotated_text_vis.ts).

*   On supported components (`ReactiveElement` and `LitModule`), use
    `this.react()` or `this.reactImmediately()` instead of registering reactions
    directly. This ensures that reactions will be properly cleaned up if the
    element is later removed (such as a layout change or leaving comparison
    mode).

*   Use
    [shared styles](https://github.com/PAIR-code/lit/blob/main/lit_nlp/client/lib/shared_styles.css)
    when possible.

## Development Tips (open-source)

If you're modifying any TypeScript code, you'll need to re-build the frontend.
You can have yarn do this automatically. In one terminal, run:

```sh
cd ~/lit/lit_nlp
yarn
yarn build --watch
```

And in the second, run the LIT server:

```sh
cd ~/lit
python -m lit_nlp.examples.<example_name> --port=5432 [optional --args]
```

You can then access the LIT UI at http://localhost:5432.

If you only change frontend files, you can use <kbd>Ctrl/Cmd+Shift+R</kbd> to do
a hard refresh in your browser, and it should automatically pick up the updated
source from the build output.

If you're modifying the Python backend, there is experimental support for
hot-reloading the LIT application logic (`app.py`) and some dependencies without
needing to reload models or datasets. See
[`dev_server.py`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/dev_server.py) for
details.

You can use the `--data_dir` flag (see
[`server_flags.py`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/server_flags.py) to
save the predictions cache to disk, and automatically reload it on a subsequent
run. In conjunction with `--warm_start`, you can use this to avoid re-running
inference during development - though if you modify the model at all, you should
be sure to remove any stale cache files.

## Custom Client / Modules

The LIT frontend can be extended with custom visualizations or interactive
modules, though this is currently provided as "best effort" support and the API
is not as mature as for Python extensions.

An example of a custom LIT client application, including a custom
(potato-themed) module can be found in
[`lit_nlp/examples/custom_module`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/examples/custom_module).
You need only define any custom modules (subclass of `LitModule`) and include
them in the build.

When you build the app, specify the directory to build with the `env.build`
flag. For example, to build the `custom_module` demo app:

```sh
yarn build --env.build=examples/custom_module
```

This builds the client app and moves all static assets to a `build` directory in
the specified directory containing the `main.ts` file
(`examples/custom_module/build`).

Finally, to serve the bundle, set the `client_root` flag in your python code to
point to this build directory. For this example we specify the build directory
in `examples/custom_module/potato_demo.py`.

```python
# Here, client build/ is in the same directory as this file
parent_dir = os.path.join(pathlib.Path(__file__).parent.absolute()
FLAGS.set_default("client_root", parent_dir, "build"))
```

You must also define a
[custom layout definition](./api.md#customizing-the-layout) in Python which
references your new module. Note that because Python enums are not extensible,
you need to reference the custom module using its HTML tag name:

```python
modules = layout.LitModuleName
POTATO_LAYOUT = layout.LitCanonicalLayout(
    upper={
        "Main": [modules.DatapointEditorModule, modules.ClassificationModule],
    },
    lower={
        "Data": [modules.DataTableModule, "potato-module"],
    },
    description="Custom layout with our spud-tastic potato module.",
)
```

See
[`potato_demo.py`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/examples/custom_module/potato_demo.py)
for the full example.

================
File: docs/documentation/_sources/getting_started.md.txt
================
# Getting Started with LIT

<!--* freshness: { owner: 'lit-dev' reviewed: '2024-06-25' } *-->

<!-- [TOC] placeholder - DO NOT REMOVE -->

## Installation

Using pip:

```sh
pip install lit-nlp
```

For more details or to install from source, see
[GitHub](https://github.com/pair-code/lit#download-and-installation).

## Hosted demos

If you want to jump in and start playing with the LIT UI, check out
https://pair-code.github.io/lit/demos/ for links to our hosted demos.

For a guide to the many features available, check out the
[UI guide](./ui_guide.md) or this
[short video](https://www.youtube.com/watch?v=j0OfBWFUqIE).

## LIT with your model <!-- DO NOT REMOVE {#custom-demos} -->

LIT provides a simple [Python API](./api.md) for use with custom models and
data, as well as components such as metrics and counterfactual generators. Most
LIT users will take this route, which involves writing a short `demo.py` binary
to link in `Model` and `Dataset` implementations and configure the server. In
most cases this can be just a few lines:

```python
  datasets = {
      'foo_data': FooDataset('/path/to/foo.tsv'),
      'bar_data': BarDataset('/path/to/bar.tfrecord'),
  }
  models = {'my_model': MyModel('/path/to/model/files')}
  lit_demo = lit_nlp.dev_server.Server(models, datasets, port=4321)
  lit_demo.serve()
```

Check out the [API documentation](./api.md#adding-models-and-data) for more, and
the [demos directory](./demos.md) for a wealth of examples. The
[components guide](./components.md) also gives an overview of interpretability
methods and other features available in LIT, and describes how to enable each
for your task.

## Using LIT in notebooks <!-- DO NOT REMOVE {#colab} -->

LIT can also be used directly from Colab and Jupyter notebooks, with the LIT UI
rendered in an output cell. See [LIT_sentiment_classifier.ipynb](https://colab.research.google.com/github/pair-code/lit/blob/dev/lit_nlp/examples/notebooks/LIT_sentiment_classifier.ipynb) for an example.

Note: if you see a 403 error in the output cell where LIT should render, you may
need to enable cookies on the Colab site, or pass a custom `port=` to the
`LitWidget` constructor.

## Stand-alone components <!-- DO NOT REMOVE {#standalone} -->

Many LIT components - such as models, datasets, metrics, and salience methods -
are stand-alone Python classes and can be easily used outside of the LIT UI. For
additional details, see the
[API documentation](./api.md#using-lit-components-outside-of-lit) and an example Colab
at [LIT_components_example.ipynb](https://colab.research.google.com/github/pair-code/lit/blob/dev/lit_nlp/examples/notebooks/LIT_components_example.ipynb).

## Run an existing example <!-- DO NOT REMOVE {#running-lit} -->

The [demos page](./demos.md) lists some of the pre-built demos available for a
variety of model types. The code for these is under [examples](https://github.com/PAIR-code/lit/blob/main/lit_nlp/examples)
;
each is a small script that loads one or more models and starts a LIT server.

Most demos can be run with a single command. To run the default one, you can do:

```sh
python -m lit_nlp.examples.glue.demo \
  --quickstart --port=4321 --alsologtostderr
```

Then navigate to https://localhost:4321 to access the UI.

For most models we recommend using a GPU, though the `--quickstart` flag above
loads a set of smaller models that run well on CPU. You can also pass
`--warm_start=1.0`, and LIT will run inference and cache the results before
server start.

For an overview of supported model types and frameworks, see the
[components guide](./components.md).

================
File: docs/documentation/_sources/glossary.md.txt
================
# Glossary

There are a few commonly-overloaded terms which refer to specific things in the
LIT APIs and codebase:

*   **Component**, a backend component in Python. Includes things like
    counterfactual generators, metrics classes, UMAP and PCA implementations,
    and salience methods.
*   **Element**, a Web Component or another HTML element. The `client/elements/`
    folder contains many custom elements used for visualizations and parts of
    the UI, but which are not full-fledged LIT Modules.
*   **Example** or **Datapoint**, an element of a dataset - the things that we
    feed to models and get predictions back.
*   **Instance**, a specific implementation of LIT (e.g. a demo.py binary) or
    server job running the former.
*   **LIT**, the Learning Interpretability Tool. Always fully capitalized,
    sometimes accompanied by a  emoji. Pronounced "lit", not "ell-eye-tee".
    Formerly known as the Language Interpretability Tool.
*   **Lit**, the web framework consisting of
    [lit-element](https://lit-element.polymer-project.org/guide) and
    [lit-html](https://lit-html.polymer-project.org/guide) and maintained by the
    Polymer project. LIT is built on this framework, but the naming is
    coincidental (we like it, of course).
*   **Model**, a machine-learning model that we're exploring or debugging with
    LIT. Doesn't need to be a single neural network; could be a pipelined or
    composite system, and may be hosted remotely.
*   **Module**, a frontend visualization module (see
    [Frontend Dev Guide](./frontend_development.md)). Strictly speaking, this is
    something that inherits from LitModule, renders a part of the UI, and
    interacts with the frontend framework. Usually found in `client/modules/`.
    All modules are elements, but not all elements are modules.
*   **Potato** (noun or verb), a frontend error. See
    [potato.io](https://potato.io/).
*   **Server**, the Python backend. A WSGI application that provides a handful
    of HTTP endpoints to serve models, datasets, and other components.
*   **Service**, a part of the frontend framework that handles state and
    provides helper methods. Most of these are global singletons, with the
    notable exception of SelectionService which is duplicated when in
    example-comparison model.
*   **Slice**, a set of examples from a dataset. Often created by **Faceting**
    along a specific feature.
*   **Widget** and **Widget Group**, elements of the frontend layout. A Widget
    is a thin wrapper over a Module, and a Widget Group contains one or more
    widgets along with header bars, resize, and minimize/maximize controls -
    roughly, like a regular GUI window. Sometimes we refer to a widget or a
    widget group as a **Panel**.

================
File: docs/documentation/_sources/index.md.txt
================
# Learning Interpretability Tool (LIT)

<!--* freshness: { owner: 'lit-dev' reviewed: '2024-08-15' } *-->

<!-- [TOC] placeholder - DO NOT REMOVE -->

Welcome to LIT, the Learning Interpretability Tool!

If you want to jump in and start playing with the LIT UI, check out the hosted demos at https://pair-code.github.io/lit/demos/.

## Research

Found LIT useful in your research? Please cite our
[system demonstration paper](https://aclanthology.org/2020.emnlp-demos.15/)!

```
@misc{tenney2020language,
    title={The Language Interpretability Tool: Extensible, Interactive Visualizations and Analysis for {NLP} Models},
    author={Ian Tenney and James Wexler and Jasmijn Bastings and Tolga Bolukbasi and Andy Coenen and Sebastian Gehrmann and Ellen Jiang and Mahima Pushkarna and Carey Radebaugh and Emily Reif and Ann Yuan},
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    year = "2020",
    publisher = "Association for Computational Linguistics",
    pages = "107--118",
    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.15",
}
```


```{toctree}
   :maxdepth: 2

   Main Site <https://pair-code.github.io/lit/>
   Getting Started <getting_started.md>
   Examples <demos.md>
   UI Guide <ui_guide.md>
   Components & Features <components.md>
   Python API <api.md>
   Frontend Development <frontend_development.md>
   Running in Docker <docker.md>
   Glossary <glossary.md>
   FAQ <faq.md>
   GitHub <https://github.com/pair-code/lit>
```

================
File: docs/documentation/_sources/ui_guide.md.txt
================
# UI Guide

<!--* freshness: { owner: 'lit-dev' reviewed: '2024-06-24' } *-->

This is a user guide for the Learning Interpretability Tool (LIT) UI.

For a quick video tour of LIT, check out this
[video](https://www.youtube.com/watch?v=CuRI_VK83dU).

<!-- [TOC] placeholder - DO NOT REMOVE -->

## General Layout

LIT lives inside a single page web application, comprised of multiple toolbars
and a main section consisting of individual modules. Modules will automatically
display if they are applicable to the current model and dataset; for example,
the module that shows classification results will only show if the model returns
`MulticlassPreds`.

![LIT overall UI](./images/lit-ui.png "LIT overall UI")

LIT's layout consist of as many as three sections, described in the API docs for
[custom layouts](./api.md#customizing-the-layout). When the layout provides more than one
major content section, they are separated by draggable dividers that are built
into LIT's toolbars (for allocating vertical space) or in the space between
sections and modules (for allocating horizontal space). Any section may include
multiple tabs, where each tab contains a collection of modules. LIT's
[pre-configured layouts](./frontend_development.md#layout) group modules into
tabs based on analytical task (e.g., metrics analysis vs. input salience
visualization vs. counterfactual example generation), but you can adopt whatever
organizational scheme you desire in your custom layouts.

### Layout Options
<!--
  TODO: Add an image of the 3 LIT layouts side by side.
-->

LIT provides three pre-configured layouts:

*   `simple`: A minimalist layout with the examples on top (either individually
    (selected by default) or in a table) and predictions on the bottom;
*   `default`: The original LIT layout with a single group of modules on top for
    exploring and selecting data, and a collection of tabs supporting different
    analytical tasks on the bottom; and
*   `three_panel`: A three-panel layout that puts exploratory data
    visualizations at full-page height on the left, tools for inspecting and
    manipulating examples and their associated predictions in the upper right,
    and a collection of tabs supporting different analytical tasks in the lower
    right. Note that this was introduced in v1.0 as an experimental feature,
    your feedback is appreciated.

## Datapoint Selections

LIT displays a loaded dataset and its model results across the set of selected
models. Users can dive into detailed results by selecting datapoints from the
dataset.

![LIT datapoint selection](./images/lit-datapoint-selection.png "LIT datapoint selection")

LIT provides two levels of precision for selections. The first is the current
selection, which consists of one or more datapoints that are selected through
one of the interactive modules (such as the *Data Table*, *Embeddings*,
*Scalars*, or *Confusion Matrix* module). When a set of datapoints is selected
in a module, this selection is reflected across all other modules, along with
the selection toolbar. For example, the *Metrics* module shows model metrics not
just across the entire dataset, but also for the current selection of
datapoints.

The second is the primary selection. This is a single datapoint within the
current selection that is being explored in more detail in modules that focus on
a single datapoint (such as the *Datapoint Editor* and *Salience Maps* modules).
If the current selection only consists of a single datapoint, then that
datapoint is also the primary selection. If the current selection consists of
multiple datapoints, the primary selection defaults to the first datapoint in
that selection but can be changed through the arrow controls in the selection
toolbar or by clicking another datapoint in the selection. The primary selection
is highlighted in a darker blue in the *Data Table* module and its ID is
displayed in the selection toolbar.

A selection of datapoints can be saved as a "slice" through the
*[Slice Editor](#slices)*. Saving a selection as a slice allows for easy
navigation back to that selection in the future. It also allows for comparison
of metrics across subsets of datapoints, as described in the
*[Metrics Module](#metrics-table)* section.

## Toolbars

There are three toolbars provided in LIT. The top bar includes the selected
model(s) and dataset, a settings button, and URL sharing functionality. Below
that is the main toolbar with the menus and controls for navigation and
selection. At the bottom of the page is a status bar.

![LIT toolbars](./images/lit-toolbars.png "LIT toolbars")

### Top Bar

#### Global Settings

The global settings dialog is accessible through the **"Configure"** button in
the top bar.

LIT can be launched with a set of models and datasets. The settings screen
allows users to select which models to analyze. Any number of models can be
analyzed together, assuming they are compatible in the input data format they
use (i.e. two different toxicity classifiers can be analyzed together for
comparison). Once a model or models is selected, you can then select from any
dataset compatible with those models.

The settings dialog also contains controls switching the layout of the tool.
This can help declutter the UI when analysis doesn't require all of the
compatible modules that LIT contains.

![LIT global settings](./images/lit-settings.png "LIT global settings")

#### URL Sharing

Much of the LIT app's state &mdash; the loaded models and datasets, selected
datapoints, minimized and/or full-screen modules &mdash; is stored in URL
parameters. The **"Copy Link"** button in the top bar allows a user to share
their specific LIT view and setup with someone else. The URL can also be copied
manually from the address bar.

The base url that will be copied with the **"Copy Link"** button can be
configured by passing the `--canonical_url=<url base>` flag to the server.

### Main Toolbar

The main toolbar is right below the top bar and contains a number of different
controls and information. The left side of the toolbar contains a set of menus
for quickly controlling datapoint selection and coloring. This includes the
following controls:

*   The **"Select datapoint"** menu provides a drop-down of several options:
    *   the **"Random"** option selects a random datapoint,
    *   the **"All related"** option adds any datapoints "related" to the
        current selection. In LIT, "related" is defined as datapoints created
        from some source datapoint (through manual editing or a datapoint
        generator), or a source datapoint that a selected datapoint was created
        from,
    *   the **"Parents"** option adds the source datapoints that the selected
        datapoints were created from,
    *   the **"Children"** option adds the datapoints created from the selected
        datapoints (through manual editing or a datapoint generator),
    *   the **Slices** option allows quick selection of an already-created slice
        of datapoints,
    *   the **"Clear selection"** button deselects all selected datapoints.
*   The **"Color by"** menu enables setting of the color of each datapoint in
    the modules that visualize all datapoints (such as the *Embeddings* and
    *Scalars* modules) by any number of datapoint features or model outputs on
    those datapoints (such as coloring by some categorical input feature, or by
    prediction error for a regression task).
*   The **Slices** menu allows adding/selecting/removing slices of datapoints.

Next to the menus is a button for pinning/unpinning a datapoint. Pinning a
datapoint puts LIT into datapoint comparison mode, where two datapoints can be
compared against each other, across all applicable modules. This mode is
described in more detail [below](#comparing-datapoints).

The right side of the toolbar displays how many datapoints are in the loaded
dataset and how many of those are currently selected. If only a single datapoint
is selected, the left and right arrow buttons in this toolbar allow cycling of
the selected datapoint through the loaded dataset. If the current selection is a
set of datapoints, then the left and right arrow buttons control which of those
datapoints is the primary selected datapoint, cycling through the datapoints in
the current selection. A **"Select random"** button allows selection of a random
datapoint, as opposed to the ordered cycling done through the left and right
arrows.The **"Select all"** and **"Clear selection"** buttons are also provided
to easily select all or none of the datapoints, respectively.

### Status Bar

The status bar at the bottom of the tool contains a text area on the left side.
If the tool is currently waiting on the results of a call to the backend (such
as for running predictions or getting embeddings), this information will be
displayed in the status bar along with an indeterminant progress bar showing
that a result is pending. If a call to the backend fails, information about the
failure will be displayed in this area in red to call out the error, and that
information will persist in the status bar until the user clicks the **"x"**
button by the error to clear the status display. The full error log can also be
displayed by clicking the error icon in the message.

## Comparing Models

By loading more than one model in the global settings controls, LIT can compare
multiple models. A subset of modules that show per-model information are then
duplicated to allow easy comparison across two models. Other modules, such the
*Embeddings* and *Metrics* modules are updated to show information from all
models.

![LIT model comparison](./images/lit-model-compare.png "LIT model comparison")

## Comparing Datapoints

Pinning a datapoint, through either the toolbar button or controls in modules
(e.g., the pin icons in Data Table rows), puts LIT into **datapoint comparison
mode**. In this mode, the pinned datapoint is used as a reference to compare the
primary selection. The pinned datapoint is indicated by a pin icon in modules
that support datapoint comparison, such as the Data Table. Any changes to the
primary selection will update datapoint comparison visualizations in all
supporting modules.

As with model comparison, some modules may be duplicated, one showing the pinned
datapoint and one showing the primary selected datapoint.

This allows for easy comparison of model results on a datapoint to any generated
counterfactual datapoints, or any other datapoint from the loaded dataset.

![LIT datapoint comparison](./images/lit-datapoint-compare.png "LIT datapoint comparison")

## Slices

The *Slice Editor* allow users to create, edit, select, and delete slices. The
current selection can be saved as a slice by giving it a name and clicking
"Create slice". The slice list allows you to select any of the previously-saved
slices. This includes the "Starred" slice that is described above in the
[Main Toolbar](#main-toolbar) section.

The feature checkboxes enable the user to facet the data by input feature when
creating a slice. In the screenshot below, we are creating a new slice named
"interesting", and have selected the checkbox to facet by the "label" feature.
In this example, the "label" feature is a feature in the dataset that for each
datapoint describes which ground truth class it belongs to for some
classification task (either "0" or "1" for this binary classification example).
So, by creating a slice with this checkbox enabled, the tool will actually
create two slices: one named "interesting label:0" for datapoints with their
label set to 0, and one named "interesting label:1" for those with their label
set to "1".

![LIT slice controls](./images/lit-slices.png "LIT slice controls")

## Module Details

This section contains details on using and interacting with individual modules
that are built into LIT. Note that this list may not be complete and additional
modules can be created and used in LIT by clients.

All modules can be toggled to be shown full-screen through use of the
full-screen button in the top-right of each module.

### Embedding Projector

When using LIT with a model that returns embeddings (or activations) in addition
to predictions, the embedding projector will show all datapoints by their
embeddings projected down to 3 dimensions. This is useful for exploring and
understanding clusters of datapoints.

![LIT embeddings](./images/lit-embeddings.png "LIT embeddings"){w=500px align=center}

The specific embedding used to generate the projection can be selected in a
dropdown, along with the method of projection (either UMAP or PCA). An
additional drop-down allows changing of the datapoint feature used for the label
of each datapoint. The labels are shown on datapoint hover or click.

The visualization can be rotated through click-and-drag interaction, and panned
through control+click-and-drag. A datapoint can be selected with a click, or a
set of datapoints can be selected using a lasso through a shift+click-and-drag
interaction.

The color of the datapoints is controlled by the color settings in the selection
toolbar.

### Data Table

The data table shows all datapoints in a simple table. Datapoints can be
selected or unselected through a click. Shift+click allows selecting a set of
consecutive datapoints, and control+click allows selecting a set of individual
datapoints one at a time. Currently selected datapoints are highlighted with a
light blue background. The primary selected datapoint is highlighted with a
darker blue background. If a set of datapoints is currently selected, clicking
on a single datapoint in that set will change it to be the primary selected
datapoint without changing the overall set of selected datapoints.

The default sort order shows datapoints in the order they were loaded from the
dataset, but with newly-generated datapoints being placed directly below their
"source" datapoint, instead of at the end of the table.

The sort order can be changed to sort by columns through use of the up and down
arrows in the table header row. Additionally, the data table can be filtered
through text, regex, numerical ranges, and column-name prefixes using a global
search box. The table can also be filtered by column through a text search using
the search buttons for each column in the header row. All columns that have
filters set on them have their search button outlined. Clicking the **"x"**
button in the search box for a column will clear that column's filter.

The **"show selected"** checkbox toggles the data table to only show the
datapoints that are currently selected.

The **"show generated"** checkbox toggles the data table to only show generated
datapoints, that is, the datapoints that have been added through modules such as
the *Datapoint Editor* or the *Counterfactual Generators*.

The **"reset view"** button returns the data table to its standard, default
view.

A **"columns"** drop-down allows showing/hiding of specific columns to customize
what the data table shows. Model predictions can be added as columns through
this dropdown, but they are not shown in the data table by default, in order to
keep the table decluttered.

Column names that exceed the maximum length are truncated with an ellipsis to
the left, and can be viewed in their entirety when hovered over. Similarly,
table cells that exceed 3 lines of text are truncated with a Show More icon,
which can be clicked to view the full content. Text cells can be collapsed to
their default state using the **"reset view"** button.

The below data table shows one sorted by the "label" field, with the "sentence"
field being filtered to show only those datapoints that contain the word "film"
in them.

![LIT data table](./images/lit-datatable.png "LIT data table"){w=500px align=center}

A datapoint can be pinned to enable comparison by clicking the pin icon on the
left side of the datapoint's table entry when the datapoint is hovered over or
selected. A pinned datapoint can be unpinned by clicking on its pin icon again.
Similarly, a datapoint can be starred and unstarred by clicking the neighboring
star icon. Starred datapoints are tracked in an automatically generated Starred
slice for convenience.

You can also export data to CSV using the copy or download buttons in the bottom
right:

![LIT data table](./images/lit-datatable-export.png "LIT data table export controls"){w=400px align=center}

This will export all data in the current table view. To export only the
selection, use the "Show only selected" toggle. To include additional columns
such as model predictions, enable them from the "Columns" dropdown.

### Datapoint Editor

The datapoint editor shows the details of the primary selected datapoint, if one
is selected. Any field can be edited, and a new datapoint created with those
edits through the **"Add"** button. Any edit to an existing datapoint must be
saved as a new datapoint to be explored, to keep datapoints immutable for
simplicity of use.

When no datapoint is selected, the editor shows a blank datapoint that can be
filled out by hand to create a completely new datapoint.

Features shown with a "(\*)" next to their name are required as model input and
must be filled out to create a new datapoint. Other fields are optional.

![LIT datapoint editor](./images/lit-datapoint-editor.png "LIT datapoint editor"){w=500px align=center}

### Datapoint Generator

The datapoint generator module allows creation of new datapoints from all
currently-selected datapoints (or the entire dataset if no datapoints are
selected) through a set of counterfactual datapoint generators. These generators
are provided by the backend and all available generators will show up as buttons
in the module. Clicking one of these buttons causes the creation of new
datapoints that are displayed in a table inside the module and can be added to
the dataset either individually, or altogether, through the add buttons.

Generators built into LIT include:

*   **Scrambler**: Scrambles the words in a text feature randomly.
*   **Back-translation**: Translates a text feature into other languages and
    then back to the source language to create paraphrases of the initial text
    feature.
*   **Hotflip**: When analyzing a classification task and the model provides
    token-based gradients, this generator will change the token with the highest
    influence on the prediction to the token with the most opposite influence.
*   **Word replacer**: Provides a text box to define a comma-separated set of
    replacements to perform (such as "great -> terrible, hi -> hello").
    Counterfactual datapoints are created for any datapoint found that contains
    the source word, with it replaced with the provided result word. Word
    replacer also supports multiple targets per word with "|" separator. For
    example, "great -> terrible | bad" will produce two outputs where "great" is
    replaced with "terrible" and "bad".

The non-text fields in the generated datapoints can be edited before adding them
to the dataset. This is important in case some datapoint feature is no longer
correct after the counterfactual generation. For example, in a sentiment
classifier, if you used the word replacer generator to replace the word "good"
with "terrible" in the input "this movie is good", then you probably want to
change the ground truth sentiment of that datapoint from 1 to 0 before you add
it to your dataset for analysis.

![LIT datapoint generator](./images/lit-datapoint-generator.png "LIT datapoint generator")

### Metrics Table

The metrics table shows model metrics for each model in a table format. The
exact metric types are determined by the python metrics component that
calculates metrics given the model types being evaluated. These can include
measures such as accuracy (for classifiers), error (for regression tasks), and
BLEU score (for translation tasks). By default, the measures are calculated and
shown for the entire dataset, and also for the current selection. Additionally,
through the **"show slices"** checkbox, the metrics table can calculate and
display metrics for each saved slice as well.

There is also a **"Facet by"** set of dataset feature checkboxes; one checkbox
for each feature in the dataset that results can be faceted by. When one or more
of these are checked, the dataset (or current selection, if there is one) is
faceted into sub groups for each of the calculated buckets, and metrics are
displayed for those subsets of the datapoints of interest. This could be used,
for example, to compare metrics for a toxicity classifier, broken down by
gender, assuming the dataset has a categorical gender feature in it.

The below screenshot shows the metrics table with metrics for the entire
dataset, for the dataset faceted into datapoints with label 0 and with label 1,
and also for two named slices that have been created by a user.

![LIT metrics](./images/lit-metrics.png "LIT metrics")

### Confusion Matrix

The confusion matrix buckets all datapoints from the dataset (or the current
selection, if one is made) into buckets in a 2D matrix. This is normally used to
compare classification predictions on a model versus the ground truth classes of
the datapoints. In this case, the axes of the matrix are configurable to be set
to any categorical field in the dataset or return from a model. For example,
when comparing two models, the confusion matrix can be set up to show
agreements/disagreements between classifications in the two models, as opposed
to agreements/disagreements between one model's classifications and the ground
truth.

The individual cells and the row and column headers are all clickable to toggle
on/off selection of the datapoints in that cell or row or column. In this way,
the confusion matrix module can be used to select points of interest, such as
all false positives in a binary classification task, or all datapoints where two
models being compared disagree on classification.

![LIT confusion matrix](./images/lit-conf-matrix.png "LIT confusion matrix")

### Scalars

The scalars module shows a set of scatter or jitter plots, one for each scalar
output of a loaded model (such as a regression score, or a classification score
for a specific class). Each of them contains all datapoints in the dataset, laid
out horizontally by the score. For classification scores, the Y axis is a random
jitter of the data to better view all datapoints. For regression scores, where
ground truth is known, the Y axis is the error in the prediction (points below
the x-axis are under-predicted).

Datapoints can be selected either though clicking, or through lasso selection by
clicking and dragging.

The color of the datapoints is controlled by the color settings in the selection
toolbar.

For binary classification tasks, this module also contains a threshold slider in
order to change the positive classification threshold at which datapoints are
classified as being in the positive class. This slider value defaults to 0.5.

For multi-class classification tasks where a null index (default class) is set
in the model specifications, this module also contains a margin slider for the
non-default classes, to control how high a classification score must be in that
class before a datapoint is classified as that class as opposed to the default
class. The margin value defaults to 0, meaning the class with the highest score
is the class the datapoint is inferred to be.

![LIT prediction scores](./images/lit-pred-score.png "LIT prediction scores")

### Model Output

Model output modules show the result of a model on the primary selected
datapoint. The visuals of these modules depend on the model task being
performed. For a simple classification task, it will show the class scores from
the model, the predicted class, and, if ground truth is available in the
dataset, it will also show the ground truth classification.

![LIT classification results](./images/lit-classification-results.png "LIT classification results")

For structured prediction tasks like span labeling, a span graph module can
display all tagged spans returned by the model, along with a visualization of
the ground truth spans if one is available in the dataset.

![LIT structured prediction](./images/lit-structured-prediction.png "LIT structured prediction"){w=800px align=center}

### Salience Maps

Salience maps show the influence of different parts of inputs features on a
model's prediction on the primary selection. This module can contain multiple
methodologies for calculating this salience, depending on the capabilities of
the model being analyzed (e.x. if the model provides gradients, then
gradient-based token-wise salience can be calculated and displayed -- see
[adding models and data](api.md#adding-models-and-data) for more). The
background of each text piece is colored by the salience of that piece on the
prediction, and hovering on any piece will display the exact value calculated
for that piece.

There is an **"autorun"** button by each methodology on the right side of the
bar (the methodoloy name is on the left side). If it is checked, then that
calculation is made when a new primary datapoint is selected. If it is
unchecked, the calculation isn't made until it is checked. This can be valuable
so that expensive, long-running saliency calculations (such as LIME) aren't
performed on every datapoint selection, but only when explicitly asked for.

![LIT saliency maps](./images/lit-salience.png "LIT saliency maps")

## User Journeys

In this section, we explore some example user journeys and how LIT enables them.

### Sentiment Analysis

How well does a sentiment classifier handle negation? We load the development
set of the Stanford Sentiment Treebank, and use the search function in LITs
data table to find the 56 datapoints containing the word not. Looking at the
*Metrics* Table, we find that surprisingly, our BERT model gets 100% of these
correct! But we might want to know if this is truly robust. With LIT, we can
select individual datapoints and look for explanations. For example, take the
negative review, Its not the ultimate depression-era gangster movie.. As
shown in the screenshot below, salience maps suggest that not and ultimate
are important to the prediction.

We can verify this by creating modified inputs, using LITs *Datapoint Editor*.
Removing not gets a strongly positive prediction from Its the ultimate
depression-era gangster movie., while replacing ultimate to get Its not the
worst depression-era gangster movie. elicits a mildly positive score from our
model.

![Sentiment analysis](./images/lit-sentiment-analysis.png "Sentiment analysis")

### Sequence salience

Sequence salience generalizes token-based salience to text-to-text models,
allowing you to explain the impact of the prompt tokens on parts of the model
output.

Check out [here](components.md#sequence-salience) for more details on how to
navigate the Sequence Salience UI module.

================
File: docs/documentation/_static/scripts/furo.js.LICENSE.txt
================
/*!
 * gumshoejs v5.1.2 (patched by @pradyunsg)
 * A simple, framework-agnostic scrollspy script.
 * (c) 2019 Chris Ferdinandi
 * MIT License
 * http://github.com/cferdinandi/gumshoe
 */

================
File: lit_nlp/__init__.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

================
File: lit_nlp/api/__init__.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

================
File: lit_nlp/api/components.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Base classes for LIT backend components."""
import abc
from collections.abc import Sequence
import inspect
from typing import Any, Optional
from lit_nlp.api import dataset as lit_dataset
from lit_nlp.api import model as lit_model
from lit_nlp.api import types
JsonDict = types.JsonDict
IndexedInput = types.IndexedInput
MetricsDict = dict[str, float]
class Interpreter(metaclass=abc.ABCMeta):
  """Base class for LIT interpretation components."""
  def description(self) -> str:
    """Return a human-readable description of this component.
    Defaults to class docstring, but subclass may override this to be
    instance-dependent - for example, including the path from which the model
    was loaded.
    Returns:
      (string) A human-readable description for display in the UI.
    """
    return inspect.getdoc(self) or ''
  @abc.abstractmethod
  def run(self,
          inputs: list[JsonDict],
          model: lit_model.Model,
          dataset: lit_dataset.Dataset,
          model_outputs: Optional[list[JsonDict]] = None,
          config: Optional[JsonDict] = None):
    """Run this component, given a model and input(s)."""
    pass
  def is_compatible(self, model: lit_model.Model,
                    dataset: lit_dataset.Dataset) -> bool:
    """Return if interpreter is compatible with the dataset and model."""
    del dataset, model  # Unused in base class
    return True
  def config_spec(self) -> types.Spec:
    """Return the configuration spec for this component.
    If there are configuration options for this component that can be set in the
    UI, then list them and their type in this spec.
    Returns:
      Spec of configuration options. Defaults to an empty spec.
    """
    return {}
  def meta_spec(self) -> types.Spec:
    """Returns the metadata spec of this component.
    Can be used to represent information about what this interpreter returns,
    for use in the UI. For example, indicating if a saliency map is signed
    or unsigned which will affect the display of the results.
    Returns:
      A spec of what this component returns, to be used to drive the UI.
    """
    return {}
class Generator(Interpreter):
  """Base class for LIT generators."""
  def run(
      self,
      inputs: list[JsonDict],
      model: lit_model.Model,
      dataset: lit_dataset.Dataset,
      model_outputs: Optional[list[JsonDict]] = None,
      config: Optional[JsonDict] = None,
  ):
    del model_outputs
    return self.generate_all(inputs, model, dataset, config)
  def generate_all(self,
                   inputs: list[JsonDict],
                   model: lit_model.Model,
                   dataset: lit_dataset.Dataset,
                   config: Optional[JsonDict] = None) -> list[list[JsonDict]]:
    """Run generation on a set of inputs.
    Args:
      inputs: sequence of inputs, following model.input_spec()
      model: optional model to use to generate new examples.
      dataset: optional dataset which the current examples belong to.
      config: optional runtime config.
    Returns:
      list of list of new generated inputs, following model.input_spec()
    """
    output = []
    for ex in inputs:
      output.append(self.generate(ex, model, dataset, config))
    return output
  @abc.abstractmethod
  def generate(self,
               example: JsonDict,
               model: lit_model.Model,
               dataset: lit_dataset.Dataset,
               config: Optional[JsonDict] = None) -> list[JsonDict]:
    """Return a list of generated examples, for a single input."""
    pass
class Metrics(Interpreter, metaclass=abc.ABCMeta):
  """Base class for LIT metrics components."""
  # Required methods implementations from Interpreter base class
  def is_compatible(self, model: lit_model.Model,
                    dataset: lit_dataset.Dataset) -> bool:
    """True if the model and dataset support metric computation."""
    for pred_spec in model.output_spec().values():
      parent_key: Optional[str] = getattr(pred_spec, 'parent', None)
      parent_spec: Optional[types.LitType] = dataset.spec().get(parent_key)
      if self.is_field_compatible(pred_spec, parent_spec):
        return True
    return False
  def meta_spec(self):
    """A dict of MetricResults defining the metrics computed by this class."""
    raise NotImplementedError('Subclass should define its own meta spec.')
  # New methods introduced by this subclass
  def is_field_compatible(
      self,
      pred_spec: types.LitType,
      parent_spec: Optional[types.LitType]) -> bool:
    """True if compatible with the prediction field and its parent."""
    del pred_spec, parent_spec  # Unused in base class
    raise NotImplementedError('Subclass should implement field compatibility.')
  def compute(
      self,
      labels: Sequence[Any],
      preds: Sequence[Any],
      label_spec: types.LitType,
      pred_spec: types.LitType,
      config: Optional[JsonDict] = None,
      indices: Optional[Sequence[types.ExampleId]] = None,
      metas: Optional[Sequence[JsonDict]] = None) -> MetricsDict:
    """Compute metric(s) given labels and predictions."""
    raise NotImplementedError('Subclass should implement this, or override '
                              'run() directly.')
class Annotator(metaclass=abc.ABCMeta):
  """Base class for LIT annotator components.
  Annotators are for adding extra fields to datapoints, using a model to
  annotate datapoints given their feature values.
  """
  def __init__(self, name: str, annotator_model: lit_model.Model):
    """Annotator constructor.
    Args:
      name: prinable name of the annotator, for use in new dataset fields.
      annotator_model: model to use to create dataset annotations.
    """
    self._name = name
    self._annotator_model = annotator_model
  @abc.abstractmethod
  def annotate(self, inputs: list[dict[str, Any]],
               dataset: lit_dataset.Dataset,
               dataset_spec_to_annotate: Optional[types.Spec] = None):
    """Annotate the provided inputs.
    Args:
      inputs: sequence of inputs, modified in-place.
      dataset: dataset which the examples belong to.
      dataset_spec_to_annotate: spec to add new annotated fields to, modified
        in-place. If none provided, then no spec is updated.
    Returns:
      Updated spec for the dataset, given the new annotations.
    """
    pass

================
File: lit_nlp/api/dataset_test.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Tests for lit_nlp.lib.model."""
import os
import types
from absl.testing import absltest
from absl.testing import parameterized
from lit_nlp.api import dataset as lit_dataset
from lit_nlp.api import types as lit_types
import numpy as np
def get_testdata_path(fname):
  return os.path.join(os.path.dirname(__file__), 'testdata', fname)
class _EmptyInitTestDataset(lit_dataset.Dataset):
  def __init__(self):
    pass
class _PassThroughInitTestDataset(lit_dataset.Dataset):
  def __init__(self, *args, **kwargs):
    pass
class _InitWithArgsTestDataset(lit_dataset.Dataset):
  def __init__(self, path: str, max_examples: int = 200, max_qps: float = 1.0):
    pass
class DatasetTest(parameterized.TestCase):
  @parameterized.named_parameters(
      ('empty_init', _EmptyInitTestDataset),
      ('pass_thru_init', _PassThroughInitTestDataset),
  )
  def test_init_spec_empty(self, dataset: lit_dataset.Dataset):
    self.assertEmpty(dataset.init_spec())
  def test_init_spec_populated(self):
    self.assertEqual(
        _InitWithArgsTestDataset.init_spec(),
        {
            'path': lit_types.String(),
            'max_examples': lit_types.Integer(default=200, required=False),
            'max_qps': lit_types.Scalar(default=1.0, required=False),
        },
    )
  @parameterized.named_parameters(
      # All base Dataset classes are incompatible with automated spec inference
      # due to the complexity of their arguments, thus return None.
      ('dataset', lit_dataset.Dataset),
      ('none_dataset', lit_dataset.NoneDataset),
  )
  def test_init_spec_none(self, dataset: lit_dataset.Dataset):
    self.assertIsNone(dataset.init_spec())
  def test_remap(self):
    """Test remap method."""
    spec = {
        'score': lit_types.Scalar(),
        'text': lit_types.TextSegment(),
    }
    datapoints = [
        {'score': 0, 'text': 'a'},
        {'score': 0, 'text': 'b'},
    ]
    dset = lit_dataset.Dataset(spec, datapoints)
    remap_dict = {'score': 'val', 'nothing': 'nada'}
    remapped_dset = dset.remap(remap_dict)
    self.assertIn('val', remapped_dset.spec())
    self.assertNotIn('score', remapped_dset.spec())
    self.assertEqual({'val': 0, 'text': 'a'}, remapped_dset.examples[0])
class InputHashTest(parameterized.TestCase):
  """Test to hash data correctly, not using _id or _meta fields."""
  @parameterized.named_parameters(
      dict(
          testcase_name='empty_example',
          example={},
          expected_hash='99914b932bd37a50b983c5e7c90ae93b',
      ),
      dict(
          testcase_name='one_field_example',
          example={'value': 1},
          expected_hash='1ff00094a5ba112cb7dd128e783d6803',
      ),
      dict(
          testcase_name='three_field_example',
          example={
              'parity': 'odd',
              'text': 'One',
              'value': 1,
          },
          expected_hash='25dd56cf3b51e8e2954575f88b2620ca',
      ),
      dict(
          testcase_name='has_id_field',
          example={
              'parity': 'odd',
              'text': 'One',
              'value': 1,
              '_id': 'some_random_id',
          },
          expected_hash='25dd56cf3b51e8e2954575f88b2620ca',
      ),
      dict(
          testcase_name='has_meta_field',
          example={
              'parity': 'odd',
              'text': 'One',
              'value': 1,
              '_meta': lit_types.InputMetadata(
                  added=None, parentId=None, source=None
              ),
          },
          expected_hash='25dd56cf3b51e8e2954575f88b2620ca',
      ),
      dict(
          testcase_name='has_id_and_meta_fields',
          example={
              'parity': 'odd',
              'text': 'One',
              'value': 1,
              '_id': 'some_random_id',
              '_meta': lit_types.InputMetadata(
                  added=None, parentId=None, source=None
              ),
          },
          expected_hash='25dd56cf3b51e8e2954575f88b2620ca',
      ),
  )
  def test_hash(
      self, example: lit_types.Input, expected_hash: lit_types.ExampleId
  ):
    input_hash = lit_dataset.input_hash(example)
    self.assertEqual(input_hash, expected_hash)
class IndexedDatasetTest(absltest.TestCase):
  _DATASET_SPEC: lit_types.Spec = {
      'parity': lit_types.CategoryLabel(vocab=['odd', 'even']),
      'text': lit_types.TextSegment(),
      'value': lit_types.Integer(),
  }
  def test_init_from_examples_without_ids(self):
    # This test ensures that IDs are computed and assigned to IndexedInput.id
    # and IndexedInput.data._id fields for examples wihtout IDs.
    examples: list[lit_types.JsonDict] = [
        {'parity': 'odd', 'text': 'one', 'value': 1},
        {'parity': 'even', 'text': 'two', 'value': 2},
        {'parity': 'odd', 'text': 'three', 'value': 3},
    ]
    dataset = lit_dataset.IndexedDataset(
        spec=self._DATASET_SPEC,
        examples=examples
    )
    for indexed_example, example, original in zip(
        dataset.indexed_examples, dataset.examples, examples
    ):
      self.assertIsInstance(example['_id'], str)
      self.assertEqual(indexed_example['id'], example['_id'])
      self.assertIsNotNone(example['_meta'])
      self.assertEqual(indexed_example['meta'], example['_meta'])
      self.assertEqual(indexed_example['data'], example)
      for key in original:
        self.assertEqual(example[key], original[key])
  def test_init_from_examples_with_ids(self):
    # This test ensures that IndexedInput.id is the same as
    # IndexedInput.data._id when initialized from examples with _id fields.
    examples: list[lit_types.JsonDict] = [
        {'parity': 'odd', 'text': 'one', 'value': 1, '_id': 'one-1-odd'},
        {'parity': 'even', 'text': 'two', 'value': 2, '_id': 'two-2-even'},
        {'parity': 'odd', 'text': 'three', 'value': 3, '_id': 'three-3-odd'},
    ]
    dataset = lit_dataset.IndexedDataset(
        spec=self._DATASET_SPEC,
        examples=examples
    )
    for indexed_example, example, original in zip(
        dataset.indexed_examples, dataset.examples, examples
    ):
      self.assertEqual(example['_id'], original['_id'])
      self.assertEqual(indexed_example['id'], original['_id'])
      self.assertIsNotNone(example['_meta'])
      self.assertEqual(indexed_example['meta'], example['_meta'])
      self.assertEqual(indexed_example['data'], example)
      for key in original:
        self.assertEqual(example[key], original[key])
  def test_init_from_indexed_examples_with_ids(self):
    # This tests initializing from fully compliant IndexedInputs.
    indexed_examples: list[lit_types.IndexedInput] = [
        lit_types.IndexedInput(
            data=types.MappingProxyType({
                'parity': 'odd',
                'text': 'one',
                'value': 1,
                '_id': 'one-1-odd',
            }),
            id='one-1-odd',
            meta=lit_types.InputMetadata(
                added=None, parentId=None, source=None
            ),
        ),
        lit_types.IndexedInput(
            data=types.MappingProxyType({
                'parity': 'even',
                'text': 'two',
                'value': 2,
                '_id': 'two-2-even',
            }),
            id='two-2-even',
            meta=lit_types.InputMetadata(
                added=None, parentId=None, source=None
            ),
        ),
        lit_types.IndexedInput(
            data=types.MappingProxyType({
                'parity': 'odd',
                'text': 'three',
                'value': 3,
                '_id': 'three-3-odd',
            }),
            id='three-3-odd',
            meta=lit_types.InputMetadata(
                added=None, parentId=None, source=None
            ),
        ),
    ]
    dataset = lit_dataset.IndexedDataset(
        spec=self._DATASET_SPEC,
        indexed_examples=indexed_examples
    )
    for indexed_example, example, original in zip(
        dataset.indexed_examples, dataset.examples, indexed_examples
    ):
      self.assertEqual(example['_id'], original['id'])
      self.assertEqual(indexed_example['id'], original['id'])
      self.assertEqual(indexed_example['meta'], example['_meta'])
      for key in (original_data := original['data']):
        self.assertEqual(example[key], original_data[key])
        self.assertEqual(indexed_example['data'][key], original_data[key])
  def test_init_from_indexed_examples_without_ids(self):
    # This test represents the case where Legacy LIT saved data has been
    # correctly initialized in an IndexedInput with a readonly view of the
    # IndexedInput.data property, but the IndexedInput.data does not have an
    # _id property.
    indexed_examples: list[lit_types.IndexedInput] = [
        lit_types.IndexedInput(
            data=types.MappingProxyType({
                'parity': 'odd',
                'text': 'one',
                'value': 1,
            }),
            id='one-1-odd',
            meta=lit_types.InputMetadata(
                added=None, parentId=None, source=None
            ),
        ),
        lit_types.IndexedInput(
            data=types.MappingProxyType({
                'parity': 'even',
                'text': 'two',
                'value': 2,
            }),
            id='two-2-even',
            meta=lit_types.InputMetadata(
                added=None, parentId=None, source=None
            ),
        ),
        lit_types.IndexedInput(
            data=types.MappingProxyType({
                'parity': 'odd',
                'text': 'three',
                'value': 3,
            }),
            id='three-3-odd',
            meta=lit_types.InputMetadata(
                added=None, parentId=None, source=None
            ),
        ),
    ]
    dataset = lit_dataset.IndexedDataset(
        spec=self._DATASET_SPEC,
        indexed_examples=indexed_examples
    )
    for indexed_example, example, original in zip(
        dataset.indexed_examples, dataset.examples, indexed_examples
    ):
      self.assertEqual(example['_id'], original['id'])
      self.assertEqual(indexed_example['id'], original['id'])
      self.assertEqual(indexed_example['meta'], example['_meta'])
      for key in (original_data := original['data']):
        self.assertEqual(example[key], original_data[key])
        self.assertEqual(indexed_example['data'][key], original_data[key])
  def test_init_from_pesudo_indexed_examples(self):
    # This test represents the case where Legacy LIT saved data is loaded via
    # load_lit_format() or some other some external process where the examples
    # are merely cast to IndexedInput. It ensure that all IndexedInput.data and
    # JsonDict example representations are readonly via MappingProxyType.
    indexed_examples = [
        {
            'data': {
                'parity': 'odd',
                'text': 'one',
                'value': 1,
            },
            'id': 'one-1-odd',
            'meta': {},
        },
        {
            'data': {
                'parity': 'even',
                'text': 'two',
                'value': 2,
            },
            'id': 'two-2-even',
            'meta': {},
        },
        {
            'data': {
                'parity': 'odd',
                'text': 'three',
                'value': 3,
            },
            'id': 'three-3-odd',
            'meta': {},
        },
    ]
    dataset = lit_dataset.IndexedDataset(
        spec=self._DATASET_SPEC,
        indexed_examples=indexed_examples
    )
    for indexed_example, example, original in zip(
        dataset.indexed_examples, dataset.examples, indexed_examples
    ):
      self.assertEqual(example['_id'], original['id'])
      self.assertEqual(indexed_example['id'], original['id'])
      self.assertEqual(indexed_example['meta'], example['_meta'])
      indexed_example_data = indexed_example['data']
      self.assertIsInstance(example, types.MappingProxyType)
      self.assertIsInstance(indexed_example_data, types.MappingProxyType)
      self.assertEqual(example, indexed_example_data)
      for key in (original_data := original['data']):
        self.assertEqual(example[key], original_data[key])
        self.assertEqual(indexed_example_data[key], original_data[key])
  def test_init_from_indexed_examples_with_inconsistent_ids(self):
    # This test ensures that the IndexedInput.id property supersedes the
    # JsonDict._id property if both are provided but their values are different.
    indexed_examples: list[lit_types.IndexedInput] = [
        lit_types.IndexedInput(
            data=types.MappingProxyType({
                'parity': 'odd',
                'text': 'one',
                'value': 1,
                '_id': 'odd-1-one',
            }),
            id='one-1-odd',
            meta=lit_types.InputMetadata(
                added=None, parentId=None, source=None
            ),
        ),
        lit_types.IndexedInput(
            data=types.MappingProxyType({
                'parity': 'even',
                'text': 'two',
                'value': 2,
                '_id': 'even-2-two',
            }),
            id='two-2-even',
            meta=lit_types.InputMetadata(
                added=None, parentId=None, source=None
            ),
        ),
        lit_types.IndexedInput(
            data=types.MappingProxyType({
                'parity': 'odd',
                'text': 'three',
                'value': 3,
                '_id': 'odd-3-three',
            }),
            id='three-3-odd',
            meta=lit_types.InputMetadata(
                added=None, parentId=None, source=None
            ),
        ),
    ]
    dataset = lit_dataset.IndexedDataset(
        spec=self._DATASET_SPEC,
        indexed_examples=indexed_examples
    )
    for indexed_example, example, original in zip(
        dataset.indexed_examples, dataset.examples, indexed_examples
    ):
      self.assertEqual(example['_id'], original['id'])
      self.assertEqual(indexed_example['id'], original['id'])
      self.assertEqual(indexed_example['meta'], example['_meta'])
      original_data = original['data']
      self.assertNotEqual(example['_id'], original_data['_id'])
      self.assertNotEqual(indexed_example['id'], original_data['_id'])
      for key in dataset.spec().keys():
        self.assertEqual(example[key], original_data[key])
        self.assertEqual(indexed_example['data'][key], original_data[key])
  def test_init_without_examples(self):
    dataset = lit_dataset.IndexedDataset(spec=self._DATASET_SPEC)
    self.assertEmpty(dataset.examples)
    self.assertEmpty(dataset.indexed_examples)
    self.assertEqual(dataset.spec(), self._DATASET_SPEC)
    self.assertIsNone(dataset.init_spec())
class DatasetLoadingTest(absltest.TestCase):
  """Test to read data from LIT JSONL format."""
  def setUp(self):
    super().setUp()
    self.data_spec = {
        'parity': lit_types.CategoryLabel(vocab=['odd', 'even']),
        'text': lit_types.TextSegment(),
        'value': lit_types.Integer(),
        'other_divisors': lit_types.SparseMultilabel(),
        'in_spanish': lit_types.TextSegment(),
        'embedding': lit_types.Embeddings(),
    }
    self.sample_examples = [
        {
            'parity': 'odd',
            'text': 'One',
            'value': 1,
            'other_divisors': [],
            'in_spanish': 'Uno',
        },
        {
            'parity': 'even',
            'text': 'Two',
            'value': 2,
            'other_divisors': [],
            'in_spanish': 'Dos',
        },
        {
            'parity': 'odd',
            'text': 'Three',
            'value': 3,
            'other_divisors': [],
            'in_spanish': 'Tres',
        },
        {
            'parity': 'even',
            'text': 'Four',
            'value': 4,
            'other_divisors': ['Two'],
            'in_spanish': 'Cuatro',
        },
        {
            'parity': 'odd',
            'text': 'Five',
            'value': 5,
            'other_divisors': [],
            'in_spanish': 'Cinco',
        },
        {
            'parity': 'even',
            'text': 'Six',
            'value': 6,
            'other_divisors': ['Two', 'Three'],
            'in_spanish': 'Seis',
        },
        {
            'parity': 'odd',
            'text': 'Seven',
            'value': 7,
            'other_divisors': [],
            'in_spanish': 'Siete',
        },
        {
            'parity': 'even',
            'text': 'Eight',
            'value': 8,
            'other_divisors': ['Two', 'Four'],
            'in_spanish': 'Ocho',
        },
        {
            'parity': 'odd',
            'text': 'Nine',
            'value': 9,
            'other_divisors': ['Three'],
            'in_spanish': 'Nueve',
        },
        {
            'parity': 'even',
            'text': 'Ten',
            'value': 10,
            'other_divisors': ['Two', 'Five'],
            'in_spanish': 'Diez',
        },
    ]
    # Add embeddings
    rand = np.random.RandomState(42)
    for ex in self.sample_examples:
      vec = rand.normal(0, 1, size=16)
      # Scale such that norm = value, for testing
      scaled = ex['value'] * vec / np.linalg.norm(vec)
      rounded = np.round(scaled, decimals=8)
      # Convert to regular list to avoid issues with assertEqual not correctly
      # handling NumPy array equality.
      ex['embedding'] = rounded.tolist()
    # Index data
    self.indexed_dataset = lit_dataset.IndexedDataset(
        spec=self.data_spec,
        examples=self.sample_examples,
    )
  def test_load_lit_format_unindexed(self):
    ds = lit_dataset.load_lit_format(
        get_testdata_path('count_examples.lit.jsonl')
    )
    self.assertEqual(self.data_spec, ds.spec())
    self.assertEqual(self.sample_examples, ds.examples)
  def test_load_lit_format_indexed(self):
    ds = lit_dataset.load_lit_format(
        get_testdata_path('count_examples.indexed.lit.jsonl'),
    )
    self.assertIsInstance(ds, lit_dataset.IndexedDataset)
    self.assertEqual(self.data_spec, ds.spec())
    self.assertEqual(self.indexed_dataset.indexed_examples, ds.indexed_examples)
    for original, loaded in zip(self.sample_examples, ds.examples):
      for key in self.indexed_dataset.spec().keys():
        self.assertEqual(original[key], loaded[key])
  def test_indexed_dataset_load(self):
    ds = self.indexed_dataset.load(
        get_testdata_path('count_examples.indexed.lit.jsonl')
    )
    self.assertIsInstance(ds, lit_dataset.IndexedDataset)
    self.assertEqual(self.data_spec, ds.spec())
    self.assertEqual(self.indexed_dataset.indexed_examples, ds.indexed_examples)
    for original, loaded in zip(self.sample_examples, ds.examples):
      for key in self.indexed_dataset.spec().keys():
        self.assertEqual(original[key], loaded[key])
  def test_write_roundtrip(self):
    tempdir = self.create_tempdir()
    output_base = os.path.join(tempdir.full_path, 'test_dataset.lit.jsonl')
    lit_dataset.write_examples(self.sample_examples, output_base)
    lit_dataset.write_spec(self.data_spec, output_base + '.spec')
    # Read back and compare contents
    ds = lit_dataset.load_lit_format(output_base)
    self.assertEqual(self.data_spec, ds.spec())
    self.assertEqual(self.sample_examples, ds.examples)
  def test_write_roundtrip_indexed(self):
    tempdir = self.create_tempdir()
    output_base = os.path.join(
        tempdir.full_path, 'test_dataset.indexed.lit.jsonl'
    )
    lit_dataset.write_examples(
        self.indexed_dataset.indexed_examples, output_base
    )
    lit_dataset.write_spec(self.data_spec, output_base + '.spec')
    # Read back and compare contents
    ds = lit_dataset.load_lit_format(output_base)
    self.assertIsInstance(ds, lit_dataset.IndexedDataset)
    self.assertEqual(self.data_spec, ds.spec())
    self.assertEqual(self.indexed_dataset.indexed_examples, ds.indexed_examples)
    for original, loaded in zip(self.sample_examples, ds.examples):
      for key in self.indexed_dataset.spec().keys():
        self.assertEqual(original[key], loaded[key])
if __name__ == '__main__':
  absltest.main()

================
File: lit_nlp/api/dataset.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Base classes for LIT models."""
from collections.abc import Callable, Mapping, Sequence
import hashlib
import glob
import inspect
import os
import random
import types
from typing import Optional, Union, cast
from absl import logging
from lit_nlp.api import types as lit_types
from lit_nlp.lib import serialize
from lit_nlp.lib import utils
ExampleId = lit_types.ExampleId
IdFnType = Callable[[lit_types.JsonDict], lit_types.ExampleId]
IndexedInput = lit_types.IndexedInput
JsonDict = lit_types.JsonDict
Spec = lit_types.Spec
LIT_FILE_EXTENSION = '.lit.jsonl'
LIT_SPEC_EXTENSION = '.spec'
INPUT_ID_FIELD = '_id'
INPUT_META_FIELD = '_meta'
INPUT_INTERNAL_FIELDS = (INPUT_ID_FIELD, INPUT_META_FIELD)
# This is used here and in caching.py, but we define here to avoid a circular
# dependency of dataset -> caching -> model -> dataset
def input_hash(example: lit_types.JsonDict) -> lit_types.ExampleId:
  """Create stable hash of an input example."""
  raw_example = {
      k: v for k, v in example.items()
      if k not in INPUT_INTERNAL_FIELDS
  }
  json_str = serialize.to_json(raw_example, simple=True, sort_keys=True)
  return lit_types.ExampleId(hashlib.md5(json_str.encode('utf-8')).hexdigest())
def write_examples(examples: Sequence[JsonDict], path: str):
  """Write examples to disk as LIT JSONL format."""
  with open(path, 'w') as fd:
    for ex in examples:
      fd.write(serialize.to_json(ex) + '\n')
def write_spec(spec: Spec, path: str):
  """Write spec to disk as LIT JSON format."""
  with open(path, 'w') as fd:
    fd.write(serialize.to_json(spec, indent=2))
class SliceWrapper(object):
  """Shim object to implement custom slicing via foo[a:b:c] rather than constructing a slice object explicitly."""
  def __init__(self, handler):
    self._handler = handler
  def __getitem__(self, slice_obj):
    return self._handler(slice_obj)
class Dataset(object):
  """Base class for LIT datasets."""
  _spec: Spec = {}
  _examples: list[JsonDict] = []
  _description: Optional[str] = None
  _base: Optional['Dataset'] = None
  def __init__(self,
               spec: Optional[Spec] = None,
               examples: Optional[list[JsonDict]] = None,
               description: Optional[str] = None,
               base: Optional['Dataset'] = None):
    """Base class constructor.
    This can derive from another dataset by passing the 'base' argument;
    if so it will pre-populate with those fields, and override only those
    specified individually as arguments.
    Args:
      spec: dataset spec
      examples: data examples (datapoints)
      description: optional human-readable description of this component
      base: optional base dataset to derive from
    """
    self._base = base
    if self._base is not None:
      self._examples = self._base.examples
      self._spec = self._base.spec()
      self._description = self._base.description()
      # In case user child class requires the instance to convert examples
      # this makes sure the user class is preserved. We cannot do this below
      # as the default method is static and does not require instance.
      self.bytes_from_lit_example = self._base.bytes_from_lit_example
      self.lit_example_from_bytes = self._base.lit_example_from_bytes
    # Override from direct arguments.
    self._examples = examples if examples is not None else self._examples
    self._spec = spec or self._spec
    self._description = description or self._description
  def description(self) -> str:
    """Return a human-readable description of this component.
    Defaults to class docstring, but subclass may override this (or simply set
    self._description) to be instance-dependent - for example, including the
    path from which the data was loaded.
    Returns:
      (string) A human-readable description for display in the UI.
    """
    return self._description or inspect.getdoc(self) or ''  # pytype: disable=bad-return-type
  @classmethod
  def init_spec(cls) -> Optional[lit_types.Spec]:
    """Attempts to infer a Spec describing a Dataset's constructor parameters.
    The Dataset base class attempts to infer a Spec for the constructor using
    `lit_nlp.api.types.infer_spec_for_func()`.
    If successful, this function will return a `dict[str, LitType]`. If
    unsucessful (i.e., the inferencer raises a `TypeError` because it encounters
    a parameter that it not supported by `infer_spec_for_func()`), this function
    will return None, log a warning describing where and how the inferencing
    failed, and LIT users **will not** be able to load new instances of this
    Dataset from the UI.
    Returns:
      A Spec representation of the Dataset's constructor, or None if a Spec
      could not be inferred.
    """
    try:
      spec = lit_types.infer_spec_for_func(cls.__init__)
    except TypeError as e:
      spec = None
      logging.warning(
          "Unable to infer init spec for dataset '%s'. %s", cls.__name__, str(e)
      )
    return spec
  def load(self, path: str):
    """Load and return additional previously-saved datapoints for this dataset.
    Args:
      path: The path to the persisted datapoint file.
    Returns:
      (Dataset) A dataset containing the loaded data.
    """
    if self._base is not None:
      return self._base.load(path)
    pass
  def save(self, examples: list[IndexedInput], path: str):
    """Save newly-created datapoints to disk in a dataset-specific format.
    Subclasses should override this method if they wish to save new, persisted
    datapoints in their own file format in addition to the LIT-specific format
    they are already saved in.
    Args:
      examples: A list of datapoints to save.
      path: The path to save the datapoints to.
    Returns:
      (string) The path to the saved data, or None if unimplemented.
    """
    if self._base is not None:
      return self._base.save(examples, path)
    pass
  def spec(self) -> Spec:
    """Return a spec describing dataset elements."""
    return self._spec
  @property
  def examples(self) -> list[JsonDict]:
    """Return examples, in format described by spec."""
    return self._examples
  def __len__(self):
    return len(self.examples)
  @property
  def slice(self):
    """Syntactic sugar, allows dataset.slice[i:j] to return a new Dataset."""
    def _slicer(slice_obj):
      return Dataset(examples=self.examples[slice_obj], base=self)
    return SliceWrapper(_slicer)
  def sample(self, n, seed=42):
    """Return a new dataset with a random subset of examples."""
    rng = random.Random(seed)
    if n < len(self.examples):
      examples = rng.sample(self.examples, n)
    else:
      logging.warning(
          'Requested sample %d is larger than dataset size %d; returning full'
          ' dataset.',
          n,
          len(self.examples),
      )
      examples = list(self.examples)
    return Dataset(examples=examples, base=self)
  def filter(self, predicate: Callable[[JsonDict], bool]):
    selected_examples = list(filter(predicate, self.examples))
    return Dataset(examples=selected_examples, base=self)
  def shuffle(self, seed=42):
    """Return a new dataset with randomized example order."""
    # random.shuffle will shuffle in-place; use sample to make a new list.
    return self.sample(n=len(self), seed=seed)
  def remap(self, field_map: Mapping[str, str]):
    """Return a copy of this dataset with some fields renamed."""
    new_spec = utils.remap_dict(self.spec(), field_map)
    new_examples = [utils.remap_dict(ex, field_map) for ex in self.examples]
    return Dataset(new_spec, new_examples, base=self)
  @staticmethod
  def lit_example_from_bytes(input_bytes: bytes) -> Optional[JsonDict]:
    """Convert bytes representation to LIT example."""
    return serialize.from_json(input_bytes.decode('utf-8'))
  @staticmethod
  def bytes_from_lit_example(lit_example: JsonDict) -> bytes:
    """Convert LIT example to bytes representation."""
    return serialize.to_json(lit_example).encode('utf-8')
class IndexedDataset(Dataset):
  """Dataset with additional indexing information."""
  _index: dict[ExampleId, IndexedInput] = {}
  def _normalize_example(
      self, data: JsonDict, ex_id: ExampleId, meta: lit_types.InputMetadata
  ):
    return types.MappingProxyType(dict(data, _id=ex_id, _meta=meta))
  def index_inputs(
      self, examples: list[lit_types.JsonDict]
  ) -> list[IndexedInput]:
    """Create indexed versions of inputs."""
    indexed = []
    for example in examples:
      ex_id = example.get(INPUT_ID_FIELD, self.id_fn(example))
      ex_meta = example.get(
          INPUT_META_FIELD,
          lit_types.InputMetadata(added=None, parentId=None, source=None),
      )
      indexed.append(
          IndexedInput(
              data=types.MappingProxyType(
                  example | {INPUT_ID_FIELD: ex_id, INPUT_META_FIELD: ex_meta}
              ),
              id=ex_id,
              meta=ex_meta,
          )
      )
    return indexed
  def __init__(
      self,
      *args,
      id_fn: Optional[IdFnType] = None,
      indexed_examples: Optional[list[IndexedInput]] = None,
      **kw,
  ):
    # The base Dataset class will initialize self._examples in this call to
    # super().__init__(), which may or may not include the _id and _meta fields.
    super().__init__(*args, **kw)
    self.id_fn = id_fn if id_fn is not None else input_hash
    if indexed_examples:
      self._indexed_examples = indexed_examples
      # Ensure that all indexed exampls provide a readonly view of their data.
      for ie in self._indexed_examples:
        if not isinstance((ie_data := ie['data']), types.MappingProxyType):
          ie['data'] = self._normalize_example(ie_data, ie['id'], ie['meta'])
    else:
      self._indexed_examples = self.index_inputs(self._examples)
    self._examples = [
        self._normalize_example(ex['data'], ex['id'], ex.get('meta', {}))
        for ex in self._indexed_examples
    ]
    self._index = {ex['id']: ex for ex in self._indexed_examples}
  @property
  def slice(self):
    """Syntactic sugar, allows .slice[i:j] to return a new IndexedDataset."""
    def _slicer(slice_obj):
      return IndexedDataset(
          indexed_examples=self.indexed_examples[slice_obj],
          id_fn=self.id_fn,
          base=self
      )
    return SliceWrapper(_slicer)
  @classmethod
  def index_all(cls, datasets: Mapping[str, Dataset], id_fn: IdFnType):
    """Convenience function to convert a dict of datasets."""
    return {name: cls(base=ds, id_fn=id_fn) for name, ds in datasets.items()}
  @property
  def indexed_examples(self) -> Sequence[IndexedInput]:
    return self._indexed_examples
  @property
  def index(self) -> Mapping[ExampleId, IndexedInput]:
    """Return a read-only view of the index."""
    return types.MappingProxyType(self._index)
  def save(self, examples: list[IndexedInput], path: str):
    """Save newly-created datapoints to disk.
    Args:
      examples: A list of datapoints to save.
      path: The path to save the datapoints to.
    Returns:
      (string) The file path of the saved datapoints.
    """
    # Attempt to save the datapoints using the base save method, which
    # datasets can override. Then also save in the lit json format and save
    # the spec as well.
    if not path.endswith(LIT_FILE_EXTENSION):
      if (base_dataset := self._base) is not None:
        base_dataset.save(examples, path)
      path += LIT_FILE_EXTENSION
    write_examples(examples, path)
    write_spec(self.spec(), path + LIT_SPEC_EXTENSION)
    return path
  def load(self, path: str):
    """Load and return additional previously-saved datapoints for this dataset.
    Args:
      path: The path to the persisted datapoint file.
    Returns:
      (IndexedDataset) A dataset containing the loaded data.
    """
    if not path.endswith(LIT_FILE_EXTENSION):
      # Try to load data using the base load method. If any data is
      # returned, then use that. Otherwise try loading the lit json extension
      # data format.
      base_dataset = self._base
      new_dataset = base_dataset.load(path) if base_dataset else None
      if new_dataset is not None:
        description = (f'{len(new_dataset)} examples from '
                       f'{path}\n{self._base.description()}')
        return IndexedDataset(
            base=new_dataset, id_fn=self.id_fn, description=description)
      path += LIT_FILE_EXTENSION
    with open(path, 'r') as fd:
      examples = [
          cast(IndexedInput, serialize.from_json(line))
          for line in fd.readlines()
      ]
    # Load the side-by-side spec if it exists on disk.
    spec_path = path + LIT_SPEC_EXTENSION
    if os.path.exists(spec_path):
      with open(spec_path, 'r') as fd:
        spec = serialize.from_json(fd.read())
    else:
      spec = None
    description = f'{len(examples)} examples from {path}'
    if self._base is not None:
      description += '\n' + self._base.description()
    return IndexedDataset(
        base=self._base,
        indexed_examples=examples,
        spec=spec,
        description=description,
        id_fn=self.id_fn)
  def __hash__(self):
    return hash(tuple([ex['id'] for ex in self._indexed_examples]))
  def __eq__(self, other):
    self_ids = [ex['id'] for ex in self._indexed_examples]
    other_ids = [ex['id'] for ex in other._indexed_examples]
    return self_ids == other_ids
def load_lit_format(
    path: str, *args, id_fn=input_hash, **kw
) -> Union[Dataset, IndexedDataset]:
  """Load data from LIT jsonl format."""
  with open(path + LIT_SPEC_EXTENSION, 'r') as fd:
    spec = serialize.from_json(fd.read())
  with open(path, 'r') as fd:
    examples = [serialize.from_json(line) for line in fd.readlines()]
  first_example_keys = set(ex.keys() if (ex := examples[0]) else [])
  # TODO(b/294233896): remove this once input representations are consolidated.
  if first_example_keys.issuperset({'id', 'data'}):
    return IndexedDataset(
        spec=spec,
        indexed_examples=cast(list[lit_types.IndexedInput], examples),
        id_fn=id_fn,
        *args,
        **kw,
    )
  else:
    return Dataset(spec=spec, examples=examples, *args, **kw)
# TODO(b/202210900): Remove "NoneDataset" once the LIT front-end constructs its
# own "NoneDataset" equivalent.
class NoneDataset(Dataset):
  """Empty dataset, with fields as the union of model specs."""
  def __init__(self, models):  # pylint: disable=super-init-not-called
    self._examples = []
    self._models = models
  def spec(self):
    combined_spec = {}
    for _, model in self._models.items():
      req_inputs = {k: v for (k, v) in model.input_spec().items() if v.required}
      combined_spec = utils.combine_specs(combined_spec, req_inputs)
    return combined_spec

================
File: lit_nlp/api/dtypes.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Dataclasses for representing structured output.
Classes in this file should be used for actual input/output data,
rather than in spec() metadata.
These classes can replace simple dicts or namedtuples, with two major
advantages:
- Type-checking (via pytype) doesn't work for dict fields, but does work for
  these dataclasses.
- Performance and memory use may be better, due to the use of __slots__
See the documentation for attr.s (https://www.attrs.org/) for more details.
Classes inheriting from DataTuple will be handled by serialize.py, and available
on the frontend as corresponding JavaScript objects.
"""
import abc
from collections.abc import Sequence
from typing import Any, Optional, Union
import attr
JsonDict = dict[str, Any]
class EnumSerializableAsValues(object):
  """Dummy class to mark that an enum's members should be serialized as their values."""
  pass
@attr.s(auto_attribs=True, frozen=True, slots=True)
class DataTuple(metaclass=abc.ABCMeta):
  """Simple dataclasses.
  These are intended to be used for actual data, such as returned by
  dataset.examples and model.predict().
  Contrast with LitType and descendants, which are used in model and dataset
  /specs/ to represent types and metadata.
  """
  def to_json(self) -> JsonDict:
    """Used by serialize.py."""
    d = attr.asdict(self, recurse=False)
    d['__class__'] = 'DataTuple'
    d['__name__'] = self.__class__.__name__
    return d
  @staticmethod
  def from_json(d: JsonDict):
    """Used by serialize.py."""
    cls = globals()[d.pop('__name__')]  # class by name from this module
    return cls(**d)
@attr.s(auto_attribs=True, frozen=True, slots=True)
class SpanLabel(DataTuple):
  """Dataclass for individual span label preds. Can use this in model preds."""
  start: int  # inclusive
  end: int  # exclusive
  label: Optional[str] = None
  align: Optional[str] = None  # name of field (segment) this aligns to
@attr.s(auto_attribs=True, frozen=True, slots=True)
class EdgeLabel(DataTuple):
  """Dataclass for individual edge label preds. Can use this in model preds."""
  span1: tuple[int, int]  # inclusive, exclusive
  span2: tuple[int, int]  # inclusive, exclusive
  label: Union[str, int, float]
@attr.s(auto_attribs=True, frozen=True, slots=True)
class AnnotationCluster(DataTuple):
  """Dataclass for annotation clusters, which may span multiple segments."""
  label: str
  spans: list[SpanLabel]
  score: Optional[float] = None
  def to_json(self) -> JsonDict:
    """Override serialization to properly convert nested objects."""
    d = super().to_json()
    d['spans'] = [s.to_json() for s in d['spans']]
    return d
# TODO(b/196886684): document API for salience interpreters.
@attr.s(auto_attribs=True, frozen=True, slots=True)
class TokenSalience(DataTuple):
  """Dataclass for a salience map over tokens."""
  tokens: Sequence[str]
  salience: Sequence[float]  # parallel to tokens
@attr.s(auto_attribs=True, frozen=True, slots=True)
class FeatureSalience(DataTuple):
  """Dataclass for a salience map over categorical and/or scalar features."""
  salience: dict[str, float]
@attr.s(auto_attribs=True, frozen=True, slots=True)
class FrameSalience(DataTuple):
  """Dataclass for a salience map over image frames in a video."""
  # A map of salience score and image bytes string by frame number
  salience: dict[str, tuple[float, Sequence[str]]]
# TODO(b/196886684): document API for salience interpreters.
@attr.s(auto_attribs=True, frozen=True, slots=True)
class SequenceSalienceMap(DataTuple):
  """Dataclass for a salience map over a target sequence."""
  tokens_in: list[str]
  tokens_out: list[str]
  # <float>[num_tokens_out, num_tokens_in + num_tokens_out]
  salience: Sequence[Sequence[float]]  # usually, a np.ndarray
@attr.s(auto_attribs=True, frozen=True, slots=True)
class RegressionResult(DataTuple):
  """Dataclass for regression interpreter result."""
  score: float
  error: Optional[float]
  squared_error: Optional[float]
@attr.s(auto_attribs=True, frozen=True, slots=True)
class ClassificationResult(DataTuple):
  """Dataclass for classification interpreter result."""
  scores: list[float]
  predicted_class: str
  correct: Optional[bool]

================
File: lit_nlp/api/layout.py
================
# Copyright 2022 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Module names and type definitions for frontend UI layouts."""
from collections.abc import Mapping, Sequence
import enum
from typing import Optional, Union
import attr
from lit_nlp.api import dtypes
# pylint: disable=invalid-name
@enum.unique
class LitModuleName(dtypes.EnumSerializableAsValues, enum.Enum):
  """List of available frontend modules.
  Entries should map the TypeScript class name to the HTML element name,
  as declared in HTMLElementTagNameMap in the .ts file defining each LitModule.
  """
  # keep-sorted start
  AnnotatedTextGoldModule = 'annotated-text-gold-module'
  AnnotatedTextModule = 'annotated-text-module'
  ClassificationModule = 'classification-module'
  ConfusionMatrixModule = 'confusion-matrix-module'
  CurvesModule = 'curves-module'
  DataTableModule = 'data-table-module'
  DatapointEditorModule = 'datapoint-editor-module'
  DiveModule = 'dive-module'
  DocumentationModule = 'documentation-module'
  EmbeddingsModule = 'embeddings-module'
  FeatureAttributionModule = 'feature-attribution-module'
  GeneratedImageModule = 'generated-image-module'
  GeneratedTextModule = 'generated-text-module'
  GeneratorModule = 'generator-module'
  LegacySequenceSalienceModule = 'legacy-sequence-salience-module'
  MetricsModule = 'metrics-module'
  MultilabelModule = 'multilabel-module'
  PdpModule = 'pdp-module'
  RegressionModule = 'regression-module'
  SalienceClusteringModule = 'salience-clustering-module'
  SalienceMapModule = 'salience-map-module'
  ScalarModule = 'scalar-module'
  SequenceSalienceModule = 'sequence-salience-module'
  SimpleDataTableModule = 'simple-data-table-module'
  # Simplified, non-replicating version of Datapoint Editor
  SimpleDatapointEditorModule = 'simple-datapoint-editor-module'
  # Non-replicating version of Datapoint Editor
  SingleDatapointEditorModule = 'single-datapoint-editor-module'
  TCAVModule = 'tcav-module'
  ThresholderModule = 'thresholder-module'
  TrainingDataAttributionModule = 'tda-module'
  # keep-sorted end
  def __call__(self, **kw):
    return ModuleConfig(self.value, **kw)
# LINT.IfChange
# TODO(lit-dev): consider making modules subclass this instead of LitModuleName.
@attr.s(auto_attribs=True)
class ModuleConfig(dtypes.DataTuple):
  module: Union[str, LitModuleName]
  requiredForTab: bool = False
  # TODO(b/172979677): support title, duplicateAsRow, numCols,
  # and startMinimized.
# Most users should use LitModuleName, but we allow fallback to strings
# so that users can reference custom modules which are defined in TypeScript
# but not included in the LitModuleName enum above.
# If a string is used, it should be the HTML element name, like foo-bar-module.
LitModuleList = Sequence[Union[str, LitModuleName, ModuleConfig]]
# Keys are names of tabs, and values are names of LitModule HTML elements, e.g.,
# data-table-module for the DataTableModule class.
LitTabGroupLayout = Mapping[str, LitModuleList]
@attr.s(auto_attribs=True)
class LayoutSettings(dtypes.DataTuple):
  hideToolbar: bool = False
  mainHeight: int = 45
  leftWidth: int = 50
  centerPage: bool = False
@attr.s(auto_attribs=True)
class LitCanonicalLayout(dtypes.DataTuple):
  """Frontend UI layout; should match client/lib/types.ts."""
  upper: LitTabGroupLayout
  lower: LitTabGroupLayout = attr.ib(factory=dict)
  left: LitTabGroupLayout = attr.ib(factory=dict)
  layoutSettings: LayoutSettings = attr.ib(factory=LayoutSettings)
  description: Optional[str] = None
  def to_json(self) -> dtypes.JsonDict:
    """Override serialization to properly convert nested objects."""
    # Not invertible, but these only go from server -> frontend anyway.
    return attr.asdict(self, recurse=True)
LitComponentLayouts = Mapping[str, LitCanonicalLayout]
# pylint: enable=invalid-name
# LINT.ThenChange(../client/lib/types.ts)
##
# Common layout definitions.
modules = LitModuleName  # pylint: disable=invalid-name
MODEL_PREDS_MODULES = (
    modules.ClassificationModule,
    modules.MultilabelModule,
    modules.RegressionModule,
    modules.GeneratedTextModule,
    modules.AnnotatedTextGoldModule,
    modules.AnnotatedTextModule,
    modules.GeneratedImageModule,
)
DEFAULT_MAIN_GROUP = (
    modules.DataTableModule,
    modules.DatapointEditorModule,
)
##
# A "simple demo server" layout.
SIMPLE_LAYOUT = LitCanonicalLayout(
    upper={
        'Editor': [
            modules.DocumentationModule,
            modules.SimpleDatapointEditorModule,
        ],
        'Examples': [modules.SimpleDataTableModule],
    },
    lower={
        'Predictions': list(MODEL_PREDS_MODULES),
        'Salience': [
            *MODEL_PREDS_MODULES,
            modules.SalienceMapModule(requiredForTab=True),
        ],
        'Sequence Salience': [
            *MODEL_PREDS_MODULES,
            modules.LegacySequenceSalienceModule(requiredForTab=True),
        ],
        'Influence': [modules.TrainingDataAttributionModule],
    },
    layoutSettings=LayoutSettings(
        hideToolbar=True,
        mainHeight=30,
        centerPage=True,
    ),
    description=(
        'A basic layout just containing a datapoint creator/editor, the '
        'predictions, and the data table. There are also some visual '
        'simplifications: the toolbar is hidden, and the modules are centered '
        'on the page rather than being full width.'
    ),
)
THREE_PANEL_LAYOUT = LitCanonicalLayout(
    left={
        'Tabular Exploration': [modules.DataTableModule],
        'Current Example': [modules.DatapointEditorModule],
        'Visual Exploration': [modules.DiveModule],
        'Embeddings': [modules.EmbeddingsModule],
        'Documentation': [modules.DocumentationModule],
    },
    upper={
        'Predictions': MODEL_PREDS_MODULES,
        'Current Example': [modules.DatapointEditorModule],
        'Counterfactuals': [modules.GeneratorModule],
    },
    lower={
        'Metrics': [
            modules.MetricsModule,
            modules.ConfusionMatrixModule,
            modules.ThresholderModule,
        ],
        'Charts': [
            modules.ScalarModule,
            modules.PdpModule,
            modules.CurvesModule,
        ],
        'Explanations': [
            modules.SalienceMapModule,
            modules.LegacySequenceSalienceModule,
            modules.FeatureAttributionModule,
        ],
        'Clustering': [modules.SalienceClusteringModule],
        'Influence': [modules.TrainingDataAttributionModule],
        'TCAV': [modules.TCAVModule],
    },
    description=(
        'A three-panel layout with tools for exploring data in the aggregate or'
        ' per-example (on the left) or reviewing prediction results (upper'
        ' right) and performance characteristics, etc. (lower left).'
    ),
)
##
# A "kitchen sink" layout with maximum functionality.
STANDARD_LAYOUT = LitCanonicalLayout(
    upper={
        'Main': [
            modules.DocumentationModule,
            modules.EmbeddingsModule,
            *DEFAULT_MAIN_GROUP,
        ]
    },
    lower={
        'Predictions': [
            *MODEL_PREDS_MODULES,
            modules.ScalarModule,
            modules.PdpModule,
        ],
        'Explanations': [
            *MODEL_PREDS_MODULES,
            modules.SalienceMapModule,
            modules.LegacySequenceSalienceModule,
            modules.FeatureAttributionModule,
        ],
        'Salience Clustering': [modules.SalienceClusteringModule],
        'Metrics': [
            modules.MetricsModule,
            modules.ConfusionMatrixModule,
            modules.CurvesModule,
            modules.ThresholderModule,
        ],
        'Influence': [modules.TrainingDataAttributionModule],
        'Counterfactuals': [
            modules.GeneratorModule,
        ],
        'TCAV': [modules.TCAVModule],
    },
    description=(
        'The default LIT layout, which includes the data table and data point '
        'editor, the performance and metrics, predictions, explanations, and '
        'counterfactuals.'
    ),
)
DEFAULT_LAYOUTS = {
    'simple': SIMPLE_LAYOUT,
    'default': STANDARD_LAYOUT,
    'three_panel': THREE_PANEL_LAYOUT,
}

================
File: lit_nlp/api/model_test.py
================
# Copyright 2020 Google LLC
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Tests for lit_nlp.lib.model."""
from absl.testing import absltest
from absl.testing import parameterized
from lit_nlp.api import dataset as lit_dataset
from lit_nlp.api import model
from lit_nlp.api import types
class _CompatibilityTestModel(model.Model):
  """Dummy model for testing Model.is_compatible_with_dataset()."""
  def __init__(self, input_spec: types.Spec):
    self._input_spec = input_spec
  def input_spec(self) -> types.Spec:
    return self._input_spec
  def output_spec(self) -> types.Spec:
    return {}
  def predict(self, inputs: list[model.JsonDict]) -> list[model.JsonDict]:
    return []
class _BatchingTestModel(model.BatchedModel):
  """A model for testing batched predictions with a minibatch size of 3."""
  def __init__(self):
    self._count = 0
  @property
  def count(self):
    """Returns the number of times predict_minibatch has been called."""
    return self._count
  # LIT API implementation
  def max_minibatch_size(self):
    return 3
  def input_spec(self):
    return {"value": types.Scalar()}
  def output_spec(self):
    return {"scores": types.RegressionScore()}
  def predict_minibatch(self, inputs: list[model.JsonDict], **kw):
    assert len(inputs) <= self.max_minibatch_size()
    self._count += 1
    return map(lambda x: {"scores": x["value"]}, inputs)
class _SavedTestModel(model.Model):
  """A dummy model imitating saved model semantics for testing init_spec()."""
  def __init__(self, path: str, *args, compute_embs: bool = False, **kwargs):
    pass
  def input_spec(self) -> types.Spec:
    return {}
  def output_spec(self) -> types.Spec:
    return {}
  def predict(self, *args, **kwargs) -> list[types.JsonDict]:
    return []
class ModelTest(parameterized.TestCase):
  @parameterized.named_parameters(
      dict(
          testcase_name="full_match",
          input_spec={
              "text_a": types.TextSegment(),
              "text_b": types.TextSegment(),
          },
          dataset_spec={
              "text_a": types.TextSegment(),
              "text_b": types.TextSegment(),
          },
          expected=True,
      ),
      dict(
          testcase_name="mismatch",
          input_spec={
              "text_a": types.TextSegment(),
              "text_b": types.TextSegment(),
          },
          dataset_spec={
              "premise": types.TextSegment(),
              "hypothesis": types.TextSegment(),
          },
          expected=False,
      ),
      dict(
          testcase_name="extra_field",
          input_spec={
              "text_a": types.TextSegment(),
              "text_b": types.TextSegment(),
          },
          dataset_spec={
              "text_a": types.TextSegment(),
              "text_b": types.TextSegment(),
              "label": types.CategoryLabel(vocab=["0", "1"]),
          },
          expected=True,
      ),
      dict(
          testcase_name="optionals",
          input_spec={
              "text": types.TextSegment(),
              "tokens": types.Tokens(parent="text", required=False),
              "label": types.CategoryLabel(vocab=["0", "1"], required=False),
          },
          dataset_spec={
              "text": types.TextSegment(),
              "label": types.CategoryLabel(vocab=["0", "1"]),
          },
          expected=True,
      ),
      dict(
          testcase_name="optionals_mismatch",
          input_spec={
              "text": types.TextSegment(),
              "tokens": types.Tokens(parent="text", required=False),
              "label": types.CategoryLabel(vocab=["0", "1"], required=False),
          },
          dataset_spec={
              "text": types.TextSegment(),
              # This label field doesn't match the one the model expects.
              "label": types.CategoryLabel(vocab=["foo", "bar"]),
          },
          expected=False,
      ),
  )
  def test_compatibility(self, input_spec: types.Spec, dataset_spec: types.Spec,
                         expected: bool):
    """Test spec compatibility between models and datasets."""
    dataset = lit_dataset.Dataset(spec=dataset_spec)
    ctm = _CompatibilityTestModel(input_spec)
    self.assertEqual(ctm.is_compatible_with_dataset(dataset), expected)
  @parameterized.named_parameters(
      dict(
          testcase_name="one_full_batch",
          inputs=[{"value": 1}, {"value": 2}, {"value": 3}],
          expected_outputs=[{"scores": 1}, {"scores": 2}, {"scores": 3}],
          expected_run_count=1,
      ),
      dict(
          testcase_name="one_partial_batch",
          inputs=[{"value": 1}],
          expected_outputs=[{"scores": 1}],
          expected_run_count=1,
      ),
      dict(
          testcase_name="multiple_full_batches",
          inputs=[{"value": 1}, {"value": 2}, {"value": 3},
                  {"value": 4}, {"value": 5}, {"value": 6}],
          expected_outputs=[{"scores": 1}, {"scores": 2}, {"scores": 3},
                            {"scores": 4}, {"scores": 5}, {"scores": 6}],
          expected_run_count=2,
      ),
      dict(
          testcase_name="multiple_partial_batches",
          inputs=[{"value": 1}, {"value": 2}, {"value": 3},
                  {"value": 4}],
          expected_outputs=[{"scores": 1}, {"scores": 2}, {"scores": 3},
                            {"scores": 4}],
          expected_run_count=2,
      ),
  )
  def test_batched_predict(self, inputs: list[model.JsonDict],
                           expected_outputs: list[model.JsonDict],
                           expected_run_count: int):
    """Tests predict() for a model with a batch size of 3."""
    # Note that TestModelBatched
    test_model = _BatchingTestModel()
    result = list(test_model.predict(inputs))
    self.assertListEqual(result, expected_outputs)
    self.assertEqual(len(result), len(inputs))
    self.assertEqual(test_model.count, expected_run_count)
  def test_init_spec_empty(self):
    self.assertEmpty(_BatchingTestModel.init_spec())
  def test_init_spec_populated(self):
    self.assertEqual(
        _SavedTestModel.init_spec(),
        {
            "path": types.String(),
            "compute_embs": types.Boolean(default=False, required=False),
        },
    )
  @parameterized.named_parameters(
      ("bad_args", _CompatibilityTestModel),
      # All ModelWrapper instances should return None, regardless of the model
      # the instance is wrapping.
      ("wrapper", model.ModelWrapper),
  )
  def test_init_spec_none(self, mdl: model.Model):
    self.assertIsNone(mdl.init_spec())
if __name__ == "__main__":
  absltest.main()

================
File: lit_nlp/api/model.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Base classes for LIT models."""
import abc
from collections.abc import Iterable, Iterator, Mapping
import inspect
import itertools
import multiprocessing.pool  # for ThreadPool
from typing import Optional, Union
from absl import logging
from lit_nlp.api import dataset as lit_dataset
from lit_nlp.api import types
from lit_nlp.lib import utils
import numpy as np
JsonDict = types.JsonDict
Spec = types.Spec
def maybe_copy_np(arr):
  """Decide if we should make a copy of an array in order to release memory.
  NumPy arrays may be views into other array objects, by which a small array can
  maintain a persistent pointer to a large block of memory that prevents it from
  being garbage collected. This can quickly lead to memory issues as such
  blocks accumulate during inference.
  Args:
    arr: a NumPy array
  Returns:
    arr, or a copy of arr
  """
  if not isinstance(arr, np.ndarray):
    return arr
  # If this is not a view of another array.
  if arr.base is None:
    return arr
  # Tensorflow provides a bridge to share memory between tensorflow and numpy
  # arrays. This looks like a view into an array but the base is a
  # tensorflow_wrapper not an array, so the view heuristics below don't work. We
  # can check for this case by checking is arr.base has the ndim attribute.
  # https://github.com/tensorflow/tensorflow/blob/6ed79e8429730c33dc894175da7a1849a8e3e57f/tensorflow/python/lib/core/ndarray_tensor_bridge.cc#L90
  if not hasattr(arr.base, 'ndim'):
    return np.copy(arr)
  # Heuristic to check if we should 'detach' this array from the parent blob.
  # We want to know if this array is a view that might leak memory.
  # The simplest check is if arr.base is larger than arr, but we don't want to
  # make unnecessary copies when this is just due to slicing along a batch,
  # because the other rows are likely still in use.
  # TODO(lit-dev): keep an eye on this, if we continue to have memory issues
  # we can make copies more aggressively.
  if arr.base.ndim > 1 and np.prod(arr.base.shape[1:]) > np.prod(arr.shape):
    return np.copy(arr)
  # If only a batch slice, reshape, or otherwise.
  return arr
def scrub_numpy_refs(output: JsonDict) -> JsonDict:
  """Scrub numpy pointers; see maybe_copy_np() and Model.predict()."""
  return {k: maybe_copy_np(v) for k, v in output.items()}
class Model(metaclass=abc.ABCMeta):
  """Base class for LIT models."""
  def description(self) -> str:
    """Return a human-readable description of this component.
    Defaults to class docstring, but subclass may override this to be
    instance-dependent - for example, including the path from which the model
    was loaded.
    Returns:
      (string) A human-readable description for display in the UI.
    """
    return inspect.getdoc(self) or ''
  def __str__(self) -> str:
    classname = self.__class__.__module__ + '.' + self.__class__.__qualname__
    indented_description = '  ' + self.description().replace('\n', '\n  ')
    return f'{classname}(...):\n{indented_description}'
  def _repr_pretty_(self, p, cycle):
    """Pretty-printing for IPython environments, both notebooks and repl."""
    if not cycle:
      p.text(str(self))
    else:
      p.text('...')
  @classmethod
  def init_spec(cls) -> Optional[Spec]:
    """Attempts to infer a Spec describing a Model's constructor parameters.
    The Model base class attempts to infer a Spec for the constructor using
    `lit_nlp.api.types.infer_spec_for_func()`.
    If successful, this function will return a `dict[str, LitType]`. If
    unsucessful (i.e., the inferencer raises a `TypeError` because it encounters
    a parameter that it not supported by `infer_spec_for_func()`), this function
    will return None, log a warning describing where and how the inferencing
    failed, and LIT users **will not** be able to load new instances of this
    model from the UI.
    Returns:
      A Spec representation of the Model's constructor, or None if a Spec could
      not be inferred.
    """
    try:
      spec = types.infer_spec_for_func(cls.__init__)
    except TypeError as e:
      spec = None
      logging.warning(
          "Unable to infer init spec for model '%s'. %s", cls.__name__, str(e)
      )
    return spec
  def is_compatible_with_dataset(self, dataset: lit_dataset.Dataset) -> bool:
    """Return true if this model is compatible with the dataset spec."""
    dataset_spec = dataset.spec()
    for key, field_spec in self.input_spec().items():
      if key in dataset_spec:
        # If the field is in the dataset, make sure it's compatible.
        if not dataset_spec[key].is_compatible(field_spec):
          return False
      else:
        # If the field isn't in the dataset, only allow if the model marks as
        # optional.
        if field_spec.required:
          return False
    return True
  @property
  def supports_concurrent_predictions(self):
    """Indcates support for multiple concurrent predict calls across threads.
    Defaults to false.
    Returns:
      (bool) True if the model can handle multiple concurrent calls to its
      `predict` method.
    """
    return False
  def load(self, path: str):
    """Load and return a new instance of this model loaded from a new path.
    By default this method does nothing. Models can override this method in
    order to allow dynamic model loading in LIT through the UI. Models
    overriding this method should use the provided path string and create and
    return a new instance of its model class.
    Args:
      path: The path to the persisted model information, used in model's
      construction.
    Returns:
      (Model) A model loaded with information from the provided path.
    """
    del path
    raise NotImplementedError('Model has no load method defined for dynamic '
                              'loading')
  @abc.abstractmethod
  def input_spec(self) -> types.Spec:
    """Return a spec describing model inputs."""
    return
  @abc.abstractmethod
  def output_spec(self) -> types.Spec:
    """Return a spec describing model outputs."""
    return
  def get_embedding_table(self) -> tuple[list[str], np.ndarray]:
    """Return the full vocabulary and embedding table.
    Implementing this is optional, but needed for some techniques such as
    HotFlip which use the embedding table to search over candidate words.
    Returns:
      (<string>[vocab_size], <float32>[vocab_size, emb_dim])
    """
    raise NotImplementedError('get_embedding_table() not implemented for ' +
                              self.__class__.__name__)
  @abc.abstractmethod
  def predict(self, inputs: Iterable[JsonDict], **kw) -> Iterable[JsonDict]:
    """Run prediction on a list of inputs and return the outputs."""
    pass
ModelMap = Mapping[str, Model]
class ModelWrapper(Model):
  """Wrapper for a LIT model.
  This class acts as an identity function, with pass-through implementations of
  the Model API. Subclasses of this can implement only those methods that need
  to be modified.
  """
  def __init__(self, model: Model):
    self._wrapped = model
  @property
  def wrapped(self):
    """Access the wrapped model."""
    return self._wrapped
  def description(self) -> str:
    return self.wrapped.description()
  @property
  def supports_concurrent_predictions(self):
    return self.wrapped.supports_concurrent_predictions
  def predict(
      self, inputs: Iterable[JsonDict], *args, **kw
  ) -> Iterable[JsonDict]:
    return self.wrapped.predict(inputs, *args, **kw)
  def load(self, path: str):
    """Load a new model and wrap it with this class."""
    new_model = self.wrapped.load(path)
    return self.__class__(new_model)
  def input_spec(self) -> types.Spec:
    return self.wrapped.input_spec()
  def output_spec(self) -> types.Spec:
    return self.wrapped.output_spec()
  ##
  # Special methods
  def get_embedding_table(self) -> tuple[list[str], np.ndarray]:
    return self.wrapped.get_embedding_table()
class BatchedModel(Model):
  """Generic base class for the batched model.
  Subclass needs to implement predict_minibatch() and optionally
  max_minibatch_size().
  """
  def max_minibatch_size(self) -> int:
    """Maximum minibatch size for this model."""
    return 1
  @property
  def supports_concurrent_predictions(self):
    return False
  @abc.abstractmethod
  def predict_minibatch(self, inputs: list[JsonDict]) -> list[JsonDict]:
    """Run prediction on a batch of inputs.
    Args:
      inputs: sequence of inputs, following model.input_spec()
    Returns:
      list of outputs, following model.output_spec()
    """
    pass
  def predict(self, inputs: Iterable[JsonDict], **kw) -> Iterable[JsonDict]:
    """Run prediction on a dataset.
    This uses minibatch inference for efficiency, but yields per-example output.
    This will also copy some NumPy arrays if they look like slices of a larger
    tensor. This adds some overhead, but reduces memory leaks by allowing the
    source tensor (which may be a large padded matrix) to be garbage collected.
    Args:
      inputs: iterable of input dicts
      **kw: additional kwargs passed to predict_minibatch()
    Returns:
      model outputs, for each input
    """
    results = self.batched_predict(inputs, **kw)
    results = (scrub_numpy_refs(res) for res in results)
    return results
  def batched_predict(
      self, inputs: Iterable[JsonDict], **kw
  ) -> Iterator[JsonDict]:
    """Internal helper to predict using minibatches."""
    minibatch_size = self.max_minibatch_size(**kw)
    minibatch = []
    for ex in inputs:
      if len(minibatch) < minibatch_size:
        minibatch.append(ex)
      if len(minibatch) >= minibatch_size:
        yield from self.predict_minibatch(minibatch, **kw)
        minibatch = []
    if len(minibatch) > 0:  # pylint: disable=g-explicit-length-test
      yield from self.predict_minibatch(minibatch, **kw)
class BatchedRemoteModel(Model):
  """Generic base class for remotely-hosted models.
  Implements concurrent request batching; subclass need only implement
  predict_minibatch() and max_minibatch_size().
  If subclass overrides __init__, it should be sure to call super().__init__()
  to set up the threadpool.
  """
  def __init__(self,
               max_concurrent_requests: int = 4,
               max_qps: Union[int, float] = 25):
    # Use a local thread pool for concurrent requests, so we can keep the server
    # busy during network transit time and local pre/post-processing.
    self._max_qps = max_qps
    self._pool = multiprocessing.pool.ThreadPool(max_concurrent_requests)
  def predict(
      self,
      inputs: Iterable[JsonDict],
      *unused_args,
      parallel=True,
      **unused_kwargs
  ) -> Iterator[JsonDict]:
    batches = utils.batch_iterator(
        inputs, max_batch_size=self.max_minibatch_size())
    batches = utils.rate_limit(batches, self._max_qps)
    if parallel:
      pred_batches = self._pool.imap(self.predict_minibatch, batches)
    else:  # don't use the threadpool; useful for debugging
      pred_batches = map(self.predict_minibatch, batches)
    return itertools.chain.from_iterable(pred_batches)
  def max_minibatch_size(self) -> int:
    """Maximum minibatch size for this model. Subclass can override this."""
    return 1
  @property
  def supports_concurrent_predictions(self):
    """Remote models can handle concurrent predictions by default."""
    return True
  @abc.abstractmethod
  def predict_minibatch(self, inputs: list[JsonDict]) -> list[JsonDict]:
    """Run prediction on a batch of inputs.
    Subclass should implement this.
    Args:
      inputs: sequence of inputs, following model.input_spec()
    Returns:
      list of outputs, following model.output_spec()
    """
    return
class ProjectorModel(BatchedModel, metaclass=abc.ABCMeta):
  """LIT Model API for dimensionality reduction."""
  ##
  # Training methods
  @abc.abstractmethod
  def fit_transform(self, inputs: Iterable[JsonDict]) -> list[JsonDict]:
    """For internal use by SciKit Learn-based models."""
    pass
  ##
  # LIT model API
  def input_spec(self):
    # 'x' denotes input features
    return {'x': types.Embeddings()}
  def output_spec(self):
    # 'z' denotes projected embeddings
    return {'z': types.Embeddings()}
  def max_minibatch_size(self, **unused_kw):
    return 1000

================
File: lit_nlp/api/types_test.py
================
"""Tests for types."""
from collections.abc import Callable
import os
from typing import Any, Optional, Union
from absl.testing import absltest
from absl.testing import parameterized
from etils import epath
from lit_nlp.api import dtypes
from lit_nlp.api import types
import numpy as np
class TypesTest(parameterized.TestCase):
  def test_inherit_parent_default_type(self):
    lit_type = types.StringLitType()
    self.assertIsInstance(lit_type.default, str)
  def test_inherit_parent_default_value(self):
    lit_type = types.SingleFieldMatcher(spec="dataset", types=["LitType"])
    self.assertIsNone(lit_type.default)
  def test_requires_parent_custom_properties(self):
    # TokenSalience requires the `signed` property of its parent class.
    with self.assertRaises(TypeError):
      _ = types.TokenSalience(**{"autorun": True})
  def test_inherit_parent_custom_properties(self):
    lit_type = types.TokenSalience(autorun=True, signed=True)
    self.assertIsNone(lit_type.default)
    lit_type = types.TokenGradients(
        grad_for="cls_emb", grad_target_field_key="grad_class")
    self.assertTrue(hasattr(lit_type, "align"))
    self.assertFalse(hasattr(lit_type, "not_a_property"))
  @parameterized.named_parameters(
      ("list[int]", [1, 2, 3], 1),
      ("np_array[int]", np.array([1, 2, 3]), 1),
      ("np_array[list[int]]", np.array([[1, 1], [2, 3]]), 2),
      ("np_array[list[int]]_2_dim", np.array([[1, 1], [2, 3]]), [2, 4]),
  )
  def test_tensor_ndim(self, value, ndim):
    emb = types.Embeddings()
    try:
      emb.validate_ndim(value, ndim)
    except ValueError:
      self.fail("Raised unexpected error.")
  @parameterized.named_parameters(
      ("ndim_wrong_size", [1, 2, 3], 2),
      ("ndim_wrong_type", np.array([[1, 1], [2, 3]]), [1]),
  )
  def test_tensor_ndim_errors(self, value, ndim):
    with self.assertRaises(ValueError):
      emb = types.Embeddings()
      emb.validate_ndim(value, ndim)
  @parameterized.named_parameters(
      ("boolean", types.Boolean(), True),
      ("embeddings_list[int]", types.Embeddings(), [1, 2]),
      ("embeddings_np_array", types.Embeddings(), np.array([1, 2])),
      ("image", types.ImageBytes(), "data:image/blah..."),
      ("scalar_float", types.Scalar(), 3.4),
      ("scalar_int", types.Scalar(), 3),
      ("scalar_numpy", types.Scalar(), np.int64(2)),
      ("text", types.TextSegment(), "hi"),
      ("tokens", types.Tokens(), ["a", "b"]),
  )
  def test_type_validate_input(self, lit_type: types.LitType, value: Any):
    spec = {"score": types.Scalar(), "text": types.TextSegment()}
    example = {}
    try:
      lit_type.validate_input(value, spec, example)
    except ValueError:
      self.fail("Raised unexpected error.")
  @parameterized.named_parameters(
      ("boolean_number", types.Boolean(), 3.14159),
      ("boolean_text", types.Boolean(), "hi"),
      ("embeddings_bool", types.Embeddings(), True),
      ("embeddings_number", types.Embeddings(), 3.14159),
      ("embeddings_text", types.Embeddings(), "hi"),
      ("image_bool", types.ImageBytes(), True),
      ("image_number", types.ImageBytes(), 3.14159),
      ("image_text", types.ImageBytes(), "hi"),
      ("scalar_text", types.Scalar(), "hi"),
      ("text_bool", types.TextSegment(), True),
      ("text_number", types.TextSegment(), 3.14159),
      ("tokens_bool", types.Tokens(), True),
      ("tokens_number", types.Tokens(), 3.14159),
      ("tokens_text", types.Tokens(), "hi"),
  )
  def test_type_validate_input_errors(self,
                                      lit_type: types.LitType,
                                      value: Any):
    spec = {"score": types.Scalar(), "text": types.TextSegment()}
    example = {}
    with self.assertRaises(ValueError):
      lit_type.validate_input(value, spec, example)
  @parameterized.named_parameters(
      dict(
          testcase_name="CategoryLabel",
          json_dict={
              "required": False,
              "annotated": False,
              "default": "",
              "vocab": ["0", "1"],
              "__name__": "CategoryLabel",
          },
          expected_type=types.CategoryLabel,
      ),
      dict(
          testcase_name="Embeddings",
          json_dict={
              "required": True,
              "annotated": False,
              "default": None,
              "__name__": "Embeddings",
          },
          expected_type=types.Embeddings,
      ),
      dict(
          testcase_name="Gradients",
          json_dict={
              "required": True,
              "annotated": False,
              "default": None,
              "align": None,
              "grad_for": "cls_emb",
              "grad_target_field_key": "grad_class",
              "__name__": "Gradients",
          },
          expected_type=types.Gradients,
      ),
      dict(
          testcase_name="MulticlassPreds",
          json_dict={
              "required": True,
              "annotated": False,
              "default": None,
              "vocab": ["0", "1"],
              "null_idx": 0,
              "parent": "label",
              "autosort": False,
              "threshold": None,
              "__name__": "MulticlassPreds",
          },
          expected_type=types.MulticlassPreds,
      ),
      dict(
          testcase_name="RegressionScore",
          json_dict={
              "required": True,
              "annotated": False,
              "min_val": 0,
              "max_val": 1,
              "default": 0,
              "step": 0.01,
              "parent": "label",
              "__name__": "RegressionScore",
          },
          expected_type=types.RegressionScore,
      ),
      dict(
          testcase_name="Scalar",
          json_dict={
              "required": True,
              "annotated": False,
              "min_val": 2,
              "max_val": 100,
              "default": 10,
              "step": 1,
              "__name__": "Scalar",
          },
          expected_type=types.Scalar,
      ),
      dict(
          testcase_name="TextSegment",
          json_dict={
              "required": True,
              "annotated": False,
              "default": "",
              "__name__": "TextSegment",
          },
          expected_type=types.TextSegment,
      ),
      dict(
          testcase_name="TokenEmbeddings",
          json_dict={
              "required": True,
              "annotated": False,
              "default": None,
              "align": "tokens_sentence",
              "__name__": "TokenEmbeddings",
          },
          expected_type=types.TokenEmbeddings,
      ),
      dict(
          testcase_name="Tokens",
          json_dict={
              "required": False,
              "annotated": False,
              "default": [],
              "parent": "sentence",
              "mask_token": None,
              "token_prefix": "##",
              "__name__": "Tokens",
          },
          expected_type=types.Tokens,
      ),
  )
  def test_from_json(self, json_dict: types.JsonDict,
                     expected_type: types.LitType):
    lit_type: types.LitType = types.LitType.from_json(json_dict)
    self.assertIsInstance(lit_type, expected_type)
    for key in json_dict:
      if key == "__name__":
        continue
      elif hasattr(lit_type, key):
        self.assertEqual(getattr(lit_type, key), json_dict[key])
      else:
        self.fail(f"Encountered unknown property {key} for type "
                  f"{lit_type.__class__.__name__}.")
  @parameterized.named_parameters(
      ("empty_dict", {}, KeyError),
      ("invalid_name_empty", {"__name__": ""}, NameError),
      ("invalid_name_none", {"__name__": None}, TypeError),
      ("invalid_name_number", {"__name__": 3.14159}, TypeError),
      ("invalid_type_name", {"__name__": "not_a_lit_type"}, NameError),
  )
  def test_from_json_errors(self, value: types.JsonDict, expected_error):
    with self.assertRaises(expected_error):
      _ = types.LitType.from_json(value)
  def test_type_validate_gentext_output(self):
    ds_spec = {
        "num": types.Scalar(),
        "text": types.TextSegment(),
    }
    out_spec = {
        "gentext": types.GeneratedText(parent="text"),
        "cands": types.GeneratedTextCandidates(parent="text")
    }
    example = {"num": 1, "text": "hi"}
    output = {"gentext": "test", "cands": [("hi", 4), ("bye", None)]}
    gentext = types.GeneratedText(parent="text")
    gentextcands = types.GeneratedTextCandidates(parent="text")
    try:
      gentext.validate_output("hi", out_spec, output, ds_spec, ds_spec, example)
      gentextcands.validate_output([("hi", 4), ("bye", None)], out_spec, output,
                                   ds_spec, ds_spec, example)
    except ValueError:
      self.fail("Raised unexpected error.")
    bad_gentext = types.GeneratedText(parent="num")
    self.assertRaises(ValueError, bad_gentext.validate_output, "hi", out_spec,
                      output, ds_spec, ds_spec, example)
    self.assertRaises(ValueError, gentextcands.validate_output,
                      [("hi", "wrong"), ("bye", None)], out_spec, output,
                      ds_spec, ds_spec, example)
    bad_gentextcands = types.GeneratedTextCandidates(parent="num")
    self.assertRaises(ValueError, bad_gentextcands.validate_output,
                      [("hi", 4), ("bye", None)], out_spec, output, ds_spec,
                      ds_spec, example)
  def test_type_validate_genurl(self):
    ds_spec = {
        "text": types.TextSegment(),
    }
    out_spec = {
        "genurl": types.GeneratedURL(align="cands"),
        "cands": types.GeneratedTextCandidates(parent="text")
    }
    example = {"text": "hi"}
    output = {"genurl": "https://blah", "cands": [("hi", 4), ("bye", None)]}
    genurl = types.GeneratedURL(align="cands")
    try:
      genurl.validate_output("https://blah", out_spec, output, ds_spec, ds_spec,
                             example)
    except ValueError:
      self.fail("Raised unexpected error.")
    self.assertRaises(ValueError, genurl.validate_output, 4,
                      out_spec, output, ds_spec, ds_spec, example)
    bad_genurl = types.GeneratedURL(align="wrong")
    self.assertRaises(ValueError, bad_genurl.validate_output, "https://blah",
                      out_spec, output, ds_spec, ds_spec, example)
  def test_tokentopk(self):
    ds_spec = {
        "text": types.TextSegment(),
    }
    out_spec = {
        "tokens": types.Tokens(),
        "preds": types.TokenTopKPreds(align="tokens")
    }
    example = {"text": "hi"}
    output = {"tokens": ["hi"], "preds": [[("one", .9), ("two", .4)]]}
    preds = types.TokenTopKPreds(align="tokens")
    try:
      preds.validate_output(
          [[("one", .9), ("two", .4)]], out_spec, output, ds_spec, ds_spec,
          example)
    except ValueError:
      self.fail("Raised unexpected error.")
    self.assertRaises(
        ValueError, preds.validate_output,
        [[("one", .2), ("two", .4)]], out_spec, output, ds_spec, ds_spec,
        example)
    self.assertRaises(
        ValueError, preds.validate_output,
        [["one", "two"]], out_spec, output, ds_spec, ds_spec, example)
    self.assertRaises(
        ValueError, preds.validate_output, ["wrong"], out_spec, output,
        ds_spec, ds_spec, example)
    bad_preds = types.TokenTopKPreds(align="preds")
    self.assertRaises(
        ValueError, bad_preds.validate_output,
        [[("one", .9), ("two", .4)]], out_spec, output, ds_spec, ds_spec,
        example)
  def test_regression(self):
    ds_spec = {
        "val": types.Scalar(),
        "text": types.TextSegment(),
    }
    out_spec = {
        "score": types.RegressionScore(parent="val"),
    }
    example = {"val": 2}
    output = {"score": 1}
    score = types.RegressionScore(parent="val")
    try:
      score.validate_output(1, out_spec, output, ds_spec, ds_spec, example)
    except ValueError:
      self.fail("Raised unexpected error.")
    self.assertRaises(ValueError, score.validate_output, "wrong",
                      out_spec, output, ds_spec, ds_spec, example)
    bad_score = types.RegressionScore(parent="text")
    self.assertRaises(ValueError, bad_score.validate_output, 1,
                      out_spec, output, ds_spec, ds_spec, example)
  def test_reference(self):
    ds_spec = {
        "text": types.TextSegment(),
        "val": types.Scalar(),
    }
    out_spec = {
        "scores": types.ReferenceScores(parent="text"),
    }
    example = {"text": "hi"}
    output = {"scores": [1, 2]}
    score = types.ReferenceScores(parent="text")
    try:
      score.validate_output([1, 2], out_spec, output, ds_spec, ds_spec, example)
      score.validate_output(np.array([1, 2]), out_spec, output, ds_spec,
                            ds_spec, example)
    except ValueError:
      self.fail("Raised unexpected error.")
    self.assertRaises(ValueError, score.validate_output, ["a"],
                      out_spec, output, ds_spec, ds_spec, example)
    bad_score = types.ReferenceScores(parent="val")
    self.assertRaises(ValueError, bad_score.validate_output, [1],
                      out_spec, output, ds_spec, ds_spec, example)
  def test_multiclasspreds(self):
    ds_spec = {
        "label": types.CategoryLabel(),
        "val": types.Scalar(),
    }
    out_spec = {
        "scores": types.MulticlassPreds(
            parent="label", null_idx=0, vocab=["a", "b"]),
    }
    example = {"label": "hi", "val": 1}
    output = {"scores": [1, 2]}
    score = types.MulticlassPreds(parent="label", null_idx=0, vocab=["a", "b"])
    try:
      score.validate_output([1, 2], out_spec, output, ds_spec, ds_spec, example)
      score.validate_output(np.array([1, 2]), out_spec, output, ds_spec,
                            ds_spec, example)
    except ValueError:
      self.fail("Raised unexpected error.")
    self.assertRaises(ValueError, score.validate_output, ["a", "b"],
                      out_spec, output, ds_spec, ds_spec, example)
    bad_score = types.MulticlassPreds(
        parent="label", null_idx=2, vocab=["a", "b"])
    self.assertRaises(ValueError, bad_score.validate_output, [1, 2],
                      out_spec, output, ds_spec, ds_spec, example)
    bad_score = types.MulticlassPreds(
        parent="val", null_idx=0, vocab=["a", "b"])
    self.assertRaises(ValueError, bad_score.validate_output, [1, 2],
                      out_spec, output, ds_spec, ds_spec, example)
  def test_annotations(self):
    ds_spec = {
        "text": types.TextSegment(),
    }
    out_spec = {
        "tokens": types.Tokens(),
        "spans": types.SpanLabels(align="tokens"),
        "edges": types.EdgeLabels(align="tokens"),
        "annot": types.MultiSegmentAnnotations(),
    }
    example = {"text": "hi"}
    output = {"tokens": ["hi"], "preds": [dtypes.SpanLabel(start=0, end=1)],
              "edges": [dtypes.EdgeLabel(span1=(0, 0), span2=(1, 1), label=0)],
              "annot": [dtypes.AnnotationCluster(label="hi", spans=[])]}
    spans = types.SpanLabels(align="tokens")
    edges = types.EdgeLabels(align="tokens")
    annot = types.MultiSegmentAnnotations()
    try:
      spans.validate_output(
          [dtypes.SpanLabel(start=0, end=1)], out_spec, output, ds_spec,
          ds_spec, example)
      edges.validate_output(
          [dtypes.EdgeLabel(span1=(0, 0), span2=(1, 1), label=0)], out_spec,
          output, ds_spec, ds_spec, example)
      annot.validate_output(
          [dtypes.AnnotationCluster(label="hi", spans=[])], out_spec,
          output, ds_spec, ds_spec, example)
    except ValueError:
      self.fail("Raised unexpected error.")
    self.assertRaises(
        ValueError, spans.validate_output, [1], out_spec, output, ds_spec,
        ds_spec, example)
    self.assertRaises(
        ValueError, edges.validate_output, [1], out_spec, output, ds_spec,
        ds_spec, example)
    self.assertRaises(
        ValueError, annot.validate_output, [1], out_spec, output, ds_spec,
        ds_spec, example)
    bad_spans = types.SpanLabels(align="edges")
    bad_edges = types.EdgeLabels(align="spans")
    self.assertRaises(
        ValueError, bad_spans.validate_output,
        [dtypes.SpanLabel(start=0, end=1)], out_spec, output, ds_spec, ds_spec,
        example)
    self.assertRaises(
        ValueError, bad_edges.validate_output,
        [dtypes.EdgeLabel(span1=(0, 0), span2=(1, 1), label=0)], out_spec,
        output, ds_spec, ds_spec, example)
  def test_gradients(self):
    ds_spec = {
        "text": types.TextSegment(),
        "target": types.CategoryLabel()
    }
    out_spec = {
        "tokens": types.Tokens(),
        "embs": types.Embeddings(),
        "grads": types.Gradients(align="tokens", grad_for="embs",
                                 grad_target_field_key="target")
    }
    example = {"text": "hi", "target": "one"}
    output = {"tokens": ["hi"], "embs": [.1, .2], "grads": [.1]}
    grads = types.Gradients(align="tokens", grad_for="embs",
                            grad_target_field_key="target")
    embs = types.Embeddings()
    try:
      grads.validate_output([.1], out_spec, output, ds_spec, ds_spec, example)
      embs.validate_output([.1, .2], out_spec, output, ds_spec, ds_spec,
                           example)
    except ValueError:
      self.fail("Raised unexpected error.")
    self.assertRaises(
        ValueError, grads.validate_output, ["bad"], out_spec, output, ds_spec,
        ds_spec, example)
    self.assertRaises(
        ValueError, embs.validate_output, ["bad"], out_spec, output, ds_spec,
        ds_spec, example)
    bad_grads = types.Gradients(align="text", grad_for="embs",
                                grad_target_field_key="target")
    self.assertRaises(
        ValueError, bad_grads.validate_output, [.1], out_spec, output, ds_spec,
        ds_spec, example)
    bad_grads = types.Gradients(align="tokens", grad_for="tokens",
                                grad_target_field_key="target")
    self.assertRaises(
        ValueError, bad_grads.validate_output, [.1], out_spec, output, ds_spec,
        ds_spec, example)
    bad_grads = types.Gradients(align="tokens", grad_for="embs",
                                grad_target_field_key="bad")
    self.assertRaises(
        ValueError, bad_grads.validate_output, [.1], out_spec, output, ds_spec,
        ds_spec, example)
  def test_tokenembsgrads(self):
    ds_spec = {
        "text": types.TextSegment(),
        "target": types.CategoryLabel()
    }
    out_spec = {
        "tokens": types.Tokens(),
        "embs": types.TokenEmbeddings(align="tokens"),
        "grads": types.TokenGradients(align="tokens", grad_for="embs",
                                      grad_target_field_key="target")
    }
    example = {"text": "hi", "target": "one"}
    output = {"tokens": ["hi"], "embs": np.array([[.1], [.2]]),
              "grads": np.array([[.1], [.2]])}
    grads = types.TokenGradients(align="tokens", grad_for="embs",
                                 grad_target_field_key="target")
    embs = types.TokenEmbeddings(align="tokens")
    try:
      grads.validate_output(np.array([[.1], [.2]]), out_spec, output, ds_spec,
                            ds_spec, example)
      embs.validate_output(np.array([[.1], [.2]]), out_spec, output, ds_spec,
                           ds_spec, example)
    except ValueError:
      self.fail("Raised unexpected error.")
    self.assertRaises(
        ValueError, grads.validate_output, np.array([.1, .2]), out_spec, output,
        ds_spec, ds_spec, example)
    self.assertRaises(
        ValueError, embs.validate_output, np.array([.1, .2]), out_spec, output,
        ds_spec, ds_spec, example)
    bad_embs = types.TokenEmbeddings(align="grads")
    self.assertRaises(
        ValueError, bad_embs.validate_output, np.array([[.1], [.2]]), out_spec,
        output, ds_spec, ds_spec, example)
  def test_attention(self):
    ds_spec = {
        "text": types.TextSegment(),
    }
    out_spec = {
        "tokens": types.Tokens(),
        "val": types.RegressionScore,
        "attn": types.AttentionHeads(align_in="tokens", align_out="tokens"),
    }
    example = {"text": "hi"}
    output = {"tokens": ["hi"], "attn": np.array([[[.1]], [[.2]]])}
    attn = types.AttentionHeads(align_in="tokens", align_out="tokens")
    try:
      attn.validate_output(np.array([[[.1]], [[.2]]]), out_spec, output,
                           ds_spec, ds_spec, example)
    except ValueError:
      self.fail("Raised unexpected error.")
    self.assertRaises(
        ValueError, attn.validate_output, np.array([.1, .2]), out_spec, output,
        ds_spec, ds_spec, example)
    bad_attn = types.AttentionHeads(align_in="tokens", align_out="val")
    self.assertRaises(
        ValueError, bad_attn.validate_output, np.array([[[.1]], [[.2]]]),
        out_spec, output, ds_spec, ds_spec, example)
    bad_attn = types.AttentionHeads(align_in="val", align_out="tokens")
    self.assertRaises(
        ValueError, bad_attn.validate_output, np.array([[[.1]], [[.2]]]),
        out_spec, output, ds_spec, ds_spec, example)
class InferSpecTarget(object):
  """A dummy class for testing infer_spec_for_func against a class."""
  def __init__(self, arg: bool = True):
    self._arg = arg
# The following is a series of identity functions used to test
# types.infer_spec_for_func(Callable[..., Any]). These need to exist in this way
# because other options (e.g., lambdas) do not support annotation, which is
# required to test infer_spec_for_func() success cases.
def _bool_param(param: bool = True) -> bool:
  return param
def _epath_pathlike_param(
    param: epath.PathLike = "/path/to/something",
) -> epath.PathLike:
  return param
def _float_param(param: float = 1.2345) -> float:
  return param
def _int_param(param: int = 1) -> int:
  return param
def _many_params(
    param_1: bool,
    param_2: Optional[float],
    param_3: int = 1,
    param_4: Optional[str] = None
) -> tuple[bool, Optional[float], int, Optional[str]]:
  return (param_1, param_2, param_3, param_4)
def _no_annotation(param="param") -> str:
  return param
def _no_annotation_or_default(param) -> Any:
  return param
def _no_args() -> Any:
  return {}
def _no_default(param: str) -> str:
  return param
def _object_param(param: object) -> object:
  return param
def _optional_bool_param(param: Optional[bool]) -> Optional[bool]:
  return param
def _optional_epath_pathlike_param(
    param: Optional[epath.PathLike],
) -> Optional[epath.PathLike]:
  return param
def _optional_float_param(param: Optional[float]) -> Optional[float]:
  return param
def _optional_int_param(param: Optional[int]) -> Optional[int]:
  return param
def _optional_os_pathlike_param(
    param: Optional[os.PathLike[str]],
) -> Optional[os.PathLike[str]]:
  return param
def _optional_scalar_param(
    param: Optional[Union[float, int]]
) -> Optional[Union[float, int]]:
  return param
def _optional_str_param(param: Optional[str]) -> Optional[str]:
  return param
def _os_pathlike_param(
    param: os.PathLike[str] = "/path/to/something",
) -> os.PathLike[str]:
  return param
def _scalar_param(param: Union[float, int]) -> Union[float, int]:
  return param
def _str_param(param: str = "str") -> str:
  return param
class InferSpecTests(parameterized.TestCase):
  @parameterized.named_parameters(
      dict(
          testcase_name="class",
          func=InferSpecTarget,
          expected_spec={"arg": types.Boolean(default=True, required=False)},
      ),
      dict(
          testcase_name="class_init",
          func=InferSpecTarget.__init__,
          expected_spec={"arg": types.Boolean(default=True, required=False)},
      ),
      dict(
          testcase_name="class_instance_init",
          func=InferSpecTarget().__init__,
          expected_spec={"arg": types.Boolean(default=True, required=False)},
      ),
      dict(
          testcase_name="empty_spec",
          func=_no_args,
          expected_spec={},
      ),
      dict(
          testcase_name="many_params",
          func=_many_params,
          expected_spec={
              "param_1": types.Boolean(required=True),
              "param_2": types.Scalar(required=False),
              "param_3": types.Integer(default=1, required=False),
              "param_4": types.String(default=None, required=False),
          },
      ),
      dict(
          testcase_name="no_annotation",
          func=_no_annotation,
          expected_spec={
              "param": types.String(default="param", required=False),
          },
      ),
      dict(
          testcase_name="no_default",
          func=_no_default,
          expected_spec={
              "param": types.String(default="", required=True),
          },
      ),
      dict(
          testcase_name="optional_bool",
          func=_optional_bool_param,
          expected_spec={
              "param": types.Boolean(required=False),
          },
      ),
      dict(
          testcase_name="optional_float",
          func=_optional_float_param,
          expected_spec={
              "param": types.Scalar(required=False),
          },
      ),
      dict(
          testcase_name="optional_int",
          func=_optional_int_param,
          expected_spec={
              "param": types.Integer(required=False),
          },
      ),
      dict(
          testcase_name="optional_scalar",
          func=_optional_scalar_param,
          expected_spec={
              "param": types.Scalar(required=False),
          },
      ),
      dict(
          testcase_name="optional_str",
          func=_optional_str_param,
          expected_spec={
              "param": types.String(required=False),
          },
      ),
      dict(
          testcase_name="single_bool",
          func=_bool_param,
          expected_spec={
              "param": types.Boolean(default=True, required=False),
          },
      ),
      dict(
          testcase_name="single_float",
          func=_float_param,
          expected_spec={
              "param": types.Scalar(default=1.2345, required=False),
          },
      ),
      dict(
          testcase_name="single_int",
          func=_int_param,
          expected_spec={
              "param": types.Integer(default=1, required=False),
          },
      ),
      dict(
          testcase_name="single_scalar",
          func=_scalar_param,
          expected_spec={
              "param": types.Scalar(required=True),
          },
      ),
      dict(
          testcase_name="single_str",
          func=_str_param,
          expected_spec={
              "param": types.String(default="str", required=False),
          },
      ),
      dict(
          testcase_name="etils_pathlike",
          func=_epath_pathlike_param,
          expected_spec={
              "param": types.String(
                  default="/path/to/something", required=False
              ),
          },
      ),
      dict(
          testcase_name="etils_optional_pathlike",
          func=_optional_epath_pathlike_param,
          expected_spec={
              "param": types.String(required=False),
          },
      ),
      dict(
          testcase_name="os_pathlike",
          func=_os_pathlike_param,
          expected_spec={
              "param": types.String(
                  default="/path/to/something", required=False
              ),
          },
      ),
      dict(
          testcase_name="os_optional_pathlike",
          func=_optional_os_pathlike_param,
          expected_spec={
              "param": types.String(required=False),
          },
      ),
  )
  def test_infer_spec_for_func(self, func: Callable[..., Any],
                               expected_spec: types.Spec):
    spec = types.infer_spec_for_func(func)
    self.assertEqual(spec, expected_spec)
  @parameterized.named_parameters(
      ("class_instance", InferSpecTarget()),
      ("lambda", lambda x: x),
      ("no_annotation_or_default", _no_annotation_or_default),
      ("not_a_callable", "not_a_callable"),
      ("unsupported_type", _object_param),
  )
  def test_infer_spec_for_func_errors(self, func: Any):
    self.assertRaises(TypeError, types.infer_spec_for_func, func)
if __name__ == "__main__":
  absltest.main()

================
File: lit_nlp/api/types.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Type classes for LIT inputs and outputs.
These are simple dataclasses used in model.input_spec() and model.output_spec()
to describe the semantics of the model outputs, while allowing clients to still
use flexible data structures.
These are used by the LIT framework to configure front-end components and to
enable different generation and visualization modules. For example, the input
spec allows LIT to automatically generate input forms for common types like text
segments or class labels, while the output spec describes how the model output
should be rendered.
"""
import abc
from collections.abc import Callable, Mapping, Sequence
import enum
import inspect
import math
import numbers
import os
from typing import Any, get_args, get_origin, NewType, Optional, TypedDict, Union
import attr
from etils import epath
from lit_nlp.api import dtypes
import numpy as np
JsonDict = Mapping[str, Any]
Input = NewType("Input", JsonDict)
ExampleId = NewType("ExampleId", str)
ScoredTextCandidates = Sequence[tuple[str, Optional[float]]]
TokenTopKPredsList = Sequence[ScoredTextCandidates]
NumericTypes = numbers.Number
class InputMetadata(TypedDict):
  added: Optional[bool]
  # pylint: disable=invalid-name
  parentId: Optional[ExampleId]   # Named to match TypeScript data structure
  # pylint: enable=invalid-name
  source: Optional[str]
class IndexedInput(TypedDict):
  data: JsonDict
  id: ExampleId
  meta: InputMetadata
##
# Base classes, for common functionality and type grouping.
@attr.s(auto_attribs=True, frozen=True, kw_only=True)
class LitType(metaclass=abc.ABCMeta):
  """Base class for LIT Types."""
  required: bool = True  # for input fields, mark if required by the model.
  annotated: bool = False  # If this type is created from an Annotator.
  show_in_data_table = True  # If true, show this info the data table.
  # TODO(lit-dev): Add defaults for all LitTypes
  default = None  # an optional default value for a given type.
  def validate_input(self, value: Any, spec: "Spec", example: Input):
    """Validate a dataset example's value against its spec in an example.
    Subtypes should override to validate a provided value and raise a ValueError
    if the value is not valid.
    Args:
      value: The value to validate against the specific LitType.
      spec: The spec of the dataset.
      example: The entire example of which the value is a part of.
    Raises:
      ValueError if validation fails.
    """
    pass
  def validate_output(self, value: Any, output_spec: "Spec",
                      output_dict: JsonDict, input_spec: "Spec",
                      dataset_spec: "Spec", input_example: Input):
    """Validate a model output value against its spec and input example.
    Subtypes should override to validate a provided value and raise a ValueError
    if the value is not valid.
    Args:
      value: The value to validate against the specific LitType.
      output_spec: The output spec of the model.
      output_dict: The entire model output for the example.
      input_spec: The input spec of the model.
      dataset_spec: The dataset spec.
      input_example: The example from which the output value is returned.
    Raises:
      ValueError if validation fails.
    """
    del output_spec, output_dict, dataset_spec
    # If not overwritten by a LitType, then validate it as an input to re-use
    # simple validation code.
    self.validate_input(value, input_spec, input_example)
  def is_compatible(self, other):
    """Check equality, ignoring some fields."""
    # We allow this class to be a subclass of the other.
    if not isinstance(self, type(other)):
      return False
    d1 = attr.asdict(self)
    d1.pop("required", None)
    d2 = attr.asdict(other)
    d2.pop("required", None)
    return d1 == d2
  def to_json(self) -> JsonDict:
    """Used by serialize.py."""
    d = attr.asdict(self)
    d["__class__"] = "LitType"
    d["__name__"] = self.__class__.__name__
    return d
  @staticmethod
  def from_json(d: JsonDict):
    """Used by serialize.py.
    Args:
      d: The JSON Object-like dictionary to attempt to parse.
    Returns:
      An instance of a LitType subclass defined by the contents of `d`.
    Raises:
      KeyError: If `d` does not have a `__name__` property.
      NameError: If `d["__name__"]` is not a `LitType` subclass.
      TypeError: If `d["__name__"]` is not a string.
    """
    try:
      type_name = d["__name__"]
    except KeyError as e:
      raise KeyError("A __name__ property is required to parse a LitType from "
                     "JSON.") from e
    if not isinstance(type_name, str):
      raise TypeError("The value of __name__ must be a string.")
    base_cls = globals().get("LitType")
    cls = globals().get(type_name)  # class by name from this module
    if cls is None or not issubclass(cls, base_cls):
      raise NameError(f"{type_name} is not a valid LitType.")
    return cls(**{k: d[k] for k in d if k != "__name__"})
Spec = dict[str, LitType]
# Attributes that should be treated as a reference to other fields.
FIELD_REF_ATTRIBUTES = frozenset(
    {"parent", "align", "align_in", "align_out", "grad_for"})
def _remap_leaf(leaf: LitType, keymap: dict[str, str]) -> LitType:
  """Remap any field references on a LitType."""
  d = attr.asdict(leaf)  # mutable
  d = {
      k: (keymap.get(v, v) if k in FIELD_REF_ATTRIBUTES else v)
      for k, v in d.items()
  }
  return leaf.__class__(**d)
def remap_spec(spec: Spec, keymap: dict[str, str]) -> Spec:
  """Rename fields in a spec, with a best-effort to also remap field references."""
  ret = {}
  for k, v in spec.items():
    new_key = keymap.get(k, k)
    new_value = _remap_leaf(v, keymap)
    ret[new_key] = new_value
  return ret
##
# Concrete type clases
# LINT.IfChange
@attr.s(auto_attribs=True, frozen=True, kw_only=True)
class StringLitType(LitType):
  """User-editable text input.
  All automated edits are disabled for this type.
  Mainly used for string inputs that have special formatting, and should only
  be edited manually.
  """
  default: str = ""
  def validate_input(self, value, spec: Spec, example: Input):
    if not isinstance(value, str):
      raise ValueError(f"{value} is of type {type(value)}, expected str")
@attr.s(auto_attribs=True, frozen=True, kw_only=True)
class TextSegment(StringLitType):
  """Text input (untokenized), a single string."""
  pass
@attr.s(auto_attribs=True, frozen=True, kw_only=True)
class ImageBytes(LitType):
  """An image, an encoded base64 ascii string (starts with 'data:image...')."""
  def validate_input(self, value, spec: Spec, example: Input):
    if not isinstance(value, str) or not value.startswith("data:image"):
      raise ValueError(f"{value} is not an encoded image string.")
@attr.s(auto_attribs=True, frozen=True, kw_only=True)
class JPEGBytes(ImageBytes):
  """As ImageBytes, but assumed to be in jpeg format."""
  def validate_input(self, value, spec: Spec, example: Input):
    if not isinstance(value, str) or not value.startswith("data:image/jpg"):
      raise ValueError(f"{value} is not an encoded JPEG image string.")
@attr.s(auto_attribs=True, frozen=True, kw_only=True)
class PNGBytes(ImageBytes):
  """As ImageBytes, but assumed to be in png format."""
  def validate_input(self, value, spec: Spec, example: Input):
    if not isinstance(value, str) or not value.startswith("data:image/png"):
      raise ValueError(f"{value} is not an encoded PNG image string.")
@attr.s(auto_attribs=True, frozen=True, kw_only=True)
class GeneratedText(TextSegment):
  """Generated (untokenized) text."""
  # Name of a TextSegment field to evaluate against
  parent: Optional[str] = None
  def validate_output(self, value, output_spec: Spec, output_dict: JsonDict,
                      input_spec: Spec, dataset_spec: Spec,
                      input_example: Input):
    if not isinstance(value, str):
      raise ValueError(f"{value} is of type {type(value)}, expected str")
    if self.parent and not isinstance(input_spec[self.parent], TextSegment):
      raise ValueError(f"parent field {self.parent} is of type "
                       f"{type(self.parent)}, expected TextSegment")
@attr.s(auto_attribs=True, frozen=True, kw_only=True)
class ListLitType(LitType):
  """List type."""
  default: Sequence[Any] = None
@attr.s(auto_attribs=True, frozen=True, kw_only=True)
class _StringCandidateList(ListLitType):
  """A list of (text, score) tuples."""
  default: ScoredTextCandidates = None
  def validate_output(self, value, output_spec: Spec, output_dict: JsonDict,
                      input_spec: Spec, dataset_spec: Spec,
                      input_example: Input):
    if not isinstance(value, list):
      raise ValueError(f"{value} is not a list")
    for v in value:
      if not (isinstance(v, tuple) and isinstance(v[0], str) and
              (v[1] is None or isinstance(v[1], NumericTypes))):
        raise ValueError(f"{v} list item is not a (str, float) tuple)")
@attr.s(auto_attribs=True, frozen=True, kw_only=True)
class GeneratedTextCandidates(_StringCandidateList):
  """Multiple candidates for GeneratedText."""
  # Name of a TextSegment field to evaluate against
  parent: Optional[str] = None
  @staticmethod
  def top_text(value: ScoredTextCandidates) -> str:
    return value[0][0] if len(value) else ""
  def validate_output(self, value, output_spec: Spec, output_dict: JsonDict,
                      input_spec: Spec, dataset_spec: Spec,
                      input_example: Input):
    super().validate_output(
        value, output_spec, output_dict, input_spec, dataset_spec,
        input_example)
    if self.parent and not isinstance(input_spec[self.parent], TextSegment):
      raise ValueError(f"parent field {self.parent} is of type "
                       f"{type(input_spec[self.parent])}, expected TextSegment")
@attr.s(auto_attribs=True, frozen=True, kw_only=True)
class ReferenceTexts(_StringCandidateList):
  """Multiple candidates for TextSegment."""
  pass
@attr.s(auto_attribs=True, frozen=True, kw_only=True)
class TopTokens(_StringCandidateList):
  """Multiple tokens with weight."""
  pass
@attr.s(auto_attribs=True, frozen=True, kw_only=True)
class ImageBytesList(ListLitType):
  """A list of ImageBytes."""
  default: Sequence[Any] = []
@attr.s(auto_attribs=True, frozen=True, kw_only=True)
class URLLitType(TextSegment):
  """TextSegment that should be interpreted as a URL."""
  pass
@attr.s(auto_attribs=True, frozen=True, kw_only=True)
class GeneratedURL(TextSegment):
  """A URL that was generated as part of a model prediction."""
  align: Optional[str] = None  # name of a field in the model output
  def validate_output(self, value, output_spec: Spec, output_dict: JsonDict,
                      input_spec: Spec, dataset_spec: Spec,
                      input_example: Input):
    super().validate_output(value, output_spec, output_dict, input_spec,
                            dataset_spec, input_example)
    if self.align and self.align not in output_spec:
      raise ValueError(f"aligned field {self.align} is not in output_spec")
@attr.s(auto_attribs=True, frozen=True, kw_only=True)
class SearchQuery(TextSegment):
  """TextSegment that should be interpreted as a search query."""
  pass
@attr.s(auto_attribs=True, frozen=True, kw_only=True)
class _StringList(ListLitType):
  """A list of strings."""
  default: Sequence[str] = []
  def validate_input(self, value, spec: Spec, example: Input):
    if not isinstance(value, list) or not all(
        [isinstance(v, str) for v in value]):
      raise ValueError(f"{value} is not a list of strings")
@attr.s(auto_attribs=True, frozen=True, kw_only=True)
class Tokens(_StringList):
  """Tokenized text."""
  default: Sequence[str] = attr.Factory(list)
  # Name of a TextSegment field from the input
  # TODO(b/167617375): should we use 'align' here?
  parent: Optional[str] = None
  mask_token: Optional[str] = None  # optional mask token for input
  token_prefix: Optional[str] = "##"  # optional prefix used in tokens
@attr.s(auto_attribs=True, frozen=True, kw_only=True)
class TokenTopKPreds(ListLitType):
  """Predicted tokens, as from a language model.
  The inner list should contain (word, probability) in descending order.
  """
  default: Sequence[ScoredTextCandidates] = None
  align: str = None  # name of a Tokens field in the model output
  parent: Optional[str] = None
  def _validate_scored_candidates(self, scored_candidates):
    """Validates a list of scored candidates."""
    prev_val = math.inf
    for scored_candidate in scored_candidates:
      if not isinstance(scored_candidate, tuple):
        raise ValueError(f"{scored_candidate} is not a tuple")
      if not isinstance(scored_candidate[0], str):
        raise ValueError(f"{scored_candidate} first element is not a str")
      if scored_candidate[1] is not None:
        if not isinstance(scored_candidate[1], NumericTypes):
          raise ValueError(f"{scored_candidate} second element is not a num")
        if prev_val < scored_candidate[1]:
          raise ValueError(
              "TokenTopKPreds candidates are not in descending order")
        else:
          prev_val = scored_candidate[1]
  def validate_output(self, value, output_spec: Spec, output_dict: JsonDict,
                      input_spec: Spec, dataset_spec: Spec,
                      input_example: Input):
    if not isinstance(value, list):
      raise ValueError(f"{value} is not a list of scored text candidates")
    for scored_candidates in value:
      self._validate_scored_candidates(scored_candidates)
    if self.align and not isinstance(output_spec[self.align], Tokens):
      raise ValueError(
          f"aligned field {self.align} is {type(output_spec[self.align])}, "
          "expected Tokens")
@attr.s(auto_attribs=True, frozen=True, kw_only=True)
class Scalar(LitType):
  """Scalar value, a single float or int."""
  min_val: float = 0
  max_val: float = 1
  default: float = 0
  step: float = .01
  def validate_input(self, value, spec: Spec, example: Input):
    if not isinstance(value, NumericTypes):
      raise ValueError(f"{value} is of type {type(value)}, expected a number")
@attr.s(auto_attribs=True, frozen=True, kw_only=True)
class RegressionScore(Scalar):
  """Regression score, a single float."""
  # name of a Scalar or RegressionScore field in input
  parent: Optional[str] = None
  def validate_output(self, value, output_spec: Spec, output_dict: JsonDict,
                      input_spec: Spec, dataset_spec: Spec,
                      input_example: Input):
    if not isinstance(value, NumericTypes):
      raise ValueError(f"{value} is of type {type(value)}, expected a number")
    if self.parent and not isinstance(dataset_spec[self.parent], Scalar):
      raise ValueError(f"parent field {self.parent} is of type "
                       f"{type(self.parent)}, expected Scalar")
@attr.s(auto_attribs=True, frozen=True, kw_only=True)
class _FloatList(ListLitType):
  """A variable-length list of floats. Not the same as a 1D tensor."""
  default: Sequence[float] = []
  def validate_input(self, value, spec: Spec, example: Input):
    if not isinstance(value, list) or not all(
        [isinstance(v, float) for v in value]
    ):
      raise ValueError(f"{value} is not a list of floats")
@attr.s(auto_attribs=True, frozen=True, kw_only=True)
class TokenScores(_FloatList):
  """Scores, aligned to tokens.
  The data should be a list of floats, one for each token.
  """
  align: str  # name of Tokens field
@attr.s(auto_attribs=True, frozen=True, kw_only=True)
class ReferenceScores(ListLitType):
  """Score of one or more target sequences."""
  default: Sequence[float] = None
  # name of a TextSegment or ReferenceTexts field in the input
  parent: Optional[str] = None
  def validate_output(self, value, output_spec: Spec, output_dict: JsonDict,
                      input_spec: Spec, dataset_spec: Spec,
                      input_example: Input):
    if isinstance(value, list):
      if not all([isinstance(v, NumericTypes) for v in value]):
        raise ValueError(f"{value} is of type {type(value)}, expected a list "
                         "of numbers")
    elif not isinstance(value, np.ndarray) or not np.issubdtype(
        value.dtype, np.number):
      raise ValueError(f"{value} is of type {type(value)}, expected a list of "
                       "numbers")
    if self.parent and not isinstance(
        input_spec[self.parent], (TextSegment, ReferenceTexts)):
      raise ValueError(f"parent field {self.parent} is of type "
                       f"{type(self.parent)}, expected TextSegment or "
                       "ReferenceTexts")
@attr.s(auto_attribs=True, frozen=True, kw_only=True)
class CategoryLabel(StringLitType):
  """Category or class label, a single string."""
  # Optional vocabulary to specify allowed values.
  # If omitted, any value is accepted.
  vocab: Optional[Sequence[str]] = None  # label names
  def validate_input(self, value, spec: Spec, example: Input):
    if not isinstance(value, str):
      raise ValueError(f"{value} is of type {type(value)}, expected str")
    if self.vocab and value not in list(self.vocab):
      raise ValueError(f"{value} is not in provided vocab")
@attr.s(auto_attribs=True, frozen=True, kw_only=True)
class _Tensor(LitType):
  """A tensor type."""
  default: Sequence[float] = None
  def validate_input(self, value, spec: Spec, example: Input):
    if isinstance(value, list):
      if not all([isinstance(v, NumericTypes) for v in value]):
        raise ValueError(f"{value} is not a list of numbers")
    elif isinstance(value, np.ndarray):
      if not np.issubdtype(value.dtype, np.number):
        raise ValueError(f"{value} is not an array of numbers")
    else:
      raise ValueError(f"{value} is not a list or ndarray of numbers")
  def validate_ndim(self, value, ndim: Union[int, list[int]]):
    """Validate the number of dimensions in a tensor.
    Args:
      value: The tensor to validate.
      ndim: Either a number of dimensions to validate that the value has, or
        a list of dimensions any of which are valid for the value to have.
    Raises:
      ValueError if validation fails.
    """
    if isinstance(ndim, int):
      ndim = [ndim]
    if isinstance(value, np.ndarray):
      if value.ndim not in ndim:
        raise ValueError(f"{value} ndim is not one of {ndim}")
    else:
      if 1 not in ndim:
        raise ValueError(f"{value} ndim is not 1. "
                         "Use a numpy array for multidimensional arrays")
@attr.s(auto_attribs=True, frozen=True, kw_only=True)
class MulticlassPreds(_Tensor):
  """Multiclass predicted probabilities, as <float>[num_labels]."""
  # Vocabulary is required here for decoding model output.
  # Usually this will match the vocabulary in the corresponding label field.
  vocab: Sequence[str]  # label names
  null_idx: Optional[int] = None  # vocab index of negative (null) label
  parent: Optional[str] = None  # CategoryLabel field in input
  autosort: Optional[bool] = False  # Enable automatic sorting
  threshold: Optional[float] = None  # binary threshold, used to compute margin
  @property
  def num_labels(self):
    return len(self.vocab)
  def validate_input(self, value, spec: Spec, example: Input):
    super().validate_input(value, spec, example)
    if self.null_idx is not None:
      if self.null_idx < 0 or self.null_idx >= self.num_labels:
        raise ValueError(f"null_idx {self.null_idx} is not in the vocab range")
  def validate_output(self, value, output_spec: Spec, output_dict: JsonDict,
                      input_spec: Spec, dataset_spec: Spec,
                      input_example: Input):
    self.validate_input(value, output_spec, input_example)
    if self.parent and not isinstance(
        dataset_spec[self.parent], CategoryLabel):
      raise ValueError(f"parent field {self.parent} is of type "
                       f"{type(self.parent)}, expected CategoryLabel")
@attr.s(auto_attribs=True, frozen=True, kw_only=True)
class SequenceTags(_StringList):
  """Sequence tags, aligned to tokens.
  The data should be a list of string labels, one for each token.
  """
  align: str  # name of Tokens field
@attr.s(auto_attribs=True, frozen=True, kw_only=True)
class SpanLabels(ListLitType):
  """Span labels aligned to tokens.
  Span labels can cover more than one token, may not cover all tokens in the
  sentence, and may overlap with each other.
  """
  default: Sequence[dtypes.SpanLabel] = None
  align: str  # name of Tokens field
  parent: Optional[str] = None
  def validate_output(self, value, output_spec: Spec, output_dict: JsonDict,
                      input_spec: Spec, dataset_spec: Spec,
                      input_example: Input):
    if not isinstance(value, list) or not all(
        [isinstance(v, dtypes.SpanLabel) for v in value]):
      raise ValueError(f"{value} is not a list of SpanLabels")
    if not isinstance(output_spec[self.align], Tokens):
      raise ValueError(f"{self.align} is not a Tokens field")
@attr.s(auto_attribs=True, frozen=True, kw_only=True)
class EdgeLabels(ListLitType):
  """Edge labels between pairs of spans.
  This is a general form for structured prediction output; each entry consists
  of (span1, span2, label). See
  https://arxiv.org/abs/1905.06316 (Tenney et al. 2019) and
  https://github.com/nyu-mll/jiant/tree/master/probing#data-format for more
  details.
  """
  default: Sequence[dtypes.EdgeLabel] = None
  align: str  # name of Tokens field
  def validate_output(self, value, output_spec: Spec, output_dict: JsonDict,
                      input_spec: Spec, dataset_spec: Spec,
                      input_example: Input):
    if not isinstance(value, list) or not all(
        [isinstance(v, dtypes.EdgeLabel) for v in value]):
      raise ValueError(f"{value} is not a list of EdgeLabel")
    if not isinstance(output_spec[self.align], Tokens):
      raise ValueError(f"{self.align} is not a Tokens field")
@attr.s(auto_attribs=True, frozen=True, kw_only=True)
class MultiSegmentAnnotations(ListLitType):
  """Very general type for in-line text annotations.
  This is a more general version of SpanLabel, EdgeLabel, and other annotation
  types, designed to represent annotations that may span multiple segments.
  The basic unit is dtypes.AnnotationCluster, which contains a label, optional
  score, and one or more SpanLabel annotations, each of which points to a
  specific segment from the input.
  TODO(lit-dev): by default, spans are treated as bytes in this context.
  Make this configurable, if some spans need to refer to tokens instead.
  """
  default: Sequence[dtypes.AnnotationCluster] = None
  exclusive: bool = False  # if true, treat as candidate list
  background: bool = False  # if true, don't emphasize in visualization
  def validate_output(self, value, output_spec: Spec, output_dict: JsonDict,
                      input_spec: Spec, dataset_spec: Spec,
                      input_example: Input):
    if not isinstance(value, list) or not all(
        [isinstance(v, dtypes.AnnotationCluster) for v in value]):
      raise ValueError(f"{value} is not a list of AnnotationCluster")
##
# Model internals, for interpretation.
@attr.s(auto_attribs=True, frozen=True, kw_only=True)
class Embeddings(_Tensor):
  """Embeddings or model activations, as fixed-length <float>[emb_dim]."""
  def validate_output(self, value, output_spec: Spec, output_dict: JsonDict,
                      input_spec: Spec, dataset_spec: Spec,
                      input_example: Input):
    super().validate_output(value, output_spec, output_dict, input_spec,
                            dataset_spec, input_example)
    self.validate_ndim(value, 1)
@attr.s(auto_attribs=True, frozen=True, kw_only=True)
class _GradientsBase(_Tensor):
  """Shared gradient attributes."""
  align: Optional[str] = None  # name of a Tokens field
  grad_for: Optional[str] = None  # name of Embeddings field
  # Name of the field in the input that can be used to specify the target class
  # for the gradients.
  grad_target_field_key: Optional[str] = None
  def validate_output(self, value, output_spec: Spec, output_dict: JsonDict,
                      input_spec: Spec, dataset_spec: Spec,
                      input_example: Input):
    super().validate_output(
        value, output_spec, output_dict, input_spec, dataset_spec,
        input_example)
    if self.align is not None:
      align_entry = (output_spec[self.align] if self.align in output_spec
                     else input_spec[self.align])
      if not isinstance(align_entry, (Tokens, ImageBytes)):
        raise ValueError(f"{self.align} is not a Tokens or ImageBytes field")
    if self.grad_for is not None and not isinstance(
        output_spec[self.grad_for], (Embeddings, TokenEmbeddings)):
      raise ValueError(f"{self.grad_for} is not a Embeddings field")
    if (self.grad_target_field_key is not None and
        self.grad_target_field_key not in input_spec):
      raise ValueError(f"{self.grad_target_field_key} is not in input_spec")
@attr.s(auto_attribs=True, frozen=True, kw_only=True)
class Gradients(_GradientsBase):
  """1D gradients with respect to embeddings."""
  def validate_output(self, value, output_spec: Spec, output_dict: JsonDict,
                      input_spec: Spec, dataset_spec: Spec,
                      input_example: Input):
    super().validate_output(
        value, output_spec, output_dict, input_spec, dataset_spec,
        input_example)
    self.validate_ndim(value, 1)
@attr.s(auto_attribs=True, frozen=True, kw_only=True)
class TokenEmbeddings(_Tensor):
  """Per-token embeddings, as <float>[num_tokens, emb_dim]."""
  align: Optional[str] = None  # name of a Tokens field
  def validate_output(self, value, output_spec: Spec, output_dict: JsonDict,
                      input_spec: Spec, dataset_spec: Spec,
                      input_example: Input):
    super().validate_output(
        value, output_spec, output_dict, input_spec, dataset_spec,
        input_example)
    self.validate_ndim(value, 2)
    if self.align is not None and not isinstance(
        output_spec[self.align], Tokens):
      raise ValueError(f"{self.align} is not a Tokens field")
@attr.s(auto_attribs=True, frozen=True, kw_only=True)
class TokenGradients(_GradientsBase):
  """Gradients for per-token inputs, as <float>[num_tokens, emb_dim]."""
  def validate_output(self, value, output_spec: Spec, output_dict: JsonDict,
                      input_spec: Spec, dataset_spec: Spec,
                      input_example: Input):
    super().validate_output(
        value, output_spec, output_dict, input_spec, dataset_spec,
        input_example)
    self.validate_ndim(value, 2)
@attr.s(auto_attribs=True, frozen=True, kw_only=True)
class ImageGradients(_GradientsBase):
  """Gradients with respect to per-pixel inputs, as a multidimensional array."""
  def validate_output(self, value, output_spec: Spec, output_dict: JsonDict,
                      input_spec: Spec, dataset_spec: Spec,
                      input_example: Input):
    super().validate_output(
        value, output_spec, output_dict, input_spec, dataset_spec,
        input_example)
    self.validate_ndim(value, [2, 3])
@attr.s(auto_attribs=True, frozen=True, kw_only=True)
class AttentionHeads(_Tensor):
  """One or more attention heads, as <float>[num_heads, num_tokens, num_tokens]."""
  # input and output Tokens fields; for self-attention these can be the same
  align_in: str
  align_out: str
  def validate_output(self, value, output_spec: Spec, output_dict: JsonDict,
                      input_spec: Spec, dataset_spec: Spec,
                      input_example: Input):
    super().validate_output(
        value, output_spec, output_dict, input_spec, dataset_spec,
        input_example)
    self.validate_ndim(value, 3)
    if self.align_in is None or not isinstance(
        output_spec[self.align_in], Tokens):
      raise ValueError(f"{self.align_in} is not a Tokens field")
    if self.align_out is None or not isinstance(
        output_spec[self.align_out], Tokens):
      raise ValueError(f"{self.align_out} is not a Tokens field")
@attr.s(auto_attribs=True, frozen=True, kw_only=True)
class SubwordOffsets(ListLitType):
  """Offsets to align input tokens to wordpieces or characters.
  offsets[i] should be the index of the first wordpiece for input token i.
  """
  default: Sequence[int] = None
  align_in: str  # name of field in data spec
  align_out: str  # name of field in model output spec
@attr.s(auto_attribs=True, frozen=True, kw_only=True)
class SparseMultilabel(_StringList):
  """Sparse multi-label represented as a list of strings."""
  vocab: Optional[Sequence[str]] = None  # label names
  separator: str = ","  # Used for display purposes.
@attr.s(auto_attribs=True, frozen=True, kw_only=True)
class SparseMultilabelPreds(_StringCandidateList):
  """Sparse multi-label predictions represented as a list of tuples.
  The tuples are of the label and the score.
  """
  default: ScoredTextCandidates = None
  vocab: Optional[Sequence[str]] = None  # label names
  parent: Optional[str] = None
@attr.s(auto_attribs=True, frozen=True, kw_only=True)
class FieldMatcher(LitType):
  """For matching spec fields.
  The front-end will perform spec matching and fill in the vocab field
  accordingly.
  """
  spec: str  # which spec to check, 'dataset', 'input', or 'output'.
  types: Union[str, Sequence[str]]  # types of LitType to match in the spec.
  vocab: Optional[Sequence[str]] = None  # names matched from the spec.
@attr.s(auto_attribs=True, frozen=True, kw_only=True)
class SingleFieldMatcher(FieldMatcher):
  """For matching a single spec field.
  UI will materialize this to a dropdown-list.
  """
  default: str = None
@attr.s(auto_attribs=True, frozen=True, kw_only=True)
class MultiFieldMatcher(FieldMatcher):
  """For matching multiple spec fields.
  UI will materialize this to multiple checkboxes. Use this when the user needs
  to pick more than one field in UI.
  """
  default: Sequence[str] = []  # default names of selected items.
  select_all: bool = False  # Select all by default (overriddes default).
@attr.s(auto_attribs=True, frozen=True, kw_only=True)
class Salience(LitType):
  """Metadata about a returned salience map."""
  autorun: bool = False  # If the saliency technique is automatically run.
  signed: bool  # If the returned values are signed.
@attr.s(auto_attribs=True, frozen=True, kw_only=True)
class TokenSalience(Salience):
  """Metadata about a returned token salience map."""
  default: dtypes.TokenSalience = None
@attr.s(auto_attribs=True, frozen=True, kw_only=True)
class FeatureSalience(Salience):
  """Metadata about a returned feature salience map."""
  default: dtypes.FeatureSalience = None
@attr.s(auto_attribs=True, frozen=True, kw_only=True)
class FrameSalience(Salience):
  """Metadata about a returned frame salience map."""
  default: dtypes.FrameSalience = None
@attr.s(auto_attribs=True, frozen=True, kw_only=True)
class ImageSalience(Salience):
  """Metadata about a returned image saliency.
  The data is returned as an image in the base64 URL encoded format, e.g.,
  data:image/jpg;base64,w4J3k1Bfa...
  """
  signed: bool = False  # If the returned values are signed.
@attr.s(auto_attribs=True, frozen=True, kw_only=True)
class SequenceSalience(Salience):
  """Metadata about a returned sequence salience map."""
  default: dtypes.SequenceSalienceMap = None
@attr.s(auto_attribs=True, frozen=True, kw_only=True)
class BooleanLitType(LitType):
  """Boolean value."""
  default: bool = False
  def validate_input(self, value, spec, example: Input):
    if not isinstance(value, bool):
      raise ValueError(f"{value} is not a boolean")
@attr.s(auto_attribs=True, frozen=True, kw_only=True)
class CurveDataPoints(LitType):
  """Represents data points of a curve.
  A list of tuples where the first and second elements of the tuple are the
  x and y coordinates of the corresponding curve point respectively.
  """
  pass
@attr.s(auto_attribs=True, frozen=True, kw_only=True)
class InfluentialExamples(LitType):
  """Represents influential examples from the training set.
  This is as returned by a training-data attribution method like TracIn or
  influence functions.
  This describes a generator component; values are Sequence[Sequence[JsonDict]].
  """
  pass
@enum.unique
class MetricBestValue(dtypes.EnumSerializableAsValues, enum.Enum):
  """The method to use to determine the best value for a Metric."""
  HIGHEST = "highest"
  LOWEST = "lowest"
  NONE = "none"
  ZERO = "zero"
@attr.s(auto_attribs=True, frozen=True, kw_only=True)
class MetricResult(LitType):
  """Score returned from the computation of a Metric."""
  default: float = 0
  description: str = ""
  best_value: MetricBestValue = MetricBestValue.NONE
@attr.s(auto_attribs=True, frozen=True, kw_only=True)
class Integer(Scalar):
  step: int = 1
  min_val: int = -32768
  max_val: int = 32767
@attr.s(auto_attribs=True, frozen=True, kw_only=True)
class SalienceTargetInfo(LitType):
  """Target information for salience interpreters, used in config_spec().
  Value is a dict with keys 'field' (str) and 'index' (Optional[int]).
  """
  default: Optional[Mapping[str, Any]] = None
# LINT.ThenChange(../client/lib/lit_types.ts)
# Type aliases for backend use.
# The following names are existing datatypes in TypeScript, so we add a
# `LitType` suffix to avoid collisions with language features on the front-end.
Boolean = BooleanLitType
String = StringLitType
URL = URLLitType
def get_type_by_name(typename: str) -> type[LitType]:
  cls = globals()[typename]
  assert issubclass(cls, LitType)
  return cls
# A map from Python's native type annotations to their LitType corollary for use
# by infer_spec_for_func().
_INFERENCE_TYPES_TO_LIT_TYPES: dict[type[Any], Callable[..., LitType]] = {
    bool: Boolean,
    Optional[bool]: Boolean,
    float: Scalar,
    Optional[float]: Scalar,
    Union[float, int]: Scalar,
    Optional[Union[float, int]]: Scalar,
    int: Integer,
    Optional[int]: Integer,
    str: String,
    Optional[str]: String,
    epath.PathLike: String,
    Optional[epath.PathLike]: String,
    os.PathLike[str]: String,
    Optional[os.PathLike[str]]: String,
}
def infer_spec_for_func(func: Callable[..., Any]) -> Spec:
  """Infers a Spec from the arguments of a Callable's signature.
  LIT uses
  [Specs](https://pair-code.github.io/lit/documentation/api.md#type-system)
  as a mechanism to communicate how the web app should construct user interface
  elements to enable user input for certain tasks, such as parameterizing an
  Interpreter or loading a Model or Dataset at runtime. This includes
  information about the type, default value (if any), required status, etc. of
  the arguments to enable robust construction of HTML input elements.
  As many LIT components are essentially Python functions that can be
  parameterized and run via the LIT web app, this function exists to automate
  the creation of Specs for some use cases, e.g., the `init_spec()` API of
  `lit_nlp.api.dataset.Dataset` and `lit_nlp.api.model.Model` classes. It
  attempts to infer a Spec for the Callable passed in as the value of `func` by:
  1.  Using `inspect.signature()` to retreive the Callable's signature info;
  2.  Processing `signature.parameters` to transform them into a corollary
      `LitType` object that is consumable by the web app, either using
      `Parameter.annotation` or by inferring a type from `Parameter.default`;
  3.  Adding an entry to a `Spec` dictionary where the key is `Paramater.name`
      and the value is the `LitType`; and then
  4.  Returning the `Spec` dictionary after all arguments are processed.
  Due to limitations of LIT's typing system and front-end support for these
  types, this function is only able to infer Specs for Callables with arguments
  of the following types (or `Optional` variants thereof) at this time. Support
  for additional types may be added in the future. A `TypeError` will be raised
  if this function encounters a type aside from those listed below.
  * `bool` ==> `Boolean()`
  * `float` ==> `Scalar()`
  * `int` ==> `Integer()`
  * `Union[float, int]` ==> `Scalar()`
  * `str` ==> `String()`
  Specs inferred by this function will not include entries for the `self`
  parameter of instance methods of classes as this is unnecessary/implied, or
  for `*args`- or `**kwargs`-like parameters of any funciton as we cannot safely
  infer how variable arguments will be mutated, passed, or used.
  Args:
    func: The Callable for which a spec will be inferred.
  Returns:
    A Spec object where the keys are the parameter names and the values are the
    `LitType` representation of that parameter (its type, default value, and
    whether or not it is required).
  Raises:
    TypeError: If unable to infer a type, the type is not supported, or `func`
      is not a `Callable`.
  """
  if not callable(func):
    raise TypeError("Attempted to infer a spec for a non-'Callable', "
                    f"'{type(func)}'.")
  signature = inspect.signature(func)
  spec: Spec = {}
  def is_param_optional(parameter: inspect.Parameter) -> bool:
    is_union = get_origin(parameter.annotation) is Union
    can_be_none = type(None) in get_args(parameter.annotation)
    return is_union and can_be_none
  for param in signature.parameters.values():
    if (param.name == "self" or
        param.kind is param.VAR_KEYWORD or
        param.kind is param.VAR_POSITIONAL):
      continue  # self, *args, and **kwargs are not returned in inferred Specs.
    # Otherwise, attempt to infer a type from the Paramater object.
    if param.annotation is param.empty and param.default is param.empty:
      fn_name = getattr(func, "__name__", repr(func))
      raise TypeError(
          f"Unable to infer a type for parameter '{param.name}' of '{fn_name}'."
          " Please add a type hint or default value, or implement a Spec"
          " literal."
      )
    if param.annotation is param.empty:
      param_type = type(param.default)
    else:
      param_type = param.annotation
    if param_type in _INFERENCE_TYPES_TO_LIT_TYPES:
      lit_type_cstr = _INFERENCE_TYPES_TO_LIT_TYPES[param_type]
      is_default_empty = param.default is param.empty
      is_optional = is_param_optional(param) or not is_default_empty
      lit_type_params = {"required": not is_optional}
      if not is_default_empty:
        lit_type_params["default"] = param.default
      spec[param.name] = lit_type_cstr(**lit_type_params)
    else:
      fn_name = getattr(func, "__name__", repr(func))
      raise TypeError(
          f"Unsupported type '{param_type}' for parameter '{param.name}' of"
          f" '{fn_name}'. If possible (e.g., this parameter is Optional),"
          " please implement a spec literal instead of using inferencing."
      )
  return spec

================
File: lit_nlp/app.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""LIT backend, as a standard WSGI app."""
import collections
from collections.abc import Callable, Iterable, Mapping, Sequence
import functools
import inspect
import math
import os
import random
import threading
from typing import Any, Optional, TypedDict, Union, cast, get_type_hints
from absl import logging
from lit_nlp.api import components as lit_components
from lit_nlp.api import dataset as lit_dataset
from lit_nlp.api import layout
from lit_nlp.api import model as lit_model
from lit_nlp.api import types
from lit_nlp.components import core
from lit_nlp.lib import caching
from lit_nlp.lib import flag_helpers
from lit_nlp.lib import serialize
from lit_nlp.lib import ui_state
from lit_nlp.lib import utils
from lit_nlp.lib import validation
from lit_nlp.lib import wsgi_app
import tqdm
JsonDict = types.JsonDict
Input = types.Input
IndexedInput = types.IndexedInput
# Export this symbol, for access from demo.py
PredsCache = caching.PredsCache
ProgressIndicator = Callable[[Iterable], Iterable]
DatasetLoader = tuple[Callable[..., lit_dataset.Dataset], Optional[types.Spec]]
DatasetLoadersMap = dict[str, DatasetLoader]
SingleModelLoader = Callable[..., lit_model.Model]
MultipleModelLoader = Callable[..., lit_model.ModelMap]
ModelLoader = tuple[
    Union[SingleModelLoader, MultipleModelLoader],
    Optional[types.Spec],
]
ModelLoadersMap = dict[str, ModelLoader]
_EMPTY_DATASET_KEY = '_union_empty'
# LINT.IfChange
class ComponentInfo(TypedDict):
  configSpec: types.Spec  # pylint: disable=invalid-name  # Named for JSON struct
  metaSpec: types.Spec    # pylint: disable=invalid-name  # Named for JSON struct
  description: str
# LINT.ThenChange(./client/lib/types.ts)
def _get_component_info(
    obj: lit_components.Interpreter,
) -> ComponentInfo:
  """Returns the ComponentInfo for an Interpreter, Generator, Metric, etc."""
  return ComponentInfo(
      configSpec=obj.config_spec(),
      metaSpec=obj.meta_spec(),
      description=obj.description(),
  )
def _get_compatible_names(
    candidates: Mapping[str, lit_components.Interpreter],
    model: lit_model.Model,
    dataset: lit_dataset.Dataset,
) -> Sequence[str]:
  """Returns the names of the candidates compatible with the model/dataset."""
  return [
      name
      for name, candidate in candidates.items()
      if candidate.is_compatible(model=model, dataset=dataset)
  ]
class LitApp(object):
  """LIT WSGI application."""
  def _build_metadata(self):
    """Build metadata from model and dataset specs."""
    model_info = {}
    for name, model in self._models.items():
      info = {
          'description': model.description(),
          'spec': {
              'input': model.input_spec(),
              'output': model.output_spec(),
          }
      }
      # List compatible datasets.
      info['datasets'] = [
          name for name, dataset in self._datasets.items()
          if model.is_compatible_with_dataset(dataset)
      ]
      if len(info['datasets']) == 0:  # pylint: disable=g-explicit-length-test
        logging.error("Error: model '%s' has no compatible datasets!", name)
      compat_gens: set[str] = set()
      compat_interps: set[str] = set()
      compat_metrics: set[str] = set()
      for d in info['datasets']:
        dataset: lit_dataset.Dataset = self._datasets[d]
        compat_gens.update(
            _get_compatible_names(self._generators, model, dataset)
        )
        compat_interps.update(
            _get_compatible_names(self._interpreters, model, dataset)
        )
        compat_metrics.update(
            _get_compatible_names(self._metrics, model, dataset)
        )
      info['generators'] = [
          name for name in self._generators.keys() if name in compat_gens
      ]
      info['interpreters'] = [
          name for name in self._interpreters.keys() if name in compat_interps
      ]
      info['metrics'] = [
          name for name in self._metrics.keys() if name in compat_metrics
      ]
      model_info[name] = info
    dataset_info = {}
    for name, ds in self._datasets.items():
      dataset_info[name] = {
          'spec': ds.spec(),
          'description': ds.description(),
          'size': len(ds),
      }
    generator_info: Mapping[str, ComponentInfo] = {
        name: _get_component_info(gen) for name, gen in self._generators.items()
    }
    interpreter_info: Mapping[str, ComponentInfo] = {
        name: _get_component_info(interp)
        for name, interp in self._interpreters.items()
    }
    metrics_info: Mapping[str, ComponentInfo] = {
        name: _get_component_info(metric)
        for name, metric in self._metrics.items()
    }
    init_specs = {
        'datasets': {n: s for n, (_, s) in self._dataset_loaders.items()},
        'models': {n: s for n, (_, s) in self._model_loaders.items()},
    }
    return {
        # Component info and specs
        'models': model_info,
        'datasets': dataset_info,
        'generators': generator_info,
        'interpreters': interpreter_info,
        'metrics': metrics_info,
        'layouts': self._layouts,
        # Global configuration
        'demoMode': self._demo_mode,
        'defaultLayout': self._default_layout,
        'canonicalURL': self._canonical_url,
        'pageTitle': self._page_title,
        'inlineDoc': self._inline_doc,
        'onboardStartDoc': self._onboard_start_doc,
        'onboardEndDoc': self._onboard_end_doc,
        'syncState': self.ui_state_tracker is not None,
        'initSpecs': init_specs,
    }
  def _get_model_spec(self, name: str):
    return self._info['models'][name]['spec']
  def _get_info(self, unused_data, **unused_kw):
    """Get model info and send to frontend."""
    return self._info
  def _reconstitute_inputs(
      self, inputs: Sequence[Union[IndexedInput, str]], dataset_name: str
  ) -> list[IndexedInput]:
    """Reconstitute any inputs sent as references (bare IDs)."""
    index = self._datasets[dataset_name].index
    # TODO(b/178228238): set up proper debug logging and hide this by default.
    # TODO(b/171513556): Reconsistute as Inputs instead of IndexedInputs
    num_aliased = sum([isinstance(ex, str) for ex in inputs])
    logging.info(
        "%d of %d inputs sent as IDs; reconstituting from dataset '%s'",
        num_aliased,
        len(inputs),
        dataset_name,
    )
    return [index[ex] if isinstance(ex, str) else ex for ex in inputs]
  def _save_datapoints(
      self,
      data,
      dataset_name: Optional[str] = None,
      path: Optional[str] = None,
      **unused_kw,
  ):
    """Save datapoints to disk."""
    if dataset_name is None:
      raise ValueError('Must provide a "dataset_name" to save datapoints.')
    if path is None:
      raise ValueError('Must provide a "path" to save datapoints.')
    if self._demo_mode:
      logging.warning('Attempted to save datapoints in demo mode.')
      return None
    return self._datasets[dataset_name].save(data['inputs'], path)
  def _load_datapoints(
      self,
      unused_data,
      dataset_name: Optional[str] = None,
      path: Optional[str] = None,
      **unused_kw,
  ):
    """Load datapoints from disk."""
    if dataset_name is None:
      raise ValueError('Must provide a "dataset_name" to load datapoints.')
    if path is None:
      raise ValueError('Must provide a "path" from which to load datapoints.')
    if self._demo_mode:
      logging.warning('Attempted to load datapoints in demo mode.')
      return None
    dataset = self._datasets[dataset_name].load(path)
    return dataset.indexed_examples
  def _get_preds(self,
                 data: types.JsonDict,
                 model: Optional[str] = None,
                 requested_types: Optional[str] = None,
                 requested_fields: Optional[str] = None,
                 **kw):
    """Get model predictions.
    Args:
      data: data payload, containing 'inputs' field
      model: name of the model to run
      requested_types: optional, comma-separated list of type names to return
      requested_fields: optional, comma-separated list of field names to return
        in addition to the ones returned due to 'requested_types'.
      **kw: additional args passed to model.predict()
    Returns:
      list[JsonDict] containing requested fields of model predictions
    Raises:
      KeyError: If `data` does not have an 'inputs' property.
      TypeError: If one of entries in `requested_types` is not a valid LitType.
      ValueError: If the model returns a different number of predictions than
        the number of inputs.
    """
    if model is None:
      raise ValueError('Must provide a "model" name to get preds from.')
    inputs = data['inputs']
    preds = list(self._models[model].predict(
        [ex['data'] for ex in inputs], **kw))
    num_preds = len(preds)
    num_inputs = len(inputs)
    if num_preds != num_inputs:
      raise ValueError(
          f'Different number of model predictions ({num_preds}) than inputs'
          f' ({num_inputs}).'
      )
    if not requested_types and not requested_fields:
      return preds
    # Figure out what to return to the frontend.
    output_spec = self._get_model_spec(model)['output']
    requested_types = requested_types.split(',') if requested_types else []
    requested_fields = requested_fields.split(',') if requested_fields else []
    logging.info('Requested types: %s, fields: %s', str(requested_types),
                 str(requested_fields))
    for t_name in requested_types:
      t_class = getattr(types, t_name, None)
      if not issubclass(t_class, types.LitType):
        raise TypeError(f"Class '{t_name}' is not a valid LitType.")
      requested_fields.extend(utils.find_spec_keys(output_spec, t_class))
    ret_keys = set(requested_fields)  # de-dupe
    # Return selected keys.
    logging.info('Will return keys: %s', str(ret_keys))
    # One record per input.
    ret = [utils.filter_by_keys(p, ret_keys.__contains__) for p in preds]
    return ret
  def _annotate_new_data(self,
                         data: types.JsonDict,
                         dataset_name: Optional[str] = None,
                         **unused_kw) -> list[IndexedInput]:
    """Fill in index and other extra data for the provided datapoints."""
    # TODO(lit-dev): unify this with hash fn on dataset objects.
    if dataset_name is None:
      raise ValueError('Must provide a "dataset_name" to annotate.')
    # Generate annotated versions of new datapoints.
    dataset = self._datasets[dataset_name]
    input_examples = [example['data'] for example in data['inputs']]
    dataset_to_annotate = lit_dataset.Dataset(
        base=dataset, examples=input_examples)
    annotated_dataset = self._run_annotators(dataset_to_annotate)
    # Add annotations and IDs to new datapoints.
    for i, example in enumerate(data['inputs']):
      new_id = caching.input_hash(example['data'])
      example['data'] = dict(annotated_dataset.examples[i], _id=new_id)
      example['id'] = new_id
    return data['inputs']  # pytype: disable=bad-return-type  # always-use-return-annotations
  def _post_new_data(
      self,
      data: types.JsonDict,
      dataset_name: Optional[str] = None,
      **unused_kw
  ) -> dict[str, str]:
    """Save datapoints provided, after annotatation, for later retrieval.
    Args:
      data: JsonDict of datapoints to add, in dict under key 'inputs', per
        format for other requests.
      dataset_name: Dataset containing the format of data to add, necessary for
        proper datapoint annotation.
    Returns:
      A dict of two URLs (minus the root of the webserver). 'load' value is
      for loading LIT with those datapoints. 'remove' value is for removing
      those new datapoints from this server after they have been loaded, if
      desired.
    Raises:
      KeyError: If the `data` dictionary does not have an "inputs" field.
      ValueError: If a "dataset_name" is not provided.
    """
    if dataset_name is None:
      raise ValueError('Must provide a "dataset_name" to save new datapoints.')
    if 'inputs' not in data:
      raise KeyError('Data dict does not contain "inputs" field.')
    data_with_metadata = [
        {'data': d, 'meta': {'added': True, 'source': 'POST', 'parentId': None}}
        for d in data['inputs']
    ]
    annotation_input = {'inputs': data_with_metadata}
    annotated_data = self._annotate_new_data(annotation_input, dataset_name)
    datapoints_id = utils.get_uuid()
    with self._saved_datapoints_lock:
      self._saved_datapoints[datapoints_id] = annotated_data
    return {
        'load': f'?saved_datapoints_id={datapoints_id}',
        'remove': f'/remove_new_data?saved_datapoints_id={datapoints_id}',
    }
  def _fetch_new_data(
      self, unused_data, saved_datapoints_id: Optional[str] = None, **unused_kw
  ):
    if not saved_datapoints_id:
      raise ValueError('Must provide a "saved_datapoints_id" to get data from.')
    with self._saved_datapoints_lock:
      if saved_datapoints_id not in self._saved_datapoints:
        raise ValueError(f'No saved data with ID: {saved_datapoints_id}')
      return self._saved_datapoints[saved_datapoints_id]
  def _remove_new_data(
      self, unused_data, saved_datapoints_id: Optional[str] = None, **unused_kw
  ):
    if not saved_datapoints_id:
      raise ValueError('Must provide a "saved_datapoints_id" to remove data.')
    with self._saved_datapoints_lock:
      if saved_datapoints_id not in self._saved_datapoints:
        raise ValueError(f'No saved data with ID: {saved_datapoints_id}')
      del self._saved_datapoints[saved_datapoints_id]
  def _get_dataset(
      self, unused_data, dataset_name: Optional[str] = None, **unused_kw
  ) -> list[IndexedInput]:
    """Attempt to get dataset, or override with a specific path."""
    if not dataset_name:
      raise ValueError('Must provide a "dataset_name" to get examples.')
    return list(self._datasets[dataset_name].indexed_examples)
  def _create_dataset(
      self,
      data: types.JsonDict,
      dataset_name: Optional[str] = None,
      **unused_kw,
  ):
    """Create a dataset, updating and returning the metadata."""
    if dataset_name is None:
      raise ValueError('No base dataset specified.')
    config: Optional[dict[str, Any]] = data.get('config')
    if config is None:
      raise ValueError('No config specified.')
    new_name: Optional[str] = config.pop('new_name', None)
    if new_name is None:
      raise ValueError('No name provided for the new dataset.')
    elif new_name in self._datasets:
      return (self._info, new_name)  # Return the existing dataset
    if (loader_info := self._dataset_loaders.get(dataset_name)) is None:
      raise ValueError(
          f'No loader information (Cls + init_spec) found for {dataset_name}'
      )
    dataset_cls, dataset_init_spec = loader_info
    if dataset_init_spec is not None:
      initializer_name = getattr(dataset_cls, '__name__', repr(dataset_cls))
      utils.validate_config_against_spec(
          config,
          dataset_init_spec,
          f'{dataset_name} ({initializer_name})',
          raise_for_unsupported=True,
      )
    new_dataset = dataset_cls(**config)
    annotated_dataset = self._run_annotators(new_dataset)
    self._datasets[new_name] = lit_dataset.IndexedDataset(
        base=annotated_dataset, id_fn=caching.input_hash
    )
    self._info = self._build_metadata()
    return (self._info, new_name)
  def _create_model(
      self, data: types.JsonDict, model_name: Optional[str] = None, **unused_kw
  ):
    """Create a model, updating and returning the metadata.
    LIT supports two types of model loaders:
    *   Single-model loaders that return an instance of `lit_model.Model`; and
    *   Multiple-model loaders that return a `Mapping[str, lit_model.Model]`.
    Multiple-model loaders are primarily used for LLM use cases, such as the
    Prompt Debugging example, where LIT needs to access the generation,
    tokenization, and salience computation features of a model separately, and
    thus initializes one lit_model.Model wrapper for each of these purposes.
    Note that the `Callable` associated with a given Multiple-model
    `ModelLoader` must take `new_name` parameter as it is assumed that this
    `Callable` will initialize multiple LIT Model wrappers for different
    functions performed by a shared model, such as the generate, tokenize, and
    salience functions of an LLM for prompt debugging use cases.
    Single-model loaders are used in most other use cases, such as
    classification and regression tasks where the prediction is more stable.
    Args:
      data: the JSON payload provided in the request.
      model_name: the model intializer to use, a key of LitApp._model_loaders.
    Returns:
      A tuple containing the updated LitApp metadata and the name of the models
      that were added.
    Raises:
      ValueError: If any of the following are missing: model_name, the config,
        or a value for new_name in the config; if there is not a model loader
        configured for the provided model_name; or if there is a name collision
        with one of the models returned by a multiple-model loader.
    """
    if model_name is None:
      raise ValueError('No base model specified.')
    if (loader_info := self._model_loaders.get(model_name)) is None:
      raise ValueError(
          f'No loader information (Cls + init_spec) found for {model_name}'
      )
    config: Optional[dict[str, Any]] = data.get('config')
    if config is None:
      raise ValueError('No config specified.')
    new_name: Optional[str] = config.pop('new_name', None)
    if not new_name:
      raise ValueError('No name provided for the new model.')
    model_initializer, model_init_spec = loader_info
    if model_init_spec is not None:
      initializer_name = getattr(
          model_initializer, '__name__', repr(model_initializer)
      )
      utils.validate_config_against_spec(
          config,
          model_init_spec,
          f'{model_name} ({initializer_name})',
          raise_for_unsupported=True,
      )
    return_type = lit_model.Model
    if inspect.isfunction(model_initializer):
      return_type = get_type_hints(model_initializer)['return']
    if Mapping in return_type.__mro__:
      model_initializer = cast(MultipleModelLoader, model_initializer)
      new_models = model_initializer(new_name=new_name, **config)
      new_model_names: list[str] = list(new_models.keys())
      model_name_collisions = [
          model_name
          for model_name in new_model_names
          if model_name in self._models
      ]
      if model_name_collisions:
        raise ValueError(f'Model(s) already exist: {model_name_collisions}.')
      for model_name, model_instance in new_models.items():
        self._models[model_name] = caching.CachingModelWrapper(
            model_instance, model_name, **self._caching_model_wrapper_kw
        )
    else:
      if new_name in self._models:
        return (self._info, new_name)  # Return the existing model
      new_model_names: list[str] = [new_name]
      model_initializer = cast(SingleModelLoader, model_initializer)
      new_model = model_initializer(**config)
      self._models[new_name] = caching.CachingModelWrapper(
          new_model, new_name, **self._caching_model_wrapper_kw
      )
    empty_dataset = lit_dataset.NoneDataset(self._models)
    self._datasets[_EMPTY_DATASET_KEY] = lit_dataset.IndexedDataset(
        base=self._run_annotators(empty_dataset), id_fn=caching.input_hash
    )
    self._info = self._build_metadata()
    return (self._info, new_model_names)
  def _get_generated(
      self,
      data: types.JsonDict,
      model: Optional[str] = None,
      dataset_name: Optional[str] = None,
      generator: Optional[str] = None,
      **unused_kw,
  ):
    """Generate new datapoints based on the request."""
    if dataset_name is None:
      raise ValueError('Must provide a "dataset_name" to get base examples.')
    if generator is None:
      raise ValueError('Must provide a "generator" name to generate examples.')
    if model is None:
      raise ValueError('Must provide a "model" name to get predictions.')
    genny: lit_components.Generator = self._generators[generator]
    config_spec: types.Spec = genny.config_spec()
    config: Optional[types.JsonDict] = data.get('config')
    if config_spec and config is not None:
      utils.validate_config_against_spec(
          config, config_spec, f'{generator} ({type(genny).__name__})'
      )
    dataset = self._datasets[dataset_name]
    # Nested list, containing generated examples from each input.
    all_generated: list[list[Input]] = genny.run(  # pytype: disable=annotation-type-mismatch  # always-use-return-annotations
        [ex['data'] for ex in data['inputs']],
        self._models[model],
        dataset,
        config=config)
    # Annotate datapoints
    def annotate_generated(datapoints):
      dataset_to_annotate = lit_dataset.Dataset(
          base=dataset, examples=datapoints)
      annotated_dataset = self._run_annotators(dataset_to_annotate)
      return annotated_dataset.examples
    annotated_generated = [
        annotate_generated(generated) for generated in all_generated
    ]
    # Add metadata.
    all_generated_indexed: list[list[IndexedInput]] = [
        dataset.index_inputs(generated) for generated in annotated_generated
    ]
    for parent, indexed_generated in zip(data['inputs'], all_generated_indexed):
      for generated in indexed_generated:
        generated['meta'].update({
            'parentId': parent['id'],
            'source': generator,
            'added': True,
        })
    return all_generated_indexed
  def _get_interpretations(
      self,
      data: types.JsonDict,
      model: Optional[str] = None,
      dataset_name: Optional[str] = None,
      interpreter: Optional[str] = None,
      # boolean but via URL param, so encoding as "0" /  "1" is safer.
      do_predict: str = '1',
      **unused_kw,
  ):
    """Run an interpretation component."""
    if dataset_name is None:
      raise ValueError('Must provide a "dataset_name" to get examples.')
    if interpreter is None:
      raise ValueError('Must provide a "interpreter" name to interpret preds.')
    if model is None:
      raise ValueError('Must provide a "model" name to get predictions.')
    interp: lit_components.Interpreter = self._interpreters[interpreter]
    mdl: lit_model.Model = self._models[model]
    config_spec: types.Spec = interp.config_spec()
    config: Optional[types.JsonDict] = data.get('config')
    if config_spec and config is not None:
      utils.validate_config_against_spec(
          config, config_spec, f'{interpreter} ({type(interp).__name__})'
      )
    model_inputs = [ex['data'] for ex in data['inputs']]
    # Get model preds before the interpreter call. Usually these are cached.
    # TODO(b/278586715): See if we can remove this path and just allow
    # interpreters to call the model directly.
    if utils.coerce_bool(do_predict):
      # Workaround so that interpreters can skip the predict() call when it
      # is unnecessary and may be slow.
      # TODO(b/278586715): Remove this once we can ensure that model_outputs
      # can be removed from the Interpreter API.
      model_outputs = list(mdl.predict(model_inputs))
      assert len(model_outputs) == len(model_inputs)
    else:
      model_outputs = None
    return interp.run(
        model_inputs,
        mdl,
        self._datasets[dataset_name],
        model_outputs=model_outputs,
        config=data.get('config'),
    )
  def _get_metrics(
      self,
      data: types.JsonDict,
      model: Optional[str] = None,
      dataset_name: Optional[str] = None,
      metrics: Optional[str] = None,
      # TODO(b/278586715): Remove this parameter once linked bug is fixed.
      do_predict: str = '1',  # bool URL param; encoding as "0" /  "1" is safer.
      **unused_kw,
  ) -> types.JsonDict:
    """Run the specified Metrics components.
    Args:
      data: JSON parsed from the HTTP Request body containing the inputs
        (required) and config (optional) for parameterizing the Metrics calls.
      model: The name of the model loaded in LIT, used to fetch the model
        predictions.
      dataset_name: The name of the dataset containing the ground truth labels
        for the provided inputs.
      metrics: An optional comma-separated string of metrics to run, if None it
        will run all Metrics loaded in this LitApp instance.
      do_predict: If true (default), will fetch the model predictions in this
        function using `_get_preds()` and pass them through to each Metrics
        component's run function.
      **unused_kw: Unused keyword arguments.
    Returns:
      A dictionary of metrics results where the keys are the name of the
      Metrics component and the values are list of dictionaries containing the
      prediction key (`pred_key`), the label key (`label_key`), and `metrics`
      for that pair of keys as a `Mapping[str, float]`.
    Raises:
      KeyError: If a model, dataset, or metric with the specified name is not
        loaded in the LitApp instance.
      ValueError: If there are no inputs.
    """
    if dataset_name is None:
      raise ValueError('Must provide a "dataset_name" to get examples.')
    if model is None:
      raise ValueError('Must provide a "model" name to get predictions.')
    inputs = data.get('inputs')
    if not inputs:
      raise ValueError('Metrics cannot be computed without inputs.')
    if metrics:
      metrics_to_run = tuple(m for m in metrics.split(',') if m)
      unknown_metrics = [m for m in metrics_to_run if m not in self._metrics]
      if unknown_metrics:
        raise KeyError(f'Requested unknown metrics "{unknown_metrics}".')
    else:
      metrics_to_run = tuple(self._metrics.keys())
    if utils.coerce_bool(do_predict):
      model_outputs = self._get_preds(data=data, model=model)
    else:
      model_outputs = None
    dataset: lit_dataset.IndexedDataset = self._datasets[dataset_name]
    mdl: lit_model.Model = self._models[model]
    config: Optional[types.JsonDict] = data.get('config')
    results: dict[str, Any] = {}
    for name in metrics_to_run:
      # TODO(b/254833485): Add type annotation once the metrics wrapper classes
      # inherit from lit_component.Metrics.
      metric = self._metrics[name]
      config_spec: types.Spec = metric.config_spec()
      if config_spec and config is not None:
        utils.validate_config_against_spec(
            config, config_spec, f'Metric {name}'
        )
      results[name] = metric.run(
          [ex['data'] for ex in inputs],
          mdl,
          dataset,
          model_outputs=model_outputs,
          config=config
      )
    return results
  def _push_ui_state(
      self,
      data: types.JsonDict,
      dataset_name: Optional[str] = None,
      **unused_kw,
  ):
    """Push UI state back to Python."""
    tracker: Optional[ui_state.UIStateTracker] = self.ui_state_tracker
    if tracker is None:
      raise RuntimeError(
          'Attempted to push UI state, but that is not enabled for this server.'
      )
    if dataset_name is None:
      raise ValueError('Must provide a "dataset_name" to get base examples.')
    options = data.get('config', {})
    tracker.update_state(
        data['inputs'], self._datasets[dataset_name], dataset_name, **options
    )
  def _validate(
      self,
      validate: Optional[flag_helpers.ValidationMode],
      enforce_dataset_fields_required: bool = False,
      report_all: bool = False,
  ):
    """Validate all datasets and models loaded for proper setup."""
    if validate is None or validate == flag_helpers.ValidationMode.OFF:
      return
    datasets_to_validate = {}
    for dataset in self._datasets:
      if validate == flag_helpers.ValidationMode.ALL:
        datasets_to_validate[dataset] = self._datasets[dataset]
      elif validate == flag_helpers.ValidationMode.FIRST:
        datasets_to_validate[dataset] = self._datasets[dataset].slice[:1]
      elif validate == flag_helpers.ValidationMode.SAMPLE:
        sample_size = math.ceil(len(self._datasets[dataset]) * 0.05)
        datasets_to_validate[dataset] = self._datasets[dataset].sample(
            sample_size)
    for dataset in datasets_to_validate:
      logging.info("Validating dataset '%s'", dataset)
      validation.validate_dataset(
          datasets_to_validate[dataset],
          enforce_all_fields_required=enforce_dataset_fields_required,
          report_all=report_all
      )
    for model, model_info in self._info['models'].items():
      for dataset in model_info['datasets']:
        logging.info("Validating model '%s' on dataset '%s'", model, dataset)
        validation.validate_model(
            self._models[model], datasets_to_validate[dataset], report_all)
  def _warm_start(self,
                  rate: float,
                  progress_indicator: Optional[ProgressIndicator] = None):
    """Warm-up the predictions cache by making some model calls."""
    assert rate >= 0 and rate <= 1
    for model, model_info in self._info['models'].items():
      for dataset_name in model_info['datasets']:
        logging.info("Warm-start of model '%s' on dataset '%s'", model,
                     dataset_name)
        all_examples: list[IndexedInput] = self._get_dataset([], dataset_name)
        if rate < 1:
          examples = random.sample(all_examples, int(len(all_examples) * rate))
          logging.info('Partial warm-start: running on %d/%d examples.',
                       len(examples), len(all_examples))
        else:
          examples = all_examples
        _ = self._get_preds(data={'inputs': examples},
                            model=model,
                            progress_indicator=progress_indicator)
  def _warm_projections(self, interpreters: list[str]):
    """Pre-compute UMAP/PCA projections with default arguments."""
    for interpreter_name in interpreters:
      if interpreter_name not in self._interpreters:
        continue
      for model, model_info in self._info['models'].items():
        for dataset_name in model_info['datasets']:
          embedding_fields = utils.find_spec_keys(model_info['spec']['output'],
                                                  types.Embeddings)
          # Only warm-start on the first embedding field, since if models return
          # many different embeddings this can take a long time.
          for field_name in embedding_fields[:1]:
            config = dict(
                dataset_name=dataset_name,
                model_name=model,
                field_name=field_name,
                use_input=False,
                proj_kw={'n_components': 3})
            data = {'inputs': [], 'config': config}
            _ = self._get_interpretations(
                data=data,
                model=model,
                dataset_name=dataset_name,
                interpreter=interpreter_name)
  def _run_annotators(self,
                      dataset: lit_dataset.Dataset) -> lit_dataset.Dataset:
    datapoints = [dict(ex) for ex in dataset.examples]
    annotated_spec = dict(dataset.spec())
    for annotator in self._annotators:
      annotator.annotate(datapoints, dataset, annotated_spec)
    return lit_dataset.Dataset(
        base=dataset, examples=datapoints, spec=annotated_spec)
  def make_handler(self, fn):
    """Convenience wrapper to handle args and serialization.
    This is a thin shim between server (handler, request, environ) and model
    logic (inputs, args, outputs).
    Args:
      fn: function (JsonDict, **kw) -> JsonDict
    Returns:
      fn wrapped as a request handler
    """
    @functools.wraps(fn)
    def _handler(app: wsgi_app.App, request, environ):
      kw = request.args.to_dict()
      # The frontend needs "simple" data (e.g. NumPy arrays converted to lists),
      # but for requests from Python we may want to use the invertible encoding
      # so that datatypes from remote models are the same as local ones.
      response_simple_json = utils.coerce_bool(
          kw.pop('response_simple_json', True)
      )
      data = serialize.from_json(request.data) if len(request.data) else None
      # Special handling to dereference IDs.
      if (
          data
          and 'inputs' in data.keys()
          and data.get('inputs')
          and 'dataset_name' in kw
      ):
        data['inputs'] = self._reconstitute_inputs(
            data['inputs'], kw['dataset_name']
        )
        # Validate that id and data._id match.
        # TODO(b/171513556): consider removing this if we can simplify the
        # data representation on the frontend so id and meta are not replicated.
        for ex in data['inputs']:
          if ex['id'] != ex['data'].get('_id'):
            raise ValueError(
                'Error: malformed example with inconsistent ids:'
                f' {str(ex)}\nfrom request'
                f' {request.path} {str(request.args.to_dict())}'
            )
      outputs = fn(data, **kw)
      response_body = serialize.to_json(outputs, simple=response_simple_json)
      return app.respond(request, response_body, 'application/json', 200)
    return _handler
  def __init__(
      self,
      models: lit_model.ModelMap,
      datasets: Mapping[str, lit_dataset.Dataset],
      generators: Optional[Mapping[str, lit_components.Generator]] = None,
      interpreters: Optional[Mapping[str, lit_components.Interpreter]] = None,
      metrics: Optional[Mapping[str, lit_components.Metrics]] = None,
      annotators: Optional[list[lit_components.Annotator]] = None,
      layouts: Optional[layout.LitComponentLayouts] = None,
      dataset_loaders: Optional[DatasetLoadersMap] = None,
      model_loaders: Optional[ModelLoadersMap] = None,
      # General server config; see server_flags.py.
      data_dir: Optional[str] = None,
      warm_start: float = 0.0,
      warm_start_progress_indicator: Optional[
          ProgressIndicator
      ] = tqdm.tqdm,  # not in server_flags
      warm_projections: bool = False,
      client_root: Optional[str] = None,
      demo_mode: bool = False,
      default_layout: Optional[str] = None,
      canonical_url: Optional[str] = None,
      page_title: Optional[str] = None,
      development_demo: bool = False,
      inline_doc: Optional[str] = None,
      onboard_start_doc: Optional[str] = None,
      onboard_end_doc: Optional[str] = None,
      sync_state: bool = False,  # notebook-only; not in server_flags
      validate: Optional[flag_helpers.ValidationMode] = None,
      report_all: bool = False,
      enforce_dataset_fields_required: bool = False,
      strict_cache_id_validation: bool = False,
  ):
    if client_root is None:
      raise ValueError('client_root must be set on application')
    self._demo_mode = demo_mode
    self._development_demo = development_demo
    self._default_layout = default_layout
    self._canonical_url = canonical_url
    self._page_title = page_title
    self._inline_doc = inline_doc
    self._onboard_start_doc = onboard_start_doc
    self._onboard_end_doc = onboard_end_doc
    self._data_dir = data_dir
    if data_dir and not os.path.isdir(data_dir):
      os.mkdir(data_dir)
    self._caching_model_wrapper_kw = dict(
        cache_dir=self._data_dir,
        strict_id_validation=strict_cache_id_validation,
        id_hash_fn=caching.input_hash,
    )
    self._layouts = layouts if layouts else layout.DEFAULT_LAYOUTS
    self._model_loaders: ModelLoadersMap = model_loaders or {}
    self._models: dict[str, caching.CachingModelWrapper] = {}
    for name, model in models.items():
      if model_loaders is None:
        # Attempt to infer an init spec for the model before we lose access to
        # the original after wrapping it in a CachingModelWrapper.
        self._model_loaders[name] = (type(model), model.init_spec())
      # Wrap model in caching wrapper and add it to the app
      self._models[name] = caching.CachingModelWrapper(
          model, name, **self._caching_model_wrapper_kw
      )
    self._annotators: list[lit_components.Annotator] = annotators or []
    self._saved_datapoints = {}
    self._saved_datapoints_lock = threading.Lock()
    tmp_datasets: dict[str, lit_dataset.Dataset] = dict(datasets)
    # TODO(b/202210900): get rid of this, just dynamically create the empty
    # dataset on the frontend.
    tmp_datasets[_EMPTY_DATASET_KEY] = lit_dataset.NoneDataset(self._models)
    self._dataset_loaders: DatasetLoadersMap = dataset_loaders or {}
    self._datasets: dict[str, lit_dataset.IndexedDataset] = {}
    for name, ds in tmp_datasets.items():
      if dataset_loaders is None:
        # Attempt to infer an init spec for the dataset before we lose access to
        # the original during dataset annotation and indexing.
        self._dataset_loaders[name] = (type(ds), ds.init_spec())
      # Anotate the dataset
      annotated_ds = self._run_annotators(ds)
      # Index the annotated dataset and add it to the app
      self._datasets[name] = lit_dataset.IndexedDataset(
          base=annotated_ds, id_fn=caching.input_hash)
    # Generator initialization
    if generators is not None:
      self._generators = generators
    else:
      self._generators = core.default_generators()
    # Interpreter initialization
    if interpreters is not None:
      self._interpreters = core.required_interpreters() | interpreters
    else:
      self._interpreters = core.default_interpreters(self._models)
    if metrics is not None:
      self._metrics = metrics
    else:
      self._metrics = core.default_metrics()
    # Component to sync state from TS -> Python. Used in notebooks.
    if sync_state:
      self.ui_state_tracker = ui_state.UIStateTracker()
    else:
      self.ui_state_tracker = None
    # Information on models, datasets, and other components.
    self._info = self._build_metadata()
    # Validate datasets and models if specified.
    self._validate(
        validate,
        enforce_dataset_fields_required=enforce_dataset_fields_required,
        report_all=report_all
    )
    # Optionally, run models to pre-populate cache.
    if warm_projections:
      logging.info(
          'Projection (dimensionality reduction) warm-start requested; '
          'will do full warm-start for all models since predictions are needed.'
      )
      warm_start = 1.0
    if warm_start > 0:
      self._warm_start(
          rate=warm_start, progress_indicator=warm_start_progress_indicator)
      self.save_cache()
      if warm_start >= 1:
        warm_projections = True
    # If you add a new embedding projector that should be warm-started,
    # also add it to the list here.
    # TODO(lit-dev): add some registry mechanism / automation if this grows to
    # more than 2-3 projection types.
    if warm_projections:
      self._warm_projections(['pca', 'umap'])
    handlers = {
        # Metadata endpoints.
        '/get_info': self._get_info,
        # Dataset-related endpoints.
        '/get_dataset': self._get_dataset,
        '/create_dataset': self._create_dataset,
        '/create_model': self._create_model,
        '/get_generated': self._get_generated,
        '/save_datapoints': self._save_datapoints,
        '/load_datapoints': self._load_datapoints,
        '/annotate_new_data': self._annotate_new_data,
        '/post_new_data': self._post_new_data,
        '/fetch_new_data': self._fetch_new_data,
        '/remove_new_data': self._remove_new_data,
        '/push_ui_state': self._push_ui_state,
        # Model prediction endpoints.
        '/get_preds': self._get_preds,
        '/get_interpretations': self._get_interpretations,
        '/get_metrics': self._get_metrics,
    }
    wrapped_handlers = {k: self.make_handler(v) for k, v in handlers.items()}
    self._wsgi_app = wsgi_app.App(
        # Wrap endpoint fns to take (handler, request, environ)
        handlers=wrapped_handlers,
        project_root=client_root,
        index_file='static/index.html',
    )
  def get_dataset_specs(self) -> dict[str, dict[str, str]]:
    datasets_with_spec = collections.defaultdict(dict)
    for name, ds in self._datasets.items():
      for field_name, lit_data_class in ds.spec().items():
        datasets_with_spec[name][field_name] = type(lit_data_class).__name__
    return datasets_with_spec
  def save_cache(self):
    for m in self._models.values():
      if isinstance(m, caching.CachingModelWrapper):
        m.save_cache()
  def __call__(self, environ, start_response):
    """Implementation of the WSGI interface."""
    return self._wsgi_app(environ, start_response)

================
File: lit_nlp/components/__init__.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

================
File: lit_nlp/components/ablation_flip_int_test.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Tests for lit_nlp.components.ablation_flip."""
from collections.abc import Iterable, Iterator
from absl.testing import absltest
from lit_nlp.api import types
from lit_nlp.components import ablation_flip
from lit_nlp.examples.glue import models as glue_models
import numpy as np
from lit_nlp.lib import file_cache
BERT_TINY_PATH = file_cache.cached_path(
    'https://storage.googleapis.com/what-if-tool-resources/lit-models/sst2_tiny.tar.gz',  # pylint: disable=line-too-long
    extract_compressed_file=True,
)
STSB_PATH = file_cache.cached_path(
    'https://storage.googleapis.com/what-if-tool-resources/lit-models/stsb_tiny.tar.gz',  # pylint: disable=line-too-long
    extract_compressed_file=True,
)
class SST2ModelNonRequiredField(glue_models.SST2Model):
  def input_spec(self):
    spec = super().input_spec()
    spec['sentence'] = types.TextSegment(required=False, default='')
    return spec
class SST2ModelWithPredictCounter(glue_models.SST2Model):
  def __init__(self, *args, **kw):
    super().__init__(*args, **kw)
    self.predict_counter = 0
  def predict(self,
              inputs: Iterable[types.JsonDict],
              **kw) -> Iterator[types.JsonDict]:
    results = super().predict(inputs, **kw)
    self.predict_counter += 1
    return results
class ModelBasedAblationFlipTest(absltest.TestCase):
  def setUp(self):
    super(ModelBasedAblationFlipTest, self).setUp()
    self.ablation_flip = ablation_flip.AblationFlip()
    # Classification model that clasifies a given input sentence.
    self.classification_model = glue_models.SST2Model(BERT_TINY_PATH)
    self.classification_config = {ablation_flip.PREDICTION_KEY: 'probas'}
    # Classification model with the 'sentence' field marked as
    # non-required.
    self.classification_model_non_required_field = SST2ModelNonRequiredField(
        BERT_TINY_PATH)
    # Classification model with a counter to count number of predict calls.
    # TODO(ataly): Consider setting up a Mock object to count number of
    # predict calls.
    self.classification_model_with_predict_counter = (
        SST2ModelWithPredictCounter(BERT_TINY_PATH))
    # Regression model determining similarity between two input sentences.
    self.regression_model = glue_models.STSBModel(STSB_PATH)
    self.regression_config = {ablation_flip.PREDICTION_KEY: 'score'}
  def test_ablation_flip_num_ex(self):
    ex = {'sentence': 'this long movie was terrible'}
    self.classification_config[ablation_flip.NUM_EXAMPLES_KEY] = 0
    self.classification_config[ablation_flip.FIELDS_TO_ABLATE_KEY] = [
        'sentence'
    ]
    self.assertEmpty(
        self.ablation_flip.generate(ex, self.classification_model, None,
                                    self.classification_config))
    self.classification_config[ablation_flip.NUM_EXAMPLES_KEY] = 1
    self.assertLen(
        self.ablation_flip.generate(ex, self.classification_model, None,
                                    self.classification_config), 1)
    self.classification_config[ablation_flip.NUM_EXAMPLES_KEY] = 2
    self.assertLen(
        self.ablation_flip.generate(ex, self.classification_model, None,
                                    self.classification_config), 2)
  def test_ablation_flip_tokens_to_ignore(self):
    ex = {'sentence': 'this long movie was terrible'}
    self.classification_config[ablation_flip.NUM_EXAMPLES_KEY] = 1
    self.classification_config[ablation_flip.TOKENS_TO_IGNORE_KEY] = [
        'terrible']
    self.classification_config[ablation_flip.FIELDS_TO_ABLATE_KEY] = [
        'sentence'
    ]
    self.assertEmpty(
        self.ablation_flip.generate(ex, self.classification_model, None,
                                    self.classification_config))
  def test_ablation_flip_num_ex_multi_input(self):
    ex = {'sentence1': 'this long movie is terrible',
          'sentence2': 'this short movie is great'}
    self.regression_config[ablation_flip.NUM_EXAMPLES_KEY] = 2
    thresh = 2
    self.regression_config[ablation_flip.REGRESSION_THRESH_KEY] = thresh
    self.regression_config[ablation_flip.FIELDS_TO_ABLATE_KEY] = [
        'sentence1',
        'sentence2',
    ]
    self.assertLen(
        self.ablation_flip.generate(ex, self.regression_model, None,
                                    self.regression_config), 2)
  def test_ablation_flip_long_sentence(self):
    sentence = (
        'this was a terrible terrible movie but I am a writing '
        'a nice long review for testing whether AblationFlip '
        'can handle long sentences with a bounded number of '
        'predict calls.')
    ex = {'sentence': sentence}
    self.classification_config[ablation_flip.NUM_EXAMPLES_KEY] = 100
    self.classification_config[ablation_flip.MAX_ABLATIONS_KEY] = 100
    self.classification_config[ablation_flip.FIELDS_TO_ABLATE_KEY] = [
        'sentence'
    ]
    model = self.classification_model_with_predict_counter
    cfs = self.ablation_flip.generate(
        ex, model, None, self.classification_config)
    # This example must yield 19 ablation_flips.
    self.assertLen(cfs, 19)
    # Number of predict calls made by ablation_flip should be upper-bounded by
    # <number of tokens in sentence> + 2**MAX_ABLATABLE_TOKENS
    num_tokens = len(model.tokenizer(sentence))
    num_predict_calls = model.predict_counter
    self.assertLessEqual(num_predict_calls,
                         num_tokens + 2**ablation_flip.MAX_ABLATABLE_TOKENS)
    # We use a smaller value of MAX_ABLATABLE_TOKENS and check that the
    # number of predict calls is smaller, and that the prediction bound still
    # holds.
    model.predict_counter = 0
    ablation_flip.MAX_ABLATABLE_TOKENS = 5
    self.assertLessEqual(model.predict_counter, num_predict_calls)
    self.assertLessEqual(model.predict_counter,
                         num_tokens + 2**ablation_flip.MAX_ABLATABLE_TOKENS)
  def test_ablation_flip_freeze_fields(self):
    ex = {'sentence1': 'this long movie is terrible',
          'sentence2': 'this long movie is great'}
    self.regression_config[ablation_flip.NUM_EXAMPLES_KEY] = 10
    thresh = 2
    self.regression_config[ablation_flip.REGRESSION_THRESH_KEY] = thresh
    self.regression_config[ablation_flip.FIELDS_TO_ABLATE_KEY] = [
        'sentence1'
    ]
    cfs = self.ablation_flip.generate(ex, self.regression_model, None,
                                      self.regression_config)
    for cf in cfs:
      self.assertEqual(cf['sentence2'], ex['sentence2'])
  def test_ablation_flip_max_ablations(self):
    ex = {'sentence': 'this movie is terrible'}
    ex_tokens = self.ablation_flip.tokenize(ex['sentence'])
    self.classification_config[ablation_flip.NUM_EXAMPLES_KEY] = 1
    self.classification_config[ablation_flip.MAX_ABLATIONS_KEY] = 1
    self.classification_config[ablation_flip.FIELDS_TO_ABLATE_KEY] = [
        'sentence'
    ]
    cfs = self.ablation_flip.generate(
        ex, self.classification_model, None, self.classification_config)
    cf_tokens = self.ablation_flip.tokenize(list(cfs)[0]['sentence'])
    self.assertLen(cf_tokens, len(ex_tokens) - 1)
    ex = {'sentence': 'this long movie is terrible and horrible.'}
    self.classification_config[ablation_flip.NUM_EXAMPLES_KEY] = 1
    self.classification_config[ablation_flip.MAX_ABLATIONS_KEY] = 1
    self.classification_config[ablation_flip.FIELDS_TO_ABLATE_KEY] = [
        'sentence'
    ]
    cfs = self.ablation_flip.generate(
        ex, self.classification_model, None, self.classification_config)
    self.assertEmpty(cfs)
  def test_ablation_flip_max_ablations_multi_input(self):
    ex = {'sentence1': 'this movie is terrible',
          'sentence2': 'this movie is great'}
    ex_tokens1 = self.ablation_flip.tokenize(ex['sentence1'])
    ex_tokens2 = self.ablation_flip.tokenize(ex['sentence2'])
    self.regression_config[ablation_flip.NUM_EXAMPLES_KEY] = 20
    self.regression_config[ablation_flip.REGRESSION_THRESH_KEY] = 2
    max_ablations = 1
    self.regression_config[ablation_flip.MAX_ABLATIONS_KEY] = max_ablations
    self.regression_config[ablation_flip.FIELDS_TO_ABLATE_KEY] = [
        'sentence1',
        'sentence2',
    ]
    cfs = self.ablation_flip.generate(ex, self.regression_model, None,
                                      self.regression_config)
    for cf in cfs:
      # Number of ablations in each field should be no more than MAX_ABLATIONS.
      cf_tokens1 = self.ablation_flip.tokenize(cf['sentence1'])
      cf_tokens2 = self.ablation_flip.tokenize(cf['sentence2'])
      self.assertGreaterEqual(
          len(cf_tokens1) + len(cf_tokens2),
          len(ex_tokens1) + len(ex_tokens2) - max_ablations)
  def test_ablation_flip_yields_multi_field_ablations(self):
    ex = {'sentence1': 'this short movie is awesome',
          'sentence2': 'this short movie is great'}
    ex_tokens1 = self.ablation_flip.tokenize(ex['sentence1'])
    ex_tokens2 = self.ablation_flip.tokenize(ex['sentence2'])
    self.regression_config[ablation_flip.NUM_EXAMPLES_KEY] = 20
    self.regression_config[ablation_flip.REGRESSION_THRESH_KEY] = 2
    self.regression_config[ablation_flip.MAX_ABLATIONS_KEY] = 5
    self.regression_config[ablation_flip.FIELDS_TO_ABLATE_KEY] = [
        'sentence1',
        'sentence2',
    ]
    cfs = self.ablation_flip.generate(ex, self.regression_model, None,
                                      self.regression_config)
    # Verify that at least one counterfactual involves ablations across
    # multiple fields.
    multi_field_ablation_found = False
    for cf in cfs:
      cf_tokens1 = self.ablation_flip.tokenize(cf['sentence1'])
      cf_tokens2 = self.ablation_flip.tokenize(cf['sentence2'])
      if ((len(cf_tokens1) < len(ex_tokens1))
          and (len(cf_tokens2) < len(ex_tokens2))):
        multi_field_ablation_found = True
        break
    self.assertTrue(multi_field_ablation_found)
  def test_ablation_flip_changes_pred_class(self):
    ex = {'sentence': 'this long movie is terrible'}
    ex_output = list(self.classification_model.predict([ex]))[0]
    pred_class = str(np.argmax(ex_output['probas']))
    self.assertEqual('0', pred_class)
    self.classification_config[ablation_flip.FIELDS_TO_ABLATE_KEY] = [
        'sentence'
    ]
    cfs = self.ablation_flip.generate(ex, self.classification_model, None,
                                      self.classification_config)
    cf_outputs = self.classification_model.predict(cfs)
    for cf_output in cf_outputs:
      self.assertNotEqual(np.argmax(ex_output['probas']),
                          np.argmax(cf_output['probas']))
  def test_ablation_flip_changes_regression_score(self):
    ex = {'sentence1': 'this long movie is terrible',
          'sentence2': 'this short movie is great'}
    self.regression_config[ablation_flip.NUM_EXAMPLES_KEY] = 2
    ex_output = list(self.regression_model.predict([ex]))[0]
    thresh = 2
    self.regression_config[ablation_flip.REGRESSION_THRESH_KEY] = thresh
    self.regression_config[ablation_flip.FIELDS_TO_ABLATE_KEY] = [
        'sentence1',
        'sentence2',
    ]
    cfs = self.ablation_flip.generate(ex, self.regression_model, None,
                                      self.regression_config)
    cf_outputs = self.regression_model.predict(cfs)
    for cf_output in cf_outputs:
      self.assertNotEqual((ex_output['score'] <= thresh),
                          (cf_output['score'] <= thresh))
  def test_ablation_flip_fails_without_pred_key(self):
    ex = {'sentence': 'this long movie is terrible'}
    with self.assertRaises(AssertionError):
      self.ablation_flip.generate(ex, self.classification_model, None, None)
  def test_ablation_flip_required_field(self):
    ex = {'sentence': 'terrible'}
    self.classification_config[ablation_flip.NUM_EXAMPLES_KEY] = 1
    self.classification_config[ablation_flip.FIELDS_TO_ABLATE_KEY] = [
        'sentence'
    ]
    self.assertEmpty(
        self.ablation_flip.generate(
            ex, self.classification_model, None, self.classification_config))
    self.assertLen(
        self.ablation_flip.generate(
            ex, self.classification_model_non_required_field,
            None, self.classification_config), 1)
if __name__ == '__main__':
  absltest.main()

================
File: lit_nlp/components/ablation_flip_test.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Tests for lit_nlp.components.ablation_flip."""
from collections.abc import Callable
from absl.testing import absltest
from absl.testing import parameterized
from lit_nlp.api import dataset as lit_dataset
from lit_nlp.api import model as lit_model
from lit_nlp.api import types
from lit_nlp.components import ablation_flip
from lit_nlp.lib import testing_utils
INPUT_SPEC_SPARSE_MULTI = {'input': types.SparseMultilabel()}
INPUT_SPEC_TEXT = {'input': types.TextSegment()}
INPUT_SPEC_URL = {'input': types.URL()}
_RegressionModelForTesting = testing_utils.RegressionModelForTesting
class _ClassificationTestModel(testing_utils.ClassificationModelForTesting):
  def __init__(self, input_spec: types.Spec):
    self._input_spec = input_spec
  def input_spec(self) -> types.Spec:
    return self._input_spec
class _BadOutputTestModel(_ClassificationTestModel):
  def output_spec(self) -> types.Spec:
    return {}
class AblationFlipTest(parameterized.TestCase):
  def setUp(self):
    super(AblationFlipTest, self).setUp()
    self.ablation_flip = ablation_flip.AblationFlip()
  @parameterized.named_parameters(
      ('cls_sparse', _ClassificationTestModel, INPUT_SPEC_SPARSE_MULTI, True),
      ('cls_text', _ClassificationTestModel, INPUT_SPEC_TEXT, True),
      ('cls_url', _ClassificationTestModel, INPUT_SPEC_URL, True),
      ('cls_none', _ClassificationTestModel, {}, False),
      ('reg_sparse', _RegressionModelForTesting, INPUT_SPEC_SPARSE_MULTI, True),
      ('reg_text', _RegressionModelForTesting, INPUT_SPEC_TEXT, True),
      ('reg_url', _RegressionModelForTesting, INPUT_SPEC_URL, True),
      ('reg_none', _RegressionModelForTesting, {}, False),
      ('bad_sparse', _BadOutputTestModel, INPUT_SPEC_SPARSE_MULTI, False),
      ('bad_text', _BadOutputTestModel, INPUT_SPEC_TEXT, False),
      ('bad_url', _BadOutputTestModel, INPUT_SPEC_URL, False),
      ('bad_none', _BadOutputTestModel, {}, False),
  )
  def test_ablation_flip_is_compatible(self,
                                       model_ctr: Callable[[types.Spec],
                                                           lit_model.Model],
                                       input_spec: types.Spec,
                                       exp: bool):
    model = model_ctr(input_spec)
    compatible = self.ablation_flip.is_compatible(
        model, lit_dataset.NoneDataset({'test': model}))
    self.assertEqual(compatible, exp)
if __name__ == '__main__':
  absltest.main()

================
File: lit_nlp/components/ablation_flip.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""AblationFlip generator that ablates input tokens to flip the prediction.
An AblationFlip is defined as a counterfactual input that ablates one or more
tokens in the input at hand in order to obtain a different prediction.
An AblationFlip is considered minimal if no strict subset of the applied token
ablations succeeds in flipping the prediction.
This generator builds on ideas from the following paper.
(1) Local Explanations via Necessity and Sufficiency: Unifying Theory and
    Practice
    David Watson, Limor Gultchin, Ankur Taly, Luciano Floridi
    UAI 2021.
    https://arxiv.org/abs/2103.14651
"""
import collections
from collections.abc import Iterator
import itertools
from typing import Any, Optional
from absl import logging
from lit_nlp.api import components as lit_components
from lit_nlp.api import dataset as lit_dataset
from lit_nlp.api import model as lit_model
from lit_nlp.api import types
from lit_nlp.components import cf_utils
from lit_nlp.lib import utils
JsonDict = types.JsonDict
Spec = types.Spec
PREDICTION_KEY = "Prediction key"
NUM_EXAMPLES_KEY = "Max number of counterfacuals to generate"
NUM_EXAMPLES_DEFAULT = 10
MAX_ABLATIONS_KEY = "Max number of token ablations"
MAX_ABLATIONS_DEFAULT = 5
TOKENS_TO_IGNORE_KEY = "Comma-separated list of tokens to never flip"
TOKENS_TO_IGNORE_DEFAULT = []
REGRESSION_THRESH_KEY = "Regression threshold"
REGRESSION_THRESH_DEFAULT = 1.0
FIELDS_TO_ABLATE_KEY = "Fields to ablate"
MAX_ABLATABLE_TOKENS = 10
ABLATED_TOKENS_KEY = "ablated_tokens"
class AblationFlip(lit_components.Generator):
  """AblationFlip generator.
  Ablates tokens in input text to generate counterfactuals that flip the
  prediction.
  This generator works for both classification and regression models. In the
  case of classification models, the returned counterfactuals are guaranteed to
  have a different prediction class as the original example. In the case of
  regression models, the returned counterfactuals are guaranteed to be on the
  opposite side of a user-provided threshold as the original example.
  The returned counterfactuals are guaranteed to be minimal in the sense that
  no strict subset of the applied ablations would have resulted in a
  prediction flip.
  The number of model predictions made by this generator is upper-bounded by
  number of tokens + 2^MAX_ABLATABLE_TOKENS. Since MAX_ABLATABLE_TOKENS is a
  constant, effectively this generator makes O(n) model predictions, where n is
  the total number of tokens across all input fields.
  """
  def __init__(self):
    # TODO(ataly): Use a more sophisticated tokenizer and detokenizer.
    self.tokenize = str.split
    self.detokenize = " ".join
  def description(self) -> str:
    # TODO(lit-dev): Find way to have newlines in the string and have it be
    # displayed correctly on the front-end.
    return """Removes minimal tokens (based on whitespace) to cause a model to
      return a different prediction.\n\nIn the
      case of classification models, the returned counterfactuals are guaranteed
      to have a different prediction class as the original example. In the case
      of regression models, the returned counterfactuals are guaranteed to be on
      the opposite side of a user-provided threshold as the original example.
      \n\nCan fail to produce counterfactuals if there is no set of ablations
      within the scope of the configuration options that cause significant model
      prediction changes.
    """
  def _subset_exists(self, cand_set, sets):
    """Checks whether a subset of 'cand_set' exists in 'sets'."""
    for s in sets:
      if s.issubset(cand_set):
        return True
    return False
  def _gen_ablation_idxs(
      self,
      loo_scores: list[tuple[str, int, float]],
      max_ablations: int,
      orig_regression_score: Optional[float] = None,
      regression_thresh: Optional[float] = None
  ) -> Iterator[tuple[tuple[str, int], ...]]:
    """Generates sets of token positions that are eligible for ablation."""
    # Order tokens by their leave-one-out ablation scores. (Note that these
    # would be negative if the ablation results in reducing the score.) We use
    # ascending order for classification tasks. For regression tasks, the order
    # is based on whether the original score is above or below the threshold --
    # we use ascending order for the former, and descending for the latter.
    loo_scores = sorted(loo_scores, key=lambda item: item[2])
    ablation_idxs = [(f, idx) for f, idx, _ in loo_scores]
    if regression_thresh and orig_regression_score <= regression_thresh:
      ablation_idxs = ablation_idxs[::-1]
    # Only consider the top tokens up to MAX_ABLATABLE_TOKENS.
    ablation_idxs = ablation_idxs[:MAX_ABLATABLE_TOKENS]
    # Consider all combinations of tokens upto length max_ablations.
    for i in range(min(len(ablation_idxs), max_ablations)):
      for s in itertools.combinations(ablation_idxs, i+1):
        yield s
  def _get_tokens(self,
                  example: JsonDict,
                  input_spec: Spec,
                  input_field: str):
    input_ty = input_spec[input_field]
    if isinstance(input_ty, types.URL):
      return cf_utils.tokenize_url(example[input_field])
    elif isinstance(input_ty, types.SparseMultilabel):
      return example[input_field]
    elif isinstance(input_ty, types.TextSegment):
      return self.tokenize(example[input_field])
    else:
      return []
  def _create_cf(self,
                 example: JsonDict,
                 input_spec: Spec,
                 ablation_idxs: list[tuple[str, int]]) -> dict[str, Any]:
    # Build a dictionary mapping input fields to the token idxs to be ablated
    # from that field.
    ablation_idxs_per_field = collections.defaultdict(list)
    for field, idx in ablation_idxs:
      ablation_idxs_per_field[field].append(idx)
    ablation_idxs_per_field.default_factory = None  # lock
    cf = dict(example)
    for field, ablation_idxs in ablation_idxs_per_field.items():
      # Original list of tokens at the field.
      orig_tokens = self._get_tokens(example, input_spec, field)
      if input_spec[field].required and len(ablation_idxs) >= len(orig_tokens):
        # Update token_idxs so that we don't end up ablating all tokens.
        ablation_idxs = ablation_idxs[:-1]
      # Modified list of tokens obtained after ablating the tokens form the
      # indices in ablation_idxs.
      modified_tokens = [
          t for i, t in enumerate(orig_tokens) if i not in ablation_idxs
      ]
      # Update the field with the modified token list.
      input_ty = input_spec[field]
      if isinstance(input_ty, types.URL):
        url = example[field]
        modified_url = cf_utils.ablate_url_tokens(url, ablation_idxs)
        cf[field] = modified_url
      elif isinstance(input_ty, types.SparseMultilabel):
        cf[field] = modified_tokens
      elif isinstance(input_ty, types.TextSegment):
        cf[field] = self.detokenize(modified_tokens)
    return cf
  def _generate_leave_one_out_ablation_score(
      self,
      example: JsonDict,
      model: lit_model.Model,
      input_spec: Spec,
      output_spec: Spec,
      orig_output: JsonDict,
      pred_key: str,
      fields_to_ablate: list[str],
      tokens_to_ignore: list[str]) -> list[tuple[str, int, float]]:
    # Returns a list of triples: field, token_idx and leave-one-out score.
    ret = []
    for field in input_spec.keys():
      if field not in example or field not in fields_to_ablate:
        continue
      tokens = self._get_tokens(example, input_spec, field)
      idxs = [
          i
          for i, token in enumerate(tokens)
          if token not in tokens_to_ignore
      ]
      cf_outputs = model.predict([
          self._create_cf(example, input_spec, [(field, idx)])
          for idx in idxs
      ])
      for idx, cf_output in zip(idxs, cf_outputs):
        loo_score = cf_utils.prediction_difference(
            cf_output, orig_output, output_spec, pred_key
        )
        ret.append((field, idx, loo_score))
    return ret
  def is_compatible(self, model: lit_model.Model,
                    dataset: lit_dataset.Dataset) -> bool:
    del dataset  # Unused by AblationFlip
    supported_inputs = (types.SparseMultilabel, types.TextSegment, types.URL)
    supported_preds = (types.MulticlassPreds, types.RegressionScore)
    valid_inputs = utils.spec_contains(model.input_spec(), supported_inputs)
    valid_outputs = utils.spec_contains(model.output_spec(), supported_preds)
    return valid_inputs and valid_outputs
  def config_spec(self) -> types.Spec:
    return {
        NUM_EXAMPLES_KEY: types.TextSegment(default=str(NUM_EXAMPLES_DEFAULT)),
        MAX_ABLATIONS_KEY: types.TextSegment(
            default=str(MAX_ABLATIONS_DEFAULT)),
        TOKENS_TO_IGNORE_KEY:
            types.Tokens(default=TOKENS_TO_IGNORE_DEFAULT),
        PREDICTION_KEY:
            types.SingleFieldMatcher(
                spec="output", types=["MulticlassPreds", "RegressionScore"]),
        REGRESSION_THRESH_KEY:
            types.TextSegment(default=str(REGRESSION_THRESH_DEFAULT)),
        FIELDS_TO_ABLATE_KEY:
            types.MultiFieldMatcher(
                spec="input",
                types=["TextSegment", "SparseMultilabel"],
                select_all=True),
    }
  def generate(self,
               example: JsonDict,
               model: lit_model.Model,
               dataset: lit_dataset.Dataset,
               config: Optional[JsonDict] = None) -> list[JsonDict]:
    """Identify minimal sets of token albations that alter the prediction."""
    del dataset  # Unused.
    config = config or {}
    num_examples = int(config.get(NUM_EXAMPLES_KEY, NUM_EXAMPLES_DEFAULT))
    max_ablations = int(config.get(MAX_ABLATIONS_KEY, MAX_ABLATIONS_DEFAULT))
    tokens_to_ignore = config.get(TOKENS_TO_IGNORE_KEY,
                                  TOKENS_TO_IGNORE_DEFAULT)
    assert model is not None, "Please provide a model for this generator."
    input_spec = model.input_spec()
    pred_key = config.get(PREDICTION_KEY, "")
    regression_thresh = float(config.get(REGRESSION_THRESH_KEY,
                                         REGRESSION_THRESH_DEFAULT))
    output_spec = model.output_spec()
    assert pred_key, "Please provide the prediction key"
    assert pred_key in output_spec, "Invalid prediction key"
    is_regression = isinstance(output_spec[pred_key], types.RegressionScore)
    if not is_regression:
      assert isinstance(output_spec[pred_key], types.MulticlassPreds), (
          "Only classification or regression models are supported")
    logging.info(r"W3lc0m3 t0 Ablatl0nFl1p \o/")
    logging.info("Original example: %r", example)
    # Check for fields to ablate.
    fields_to_ablate = list(config.get(FIELDS_TO_ABLATE_KEY, []))
    if not fields_to_ablate:
      return []
    # Get model outputs.
    orig_output = list(model.predict([example]))[0]
    loo_scores = self._generate_leave_one_out_ablation_score(
        example, model, input_spec, output_spec, orig_output, pred_key,
        fields_to_ablate, tokens_to_ignore)
    if isinstance(output_spec[pred_key], types.RegressionScore):
      ablation_idxs_generator = self._gen_ablation_idxs(
          loo_scores,
          max_ablations,
          orig_output[pred_key],
          regression_thresh,
      )
    else:
      ablation_idxs_generator = self._gen_ablation_idxs(
          loo_scores, max_ablations
      )
    tokens_map = {}
    for field in input_spec.keys():
      tokens = self._get_tokens(example, input_spec, field)
      if not tokens:
        continue
      tokens_map[field] = tokens
    successful_cfs = []
    successful_positions = []
    for ablation_idxs in ablation_idxs_generator:
      if len(successful_cfs) >= num_examples:
        return successful_cfs
      # If a subset of the set of tokens have already been successful in
      # obtaining a flip, we continue. This ensures that we only consider
      # sets of tokens that are minimal.
      if self._subset_exists(set(ablation_idxs), successful_positions):
        continue
      # Create counterfactual and obtain model prediction.
      cf = self._create_cf(example, input_spec, ablation_idxs)  # pytype: disable=wrong-arg-types  # enable-nested-classes
      cf_output = list(model.predict([cf]))[0]
      # Check if counterfactual results in a prediction flip.
      if cf_utils.is_prediction_flip(
          cf_output, orig_output, output_spec, pred_key, regression_thresh):
        # Prediction flip found!
        cf_utils.update_prediction(cf, cf_output, output_spec, pred_key)
        cf[ABLATED_TOKENS_KEY] = str(
            [f"{field}[{tokens_map[field][idx]}]"
             for field, idx in ablation_idxs])
        successful_cfs.append(cf)
        successful_positions.append(set(ablation_idxs))
    return successful_cfs

================
File: lit_nlp/components/annotators.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Annotator implementations."""
from typing import Any, Optional
import attr
from lit_nlp.api import components as lit_components
from lit_nlp.api import dataset as lit_dataset
from lit_nlp.api import types
from lit_nlp.lib import utils
JsonDict = types.JsonDict
Spec = types.Spec
class PerFieldAnnotator(lit_components.Annotator):
  """Per-field annotator.
  Annotates individual fields from a dataset given an annotator model that takes
  a single input field in its input_spec() to create annotations.
  Each annotated field will be named as
  '{annotator name}:{annotator model output key}:{dataset field key}'.
  """
  def annotate(self, inputs: list[dict[str, Any]],
               dataset: lit_dataset.Dataset,
               dataset_spec_to_annotate: Optional[types.Spec] = None):
    if len(self._annotator_model.input_spec().items()) != 1:
      raise ValueError('Annotator model provided to PerFieldAnnotator does not '
                       'operate on a single field')
    datasets = {}
    for input_name, input_type in self._annotator_model.input_spec().items():
      # Do remap of inputs based on input name needed by annotator.
      ds_keys = utils.find_spec_keys(dataset.spec(), type(input_type))
      for ds_key in ds_keys:
        temp_ds = lit_dataset.Dataset(examples=inputs, base=dataset)
        datasets[ds_key] = temp_ds.remap({ds_key: input_name})
    for ds_key, ds in datasets.items():
      outputs = self._annotator_model.predict(ds.examples)
      for output_name, output_type in self._annotator_model.output_spec(
          ).items():
        # Update dataset spec with new annotated field.
        field_name = f'{self._name}:{output_name}:{ds_key}'
        if dataset_spec_to_annotate:
          dataset_spec_to_annotate[field_name] = attr.evolve(
              output_type, annotated=True)
        # Update all examples with annotator output.
        for example, output in zip(inputs, outputs):
          example[field_name] = output[output_name]

================
File: lit_nlp/components/backtranslator.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Backtranslation generator through Google Cloud Translate API."""
from collections.abc import Sequence
from typing import Optional
from absl import logging
from google.cloud import translate_v2 as translate
from lit_nlp.api import components as lit_components
from lit_nlp.api import dataset as lit_dataset
from lit_nlp.api import model as lit_model
from lit_nlp.api import types
from lit_nlp.lib import utils
import pandas as pd
_JsonDict = types.JsonDict
FIELDS_TO_BACKTRANSLATE_KEY = 'Fields to backtranslate'
class Backtranslator(lit_components.Generator):
  """Use Cloud Translate API as a Generator.
  In order to use this generator, you must have Cloud Translation set up
  through a Google Cloud project as described at
  https://cloud.google.com/translate/docs/setup and have downloaded your
  application credentials file locally and set the
  GOOGLE_APPLICATION_CREDENTIALS environment variable to point to that file.
  """
  def __init__(self,
               source_language: str = 'en',
               pivot_languages: Sequence[str] = ('fr', 'de')):
    self._source_lang = source_language
    self._pivot_langs = pivot_languages
    self._translate_client = translate.Client()
  def config_spec(self) -> types.Spec:
    return {
        FIELDS_TO_BACKTRANSLATE_KEY:
            types.MultiFieldMatcher(
                spec='input',
                types=['TextSegment'],
                select_all=True),
    }
  def generate_all(self,
                   inputs: list[_JsonDict],
                   model: lit_model.Model,
                   dataset: lit_dataset.Dataset,
                   config: Optional[_JsonDict] = None) -> list[list[_JsonDict]]:
    """Run generation on a set of inputs.
    If more than one field is to be backtranslated, each field is independently
    backtranslated per example. For example, if there are two fields to be
    backtranslated, this method will generate two examples per pivot language.
    Use this batch API by default, so we can make parallel requests.
    Args:
      inputs: sequence of inputs, following dataset.spec()
      model: unused
      dataset: dataset, used to access dataset.spec()
      config: additional runtime options
    Returns:
      list of list of new generated inputs, following dataset.spec()
    """
    del model
    all_outputs: list[list[_JsonDict]] = [[] for _ in inputs]
    config: _JsonDict = config or {}
    # Find text fields.
    text_fields = utils.find_spec_keys(dataset.spec(), types.TextSegment)
    # If config key is missing, backtranslate all text fields.
    fields_to_backtranslate: Sequence[str] = list(
        config.get(FIELDS_TO_BACKTRANSLATE_KEY, text_fields))
    candidates_by_field: dict[str, list[list[str]]] = {}
    for field_name in fields_to_backtranslate:
      texts = [ex[field_name] for ex in inputs]
      candidates_by_field[field_name] = self.generate_from_texts(texts)
    # Generate by substituting in each field.
    # TODO(lit-team): substitute on a combination of fields?
    for field_name, candidates in candidates_by_field.items():
      for i, (inp, cands) in enumerate(zip(inputs, candidates)):
        for cand in cands:
          all_outputs[i].append(utils.make_modified_input(
              inp, {field_name: cand}, 'Backtranslator'
          ))
    return all_outputs
  def generate(self,
               example: _JsonDict,
               model: lit_model.Model,
               dataset: lit_dataset.Dataset,
               config: Optional[_JsonDict] = None) -> list[_JsonDict]:
    """Generate from a single example."""
    return self.generate_all([example], model, dataset, config=config)[0]
  def generate_from_texts(self, texts: list[str]) -> list[list[str]]:
    """Run backtranslation on the list of strings."""
    # Use Pandas to keep track of metadata, so we can batch MT inputs
    # without losing track of which example they belong to.
    # Prepare input DataFrame
    dataframes = []
    for lang in self._pivot_langs:
      df = pd.DataFrame(data={'source': texts}).reset_index()
      df['pivot_language'] = lang
      dataframes.append(df)
    df = pd.concat(dataframes, axis=0).sort_values(by='index')
    # Forward translation
    # pylint: disable=g-complex-comprehension
    mt_inputs = [{
        'source': text,
        'source_language': self._source_lang,
        'target_language': lang
    } for text, lang in zip(df['source'], df['pivot_language'])]
    result = []
    for mt_input in mt_inputs:
      result.append(
          self._translate_client.translate(
              mt_input['source'],
              target_language=mt_input['target_language'],
              source_language=mt_input['source_language']))
    all_translations = [[r['translatedText']] for r in result]
    # Track metadata by replicating input rows
    # TODO(iftenney): replace with DataFrame.explode() once we can use
    # pandas 0.25
    rows = []
    for i, translation_set in enumerate(all_translations):
      for translation in translation_set:
        row = dict(df.iloc[i])
        row['pivot'] = translation
        rows.append(row)
    # Forward translation results
    intermediate_df = pd.DataFrame.from_records(rows)
    # TODO(lit-team): yield a chunk with intermediate state at this point,
    # for visualization before reverse translate is complete.
    # Reverse translation
    # pylint: disable=g-complex-comprehension
    mt_inputs = [{
        'source': text,
        'source_language': src,
        'target_language': self._source_lang
    } for text, src in zip(intermediate_df['pivot'],
                           intermediate_df['pivot_language'])]
    logging.info('Reverse: %d translations requested.', len(mt_inputs))
    result = []
    for mt_input in mt_inputs:
      result.append(
          self._translate_client.translate(
              mt_input['source'],
              target_language=mt_input['target_language'],
              source_language=mt_input['source_language']))
    all_translations = [[r['translatedText']] for r in result]
    # Track metadata by replicating input rows
    # TODO(iftenney): replace with DataFrame.explode() once we can use
    # pandas 0.25
    rows = []
    for i, translation_set in enumerate(all_translations):
      for translation in translation_set:
        row = dict(intermediate_df.iloc[i])
        row['target'] = translation
        rows.append(row)
    final_df = pd.DataFrame.from_records(rows)
    # Since we kept the indices in the DataFrame, we can group over these to get
    # the paraphrase candidates for each input.
    # this gives a list(list(str))
    candidates = list(
        final_df.groupby(by='index').agg({'target': list})['target'])
    return candidates

================
File: lit_nlp/components/cf_utils_test.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Tests for cf_utils."""
from absl.testing import absltest
from lit_nlp.components import cf_utils
class CfUtilsTest(absltest.TestCase):
  def test_tokenize_url(self):
    url = "http://www.google.com"
    tokens = ["http", "www", "google", "com"]
    self.assertEqual(tokens, cf_utils.tokenize_url(url))
    url = "http://www.gmail.com/mail/u/1/#inbox"
    tokens = ["http", "www", "gmail", "com", "mail", "u", "1", "inbox"]
    self.assertEqual(tokens, cf_utils.tokenize_url(url))
    url = "foobar"
    tokens = ["foobar"]
    self.assertEqual(tokens, cf_utils.tokenize_url(url))
    url = ""
    self.assertEmpty(cf_utils.tokenize_url(url))
    url = "/"
    self.assertEmpty(cf_utils.tokenize_url(url))
    url = "//"
    self.assertEmpty(cf_utils.tokenize_url(url))
    url = "//foobar/"
    tokens = ["foobar"]
    self.assertEqual(tokens, cf_utils.tokenize_url(url))
  def test_ablate_url_tokens(self):
    url = "http://www.gmail.com/mail/u/1/#inbox"
    token_idxs_to_ablate = [0]
    expected_url = "://www.gmail.com/mail/u/1/#inbox"
    self.assertEqual(expected_url,
                     cf_utils.ablate_url_tokens(url, token_idxs_to_ablate))
    token_idxs_to_ablate = [0, 3, 4]
    expected_url = "://www.gmail.//u/1/#inbox"
    self.assertEqual(expected_url,
                     cf_utils.ablate_url_tokens(url, token_idxs_to_ablate))
    token_idxs_to_ablate = [4, 3, 0]
    expected_url = "://www.gmail.//u/1/#inbox"
    self.assertEqual(expected_url,
                     cf_utils.ablate_url_tokens(url, token_idxs_to_ablate))
    token_idxs_to_ablate = [0, 1, 2, 3, 4, 5, 6, 7]
    expected_url = "://..////#"
    self.assertEqual(expected_url,
                     cf_utils.ablate_url_tokens(url, token_idxs_to_ablate))
    token_idxs_to_ablate = []
    self.assertEqual(url,
                     cf_utils.ablate_url_tokens(url, token_idxs_to_ablate))
if __name__ == "__main__":
  absltest.main()

================
File: lit_nlp/components/cf_utils.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Utility functions for generating counterfactuals."""
import re
from typing import Any, Optional, cast
from lit_nlp.api import types
import numpy as np
def update_prediction(
    example: dict[str, Any],
    example_output: types.JsonDict,
    output_spec: types.JsonDict,
    pred_key: str,
):
  """Updates prediction score and label (if classification model) in the provided example."""
  prediction = example_output[pred_key]
  example[pred_key] = prediction
  pred_spec = output_spec[pred_key]
  if isinstance(pred_spec, types.MulticlassPreds):
    # Update label
    # TODO(lit-dev): provide a general system for handling labels on
    # generated examples.
    pred_spec = cast(types.MulticlassPreds, pred_spec)
    label_key = pred_spec.parent
    label_names = pred_spec.vocab
    pred_class = np.argmax(prediction)
    example_label = label_names[pred_class]
    if label_key is not None:
      example[label_key] = example_label
def is_prediction_flip(
    cf_output: types.JsonDict,
    orig_output: types.JsonDict,
    output_spec: types.JsonDict,
    pred_key: str,
    regression_thresh: Optional[float] = None,
) -> bool:
  """Check if cf_output and  orig_output specify different prediciton classes."""
  if isinstance(output_spec[pred_key], types.RegressionScore):
    # regression model. We use the provided threshold to binarize the output.
    cf_pred_class = (cf_output[pred_key] <= regression_thresh)
    orig_pred_class = (orig_output[pred_key] <= regression_thresh)
  else:
    cf_pred_class = np.argmax(cf_output[pred_key])
    orig_pred_class = np.argmax(orig_output[pred_key])
  return cf_pred_class != orig_pred_class
def prediction_difference(
    cf_output: types.JsonDict,
    orig_output: types.JsonDict,
    output_spec: types.JsonDict,
    pred_key: str,
) -> float:
  """Returns the difference in prediction between cf_output and orig_output."""
  if isinstance(output_spec[pred_key], types.RegressionScore):
    # regression model. We use the provided threshold to binarize the output.
    cf_pred = cf_output[pred_key]
    orig_pred = orig_output[pred_key]
  else:
    orig_pred_class = np.argmax(orig_output[pred_key])
    cf_pred = cf_output[pred_key][orig_pred_class]
    orig_pred = orig_output[pred_key][orig_pred_class]
  return cf_pred - orig_pred
def _tokenize_url(url: str) -> list[tuple[str, int, int]]:
  """Tokenizes a URL and returns list of triples specifying the token string and its start and end position."""
  if not url:
    return []
  separator_regex = "[^a-zA-Z0-9]"
  separator_matches = list(re.finditer(separator_regex, url))
  if not separator_matches:
    # If no separator is found, use the entire string.
    return [(url, 0, len(url))]
  tokens = []  # a list of tuples of token, start index, end index.
  start_idx = 0  # start index for next token
  for i in range(len(separator_matches)):
    sep = separator_matches[i]
    sep_start_idx = sep.start()
    if sep_start_idx > start_idx:
      tokens.append((url[start_idx:sep_start_idx], start_idx, sep_start_idx))
    start_idx = sep.end()
  if start_idx < len(url):
    tokens.append((url[start_idx:], start_idx, len(url)))
  return tokens
def tokenize_url(url: str) -> list[str]:
  """Tokenizes the provided URL and returns a list of token strings."""
  url_tokens = _tokenize_url(url)
  return [t for t, _, _ in url_tokens]
def ablate_url_tokens(url: str, token_idxs_to_ablate: tuple[int, ...]) -> str:
  """Ablates the tokens at the provided indices and returns the resulting URL."""
  url_tokens = _tokenize_url(url)
  start = 0
  modified_url_pieces = []
  token_idxs_to_ablate = sorted(token_idxs_to_ablate)
  for token_idx in token_idxs_to_ablate:
    assert token_idx < len(url_tokens), (
        "token_idxs_to_ablate must all fall in the range 0 to number of tokens"
        " returned by tokenize_url")
    _, token_start, token_end = url_tokens[token_idx]
    modified_url_pieces.append(url[start:token_start])
    start = token_end
  modified_url_pieces.append(url[start:])
  return "".join(modified_url_pieces)

================
File: lit_nlp/components/citrus/__init__.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

================
File: lit_nlp/components/citrus/helpers.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Helper classes and functions for explaining text classifiers."""
from collections.abc import Sequence
import math
from typing import Any, Optional
import attr
import numpy as np
TOP_K_AVG_RATIO = 0.1
# TODO(xnlp-dev): b/156918552 Maybe merge PosthocExplanation and TextRationale.
# TODO(xnlp-dev): b/156912351 Describe xNLP components in a design doc.
@attr.s(auto_attribs=True)
class PosthocExplanation:
  """Represents a post-hoc explanation with feature importance scores.
  Attributes:
    features: the names of the features to attribute to;
      typically these are tokens.
    feature_importance: Feature importance scores for each input feature. These
      are the coefficients of the linear model that was fitted to mimic the
      behavior of a (black-box) prediction function.
    intercept: The intercept of the fitted linear model. This is the independent
      term that is added to make a prediction.
    model: The fitted linear model. An explanation only contains this if it was
      explicitly requested from the explanation method.
    score: The R^2 score of the fitted linear model on the perturbations and
      their labels. This reflects how well the linear model was able to fit to
      the perturbation set.
    prediction: The prediction of the linear model on the full input sentence,
      i.e., an all-true boolean mask.
  """
  features: Sequence[str]
  feature_importance: np.ndarray
  intercept: Optional[float] = None
  model: Optional[Any] = None
  score: Optional[float] = None
  prediction: Optional[float] = None
class TextRationale:
  """A text with a rationale explanation."""
  def __init__(self,
               text: str,
               token_weights: list[float],
               top_k_ratio: float = TOP_K_AVG_RATIO):
    """Initializes with a text and a list of token weights.
    Args:
      text: A full-text input to a classifier with tokens separated with ' '.
      token_weights: A list of token weights (in the token position order).
      top_k_ratio: Rationale size in tokens is defined proportional to the input
        length (in tokens). The percentages are given for the ERASER datasets
        and are 10% on average.
    """
    self.text = text
    self.tokens = str.split(text)
    assert len(self.tokens) == len(token_weights), 'Token count does not match.'
    self.token_weights = token_weights
    # Round to the closest equal or larger integer.
    top_k_value = math.ceil(len(self.tokens) * top_k_ratio)
    self.top_k_ids = np.argsort(token_weights)[-top_k_value:]
    self.top_k_ids = list(reversed(self.top_k_ids))
    self.top_k_ids_set = set(self.top_k_ids)
  def get_rationale_text(self, mask_token: Optional[str] = None) -> str:
    """Returns the text covering only the rationale.
    Args:
      mask_token: Token to use for all the tokens not in the rationale.
    Returns: A string representing the source text with everything but rationale
      masked.
    """
    result = []
    for i, token in enumerate(self.tokens):
      if i in self.top_k_ids_set:
        result.append(token)
      elif mask_token:
        result.append(mask_token)
    return ' '.join(result)
  def get_text_wo_rationale(self, mask_token: Optional[str] = None) -> str:
    """Returns the text without the rationale.
    Args:
      mask_token: Token to use for all the tokens in the rationale.
    Returns: A string representing the source text with the rationale masked.
    """
    result = []
    for i, token in enumerate(self.tokens):
      if i not in self.top_k_ids_set:
        result.append(token)
      elif mask_token:
        result.append(mask_token)
    return ' '.join(result)

================
File: lit_nlp/components/citrus/lime_test.py
================
import collections
import functools
from absl.testing import absltest
from absl.testing import parameterized
from lime import lime_text as original_lime
from lit_nlp.components.citrus import lime
from lit_nlp.components.citrus import utils
import numpy as np
from scipy import special
from scipy import stats
class LimeTest(parameterized.TestCase):
  def test_sample_masks_returns_correct_shape_and_type(self):
    """Tests if LIME mask samples have the right shape and type."""
    num_samples = 2
    num_features = 3
    masks = lime.sample_masks(num_samples, num_features, seed=0)
    self.assertEqual(np.dtype('bool'), masks.dtype)
    self.assertEqual((num_samples, num_features), masks.shape)
  def test_sample_masks_contains_extreme_samples(self):
    """Tests if the masks contain extreme samples (1 or all features)."""
    num_samples = 1000
    num_features = 10
    masks = lime.sample_masks(num_samples, num_features, seed=0)
    num_disabled = (~masks).sum(axis=-1)
    self.assertEqual(1, min(num_disabled))
    self.assertEqual(num_features, max(num_disabled))
  def test_sample_masks_returns_uniformly_distributed_masks(self):
    """Tests if the masked positions are uniformly distributed."""
    num_samples = 10000
    num_features = 100
    masks = lime.sample_masks(num_samples, num_features, seed=0)
    # The mean should be ~0.5, but this is also true when normally distributed.
    np.testing.assert_almost_equal(masks.mean(), 0.5, decimal=2)
    # We should see each possible masked count approx. the same number of times.
    # We check this by looking at the entropy which should be around 1.0.
    counter = collections.Counter(masks.sum(axis=-1))
    entropy = stats.entropy(list(counter.values()), base=num_features)
    np.testing.assert_almost_equal(entropy, 1.0, decimal=2)
  def test_get_perturbations_returns_correctly_masked_string(self):
    """Tests obtaining perturbations from tokens and a mask."""
    sentence = 'It is a great movie but also somewhat bad .'
    tokens = sentence.split()
    # We create a mock mask with False for tokens with an 'a', True otherwise.
    masks = np.array([[False if 'a' in token else True for token in tokens]])
    perturbations = list(lime.get_perturbations(tokens, masks, mask_token='_'))
    expected = 'It is _ _ movie but _ _ _ .'
    self.assertEqual(expected, perturbations[0])
  @parameterized.named_parameters(
      {
          'testcase_name': 'is_one_for_zero_distance',
          'distance': 0.,
          'kernel_width': 10,
          'expected': 1.,
      }, {
          'testcase_name': 'is_zero_for_exp_kernel_width_distance',
          'distance': np.exp(10),
          'kernel_width': 10,
          'expected': 0.,
      })
  def test_exponential_kernel(self, distance, kernel_width, expected):
    """Tests a few known exponential kernel results."""
    result = lime.exponential_kernel(distance, kernel_width)
    np.testing.assert_almost_equal(expected, result)
  @parameterized.named_parameters(
      {
          'testcase_name': 'correctly_identifies_important_tokens_for_1d_input',
          'sentence': 'It is a great movie but also somewhat bad .',
          'num_samples': 1000,
          'positive_token': 'great',
          'negative_token': 'bad',
          'num_classes': 1,
          'class_to_explain': None,
      }, {
          'testcase_name': 'correctly_identifies_important_tokens_for_2d_input',
          'sentence': 'It is a great movie but also somewhat bad .',
          'num_samples': 1000,
          'positive_token': 'great',
          'negative_token': 'bad',
          'num_classes': 2,
          'class_to_explain': 1,
      }, {
          'testcase_name': 'correctly_identifies_important_tokens_for_3d_input',
          'sentence': 'It is a great movie but also somewhat bad .',
          'num_samples': 1000,
          'positive_token': 'great',
          'negative_token': 'bad',
          'num_classes': 3,
          'class_to_explain': 2,
      })
  def test_explain(self, sentence, num_samples, positive_token, negative_token,
                   num_classes, class_to_explain):
    """Tests explaining text classifiers with various output dimensions."""
    def _predict_fn(sentences):
      """Mock prediction function."""
      rs = np.random.RandomState(seed=0)
      predictions = []
      for sentence in sentences:
        probs = rs.uniform(0., 1., num_classes)
        # To check if LIME finds the right positive/negative correlations.
        if negative_token in sentence:
          probs[class_to_explain] = probs[class_to_explain] - 1.
        if positive_token in sentence:
          probs[class_to_explain] = probs[class_to_explain] + 1.
        predictions.append(probs)
      predictions = np.stack(predictions, axis=0)
      if num_classes == 1:
        return np.squeeze(special.expit(predictions), -1)
      else:
        return special.softmax(predictions, axis=-1)
    explanation = lime.explain(
        sentence,
        _predict_fn,
        class_to_explain=class_to_explain,
        num_samples=num_samples,
        tokenizer=str.split)
    self.assertLen(explanation.feature_importance, len(sentence.split()))
    # The positive word should have the highest attribution score.
    positive_token_idx = sentence.split().index(positive_token)
    self.assertEqual(positive_token_idx,
                     np.argmax(explanation.feature_importance))
    # The negative word should have the lowest attribution score.
    negative_token_idx = sentence.split().index(negative_token)
    self.assertEqual(negative_token_idx,
                     np.argmin(explanation.feature_importance))
  @parameterized.named_parameters({
      'testcase_name': 'correctly_identifies_important_tokens_for_regression',
      'sentence': 'It is a great movie but also somewhat bad .',
      'num_samples': 1000,
      'positive_token': 'great',
      'negative_token': 'bad',
  })
  def test_explain_regression(self, sentence, num_samples, positive_token,
                              negative_token):
    """Tests explaining text classifiers with various output dimensions."""
    def _predict_fn(sentences):
      """Mock prediction function."""
      rs = np.random.RandomState(seed=0)
      predictions = []
      for sentence in sentences:
        output = rs.uniform(-2., 2.)
        # To check if LIME finds the right positive/negative correlations.
        if negative_token in sentence:
          output -= rs.uniform(0., 2.)
        if positive_token in sentence:
          output += rs.uniform(0., 2.)
        predictions.append(output)
      predictions = np.stack(predictions, axis=0)
      return predictions
    explanation = lime.explain(
        sentence, _predict_fn, num_samples=num_samples, tokenizer=str.split)
    self.assertLen(explanation.feature_importance, len(sentence.split()))
    # The positive word should have the highest attribution score.
    positive_token_idx = sentence.split().index(positive_token)
    self.assertEqual(positive_token_idx,
                     np.argmax(explanation.feature_importance))
    # The negative word should have the lowest attribution score.
    negative_token_idx = sentence.split().index(negative_token)
    self.assertEqual(negative_token_idx,
                     np.argmin(explanation.feature_importance))
  def test_explain_returns_explanation_with_intercept(self):
    """Tests if the explanation contains an intercept value."""
    def _predict_fn(sentences):
      return np.random.uniform(0., 1., [len(list(sentences)), 2])
    explanation = lime.explain('Test sentence', _predict_fn, 1, num_samples=5)
    self.assertNotEqual(explanation.intercept, 0.)
  def test_explain_returns_explanation_with_model(self):
    """Tests if the explanation contains the model."""
    def _predict_fn(sentences):
      return np.random.uniform(0., 1., [len(list(sentences)), 2])
    explanation = lime.explain(
        'Test sentence',
        _predict_fn,
        class_to_explain=1,
        num_samples=5,
        return_model=True)
    self.assertIsNotNone(explanation.model)
  def test_explain_returns_explanation_with_score(self):
    """Tests if the explanation contains a linear model score."""
    def _predict_fn(sentences):
      return np.random.uniform(0., 1., [len(list(sentences)), 2])
    explanation = lime.explain(
        'Test sentence',
        _predict_fn,
        class_to_explain=1,
        num_samples=5,
        return_score=True)
    self.assertIsNotNone(explanation.score)
  def test_explain_returns_explanation_with_prediction(self):
    """Tests if the explanation contains a prediction."""
    def _predict_fn(sentences):
      return np.random.uniform(0., 1., [len(list(sentences)), 2])
    explanation = lime.explain(
        'Test sentence',
        _predict_fn,
        class_to_explain=1,
        num_samples=5,
        return_prediction=True)
    self.assertIsNotNone(explanation.prediction)
  @parameterized.named_parameters(
      {
          'testcase_name': 'for_2d_input',
          'sentence': ' '.join(list('abcdefghijklmnopqrstuvwxyz')),
          'num_samples': 5000,
          'num_classes': 2,
          'class_to_explain': 1,
      }, {
          'testcase_name': 'for_3d_input',
          'sentence': ' '.join(list('abcdefghijklmnopqrstuvwxyz')),
          'num_samples': 5000,
          'num_classes': 3,
          'class_to_explain': 2,
      })
  def test_explain_matches_original_lime(self, sentence, num_samples,
                                         num_classes, class_to_explain):
    """Tests if Citrus LIME matches the original implementation."""
    list('abcdefghijklmnopqrstuvwxyz')
    # Assign some weight to each token a-z.
    # Each token contributes positively/negatively to the prediction.
    rs = np.random.RandomState(seed=0)
    token_weights = {token: rs.normal() for token in sentence.split()}
    token_weights[lime.DEFAULT_MASK_TOKEN] = 0.
    def _predict_fn(sentences):
      """Mock prediction function."""
      rs = np.random.RandomState(seed=0)
      predictions = []
      for sentence in sentences:
        probs = rs.normal(0., 0.1, size=num_classes)
        # To check if LIME finds the right positive/negative correlations.
        for token in sentence.split():
          probs[class_to_explain] += token_weights[token]
        predictions.append(probs)
      return np.stack(predictions, axis=0)
    # Explain the prediction using Citrus LIME.
    explanation = lime.explain(
        sentence,
        _predict_fn,
        class_to_explain=class_to_explain,
        num_samples=num_samples,
        tokenizer=str.split,
        mask_token=lime.DEFAULT_MASK_TOKEN,
        kernel=functools.partial(
            lime.exponential_kernel, kernel_width=lime.DEFAULT_KERNEL_WIDTH))
    scores = explanation.feature_importance  # <float32>[seq_len]
    scores = utils.normalize_scores(scores, make_positive=False)
    # Explain the prediction using original LIME.
    original_lime_explainer = original_lime.LimeTextExplainer(
        class_names=map(str, np.arange(num_classes)),
        mask_string=lime.DEFAULT_MASK_TOKEN,
        kernel_width=lime.DEFAULT_KERNEL_WIDTH,
        split_expression=str.split,
        bow=False)
    num_features = len(sentence.split())
    original_explanation = original_lime_explainer.explain_instance(
        sentence,
        _predict_fn,
        labels=(class_to_explain,),
        num_features=num_features,
        num_samples=num_samples)
    # original_explanation.local_exp is a dict that has a key class_to_explain,
    # which gives a sequence of (index, score) pairs.
    # We convert it to an array <float32>[seq_len] with a score per position.
    original_scores = np.zeros(num_features)
    for index, score in original_explanation.local_exp[class_to_explain]:
      original_scores[index] = score
    original_scores = utils.normalize_scores(
        original_scores, make_positive=False)
    # Test that Citrus LIME and original LIME match.
    np.testing.assert_allclose(scores, original_scores, atol=0.01)
if __name__ == '__main__':
  absltest.main()

================
File: lit_nlp/components/citrus/lime.py
================
"""Local Interpretable Model-agnostic Explanations (LIME).
LIME was proposed in the following paper:
> "Why Should I Trust You?": Explaining the Predictions of Any Classifier
> Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin
> https://arxiv.org/abs/1602.04938
LIME explains classifiers by returning a feature attribution score
for each input feature. It works as follows:
1) Sample perturbation masks. First the number of masked features is sampled
   (uniform, at least 1), and then that number of features are randomly chosen
   to be masked out (without replacement).
2) Get predictions from the model for those perturbations. Use these as labels.
3) Fit a linear model to associate the input positions indicated by the binary
   mask with the resulting predicted label.
The resulting feature importance scores are the linear model coefficients for
the requested output class or (in case of regression) the output score.
This is a reimplementation of the original https://github.com/marcotcr/lime
and is tested for compatibility. This version supports applying LIME to text
input, also in case of regression and binary-classification where the
prediction function only outputs a scalar for each input sentence.
"""
from collections.abc import Callable, Iterable, Sequence
import functools
from typing import Any, Optional
from lit_nlp.components.citrus import helpers
from lit_nlp.components.citrus import utils
import numpy as np
from sklearn import linear_model
from sklearn import metrics
DEFAULT_MASK_TOKEN = '<unk>'
DEFAULT_NUM_SAMPLES = 3000
DEFAULT_SOLVER = 'cholesky'
DEFAULT_KERNEL_WIDTH = utils.DEFAULT_KERNEL_WIDTH
exponential_kernel = utils.exponential_kernel
def sample_masks(num_samples: int,
                 num_features: int,
                 seed: Optional[int] = None):
  """Samples LIME masks with at least 1 position disabled per sampled mask.
  The number of disabled features is sampled from a uniform distribution.
  Args:
    num_samples: The number of samples.
    num_features: The number of features to sample a mask for. Typically this is
      the number of tokens in the sentence.
    seed: Set this to an integer to make the sampling deterministic.
  Returns:
    Masks <bool>[num_samples, num_features] indicating which features are
    enabled (True) and which ones are disabled (False).
  """
  rng = np.random.RandomState(seed)
  positions = np.tile(np.arange(num_features), (num_samples, 1))
  permutation_fn = np.vectorize(rng.permutation, signature='(n)->(n)')
  permutations = permutation_fn(positions)  # A shuffled range of positions.
  num_disabled_features = rng.randint(1, num_features + 1, (num_samples, 1))
  # For num_disabled_features[i] == 2, this will set indices 0 and 1 to False.
  return permutations >= num_disabled_features
def get_perturbations(tokens: Sequence[str],
                      masks: np.ndarray,
                      mask_token: str = '<unk>') -> Iterable[str]:
  """Returns strings with the masked tokens replaced with `mask_token`."""
  for mask in masks:
    parts = [t if mask[i] else mask_token for i, t in enumerate(tokens)]
    yield ' '.join(parts)
def explain(
    sentence: str,
    predict_fn: Callable[[Iterable[str]], np.ndarray],
    class_to_explain: Optional[int] = None,
    num_samples: int = DEFAULT_NUM_SAMPLES,
    tokenizer: Any = str.split,
    mask_token: str = DEFAULT_MASK_TOKEN,
    alpha: float = 1.0,
    solver: str = DEFAULT_SOLVER,
    kernel: Callable[..., np.ndarray] = utils.exponential_kernel,
    distance_fn: Callable[..., np.ndarray] = functools.partial(
        metrics.pairwise.pairwise_distances, metric='cosine'),
    distance_scale: float = 100.,
    return_model: bool = False,
    return_score: bool = False,
    return_prediction: bool = False,
    seed: Optional[int] = None,
) -> helpers.PosthocExplanation:
  """Returns the LIME explanation for a given sentence.
  By default, this function returns an explanation object containing feature
  importance scores and the intercept. Optionally, more information can be
  returned, such as the linear model, the score of the model on the perturbation
  set, and the prediction that the linear model makes on the original sentence.
  Args:
    sentence: An input to be explained.
    predict_fn: A prediction function that returns an array of outputs given a
      list of inputs. The output shape is [len(inputs)] for regression and
      binary classification (with scalar output), and [len(inputs), num_classes]
      for multi-class classification.
    class_to_explain: The class ID to explain in case of multi-class
      classification, where `predict_fn` returns outputs with multiple
      dimensions for each input. For example, use 2 to explain the third class
      in 3-class classification. For regression and binary classification, where
      `predict_fn` returns a scalar for each input, this does not need to be
      set.
    num_samples: The number of n-grams to sample.
    tokenizer: A function that splits the input sentence into tokens.
    mask_token: The token that is used for masking tokens, e.g., '<unk>'.
    alpha: Regularization strength of the linear approximation model. See
      `sklearn.linear_model.Ridge` for details.
    solver: Solver to use in the linear approximation model. See
      `sklearn.linear_model.Ridge` for details.
    kernel: A kernel function to be used on the distance function. By default,
      use the exponential kernel with kernel width utils.DEFAULT_KERNEL_WIDTH.
    distance_fn: A distance function to use in range [0, 1]. Default: cosine.
    distance_scale: A scalar factor multiplied with the distances before the
      kernel is applied.
    return_model: Returns the fitted linear model.
    return_score: Returns the score of the linear model on the perturbations.
      This is the R^2 of the linear model predictions w.r.t. their targets.
    return_prediction: Returns the prediction of the linear model on the full
      original sentence.
    seed: Optional random seed to make the explanation deterministic.
  Returns:
    The explanation for the requested class.
  """
  # TODO(bastings): Provide sentence already tokenized to reduce split/join ops.
  tokens = tokenizer(sentence)
  if not tokens:
    return helpers.PosthocExplanation(
        features=[], feature_importance=np.array([], dtype=np.float32))
  masks = sample_masks(num_samples + 1, len(tokens), seed=seed)
  assert masks.shape[0] == num_samples + 1, 'Expected num_samples + 1 masks.'
  all_true_mask = np.ones_like(masks[0], dtype=bool)
  masks[0] = all_true_mask  # First mask is the full sentence.
  perturbations = list(get_perturbations(tokens, masks, mask_token))
  outputs = predict_fn(perturbations)
  if len(outputs.shape) > 1:
    assert class_to_explain is not None, \
        'class_to_explain needs to be set when `predict_fn` returns a 2D tensor'
    outputs = outputs[:, class_to_explain]  # We are only interested in 1 class.
  distances = distance_fn(all_true_mask.reshape(1, -1), masks).flatten()
  distances = distance_scale * distances
  distances = kernel(distances)
  # Fit a linear model for the requested output class.
  model = linear_model.Ridge(
      alpha=alpha, solver=solver, random_state=seed).fit(
          masks, outputs, sample_weight=distances)
  explanation = helpers.PosthocExplanation(
      features=tokens,
      feature_importance=model.coef_,
      intercept=model.intercept_)
  if return_model:
    explanation.model = model
  if return_score:
    explanation.score = model.score(masks, outputs)
  if return_prediction:
    explanation.prediction = model.predict(all_true_mask.reshape(1, -1))
  return explanation

================
File: lit_nlp/components/citrus/utils_test.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Tests for language.google.xnlp.citrus.utils."""
from absl.testing import absltest
from absl.testing import parameterized
from lit_nlp.components.citrus import utils
import numpy as np
import tensorflow.compat.v2 as tf
class UtilsTest(parameterized.TestCase, tf.test.TestCase):
  def setUp(self):
    """Resets random seed for each test."""
    super().setUp()
    np.random.seed(0)
  @parameterized.named_parameters(
      {
          'testcase_name': ('Test normalization of random scores, retaining '
                            'the original sign of each value.'),
          'scores': np.random.normal(size=100),
          'make_positive': False,
      }, {
          'testcase_name': 'Test normalization of random scores.',
          'scores': np.random.normal(size=100),
          'make_positive': True,
      }, {
          'testcase_name': 'Test normalization of scores that sum to 0.',
          'scores': np.array([-2.0, 2.0]),
          'make_positive': True,
      }, {
          'testcase_name': 'Test normalization of scores are all 0.',
          'scores': np.array([0.0, 0.0]),
          'make_positive': True,
      }, {
          'testcase_name': 'Test normalization of 1-dim scores.',
          'scores': np.array([-0.5]),
          'make_positive': False,
      })
  def test_normalize_scores(self, scores, make_positive):
    """Check if the scores sum to 1 after taking their absolute values."""
    original_min = np.min(scores)
    scores = utils.normalize_scores(scores, make_positive=make_positive)
    self.assertAllClose(1.0, np.abs(scores).sum(-1))
    if not make_positive:  # Keep the sign of originally negative values.
      self.assertLessEqual(np.min(scores), np.max([0.0, original_min]))
if __name__ == '__main__':
  absltest.main()

================
File: lit_nlp/components/citrus/utils.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Utility functions for explaining text classifiers."""
import numpy as np
DEFAULT_KERNEL_WIDTH = 25
def normalize_scores(scores: np.ndarray,
                     make_positive: bool = False) -> np.ndarray:
  """Makes the absolute values sum to 1, optionally making them all positive."""
  if len(scores) < 1:
    return scores
  scores = scores + np.finfo(np.float32).eps
  if make_positive:
    scores = np.abs(scores)
  return scores / np.abs(scores).sum(-1)
def exponential_kernel(
    distance: float, kernel_width: float = DEFAULT_KERNEL_WIDTH) -> np.ndarray:
  """The exponential kernel."""
  return np.sqrt(np.exp(-(distance**2) / kernel_width**2))

================
File: lit_nlp/components/classification_results_test.py
================
# Copyright 2022 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Tests for lit_nlp.components.classification_results."""
from absl.testing import absltest
from absl.testing import parameterized
from lit_nlp.api import dataset as lit_dataset
from lit_nlp.api import dtypes
from lit_nlp.api import model as lit_model
from lit_nlp.components import classification_results
from lit_nlp.lib import testing_utils
import numpy as np
class ClassificationResultsTest(parameterized.TestCase):
  def setUp(self):
    super(ClassificationResultsTest, self).setUp()
    self.interpreter = classification_results.ClassificationInterpreter()
  @parameterized.named_parameters(
      ('classification', testing_utils.ClassificationModelForTesting(), True),
      ('regression', testing_utils.RegressionModelForTesting({}), False),
  )
  def test_is_compatible(self, model: lit_model.Model, epxected: bool):
    compat = self.interpreter.is_compatible(
        model, lit_dataset.NoneDataset({'test': model}))
    self.assertEqual(compat, epxected)
  def test_no_label(self):
    dataset = lit_dataset.Dataset(None, None)
    inputs = [
        {}, {}, {}
    ]
    results = self.interpreter.run(
        inputs, testing_utils.ClassificationModelForTesting(), dataset
    )
    expected_results = [
        {'probas': dtypes.ClassificationResult([0.2, 0.8], '1', None)},
        {'probas': dtypes.ClassificationResult([0.2, 0.8], '1', None)},
        {'probas': dtypes.ClassificationResult([0.2, 0.8], '1', None)},
    ]
    self.assertListEqual(['probas'], list(results[0].keys()))
    for result, expected in zip(results, expected_results):
      np.testing.assert_array_equal(
          expected['probas'].scores, result['probas'].scores
      )
      self.assertEqual(
          expected['probas'].predicted_class, result['probas'].predicted_class
      )
      self.assertIsNone(result['probas'].correct)
  def test_no_margins(self):
    dataset = lit_dataset.Dataset(None, None)
    inputs = [
        {'label': '0'}, {'label': '1'}, {'label': '0'}
    ]
    results = self.interpreter.run(
        inputs, testing_utils.ClassificationModelForTesting(), dataset
    )
    expected_results = [
        {'probas': dtypes.ClassificationResult([0.2, 0.8], '1', False)},
        {'probas': dtypes.ClassificationResult([0.2, 0.8], '1', True)},
        {'probas': dtypes.ClassificationResult([0.2, 0.8], '1', False)},
    ]
    self.assertListEqual(['probas'], list(results[0].keys()))
    for result, expected in zip(results, expected_results):
      np.testing.assert_array_equal(
          expected['probas'].scores, result['probas'].scores
      )
      self.assertEqual(
          expected['probas'].predicted_class,
          result['probas'].predicted_class,
      )
      self.assertEqual(expected['probas'].correct, result['probas'].correct)
  def test_single_margin(self):
    config = {'probas': {'': {'margin': 4, 'facetData': {'facets': {}}}}}
    dataset = lit_dataset.Dataset(None, None)
    inputs = [{'label': '0'}, {'label': '1'}, {'label': '0'}]
    results = self.interpreter.run(
        inputs,
        testing_utils.ClassificationModelForTesting(),
        dataset,
        None,
        config,
    )
    expected_results = [
        {'probas': dtypes.ClassificationResult([0.2, 0.8], '0', True)},
        {'probas': dtypes.ClassificationResult([0.2, 0.8], '0', False)},
        {'probas': dtypes.ClassificationResult([0.2, 0.8], '0', True)},
    ]
    self.assertListEqual(['probas'], list(results[0].keys()))
    for result, expected in zip(results, expected_results):
      np.testing.assert_array_equal(
          expected['probas'].scores, result['probas'].scores
      )
      self.assertEqual(
          expected['probas'].predicted_class,
          result['probas'].predicted_class,
      )
      self.assertEqual(expected['probas'].correct, result['probas'].correct)
  def test_faceted_margins_text(self):
    config = {
        'probas': {
            'hi': {'margin': 4, 'facetData': {'facets': {'s': {'val': 'hi'}}}},
            'bye': {
                'margin': -4,
                'facetData': {'facets': {'s': {'val': 'bye'}}},
            },
        }
    }
    dataset = lit_dataset.Dataset(None, None)
    inputs = [
        {'label': '0', 's': 'hi'},
        {'label': '1', 's': 'hi'},
        {'label': '0', 's': 'bye'},
    ]
    results = self.interpreter.run(
        inputs,
        testing_utils.ClassificationModelForTesting(),
        dataset,
        None,
        config,
    )
    expected_results = [
        {'probas': dtypes.ClassificationResult([0.2, 0.8], '0', True)},
        {'probas': dtypes.ClassificationResult([0.2, 0.8], '0', False)},
        {'probas': dtypes.ClassificationResult([0.2, 0.8], '1', False)},
    ]
    self.assertListEqual(['probas'], list(results[0].keys()))
    for result, expected in zip(results, expected_results):
      np.testing.assert_array_equal(
          expected['probas'].scores, result['probas'].scores
      )
      self.assertEqual(
          expected['probas'].predicted_class,
          result['probas'].predicted_class,
      )
      self.assertEqual(expected['probas'].correct, result['probas'].correct)
  def test_faceted_margins_num(self):
    config = {
        'probas': {
            'high': {
                'margin': 4,
                'facetData': {'facets': {'n': {'val': [2, 3]}}},
            },
            'low': {
                'margin': -4,
                'facetData': {'facets': {'n': {'val': [0, 2]}}},
            },
        }
    }
    dataset = lit_dataset.Dataset(None, None)
    inputs = [
        {'label': '0', 'n': 2.5},
        {'label': '1', 'n': 2.1},
        {'label': '0', 'n': 1.5},
    ]
    results = self.interpreter.run(
        inputs,
        testing_utils.ClassificationModelForTesting(),
        dataset,
        None,
        config,
    )
    expected_results = [
        {'probas': dtypes.ClassificationResult([0.2, 0.8], '0', True)},
        {'probas': dtypes.ClassificationResult([0.2, 0.8], '0', False)},
        {'probas': dtypes.ClassificationResult([0.2, 0.8], '1', False)},
    ]
    self.assertListEqual(['probas'], list(results[0].keys()))
    for result, expected in zip(results, expected_results):
      np.testing.assert_array_equal(
          expected['probas'].scores, result['probas'].scores
      )
      self.assertEqual(
          expected['probas'].predicted_class,
          result['probas'].predicted_class,
      )
      self.assertEqual(expected['probas'].correct, result['probas'].correct)
if __name__ == '__main__':
  absltest.main()

================
File: lit_nlp/components/classification_results.py
================
# Copyright 2022 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""An interpreter for analyzing classification results."""
from collections.abc import Sequence
import numbers
from typing import cast, Optional
from lit_nlp.api import components as lit_components
from lit_nlp.api import dataset as lit_dataset
from lit_nlp.api import dtypes
from lit_nlp.api import model as lit_model
from lit_nlp.api import types
from lit_nlp.lib import utils as lit_utils
import numpy as np
JsonDict = types.JsonDict
IndexedInput = types.IndexedInput
Spec = types.Spec
def get_margin_for_input(margin_config: Optional[JsonDict] = None,
                         inp: Optional[JsonDict] = None) -> float:
  """Returns the margin value given a margin config and input example."""
  # When no margin config provided, then the margin of 0 indicates that the
  # class with the highest score is the predicted class.
  if not margin_config:
    return 0
  # Check each facet in the margin config to see if the input matches the
  # facet. If so, then use the margin value for that facet from the config.
  for margin_entry in margin_config.values():
    facet_info = (margin_entry['facetData']['facets']
                  if 'facetData' in margin_entry else {})
    match = True
    if inp:
      for feat, facet_info in facet_info.items():
        value = facet_info['val']
        if (isinstance(inp[feat], numbers.Number) and
            not isinstance(inp[feat], bool)):
          # If the facet is a numeric range string, extract the min and max
          # and check the value against that range.
          min_val = value[0]
          max_val = value[1]
          if not (inp[feat] >= min_val and inp[feat] < max_val):
            match = False
            break
        # If the facet is a standard value, check the feature value for
        # equality to it.
        elif inp[feat] != value:
          match = False
          break
    if match:
      return margin_entry['margin']
  return 0
def get_classifications(
    preds: Sequence[np.ndarray], pred_spec: types.MulticlassPreds,
    margin_config: Optional[Sequence[float]] = None) -> Sequence[int]:
  """Get predicted class indices given prediction scores and configs."""
  # If there is a margin set for the prediction, take the log of the prediction
  # scores and add the margin to the null indexes value before taking argmax
  # to find the predicted class.
  if margin_config is not None:
    null_idx = pred_spec.null_idx
    pred_idxs = []
    null_idx_one_hot = np.eye(len(pred_spec.vocab))[null_idx]
    for p, margin in zip(preds, margin_config):
      logit_mask = margin * null_idx_one_hot
      pred_idx = np.argmax(np.log(p) + logit_mask)
      pred_idxs.append(pred_idx)
  else:
    pred_idxs = [np.argmax(p) for p in preds]
  return pred_idxs
class ClassificationInterpreter(lit_components.Interpreter):
  """Calculates and returns classification results, using thresholds."""
  def run(  # pytype: disable=signature-mismatch  # overriding-parameter-type-checks
      self,
      inputs: list[JsonDict],
      model: lit_model.Model,
      dataset: lit_dataset.IndexedDataset,
      model_outputs: Optional[list[JsonDict]] = None,
      config: Optional[JsonDict] = None):
    # Find the prediction field key in the model output to use for calculations.
    output_spec = model.output_spec()
    supported_keys = self._find_supported_pred_keys(output_spec)
    results: list[dict[str, dtypes.ClassificationResult]] = []
    # Run prediction if needed:
    if model_outputs is None:
      model_outputs = list(model.predict(inputs))
    for i, inp in enumerate(inputs):
      input_result: dict[str, dtypes.ClassificationResult] = {}
      for key in supported_keys:
        margin = get_margin_for_input(
            config[key] if (config and key in config) else None, inp)
        field_spec = cast(types.MulticlassPreds, output_spec[key])
        scores = model_outputs[i][key]
        pred_idx = get_classifications(
            [scores], field_spec, [margin])[0]
        pred_class = field_spec.vocab[pred_idx]
        correct = None
        # If there is ground truth information, calculate error and squared
        # error.
        if (field_spec.parent and field_spec.parent in inp):
          correct = pred_class == inp[field_spec.parent]
        result = dtypes.ClassificationResult(scores, pred_class, correct)
        input_result[key] = result
      results.append(input_result)
    return results
  def is_compatible(self, model: lit_model.Model,
                    dataset: lit_dataset.Dataset) -> bool:
    del dataset  # Unused during model classification
    return lit_utils.spec_contains(model.output_spec(), types.MulticlassPreds)
  def _find_supported_pred_keys(self, output_spec: types.Spec) -> list[str]:
    return lit_utils.find_spec_keys(output_spec, types.MulticlassPreds)

================
File: lit_nlp/components/core.py
================
# Copyright 2022 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Helpers for getting default values for LitApp configurations."""
from lit_nlp.api import components as lit_components
from lit_nlp.api import model as lit_model
from lit_nlp.components import ablation_flip
from lit_nlp.components import classification_results
from lit_nlp.components import curves
from lit_nlp.components import gradient_maps
from lit_nlp.components import hotflip
from lit_nlp.components import lime_explainer
from lit_nlp.components import metrics
from lit_nlp.components import model_salience
from lit_nlp.components import nearest_neighbors
from lit_nlp.components import pca
from lit_nlp.components import pdp
from lit_nlp.components import projection
from lit_nlp.components import regression_results
from lit_nlp.components import salience_clustering
from lit_nlp.components import scrambler
from lit_nlp.components import tcav
from lit_nlp.components import thresholder
from lit_nlp.components import word_replacer
# pylint: disable=g-import-not-at-top
# pytype: disable=import-error
try:
  from lit_nlp.components import shap_explainer
  _SHAP_AVAILABLE = True
except (ModuleNotFoundError, ImportError):
  _SHAP_AVAILABLE = False
try:
  from lit_nlp.components import umap
  _UMAP_AVAILABLE = True
except (ModuleNotFoundError, ImportError):
  _UMAP_AVAILABLE = False
# pylint: enable=g-import-not-at-top
# pytype: enable=import-error
def default_generators() -> dict[str, lit_components.Generator]:
  """Returns a dict of the default generators used in a LitApp."""
  return {
      'Ablation Flip': ablation_flip.AblationFlip(),
      'Hotflip': hotflip.HotFlip(),
      'Scrambler': scrambler.Scrambler(),
      'Word Replacer': word_replacer.WordReplacer(),
  }
def required_interpreters() -> dict[str, lit_components.Interpreter]:
  """Returns a dict of required interpreters.
  These are used by multiple core modules, and without them the frontend will
  likely throw errors.
  """
  # Ensure the prediction analysis interpreters are included.
  prediction_analysis_interpreters: dict[str, lit_components.Interpreter] = {
      'classification': classification_results.ClassificationInterpreter(),
      'regression': regression_results.RegressionInterpreter(),
  }
  return prediction_analysis_interpreters
def default_interpreters(
    models: dict[str, lit_model.Model]
) -> dict[str, lit_components.Interpreter]:
  """Returns a dict of the default interpreters used in a LitApp.
  Args:
    models: A dictionary of models that included in the LitApp that may provide
      their own salience information.
  """
  interpreters = required_interpreters()
  # Ensure the embedding-based interpreters are included.
  embedding_interpreters: dict[str, lit_components.Interpreter] = {
      'nearest neighbors': nearest_neighbors.NearestNeighbors(),
      # Embedding projectors expose a standard interface, but get special
      # handling so we can precompute the projections if requested.
      'pca': projection.ProjectionManager(pca.PCAModel),
  }
  if _UMAP_AVAILABLE:
    embedding_interpreters['umap'] = projection.ProjectionManager(
        umap.UmapModel
    )
  gradient_map_interpreters: dict[str, lit_components.Interpreter] = {
      'Grad L2 Norm': gradient_maps.GradientNorm(),
      'Grad  Input': gradient_maps.GradientDotInput(),
      'Integrated Gradients': gradient_maps.IntegratedGradients(),
      'LIME': lime_explainer.LIME(),
  }
  # pyformat: disable
  core_interpreters: dict[str, lit_components.Interpreter] = {
      'Model-provided salience': model_salience.ModelSalience(models),
      'tcav': tcav.TCAV(),
      'curves': curves.CurvesInterpreter(),
      'thresholder': thresholder.Thresholder(),
      'pdp': pdp.PdpInterpreter(),
      'Salience Clustering': salience_clustering.SalienceClustering(
          dict(gradient_map_interpreters)
      ),
  }
  # pyformat: enable
  if _SHAP_AVAILABLE:
    core_interpreters['Tabular SHAP'] = shap_explainer.TabularShapExplainer()
  interpreters.update(
      **core_interpreters, **gradient_map_interpreters, **embedding_interpreters
  )
  return interpreters
# TODO(b/254833485): Update typing to be a dict[str, lit_components.Metrics]
# once the Wrapper classes in metrics.py inherit from lit_components.Metrics.
def default_metrics() -> dict[str, lit_components.Interpreter]:
  return {
      'regression': metrics.RegressionMetrics(),
      'multiclass': metrics.MulticlassMetrics(),
      'multilabel': metrics.MultilabelMetrics(),
      'paired': metrics.MulticlassPairedMetrics(),
      'bleu': metrics.CorpusBLEU(),
      'rouge': metrics.RougeL(),
      'exactmatch': metrics.ExactMatchMetrics(),
  }

================
File: lit_nlp/components/curves_test.py
================
# Copyright 2022 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Tests for lit_nlp.components.curves."""
from typing import NamedTuple
from absl.testing import absltest
from absl.testing import parameterized
from lit_nlp.api import dataset as lit_dataset
from lit_nlp.api import model as lit_model
from lit_nlp.api import types as lit_types
from lit_nlp.components import curves
from lit_nlp.lib import caching
# Labels used in the test dataset.
COLORS = ['red', 'green', 'blue']
_Curve = list[tuple[float, float]]
_Model = lit_model.BatchedModel
class _DataEntryForTesting(NamedTuple):
  prediction: tuple[float, float, float]
  label: str
TEST_DATA = {
    0: _DataEntryForTesting((0.7, 0.2, 0.1), 'red'),
    1: _DataEntryForTesting((0.3, 0.5, 0.2), 'red'),
    2: _DataEntryForTesting((0.6, 0.1, 0.3), 'blue'),
}
class _StaticTestModel(_Model):
  """A test model for the interpreter that uses 'TEST_DATA' as model output."""
  def input_spec(self) -> lit_types.Spec:
    return {'x': lit_types.Scalar()}
  def output_spec(self) -> lit_types.Spec:
    return {
        'pred': lit_types.MulticlassPreds(vocab=COLORS, parent='label'),
        'aux_pred': lit_types.MulticlassPreds(vocab=COLORS, parent='label')
    }
  def predict_minibatch(
      self, inputs: list[lit_types.JsonDict], **unused
  ) -> list[lit_types.JsonDict]:
    output = []
    def predict_example(ex: lit_types.JsonDict) -> tuple[float, float, float]:
      x = ex['x']
      return TEST_DATA[x].prediction
    for example in inputs:
      output.append({
          'pred': predict_example(example),
          'aux_pred': [1 / 3, 1 / 3, 1 / 3]
      })
    return output
class _IncompatiblePredictionTestModel(_Model):
  """A model with unsupported output type."""
  def input_spec(self) -> lit_types.Spec:
    return {'x': lit_types.Scalar()}
  def output_spec(self) -> lit_types.Spec:
    return {'pred': lit_types.RegressionScore(parent='label')}
  def predict_minibatch(
      self, inputs: list[lit_types.JsonDict], **unused
  ) -> list[lit_types.JsonDict]:
    return []
class _NoParentTestModel(_Model):
  """A model that doesn't specify the ground truth field in the dataset."""
  def input_spec(self) -> lit_types.Spec:
    return {'x': lit_types.Scalar()}
  def output_spec(self) -> lit_types.Spec:
    return {'pred': lit_types.MulticlassPreds(vocab=COLORS)}
  def predict_minibatch(
      self, inputs: list[lit_types.JsonDict], **unused_kw
  ) -> list[lit_types.JsonDict]:
    return []
class _StaticTestDataset(lit_dataset.Dataset):
  """Dataset for testing the interpreter that uses 'TEST_DATA' as the source."""
  def spec(self) -> lit_types.Spec:
    return {
        'x': lit_types.Scalar(),
        'label': lit_types.Scalar(),
    }
  @property
  def examples(self) -> list[lit_types.JsonDict]:
    return [{'x': x, 'label': entry.label} for x, entry in TEST_DATA.items()]
class CurvesInterpreterTest(parameterized.TestCase):
  """Tests CurvesInterpreter."""
  def setUp(self):
    super().setUp()
    self.dataset = lit_dataset.IndexedDataset(
        base=_StaticTestDataset(), id_fn=caching.input_hash
    )
    self.model = _StaticTestModel()
    self.ci = curves.CurvesInterpreter()
  def test_label_not_in_config(self):
    """The interpreter throws an error if the config doesn't have Label."""
    with self.assertRaises(ValueError):
      self.ci.run(
          inputs=self.dataset.examples,
          model=self.model,
          dataset=self.dataset,
      )
  def test_model_output_is_missing_in_config(self):
    """Tests the case when the name of the model output is absent in config.
    The interpreter throws an error if the name of the output is absent.
    """
    with self.assertRaises(ValueError):
      self.ci.run(
          inputs=self.dataset.examples,
          model=self.model,
          dataset=self.dataset,
          config={'Label': 'red'},
      )
  def test_config_spec(self):
    """Tests that the interpreter config has correct fields of correct type."""
    spec = self.ci.config_spec()
    self.assertIn(curves.TARGET_LABEL_KEY, spec)
    self.assertIsInstance(
        spec[curves.TARGET_LABEL_KEY], lit_types.CategoryLabel
    )
    self.assertIn(curves.TARGET_PREDICTION_KEY, spec)
    self.assertIsInstance(
        spec[curves.TARGET_PREDICTION_KEY], lit_types.SingleFieldMatcher
    )
  def test_meta_spec(self):
    """Tests that the interpreter meta has correct fields of correct type."""
    spec = self.ci.meta_spec()
    self.assertIn(curves.ROC_DATA, spec)
    self.assertIsInstance(spec[curves.ROC_DATA], lit_types.CurveDataPoints)
    self.assertIn(curves.PR_DATA, spec)
    self.assertIsInstance(spec[curves.PR_DATA], lit_types.CurveDataPoints)
  @parameterized.named_parameters(
      ('valid', _StaticTestModel(), True),
      ('no_multiclass_pred', _IncompatiblePredictionTestModel(), False),
      ('no_parent', _NoParentTestModel(), False),
  )
  def test_model_compatibility(self, model: _Model, exp_is_compat: bool):
    """A model is incompatible if prediction is not MulticlassPreds."""
    self.assertEqual(
        self.ci.is_compatible(model, _StaticTestDataset()), exp_is_compat
    )
if __name__ == '__main__':
  absltest.main()

================
File: lit_nlp/components/curves.py
================
# Copyright 2022 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""An interpreters for generating data for ROC and PR curves."""
from collections.abc import Sequence
from typing import cast, Optional
from lit_nlp.api import components as lit_components
from lit_nlp.api import dataset as lit_dataset
from lit_nlp.api import model as lit_model
from lit_nlp.api import types
from lit_nlp.lib import utils as lit_utils
import numpy as np
from sklearn import metrics
JsonDict = types.JsonDict
IndexedInput = types.IndexedInput
Spec = types.Spec
# The config key for specifying model output to use for calculations.
TARGET_PREDICTION_KEY = 'Prediction field'
# The config key for specifying the class label to use for calculations.
TARGET_LABEL_KEY = 'Label'
# They field name in the interpreter output that contains ROC curve data.
ROC_DATA = 'roc_data'
# They field name in the interpreter output that contains PR curve data.
PR_DATA = 'pr_data'
class CurvesInterpreter(lit_components.Interpreter):
  """Returns data for rendering ROC and Precision-Recall curves."""
  def run(self,
          inputs: Sequence[JsonDict],
          model: lit_model.Model,
          dataset: lit_dataset.Dataset,
          model_outputs: Optional[Sequence[JsonDict]] = None,
          config: Optional[JsonDict] = None):
    if not config:
      raise ValueError('Curves required config parameters but received none.')
    if (target_label := config.get(TARGET_LABEL_KEY)) is None:
      raise ValueError(
          f'The config \'{TARGET_LABEL_KEY}\' field should contain the positive'
          f' class label.')
    # Find the prediction field key in the model output to use for calculations.
    output_spec = model.output_spec()
    if TARGET_PREDICTION_KEY in config:
      predictions_key: str = config[TARGET_PREDICTION_KEY]
    elif len(pred_keys := self._find_supported_pred_keys(output_spec)) == 1:
      predictions_key: str = pred_keys[0]
    else:
      raise ValueError(
          'Unable to determine prediction field. Please provide one via the'
          f' "{TARGET_PREDICTION_KEY}" field in the CallConfig or update the'
          ' model spec to output a single MulticlassPreds field.'
      )
    if not inputs:
      return {ROC_DATA: [], PR_DATA: []}
    # Run prediction if needed:
    if model_outputs is None:
      model_outputs = list(model.predict(inputs))
    # Get scores for the target label.
    pred_spec = output_spec.get(predictions_key)
    if not isinstance(pred_spec, types.MulticlassPreds):
      raise TypeError(
          f'Expected {predictions_key} to be a MulticlassPreds field, but got a'
          f' {type(pred_spec).__name__}'
      )
    labels = pred_spec.vocab
    target_index = labels.index(target_label)
    scores = [o[predictions_key][target_index] for o in model_outputs]
    # Get ground truth for the target label.
    parent_key = pred_spec.parent
    ground_truth_list = []
    for ex in inputs:
      ground_truth_label = ex[parent_key]
      ground_truth = 1.0 if ground_truth_label == target_label else 0.0
      ground_truth_list.append(ground_truth)
    # Compute ROC curve data.
    x, y, _ = metrics.roc_curve(ground_truth_list, scores)
    roc_data = list(zip(np.nan_to_num(x), np.nan_to_num(y)))
    roc_data.sort(key=lambda x: x[0])
    # Compute PR curve data.
    x, y, _ = metrics.precision_recall_curve(ground_truth_list, scores)
    pr_data = list(zip(np.nan_to_num(x), np.nan_to_num(y)))
    pr_data.sort(key=lambda x: x[0])
    # Create and return the result.
    return {ROC_DATA: roc_data, PR_DATA: pr_data}
  def is_compatible(
      self, model: lit_model.Model, dataset: lit_dataset.Dataset
  ) -> bool:
    """True if using a classification model and dataset has ground truth."""
    output_spec = model.output_spec()
    supported_keys = self._find_supported_pred_keys(output_spec)
    has_parents = all(
        cast(types.MulticlassPreds, output_spec[key]).parent in dataset.spec()
        for key in supported_keys
    )
    return bool(supported_keys) and has_parents
  def config_spec(self) -> types.Spec:
    # If a model is a multiclass classifier, a user can specify which
    # class label to use for plotting the curves. If the label is not
    # specified then the label with index 0 is used by default.
    return {
        TARGET_LABEL_KEY: types.CategoryLabel(),
        TARGET_PREDICTION_KEY: types.SingleFieldMatcher(
            spec='output', types=['MulticlassPreds'], required=False
        ),
    }
  def meta_spec(self) -> types.Spec:
    return {ROC_DATA: types.CurveDataPoints(), PR_DATA: types.CurveDataPoints()}
  def _find_supported_pred_keys(self, output_spec: types.Spec) -> list[str]:
    """Returns the list of supported prediction keys in the model output.
    Args:
      output_spec: The model output specification.
    Returns:
      The list of keys.
    """
    all_keys = lit_utils.find_spec_keys(output_spec, types.MulticlassPreds)
    supported_keys = [
        k for k in all_keys
        if cast(types.MulticlassPreds, output_spec[k]).parent
    ]
    return supported_keys

================
File: lit_nlp/components/gradient_maps_test.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Tests for lit_nlp.components.gradient_maps."""
from absl.testing import absltest
from absl.testing import parameterized
from lit_nlp.api import dataset as lit_dataset
from lit_nlp.components import gradient_maps
from lit_nlp.lib import testing_utils
import numpy as np
_CLASS_KEY = gradient_maps.CLASS_KEY
_INTERPOLATION_KEY = gradient_maps.INTERPOLATION_KEY
_NORMALIZATION_KEY = gradient_maps.NORMALIZATION_KEY
class GradientMapsTest(parameterized.TestCase):
  def setUp(self):
    super(GradientMapsTest, self).setUp()
    self.ig = gradient_maps.IntegratedGradients()
  # Integrated gradients tests
  def test_gradient_maps(self):
    self.ig = gradient_maps.IntegratedGradients()
    # Basic test with dummy outputs from the model.
    inputs = [{'segment': '_'}]
    model = testing_utils.ClassificationModelForTesting()
    dataset = lit_dataset.Dataset(None, None)
    output = self.ig.run(inputs, model, dataset)
    self.assertLen(output, 1)
    salience = output[0]['input_embs_grad'].salience
    target = np.array([0.25, 0.25, 0.25, 0.25])
    self.assertTrue((salience == target).all())
  def test_get_baseline(self):
    self.ig = gradient_maps.IntegratedGradients()
    result = self.ig.get_baseline(
        np.array([[[1, 1, 1, 1], [2, 3, 6, 7], [3, 4, 5, 6]]]))
    target = np.array([[[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]])
    np.testing.assert_almost_equal(result, target)
  def test_estimate_integral(self):
    self.ig = gradient_maps.IntegratedGradients()
    result = self.ig.estimate_integral(np.array([[0], [1], [2]]))
    target = np.array([1])
    np.testing.assert_almost_equal(result, target)
    result = self.ig.estimate_integral(np.array([[0, 0], [0, 0], [0, 1]]))
    target = np.array([0, 0.25])
    np.testing.assert_almost_equal(result, target)
  def test_get_interpolated_inputs(self):
    self.ig = gradient_maps.IntegratedGradients()
    result = self.ig.get_interpolated_inputs(np.array([[0], [1]]),
                                             np.array([[1], [0]]), 2)
    target = np.array([[[0], [1]], [[0.5], [0.5]], [[1], [0]]])
    np.testing.assert_almost_equal(result, target)
    np.testing.assert_almost_equal(result, target)
  @parameterized.named_parameters(
      ('default', None, None, None, None),
      ('autorun only', True, None, None, None),
      ('config values only', None, 'test_class_key', 50, False),
      ('autorun + config values', True, 'test_class_key', 50, False))
  def test_ig_init_args(self, autorun, class_key, interpolation_steps,
                        normalize):
    params = {}
    if autorun is not None:
      params['autorun'] = autorun
    if class_key is not None:
      params['class_key'] = class_key
    if interpolation_steps is not None:
      params['interpolation_steps'] = interpolation_steps
    if normalize is not None:
      params['normalize'] = normalize
    ig = gradient_maps.IntegratedGradients(**params)
    config_spec = ig.config_spec()
    if autorun is not None:
      self.assertEqual(ig.meta_spec()['saliency'].autorun, autorun)
    if class_key is not None:
      self.assertEqual(config_spec[_CLASS_KEY].default, class_key)
    if interpolation_steps is not None:
      self.assertEqual(config_spec[_INTERPOLATION_KEY].default,
                       interpolation_steps)
    if normalize is not None:
      self.assertEqual(config_spec[_NORMALIZATION_KEY].default, normalize)
if __name__ == '__main__':
  absltest.main()

================
File: lit_nlp/components/gradient_maps.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Gradient-based attribution."""
from typing import cast, Optional
from absl import logging
from lit_nlp.api import components as lit_components
from lit_nlp.api import dataset as lit_dataset
from lit_nlp.api import dtypes
from lit_nlp.api import model as lit_model
from lit_nlp.api import types
from lit_nlp.components.citrus import utils as citrus_utils
from lit_nlp.lib import utils
import numpy as np
JsonDict = types.JsonDict
Spec = types.Spec
CLASS_KEY = 'Class to explain'
NORMALIZATION_KEY = 'Normalize'
INTERPOLATION_KEY = 'Interpolation steps'
class GradientNorm(lit_components.Interpreter):
  """Salience map from gradient L2 norm."""
  def find_fields(self, output_spec: Spec) -> list[str]:
    # Find TokenGradients fields
    supported_fields: list[str] = []
    # Check that these are aligned to Tokens fields
    for f in utils.find_spec_keys(output_spec, types.TokenGradients):
      tokens_field = cast(types.TokenGradients, output_spec[f]).align
      is_valid_tokens = (
          tokens_field is not None and tokens_field in output_spec and
          isinstance(output_spec[tokens_field], types.Tokens))
      if not is_valid_tokens:
        logging.info('Skipping %s. Invalid tokens field, %s', str(f),
                     str(tokens_field))
        continue
      supported_fields.append(f)
    return supported_fields
  def _interpret(self, grads: np.ndarray, tokens: np.ndarray):
    assert grads.shape[0] == len(tokens)
    # Norm of dy/d(embs)
    grad_norm = np.linalg.norm(grads, axis=1)
    grad_norm /= np.sum(grad_norm)
    # <float32>[num_tokens]
    return grad_norm
  def run(self,
          inputs: list[JsonDict],
          model: lit_model.Model,
          dataset: lit_dataset.Dataset,
          model_outputs: Optional[list[JsonDict]] = None,
          config: Optional[JsonDict] = None) -> Optional[list[JsonDict]]:
    """Run this component, given a model and input(s)."""
    del dataset, config
    # Find gradient fields to interpret
    output_spec = model.output_spec()
    grad_fields = self.find_fields(output_spec)
    logging.info('Found fields for gradient attribution: %s', str(grad_fields))
    if len(grad_fields) == 0:  # pylint: disable=g-explicit-length-test
      return None
    # Run model, if needed.
    if model_outputs is None:
      model_outputs = list(model.predict(inputs))
    assert len(model_outputs) == len(inputs)
    all_results: list[JsonDict] = []
    for o in model_outputs:
      result: dict[str, dtypes.TokenSalience] = {}
      for grad_field in grad_fields:
        token_field = cast(types.TokenGradients, output_spec[grad_field]).align
        tokens = o[token_field]
        scores = self._interpret(o[grad_field], tokens)
        result[grad_field] = dtypes.TokenSalience(tokens, scores)
      all_results.append(result)
    return all_results
  def is_compatible(self, model: lit_model.Model,
                    dataset: lit_dataset.Dataset) -> bool:
    del dataset  # Unused by Grad L2 Norm
    return bool(self.find_fields(model.output_spec()))
  def meta_spec(self) -> types.Spec:
    return {'saliency': types.TokenSalience(autorun=True, signed=False)}
class GradientDotInput(lit_components.Interpreter):
  """Salience map using the values of gradient * input as attribution."""
  def find_fields(self, output_spec: Spec) -> list[str]:
    # Find and check that TokenGradients fields are aligned to Tokens fields
    aligned_fields = []
    for f in utils.find_spec_keys(output_spec, types.TokenGradients):
      field_spec = cast(types.TokenGradients, output_spec[f])
      tokens_field = field_spec.align
      is_valid_tokens = (
          tokens_field is not None and tokens_field in output_spec and
          isinstance(output_spec[tokens_field], types.Tokens))
      if not is_valid_tokens:
        logging.info('Skipping %s. Invalid tokens field, %s', str(f),
                     str(tokens_field))
        continue
      embeddings_field = field_spec.grad_for
      is_valid_embeddings = (
          embeddings_field is not None and embeddings_field in output_spec and
          isinstance(output_spec[embeddings_field], types.TokenEmbeddings))
      if not is_valid_embeddings:
        logging.info('Skipping %s. Invalid emebeddings field, %s.', str(f),
                     str(tokens_field))
        continue
      aligned_fields.append(f)
    return aligned_fields
  def _interpret(self, grads: np.ndarray, embs: np.ndarray):
    assert grads.shape == embs.shape
    # dot product of gradients and embeddings
    # <float32>[num_tokens]
    grad_dot_input = np.sum(grads * embs, axis=-1)
    scores = citrus_utils.normalize_scores(grad_dot_input)
    return scores
  def run(self,
          inputs: list[JsonDict],
          model: lit_model.Model,
          dataset: lit_dataset.Dataset,
          model_outputs: Optional[list[JsonDict]] = None,
          config: Optional[JsonDict] = None) -> Optional[list[JsonDict]]:
    """Run this component, given a model and input(s)."""
    # Find gradient fields to interpret
    output_spec = model.output_spec()
    grad_fields = self.find_fields(output_spec)
    logging.info('Found fields for gradient attribution: %s', str(grad_fields))
    if len(grad_fields) == 0:  # pylint: disable=g-explicit-length-test
      return None
    # Run model, if needed.
    if model_outputs is None:
      model_outputs = list(model.predict(inputs))
    assert len(model_outputs) == len(inputs)
    all_results: list[JsonDict] = []
    for o in model_outputs:
      result: dict[str, dtypes.TokenSalience] = {}
      for grad_field in grad_fields:
        embeddings_field = cast(types.TokenGradients,
                                output_spec[grad_field]).grad_for
        scores = self._interpret(o[grad_field], o[embeddings_field])
        token_field = cast(types.TokenGradients, output_spec[grad_field]).align
        tokens = o[token_field]
        result[grad_field] = dtypes.TokenSalience(tokens, scores)
      all_results.append(result)
    return all_results
  def is_compatible(self, model: lit_model.Model,
                    dataset: lit_dataset.Dataset) -> bool:
    del dataset  # Unused by Grad*Input
    return bool(self.find_fields(model.output_spec()))
  def meta_spec(self) -> types.Spec:
    return {'saliency': types.TokenSalience(autorun=True, signed=True)}
class IntegratedGradients(lit_components.Interpreter):
  """Salience map from Integrated Gradients.
  Integrated Gradients is an attribution method originally proposed in
  Sundararajan et al. (https://arxiv.org/abs/1703.01365), which attributes an
  importance value for each input feature based on the gradients of the model
  output with respect to the input. The feature attribution values are
  calculated by taking the integral of gradients along a straight path from a
  baseline to the input being analyzed. The original implementation can be
  found at: https://github.com/ankurtaly/Integrated-Gradients/blob/master/
  BertModel/bert_model_utils.py
  This component requires that the following fields in the model spec. Field
  names like `embs` are placeholders; you can call them whatever you like,
  and as with other LIT components multiple segments are supported.
    Output:
      - TokenEmbeddings (`embs`) to return the input embeddings
      - TokenGradients (`grads`) to return gradients w.r.t. `embs`
      - A label field (`target`) to return the label that `grads`
        was computed for. This is usually a CategoryLabel, but can be anything
        since it will just be fed back into the model.
    Input
      - TokenEmbeddings (`embs`) to accept the modified input embeddings
      - A label field to (`target`) to pin the gradient target to the same
        label for all integral steps, since the argmax prediction may change.
  """
  def __init__(self,
               autorun: bool = False,
               class_key: str = '',
               interpolation_steps: int = 30,
               normalize: bool = True):
    """Cretaes an IntegratedGradients interpreter.
    Args:
      autorun: Determines if this intepreter should run automatically.
      class_key: The class to explain.
      interpolation_steps: The number of steps to interpolate.
      normalize: Flag to enable/disable normalization.
    """
    self._autorun: bool = autorun
    self._class_key: str = class_key
    self._interpolation_steps: int = interpolation_steps
    self._normalize: bool = normalize
  def find_fields(self, input_spec: Spec, output_spec: Spec) -> list[str]:
    # Find and check that TokenGradients fields are aligned to Tokens fields
    aligned_fields = []
    for f in utils.find_spec_keys(output_spec, types.TokenGradients):
      field_spec = cast(types.TokenGradients, output_spec[f])
      tokens_field = field_spec.align
      embeddings_field = field_spec.grad_for
      grad_key = field_spec.grad_target_field_key
      if not isinstance(output_spec.get(tokens_field), types.Tokens):
        logging.info('Skipping %s. Invalid tokens field, %s.', str(f),
                     str(tokens_field))
        continue
      is_embs_valid = (
          isinstance(input_spec.get(embeddings_field),
                     types.TokenEmbeddings) and
          isinstance(output_spec.get(embeddings_field), types.TokenEmbeddings))
      if not is_embs_valid:
        logging.info('Skipping %s. Invalid embeddings field, %s.', str(f),
                     str(tokens_field))
        continue
      is_grad_cls_valid = grad_key in input_spec
      if not is_grad_cls_valid:
        logging.info('Skipping %s. Invalid gradient class field, %s.', str(f),
                     str(tokens_field))
        continue
      aligned_fields.append(f)
    return aligned_fields
  def get_interpolated_inputs(self, baseline: np.ndarray, target: np.ndarray,
                              num_steps: int) -> np.ndarray:
    """Gets num_step linearly interpolated inputs from baseline to target."""
    if num_steps <= 0: return np.array([])
    if num_steps == 1: return np.array([baseline, target])
    delta = target - baseline  # <float32>[num_tokens, emb_size]
    # Creates scale values array of shape [num_steps, num_tokens, emb_dim],
    # where the values in scales[i] are the ith step from np.linspace.
    # <float32>[num_steps, 1, 1]
    scales = np.linspace(0, 1, num_steps + 1,
                         dtype=np.float32)[:, np.newaxis, np.newaxis]
    shape = (num_steps + 1,) + delta.shape
    # <float32>[num_steps, num_tokens, emb_size]
    deltas = scales * np.broadcast_to(delta, shape)
    interpolated_inputs = baseline + deltas
    return interpolated_inputs  # <float32>[num_steps, num_tokens, emb_size]
  def estimate_integral(self, path_gradients: np.ndarray) -> np.ndarray:
    """Estimates the integral of the path_gradients using trapezoid rule."""
    path_gradients = (path_gradients[:-1] + path_gradients[1:]) / 2
    # There are num_steps elements in the path_gradients. Summing num_steps - 1
    # terms and dividing by num_steps - 1 is equivalent to taking
    # the average.
    return np.average(path_gradients, axis=0)
  def get_baseline(self, embeddings: np.ndarray) -> np.ndarray:
    """Returns baseline embeddings to use in Integrated Gradients."""
    # Replaces embeddings in the original input with the zero embedding, or
    # with the specified token embedding.
    baseline = np.zeros_like(embeddings)
    # TODO(ellenj): Add option to use a token's embedding as the baseline.
    return baseline
  def get_salience_result(self, model_input: JsonDict, model: lit_model.Model,
                          interpolation_steps: int, normalize: bool,
                          class_to_explain: str, model_output: JsonDict,
                          grad_fields: list[str]):
    result = {}
    output_spec = model.output_spec()
    # We ensure that the embedding and gradient class fields are present in the
    # model's input spec in find_fields().
    embeddings_fields = [
        cast(types.TokenGradients,
             output_spec[grad_field]).grad_for for grad_field in grad_fields]
    # The gradient class input is used to specify the target class of the
    # gradient calculation (if unspecified, this option defaults to the argmax,
    # which could flip between interpolated inputs).
    # If class_to_explain is emptystring, then explain the argmax class.
    grad_class_key = cast(types.TokenGradients,
                          output_spec[grad_fields[0]]).grad_target_field_key
    if class_to_explain == '':  # pylint: disable=g-explicit-bool-comparison
      if grad_class_key in model_output:
        grad_class = model_output[grad_class_key]
      else:
        # If this is not present, should error because we can't infer
        # what class to use.
        grad_class = model_input[grad_class_key]
    else:
      grad_class = class_to_explain
    interpolated_inputs = {}
    all_embeddings = []
    all_baselines = []
    for embed_field in embeddings_fields:
      # <float32>[num_tokens, emb_size]
      embeddings = np.array(model_output[embed_field])
      all_embeddings.append(embeddings)
      # Starts with baseline of zeros. <float32>[num_tokens, emb_size]
      baseline = self.get_baseline(embeddings)
      all_baselines.append(baseline)
      # Get interpolated inputs from baseline to original embedding.
      # <float32>[interpolation_steps, num_tokens, emb_size]
      interpolated_inputs[embed_field] = self.get_interpolated_inputs(
          baseline, embeddings, interpolation_steps)
    # Create model inputs and populate embedding field(s).
    inputs_with_embeds = []
    for i in range(interpolation_steps):
      # Interpolates embeddings for all inputs simultaneously.
      # Each entry is <float32>[num_tokens, emb_size]
      updates = {k: interpolated_inputs[k][i] for k in embeddings_fields}
      updates[grad_class_key] = grad_class
      input_copy = utils.make_modified_input(model_input, updates, 'IG')
      inputs_with_embeds.append(input_copy)
    embed_outputs = model.predict(inputs_with_embeds)
    # Create list with concatenated gradients for each interpolate input.
    gradients = []
    for o in embed_outputs:
      # <float32>[total_num_tokens, emb_size]
      interp_gradients = np.concatenate([o[field] for field in grad_fields])
      gradients.append(interp_gradients)
    # <float32>[interpolation_steps, total_num_tokens, emb_size]
    path_gradients = np.stack(gradients, axis=0)
    # Calculate integral
    # <float32>[total_num_tokens, emb_size]
    integral = self.estimate_integral(path_gradients)
    # <float32>[total_num_tokens, emb_size]
    concat_embeddings = np.concatenate(all_embeddings)
    # <float32>[total_num_tokens, emb_size]
    concat_baseline = np.concatenate(all_baselines)
    # <float32>[total_num_tokens, emb_size]
    integrated_gradients = integral * (np.array(concat_embeddings) -
                                       np.array(concat_baseline))
    # Dot product of integral values and (embeddings - baseline).
    # <float32>[total_num_tokens]
    attributions = np.sum(integrated_gradients, axis=-1)
    # <float32>[total_num_tokens]
    scores = citrus_utils.normalize_scores(
        attributions) if normalize else attributions
    for grad_field in grad_fields:
      # Format as salience map result.
      token_field = cast(types.TokenGradients, output_spec[grad_field]).align
      tokens = model_output[token_field]
      # Only use the scores that correspond to the tokens in this grad_field.
      # The gradients for all input embeddings were concatenated in the order
      # of the grad fields, so they can be sliced out in the same order.
      sliced_scores = scores[:len(tokens)]  # <float32>[num_tokens in field]
      scores = scores[len(tokens):]  # <float32>[num_remaining_tokens]
      assert len(tokens) == len(sliced_scores)
      result[grad_field] = dtypes.TokenSalience(tokens, sliced_scores)
    return result
  def run(self,
          inputs: list[JsonDict],
          model: lit_model.Model,
          dataset: lit_dataset.Dataset,
          model_outputs: Optional[list[JsonDict]] = None,
          config: Optional[JsonDict] = None) -> Optional[list[JsonDict]]:
    """Run this component, given a model and input(s)."""
    config = config or {}
    class_to_explain = config.get(CLASS_KEY, self._class_key)
    try:
      interpolation_steps = int(config.get(INTERPOLATION_KEY,
                                           self._interpolation_steps))
    except ValueError as parse_error:
      raise RuntimeError(
          'Failed to parse interpolation steps'
          f'from "{config[INTERPOLATION_KEY]}".') from parse_error
    normalization = config.get(NORMALIZATION_KEY, self._normalize)
    # Find gradient fields to interpret
    input_spec = model.input_spec()
    output_spec = model.output_spec()
    grad_fields = self.find_fields(input_spec, output_spec)
    logging.info('Found fields for integrated gradients: %s', str(grad_fields))
    if len(grad_fields) == 0:  # pylint: disable=g-explicit-length-test
      return None
    # Run model, if needed.
    if model_outputs is None:
      model_outputs = list(model.predict(inputs))
    all_results = []
    for model_output, model_input in zip(model_outputs, inputs):
      result = self.get_salience_result(model_input, model, interpolation_steps,
                                        normalization, class_to_explain,
                                        model_output, grad_fields)
      all_results.append(result)
    return all_results
  def is_compatible(self, model: lit_model.Model,
                    dataset: lit_dataset.Dataset) -> bool:
    del dataset  # Unused by IG
    return bool(self.find_fields(model.input_spec(), model.output_spec()))
  def config_spec(self) -> types.Spec:
    return {
        CLASS_KEY: types.TextSegment(default=self._class_key),
        NORMALIZATION_KEY: types.Boolean(default=self._normalize),
        INTERPOLATION_KEY:
            types.Scalar(
                min_val=5,
                max_val=100,
                default=self._interpolation_steps,
                step=1)
    }
  def meta_spec(self) -> types.Spec:
    return {'saliency': types.TokenSalience(autorun=self._autorun, signed=True)}

================
File: lit_nlp/components/hotflip_int_test.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Tests for lit_nlp.components.hotflip."""
from absl.testing import absltest
from absl.testing import parameterized
from lit_nlp.components import hotflip
# TODO(lit-dev): Move glue_models out of lit_nlp/examples
from lit_nlp.examples.glue import models as glue_models
import numpy as np
from lit_nlp.lib import file_cache
BERT_TINY_PATH = file_cache.cached_path(
    'https://storage.googleapis.com/what-if-tool-resources/lit-models/sst2_tiny.tar.gz',  # pylint: disable=line-too-long
    extract_compressed_file=True,
)
STSB_PATH = file_cache.cached_path(
    'https://storage.googleapis.com/what-if-tool-resources/lit-models/stsb_tiny.tar.gz',  # pylint: disable=line-too-long
    extract_compressed_file=True,
)
_CONFIG_CLASSIFICATION = {
    hotflip.FIELDS_TO_HOTFLIP_KEY: ['tokens_sentence'],
    hotflip.PREDICTION_KEY: 'probas',
}
_CONFIG_REGRESSION = {
    hotflip.FIELDS_TO_HOTFLIP_KEY: ['tokens_sentence1', 'tokens_sentence2'],
    hotflip.PREDICTION_KEY: 'score',
    hotflip.REGRESSION_THRESH_KEY: 2,
}
_SST2_EXAMPLE = {'sentence': 'this long movie is terrible.'}
_STSB_EXAMPLE = {
    'sentence1': 'this long movie is terrible.',
    'sentence2': 'this short movie is great.'
}
class HotflipIntegrationTest(parameterized.TestCase):
  def __init__(self, *args, **kwargs):
    super(HotflipIntegrationTest, self).__init__(*args, **kwargs)
    self.classification_model = glue_models.SST2Model(BERT_TINY_PATH)
    self.regression_model = glue_models.STSBModel(STSB_PATH)
  def setUp(self):
    super(HotflipIntegrationTest, self).setUp()
    self.hotflip = hotflip.HotFlip()
  @parameterized.named_parameters(
      ('0_examples', 0),
      ('1_examples', 1),
      ('2_examples', 2),
  )
  def test_hotflip_num_ex(self, num_examples: int):
    config = {**_CONFIG_CLASSIFICATION, hotflip.NUM_EXAMPLES_KEY: num_examples}
    counterfactuals = self.hotflip.generate(_SST2_EXAMPLE,
                                            self.classification_model, None,
                                            config)
    self.assertLen(counterfactuals, num_examples)
  @parameterized.named_parameters(
      ('0_examples', 0),
      ('1_examples', 1),
      ('2_examples', 2),
  )
  def test_hotflip_num_ex_multi_input(self, num_examples: int):
    config = {**_CONFIG_REGRESSION, hotflip.NUM_EXAMPLES_KEY: num_examples}
    counterfactuals = self.hotflip.generate(_STSB_EXAMPLE,
                                            self.regression_model, None, config)
    self.assertLen(counterfactuals, num_examples)
  @parameterized.named_parameters(
      ('terrible', ['terrible'], [4]),
      ('long_terrible', ['long', 'terrible'], [1, 4]),
  )
  def test_hotflip_freeze_tokens(self, ignore: list[str],
                                 exp_indexes: list[int]):
    config = {
        **_CONFIG_CLASSIFICATION,
        hotflip.NUM_EXAMPLES_KEY: 10,
        hotflip.TOKENS_TO_IGNORE_KEY: ignore,
    }
    counterfactuals = self.hotflip.generate(
        _SST2_EXAMPLE, self.classification_model, None, config)
    self.assertEqual(len(ignore), len(exp_indexes))
    for target, index in zip(ignore, exp_indexes):
      for counterfactual in counterfactuals:
        tokens = counterfactual['tokens_sentence']
        self.assertEqual(target, tokens[index])
  def test_hotflip_freeze_tokens_multi_input(self):
    config = {
        **_CONFIG_REGRESSION,
        hotflip.NUM_EXAMPLES_KEY: 10,
        hotflip.TOKENS_TO_IGNORE_KEY: ['long', 'terrible'],
    }
    ex = {
        'sentence1': 'this long movie is terrible.',
        'sentence2': 'this long movie is great.',
    }
    counterfactuals = self.hotflip.generate(ex, self.regression_model, None,
                                            config)
    for cf in counterfactuals:
      tokens1 = cf['tokens_sentence1']
      tokens2 = cf['tokens_sentence2']
      self.assertEqual('terrible', tokens1[4])
      self.assertEqual('long', tokens1[1])
      self.assertEqual('long', tokens2[1])
  def test_hotflip_max_flips(self):
    config = {
        **_CONFIG_CLASSIFICATION,
        hotflip.MAX_FLIPS_KEY: 1,
        hotflip.NUM_EXAMPLES_KEY: 1,
    }
    ex = _SST2_EXAMPLE
    ex_output = list(self.classification_model.predict([ex]))[0]
    ex_tokens = ex_output['tokens_sentence']
    cfs_1 = self.hotflip.generate(ex, self.classification_model, None, config)
    cf_tokens = list(cfs_1)[0]['tokens_sentence']
    ex = {'sentence': 'this long movie is terrible and horrible.'}
    cfs_2 = self.hotflip.generate(ex, self.classification_model, None, config)
    self.assertEqual(1, sum([1 for i, t in enumerate(cf_tokens)
                             if t != ex_tokens[i]]))
    self.assertEmpty(cfs_2)
  def test_hotflip_max_flips_multi_input(self):
    config = {
        **_CONFIG_REGRESSION,
        hotflip.MAX_FLIPS_KEY: 1,
        hotflip.NUM_EXAMPLES_KEY: 20,
    }
    ex = _STSB_EXAMPLE
    ex_output = list(self.regression_model.predict([ex]))[0]
    ex_tokens1 = ex_output['tokens_sentence1']
    ex_tokens2 = ex_output['tokens_sentence2']
    cfs = self.hotflip.generate(ex, self.regression_model, None, config)
    for cf in cfs:
      # Number of flips in each field should be no more than MAX_FLIPS.
      cf_tokens1 = cf['tokens_sentence1']
      cf_tokens2 = cf['tokens_sentence2']
      self.assertLessEqual(sum([1 for i, t in enumerate(cf_tokens1)
                                if t != ex_tokens1[i]]), 1)
      self.assertLessEqual(sum([1 for i, t in enumerate(cf_tokens2)
                                if t != ex_tokens2[i]]), 1)
  def test_hotflip_only_flip_one_field(self):
    config = {**_CONFIG_REGRESSION, hotflip.NUM_EXAMPLES_KEY: 10}
    ex = _STSB_EXAMPLE
    cfs = self.hotflip.generate(ex, self.regression_model, None, config)
    for cf in cfs:
      self.assertTrue(
          (cf['sentence1'] == ex['sentence1']) or
          (cf['sentence2'] == ex['sentence2']))
  def test_hotflip_changes_pred_class(self):
    config = _CONFIG_CLASSIFICATION
    ex = _SST2_EXAMPLE
    ex_output = list(self.classification_model.predict([ex]))[0]
    pred_class = str(np.argmax(ex_output['probas']))
    cfs = self.hotflip.generate(ex, self.classification_model, None, config)
    cf_outputs = self.classification_model.predict(cfs)
    self.assertEqual('0', pred_class)
    for cf_output in cf_outputs:
      self.assertNotEqual(np.argmax(ex_output['probas']),
                          np.argmax(cf_output['probas']))
  def test_hotflip_changes_regression_score(self):
    config = {**_CONFIG_REGRESSION, hotflip.NUM_EXAMPLES_KEY: 2}
    ex = _STSB_EXAMPLE
    thresh = config[hotflip.REGRESSION_THRESH_KEY]
    ex_output = list(self.regression_model.predict([ex]))[0]
    cfs = self.hotflip.generate(ex, self.regression_model, None, config)
    cf_outputs = self.regression_model.predict(cfs)
    for cf_output in cf_outputs:
      self.assertNotEqual((ex_output['score'] <= thresh),
                          (cf_output['score'] <= thresh))
if __name__ == '__main__':
  absltest.main()

================
File: lit_nlp/components/hotflip_test.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Tests for lit_nlp.components.hotflip."""
from typing import Optional
from absl.testing import absltest
from absl.testing import parameterized
from lit_nlp.api import dataset as lit_dataset
from lit_nlp.api import model as lit_model
from lit_nlp.api import types as lit_types
from lit_nlp.components import hotflip
from lit_nlp.lib import utils
import numpy as np
class TestClassificationModel(lit_model.Model):
  def input_spec(self) -> dict[str, lit_types.LitType]:
    return {
        'sentence':
            lit_types.TextSegment(),
        'tokens_sentence':
            lit_types.Tokens(parent='sentence', required=False),
        'input_embs_sentence':
            lit_types.TokenEmbeddings(align='tokens_sentence', required=False),
    }
  def output_spec(self) -> dict[str, lit_types.LitType]:
    return {
        'probas':
            lit_types.MulticlassPreds(vocab=[]),
        'grad_class':
            lit_types.CategoryLabel(required=False, vocab=[]),
        'tokens':
            lit_types.Tokens(),
        'tokens_sentence':
            lit_types.Tokens(parent='sentence', required=False),
        'token_grad_sentence':
            lit_types.TokenGradients(
                align='tokens_sentence',
                grad_for='input_embs_sentence',
                grad_target_field_key='grad_class'),
    }
  def get_embedding_table(self):
    return ([], np.ndarray([]))
  def predict(
      self, inputs: list[lit_model.JsonDict]
  ) -> list[lit_model.JsonDict]:
    pass
class TestRegressionModel(lit_model.Model):
  def input_spec(self) -> dict[str, lit_types.LitType]:
    return {
        'sentence1':
            lit_types.TextSegment(),
        'tokens_sentence1':
            lit_types.Tokens(parent='sentence1', required=False),
        'input_embs_sentence1':
            lit_types.TokenEmbeddings(align='tokens_sentence1', required=False),
        'sentence2':
            lit_types.TextSegment(),
        'tokens_sentence2':
            lit_types.Tokens(parent='sentence2', required=False),
        'input_embs_sentence2':
            lit_types.TokenEmbeddings(align='tokens_sentence2', required=False),
    }
  def output_spec(self) -> dict[str, lit_types.LitType]:
    return {
        'score':
            lit_types.RegressionScore(),
        'grad_class':
            lit_types.CategoryLabel(required=False, vocab=[]),
        'tokens':
            lit_types.Tokens(),
        'tokens_sentence1':
            lit_types.Tokens(parent='sentence1', required=False),
        'token_grad_sentence1':
            lit_types.TokenGradients(
                align='tokens_sentence1',
                grad_for='input_embs_sentence1',
                grad_target_field_key='grad_class'),
        'tokens_sentence2':
            lit_types.Tokens(parent='sentence2', required=False),
        'token_grad_sentence2':
            lit_types.TokenGradients(
                align='tokens_sentence2',
                grad_for='input_embs_sentence2',
                grad_target_field_key='grad_class')
    }
  def get_embedding_table(self):
    return ([], np.ndarray([]))
  def predict(
      self, inputs: list[lit_model.JsonDict]
  ) -> list[lit_model.JsonDict]:
    pass
class ModelWithoutCallableEmbeddingTable():
  def __init__(self, base: lit_model.Model):
    self.get_embedding_table = 'get_embedding_table'
    self.base = base
  def input_spec(self):
    return self.base.input_spec()
  def output_spec(self):
    return self.base.output_spec()
class ModelWithoutEmbeddingTable():
  def __init__(self, base: lit_model.Model):
    self.base = base
  def input_spec(self):
    return self.base.input_spec()
  def output_spec(self):
    return self.base.output_spec()
  def get_embedding_table(self):
    raise NotImplementedError()
class ModelWithoutGradients():
  def __init__(self, base: lit_model.Model):
    self.base = base
  def input_spec(self):
    return self.base.input_spec()
  def output_spec(self):
    ret = self.base.output_spec()
    token_gradient_keys = utils.find_spec_keys(ret, lit_types.TokenGradients)
    for k in token_gradient_keys:
      ret.pop(k, None)
    return ret
class ModelWithoutTokens():
  def __init__(self, base: lit_model.Model):
    self.base = base
  def input_spec(self):
    ret = self.base.input_spec()
    token_keys = utils.find_spec_keys(ret, lit_types.Tokens)
    for k in token_keys:
      ret.pop(k, None)
    return ret
  def output_spec(self):
    return self.base.output_spec()
_CLASSIFICATION_GOOD = TestClassificationModel()
_CLASSIFICATION_NO_CALLABLE_EMB = ModelWithoutCallableEmbeddingTable(
    _CLASSIFICATION_GOOD)
_CLASSIFICATION_NO_EMBEDDING_TABLE = ModelWithoutEmbeddingTable(
    _CLASSIFICATION_GOOD)
_CLASSIFICATION_NO_GRADIENTS = ModelWithoutGradients(_CLASSIFICATION_GOOD)
_CLASSIFICATION_NO_TOKENS = ModelWithoutTokens(_CLASSIFICATION_GOOD)
_REGRESSION_GOOD = TestRegressionModel()
_REGRESSION_NO_CALLABLE_EMB = ModelWithoutCallableEmbeddingTable(
    _REGRESSION_GOOD)
_REGRESSION_NO_EMBEDDING_TABLE = ModelWithoutEmbeddingTable(_REGRESSION_GOOD)
_REGRESSION_NO_GRADIENTS = ModelWithoutGradients(_REGRESSION_GOOD)
_REGRESSION_NO_TOKENS = ModelWithoutTokens(_REGRESSION_GOOD)
class HotflipTest(parameterized.TestCase):
  def setUp(self):
    super(HotflipTest, self).setUp()
    self.hotflip = hotflip.HotFlip()
  @parameterized.named_parameters(
      ('cls', _CLASSIFICATION_GOOD, True),
      ('cls_no_call_emb_table', _CLASSIFICATION_NO_CALLABLE_EMB, False),
      ('cls_no_emb_table', _CLASSIFICATION_NO_EMBEDDING_TABLE, False),
      ('cls_no_gradients', _CLASSIFICATION_NO_GRADIENTS, False),
      ('cls_no_tokens', _CLASSIFICATION_NO_TOKENS, False),
      ('regression', _REGRESSION_GOOD, True),
      ('regression_no_call_emb_table', _REGRESSION_NO_CALLABLE_EMB, False),
      ('regression_no_emb_table', _REGRESSION_NO_EMBEDDING_TABLE, False),
      ('regression_no_gradients', _REGRESSION_NO_GRADIENTS, False),
      ('regression_no_tokens', _REGRESSION_NO_TOKENS, False),
  )
  def test_is_compatible(self, model: lit_model.Model, expected_compat: bool):
    compat = self.hotflip.is_compatible(
        model, lit_dataset.NoneDataset({'test': model}))
    self.assertEqual(expected_compat, compat)
  @parameterized.named_parameters(
      ('cls_no_align', _CLASSIFICATION_GOOD, lit_types.MulticlassPreds,
       ['probas'], None),
      ('cls_align', _CLASSIFICATION_GOOD, lit_types.TokenGradients,
       ['token_grad_sentence'], 'tokens_sentence'),
      ('cls_empty', _CLASSIFICATION_GOOD, lit_types.TokenGradients, [],
       'input_embs_sentence'),
      ('reg_no_align', _REGRESSION_GOOD, lit_types.RegressionScore, ['score'
                                                                    ], None),
      ('reg_align', _REGRESSION_GOOD, lit_types.TokenGradients,
       ['token_grad_sentence1'], 'tokens_sentence1'),
      ('reg_empty', _REGRESSION_GOOD, lit_types.TokenGradients, [],
       'input_embs_sentence1'),
  )
  def test_find_fields(self, model: lit_model.Model, to_find: lit_types.LitType,
                       expected_fields: list[str], align_field: Optional[str]):
    found = self.hotflip.find_fields(model.output_spec(), to_find, align_field)
    self.assertEqual(found, expected_fields)
if __name__ == '__main__':
  absltest.main()

================
File: lit_nlp/components/hotflip.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""HotFlip generator that perturbs input tokens to flip the prediction.
A HotFlip is defined as a counterfactual input that alters one or more
tokens in the input at hand in order to obtain a different prediction.
A hotflip is considered minimal if no strict subset of the applied token flips
succeeds in flipping the prediction.
This generator extends ideas from the following papers.
(1) HotFlip: White-Box Adversarial Examples for Text Classification
    Javid Ebrahimi, Anyi Rao, Daniel Lowd, Dejing Dou
    ACL 2018.
    https://www.aclweb.org/anthology/P18-2006/
(2) Local Explanations via Necessity and Sufficiency: Unifying Theory and
    Practice
    David Watson, Limor Gultchin, Ankur Taly, Luciano Floridi
    UAI 2021.
    https://arxiv.org/abs/2103.14651
"""
from collections.abc import Iterator, Mapping
import itertools
from typing import Any, cast, Optional
from absl import logging
from lit_nlp.api import components as lit_components
from lit_nlp.api import dataset as lit_dataset
from lit_nlp.api import model as lit_model
from lit_nlp.api import types
from lit_nlp.components import cf_utils
from lit_nlp.lib import utils
import numpy as np
JsonDict = types.JsonDict
Spec = types.Spec
PREDICTION_KEY = "Prediction key"
NUM_EXAMPLES_KEY = "Max number of counterfactuals to generate"
NUM_EXAMPLES_DEFAULT = 5
MAX_FLIPS_KEY = "Max number of token flips"
MAX_FLIPS_DEFAULT = 3
TOKENS_TO_IGNORE_KEY = "Comma-separated list of tokens to never flip"
TOKENS_TO_IGNORE_DEFAULT = []
REGRESSION_THRESH_KEY = "Regression threshold"
REGRESSION_THRESH_DEFAULT = 0.0
MAX_FLIPPABLE_TOKENS = 10
FIELDS_TO_HOTFLIP_KEY = "Fields to flip tokens in"
class HotFlip(lit_components.Generator):
  """HotFlip generator.
  This implemention uses a single backward pass to estimate the gradient of
  each token and uses them to heuristically estimate the impact of perturbing
  the token.
  This generator works for both classification and regression models. In the
  case of classification models, the returned counterfactuals are guaranteed to
  have a different prediction class as the original example. In the case of
  regression models, the returned counterfactuals are guaranteed to be on the
  opposite side of a user-provided threshold as the original example.
  The returned counterfactuals are guaranteed to be minimal in the sense that
  no strict subset of the applied token perturbations would have resulted in a
  prediction flip.
  """
  def description(self) -> str:
    # TODO(lit-dev): Find way to have newlines in the string and have it be
    # displayed correctly on the front-end.
    return """Uses token-wise gradients to find minimal tokens changes that
      cause the model to return a different prediction.\n\nIn the
      case of classification models, the returned counterfactuals are guaranteed
      to have a different prediction class as the original example. In the case
      of regression models, the returned counterfactuals are guaranteed to be on
      the opposite side of a user-provided threshold as the original example.
      \n\nCan fail to produce counterfactuals if there is no set of token
      changes within the scope of the configuration options that cause
      significant model prediction changes.
    """
  def is_compatible(self, model: lit_model.Model,
                    dataset: lit_dataset.Dataset) -> bool:
    """Returns true if the given model is compatible with HotFlip."""
    del dataset  # Unused by HotFlip
    get_embedding_table = getattr(model, "get_embedding_table", None)
    if not callable(get_embedding_table):
      return False
    try:
      table = get_embedding_table()
      if not isinstance(table, tuple): return False
      vocab, embs_dims = table
      if not isinstance(vocab, list): return False
      if not isinstance(embs_dims, np.ndarray): return False
      # TODO(lit-dev): Further validate the shape of the embeddings table?
    except NotImplementedError:
      return False
    input_spec = model.input_spec()
    output_spec = model.output_spec()
    for grad_key in utils.find_spec_keys(output_spec, types.TokenGradients):
      grad_field = cast(types.TokenGradients, output_spec.get(grad_key))
      aligned_field: Optional[types.LitType] = input_spec.get(grad_field.align)
      if isinstance(aligned_field, types.Tokens):
        return True
    return False
  def find_fields(self,
                  spec: Spec,
                  typ: type[types.LitType],
                  align_field: Optional[str] = None) -> list[str]:
    # Find fields of provided 'typ'.
    fields = utils.find_spec_keys(spec, typ)
    if align_field is None:
      return fields
    # Only return fields that are aligned to fields with name specified by
    # align_field.
    return [f for f in fields
            if getattr(spec[f], "align", None) == align_field]
  def _get_tokens_and_gradients(self, input_spec: Spec,
                                output_spec: Spec, output: JsonDict,
                                selected_fields: list[str]):
    """Returns a dictionary mapping token fields to tokens and gradients."""
    # Find selected token fields.
    input_spec_keys = set(utils.find_spec_keys(input_spec, types.Tokens))
    logging.info("input_spec_keys: %r", input_spec_keys)
    selected_input_spec_keys = list(input_spec_keys & set(selected_fields))
    logging.info("selected_input_spec_keys: %r", selected_input_spec_keys)
    token_fields = [key for key in selected_input_spec_keys
                    if input_spec[key].is_compatible(output_spec.get(key))]
    if len(token_fields) == 0:  # pylint: disable=g-explicit-length-test
      return {}
    ret = {}
    for token_field in token_fields:
      # Get tokens, token gradients and token embeddings.
      tokens = output[token_field]
      grad_fields = self.find_fields(output_spec, types.TokenGradients,
                                     token_field)
      assert grad_fields, (
          f"No gradients found for {token_field}. Cannot use HotFlip. :-(")
      assert len(grad_fields) == 1, (
          f"Multiple gradients found for {token_field}."
          f"Cannot use HotFlip. :-(")
      grads = output[grad_fields[0]] if grad_fields else None
      ret[token_field] = [tokens, grads]
    return ret
  def config_spec(self) -> types.Spec:
    return {
        NUM_EXAMPLES_KEY:
            types.TextSegment(default=str(NUM_EXAMPLES_DEFAULT)),
        MAX_FLIPS_KEY:
            types.TextSegment(default=str(MAX_FLIPS_DEFAULT)),
        TOKENS_TO_IGNORE_KEY:
            types.Tokens(default=TOKENS_TO_IGNORE_DEFAULT),
        PREDICTION_KEY:
            types.SingleFieldMatcher(
                spec="output", types=["MulticlassPreds", "RegressionScore"]),
        REGRESSION_THRESH_KEY:
            types.TextSegment(default=str(REGRESSION_THRESH_DEFAULT)),
        FIELDS_TO_HOTFLIP_KEY:
            types.MultiFieldMatcher(
                spec="input", types=["Tokens"], select_all=True),
    }
  def _subset_exists(self, cand_set, sets):
    """Checks whether a subset of 'cand_set' exists in 'sets'."""
    for s in sets:
      if s.issubset(cand_set):
        return True
    return False
  def _gen_token_idxs_to_flip(
      self, tokens: list[str], token_grads: np.ndarray, max_flips: int,
      tokens_to_ignore: list[str]) -> Iterator[tuple[int, ...]]:
    """Generates sets of token positions that are eligible for flipping."""
    # Consider all combinations of tokens upto length max_flips.
    # We will iterate through this list (sortted by cardinality) and at each
    # iteration, replace the selected tokens with corresponding replacement
    # tokens and checks if the prediction flips. At each cardinality, we will
    # consider combinations by ordering tokens by gradient L2 in order to
    # prioritize flipping tokens that may have the largest impact on the
    # prediction.
    token_grads_l2 = np.sum(token_grads * token_grads, axis=-1)
    # TODO(ataly, bastings): Consider sorting by attributions (either
    # Integrated Gradients or Shapley values).
    token_idxs = np.argsort(token_grads_l2)[::-1]
    token_idxs_to_flip = [idx for idx in token_idxs
                          if tokens[idx] not in tokens_to_ignore]
    # If the number of tokens considered for flipping is larger than
    # MAX_FLIPPABLE_TOKENS we only consider the top tokens.
    token_idxs_to_flip = token_idxs_to_flip[:MAX_FLIPPABLE_TOKENS]
    for i in range(min(len(token_idxs_to_flip), max_flips)):
      for s in itertools.combinations(token_idxs_to_flip, i+1):
        yield s
  def _flip_tokens(self, tokens: list[str], token_idxs: tuple[int, ...],
                   replacement_tokens: list[str]) -> list[str]:
    """Perturbs tokens at the indices specified in 'token_idxs'."""
    modified_tokens = [replacement_tokens[j] if j in token_idxs else t
                       for j, t in enumerate(tokens)]
    return modified_tokens
  def _create_cf(self, example: JsonDict, token_field: str, text_field: str,
                 tokens: list[str], token_idxs: tuple[int, ...],
                 replacement_tokens: list[str]) -> Mapping[str, Any]:
    cf = dict(example)
    modified_tokens = self._flip_tokens(
        tokens, token_idxs, replacement_tokens)
    # TODO(iftenney, bastings): call a model-provided detokenizer here?
    # Though in general tokenization isn't invertible and it's possible for
    # HotFlip to produce wordpiece sequences that don't correspond to any
    # input string.
    cf = utils.make_modified_input(
        cf,
        {token_field: modified_tokens, text_field: " ".join(modified_tokens)},
        "HOTFLIP"
    )
    return cf
  def _get_replacement_tokens(self,
                              embedding_matrix: np.ndarray,
                              inv_vocab: list[str],
                              token_grads: np.ndarray,
                              direction: int = -1) -> list[str]:
    """Identifies replacement tokens for each token position."""
    token_grads = token_grads * direction
    # Compute dot product of each input token gradient with the embedding
    # matrix, and pick the argmin.
    # TODO(ataly): Only consider tokens that have the same part-of-speech
    # tag as the original token and/or a certain cosine similarity with the
    # original token.
    replacement_token_ids = np.argmax(
        (np.expand_dims(embedding_matrix, 1) @ token_grads.T).squeeze(1),
        axis=0)
    replacement_tokens = [inv_vocab[id] for id in replacement_token_ids]
    return replacement_tokens
  def generate(self,
               example: JsonDict,
               model: lit_model.Model,
               dataset: lit_dataset.Dataset,
               config: Optional[JsonDict] = None) -> list[JsonDict]:
    """Identify minimal sets of token flips that alter the prediction."""
    del dataset  # Unused.
    config = config or {}
    num_examples = int(config.get(NUM_EXAMPLES_KEY, NUM_EXAMPLES_DEFAULT))
    max_flips = int(config.get(MAX_FLIPS_KEY, MAX_FLIPS_DEFAULT))
    tokens_to_ignore = config.get(TOKENS_TO_IGNORE_KEY,
                                  TOKENS_TO_IGNORE_DEFAULT)
    pred_key = config.get(PREDICTION_KEY, "")
    regression_thresh = float(config.get(REGRESSION_THRESH_KEY,
                                         REGRESSION_THRESH_DEFAULT))
    assert model is not None, "Please provide a model for this generator."
    input_spec = model.input_spec()
    output_spec = model.output_spec()
    assert pred_key, "Please provide the prediction key"
    assert pred_key in output_spec, "Invalid prediction key"
    is_regression = False
    if isinstance(output_spec[pred_key], types.RegressionScore):
      is_regression = True
    else:
      assert isinstance(output_spec[pred_key], types.MulticlassPreds), (
          "Only classification or regression models are supported")
    logging.info(r"W3lc0m3 t0 H0tFl1p \o/")
    logging.info("Original example: %r", example)
    # Get model outputs.
    orig_output = list(model.predict([example]))[0]
    # Check config for selected fields.
    selected_fields = list(config.get(FIELDS_TO_HOTFLIP_KEY, []))
    if not selected_fields:
      return []
    # Get tokens (corresponding to each text input field) and corresponding
    # gradients.
    tokens_and_gradients = self._get_tokens_and_gradients(
        input_spec, output_spec, orig_output, selected_fields)
    assert tokens_and_gradients, (
        "No token fields found. Cannot use HotFlip. :-(")
    # Copy tokens into input example.
    example = dict(example)
    for token_field, v in tokens_and_gradients.items():
      tokens, _ = v
      example[token_field] = tokens
    inv_vocab, embedding_matrix = model.get_embedding_table()
    assert len(inv_vocab) == embedding_matrix.shape[0], (
        "Vocab/embeddings size mismatch.")
    successful_cfs = []
    # TODO(lit-team): use only 1 sequence as input (configurable in UI).
    # TODO(lit-team): Refactor the following code so that it's not so deeply
    # nested (and easier to track loop state).
    for token_field, v in tokens_and_gradients.items():
      tokens, grads = v
      text_field = input_spec[token_field].parent  # pytype: disable=attribute-error
      logging.info("Identifying Hotflips for input field: %s", str(text_field))
      direction = -1
      if is_regression:
        # We want the replacements to increase the prediction score if the
        # original score is below the threshold, and decrease otherwise.
        direction = (1 if orig_output[pred_key] <= regression_thresh else -1)
      replacement_tokens = self._get_replacement_tokens(
          embedding_matrix, inv_vocab, grads, direction)
      successful_positions = []
      for token_idxs in self._gen_token_idxs_to_flip(
          tokens, grads, max_flips, tokens_to_ignore):
        if len(successful_cfs) >= num_examples:
          return successful_cfs
        # If a subset of the set of tokens have already been successful in
        # obtaining a flip, we continue. This ensures that we only consider
        # sets of token flips that are minimal.
        if self._subset_exists(set(token_idxs), successful_positions):
          continue
        # Create counterfactual.
        cf = self._create_cf(example, token_field, text_field, tokens,
                             token_idxs, replacement_tokens)
        # Obtain model prediction.
        cf_output = list(model.predict([cf]))[0]
        if cf_utils.is_prediction_flip(
            cf_output, orig_output, output_spec, pred_key, regression_thresh):
          # Prediciton flip found!
          cf_utils.update_prediction(cf, cf_output, output_spec, pred_key)
          successful_cfs.append(cf)
          successful_positions.append(set(token_idxs))
    return successful_cfs

================
File: lit_nlp/components/image_gradient_maps_test.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Tests for lit_nlp.components.gradient_maps."""
from absl.testing import absltest
from lit_nlp.api import dataset as lit_dataset
from lit_nlp.api import model as lit_model
from lit_nlp.api import types as lit_types
from lit_nlp.components import image_gradient_maps
from lit_nlp.lib import image_utils
import numpy as np
from PIL import Image as PILImage
JsonDict = lit_types.JsonDict
class ClassificationTestModel(lit_model.BatchedModel):
  LABELS = ['Dummy', 'Cat', 'Dog']
  GRADIENT_SHAPE = (60, 40, 3)
  def max_minibatch_size(self) -> int:
    return 10
  def predict_minibatch(self, inputs: list[JsonDict]) -> list[JsonDict]:
    result = []
    for i, _ in enumerate(inputs):
      result.append({
          'preds': np.random.rand(len(self.LABELS)),
          'grads': np.random.rand(*self.GRADIENT_SHAPE),
          'grad_target': self.LABELS[i],
      })
    return result
  def input_spec(self):
    return {
        'image':
            lit_types.ImageBytes(),
        'grad_target':
            lit_types.CategoryLabel(vocab=self.LABELS, required=False)
    }
  def output_spec(self):
    return {
        'preds':
            lit_types.MulticlassPreds(vocab=self.LABELS),
        'grads':
            lit_types.ImageGradients(
                align='image', grad_target_field_key='grad_target'),
    }
class RegressionTestModel(lit_model.BatchedModel):
  """A test model for testing the regression case."""
  GRADIENT_SHAPE = (40, 20, 3)
  def max_minibatch_size(self) -> int:
    return 10
  def predict_minibatch(self, inputs: list[JsonDict]) -> list[JsonDict]:
    """Simulates regression of x1 + 2 * x2 using elements of the image array."""
    result = []
    for example in inputs:
      img = example['image']
      if isinstance(img, str):
        img = image_utils.convert_image_str_to_array(
            img, shape=self.GRADIENT_SHAPE)
      pred = img[0, 0, 0] + img[1, 0, 0] * 2
      grad = np.zeros(shape=self.GRADIENT_SHAPE)
      grad[0, 0, 0] = 1
      grad[1, 0, 0] = 2
      result.append({'pred': pred, 'grads': grad})
    return result
  def input_spec(self):
    return {
        'image': lit_types.ImageBytes(),
    }
  def output_spec(self):
    return {
        'pred': lit_types.RegressionScore(),
        'grads': lit_types.ImageGradients(align='image'),
    }
class ImageGradientsMapsTest(absltest.TestCase):
  def test_interpreter(self):
    interpreter = image_gradient_maps.VanillaGradients()
    model = ClassificationTestModel()
    self.assertTrue(interpreter.is_compatible(
        model, lit_dataset.NoneDataset({'test': model})))
    pil_image = PILImage.new(mode='RGB', size=(300, 200))
    inp = {'image': image_utils.convert_pil_to_image_str(pil_image)}
    run_output = interpreter.run(
        inputs=[inp], model=model, dataset=lit_dataset.Dataset())[0]
    self.assertIn('grads', run_output)
    self.assertIsInstance(run_output['grads'], str)
  def test_regression(self):
    interpreter = image_gradient_maps.GuidedIG()
    model = RegressionTestModel()
    self.assertTrue(interpreter.is_compatible(
        model, lit_dataset.NoneDataset({'test': model})))
    input_image_array = np.zeros(shape=[20, 15, 3], dtype=np.uint8)
    input_image_array[0, 0, 0] = 10
    input_image_array[1, 0, 0] = 20
    pil_image = PILImage.fromarray(input_image_array, mode='RGB')
    inp = {'image': image_utils.convert_pil_to_image_str(pil_image)}
    run_output = interpreter.run(
        inputs=[inp], model=model, dataset=lit_dataset.Dataset())[0]
    self.assertIn('grads', run_output)
    overlay_str = run_output['grads']
    overlay_bytes = image_utils.convert_image_str_to_array(
        overlay_str, shape=RegressionTestModel.GRADIENT_SHAPE)
    self.assertIsNotNone(overlay_bytes)
    self.assertSequenceEqual(overlay_bytes.shape,
                             RegressionTestModel.GRADIENT_SHAPE)
if __name__ == '__main__':
  absltest.main()

================
File: lit_nlp/components/image_gradient_maps.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""A collection of gradient based saliency interpreters for images.
This module implements interpreters that use the pair-code saliency library to
generate gradient based saliency maps.
"""
import abc
from collections.abc import Callable
from typing import Any, NamedTuple, Optional, cast
from absl import logging
from lit_nlp.api import components as lit_components
from lit_nlp.api import dataset as lit_dataset
from lit_nlp.api import model as lit_model
from lit_nlp.api import types
from lit_nlp.lib import image_utils
from lit_nlp.lib import utils as lit_utils
import numpy as np
import saliency
JsonDict = types.JsonDict
Spec = types.Spec
TARGET_INFO_KEY = '_salience_target'
# The key name for a configuration parameter that specifies the number
# of steps to use for integrated gradients approximation.
INTERPOLATION_KEY = 'Interpolation steps'
# The key name for a configuration parameter that specifies the maximum distance
# that the Guided IG path can deviate from the straight line.
MAX_DIST_KEY = 'Max. distance'
# Visualization constants.
# The color map to use for signed saliency methods such as Integrated
# Gradients.
DIVERGING_COLOR_MAP = 'bwr'
# The color map to use for unsigned saliency methods such as XRAI.
SEQUENTIAL_COLOR_MAP = 'summer'
# The amount of clipping to apply for saliency visualization. E.g., the value of
# 0.01 means that the top 1% of saliency values are clipped to match the 99%
# percentile.
CLIPPING_FRACTION = 0.01
# The value of alpha channel to apply to the saliency visualization layer that
# displays regions instead of pixels. The lower is the value the more visible is
# the original image underneath the saliency layer.
AREA_SALIENCY_ALPHA_MUL = 0.7
# The value of alpha channel to apply to the pixel-based saliency visualization
# layer that  The lower is the value the more visible is the original image
# underneath the saliency layer.
PIXEL_SALIENCY_ALPHA_MUL = 1.0
# Default value of IG steps.
IG_STEPS = 10
_SUPPORTED_PRED_TYPES = (types.MulticlassPreds, types.RegressionScore)
class SupportedFields(NamedTuple):
  """The collection of field names that are required to calculate saliency."""
  grad_field_key: str
  image_field_key: str
  grad_target_field_key: str
  preds_field_key: str
def find_supported_fields(input_spec: Spec,
                          output_spec: Spec) -> Optional[SupportedFields]:
  """Returns fields from the model specs that are needed for saliency ."""
  # Find all ImageGradients fields.
  grad_field_keys = lit_utils.find_spec_keys(output_spec, types.ImageGradients)
  # Models with more than one gradient field are not supported.
  if not grad_field_keys or len(grad_field_keys) != 1:
    logging.warning('Models must have exactly 1 ImageGradients field, found %i',
                    len(grad_field_keys))
    return None
  grad_field_key = grad_field_keys[0]
  grad_field_value = cast(types.ImageGradients, output_spec[grad_field_key])
  # Find image fields that correspond to grad_field.
  image_field_key = grad_field_value.align
  if not isinstance(input_spec.get(image_field_key), types.ImageBytes):
    logging.warning(
        'Could not find aligned ImageBytes field, %s, in input spec',
        str(grad_field_value.align))
    return None
  # Find gradient target fields in the input if it is a multiclass
  # classification model. The value of None means that it is a regression or
  # single class classification model.
  multiclass = grad_field_value.grad_target_field_key is not None
  if multiclass:
    grad_target_field_key = grad_field_value.grad_target_field_key
    if not isinstance(
        input_spec.get(grad_target_field_key), types.CategoryLabel):
      logging.warning(
          'Could not find compatible CategoryLabel field, %s, in input spec',
          str(grad_target_field_key))
      return None
  else:
    grad_target_field_key = None
  # Find prediction field names.
  preds_field_keys = lit_utils.find_spec_keys(
      output_spec, _SUPPORTED_PRED_TYPES
  )
  # Models with more than one prediction field are not supported.
  if not preds_field_keys or len(preds_field_keys) != 1:
    logging.warning('Models must have exactly 1 predicition field, found %i',
                    len(preds_field_keys))
    return None
  preds_field_key = preds_field_keys[0]
  return SupportedFields(
      grad_field_key=grad_field_key,
      image_field_key=image_field_key,
      grad_target_field_key=grad_target_field_key,
      preds_field_key=preds_field_key)
CallModelFunction = Callable[
    [np.ndarray, Any, list[str]], dict[str, np.ndarray]
]
def get_call_model_func(
    model: lit_model.Model,
    model_input: JsonDict,
    image_field_key: str,
    grad_field_key: str,
    grad_target_field_key: str,
    grad_target_label: str,
) -> CallModelFunction:
  """Returns a function that is used by the Saliency library to get gradients.
  Args:
    model: LIT model that is used to calculate actual gradients.
    model_input: the model input.
    image_field_key: the name (key) of the field in the model input that
      contains the image data with respect to which the gradients should be
      calculated.
    grad_field_key: the name (key) of the field in the model output that
      contains the computed gradients.
    grad_target_field_key: the name (key) of the field in the model input that
      is used to specify the label for which the gradients should be calculated.
      If the value is None then it is a regression or a single class
      classification model that has only one output.
    grad_target_label: the value of the label that should be passed as the
      `grad_target_field_name` value.
  Returns:
    The function that should be passed to the Saliency library.
  """
  def call_model_func(
      x_value_batch: np.ndarray, call_model_args, expected_keys: list[str]
  ) -> dict[str, np.ndarray]:
    """This function is called by the Saliency library to calculate gradients.
    Args:
      x_value_batch: a batch of inputs with respect to which the gradients
        should be calculated.
      call_model_args: unused.
      expected_keys: the list of expected keys that the return value should
        contain.
    Returns:
      A dictionary with gradients values for the batch.
    """
    del call_model_args  # Unused.
    # Iterate through the batch of saliency lib inputs and convert them to
    # a batch acceptable by the LIT model.
    model_inputs = []
    for x_value in x_value_batch:
      updates = {image_field_key: x_value}
      if grad_target_field_key is not None:
        updates[grad_target_field_key] = grad_target_label
      input_copy = lit_utils.make_modified_input(
          model_input, updates, 'ImageSalience'
      )
      model_inputs.append(input_copy)
    # Call the model to obtain gradients.
    predictions = model.predict(inputs=model_inputs)
    # Gradient results formatted for the saliency library.
    gradients_batch = [p[grad_field_key] for p in predictions]
    assert saliency.core.base.OUTPUT_LAYER_VALUES not in expected_keys
    # Convert the result to the format acceptable by the saliency library.
    return {
        saliency.core.base.INPUT_OUTPUT_GRADIENTS: np.asarray(gradients_batch),
    }
  return call_model_func
class SaliencyLibInterpreter(lit_components.Interpreter, metaclass=abc.ABCMeta):
  """A base class for all interpreters that use the Saliency library."""
  def run(
      self,
      inputs: list[JsonDict],
      model: lit_model.Model,
      dataset: lit_dataset.Dataset,
      model_outputs: Optional[list[JsonDict]] = None,
      config: Optional[JsonDict] = None,
  ) -> Optional[list[JsonDict]]:
    """Runs the component, given a model and input(s)."""
    input_spec = model.input_spec()
    output_spec = model.output_spec()
    config = config or {}
    if not inputs:
      return []
    # Find all fields required for the interpretation.
    supported_fields = find_supported_fields(input_spec, output_spec)
    if supported_fields is None:
      return None
    grad_field_key = supported_fields.grad_field_key
    image_field_key = supported_fields.image_field_key
    grad_target_field_key = supported_fields.grad_target_field_key
    if target_config := config.get(TARGET_INFO_KEY):
      preds_field_key = target_config['field']
    else:
      preds_field_key = supported_fields.preds_field_key
    preds_field_spec = output_spec[preds_field_key]
    if not isinstance(preds_field_spec, _SUPPORTED_PRED_TYPES):
      logging.warning(
          "Image Salience is not compatible with field '%s'", preds_field_key
      )
      return None
    # Determine the shape of gradients by calling the model with a single input
    # and extracting the shape from the gradient output.
    first_example_preds = list(model.predict([inputs[0]]))[0]
    grad_shape = first_example_preds[grad_field_key].shape
    # If it is a multiclass model, find the labels with respect to which we
    # should compute the gradients.
    if isinstance(preds_field_spec, types.MulticlassPreds):
      # Get class labels.
      label_vocab = list(preds_field_spec.vocab)
      if (target_config := config.get(TARGET_INFO_KEY)) and (
          target_class := target_config.get('index')
      ):
        grad_target_labels = [label_vocab[target_class] for _ in inputs]
      else:
        # Run the model in order to find the gradient target labels.
        outputs = list(model.predict(inputs))
        grad_target_labels = []
        for model_input, model_output in zip(inputs, outputs):
          if model_input.get(grad_target_field_key) is not None:
            grad_target_labels.append(model_input[grad_target_field_key])
          else:
            max_idx = int(np.argmax(model_output[preds_field_key]))
            grad_target_labels.append(label_vocab[max_idx])
    else:  # RegressionScore
      grad_target_labels = [None] * len(inputs)
    saliency_object = self.get_saliency_object()  # get this on class init?
    extra_saliency_params = self.get_extra_saliency_params(config)
    all_results = []
    for example, grad_target_label in zip(inputs, grad_target_labels):
      result = {}
      image_str = example[image_field_key]
      saliency_input = image_utils.convert_image_str_to_array(
          image_str=image_str, shape=grad_shape)
      call_model_func = get_call_model_func(
          model=model,
          model_input=example,
          image_field_key=image_field_key,
          grad_field_key=grad_field_key,
          grad_target_field_key=grad_target_field_key,
          grad_target_label=grad_target_label)
      attribution = self.make_saliency_call(
          saliency_object=saliency_object,
          x_value=saliency_input,
          call_model_function=call_model_func,
          extra_saliency_params=extra_saliency_params)
      if attribution.ndim == 3:
        attribution = attribution.sum(axis=2)
      viz_params = self.get_visualization_params()
      overlaid_image = image_utils.overlay_pixel_saliency(
          image_str, attribution, **viz_params)
      result[grad_field_key] = image_utils.convert_pil_to_image_str(
          overlaid_image)
      all_results.append(result)
    return all_results
  def is_compatible(self, model: lit_model.Model,
                    dataset: lit_dataset.Dataset) -> bool:
    del dataset  # Unused as salience comes from the model.
    fields = find_supported_fields(model.input_spec(), model.output_spec())
    return fields is not None
  def config_spec(self) -> types.Spec:
    return {'_salience_target': types.SalienceTargetInfo()}
  def meta_spec(self) -> types.Spec:
    return {'saliency': types.ImageSalience(autorun=False)}
  def make_saliency_call(
      self,
      saliency_object: saliency.core.CoreSaliency,
      x_value: np.ndarray,
      call_model_function: CallModelFunction,
      extra_saliency_params: dict[str, Any],
  ) -> np.ndarray:
    return saliency_object.GetMask(
        x_value=x_value,
        call_model_function=call_model_function,
        **extra_saliency_params)
  @abc.abstractmethod
  def get_saliency_object(self):
    """Returns a saliency library instance to be used for the explanation."""
    pass
  @abc.abstractmethod
  def get_extra_saliency_params(self, config: JsonDict) -> dict[str, Any]:
    """Returns extra parameters to be passed to the GetMask() method."""
    pass
  @abc.abstractmethod
  def get_visualization_params(self) -> dict[str, Any]:
    """Returns visualization parameters."""
    pass
class VanillaGradients(SaliencyLibInterpreter):
  """Vanilla gradients interpreter."""
  def get_saliency_object(self) -> saliency.core.CoreSaliency:
    return saliency.core.GradientSaliency()
  def get_extra_saliency_params(self, unused_config) -> dict[str, Any]:
    return {}
  def get_visualization_params(self) -> dict[str, Any]:
    return {
        'cm_name': DIVERGING_COLOR_MAP,
        'clip_fraction': CLIPPING_FRACTION,
        'alpha_mul': PIXEL_SALIENCY_ALPHA_MUL,
        'signed': True,
        'pixel_saliency': True
    }
class IntegratedGradients(SaliencyLibInterpreter):
  """Integrated Gradients interpreter."""
  def get_saliency_object(self) -> saliency.core.CoreSaliency:
    return saliency.core.IntegratedGradients()
  def get_extra_saliency_params(self, config: JsonDict) -> dict[str, Any]:
    return {'x_steps': int(config.get(INTERPOLATION_KEY, IG_STEPS))}
  def config_spec(self) -> types.Spec:
    return super().config_spec() | {
        INTERPOLATION_KEY: types.Scalar(
            min_val=5, max_val=200, default=IG_STEPS, step=1, required=False
        )
    }
  def get_visualization_params(self) -> dict[str, Any]:
    return {
        'cm_name': DIVERGING_COLOR_MAP,
        'clip_fraction': CLIPPING_FRACTION,
        'alpha_mul': PIXEL_SALIENCY_ALPHA_MUL,
        'signed': True,
        'pixel_saliency': True
    }
class BlurIG(SaliencyLibInterpreter):
  """Blur IG interpreter."""
  def get_saliency_object(self) -> saliency.core.CoreSaliency:
    return saliency.core.BlurIG()
  def get_extra_saliency_params(self, config: JsonDict) -> dict[str, Any]:
    return {'steps': int(config.get(INTERPOLATION_KEY, IG_STEPS))}
  def config_spec(self) -> types.Spec:
    return super().config_spec() | {
        INTERPOLATION_KEY: types.Scalar(
            min_val=5, max_val=200, default=IG_STEPS, step=1, required=False
        )
    }
  def get_visualization_params(self) -> dict[str, Any]:
    return {
        'cm_name': DIVERGING_COLOR_MAP,
        'clip_fraction': CLIPPING_FRACTION,
        'alpha_mul': PIXEL_SALIENCY_ALPHA_MUL,
        'signed': True,
        'pixel_saliency': True
    }
class GuidedIG(SaliencyLibInterpreter):
  """Guided Integrated Gradients interpreter."""
  def get_saliency_object(self) -> saliency.core.CoreSaliency:
    return saliency.core.GuidedIG()
  def get_extra_saliency_params(self, config: JsonDict) -> dict[str, Any]:
    return {
        'x_steps': int(config.get(INTERPOLATION_KEY, IG_STEPS)),
        'max_dist': float(config.get(MAX_DIST_KEY, 0.1)),
        'fraction': 0.25,
    }
  def config_spec(self) -> types.Spec:
    return super().config_spec() | {
        INTERPOLATION_KEY: types.Scalar(
            min_val=5, max_val=200, default=IG_STEPS, step=1, required=False
        ),
        MAX_DIST_KEY: types.Scalar(
            min_val=0.0, max_val=1.0, default=0.1, step=0.02, required=False
        ),
    }
  def get_visualization_params(self) -> dict[str, Any]:
    return {
        'cm_name': DIVERGING_COLOR_MAP,
        'clip_fraction': CLIPPING_FRACTION,
        'alpha_mul': PIXEL_SALIENCY_ALPHA_MUL,
        'signed': True,
        'pixel_saliency': True
    }
class XRAI(SaliencyLibInterpreter):
  """XRAI Interpreter."""
  def get_saliency_object(self) -> saliency.core.CoreSaliency:
    return saliency.core.XRAI()
  def get_extra_saliency_params(self, config: JsonDict) -> dict[str, Any]:
    return {'steps': int(config.get(INTERPOLATION_KEY, IG_STEPS))}
  def config_spec(self) -> types.Spec:
    return super().config_spec() | {
        INTERPOLATION_KEY: types.Scalar(
            min_val=5, max_val=200, default=IG_STEPS, step=1, required=False
        )
    }
  def make_saliency_call(
      self,
      saliency_object: saliency.core.CoreSaliency,
      x_value: np.ndarray,
      call_model_function: CallModelFunction,
      extra_saliency_params: dict[str, Any],
  ) -> np.ndarray:
    xrai_params = saliency.core.XRAIParameters(
        steps=extra_saliency_params['steps'], algorithm='fast'
    )
    xrai_output = cast(saliency.core.XRAI, saliency_object).GetMaskWithDetails(
        x_value=x_value,
        call_model_function=call_model_function,
        extra_parameters=xrai_params)
    return xrai_output.attribution_mask
  def get_visualization_params(self) -> dict[str, Any]:
    return {
        'cm_name': SEQUENTIAL_COLOR_MAP,
        'clip_fraction': CLIPPING_FRACTION,
        'alpha_mul': AREA_SALIENCY_ALPHA_MUL,
        'signed': False,
        'pixel_saliency': False
    }
class XRAIGIG(SaliencyLibInterpreter):
  """XRAI Interpreter that uses Guided IG as the base attribution."""
  def get_saliency_object(self) -> saliency.core.CoreSaliency:
    return saliency.core.XRAI()
  def get_extra_saliency_params(self, config: JsonDict) -> dict[str, Any]:
    return {
        'x_steps': int(config.get(INTERPOLATION_KEY, IG_STEPS)),
        'max_dist': float(config.get(MAX_DIST_KEY, 0.1)),
        'fraction': 0.25,
    }
  def config_spec(self) -> types.Spec:
    return super().config_spec() | {
        INTERPOLATION_KEY: types.Scalar(
            min_val=5, max_val=200, default=IG_STEPS, step=1, required=False
        ),
        MAX_DIST_KEY: types.Scalar(
            min_val=0.0, max_val=1.0, default=0.1, step=0.02, required=False
        ),
    }
  def make_saliency_call(
      self,
      saliency_object: saliency.core.CoreSaliency,
      x_value: np.ndarray,
      call_model_function: CallModelFunction,
      extra_saliency_params: dict[str, Any],
  ) -> np.ndarray:
    gig_object = saliency.core.GuidedIG()
    gig_saliency = gig_object.GetMask(
        x_value=x_value,
        call_model_function=call_model_function,
        **extra_saliency_params)
    xrai_params = saliency.core.XRAIParameters(
        steps=extra_saliency_params['x_steps'], algorithm='fast')
    xrai_output = cast(saliency.core.XRAI, saliency_object).GetMaskWithDetails(
        x_value=x_value,
        call_model_function=call_model_function,
        base_attribution=gig_saliency,
        extra_parameters=xrai_params)
    return xrai_output.attribution_mask
  def get_visualization_params(self) -> dict[str, Any]:
    return {
        'cm_name': SEQUENTIAL_COLOR_MAP,
        'clip_fraction': CLIPPING_FRACTION,
        'alpha_mul': AREA_SALIENCY_ALPHA_MUL,
        'signed': False,
        'pixel_saliency': False
    }
def all_interpreters():
  return {
      'Grad': VanillaGradients(),
      'Integrated Gradients': IntegratedGradients(),
      'Blur IG': BlurIG(),
      'Guided IG': GuidedIG(),
      'XRAI': XRAI(),
      'XRAI GIG': XRAIGIG(),
  }

================
File: lit_nlp/components/index.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Indexer class for fast nearest neighbor lookups."""
import collections
from collections.abc import Mapping
import os
# TODO(b/151080311): don't use pickle for this.
import pickle
from typing import Optional
from absl import logging
import annoy
from lit_nlp.api import dataset as lit_data
from lit_nlp.api import model as lit_model
from lit_nlp.api import types as lit_types
from lit_nlp.lib import utils
class Indexer(object):
  """Class to build and search annoy indices.
  TODO(b/162421415): Split the managing of indices and the index logic.
  Upon instantiation of this class, it will first create empty indices for
  every model/embedding/dataset triple. It will then search for existing indices
  in the data directory and fill those. If the flag `initialize_new_indices` is
  set, it will fill the remaining indices by iterating over the data.
  During the saving process, both the index and a mapping from index-row to
  example are saved. These are used during the nearest neighbor lookup to return
  the closest example.
  Attributes:
     models: specification akin to the LIT server.
     datasets: specification akin to the LIT server.
     data_dir: path for (de-)serialization of indices.
     initialize_new_indices: whether to build new indices or simply load
       existing ones.
  """
  def __init__(
      self,
      models: Mapping[str, lit_model.Model],
      datasets: Mapping[str, lit_data.IndexedDataset],
      data_dir: Optional[str],
      initialize_new_indices: Optional[bool] = False,
  ):
    self.datasets = datasets
    self._indices = collections.OrderedDict()
    self._example_lookup = collections.defaultdict(dict)
    # Indicator whether to build new indices. If False, only load existing ones.
    self._initialize_new_indices = initialize_new_indices
    # Ensure directory to save indices exists.
    if not os.path.isdir(data_dir):
      os.mkdir(data_dir)
    self._data_dir = data_dir
    self._models = models
    # Create/Load indices.
    for model_name, model in self._models.items():
      compatible_datasets = [
          dataset_name for dataset_name, dataset in self.datasets.items()
          if model.is_compatible_with_dataset(dataset)
      ]
      for dataset in compatible_datasets:
        self._create_empty_indices(model_name, dataset)
        self._fill_indices(model_name, dataset)
    # Update cache with all indices.
    self._save_lookups()
  def _get_index_key(self, model_name, dataset_name, embedding_name):
    """Returns the key of an index, added to avoid collisions."""
    index_key = f"{dataset_name}:{model_name}:{embedding_name}"
    return index_key
  def _get_lookup_key(self, model_name, dataset_name):
    """Returns the key of a text lookup table."""
    lookup_key = f"{dataset_name}:{model_name}"
    return lookup_key
  def _get_index_path(self, index_key):
    """Get the file path for an index."""
    file_path = os.path.join(self._data_dir, f"{index_key}.ann")
    return file_path
  def _get_lookup_path(self, lookup_key):
    """Get the file path for the lookup index."""
    file_path = os.path.join(self._data_dir, lookup_key + ".pkl")
    return file_path
  def _create_empty_indices(self, model_name, dataset_name):
    """Create the empty indices for a model and dataset."""
    model = self._models[model_name]
    examples = self.datasets[dataset_name].indexed_examples
    model_embeddings_names = utils.find_spec_keys(model.output_spec(),
                                                  lit_types.Embeddings)
    if not model_embeddings_names:
      return
    # To first create an index, we need to know the shapes - peek at first ex.
    peeked_example = list(model.predict([examples[0]["data"]]))[0]
    for emb_name in model_embeddings_names:
      index_key = self._get_index_key(model_name, dataset_name, emb_name)
      emb_dimension = len(peeked_example[emb_name])
      assert self._indices.get(index_key) is None, "Index already exists."
      self._indices[index_key] = annoy.AnnoyIndex(emb_dimension, "euclidean")
  def _load_lookup(self, lookup_key):
    """Loads a lookup table from index to data example."""
    lookup_path = self._get_lookup_path(lookup_key)
    if not os.path.exists(lookup_path):
      return {}
    with open(lookup_path, "rb") as f:
      return pickle.load(f)
  def _fill_indices(self, model_name, dataset_name):
    """Create all indices for a single model."""
    model = self._models.get(model_name)
    assert model is not None, "Invalid model name."
    examples = self.datasets[dataset_name].examples
    model_embeddings_names = utils.find_spec_keys(model.output_spec(),
                                                  lit_types.Embeddings)
    lookup_key = self._get_lookup_key(model_name, dataset_name)
    # If the model has no embeddings to extract, skip the following.
    if not model_embeddings_names:
      return
    # Load from file if it exists.
    for emb_name in model_embeddings_names:
      # Initialize the index object in self._indices with serialized index.
      self._init_index_from_file(model_name, dataset_name, emb_name)
    # Load example lookup dictionary from file.
    self._example_lookup[lookup_key] = self._load_lookup(lookup_key)
    # Identify which indices need to be initialized.
    embeddings_to_index = [
        emb_name for emb_name in model_embeddings_names
        if not self._is_index_initialized(model_name, dataset_name, emb_name)
    ]
    # Early exit if all embeddings are now initialized.
    if not embeddings_to_index:
      return
    # Cold start: Get embeddings for non-initialized settings.
    if self._initialize_new_indices:
      for res_ix, (result, example) in enumerate(
          zip(model.predict(examples), examples)):
        for emb_name in embeddings_to_index:
          index_key = self._get_index_key(model_name, dataset_name, emb_name)
          # Initialize saving in the first iteration.
          if res_ix == 0:
            file_path = self._get_index_path(index_key)
            self._indices[index_key].on_disk_build(file_path)
          index = self._indices.get(index_key)
          assert index is not None, "Index needs to be created first."
          # Each item has an incrementing ID res_ix.
          self._indices[index_key].add_item(res_ix, result[emb_name])
        # Add item to lookup table.
        self._example_lookup[lookup_key][res_ix] = example
      # Create the trees from the indices - using 10 as recommended by doc.
      for emb_name in embeddings_to_index:
        index_key = self._get_index_key(model_name, dataset_name, emb_name)
        logging.info("Creating new index: %s", index_key)
        self._indices[index_key].build(10)
        index_size = self._indices[index_key].get_n_items()
        logging.info("Created new index with %s items.", index_size)
  def _is_index_initialized(self, model_name, dataset_name, emb_name):
    """Checks if an index is already initialized (num trees > 0)."""
    index_key = self._get_index_key(model_name, dataset_name, emb_name)
    return self._indices[index_key].get_n_trees() > 0
  def _init_index_from_file(self, model_name, dataset_name, emb_name):
    """If possible, load indices from file for a model/data combination."""
    index_key = self._get_index_key(model_name, dataset_name, emb_name)
    index_path = self._get_index_path(index_key)
    if os.path.exists(index_path):
      logging.info("Loading from cache: %s", index_path)
      self._indices[index_key].load(index_path)
      index_size = self._indices[index_key].get_n_items()
      logging.info("Loaded index with %s items.", index_size)
  def _save_lookups(self):
    """Iterate over indices and lookup tables and save them."""
    # Save the lookup tables.
    for lookup_key, lookup_table in self._example_lookup.items():
      file_path = self._get_lookup_path(lookup_key)
      with open(file_path, "wb") as f:
        pickle.dump(lookup_table, f, pickle.HIGHEST_PROTOCOL)
  def find_nn(
      self,
      model_name: str,
      dataset_name: str,
      embedding_name: str,
      embedding: list[float],
      num_neighbors: Optional[int] = 25,
  ):
    """Find the nearest neighbor in index for an embedding.
    This function implements the search API for this class.
    The model/dataset/embedding combination maps to a unique index. Within, we
    are looking up the num_neighbors nearest neighbors of the embedding arg.
    Args:
      model_name: The identifier of the model.
      dataset_name: The identifier of the dataset.
      embedding_name: The identifier of the embedding within the model spec.
      embedding: The embedding we aim to look up.
      num_neighbors: How many neighbors we should return.
    Returns:
      neighbors: A list[dict] with nearest neighbor examples.
    """
    index_key = self._get_index_key(model_name, dataset_name, embedding_name)
    index = self._indices.get(index_key)
    assert index is not None, f"No index found for {index_key}."
    # Query for the neighbors.
    neighbor_indices, distances = index.get_nns_by_vector(
        vector=embedding, n=num_neighbors, include_distances=True)
    # Convert neighbors to texts.
    lookup_key = self._get_lookup_key(model_name, dataset_name)
    neighbor_examples = [
        self._example_lookup[lookup_key][ix] for ix in neighbor_indices
    ]
    # TODO(lit-dev): make the distance metadata that can be returned.
    del distances
    return neighbor_examples

================
File: lit_nlp/components/lime_explainer_test.py
================
"""Tests for lime_explainer."""
from absl.testing import absltest
from absl.testing import parameterized
from lit_nlp.components import lime_explainer
CLASS_KEY = lime_explainer.CLASS_KEY
KERNEL_WIDTH_KEY = lime_explainer.KERNEL_WIDTH_KEY
MASK_KEY = lime_explainer.MASK_KEY
NUM_SAMPLES_KEY = lime_explainer.NUM_SAMPLES_KEY
SEED_KEY = lime_explainer.SEED_KEY
class LimeExplainerTest(parameterized.TestCase):
  @parameterized.named_parameters(
      ('default', None, None, None, None, None, None),
      ('autorun only', True, None, None, None, None, None),
      ('config values only', None, -1, 128, '<UNK>', 128, 0),
      ('autorun + config values', True, -1, 128, '<UNK>', 128, 0),
  )
  def test_lime_init_args(self, autorun, class_index, kernel_width, mask_token,
                          num_samples, seed):
    params = {}
    if autorun is not None:
      params['autorun'] = autorun
    if kernel_width is not None: params['kernel_width'] = kernel_width
    if mask_token is not None: params['mask_token'] = mask_token
    if num_samples is not None: params['num_samples'] = num_samples
    if seed is not None: params['seed'] = seed
    lime = lime_explainer.LIME(**params)
    config_spec = lime.config_spec()
    if autorun is not None:
      self.assertEqual(lime.meta_spec()['saliency'].autorun, autorun)
    if class_index is not None:
      self.assertEqual(config_spec[CLASS_KEY].default, str(class_index))
    if kernel_width is not None:
      self.assertEqual(config_spec[KERNEL_WIDTH_KEY].default, str(kernel_width))
    if mask_token is not None:
      self.assertEqual(config_spec[MASK_KEY].default, str(mask_token))
    if num_samples is not None:
      self.assertEqual(config_spec[NUM_SAMPLES_KEY].default, str(num_samples))
    if seed is not None:
      self.assertEqual(config_spec[SEED_KEY].default, str(seed))
if __name__ == '__main__':
  absltest.main()

================
File: lit_nlp/components/lime_explainer.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""LIME perturbation-based feature attribution for text sequences."""
from collections.abc import Iterable
import functools
from typing import Any, Optional
from absl import logging
from lit_nlp.api import components as lit_components
from lit_nlp.api import dataset as lit_dataset
from lit_nlp.api import dtypes
from lit_nlp.api import model as lit_model
from lit_nlp.api import types
from lit_nlp.components.citrus import lime
from lit_nlp.components.citrus import utils as citrus_util
from lit_nlp.lib import utils
import numpy as np
JsonDict = types.JsonDict
Spec = types.Spec
_SUPPORTED_PRED_TYPES = (types.MulticlassPreds, types.RegressionScore,
                         types.SparseMultilabelPreds)
TARGET_INFO_KEY = '_salience_target'
TARGET_HEAD_KEY = 'Output field to explain'  # TODO(b/205996131): remove
CLASS_KEY = 'Class index to explain'  # TODO(b/205996131): remove
KERNEL_WIDTH_KEY = 'Kernel width'
MASK_KEY = 'Mask'
NUM_SAMPLES_KEY = 'Number of samples'
SEED_KEY = 'Seed'
def _predict_fn(strings: Iterable[str], model: Any, original_example: JsonDict,
                text_key: str, pred_key: str, pred_type_info: types.LitType):
  """Given raw strings, return scores. Used by `lime.explain`.
  Adjust the `original_example` by changing the value of the field `text_key`
  by the values in `strings`, and run model prediction on each adjusted example,
  returning a list of scores, one entry per adjusted example.
  Args:
    strings: The adjusted strings to set in the original example.
    model: The model to run.
    original_example: The original example to adjust.
    text_key: The field in which to adjust the original example with the
      provided strings.
    pred_key: The key to the model's output field to explain.
    pred_type_info: The `LitType` value for the model's output field to explain.
  Returns:
    A list of scores for the model output on the adjusted examples. For
    regression tasks, a 1D list of values. For classification and multi-label,
    a 2D list, where each entry is a list of scores for each possible label,
    in order by the class index.
  """
  # Prepare example objects to be fed to the model for each sentence/string.
  model_inputs = [
      utils.make_modified_input(original_example, {text_key: s}, 'LIME')
      for s in strings
  ]
  # Get model predictions for the examples.
  model_outputs = model.predict(model_inputs)
  outputs = [output[pred_key] for output in model_outputs]
  if isinstance(pred_type_info, types.SparseMultilabelPreds):
    assert pred_type_info.vocab, (
        f'No vocab found for {pred_key} field. Cannot use LIME.')
    # Convert list of class/score tuples to a list of scores for each possible
    # class, in class index order.
    output_arr = np.zeros([len(outputs), len(pred_type_info.vocab)])
    for i, pred in enumerate(outputs):
      for p in pred:
        class_idx = pred_type_info.vocab.index(p[0])
        output_arr[i, class_idx] = p[1]
    outputs = output_arr
  else:
    # Make outputs 1D in case of regression or binary classification.
    # <float32>[len(strings)] or <float32>[len(strings), num_labels].
    outputs = np.array(outputs)
    if outputs.ndim == 2 and outputs.shape[1] == 1:
      outputs = np.squeeze(outputs, axis=1)
  return outputs
def get_class_to_explain(
    model: Any,
    pred_key: str,
    example: JsonDict,
    provided_class_to_explain: Optional[int] = None,
) -> Optional[int]:
  """Return the class index to explain.
  The provided class index can be None, in which case this method determines
  which class to explain based on the class with the highest prediction score
  for the provided example.
  Args:
    model: The model to run.
    pred_key: The key to the model's output field to explain.
    example: The example to explain.
    provided_class_to_explain: The class index provided to this explainer, or
      None to use the argmax prediction.
  Returns:
    The true class index to explain, in the range [0, class vocab length).
  """
  pred_spec = model.output_spec()[pred_key]
  # If provided_class_to_explain is -1, then explain the argmax class, for both
  # multiclass and sparse multilabel tasks.
  if provided_class_to_explain is None and isinstance(
      pred_spec, (types.MulticlassPreds, types.SparseMultilabelPreds)
  ):
    pred = list(model.predict([example]))[0][pred_key]
    if isinstance(pred_spec, types.MulticlassPreds):
      return np.argmax(pred)
    else:
      # For sparse multi-label, sort class/score tuples to find the
      # highest-scoring class and get its vocab index.
      pred.sort(key=lambda elem: elem[1], reverse=True)
      class_name_to_explain = pred[0][0]
      return pred_spec.vocab.index(class_name_to_explain)
  else:
    return provided_class_to_explain
class LIME(lit_components.Interpreter):
  """Local Interpretable Model-agnostic Explanations (LIME)."""
  def __init__(
      self,
      autorun: bool = False,
      kernel_width: int = 256,
      mask_token: str = '[MASK]',
      num_samples: int = 256,
      seed: Optional[int] = None,
  ):
    """Creates a LIME interpreter.
    Args:
      autorun: Determines if this intepreter should run automatically.
      kernel_width: Size of the kernel.
      mask_token: Mask token from the tokenizer
      num_samples: Number of samples to take.
      seed: A seed value for the random seed.
    """
    self._autorun: bool = autorun
    self._kernel_width: str = str(kernel_width)
    self._mask_token: str = mask_token
    self._num_samples: str = str(num_samples)
    self._seed: str = str(seed) if seed is not None else ''
  def run(
      self,
      inputs: list[JsonDict],
      model: lit_model.Model,
      dataset: lit_dataset.Dataset,
      model_outputs: Optional[list[JsonDict]] = None,
      config: Optional[JsonDict] = None,
  ) -> Optional[list[JsonDict]]:
    """Run this component, given a model and input(s)."""
    config_defaults = {k: v.default for k, v in self.config_spec().items()}
    config = dict(config_defaults, **(config or {}))  # update and return
    kernel_width = int(config[KERNEL_WIDTH_KEY])
    num_samples = int(config[NUM_SAMPLES_KEY])
    mask_string = (config[MASK_KEY])
    # pylint: disable=g-explicit-bool-comparison
    seed = int(config[SEED_KEY]) if config[SEED_KEY] != '' else None
    # pylint: enable=g-explicit-bool-comparison
    # Find keys of input (text) segments to explain.
    # Search in the input spec, since it's only useful to look at ones that are
    # used by the model.
    text_keys = utils.find_spec_keys(model.input_spec(), types.TextSegment)
    if not text_keys:
      logging.warning('LIME requires text inputs.')
      return None
    logging.info('Found text fields for LIME attribution: %s', str(text_keys))
    available_pred_keys = utils.find_spec_keys(
        model.output_spec(), _SUPPORTED_PRED_TYPES
    )
    if not available_pred_keys:
      logging.warning('LIME did not find any supported output fields.')
      return None
    if (field := config[TARGET_HEAD_KEY]) and (
        cls_idx := int(config[CLASS_KEY])
    ) != -1:
      # TODO(b/205996131): remove this case
      pred_key = field
      provided_class_to_explain = cls_idx
    elif target_config := config.get(TARGET_INFO_KEY):
      pred_key = target_config['field']
      if pred_key not in available_pred_keys:
        logging.warning("LIME is not compatible with field '%s'", pred_key)
        return None
      # May be None, if there's no label vocab.
      provided_class_to_explain = target_config.get('index')
    else:
      pred_key = available_pred_keys[0]
      provided_class_to_explain = None  # use model prediction
    pred_type_info = model.output_spec()[pred_key]
    all_results = []
    # Explain each input.
    for example in inputs:
      # dict[field name -> interpretations]
      result = {}
      predict_fn = functools.partial(
          _predict_fn,
          model=model,
          original_example=example,
          pred_key=pred_key,
          pred_type_info=pred_type_info,
      )
      class_to_explain = get_class_to_explain(
          model, pred_key, example, provided_class_to_explain
      )
      # Explain each text segment in the input, keeping the others constant.
      for text_key in text_keys:
        input_string = example[text_key]
        logging.info('Explaining: %s', input_string)
        # Perturbs the input string, gets model predictions, fits linear model.
        explanation = lime.explain(
            sentence=input_string,
            predict_fn=functools.partial(predict_fn, text_key=text_key),
            # `class_to_explain` is ignored when predict_fn output is a scalar.
            class_to_explain=class_to_explain,
            num_samples=num_samples,
            tokenizer=str.split,
            mask_token=mask_string,
            kernel=functools.partial(
                lime.exponential_kernel, kernel_width=kernel_width),
            seed=seed)
        # Turn the LIME explanation into a list following original word order.
        scores = explanation.feature_importance
        # TODO(lit-dev): Move score normalization to the UI.
        scores = citrus_util.normalize_scores(scores)
        result[text_key] = dtypes.TokenSalience(explanation.features, scores)
      all_results.append(result)
    return all_results
  def config_spec(self) -> types.Spec:
    return {
        TARGET_INFO_KEY: types.SalienceTargetInfo(),
        # TODO(b/205996131): remove TARGET_HEAD_KEY field
        TARGET_HEAD_KEY: types.SingleFieldMatcher(
            spec='output',
            types=[c.__name__ for c in _SUPPORTED_PRED_TYPES],
            required=False,
        ),
        # TODO(b/205996131): remove CLASS_KEY field
        CLASS_KEY: types.TextSegment(default='-1', required=False),
        MASK_KEY: types.TextSegment(default=self._mask_token, required=False),
        KERNEL_WIDTH_KEY: types.TextSegment(
            default=self._kernel_width, required=False
        ),
        NUM_SAMPLES_KEY: types.TextSegment(
            default=self._num_samples, required=False
        ),
        SEED_KEY: types.TextSegment(default=self._seed, required=False),
    }
  def is_compatible(self, model: lit_model.Model,
                    dataset: lit_dataset.Dataset) -> bool:
    del dataset  # Unused as salience comes from the model
    text_keys = utils.spec_contains(model.input_spec(), types.TextSegment)
    pred_keys = utils.spec_contains(model.output_spec(), _SUPPORTED_PRED_TYPES)
    return text_keys and pred_keys
  def meta_spec(self) -> types.Spec:
    return {'saliency': types.TokenSalience(autorun=self._autorun, signed=True)}

================
File: lit_nlp/components/metrics_test.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Tests for lit_nlp.components.metrics."""
from typing import Optional, Union
from absl.testing import absltest
from absl.testing import parameterized
from lit_nlp.api import dataset as lit_dataset
from lit_nlp.api import dtypes
from lit_nlp.api import model as lit_model
from lit_nlp.api import types
from lit_nlp.components import metrics
from lit_nlp.lib import testing_utils
LitType = types.LitType
class _GenTextTestModel(lit_model.BatchedModel):
  def input_spec(self) -> types.Spec:
    return {'input': types.TextSegment()}
  def output_spec(self) -> types.Spec:
    return {'output': types.GeneratedText(parent='input')}
  def predict_minibatch(self,
                        inputs: list[types.JsonDict]) -> list[types.JsonDict]:
    return [{'output': 'test_output'}] * len(inputs)
class _GenTextCandidatesTestModel(lit_model.BatchedModel):
  def input_spec(self) -> types.Spec:
    return {
        'input': types.TextSegment(),
        'label': types.MultiSegmentAnnotations(),
    }
  def output_spec(self) -> types.Spec:
    return {'output': types.GeneratedTextCandidates(parent='input')}
  def predict_minibatch(self,
                        inputs: list[types.JsonDict]) -> list[types.JsonDict]:
    return [
        {'output': [('gen_text one', 0.8), ('gen_text two', 0.3)]}
    ] * len(inputs)
_CLASSIFICATION_MODEL = testing_utils.ClassificationModelForTesting()
_GENERATED_TEXT_MODEL = _GenTextTestModel()
_GEN_TEXT_CANDS_MODEL = _GenTextCandidatesTestModel()
_REGRESSION_MODEL = testing_utils.IdentityRegressionModelForTesting()
class RegressionMetricsTest(parameterized.TestCase):
  def setUp(self):
    super(RegressionMetricsTest, self).setUp()
    self.metrics = metrics.RegressionMetrics()
  def test_meta_spec(self):
    meta_spec = self.metrics.meta_spec()
    self.assertLen(meta_spec, 3)
    self.assertIn('mse', meta_spec)
    self.assertIn('pearsonr', meta_spec)
    self.assertIn('spearmanr', meta_spec)
    for spec in meta_spec.values():
      self.assertIsInstance(spec, types.MetricResult)
  @parameterized.named_parameters(
      ('cls_model', _CLASSIFICATION_MODEL, False),
      ('gen_text_model', _GENERATED_TEXT_MODEL, False),
      ('reg_model', _REGRESSION_MODEL, True),
  )
  def test_is_compatible(self, model: lit_model.Model, expected: bool):
    """Always false to prevent use as explainer."""
    compat = self.metrics.is_compatible(
        model, lit_dataset.NoneDataset({'test': model}))
    self.assertEqual(compat, expected)
  @parameterized.named_parameters(
      ('regression', types.RegressionScore(), True),
      ('mulitclass', types.MulticlassPreds(vocab=['']), False),
      ('generated text', types.GeneratedText(), False))
  def test_is_field_compatible(self, pred: LitType, expected: bool):
    self.assertEqual(self.metrics.is_field_compatible(pred, None), expected)
  @parameterized.named_parameters(
      ('correct', [1, 2, 3, 4], [1, 2, 3, 4], 0, 1.0, 1.0),
      ('incorrect', [1, 2, 3, 4], [-5, -10, 5, 6], 47.0, 0.79559, 0.799999),
      ('some_correct', [1, 2, 3, 4], [1, 2, 5.5, 6.3], 2.885, 0.96566, 1.0),
  )
  def test_compute(self, labels: list[float], preds: list[float], mse: float,
                   pearsonr: float, spearmanr: float):
    expected = {'mse': mse, 'pearsonr': pearsonr, 'spearmanr': spearmanr}
    result = self.metrics.compute(labels, preds,
                                  types.RegressionScore(),
                                  types.RegressionScore())
    testing_utils.assert_deep_almost_equal(self, result, expected)
  def test_compute_empty(self):
    result = self.metrics.compute([], [], types.RegressionScore(),
                                  types.RegressionScore())
    testing_utils.assert_deep_almost_equal(self, result, {})
class MulticlassMetricsTest(parameterized.TestCase):
  def setUp(self):
    super(MulticlassMetricsTest, self).setUp()
    self.metrics = metrics.MulticlassMetricsImpl()
  def test_meta_spec(self):
    meta_spec = self.metrics.meta_spec()
    self.assertLen(meta_spec, 7)
    self.assertIn('accuracy', meta_spec)
    self.assertIn('precision', meta_spec)
    self.assertIn('recall', meta_spec)
    self.assertIn('f1', meta_spec)
    self.assertIn('auc', meta_spec)
    self.assertIn('aucpr', meta_spec)
    self.assertIn('num_missing_labels', meta_spec)
    for spec in meta_spec.values():
      self.assertIsInstance(spec, types.MetricResult)
  @parameterized.named_parameters(
      ('cls_model', _CLASSIFICATION_MODEL, True),
      ('reg_model', _REGRESSION_MODEL, False),
      ('gen_text_model', _GENERATED_TEXT_MODEL, False),
  )
  def test_is_compatible(self, model: lit_model.Model, expected: bool):
    """Always false to prevent use as explainer."""
    compat = self.metrics.is_compatible(
        model, lit_dataset.NoneDataset({'test': model}))
    self.assertEqual(compat, expected)
  @parameterized.named_parameters(
      ('multiclass', types.MulticlassPreds(vocab=['']), None, True),
      ('regression', types.RegressionScore(), None, False),
      ('generated text', types.GeneratedText(), None, False))
  def test_is_field_compatible(self, pred: LitType, parent: LitType,
                               expected: bool):
    self.assertEqual(
        self.metrics.is_field_compatible(pred, parent), expected)
  @parameterized.named_parameters(
      (
          'correct', ['0', '1', '2'], ['1', '2', '0', '1'],
          [[0, 1, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0]],
          1.0, 1.0, 1.0, 1.0
      ),
      (
          'incorrect', ['0', '1', '2'], ['1', '2', '0', '1'],
          [[.1, .4, .5], [.2, .7, .1], [.1, 0, .9], [1, 0, 0]],
          0.0, 0.0, 0.0, 0.0
      ),
      (
          'some_correct', ['0', '1', '2'], ['1', '2', '0', '1'],
          [[.1, .4, .5], [0, .1, .9], [.1, 0, .9], [0, 1, 0]],
          0.5, 0.57143, 0.5, 0.66667
      ),
      (
          'some_correct_4_class', ['0', '1', '2', '3'], ['1', '0', '2', '3'],
          [[.1, .4, .2, .3], [.9, .1, 0, 0], [0, .3, .5, .2], [.1, .1, .5, .3]],
          0.75, 0.66667, 0.66667, 0.66667
      ),
  )
  def test_compute_multiclass(
      self, vocab: list[str], labels: list[str], preds: list[list[int]],
      accuracy: float, f1: float, precision: float, recall: float):
    expected = {
        'accuracy': accuracy,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }
    result = self.metrics.compute(
        labels, preds, types.CategoryLabel(),
        types.MulticlassPreds(vocab=vocab, null_idx=0))
    testing_utils.assert_deep_almost_equal(self, result, expected)
  def test_compute_no_null_index(self):
    result = self.metrics.compute(
        ['1', '2', '0', '1'],
        [[.1, .4, .5], [0, .1, .9], [.1, 0, .9], [0, 1, 0]],
        types.CategoryLabel(), types.MulticlassPreds(vocab=['0', '1', '2']))
    testing_utils.assert_deep_almost_equal(self, result, {'accuracy': 0.5})
  def test_compute_correct_single_class(self):
    result = self.metrics.compute(
        ['1', '1'], [[.1, .9], [.2, .8]], types.CategoryLabel(),
        types.MulticlassPreds(vocab=['0', '1'], null_idx=0))
    testing_utils.assert_deep_almost_equal(self, result, {
        'accuracy': 1.0,
        # No AUC in this case.
        'aucpr': 1.0,
        'f1': 1.0,
        'precision': 1.0,
        'recall': 1.0,
    })
  def test_compute_almost_correct_single_class_with_null_idx_0(self):
    result = self.metrics.compute(
        ['1', '0', '1'], [[.1, .9], [.9, .1], [.8, .2]], types.CategoryLabel(),
        types.MulticlassPreds(vocab=['0', '1'], null_idx=0))
    testing_utils.assert_deep_almost_equal(
        self, result, {
            'accuracy': 0.66667,
            'auc': 1.0,
            'aucpr': 1.0,
            'f1': 0.66667,
            'precision': 1.0,
            'recall': 0.5,
        })
  def test_compute_empty_labels(self):
    result = self.metrics.compute(
        [], [], types.CategoryLabel(),
        types.MulticlassPreds(vocab=['0', '1', '2'], null_idx=0))
    testing_utils.assert_deep_almost_equal(self, result, {})
class MulticlassPairedMetricsTest(parameterized.TestCase):
  def setUp(self):
    super(MulticlassPairedMetricsTest, self).setUp()
    self.metrics = metrics.MulticlassPairedMetricsImpl()
  def test_meta_spec(self):
    meta_spec = self.metrics.meta_spec()
    self.assertLen(meta_spec, 3)
    self.assertIn('num_pairs', meta_spec)
    self.assertIn('swap_rate', meta_spec)
    self.assertIn('mean_jsd', meta_spec)
    for spec in meta_spec.values():
      self.assertIsInstance(spec, types.MetricResult)
  @parameterized.named_parameters(
      ('cls_model', _CLASSIFICATION_MODEL, True),
      ('reg_model', _REGRESSION_MODEL, False),
      ('gen_text_model', _GENERATED_TEXT_MODEL, False),
  )
  def test_is_compatible(self, model: lit_model.Model, expected: bool):
    """Always false to prevent use as explainer."""
    compat = self.metrics.is_compatible(
        model, lit_dataset.NoneDataset({'test': model}))
    self.assertEqual(compat, expected)
  @parameterized.named_parameters(
      ('multiclass', types.MulticlassPreds(vocab=['']), True),
      ('regression', types.RegressionScore(), False),
      ('generated text', types.GeneratedText(), False))
  def test_is_field_compatible(self, pred: LitType, expected: bool):
    self.assertEqual(self.metrics.is_field_compatible(pred, None), expected)
  @parameterized.named_parameters(
      ('no_swaps', [[0, 1], [0, 1], [1, 0], [1, 0]], 0, 0.0, 0.0),
      ('one_swap', [[0, 1], [1, 0], [1, 0], [1, 0]], 0, 0.34657, 0.5),
      ('two_swaps', [[0, 1], [1, 0], [1, 0], [0, 1]], 0, 0.69315, 1.0),
      ('no_null_index', [[0, 1], [1, 0], [1, 0], [0, 1]], None, 0.69315, 1.0),
  )
  def test_compute(
      self,
      preds: list[list[int]],
      null_idx: Optional[int],
      mean_jsd: float,
      swap_rate: float,
  ):
    labels = ['1', '1', '0', '0']
    indices = ['7f7f85', '345ac4', '3a3112', '88bcda']
    metas = [{'parentId': '345ac4'}, {}, {}, {'parentId': '3a3112'}]
    expected = {'mean_jsd': mean_jsd, 'num_pairs': 2, 'swap_rate': swap_rate}
    result = self.metrics.compute(
        labels,
        preds,
        types.CategoryLabel(),
        types.MulticlassPreds(vocab=['0', '1'], null_idx=null_idx),
        indices=indices,
        metas=metas,
    )
    testing_utils.assert_deep_almost_equal(self, result, expected)
  def test_compute_empty(self):
    result = self.metrics.compute(
        [],
        [],
        types.CategoryLabel(),
        types.MulticlassPreds(vocab=['0', '1'], null_idx=0),
        indices=[],
        metas=[]
    )
    testing_utils.assert_deep_almost_equal(self, result, {})
_MULTI_LABEL_VOCAB = ['1', '2', '3', '4', '5']
class MultilabelMetricsTest(parameterized.TestCase):
  def setUp(self):
    super(MultilabelMetricsTest, self).setUp()
    self.metrics = metrics.MultilabelMetrics()
  @parameterized.named_parameters(
      ('bad_parent', types.SparseMultilabelPreds(), types.Scalar(), False),
      ('bad_pred', types.RegressionScore(), types.SparseMultilabel(), False),
      ('yes', types.SparseMultilabelPreds(), types.SparseMultilabel(), True),
  )
  def test_is_field_compatible(
      self, pred: LitType, parent: LitType, expected: bool
  ):
    self.assertEqual(self.metrics.is_field_compatible(pred, parent), expected)
  def test_meta_spec(self):
    meta_spec = self.metrics.meta_spec()
    self.assertLen(meta_spec, 4)
    self.assertIn('exactmatch', meta_spec)
    self.assertIn('precision', meta_spec)
    self.assertIn('recall', meta_spec)
    self.assertIn('f1', meta_spec)
    for spec in meta_spec.values():
      self.assertIsInstance(spec, types.MetricResult)
  @parameterized.named_parameters(
      dict(
          testcase_name='all_correct_inferred_full_vocab',
          labels=[
              ['1', '3'],
              ['2', '4'],
              ['5']
          ],
          preds=[
              [('1', 1.0), ('2', 0.0), ('3', 1.0), ('4', 0.0), ('5', 0.0)],
              [('1', 0.0), ('2', 1.0), ('3', 0.0), ('4', 1.0), ('5', 0.0)],
              [('1', 0.0), ('2', 0.0), ('3', 0.0), ('4', 0.0), ('5', 1.0)],
          ],
          label_spec=types.SparseMultilabel(),
          pred_spec=types.SparseMultilabelPreds(vocab=_MULTI_LABEL_VOCAB),
          expected={
              'exactmatch': 1,
              'precision': 1,
              'recall': 1,
              'f1': 1,
          },
      ),
      dict(
          testcase_name='all_correct_inferred_sparse_vocab',
          labels=[
              ['1', '3'],
              ['2', '4'],
              ['5']
          ],
          preds=[
              [('1', 1.0), ('3', 1.0)],
              [('2', 1.0), ('4', 1.0)],
              [('5', 1.0)],
          ],
          label_spec=types.SparseMultilabel(),
          pred_spec=types.SparseMultilabelPreds(),
          expected={
              'exactmatch': 1,
              'precision': 1,
              'recall': 1,
              'f1': 1,
          },
      ),
      dict(
          testcase_name='all_correct_label_spec_vocab',
          labels=[
              ['1'],
              ['2'],
              ['5']
          ],
          preds=[
              [('1', 1.0)],
              [('2', 1.0)],
              [('5', 1.0)],
          ],
          label_spec=types.SparseMultilabel(vocab=_MULTI_LABEL_VOCAB),
          pred_spec=types.SparseMultilabelPreds(),
          expected={
              'exactmatch': 1,
              'precision': 1,
              'recall': 1,
              'f1': 1,
          },
      ),
      dict(
          testcase_name='all_correct_pred_spec_vocab',
          labels=[
              ['1'],
              ['2'],
              ['5']
          ],
          preds=[
              [('1', 1.0)],
              [('2', 1.0)],
              [('5', 1.0)],
          ],
          label_spec=types.SparseMultilabel(),
          pred_spec=types.SparseMultilabelPreds(vocab=_MULTI_LABEL_VOCAB),
          expected={
              'exactmatch': 1,
              'precision': 1,
              'recall': 1,
              'f1': 1,
          },
      ),
      dict(
          testcase_name='all_wrong_inferred_disjoint_vocab',
          labels=[['1']],
          preds=[[('3', 1.0), ('4', 1.0)]],
          label_spec=types.SparseMultilabel(),
          pred_spec=types.SparseMultilabelPreds(),
          expected={
              'exactmatch': 0,
              'precision': 0,
              'recall': 0,
              'f1': 0,
          },
      ),
      dict(
          testcase_name='all_wrong_inferred_full_vocab',
          labels=[
              ['1', '3'],
              ['2', '4'],
              ['5']
          ],
          preds=[
              [('1', 0.0), ('2', 0.0), ('3', 0.0), ('4', 0.0), ('5', 0.5)],
              [('1', 0.9), ('2', 0.0), ('3', 0.9), ('4', 0.0), ('5', 0.0)],
              [('1', 0.0), ('2', 1.0), ('3', 0.0), ('4', 1.0), ('5', 0.0)],
          ],
          label_spec=types.SparseMultilabel(),
          pred_spec=types.SparseMultilabelPreds(),
          expected={
              'exactmatch': 0,
              'precision': 0,
              'recall': 0,
              'f1': 0,
          },
      ),
      dict(
          testcase_name='all_wrong_inferred_sparse_vocab',
          labels=[
              ['1', '3'],
              ['2', '4'],
              ['5']
          ],
          preds=[
              [('5', 0.5)],
              [('1', 0.9), ('3', 0.9)],
              [('2', 1.0), ('4', 1.0)],
          ],
          label_spec=types.SparseMultilabel(),
          pred_spec=types.SparseMultilabelPreds(),
          expected={
              'exactmatch': 0,
              'precision': 0,
              'recall': 0,
              'f1': 0,
          },
      ),
      dict(
          testcase_name='mixed_inferred_disjoint_vocab',
          labels=[
              ['1', '3'],
              ['5']
          ],
          preds=[
              [('1', 1.0), ('4', 1.0)],
              [('4', 1.0)],
          ],
          label_spec=types.SparseMultilabel(),
          pred_spec=types.SparseMultilabelPreds(),
          expected={
              'exactmatch': 0,
              'precision': 0.25,
              'recall': 0.25,
              'f1': 0.25,
          },
      ),
      dict(
          testcase_name='mixed_inferred_full_vocab',
          labels=[
              ['1', '3'],
              ['2', '4'],
              ['5']
          ],
          preds=[
              [('1', 1.0), ('2', 0.0), ('3', 0.0), ('4', 1.0), ('5', 0.0)],
              [('1', 0.0), ('2', 1.0), ('3', 0.0), ('4', 0.0), ('5', 0.0)],
              [('1', 0.0), ('2', 0.0), ('3', 0.0), ('4', 0.0), ('5', 1.0)],
          ],
          label_spec=types.SparseMultilabel(),
          pred_spec=types.SparseMultilabelPreds(),
          expected={
              'exactmatch': 0.3333,
              'precision': 0.8333,
              'recall': 0.6667,
              'f1': 0.7222,
          },
      ),
      dict(
          testcase_name='mixed_inferred_sparse_vocab',
          labels=[
              ['1', '3'],
              ['2', '4'],
              ['5']
          ],
          preds=[
              [('1', 1.0), ('4', 1.0)],
              [('2', 1.0)],
              [('5', 1.0)],
          ],
          label_spec=types.SparseMultilabel(),
          pred_spec=types.SparseMultilabelPreds(),
          expected={
              'exactmatch': 0.3333,
              'precision': 0.8333,
              'recall': 0.6667,
              'f1': 0.7222,
          },
      ),
      dict(
          testcase_name='mixed_label_spec_vocab',
          labels=[
              ['1', '3'],
              ['2', '4'],
              ['5']
          ],
          preds=[
              [('1', 1.0), ('4', 1.0)],
              [('2', 1.0)],
              [('5', 1.0)],
          ],
          label_spec=types.SparseMultilabel(vocab=_MULTI_LABEL_VOCAB),
          pred_spec=types.SparseMultilabelPreds(),
          expected={
              'exactmatch': 0.3333,
              'precision': 0.8333,
              'recall': 0.6667,
              'f1': 0.7222,
          },
      ),
      dict(
          testcase_name='mixed_label_spec_vocab_superset_of_observed_vocab',
          labels=[
              ['1', '3'],
              ['5']
          ],
          preds=[
              [('1', 1.0), ('4', 1.0)],
              [('4', 1.0)],
          ],
          label_spec=types.SparseMultilabel(vocab=_MULTI_LABEL_VOCAB),
          pred_spec=types.SparseMultilabelPreds(),
          expected={
              'exactmatch': 0,
              'precision': 0.25,
              'recall': 0.25,
              'f1': 0.25,
          },
      ),
      dict(
          testcase_name='mixed_pred_spec_vocab',
          labels=[
              ['1', '3'],
              ['2', '4'],
              ['5']
          ],
          preds=[
              [('1', 1.0), ('4', 1.0)],
              [('2', 1.0)],
              [('5', 1.0)],
          ],
          label_spec=types.SparseMultilabel(),
          pred_spec=types.SparseMultilabelPreds(vocab=_MULTI_LABEL_VOCAB),
          expected={
              'exactmatch': 0.3333,
              'precision': 0.8333,
              'recall': 0.6667,
              'f1': 0.7222,
          },
      ),
      dict(
          testcase_name='mixed_pred_spec_vocab_superset_of_observed_vocab',
          labels=[
              ['1', '3'],
              ['5']
          ],
          preds=[
              [('1', 1.0), ('4', 1.0)],
              [('4', 1.0)],
          ],
          label_spec=types.SparseMultilabel(),
          pred_spec=types.SparseMultilabelPreds(vocab=_MULTI_LABEL_VOCAB),
          expected={
              'exactmatch': 0,
              'precision': 0.25,
              'recall': 0.25,
              'f1': 0.25,
          },
      ),
  )
  def test_compute(self, labels, preds, label_spec, pred_spec, expected):
    result = self.metrics.compute(labels, preds, label_spec, pred_spec)
    testing_utils.assert_deep_almost_equal(self, result, expected)
  @parameterized.named_parameters(
      ('no_labels', [], [('1', 0.1)]),
      ('no_labels_no_preds', ['1'], []),
      ('no_preds', [], []),
  )
  def test_compute_empty(self, labels, preds):
    self.assertEmpty(
        self.metrics.compute(
            labels,
            preds,
            types.SparseMultilabel(),
            types.SparseMultilabelPreds(),
        )
    )
  @parameterized.named_parameters(
      dict(
          testcase_name='bad_label_spec',
          error_type=TypeError,
          labels=[['1']],
          preds=[[('1', 0.1)]],
          label_spec=types.CategoryLabel(),
          pred_spec=types.SparseMultilabelPreds(),
      ),
      dict(
          testcase_name='bad_pred_spec',
          error_type=TypeError,
          labels=[['1']],
          preds=[[('1', 0.1)]],
          label_spec=types.SparseMultilabel(),
          pred_spec=types.MulticlassPreds(vocab=[]),
      ),
      dict(
          testcase_name='more_labels_than_preds',
          error_type=ValueError,
          labels=[['1'], ['2']],
          preds=[[('1', 0.1)]],
          label_spec=types.SparseMultilabel(),
          pred_spec=types.SparseMultilabelPreds(),
      ),
      dict(
          testcase_name='more_preds_than_labels',
          error_type=ValueError,
          labels=[['1']],
          preds=[[('1', 0.1)], [('2', 0.2)]],
          label_spec=types.SparseMultilabel(),
          pred_spec=types.SparseMultilabelPreds(),
      ),
  )
  def test_compute_errors(
      self, error_type, labels, preds, label_spec, pred_spec
  ):
    with self.assertRaises(error_type):
      self.metrics.compute(labels, preds, label_spec, pred_spec)
class CorpusBLEUTest(parameterized.TestCase):
  def setUp(self):
    super(CorpusBLEUTest, self).setUp()
    self.metrics = metrics.CorpusBLEU()
  def test_meta_spec(self):
    meta_spec = self.metrics.meta_spec()
    self.assertLen(meta_spec, 2)
    self.assertIn('corpus_bleu', meta_spec)
    self.assertIn('corpus_bleu@1', meta_spec)
    for spec in meta_spec.values():
      self.assertIsInstance(spec, types.MetricResult)
  @parameterized.named_parameters(
      ('cls_model', _CLASSIFICATION_MODEL, False),
      ('reg_model', _REGRESSION_MODEL, False),
      ('gen_text_model', _GENERATED_TEXT_MODEL, True),
  )
  def test_is_compatible(self, model: lit_model.Model, expected: bool):
    """Always false to prevent use as explainer."""
    compat = self.metrics.is_compatible(
        model, lit_dataset.NoneDataset({'test': model}))
    self.assertEqual(compat, expected)
  @parameterized.named_parameters(
      ('generated text, str', types.GeneratedText(), types.StringLitType(),
       True),
      ('candidates, str', types.GeneratedTextCandidates(),
       types.StringLitType(), True),
      ('bad pred, good parent', types.Scalar(), types.StringLitType(), False),
      ('good pred, bad parent', types.GeneratedText(), types.Scalar(), False),
      ('both bad', types.Scalar(), types.Scalar(), False))
  def test_is_field_compatible(self, pred: LitType, parent: LitType,
                               expected: bool):
    self.assertEqual(self.metrics.is_field_compatible(pred, parent), expected)
  @parameterized.named_parameters(
      ('correct', ['This is a test.', 'Test one', 'A third test'], 100.0000),
      (
          'some_different',
          ['This is a test.', 'Test two', 'A third test example'], 68.037493
      ),
      (
          'all_different',
          ['these test.', 'Test two', 'A third test example'], 29.508062
      ),
  )
  def test_compute(self, preds: list[str], score: float):
    labels = ['This is a test.', 'Test one', 'A third test']
    expected = {'corpus_bleu': score}
    result = self.metrics.compute(labels, preds, types.GeneratedText(),
                                  types.GeneratedText())
    testing_utils.assert_deep_almost_equal(self, result, expected)
  def test_compute_empty_labels(self):
    result = self.metrics.compute([], [], types.GeneratedText(),
                                  types.GeneratedText())
    testing_utils.assert_deep_almost_equal(self, result, {})
  def test_compute_with_candidates(self):
    # Should only score the first one (@1).
    labels = ['This is a test.', 'Test two']
    preds = [
        [('This is a test.', -1.0), ('foobar', -20.0)],
        [('Test two', -1.0), ('spam', -20.0)],
    ]
    result = self.metrics.compute(labels, preds, types.TextSegment(),
                                  types.GeneratedTextCandidates())
    testing_utils.assert_deep_almost_equal(self, result,
                                           {'corpus_bleu@1': 100.0000})
class RougeLTest(parameterized.TestCase):
  def setUp(self):
    super(RougeLTest, self).setUp()
    self.metrics = metrics.RougeL()
  def test_meta_spec(self):
    meta_spec = self.metrics.meta_spec()
    self.assertLen(meta_spec, 2)
    self.assertIn('rougeL', meta_spec)
    self.assertIn('rougeL@1', meta_spec)
    for spec in meta_spec.values():
      self.assertIsInstance(spec, types.MetricResult)
  @parameterized.named_parameters(
      ('cls_model', _CLASSIFICATION_MODEL, False),
      ('reg_model', _REGRESSION_MODEL, False),
      ('gen_text_model', _GENERATED_TEXT_MODEL, True),
  )
  def test_is_compatible(self, model: lit_model.Model, expected: bool):
    """Always false to prevent use as explainer."""
    compat = self.metrics.is_compatible(
        model, lit_dataset.NoneDataset({'test': model}))
    self.assertEqual(compat, expected)
  @parameterized.named_parameters(
      ('generated text + str', types.GeneratedText(), types.StringLitType(),
       True),
      ('candidates + str', types.GeneratedTextCandidates(),
       types.StringLitType(), True),
      ('bad pred, good parent', types.Scalar(), types.StringLitType(), False),
      ('good pred, bad parent', types.GeneratedText(), types.Scalar(), False),
      ('both bad', types.Scalar(), types.Scalar(), False))
  def test_is_field_compatible(self, pred: LitType, parent: LitType,
                               expected: bool):
    self.assertEqual(self.metrics.is_field_compatible(pred, parent), expected)
  @parameterized.named_parameters(
      ('correct', ['This is a test.', 'Test one', 'A third test'], 1.0),
      (
          'some_different',
          ['This is a test.', 'Test two', 'A third test example'], 0.785714
      ),
      (
          'all_different',
          ['these test.', 'Test two', 'A third test example'], 0.563492
      ),
  )
  def test_compute(self, preds: list[str], score: float):
    labels = ['This is a test.', 'Test one', 'A third test']
    expected = {'rougeL': score}
    result = self.metrics.compute(labels, preds, types.TextSegment(),
                                  types.GeneratedText())
    testing_utils.assert_deep_almost_equal(self, result, expected)
  def test_compute_empty(self):
    result = self.metrics.compute([], [], types.GeneratedText(),
                                  types.GeneratedText())
    testing_utils.assert_deep_almost_equal(self, result, {})
  def test_compute_with_candidates(self):
    # Should only score the first one (@1).
    labels = ['This is a test.', 'Test two']
    preds = [
        [('This is a test.', -1.0), ('foobar', -20.0)],
        [('Test two', -1.0), ('spam', -20.0)],
    ]
    result = self.metrics.compute(labels, preds, types.TextSegment(),
                                  types.GeneratedTextCandidates())
    testing_utils.assert_deep_almost_equal(self, result, {'rougeL@1': 1.0})
_MULTI_SEG_ANNOTATION_LABELS = [
    [dtypes.AnnotationCluster(label='one', spans=[])],
    [dtypes.AnnotationCluster(label='two', spans=[])],
]
class ExactMatchTest(parameterized.TestCase):
  def setUp(self):
    super().setUp()
    self.metrics = metrics.ExactMatchMetrics()
  def test_meta_spec(self):
    meta_spec = self.metrics.meta_spec()
    self.assertLen(meta_spec, 2)
    self.assertIn('exactmatch', meta_spec)
    self.assertIn('exactmatch@1', meta_spec)
    for spec in meta_spec.values():
      self.assertIsInstance(spec, types.MetricResult)
  @parameterized.named_parameters(
      dict(
          testcase_name='classification',
          model=_CLASSIFICATION_MODEL,
          expected=False,
      ),
      dict(
          testcase_name='regression',
          model=_REGRESSION_MODEL,
          expected=False,
      ),
      dict(
          testcase_name='gen_text',
          model=_GENERATED_TEXT_MODEL,
          expected=True,
      ),
      dict(
          testcase_name='gen_text_cands',
          model=_GEN_TEXT_CANDS_MODEL,
          expected=True,
      ),
  )
  def test_is_compatible(self, model: LitType, expected: bool):
    compat = self.metrics.is_compatible(
        model, lit_dataset.NoneDataset({'test': model}))
    self.assertEqual(compat, expected)
  @parameterized.named_parameters(
      dict(
          testcase_name='gentext_multi_segment_annotations',
          pred=types.GeneratedText(),
          parent=types.MultiSegmentAnnotations(),
          expected=True,
      ),
      dict(
          testcase_name='gentext_text',
          pred=types.GeneratedText(),
          parent=types.TextSegment(),
          expected=True,
      ),
      dict(
          testcase_name='gencands_multi_segment_annotations',
          pred=types.GeneratedTextCandidates(),
          parent=types.MultiSegmentAnnotations(),
          expected=True,
      ),
      dict(
          testcase_name='gencands_text',
          pred=types.GeneratedTextCandidates(),
          parent=types.TextSegment(),
          expected=True,
      ),
      dict(
          testcase_name='gentext_scalar',
          pred=types.GeneratedText(),
          parent=types.Scalar(),
          expected=False,
      ),
      dict(
          testcase_name='gencands_scalar',
          pred=types.GeneratedTextCandidates(),
          parent=types.Scalar(),
          expected=False,
      ),
      dict(
          testcase_name='text_text',
          pred=types.TextSegment(),
          parent=types.TextSegment(),
          expected=False,
      ),
      dict(
          testcase_name='text_scalar',
          pred=types.TextSegment(),
          parent=types.Scalar(),
          expected=False,
      ),
  )
  def test_is_field_compatible(self,
                               pred: LitType,
                               parent: LitType,
                               expected: bool):
    self.assertEqual(self.metrics.is_field_compatible(pred, parent), expected)
  @parameterized.named_parameters(
      # Without labels or preds, it should return an empty dict
      dict(
          testcase_name='no_labels',
          labels=[],
          preds=['one', 'two'],
          label_spec=types.TextSegment(),
          preds_spec=types.GeneratedText(),
          expected={},
      ),
      dict(
          testcase_name='no_preds',
          labels=['one', 'two'],
          preds=[],
          label_spec=types.TextSegment(),
          preds_spec=types.GeneratedText(),
          expected={},
      ),
      # Tests for all, some, and none correct w/ MultiSegmentAnnotations labels
      dict(
          testcase_name='correct_multi_segment_annotations_gentext',
          labels=_MULTI_SEG_ANNOTATION_LABELS,
          preds=['one', 'two'],
          label_spec=types.MultiSegmentAnnotations(),
          preds_spec=types.GeneratedText(),
          expected={'exactmatch': 1.0},
      ),
      dict(
          testcase_name='correct_multi_segment_annotations_gencands',
          labels=_MULTI_SEG_ANNOTATION_LABELS,
          preds=[[('one', None)], [('two', None)]],
          label_spec=types.MultiSegmentAnnotations(),
          preds_spec=types.GeneratedTextCandidates(),
          expected={'exactmatch@1': 1.0},
      ),
      dict(
          testcase_name='some_multi_segment_annotations_gentext',
          labels=_MULTI_SEG_ANNOTATION_LABELS,
          preds=['one', 'four'],
          label_spec=types.MultiSegmentAnnotations(),
          preds_spec=types.GeneratedText(),
          expected={'exactmatch': 0.5},
      ),
      dict(
          testcase_name='some_multi_segment_annotations_gencands',
          labels=_MULTI_SEG_ANNOTATION_LABELS,
          preds=[[('one', None)], [('four', None)]],
          label_spec=types.MultiSegmentAnnotations(),
          preds_spec=types.GeneratedTextCandidates(),
          expected={'exactmatch@1': 0.5},
      ),
      dict(
          testcase_name='none_multi_segment_annotations_gentext',
          labels=_MULTI_SEG_ANNOTATION_LABELS,
          preds=['three', 'four'],
          label_spec=types.MultiSegmentAnnotations(),
          preds_spec=types.GeneratedText(),
          expected={'exactmatch': 0.0},
      ),
      dict(
          testcase_name='none_multi_segment_annotations_gencands',
          labels=_MULTI_SEG_ANNOTATION_LABELS,
          preds=[[('three', None)], [('four', None)]],
          label_spec=types.MultiSegmentAnnotations(),
          preds_spec=types.GeneratedTextCandidates(),
          expected={'exactmatch@1': 0.0},
      ),
      # Tests for all, some, and none correct w/ TextSegment labels
      dict(
          testcase_name='correct_text_gentext',
          labels=['one', 'two'],
          preds=['one', 'two'],
          label_spec=types.TextSegment(),
          preds_spec=types.GeneratedText(),
          expected={'exactmatch': 1.0},
      ),
      dict(
          testcase_name='correct_text_gencands',
          labels=['one', 'two'],
          preds=[[('one', None)], [('two', None)]],
          label_spec=types.TextSegment(),
          preds_spec=types.GeneratedTextCandidates(),
          expected={'exactmatch@1': 1.0},
      ),
      dict(
          testcase_name='some_text_gentext',
          labels=['one', 'two'],
          preds=['one', 'four'],
          label_spec=types.TextSegment(),
          preds_spec=types.GeneratedText(),
          expected={'exactmatch': 0.5},
      ),
      dict(
          testcase_name='some_text_gencands',
          labels=['one', 'two'],
          preds=[[('one', None)], [('four', None)]],
          label_spec=types.TextSegment(),
          preds_spec=types.GeneratedTextCandidates(),
          expected={'exactmatch@1': 0.5},
      ),
      dict(
          testcase_name='none_text_gentext',
          labels=['one', 'two'],
          preds=['three', 'four'],
          label_spec=types.TextSegment(),
          preds_spec=types.GeneratedText(),
          expected={'exactmatch': 0.0},
      ),
      dict(
          testcase_name='none_text_gencands',
          labels=['one', 'two'],
          preds=[[('three', None)], [('four', None)]],
          label_spec=types.TextSegment(),
          preds_spec=types.GeneratedTextCandidates(),
          expected={'exactmatch@1': 0.0},
      ),
  )
  def test_compute(self,
                   labels: Union[list[str],
                                 list[list[dtypes.AnnotationCluster]]],
                   preds,
                   label_spec: Union[types.MultiSegmentAnnotations,
                                     types.TextSegment],
                   preds_spec: Union[types.GeneratedText,
                                     types.GeneratedTextCandidates],
                   expected: dict[str, float]):
    result = self.metrics.compute(labels, preds, label_spec, preds_spec)
    testing_utils.assert_deep_almost_equal(self, result, expected)
  @parameterized.named_parameters(
      dict(
          testcase_name='invalid_labels_gentext',
          label_spec=types.Scalar(),
          preds_spec=types.GeneratedText(),
      ),
      dict(
          testcase_name='invalid_labels_gentextcandidates',
          label_spec=types.Scalar(),
          preds_spec=types.GeneratedTextCandidates(),
      ),
      dict(
          testcase_name='invalid_preds_text',
          label_spec=types.TextSegment(),
          preds_spec=types.Scalar(),
      ),
      dict(
          testcase_name='invalid_preds_multi_segment_annotations',
          label_spec=types.MultiSegmentAnnotations(),
          preds_spec=types.Scalar(),
      ),
  )
  def test_compute_spec_exceptions(self,
                                   label_spec: types.LitType,
                                   preds_spec: types.LitType):
    inputs = ['one', 'two', 'three']
    preds = ['one', 'two', 'three']
    with self.assertRaises(TypeError):
      self.metrics.compute(inputs, preds, label_spec, preds_spec)
if __name__ == '__main__':
  absltest.main()

================
File: lit_nlp/components/metrics.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Metric component and implementations."""
import collections
from typing import Any, Callable, Optional, Sequence, Union, cast
from absl import logging
from lit_nlp.api import components as lit_components
from lit_nlp.api import dataset as lit_dataset
from lit_nlp.api import model as lit_model
from lit_nlp.api import types
from lit_nlp.components import classification_results
import numpy as np
import sacrebleu
from scipy import stats as scipy_stats
from scipy.spatial import distance as scipy_distance
from sklearn import metrics as sklearn_metrics
from sklearn import preprocessing as sklearn_preprocessing
from rouge_score import rouge_scorer
JsonDict = types.JsonDict
IndexedInput = types.IndexedInput
LitType = types.LitType
Spec = types.Spec
_MultiLabelBinarizer = sklearn_preprocessing.MultiLabelBinarizer
def map_pred_keys(
    data_spec: Spec, model_output_spec: Spec,
    predicate: Callable[[LitType, Optional[LitType]], bool]) -> dict[str, str]:
  """Returns a map of compatible output fields and their parent input fields."""
  ret = {}
  for pred_key, pred_spec in model_output_spec.items():
    if not hasattr(pred_spec, 'parent'):
      # Skip fields like AttentionHeads that don't define parent= at all.
      # Don't log for these as this can be very spammy.
      continue
    parent_key: Optional[str] = getattr(pred_spec, 'parent', None)
    if parent_key is None:
      logging.info("Skipping '%s': No parent provided.", pred_key)
      continue
    if parent_key not in data_spec:
      logging.info(
          "Skipping '%s': parent field '%s' not found in dataset.",
          pred_key,
          parent_key,
      )
      continue
    parent_spec: LitType = data_spec[parent_key]
    if predicate(pred_spec, parent_spec):
      ret[pred_key] = parent_key
    else:
      logging.info("Skipping '%s': incompatible parent '%s'.", pred_key,
                   parent_key)
      continue
  return ret
def nan_to_none(metrics: dict[str, float]) -> dict[str, Optional[float]]:
  # NaN is not a valid JSON value, so replace with None which will be
  # serialized as null.
  # TODO(lit-dev): consider moving this logic to serialize.py?
  return {k: (v if not np.isnan(v) else None) for k, v in metrics.items()}
class SimpleMetrics(lit_components.Metrics):
  """Base class for built-in metrics rendered in the main metrics table."""
  def run(self,
          inputs: Sequence[JsonDict],
          model: lit_model.Model,
          dataset: lit_dataset.Dataset,
          model_outputs: Optional[list[JsonDict]] = None,
          config: Optional[JsonDict] = None):
    if model_outputs is None:
      model_outputs = list(model.predict(inputs))
    output_spec = model.output_spec()
    field_map = map_pred_keys(
        dataset.spec(), output_spec, self.is_field_compatible
    )
    ret = []
    for pred_key, label_key in field_map.items():
      # Extract fields
      labels = [ex[label_key] for ex in inputs]
      preds = [mo[pred_key] for mo in model_outputs]
      indices = [ex.get('_id') for ex in inputs]
      metas = [ex.get('_meta', {}) for ex in inputs]
      # Compute metrics, as dict(str -> float)
      metrics = self.compute(
          labels,
          preds,
          label_spec=dataset.spec()[label_key],
          pred_spec=output_spec[pred_key],
          indices=indices,
          metas=metas,
          config=config.get(pred_key) if config else None,
      )
      # Format for frontend.
      ret.append({
          'pred_key': pred_key,
          'label_key': label_key,
          'metrics': nan_to_none(metrics)
      })
    return ret
# TODO(b/254833485): Convert to inherit from lit_components.Metrics so that
# promotion of Metrics to a top-level class more direct.
class ClassificationMetricsWrapper(lit_components.Interpreter):
  """Wrapper for classification metrics interpreters.
  Gets margin setting for each input based on raw scores and on provided
  margins in the config, which can be faceted by input feature values. Then
  passes the raw scores example-specific margins to the metrics interpreter that
  this class wraps for calculation of metrics.
  """
  def __init__(self, metrics: SimpleMetrics):
    self._metrics = metrics
  def is_compatible(self, model: lit_model.Model,
                    dataset: lit_dataset.Dataset) -> bool:
    """Metrics should always return false for Model-level compatibility."""
    return self._metrics.is_compatible(model, dataset)
  def is_field_compatible(self, pred_spec: LitType,
                          parent_spec: Optional[LitType]) -> bool:
    """Return true if compatible with this field."""
    return self._metrics.is_field_compatible(pred_spec, parent_spec)
  def meta_spec(self) -> dict[str, types.LitType]:
    return self._metrics.meta_spec()
  def run(self,
          inputs: list[JsonDict],
          model: lit_model.Model,
          dataset: lit_dataset.Dataset,
          model_outputs: Optional[list[JsonDict]] = None,
          config: Optional[JsonDict] = None):
    # Get margin for each input for each pred key and add them to a config dict
    # to pass to the wrapped metrics.
    field_map = map_pred_keys(
        dataset.spec(), model.output_spec(), self.is_field_compatible
    )
    margin_config = {}
    for pred_key in field_map:
      field_config = config.get(pred_key) if config else None
      margins = [
          classification_results.get_margin_for_input(field_config, inp)
          for inp in inputs
      ]
      margin_config[pred_key] = margins
    return self._metrics.run(inputs, model, dataset, model_outputs,
                             margin_config)
class RegressionMetrics(SimpleMetrics):
  """Standard regression metrics."""
  def is_field_compatible(self, pred_spec: LitType,
                          parent_spec: Optional[LitType]) -> bool:
    """Return true if compatible with this field."""
    del parent_spec
    return isinstance(pred_spec, types.RegressionScore)
  def meta_spec(self) -> dict[str, types.LitType]:
    return {
        'mse': types.MetricResult(
            best_value=types.MetricBestValue.ZERO,
            description='Mean squared error: Estimates the mean of the '
                        'square of the differences between the estimated value '
                        'and the actual value. Closer to 0 is better.'),
        'pearsonr': types.MetricResult(
            description="Pearson's R: Measures the linear correlation between "
                        "the estimated value and the actual value. Values "
                        "closer to 1 indicate a strong positive correlation "
                        "and values closee to -1 indicate a strong negative "
                        "correlation."),
        'spearmanr': types.MetricResult(
            description="Spearman's Rho: Measures the rank correlation between "
                        "the estimated and actual values. Values closer to 1 "
                        "indicate a strong positive correlation and values "
                        "closer to -1 indicate a strong negative correlation."),
    }
  def compute(self,
              labels: Sequence[float],
              preds: Sequence[float],
              label_spec: LitType,
              pred_spec: LitType,
              config: Optional[JsonDict] = None,
              indices: Optional[Sequence[types.ExampleId]] = None,
              metas: Optional[Sequence[JsonDict]] = None) -> dict[str, float]:
    """Compute the MSE and Pearson's & Spearman's R for regression predictions.
    Args:
      labels: Ground truth values for each prediction.
      preds: The models predicted regression scores, aligned with `labels`.
      label_spec: A Scalar spec for the model's label field.
      pred_spec: A RegressionScore spec for the model's prediction field.
      config: Unused configuration dict inherited from super class.
      indices: Unused list of IDs, aligned with `preds` and `labels`.
      metas: Unused list of LitMetadatas, aligned with `preds` and `labels`.
    Returns:
      A dict containing the mean squared error (key=`mse`), Pearson's R
      (key=`pearsonr`) and Spearmean's R (key=`spearmanr`) for each prediction.
      If `preds` or `labels` are empty, returns an empty dict.
    Raises:
      TypeError: `label_spec` is not a `Scalar` or `pred_spec` is not a
        `RegressionScore`. Note overriding the type information in the method
        signature will produce a signature mismatch error in PyType, see
        https://google.github.io/pytype/errors.html#signature-mismatch
    """
    del config, indices, metas
    if not labels or not preds:
      return {}
    if not isinstance(label_spec, types.Scalar):
      raise TypeError('label_spec must be a Scalar, received '
                      f'{type(label_spec).__name__}')
    if not isinstance(pred_spec, types.RegressionScore):
      raise TypeError('pred_spec must be a RegressionScore, received '
                      f'{type(pred_spec).__name__}')
    mse = sklearn_metrics.mean_squared_error(labels, preds)
    if len(labels) < 2:  # Check if only one point selected.
      pearsonr = np.nan
    else:
      pearsonr = scipy_stats.pearsonr(labels, preds)[0]
    spearmanr = scipy_stats.spearmanr(labels, preds)[0]
    return {'mse': mse, 'pearsonr': pearsonr, 'spearmanr': spearmanr}
class MulticlassMetricsImpl(SimpleMetrics):
  """Aggregate metrics for multi-class output."""
  def get_all_metrics(self,
                      y_true: Sequence[int],
                      y_pred_probs: Sequence[np.ndarray],
                      pred_spec: types.MulticlassPreds,
                      config: Optional[JsonDict] = None,
                      null_idx: Optional[int] = None):
    # Filter out unlabeled examples before calculating metrics.
    total_len = len(y_true)
    labeled_example_indices = [
        index for index, y in enumerate(y_true) if y != -1
    ]
    y_true = [y_true[i] for i in labeled_example_indices]
    y_pred_probs = [y_pred_probs[i] for i in labeled_example_indices]
    y_pred = classification_results.get_classifications(y_pred_probs, pred_spec,
                                                        config)
    y_pred = [y_pred[i] for i in labeled_example_indices]
    ret = collections.OrderedDict()
    ret['accuracy'] = sklearn_metrics.accuracy_score(y_true, y_pred)
    # TODO(lit-team): compute macro averages as well?
    # If task has a null class then compute P,R,F1 by treating
    # null_idx as the negative / "other" class.
    if null_idx is not None:
      # Note: labels here are indices.
      labels: list[int] = [
          i for i in range(len(pred_spec.vocab)) if i != null_idx
      ]
      ret['precision'] = sklearn_metrics.precision_score(
          y_true, y_pred, labels=labels, average='micro')
      ret['recall'] = sklearn_metrics.recall_score(
          y_true, y_pred, labels=labels, average='micro')
      ret['f1'] = sklearn_metrics.f1_score(
          y_true, y_pred, labels=labels, average='micro')
      # The target type used in computing metrics will be 'binary'.
      # Reshape predictions to only include those of the positive class.
      if len(pred_spec.vocab) == 2:
        y_score = [1 - p[null_idx] for p in y_pred_probs
                  ]  # <float[]>[num_examples]
        y_true_indicators = [y != null_idx for y in y_true]
        # AUC is not defined when there is only 1 unique class.
        if len(np.unique(y_true)) > 1:
          ret['auc'] = sklearn_metrics.roc_auc_score(
              y_true_indicators, y_score, average='micro')
        ret['aucpr'] = sklearn_metrics.average_precision_score(
            y_true_indicators, y_score, average='micro')
    if len(labeled_example_indices) != total_len:
      ret['num_missing_labels'] = total_len - len(labeled_example_indices)
    return ret
  def is_field_compatible(self, pred_spec: LitType,
                          parent_spec: Optional[LitType]) -> bool:
    """Return true if compatible with this field."""
    del parent_spec
    return isinstance(pred_spec, types.MulticlassPreds)
  def meta_spec(self) -> dict[str, types.LitType]:
    return {
        'accuracy': types.MetricResult(
            best_value=types.MetricBestValue.HIGHEST,
            description='The proportion of correct labels predicted by the '
                        'model. Closer to 1 is better.'),
        'precision': types.MetricResult(
            best_value=types.MetricBestValue.HIGHEST,
            description='The proportion of correct predictions for this class '
                        'out of all predictions of this class. Closer to 1 is '
                        'better.'),
        'recall': types.MetricResult(
            best_value=types.MetricBestValue.HIGHEST,
            description='The proportion of correct predictions for this class '
                        'out of all datapoints in this class. Closer to 1 is '
                        'better.'),
        'f1': types.MetricResult(
            best_value=types.MetricBestValue.HIGHEST,
            description='The performance of the model as the harmonic mean of '
                        'precision and recall. Closer to 1 is better.'),
        'auc': types.MetricResult(
            best_value=types.MetricBestValue.HIGHEST,
            description='Area under the ROC curve. Closer to 1 is better.'),
        'aucpr': types.MetricResult(
            best_value=types.MetricBestValue.HIGHEST,
            description='Area under the PR curve. Closer to 1 is better.'),
        'num_missing_labels': types.MetricResult(
            best_value=types.MetricBestValue.ZERO,
            description='The number of predictions that did not have ground '
                        'truth labels. Closer to 0 is better.'),
    }
  def compute(self,
              labels: Sequence[str],
              preds: Sequence[np.ndarray],
              label_spec: LitType,
              pred_spec: LitType,
              config: Optional[JsonDict] = None,
              indices: Optional[Sequence[types.ExampleId]] = None,
              metas: Optional[Sequence[JsonDict]] = None) -> dict[str, float]:
    """Compute standard metrics for multiclass predictions.
    Args:
      labels: Ground truth class for each prediction.
      preds: The models predicted class label, aligned with `labels`.
      label_spec: Unused field spec from super class
      pred_spec: A MulticlassPreds spec for the model's prediction field.
      config: Unused configuration dict inherited from super class.
      indices: Unused list of IDs, aligned with `preds` and `labels`.
      metas: Unused list of LitMetadatas, aligned with `preds` and `labels`.
    Returns:
      A dict containing the `accuracy`, `precission`, `recall`, `f1`, `auc`,
      `aucpr`, and `num_missing_labels` scores for the provided predictions.
      If `preds` or `labels` are empty, returns an empty dict.
    Raises:
      TypeError: `pred_spec` is not `MulticlassPreds`. Note overriding the type
        information in the method signature will produce a signature mismatch
        error in PyType, see
        https://google.github.io/pytype/errors.html#signature-mismatch
    """
    # TODO(lit-dev): compare on strings instead of converting to indices?
    # This should be more robust to skew in label sets.
    del label_spec, indices, metas
    if not labels or not preds:
      return {}
    if not isinstance(pred_spec, types.MulticlassPreds):
      raise TypeError('pred_spec must be a MulticlassPreds, received '
                      f'{type(pred_spec).__name__}')
    label_idxs = [
        pred_spec.vocab.index(label) if label in pred_spec.vocab else -1
        for label in labels
    ]
    return self.get_all_metrics(
        label_idxs,
        preds,
        pred_spec,
        null_idx=pred_spec.null_idx,
        config=config)
class MulticlassMetrics(ClassificationMetricsWrapper):
  def __init__(self):
    ClassificationMetricsWrapper.__init__(self, MulticlassMetricsImpl())
class MulticlassPairedMetricsImpl(SimpleMetrics):
  """Paired analysis between generated datapoints and their parents.
  Currently, this computes the swap rate, which is a measure of how often the
  generated datapoint causes the model to change its prediction. We also report
  mean JSD between model(d) and model(d') as a "soft" measure of the response of
  the model to the perturbations.
  """
  def meta_spec(self) -> types.Spec:
    return {
        'num_pairs': types.MetricResult(
            description='The number of pairs found/analyzed.'),
        'swap_rate': types.MetricResult(
            best_value=types.MetricBestValue.ZERO,
            description='The proportion of time the prediction differs between '
                        'the pair of examples. Closer to 0 is better.'),
        'mean_jsd': types.MetricResult(
            best_value=types.MetricBestValue.ZERO,
            description='Mean Jensen-Shannon distance measures the similarity '
                        'between two probability distributions. Closer to 0 is '
                        'better.'),
    }
  def is_field_compatible(self, pred_spec: LitType,
                          parent_spec: Optional[LitType]) -> bool:
    """Return true if compatible with this field."""
    del parent_spec
    return isinstance(pred_spec, types.MulticlassPreds)
  @staticmethod
  def find_pairs(indices: Sequence[types.ExampleId],
                 metas: Sequence[JsonDict]) -> list[tuple[int, int]]:
    """Find valid pairs in the current selection, and return list indices."""
    id_to_position = {example_id: i for i, example_id in enumerate(indices)}
    pairs = []  # (i,j) relative to labels and preds lists
    for this_id, meta in zip(indices, metas):
      if 'parentId' not in meta:
        continue  # skip if no parent
      parent_id = meta['parentId']
      if parent_id not in id_to_position:
        continue  # skip if parent not in current selection
      pairs.append((id_to_position[parent_id], id_to_position[this_id]))
    return pairs
  def compute(
      self,
      labels: Sequence[Any],
      preds: Sequence[Any],
      label_spec: LitType,
      pred_spec: LitType,
      config: Optional[JsonDict] = None,
      indices: Optional[Sequence[types.ExampleId]] = None,
      metas: Optional[Sequence[JsonDict]] = None) -> dict[str, float]:
    """Compute standard paired metrics for multiclass predictions.
    Args:
      labels: Unused list of ground truth values from the super class.
      preds: The models predicted class label, aligned with `labels`.
      label_spec: Unused field spec from the super class.
      pred_spec: A MulticlassPreds spec for the model's prediction field.
      config: Optional margins for computing classification results.
      indices: The ID for each IndexedInput, aligned with `preds` and `labels`.
      metas: The metadata for each Input, aligned with `preds` and `labels`.
    Returns:
      A dict containing the `num_pairs`, `swap_rate`, and `mean_jsd` values for
      the provided `preds`. If `num_pairs` is 0, returns an empty dict.
    Raises:
      TypeError: `pred_spec` is not `MulticlassPreds`. Note overriding the type
        information in the method signature will produce a signature mismatch
        error in PyType, see
        https://google.github.io/pytype/errors.html#signature-mismatch
    """
    del labels  # Unused; we only care about preds.
    del label_spec  # Unused; we only care about preds.
    ret = collections.OrderedDict()
    pairs = self.find_pairs(indices, metas)
    ret['num_pairs'] = len(pairs)
    if ret['num_pairs'] == 0:
      return {}
    if not isinstance(pred_spec, types.MulticlassPreds):
      raise TypeError('pred_spec must be a MulticlassPreds, received '
                      f'{type(pred_spec).__name__}')
    pred_idxs = classification_results.get_classifications(
        preds, pred_spec, config)
    # 'swapped' just means the prediction changed.
    is_swapped = [(pred_idxs[i] != pred_idxs[j]) for i, j in pairs]
    ret['swap_rate'] = np.mean(is_swapped)
    # Jensen-Shannon divergence, as a soft measure of prediction change.
    jsds = [
        scipy_distance.jensenshannon(preds[i], preds[j])**2 for i, j in pairs
    ]
    ret['mean_jsd'] = np.mean(jsds)
    return ret
class MulticlassPairedMetrics(ClassificationMetricsWrapper):
  def __init__(self):
    ClassificationMetricsWrapper.__init__(self, MulticlassPairedMetricsImpl())
class MultilabelMetrics(SimpleMetrics):
  """Metrics for assessing the performance of multi-label learning models."""
  def is_field_compatible(
      self, pred_spec: types.LitType, parent_spec: Optional[types.LitType]
  ) -> bool:
    """Determines the compatibility of a field with these metrics.
    Args:
      pred_spec: The field in the model's output spec containing the predicted
        labels, must be a `SparseMultilabelPreds` type.
      parent_spec: The field in the dataset containing the ground truth, must be
        a `SparseMultilabel` field.
    Returns:
      True if the pred_spec and parent_spec pair are compatible.
    """
    pred_suppported = isinstance(pred_spec, types.SparseMultilabelPreds)
    parent_supported = isinstance(parent_spec, types.SparseMultilabel)
    return pred_suppported and parent_supported
  def meta_spec(self) -> types.Spec:
    """Returns the Spec describing the computed metrics."""
    return {
        'exactmatch': types.MetricResult(
            best_value=types.MetricBestValue.HIGHEST,
            description=(
                'Multi-label accuracy is the exact match ratio; the proportion '
                'of exact matches between the predicted labels and the true '
                'labels across all examples. Closer to 1 is better.'
            ),
        ),
        'precision': types.MetricResult(
            best_value=types.MetricBestValue.HIGHEST,
            description=(
                'The mean proportion of correctly predicted labels out of all '
                'predicted labels across examples. Closer to 1 is better.'
            ),
        ),
        'recall': types.MetricResult(
            best_value=types.MetricBestValue.HIGHEST,
            description=(
                'The mean proportion of correctly predicted labels relative to '
                'the true labels across examples. Closer to 1 is better.'
            ),
        ),
        'f1': types.MetricResult(
            best_value=types.MetricBestValue.HIGHEST,
            description=(
                'The mean performance of the model (i.e., the harmonic mean of '
                'precision and recall) across examples. Closer to 1 is better.'
            ),
        ),
    }
  def compute(
      self,
      labels: Sequence[Sequence[str]],
      preds: Sequence[types.ScoredTextCandidates],
      label_spec: types.LitType,
      pred_spec: types.LitType,
      config: Optional[types.JsonDict] = None,
      indices: Optional[Sequence[types.ExampleId]] = None,
      metas: Optional[Sequence[JsonDict]] = None
  ) -> lit_components.MetricsDict:
    """Computes standard metrics for multi-label classification models.
    Args:
      labels: Ground truth against which predictions are compared.
      preds: The predictions made by the model.
      label_spec: A `SparseMultilabel` instance describing the types of elements
        in `labels`.
      pred_spec: A `SparseMultilabelPreds` instance describing the types of
        elements in `preds`.
      config: unused parameter from base class.
      indices: Unused list of IDs, aligned with `preds` and `labels`.
      metas: Unused list of LitMetadatas, aligned with `preds` and `labels`.
    Returns:
      A dict containing the accuracy (exact match ratio), precision, recall, and
      F1 score for the provided predictions given true labels.
    Raises:
      TypeError: If `label_spec` is not a `SparseMultilabel` instance or
        `pred_spec` is not a `SparseMultilabelPreds` instance.
      ValueError: If `labels` is not the same length as `preds`.
    """
    # TODO(b/271864674): Use this config dict to get user-defined thresholds
    del config, indices, metas  # unused in multi-label metrics, for now.
    if not labels or not preds:
      return {}
    num_labels = len(labels)
    num_preds = len(preds)
    if num_labels != num_preds:
      raise ValueError(
          'Must have exactly as many labels as predictions. Received '
          f'{num_labels} labels and {num_preds} preds.'
      )
    if not isinstance(label_spec, types.SparseMultilabel):
      raise TypeError(
          'label_spec must be a SparseMultilabel, received '
          f'{type(label_spec).__name__}'
      )
    if not isinstance(pred_spec, types.SparseMultilabelPreds):
      raise TypeError(
          'pred_spec must be a SparseMultilabelPreds, received '
          f'{type(pred_spec).__name__}'
      )
    # Learn the complete vocabulary of the possible labels
    if pred_spec.vocab:     # Try to get the labels from the model's output spec
      all_labels: list[Sequence[str]] = [pred_spec.vocab]
    elif label_spec.vocab:  # Or, try to get them from the dataset spec
      all_labels: list[Sequence[str]] = [label_spec.vocab]
    else:                   # Otherwise, derive them from the observed labels
      # WARNING: this is only correct for metrics like precision, recall, and
      # exact-match accuracy which do not depend on knowing the full label set.
      # For per-label accuracy this will give incorrect results if not all
      # labels are observed in a given sample.
      all_labels: list[Sequence[str]] = []
      all_labels.extend(labels)
      all_labels.extend([{l for l, _ in p} for p in preds])
    binarizer = _MultiLabelBinarizer()
    binarizer.fit(all_labels)
    # Next, extract the labels from the ScoredTextCandidates for binarization.
    pred_labels = [
        # TODO(b/271864674): Update this set comprehension to respect
        # user-defined margins from the config dict or pred_spec.threshold.
        {l for l, s in p if s is not None and s > 0.5} for p in preds
    ]
    # Transform the true and predicted labels into the binarized vector space.
    v_true = binarizer.transform(labels)
    v_pred = binarizer.transform(pred_labels)
    # Compute and return the metrics
    return {
        'exactmatch': sklearn_metrics.accuracy_score(v_true, v_pred),
        'precision': sklearn_metrics.precision_score(
            v_true, v_pred, average='samples'
        ),
        'recall': sklearn_metrics.recall_score(
            v_true, v_pred, average='samples'
        ),
        'f1': sklearn_metrics.f1_score(v_true, v_pred, average='samples'),
    }
class CorpusBLEU(SimpleMetrics):
  """Corpus BLEU score using SacreBLEU."""
  BLEU_SMOOTHING_VAL = 0.1
  def is_field_compatible(self, pred_spec: LitType,
                          parent_spec: Optional[LitType]) -> bool:
    """Return true if compatible with this field."""
    is_pred_comaptible = isinstance(
        pred_spec, (types.GeneratedText, types.GeneratedTextCandidates))
    is_parent_compatible = isinstance(parent_spec, types.StringLitType)
    return is_pred_comaptible and is_parent_compatible
  def meta_spec(self) -> dict[str, types.LitType]:
    return {
        'corpus_bleu': types.MetricResult(
            best_value=types.MetricBestValue.HIGHEST,
            description='BLEU score, a measure of text quality, over an entire '
                        'corpus. Closer to 1 is better.'),
        'corpus_bleu@1': types.MetricResult(
            best_value=types.MetricBestValue.HIGHEST,
            description='BLEU score, a measure of text quality, over an entire '
                        'corpus for the top predicted candidate. Closer to 1 '
                        'is better.'),
    }
  def compute(self,
              labels: Sequence[str],
              preds: Sequence[Union[str, types.ScoredTextCandidates]],
              label_spec: LitType,
              pred_spec: LitType,
              config: Optional[JsonDict] = None,
              indices: Optional[Sequence[types.ExampleId]] = None,
              metas: Optional[Sequence[JsonDict]] = None) -> dict[str, float]:
    """Compute CorpusBLEU score using the SacreBLEU library.
    Args:
      labels: Ground truth values for each prediction.
      preds: The models predicted values, aligned with `labels`.
      label_spec: Unused field spec from super class.
      pred_spec: A `GeneratedText` or `GeneratedTextCandidates` spec for the
        model's prediction field.
      config: Unused configuration dict inherited from super class.
      indices: Unused list of IDs, aligned with `preds` and `labels`.
      metas: Unused list of LitMetadatas, aligned with `preds` and `labels`.
    Returns:
      A dict containing the CorpusBLEU score for each prediction, stored in the
      `corpus_bleu` key if `pred_spec` is `GeneratedText` or the
      `corpus_bleu@1` key if `pred_spec` is `GeneratedTextCandidates`.
      If `preds` or `labels` are empty, returns an empty dict.
    Raises:
      TypeError: `pred_spec` is not `GeneratedText`/`GeneratedTextCandidates`.
        Note overriding the type information in the method signature will
        produce a signature mismatch error in PyType, see
        https://google.github.io/pytype/errors.html#signature-mismatch
    """
    del label_spec, config, indices, metas
    if not labels or not preds:
      return {}
    if not isinstance(pred_spec,
                      (types.GeneratedText, types.GeneratedTextCandidates)):
      raise TypeError('pred_spec must be a GeneratedText or '
                      'GeneratedTextCandidates, received '
                      f'{type(pred_spec).__name__}')
    name_suffix = ''
    if isinstance(pred_spec, types.GeneratedTextCandidates):
      preds = [types.GeneratedTextCandidates.top_text(v) for v in preds]
      name_suffix = '@1'
    bleu = sacrebleu.raw_corpus_bleu(preds, [labels], self.BLEU_SMOOTHING_VAL)
    return {'corpus_bleu' + name_suffix: bleu.score}
class RougeL(SimpleMetrics):
  """RougeL score for generation tasks."""
  def __init__(self, *args, **kw):
    super().__init__(*args, **kw)
    self._scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
  def _score(self, reference, prediction):
    return self._scorer.score(
        target=reference, prediction=prediction)['rougeL'].fmeasure
  def is_field_compatible(self, pred_spec: LitType,
                          parent_spec: Optional[LitType]) -> bool:
    """Return true if compatible with this field."""
    is_pred_comaptible = isinstance(
        pred_spec, (types.GeneratedText, types.GeneratedTextCandidates))
    is_parent_compatible = isinstance(parent_spec, types.StringLitType)
    return is_pred_comaptible and is_parent_compatible
  def meta_spec(self) -> dict[str, types.LitType]:
    return {
        'rougeL': types.MetricResult(
            best_value=types.MetricBestValue.HIGHEST,
            description='ROUGE score, a measure of text quality, for the '
                        'longest common subsequence in the text. Closer to 1 '
                        'is better.'),
        'rougeL@1': types.MetricResult(
            best_value=types.MetricBestValue.HIGHEST,
            description='ROUGE score, a measure of text quality, for the '
                        'longest common subsequence in the text for the top '
                        'predicted candidate. Closer to 1 is better.')
    }
  def compute(self,
              labels: Sequence[str],
              preds: Sequence[Union[str, types.ScoredTextCandidates]],
              label_spec: LitType,
              pred_spec: LitType,
              config: Optional[JsonDict] = None,
              indices: Optional[Sequence[types.ExampleId]] = None,
              metas: Optional[Sequence[JsonDict]] = None) -> dict[str, float]:
    """Compute the RougeL score using the RougeScorer library.
    Args:
      labels: Ground truth values for each prediction.
      preds: The models predicted values, aligned with `labels`.
      label_spec: Unused field spec from super class.
      pred_spec: A `GeneratedText` or `GeneratedTextCandidates` spec for the
        model's prediction field.
      config: Unused configuration dict inherited from super class.
      indices: The ID for each IndexedInput, aligned with `preds` and `labels`.
      metas: The metadata for each Input, aligned with `preds` and `labels`.
    Returns:
      A dict containing the RougeL score for each prediction, stored in the
      `rougeL` key if `pred_spec` is `GeneratedText` or the `rougeL@1` key if
      `pred_spec` is `GeneratedTextCandidates`. If `preds` or `labels` are
      empty, returns an empty dict.
    Raises:
      TypeError: `pred_spec` is not `GeneratedText`/`GeneratedTextCandidates`.
        Note overriding the type information in the method signature will
        produce a signature mismatch error in PyType, see
        https://google.github.io/pytype/errors.html#signature-mismatch
    """
    del label_spec, config, indices
    if not labels or not preds:
      return {}
    if not isinstance(pred_spec,
                      (types.GeneratedText, types.GeneratedTextCandidates)):
      raise TypeError('pred_spec must be a GeneratedText or '
                      'GeneratedTextCandidates, received '
                      f'{type(pred_spec).__name__}')
    name_suffix = ''
    if isinstance(pred_spec, types.GeneratedTextCandidates):
      preds = [types.GeneratedTextCandidates.top_text(v) for v in preds]
      name_suffix = '@1'
    scores = list(map(self._score, labels, preds))
    return {'rougeL' + name_suffix: np.mean(scores)}
class BinaryConfusionMetricsImpl(SimpleMetrics):
  """Confusion matrix values for binary classification."""
  def get_all_metrics(self,
                      y_true: Sequence[int],
                      y_pred: Sequence[int],
                      vocab: Sequence[str],
                      null_idx: Optional[int] = None):
    # Filter out unlabeled examples before calculating metrics.
    labeled_example_indices = [
        index for index, y in enumerate(y_true) if y != -1
    ]
    y_true = [y_true[i] for i in labeled_example_indices]
    y_pred = [y_pred[i] for i in labeled_example_indices]
    # Return binary confusion matrix entries.
    ret = collections.OrderedDict()
    matrix = sklearn_metrics.confusion_matrix(y_true, y_pred, labels=[0, 1])
    ret['TN'] = matrix[0][0]
    ret['FP'] = matrix[0][1]
    ret['FN'] = matrix[1][0]
    ret['TP'] = matrix[1][1]
    return ret
  def meta_spec(self) -> dict[str, types.LitType]:
    return {
        'FN': types.MetricResult(
            best_value=types.MetricBestValue.ZERO,
            description='The number of false negatives predicted by the model. '
                        'Closer to 0 is better.'),
        'FP': types.MetricResult(
            best_value=types.MetricBestValue.ZERO,
            description='The number of false positives predicted by the model. '
                        'Closer to 0 is better.'),
        'TN': types.MetricResult(
            best_value=types.MetricBestValue.HIGHEST,
            description='The number of true negatives predicted by the model. '
                        'Higher is better.'),
        'TP': types.MetricResult(
            best_value=types.MetricBestValue.HIGHEST,
            description='The number of true positives predicted by the model. '
                        'Hugher is better.'),
    }
  def is_field_compatible(self, pred_spec: LitType,
                          parent_spec: Optional[LitType]) -> bool:
    """Return true if binary classification with ground truth."""
    if not (isinstance(pred_spec, types.MulticlassPreds) and
            isinstance(parent_spec, types.CategoryLabel)):
      return False
    class_spec = cast(types.MulticlassPreds, pred_spec)
    return len(class_spec.vocab) == 2
  def compute(self,
              labels: Sequence[str],
              preds: Sequence[np.ndarray],
              label_spec: LitType,
              pred_spec: LitType,
              config: Optional[JsonDict] = None,
              indices: Optional[Sequence[types.ExampleId]] = None,
              metas: Optional[Sequence[JsonDict]] = None) -> dict[str, float]:
    """Compute binary classification metrics using Scikit-Learn.
    Args:
      labels: Ground truth class label for each prediction.
      preds: The models predicted class label, aligned with `labels`.
      label_spec: Unused field spec from the super class.
      pred_spec: A `MulticlassPreds` spec for the model's prediction field.
      config: Optional margins for computing classification results.
      indices: Unused list of IDs, aligned with `preds` and `labels`.
      metas: Unused list of LitMetadatas, aligned with `preds` and `labels`.
    Returns:
      A dict containing the true negative (`TN`), false positive (`FP`), false
      negative (`FN`), and true positive (`TN`) scores. If `labels` or `preds`
      is empty, returns an empty dict.
    Raises:
      TypeError: `pred_spec` is not `MulticlassPreds`. Note overriding the type
        information in the method signature will produce a signature mismatch
        error in PyType, see
        https://google.github.io/pytype/errors.html#signature-mismatch
    """
    del label_spec, indices, metas  # Unused; get vocab from pred_spec.
    if not labels or not preds:
      return {}
    if not isinstance(pred_spec, types.MulticlassPreds):
      raise TypeError('pred_spec must be a MulticlassPreds, received '
                      f'{type(pred_spec).__name__}')
    label_idxs = [
        pred_spec.vocab.index(label) if label in pred_spec.vocab else -1
        for label in labels
    ]
    # Get classifications using possible margin value to control threshold
    # of positive classification.
    pred_idxs = classification_results.get_classifications(
        preds, pred_spec, config)
    return self.get_all_metrics(
        label_idxs, pred_idxs, pred_spec.vocab, null_idx=pred_spec.null_idx)
class BinaryConfusionMetrics(ClassificationMetricsWrapper):
  def __init__(self):
    ClassificationMetricsWrapper.__init__(self, BinaryConfusionMetricsImpl())
class ExactMatchMetrics(SimpleMetrics):
  """Exact match metrics for text generations."""
  def meta_spec(self) -> types.Spec:
    """Returns the spec for the Exact Match metrics.
    Returns
      A dict of MetricResult specs for the metrics computed by this class.
    """
    return {
        'exactmatch': types.MetricResult(
            best_value=types.MetricBestValue.HIGHEST,
            description='The proportion of exact matches. Closer to 1 is '
                        'better.',
        ),
        'exactmatch@1': types.MetricResult(
            best_value=types.MetricBestValue.HIGHEST,
            description='The proportion of exact matches for the top predicted '
                        'candidate. Closer to 1 is better.',
        )
    }
  def is_field_compatible(self, pred_spec: LitType,
                          parent_spec: Optional[LitType]) -> bool:
    """Return true if compatible with this field.
    Args:
      pred_spec: The field in the model's output spec containing the generated
          text, must be of type GeneratedText or GeneratedTextCandidates.
      parent_spec: The field in the dataset containing the ground truth, must be
          of type MultiSegmentAnnotations or TextSegment.
    Returns:
      True if the pred_spec and parent_spec pair are compatible.
    """
    pred_supported = isinstance(pred_spec, (types.GeneratedText,
                                            types.GeneratedTextCandidates))
    parent_supported = isinstance(parent_spec, (types.TextSegment,
                                                types.MultiSegmentAnnotations))
    return pred_supported and parent_supported
  def compute(
      self,
      labels: Sequence[Any],
      preds: Sequence[Any],
      label_spec: types.LitType,
      pred_spec: types.LitType,
      config: Optional[JsonDict] = None,
      indices: Optional[Sequence[types.ExampleId]] = None,
      metas: Optional[Sequence[JsonDict]] = None) -> lit_components.MetricsDict:
    """Compute exact matches between labels and predictions.
    Args:
      labels: Ground truth against which predictions are compared.
      preds: The predictions made by the model.
      label_spec: A `MultiSegmentAnnotations` or `TextSegment` spec  describing
          the types of elements in `labels`.
      pred_spec: A `GeneratedText` or `GeneratedTextCandidates` spec describing
          the types of elements in `preds`.
      config: unused parameter from base class.
      indices: Unused list of IDs, aligned with `preds` and `labels`.
      metas: Unused list of LitMetadatas, aligned with `preds` and `labels`.
    Returns:
      A dict containing the proportion of exact matches in the predictions,
      stored in the `exactmatch` key if `pred_spec` is `GeneratedText` or the
      `exactmatch@1` key if `pred_spec` is `GeneratedTextCandidates`.
    """
    del config, indices, metas
    if not labels or not preds:
      return {}
    if not isinstance(label_spec,
                      (types.TextSegment, types.MultiSegmentAnnotations)):
      raise TypeError('label_spec must be a TextSegment or '
                      'MultiSegmentAnnotations, received '
                      f'{type(pred_spec).__name__}')
    if not isinstance(pred_spec,
                      (types.GeneratedText, types.GeneratedTextCandidates)):
      raise TypeError('pred_spec must be a GeneratedText or '
                      'GeneratedTextCandidates, received '
                      f'{type(pred_spec).__name__}')
    if isinstance(pred_spec, types.GeneratedTextCandidates):
      texts = [types.GeneratedTextCandidates.top_text(v) for v in preds]
      name_suffix = '@1'
    else:
      texts = preds
      name_suffix = ''
    matches = 0
    for label, pred in zip(labels, texts):
      if isinstance(label_spec, types.MultiSegmentAnnotations):
        # MultiSegmentAnnotations means that labels is a
        # Sequence[api.dtypes.AnnotationCluster].
        answers = [annotation.label for annotation in label]
        if any(pred == answer for answer in answers):
          matches += 1
      else:
        # Otherwise, labels is a Sequence[str].
        if pred == label:
          matches += 1
    return {f'exactmatch{name_suffix}': matches/len(preds)}

================
File: lit_nlp/components/minimal_targeted_counterfactuals_test.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Tests for lit_nlp.components.minimal_targeted_counterfactuals."""
from unittest import mock
from absl.testing import absltest
from lit_nlp.api import dataset as lit_dataset
from lit_nlp.api import model as lit_model
from lit_nlp.api import types as lit_types
from lit_nlp.components import minimal_targeted_counterfactuals
from lit_nlp.lib import caching
import numpy as np
import scipy.special as scipy_special
ANIMALS = ['unknown', 'elephant', 'ant', 'whale', 'seal']
class ClassificationTestDataset(lit_dataset.Dataset):
  """A test dataset for classification testing."""
  def spec(self) -> lit_types.Spec:
    return {
        'size': lit_types.CategoryLabel(vocab=['small', 'medium', 'large']),
        'weight': lit_types.Scalar(),
        'legs': lit_types.Boolean(),
        'description': lit_types.StringLitType(),
        'animal': lit_types.CategoryLabel(vocab=ANIMALS),
    }
  @property
  def examples(self) -> list[lit_types.JsonDict]:
    return [
        {
            'size': 'small',
            'weight': 0.01,
            'legs': True,
            'description': 'small but strong',
            'animal': 'ant'
        },
        {
            'size': 'large',
            'weight': 0.8,
            'legs': True,
            'description': 'has a trunk',
            'animal': 'elephant'
        },
        {
            'size': 'medium',
            'weight': 0.2,
            'legs': False,
            'description': 'makes strange sounds',
            'animal': 'seal'
        },
        {
            'size': 'large',
            'weight': 2.5,
            'legs': False,
            'description': 'excellent water displacement',
            'animal': 'whale'
        },
    ]
class ClassificationTestModel(lit_model.BatchedModel):
  """A test model for testing tabular hot-flips on classification tasks."""
  def __init__(self, dataset: lit_dataset.Dataset) -> None:
    super().__init__()
    self._dataset = dataset
  def max_minibatch_size(self, **unused) -> int:
    return 2
  def input_spec(self) -> lit_types.Spec:
    return {
        'size': lit_types.CategoryLabel(vocab=['small', 'medium', 'large']),
        'weight': lit_types.Scalar(),
        'legs': lit_types.Boolean(),
        'description': lit_types.StringLitType(),
    }
  def output_spec(self) -> lit_types.Spec:
    return {
        'preds':
            lit_types.MulticlassPreds(
                parent='animal', vocab=ANIMALS, null_idx=0)
    }
  def predict_minibatch(
      self, inputs: list[lit_types.JsonDict], **unused
  ) -> list[lit_types.JsonDict]:
    output = []
    def predict_example(ex: lit_types.JsonDict) -> lit_types.JsonDict:
      """Returns model predictions for a given example.
      The method uses the animal test dataset as the ground truth. The method
      compares the given example features to the dataset features for all
      animals. The closer the feature values are, the higher the contribution to
      the corresponding class logit is.
      Args:
        ex: an example to run prediction for.
      Returns:
        The softmax values for the animal class prediction.
      """
      # Logit values for ['unknown', 'elephant', 'ant', 'whale'].
      logits = np.zeros((len(ANIMALS),))
      for db_rec in self._dataset.examples:
        animal_index = ANIMALS.index(db_rec['animal'])
        for field_name in self._dataset.spec():
          if ex[field_name] is None or db_rec[field_name] is None:
            continue
          if field_name == 'animal':
            continue
          field_spec_value = self._dataset.spec()[field_name]
          if (isinstance(field_spec_value, lit_types.CategoryLabel) or
              isinstance(field_spec_value, lit_types.Boolean)) and (
                  ex[field_name] == db_rec[field_name]):
            logits[animal_index] += 1
          if isinstance(field_spec_value, lit_types.Scalar):
            logits[animal_index] += 1.0 - abs(ex[field_name] -
                                              db_rec[field_name])
      return scipy_special.softmax(logits)
    for example in inputs:
      output.append({'preds': predict_example(example)})
    return output
class RegressionTestDataset(lit_dataset.Dataset):
  """A test dataset for regression testing."""
  def spec(self) -> lit_types.Spec:
    return {
        'x_1': lit_types.Scalar(),
        'x_2': lit_types.Scalar(),
        'y': lit_types.Scalar(),
    }
  @property
  def examples(self) -> list[lit_types.JsonDict]:
    return [
        {
            'x_1': 0.0,
            'x_2': 0.0,
            'y': 0.0
        },
        {
            'x_1': 0.5,
            'x_2': 0.4,
            'y': 1.0
        },
    ]
class RegressionTestModel(lit_model.BatchedModel):
  """A test model for testing tabular hot-flips on regression tasks."""
  def max_minibatch_size(self, **unused) -> int:
    return 2
  def input_spec(self) -> lit_types.Spec:
    return {
        'x_1': lit_types.Scalar(),
        'x_2': lit_types.Scalar(),
    }
  def output_spec(self) -> lit_types.Spec:
    return {'score': lit_types.RegressionScore(parent='y')}
  def predict_minibatch(
      self, inputs: list[lit_types.JsonDict], **unused
  ) -> list[lit_types.JsonDict]:
    output = []
    def predict_example(ex: lit_types.JsonDict) -> lit_types.JsonDict:
      x1 = ex['x_1']
      x2 = ex['x_2']
      return 2 * x1**2 + x2
    for example in inputs:
      output.append({'score': predict_example(example)})
    return output
class ClassificationTabularMtcTest(absltest.TestCase):
  """Tests tabular hot-flips on classification tasks."""
  def setUp(self):
    super().setUp()
    dataset = lit_dataset.IndexedDataset(
        base=ClassificationTestDataset(), id_fn=caching.input_hash)
    self._dataset = dataset
    self._model = ClassificationTestModel(self._dataset)
    self._gen = minimal_targeted_counterfactuals.TabularMTC()
    self._example = {
        'size': 'large',
        'weight': 1.2,
        'legs': False,
        'description': 'big water animal',
        'animal': 'whale'
    }
    self._config = {
        'Prediction key': 'preds',
        'dataset_name': 'classification_test_dataset'
    }
  def test_test_model(self):
    """Tests the tests model predict method."""
    dataset = ClassificationTestDataset()
    model = ClassificationTestModel(dataset)
    preds = list(model.predict(dataset.examples))
    self.assertEqual(np.argmax(preds[0]['preds']), 2)
    self.assertEqual(np.argmax(preds[1]['preds']), 1)
    self.assertEqual(np.argmax(preds[2]['preds']), 4)
    self.assertEqual(np.argmax(preds[3]['preds']), 3)
  def test_prediction_key_required(self):
    """Tests the case when the client doesn't specify the prediction key."""
    self._config['Prediction key'] = ''
    with self.assertRaisesRegex(ValueError,
                                'Please provide the prediction key'):
      self._gen.generate(
          example=self._example,
          model=self._model,
          dataset=self._dataset,
          config=self._config)
  def test_incorrect_prediction_key(self):
    """Tests the case when the client specifies a key that doesn't exist."""
    self._config['Prediction key'] = 'wrong_key'
    with self.assertRaisesRegex(ValueError, 'Invalid prediction key'):
      self._gen.generate(
          example=self._example,
          model=self._model,
          dataset=self._dataset,
          config=self._config)
  def test_unsupported_model(self):
    """Tests the case when the passed model is not supported."""
    mocked_model = mock.MagicMock()
    output_spec = {'preds': lit_types.ImageBytes}
    mocked_model.output_spec = mock.MagicMock(return_value=output_spec)
    with self.assertRaisesRegex(
        ValueError, 'Only classification and regression models are supported'):
      self._gen.generate(
          example=self._example,
          model=mocked_model,
          dataset=self._dataset,
          config=self._config)
  def test_no_model(self):
    """Tests the case when no model is passed."""
    with self.assertRaisesRegex(ValueError,
                                'Please provide a model for this generator'):
      self._gen.generate(
          example=self._example,
          model=None,
          dataset=self._dataset,
          config=self._config)
  def test_max_number_of_records(self):
    """Tests that a client can specify a desired number of flips to return."""
    self._config['Number of examples'] = '2'
    result = self._gen.generate(
        example=self._example,
        model=self._model,
        dataset=self._dataset,
        config=self._config)
    self.assertLen(result, 2)
  def test_text_fields_equal_to_target(self):
    """Tests that non-scalar non-categorical features has correct value.
    The values of non-scalar, non-categorical features should be the same as in
    the input example.
    """
    output = self._gen.generate(
        example=self._example,
        model=self._model,
        dataset=self._dataset,
        config=self._config)
    s = {o['description'] for o in output}
    self.assertLen(s, 1)
    self.assertIn('big water animal', s)
  def test_mtc_prediction_is_argmax(self):
    output = self._gen.generate(
        example=self._example,
        model=self._model,
        dataset=self._dataset,
        config=self._config)
    y_actual = output[0]['animal']
    y_expected = self._predict_and_return_argmax_label(output[0])
    self.assertEqual(y_actual, y_expected)
  def test_output_is_counterfactuals(self):
    """Tests that the returned values are indeed counterfactuals."""
    output = self._gen.generate(
        example=self._example,
        model=self._model,
        dataset=self._dataset,
        config=self._config)
    self.assertGreaterEqual(len(output), 1)
    target_prediction = self._predict_and_return_argmax_label(self._example)
    for cf_example in output:
      cf_prediction = self._predict_and_return_argmax_label(cf_example)
      self.assertNotEqual(cf_prediction, target_prediction)
  def test_config_spec(self):
    """Tests that the generator returns spec with correct fields."""
    spec = self._gen.config_spec()
    self.assertIn('Number of examples', spec)
    self.assertIn('Maximum number of columns to change', spec)
    self.assertIn('Regression threshold', spec)
    self.assertIn('Prediction key', spec)
  def test_example_field_is_none(self):
    """Tests the case when a feature is assigned None value."""
    self._example['weight'] = None
    output = self._gen.generate(
        example=self._example,
        model=self._model,
        dataset=self._dataset,
        config=self._config)
    self.assertNotEmpty(output)
  def _predict_and_return_argmax_label(self, example):
    """Given an example, returns the index of the top prediction."""
    model_out = self._model.predict([example])
    softmax = list(model_out)[0]['preds']
    argmax = np.argmax(softmax)
    return self._model.output_spec()['preds'].vocab[argmax]
class RegressionTabularMtcTest(absltest.TestCase):
  """Tests tabular hot-flips with regression models."""
  def setUp(self):
    super().setUp()
    dataset = lit_dataset.IndexedDataset(
        base=RegressionTestDataset(), id_fn=caching.input_hash)
    self._dataset = dataset
    self._model = RegressionTestModel()
    self._gen = minimal_targeted_counterfactuals.TabularMTC()
    self._example = {'x_1': 1.0, 'x_2': 1.0}
    self._config = {
        'Prediction key': 'score',
        'dataset_name': 'regression_test_dataset'
    }
  def test_test_regression_model(self):
    """Tests the predict method of the regression model."""
    model = RegressionTestModel()
    example = {'x_1': 3, 'x_2': 2}
    pred = list(model.predict([example]))[0]
    self.assertEqual(pred['score'], 20)
  def test_output_is_below_threshold_counterfactuals(self):
    """Tests the case when the target prediction is above the threshold.
    If the target (reference) prediction is above the decision boundary
    threshold, the predictions for all counterfactuals should be below the
    threshold.
    """
    threshold = 2.8
    self._config['Regression threshold'] = str(threshold)
    self._example = {'x_1': 1.0, 'x_2': 1.0}
    output = self._gen.generate(
        example=self._example,
        model=self._model,
        dataset=self._dataset,
        config=self._config)
    target_score = self._predict_and_return_score(self._example)
    self.assertGreaterEqual(target_score, threshold)
    self.assertNotEmpty(output)
    for cf_example in output:
      cf_score = self._predict_and_return_score(cf_example)
      self.assertLess(cf_score, threshold)
  def test_output_is_above_threshold_counterfactuals(self):
    """Tests the case when the target prediction is below the threshold.
    If the target (reference) prediction is below the decision boundary
    threshold, the predictions for all counterfactuals should be above or equal
    the threshold.
    """
    threshold = 0.1
    self._config['Regression threshold'] = str(threshold)
    self._example = {'x_1': 0.0, 'x_2': -5.0}
    output = self._gen.generate(
        example=self._example,
        model=self._model,
        dataset=self._dataset,
        config=self._config)
    target_score = self._predict_and_return_score(self._example)
    self.assertLess(target_score, threshold)
    self.assertNotEmpty(output)
    for cf_example in output:
      cf_score = self._predict_and_return_score(cf_example)
      self.assertGreaterEqual(cf_score, threshold)
  def test_no_counterfactuals_found(self):
    """Tests the case when there no counterfactuals in the database."""
    threshold = 4.0
    self._config['Regression threshold'] = str(threshold)
    self._example = {'x_1': 1.0, 'x_2': 1.0}
    output = self._gen.generate(
        example=self._example,
        model=self._model,
        dataset=self._dataset,
        config=self._config)
    self.assertEmpty(output)
  def test_max_num_of_changed_columns(self):
    """Tests the client can set the number of features that can be changed."""
    self._config['Regression threshold'] = '0.25'
    self._config['Maximum number of columns to change'] = '1'
    self._example = {'x_1': 0.3, 'x_2': 0.3}
    output_1 = self._gen.generate(
        example=self._example,
        model=self._model,
        dataset=self._dataset,
        config=self._config)
    self._config['Maximum number of columns to change'] = '2'
    output_2 = self._gen.generate(
        example=self._example,
        model=self._model,
        dataset=self._dataset,
        config=self._config)
    self.assertNotEmpty(output_1)
    self.assertNotEmpty(output_2)
    self.assertGreater(len(output_2), len(output_1))
  def test_parent_field_updated(self):
    threshold = 0.8
    self._config['Regression threshold'] = str(threshold)
    self._example = {'x_1': 0.0, 'x_2': 0.0}
    output = self._gen.generate(
        example=self._example,
        model=self._model,
        dataset=self._dataset,
        config=self._config)
    y_actual = output[0]['y']
    y_expected = self._predict_and_return_score(output[0])
    self.assertEqual(y_actual, y_expected)
  def _predict_and_return_score(self, example):
    """Given an example, returns the regression score."""
    model_out = self._model.predict([example])
    return list(model_out)[0]['score']
if __name__ == '__main__':
  absltest.main()

================
File: lit_nlp/components/minimal_targeted_counterfactuals.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Minimal Targeted Counterfactual generator for tabular datasets.
A Minimal Targeted Counterfactual is defined as a counterfactual input that is
acquired by manipulating the original input features in order to obtain a
different prediction.
In contrast to (1), this implementation does not require access to the model
gradients. Instead, it uses examples from a dataset in order to find a set of
closest counterfactuals. Next, the closest counterfactuals and the original
input are linearly interpolated in order to find even closer counterfactuals.
Only scalar features are used in the interpolation search.
Only scalar and categorical features are used for the search of counterfactuals.
The features of other types are always assigned the value of the original input.
The implementation supports both classification and regression models. In case
of a regression model, the caller can specify a threshold value that
represents a decision boundary between the 'true' and 'false' values. If the
threshold is not specified then value 0.0 is used as the threshold.
The implementation aims to find the minimal set of counterfactuals. The set is
minimal if for each counterfactual in the set there exist no other
counterfactuals in the set that differ in the same features or subset of these
features and having smaller distance (cost) to the reference example. See (2)
for more details. The distances between two data points are measured as
described in (3).
The caller of the generator can set the maximum number of features that can
differ from the original example in order for the counterfactual to qualify.
In addition, the caller can specify the desired number of counterfactuals to be
returned.
References:
(1) HotFlip: White-Box Adversarial Examples for Text Classification.
    Javid Ebrahimi, Anyi Rao, Daniel Lowd, Dejing Dou
    ACL 2018.
    https://www.aclweb.org/anthology/P18-2006/
(2) Local Explanations via Necessity and Sufficiency: Unifying Theory and
    Practice. David Watson, Limor Gultchin, Ankur Taly, Luciano Floridi.
    UAI 2021.
    https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3825636
(3) The What-If Tool: Interactive Probing of Machine Learning Models.
    James Wexler, Mahima Pushkarna, Tolga Bolukbasi, Martin Wattenberg,
    Fernanda Vigas, Jimbo Wilson
    IEEE 2020.
    https://ieeexplore.ieee.org/abstract/document/8807255
"""
import collections
from typing import Any, cast, Optional
from absl import logging
from lit_nlp.api import components as lit_components
from lit_nlp.api import dataset as lit_dataset
from lit_nlp.api import model as lit_model
from lit_nlp.api import types as lit_types
from lit_nlp.components import cf_utils
from lit_nlp.lib import caching
from lit_nlp.lib import utils
import numpy as np
JsonDict = lit_types.JsonDict
PREDICTION_KEY = 'Prediction key'
NUM_EXAMPLES_KEY = 'Number of examples'
NUM_EXAMPLES_DEFAULT = 5
MAX_FLIPS_KEY = 'Maximum number of columns to change'
MAX_FLIPS_DEFAULT = 3
REGRESSION_THRESH_KEY = 'Regression threshold'
REGRESSION_THRESH_DEFAULT = 0.0
# The maximum number of examples that will be searched per combination.
MAX_EXAMPLES_PER_COMBINATION = 50
class TabularMTC(lit_components.Generator):
  """The Minimal Targeted Counterfactual generator for tabular data.
  This generator looks for counterfactuals that are close to the original input
  without using gradients. First, the generator finds counterfactual examples
  from the dataset. Then, it changes features in the original input towards the
  counterfactual input until the decision boundary is found. During the search
  for a closer counterfactual, the algorithm manipulates different subsets of
  features, while keeping other features frozen. Thus, the algorithm tries to
  find the closest counterfactual that differs only in a single feature value,
  two feature values, etc., up to the selected "Maximum number of columns to
  change" configuration parameter. Only scalar and categorical features are
  changed during the search. The features of other types are always assigned the
  values of the original input.
  The implementation supports both classification and regression models. In case
  of a regression model, the "Regression threshold" parameter sets a decision
  boundary between the 'true' and 'false' values.
  The generator weakly guarantees that the result set of counterfactuals
  is minimal, i.e. for every counterfactual in the set, there exist no other
  counterfactual that differs in the same or smaller set of features and is
  closer to the original input.
  """
  # Hold dataset statistics such as standard deviation for scalar features and
  # the probability of having the same value as other random example for
  # categorical features. The outer dictionary key is the name of a dataset.
  # The nested dictionary key is the name of the example field. The value is
  # either the corresponding standard deviation for a scalar feature or the
  # probability for a categorical feature.
  _datasets_stats: dict[str, dict[str, float]] = {}
  def is_compatible(self, model: lit_model.Model,
                    dataset: lit_dataset.Dataset) -> bool:
    supported_input_types = (lit_types.Boolean, lit_types.CategoryLabel,
                             lit_types.Scalar)
    dataset_fields = set(
        utils.find_spec_keys(dataset.spec(), supported_input_types))
    model_in_fields = set(
        utils.find_spec_keys(model.input_spec(), supported_input_types))
    model_out_fields = utils.spec_contains(
        model.output_spec(),
        (lit_types.MulticlassPreds, lit_types.RegressionScore))
    intersection = dataset_fields.intersection(model_in_fields)
    return bool(intersection) and model_out_fields
  def generate(self,
               example: JsonDict,
               model: lit_model.Model,
               dataset: lit_dataset.Dataset,
               config: Optional[JsonDict] = None) -> list[JsonDict]:
    # Perform validation and retrieve configuration.
    if not model:
      raise ValueError('Please provide a model for this generator.')
    config = config or {}
    num_examples = int(config.get(NUM_EXAMPLES_KEY, NUM_EXAMPLES_DEFAULT))
    max_flips = int(config.get(MAX_FLIPS_KEY, MAX_FLIPS_DEFAULT))
    pred_key = config.get(PREDICTION_KEY, '')
    regression_thresh = float(
        config.get(REGRESSION_THRESH_KEY, REGRESSION_THRESH_DEFAULT))
    dataset_name = config.get('dataset_name')
    if not dataset_name:
      raise ValueError('The dataset name must be in the config.')
    output_spec = model.output_spec()
    if not pred_key:
      raise ValueError('Please provide the prediction key.')
    if pred_key not in output_spec:
      raise ValueError('Invalid prediction key.')
    if (not (isinstance(output_spec[pred_key], lit_types.MulticlassPreds) or
             isinstance(output_spec[pred_key], lit_types.RegressionScore))):
      raise ValueError(
          'Only classification and regression models are supported')
    # Calculate dataset statistics if it has never been calculated. The
    # statistics include such information as 'standard deviation' for scalar
    # features and probabilities for categorical features.
    if dataset_name not in self._datasets_stats:
      self._datasets_stats[dataset_name] = self._calculate_stats(dataset)
    # Find predicted class of the original example.
    original_pred = list(model.predict([example]))[0]
    # Find dataset examples that are flips.
    filtered_examples = self._filter_ds_examples(  # pytype: disable=wrong-arg-types  # enable-nested-classes
        dataset=dataset,
        model=model,
        reference_output=original_pred,
        pred_key=pred_key,
        regression_thresh=regression_thresh)
    supported_field_names = self._find_all_fields_to_consider(
        ds_spec=dataset.spec(),
        model_input_spec=model.input_spec(),
        example=example)
    candidates: list[JsonDict] = []
    # Iterate through all possible feature combinations.
    combs = utils.find_all_combinations(supported_field_names, 1, max_flips)
    for comb in combs:
      # Sort all dataset examples with respect to the given combination.
      sorted_examples = self._sort_and_filter_examples(
          examples=filtered_examples,
          ref_example=example,
          fields=comb,
          dataset=dataset,
          dataset_name=dataset_name)
      if not sorted_examples:
        continue
      # As an optimization trick, check whether the farthest example is a flip.
      # If it is not a flip then skip the current combination of features.
      # This optimization makes the minimum set guarantees weaker but
      # significantly improves the search speed.
      flip = self._find_hot_flip(
          ref_example=example,
          ds_example=sorted_examples[-1],
          features_to_consider=comb,
          model=model,
          target_pred=original_pred,
          pred_key=pred_key,
          dataset=dataset,
          interpolate=False,
          regression_threshold=regression_thresh)
      if not flip:
        logging.info('Skipped combination %s', comb)
        continue
      # Iterate through the sorted examples until the first flip is found.
      # TODO(b/204200758): improve performance by batching the predict requests.
      for ds_example in sorted_examples:
        flip = self._find_hot_flip(
            ref_example=example,
            ds_example=ds_example,
            features_to_consider=comb,
            model=model,
            target_pred=original_pred,
            pred_key=pred_key,
            dataset=dataset,
            interpolate=True,
            regression_threshold=regression_thresh)
        if flip:
          self._add_if_not_strictly_worse(
              example=flip,
              other_examples=candidates,
              ref_example=example,
              dataset=dataset,
              dataset_name=dataset_name,
              model=model)
          break
      if len(candidates) >= num_examples:
        break
    # Calculate distances for the found hot flips.
    candidate_tuples = []
    for flip_example in candidates:
      distance, diff_fields = self._calculate_L1_distance(
          example_1=example,
          example_2=flip_example,
          dataset=dataset,
          dataset_name=dataset_name,
          model=model)
      if distance > 0:
        candidate_tuples.append((distance, diff_fields, flip_example))
    # Order the dataset entries based on the distance to the given example.
    candidate_tuples.sort(key=lambda e: e[0])
    if len(candidate_tuples) > num_examples:
      candidate_tuples = candidate_tuples[0:num_examples]
    # e[2] contains the hot-flip examples in the distances list of tuples.
    return [e[2] for e in candidate_tuples]
  def _filter_ds_examples(
      self,
      dataset: lit_dataset.IndexedDataset,
      model: lit_model.Model,
      reference_output: JsonDict,
      pred_key: str,
      regression_thresh: Optional[float] = None) -> list[JsonDict]:
    """Reads all dataset examples and returns only those that are flips."""
    if not isinstance(dataset, lit_dataset.IndexedDataset):
      raise ValueError(
          'Only indexed datasets are currently supported by the TabularMTC'
          'generator.')
    examples = list(dataset.examples)
    filtered_examples = []
    preds = model.predict(examples)
    # Find all DS examples that are flips with respect to the reference example.
    for example, pred in zip(examples, preds):
      flip = cf_utils.is_prediction_flip(
          cf_output=pred,
          orig_output=reference_output,
          output_spec=model.output_spec(),
          pred_key=pred_key,
          regression_thresh=regression_thresh)
      if flip:
        candidate_example = dict(example)
        self._find_dataset_parent_and_set(
            model_output_spec=model.output_spec(),
            pred_key=pred_key,
            dataset_spec=dataset.spec(),
            example=candidate_example,
            predicted_value=pred[pred_key])
        filtered_examples.append(candidate_example)
    return filtered_examples
  def config_spec(self) -> lit_types.Spec:
    return {
        NUM_EXAMPLES_KEY:
            lit_types.Scalar(
                min_val=1, max_val=20, default=NUM_EXAMPLES_DEFAULT, step=1),
        MAX_FLIPS_KEY:
            lit_types.Scalar(
                min_val=1, max_val=10, default=MAX_FLIPS_DEFAULT, step=1),
        PREDICTION_KEY:
            lit_types.SingleFieldMatcher(
                spec='output', types=['MulticlassPreds', 'RegressionScore']),
        REGRESSION_THRESH_KEY:
            lit_types.TextSegment(default=str(REGRESSION_THRESH_DEFAULT)),
    }
  def _find_hot_flip(
      self,
      ref_example: JsonDict,
      ds_example: JsonDict,
      features_to_consider: list[str],
      model: lit_model.Model,
      target_pred: JsonDict,
      pred_key: str,
      dataset: lit_dataset.Dataset,
      interpolate: bool,
      regression_threshold: Optional[float] = None,
  ) -> Optional[JsonDict]:
    """Finds a hot-flip example for a given target example and DS example.
    Args:
      ref_example: target example for which the counterfactuals should be found.
      ds_example: a dataset example that should be used as a starting point for
        the search.
      features_to_consider: the list of feature keys that can be changed during
        the search.
      model: model to use for getting predictions.
      target_pred: model prediction that corresponds to `ref_example`.
      pred_key: the name of the field in model predictions that contains the
        prediction value for the counterfactual search.
      dataset: a dataset object that contains `ds_example`.
      interpolate: if True, the method tries to find a closer counterfactual
        using interpolation.
      regression_threshold: the threshold to use if `model` is a regression
        model. This parameter is ignored for classification models.
    Returns:
      A hot-flip counterfactual that satisfy the criteria.
    """
    # All features other than `features_to_consider` should be assigned the
    # value of the target example.
    candidate_example = dict(ds_example)
    for field_name in ref_example:
      if (field_name not in features_to_consider and
          field_name in model.input_spec()):
        candidate_example[field_name] = ref_example[field_name]
    flip, predicted_value = self._is_flip(
        model=model,
        cf_example=candidate_example,
        orig_output=target_pred,
        pred_key=pred_key,
        regression_thresh=regression_threshold)
    if not flip:
      return None
    # Find closest flip by moving scalar values closer to the target.
    closest_flip = None
    if interpolate:
      closest_flip = self._find_closer_flip_using_interpolation(
          ref_example, candidate_example, target_pred, pred_key, model, dataset,
          regression_threshold)
    # If we found a closer flip through interpolation then use it,
    # otherwise use the previously found flip.
    if closest_flip is not None:
      return closest_flip
    else:
      self._find_dataset_parent_and_set(
          model_output_spec=model.output_spec(),
          pred_key=pred_key,
          dataset_spec=dataset.spec(),
          example=candidate_example,
          predicted_value=predicted_value)
      return candidate_example
  def _find_closer_flip_using_interpolation(
      self,
      ref_example: JsonDict,
      known_flip: JsonDict,
      target_pred: JsonDict,
      pred_key: str,
      model: lit_model.Model,
      dataset: lit_dataset.Dataset,
      regression_threshold: Optional[float] = None,
      max_attempts: int = 4) -> Optional[JsonDict]:
    """Looks for the decision boundary between two examples using interpolation.
    The method searches for a flip that is closer to the `target example` than
    `known_flip`. The method performs the binary search by interpolating scalar
    values.
    Args:
      ref_example: an example for which the flip is searched.
      known_flip: an example that represents a known flip.
      target_pred: the model prediction at `ref_example`.
      pred_key: the named of the field inside `target_pred` that holds the
        prediction value.
      model: model to use for running predictions.
      dataset: dataset that contains `known_flip`.
      regression_threshold: threshold to use for regression models.
      max_attempts: number of binary search attempts.
    Returns:
      The counterfactual (flip) if found; 'None' otherwise.
    """
    min_alpha = 0.0
    max_alpha = 1.0
    closest_flip = None
    input_spec = model.input_spec()
    has_scalar = False
    for _ in range(max_attempts):
      # Interpolate the scalar values using binary search.
      current_alpha = (min_alpha + max_alpha) / 2
      candidate = dict(known_flip)
      for field in ref_example:
        if (field in candidate and field in input_spec and
            isinstance(input_spec[field], lit_types.Scalar) and
            candidate[field] is not None and ref_example[field] is not None):
          candidate[field] = known_flip[field] * (
              1 - current_alpha) + ref_example[field] * current_alpha
          has_scalar = True
      # The interpolation makes sense only for scalar values. If there are no
      # scalar fields that can be interpolated then terminate the search.
      if not has_scalar:
        return None
      flip, predicted_value = self._is_flip(
          model=model,
          cf_example=candidate,
          orig_output=target_pred,
          pred_key=pred_key,
          regression_thresh=regression_threshold)
      if flip:
        self._find_dataset_parent_and_set(
            model_output_spec=model.output_spec(),
            pred_key=pred_key,
            dataset_spec=dataset.spec(),
            example=candidate,
            predicted_value=predicted_value)
        closest_flip = candidate
        min_alpha = current_alpha
      else:
        max_alpha = current_alpha
    return closest_flip
  def _is_flip(self,
               model: lit_model.Model,
               cf_example: JsonDict,
               orig_output: JsonDict,
               pred_key: str,
               regression_thresh: Optional[float] = None) -> tuple[bool, Any]:
    cf_output = list(model.predict([cf_example]))[0]
    feature_predicted_value = cf_output[pred_key]
    return cf_utils.is_prediction_flip(
        cf_output=cf_output,
        orig_output=orig_output,
        output_spec=model.output_spec(),
        pred_key=pred_key,
        regression_thresh=regression_thresh), feature_predicted_value
  def _find_all_fields_to_consider(
      self,
      ds_spec: lit_dataset.Spec,
      model_input_spec: lit_model.Spec,
      example: Optional[JsonDict] = None) -> list[str]:
    overlapping = set(ds_spec.keys()).intersection(model_input_spec.keys())
    supported = [f for f in overlapping if self._is_supported(ds_spec[f])]
    if example:
      supported = [f for f in supported if example[f] is not None]
    return supported
  def _calculate_stats(self, dataset: lit_dataset.Dataset) -> dict[str, float]:
    # Iterate through all examples in the dataset and store column values
    # in individual lists to facilitate future computation.
    field_values = {}
    spec = dataset.spec()
    supported_fields = [name for name in spec if self._is_supported(spec[name])]
    for example in dataset.examples:
      for field_name in supported_fields:
        if example[field_name] is None:
          continue
        if field_name not in field_values:
          field_values[field_name] = []
        field_values[field_name].append(example[field_name])
    # Compute the necessary statistics: standard deviation for scalar fields and
    # probability of having same value for categorical and categorical fields.
    field_stats = {}
    for field_name, values in field_values.items():
      field_spec = spec[field_name]
      if self._is_scalar(field_spec):
        field_stats[field_name] = self._calculate_std_dev(values)
      elif self._is_categorical(field_spec):
        field_stats[field_name] = self._calculate_categorical_prob(values)
      else:
        assert False, 'Should never be reached.'
    # Cache the stats for the given dataset.
    return field_stats
  def _calculate_std_dev(self, values: list[float]) -> float:
    return np.std(values)
  def _calculate_categorical_prob(self, values: list[float]) -> float:
    """Returns probability of two values from the list having the same value."""
    counts = collections.Counter(values)
    prob = 0.0
    for bucket in counts:
      prob += (counts[bucket] / len(values))**2
    return prob
  def _calculate_L1_distance(
      self,
      example_1: JsonDict,
      example_2: JsonDict,
      dataset: lit_dataset.Dataset,
      dataset_name: str,
      model: Optional[lit_model.Model] = None,
      field_names: Optional[list[str]] = None) -> tuple[float, list[str]]:
    """Calculates L1 distance between two input examples.
    Only categorical and scalar example features are considered. For categorical
    features, the distance is calculated as the probability of the feature
    having the same for two random (with replacement) examples. For scalar
    features, the unit of distance is equal to the standard deviation of all
    feature values.
    Only features that are in the intersection of the model and dataset features
    are considered.
    If a feature value of either of the examples is None, such feature is
    ignored in distance calculation and the name of the feature is not included
    in the result feature list (see Returns description).
    Args:
      example_1: a first example to measure distance for.
      example_2: a second example to measure distance for.
      dataset: a dataset that contains the information about the feature types.
      dataset_name: name of the dataset.
      model: a model that contains the information about the input feature
        types.
      field_names: if set then the distance calculation only considers these
        fields.
    Returns:
      A tuple that contains the L1 distance and the list of features that were
      used in the distance calculation. The list of features will only contain
    """
    assert model or field_names
    distance = 0
    diff_fields = []
    if field_names is None:
      assert model
      field_names = self._find_all_fields_to_consider(
          ds_spec=dataset.spec(), model_input_spec=model.input_spec())
    for field_name in field_names:
      field_spec = dataset.spec()[field_name]
      field_stats = self._datasets_stats[dataset_name]
      assert self._is_supported(field_spec)
      assert field_name in field_stats, f'{field_name}, {field_stats.keys()}'
      if example_1[field_name] == example_2[field_name]:
        continue
      if (example_1[field_name] is None) or (example_2[field_name] is None):
        continue
      diff_fields.append(field_name)
      if self._is_scalar(field_spec):
        std_dev = field_stats[field_name]
        if std_dev != 0:
          distance += abs(example_1[field_name] -
                          example_2[field_name]) / std_dev
      else:
        same_prob = field_stats[field_name]
        distance += same_prob
    return distance, diff_fields
  def _is_categorical(self, field_spec: lit_types.LitType) -> bool:
    """Checks whether a field is of categorical type."""
    return (isinstance(field_spec, lit_types.Boolean) or
            isinstance(field_spec, lit_types.CategoryLabel))
  def _is_scalar(self, field_spec: lit_types.LitType) -> bool:
    """Checks whether a field is of scalar type."""
    return isinstance(field_spec, lit_types.Scalar)
  def _is_supported(self, field_spec: lit_types.LitType) -> bool:
    """Checks whether a field should be used in distance calculation."""
    return self._is_scalar(field_spec) or self._is_categorical(field_spec)
  def _find_dataset_parent(self, model_output_spec: lit_types.Spec,
                           pred_key: str,
                           dataset_spec: lit_types.Spec) -> Optional[str]:
    """Finds a field in dataset that is a parent of the model prediction."""
    output_feature = model_output_spec[pred_key]
    parent = getattr(output_feature, 'parent', None)
    if parent not in dataset_spec:
      return None
    return parent
  def _find_dataset_parent_and_set(self, model_output_spec: lit_types.Spec,
                                   pred_key: str, dataset_spec: lit_types.Spec,
                                   example: dict[str, Any],
                                   predicted_value: Any) -> None:
    """Finds example parent field and assigns prediction value to it."""
    parent = self._find_dataset_parent(model_output_spec, pred_key,
                                       dataset_spec)
    if parent is not None:
      if isinstance(model_output_spec[pred_key], lit_types.MulticlassPreds):
        argmax = np.argmax(predicted_value)
        pred_field = cast(lit_types.MulticlassPreds,
                          model_output_spec[pred_key])
        label = pred_field.vocab[argmax]
        example[parent] = label
      else:
        example[parent] = predicted_value
  def _sort_and_filter_examples(self, examples: list[JsonDict],
                                ref_example: JsonDict, fields: list[str],
                                dataset: lit_dataset.Dataset,
                                dataset_name: str) -> list[JsonDict]:
    # Keep only those examples which field values are different from the
    # reference example.
    filtered_examples = []
    for example in examples:
      should_keep = True
      for field in fields:
        if example[field] == ref_example[field]:
          should_keep = False
          break
      if should_keep:
        filtered_examples.append(example)
    if not filtered_examples:
      return []
    # Deduplicate examples.
    dedup_hashes = set()
    dedup_examples = []
    for example in filtered_examples:
      h = self._create_hash(example, fields)
      if h not in dedup_hashes:
        dedup_examples.append(example)
        dedup_hashes.add(h)
    if len(dedup_examples) > MAX_EXAMPLES_PER_COMBINATION:
      dedup_examples = dedup_examples[:MAX_EXAMPLES_PER_COMBINATION]
    # Calculate distances with respect to the reference example taking into
    # consideration only the given fields.
    distances = []  # type: list[float]
    for example in dedup_examples:
      distance, _ = self._calculate_L1_distance(
          example_1=example,
          example_2=ref_example,
          dataset=dataset,
          dataset_name=dataset_name,
          field_names=fields)
      distances.append(distance)
    # Sort the filtered examples based on the distances.
    sorted_tuples = list(
        zip(*sorted(zip(dedup_examples, distances), key=lambda e: e[1])))[0]
    return list(sorted_tuples)
  def _add_if_not_strictly_worse(self, example: JsonDict,
                                 other_examples: list[JsonDict],
                                 ref_example: JsonDict,
                                 dataset: lit_dataset.Dataset,
                                 dataset_name: str, model: lit_model.Model):
    for other_example in other_examples:
      is_worse = self._is_strictly_worse(
          example_1=example,
          example_2=other_example,
          ref_example=ref_example,
          dataset=dataset,
          dataset_name=dataset_name,
          model=model)
      if is_worse:
        return
    other_examples.append(example)
  def _is_strictly_worse(self, example_1: JsonDict, example_2: JsonDict,
                         ref_example: JsonDict, dataset: lit_dataset.Dataset,
                         dataset_name: str, model: lit_model.Model) -> bool:
    """Calculates whether example_1 is strictly worse (or eq) than example_2."""
    # An example is strictly worse than other example if it differs in
    # the same or more features and has higher distance to the reference
    # example.
    ex_1_dist, ex_1_features = self._calculate_L1_distance(
        example_1=example_1,
        example_2=ref_example,
        dataset=dataset,
        dataset_name=dataset_name,
        model=model)
    ex_2_dist, ex_2_features = self._calculate_L1_distance(
        example_1=example_2,
        example_2=ref_example,
        dataset=dataset,
        dataset_name=dataset_name,
        model=model)
    if ex_1_dist < ex_2_dist:
      return False
    if set(ex_2_features).issubset(set(ex_1_features)):
      return True
    return False
  def _create_hash(self, example: JsonDict, fields: list[str]) -> str:
    json_map = lit_types.Input(
        {k: v for k, v in example.items() if k in fields})
    return caching.input_hash(json_map)

================
File: lit_nlp/components/model_salience.py
================
"""Interpreter component for models that return their own salience."""
from typing import Optional, Union
from lit_nlp.api import components
from lit_nlp.api import dataset as lit_dataset
from lit_nlp.api import dtypes
from lit_nlp.api import model as lit_model
from lit_nlp.api import types
from lit_nlp.lib import utils
JsonDict = types.JsonDict
# Salience types are for feature-wise salience, token-wise salience,
# sequence-based salience, or a string (base64 encoded image string) for image
# salience.
SalienceTypes = Union[dtypes.FeatureSalience, dtypes.TokenSalience,
                      dtypes.SequenceSalienceMap, str]
_SALIENCE_FIELD_TYPES = (
    types.FeatureSalience, types.ImageSalience, types.TokenSalience,
    types.SequenceSalience)
class ModelSalience(components.Interpreter):
  """Model-provided salience interpreter."""
  def __init__(self, models: dict[str, lit_model.Model]):
    # Populate saliency fields in meta spec based on saliency returned by
    # model output specs.
    self._spec = {}
    for model_name, model in models.items():
      fields = self.find_fields(model)
      for field in fields:
        self._spec[f'{model_name}:{field}'] = model.output_spec()[field]
  def find_fields(self, model: lit_model.Model) -> list[str]:
    return utils.find_spec_keys(model.output_spec(), _SALIENCE_FIELD_TYPES)
  def _run_single(self, ex: JsonDict, mo: JsonDict, fields: list[str],
                  model: lit_model.Model) -> dict[str, SalienceTypes]:
    # Extract the saliency outputs from the model.
    result = {}
    for sal_field in fields:
      result[sal_field] = mo[sal_field]
    return result
  def run(self,
          inputs: list[JsonDict],
          model: lit_model.Model,
          dataset: lit_dataset.Dataset,
          model_outputs: Optional[list[JsonDict]] = None,
          config: Optional[JsonDict] = None) -> Optional[list[JsonDict]]:
    del dataset
    del config
    fields = self.find_fields(model)
    # Run model, if needed.
    if model_outputs is None:
      model_outputs = list(model.predict(inputs))
    assert len(model_outputs) == len(inputs)
    return [
        self._run_single(ex, mo, fields, model)
        for ex, mo in zip(inputs, model_outputs)
    ]
  def is_compatible(self, model: lit_model.Model,
                    dataset: lit_dataset.Dataset) -> bool:
    del dataset  # Unused as salience comes from the model.
    return utils.spec_contains(model.output_spec(), _SALIENCE_FIELD_TYPES)
  def meta_spec(self) -> types.Spec:
    return self._spec

================
File: lit_nlp/components/nearest_neighbors_test.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Tests for lit_nlp.components.gradient_maps."""
from absl.testing import absltest
from lit_nlp.api import dataset as lit_dataset
from lit_nlp.api import model as lit_model
from lit_nlp.api import types as lit_types
from lit_nlp.components import nearest_neighbors
from lit_nlp.lib import caching  # for hash id fn
from lit_nlp.lib import testing_utils
import numpy as np
JsonDict = lit_types.JsonDict
class TestModelNearestNeighbors(lit_model.BatchedModel):
  """Implements lit.Model interface for nearest neighbors.
     Returns the same output for every input.
  """
  # LIT API implementation
  def max_minibatch_size(self, **unused_kw):
    return 3
  def input_spec(self):
    return {'segment': lit_types.TextSegment}
  def output_spec(self):
    return {'probas': lit_types.MulticlassPreds(
        parent='label',
        vocab=['0', '1'],
        null_idx=0),
            'input_embs': lit_types.TokenEmbeddings(align='tokens'),
            }
  def predict_minibatch(self, inputs: list[JsonDict], **kw):
    embs = [np.array([0, 0, 0, 0]),
            np.array([1, 1, 1, 0]),
            np.array([5, 8, -10, 0])]
    probas = np.array([0.2, 0.8])
    return [{'probas': probas, 'input_embs': embs[i]}
            for i, _ in enumerate(inputs)]
class NearestNeighborTest(absltest.TestCase):
  def setUp(self):
    super(NearestNeighborTest, self).setUp()
    self.nearest_neighbors = nearest_neighbors.NearestNeighbors()
  def test_run_nn(self):
    examples = [
        {
            'segment': 'a',
            '_id': 'a'
        },
        {
            'segment': 'b',
            '_id': 'b'
        },
        {
            'segment': 'c',
            '_id': 'c'
        },
    ]
    model = TestModelNearestNeighbors()
    dataset = lit_dataset.IndexedDataset(id_fn=caching.input_hash,
                                         examples=examples)
    config = {
        'embedding_name': 'input_embs',
        'num_neighbors': 2,
    }
    result = self.nearest_neighbors.run(
        dataset.examples[1:2], model, dataset, config=config
    )
    expected = {'nearest_neighbors': [
        {'id': '1', 'nn_distance': 0.0},
        {'id': '0', 'nn_distance': 1.7320508075688772}]}
    self.assertLen(result, 1)
    testing_utils.assert_deep_almost_equal(self, expected, result[0])
if __name__ == '__main__':
  absltest.main()

================
File: lit_nlp/components/nearest_neighbors.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Finds the k nearest neighbors to an input embedding."""
from collections.abc import Sequence
import dataclasses
from typing import Optional
from lit_nlp.api import components as lit_components
from lit_nlp.api import dataset as lit_dataset
from lit_nlp.api import model as lit_model
from lit_nlp.api import types
from lit_nlp.lib import utils
import numpy as np
from scipy.spatial import distance
JsonDict = types.JsonDict
IndexedInput = types.IndexedInput
Spec = types.Spec
@dataclasses.dataclass
class NearestNeighborsConfig(object):
  """Config options for Nearest Neighbors component."""
  embedding_name: str = ''
  num_neighbors: Optional[int] = 10
  use_input: Optional[bool] = False
_NN_CONFIG_FIELDS = [
    field.name for field in dataclasses.fields(NearestNeighborsConfig)]
class NearestNeighbors(lit_components.Interpreter):
  """Computes nearest neighbors of an example embedding.
  Required Model Output:
    - Embeddings (`emb_layer`) to return the input embeddings
        for a layer
  """
  def is_compatible(
      self, model: lit_model.Model, dataset: lit_dataset.Dataset
  ) -> bool:
    dataset_embs = utils.spec_contains(dataset.spec(), types.Embeddings)
    model_out_embs = utils.spec_contains(model.output_spec(), types.Embeddings)
    return dataset_embs or model_out_embs
  def run(
      self,
      inputs: Sequence[JsonDict],
      model: lit_model.Model,
      dataset: lit_dataset.Dataset,
      model_outputs: Optional[list[JsonDict]] = None,
      config: Optional[JsonDict] = None) -> Optional[list[JsonDict]]:
    """Finds the nearest neighbors of the example specified in the config.
    Args:
      inputs: the dataset example to find nearest neighbors for.
      model: the model being explained.
      dataset: the dataset which the current examples belong to.
      model_outputs: optional model outputs from calling model.predict(inputs).
      config: a config which should specify:
        {
          'num_neighbors': [the number of nearest neighbors to return]
          'embedding_name': [the name of the embedding field to use]
          'use_input': [Optional boolean if the embedding comes from input data]
        }
    Raises:
      KeyError: Cannot find the embedding field in the relevant spec
      TypeError: `config` argument not provided
      ValueError: indexed_inputs requires exactly one input
    Returns:
      A JsonDict containing the a list of num_neighbors nearest neighbors,
      where each has the example id and distance from the main example.
    """
    if not config:
      raise TypeError('config must be provided')
    if not (isinstance(dataset, lit_dataset.IndexedDataset)):
      raise TypeError('Nearest neighbors requires an IndexedDataset to track '
                      'uniqueness by ID.')
    nnconf = NearestNeighborsConfig(**{
        k: v for k, v in config.items() if k in _NN_CONFIG_FIELDS
    })
    # TODO(lit-dev): Add support for selecting nearest neighbors of a set.
    if len(inputs) != 1:
      raise ValueError('indexed_inputs must contain exactly 1 example, found '
                       f'{len(inputs)}.')
    if nnconf.use_input:
      if not dataset.spec().get(nnconf.embedding_name):
        raise KeyError('Could not find embeddings field, '
                       f'{nnconf.embedding_name} in dataset spec')
      # If using input values, then treat inputs as outputs instead of running
      # the model.
      dataset_outputs = dataset.examples
      example_outputs = inputs
    else:
      if not model.output_spec().get(nnconf.embedding_name):
        raise KeyError('Could not find embeddings field, '
                       f'{nnconf.embedding_name} in model output spec')
      dataset_outputs = list(model.predict(dataset.examples))
      example_outputs = list(model.predict(inputs))
    example_output = example_outputs[0]
    # <float32>[emb_size]
    dataset_embs = [output[nnconf.embedding_name] for output in dataset_outputs]
    dataset_embs = [emb.astype(np.float32) for emb in dataset_embs]
    example_embs = [example_output[nnconf.embedding_name]]
    example_embs = [emb.astype(np.float32) for emb in example_embs]
    distances = distance.cdist(example_embs, dataset_embs)[0]
    sorted_indices = np.argsort(distances)
    k = nnconf.num_neighbors
    k_nearest_neighbors = [
        {'id': dataset.examples[original_index]['_id'],
         'nn_distance': distances[original_index]
         } for original_index in sorted_indices[:k]]
    return [{'nearest_neighbors': k_nearest_neighbors}]

================
File: lit_nlp/components/pca_test.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Tests for lit_nlp.components.pca."""
from absl.testing import absltest
from lit_nlp.components import pca
from lit_nlp.lib import testing_utils
import numpy as np
class PcaTest(absltest.TestCase):
  def test_fit_transform(self):
    pca_model = pca.PCAModel(n_components=3)
    # Make dummy embeddings.
    n = 100
    inputs = testing_utils.fake_projection_input(n, 10)
    outputs = pca_model.fit_transform(inputs)
    outputs_list = list(outputs)
    # Check that the output dict keys are correct.
    self.assertIn('z', outputs_list[0])
    # Check that the _fitted flag has been flipped.
    self.assertTrue(pca_model._fitted)
    # Check that the output shape is correct.
    output_np = np.array([o['z'] for o in outputs_list])
    shape = output_np.shape
    expected_shape = (n, 3)
    self.assertFalse(np.any(np.iscomplex(output_np)))
    self.assertEqual(shape, expected_shape)
  def test_predict_minibatch(self):
    pca_model = pca.PCAModel(n_components=3)
    # Test falsy return value when pca hasn't been initialized.
    num_dims = 10
    inputs = testing_utils.fake_projection_input(1, num_dims)
    output = pca_model.predict_minibatch(inputs)
    self.assertEqual(list(output)[0]['z'], [0, 0, 0])
    # Make dummy embeddings to warm start.
    pca_model.fit_transform(testing_utils.fake_projection_input(100, num_dims))
    # Test that we can now predict a minibatch.
    output = pca_model.predict_minibatch(inputs)
    output_shape = np.array(list(output)[0]['z']).shape
    self.assertFalse(np.any(np.iscomplex(output)))
    self.assertEqual(output_shape, (3,))
if __name__ == '__main__':
  absltest.main()

================
File: lit_nlp/components/pca.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Implementation of PCA as a dimensionality reduction model."""
from absl import logging
from lit_nlp.api import model
from lit_nlp.lib import utils
import numpy as np
class PCAModel(model.ProjectorModel):
  """LIT model API implementation for PCA."""
  def __init__(self, **pca_kw):
    self._fitted = False
    self._num_components = pca_kw["n_components"]
  ##
  # Training methods
  def fit_transform(self, inputs):
    x_input = [i["x"] for i in inputs]
    if not x_input:
      return []
    x_train = np.stack(x_input)
    logging.info("PCA input x_train: %s", str(x_train.shape))
    # Center columns around mean.
    self._mean = np.mean(x_train, 0)
    x_train = x_train - self._mean
    # Find PCA projection.
    cov = np.dot(x_train.T, x_train) / x_train.shape[0]
    evals, evecs = np.linalg.eig(cov)
    # Sort by strongest eigenvalues
    key = np.argsort(evals)[::-1][:self._num_components]
    self._evecs = evecs[:, key]
    self._fitted = True
    # Apply PCA projection
    zs = np.dot(x_train, self._evecs)
    return ({"z": utils.coerce_real(z)} for z in zs)
  ##
  # LIT model API
  def predict_minibatch(self, inputs, **unused_kw):
    if not self._fitted:
      return ({"z": [0, 0, 0]} for _ in inputs)
    x = np.stack([i["x"] for i in inputs])
    x = x - self._mean
    zs = np.dot(x, self._evecs)
    return ({"z": utils.coerce_real(z)} for z in zs)

================
File: lit_nlp/components/pdp_test.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Tests for lit_nlp.components.pdp."""
from absl.testing import absltest
from lit_nlp.api import dataset as lit_dataset
from lit_nlp.api import model as lit_model
from lit_nlp.api import types as lit_types
from lit_nlp.components import pdp
from lit_nlp.lib import testing_utils
_MODEL_INPUT_SPEC: lit_types.Spec = {
    'num': lit_types.Scalar(),
    'cats': lit_types.CategoryLabel(vocab=['One', 'None']),
}
class TestRegressionPdp(lit_model.BatchedModel):
  def input_spec(self):
    return _MODEL_INPUT_SPEC
  def output_spec(self):
    return {'score': lit_types.RegressionScore()}
  def predict_minibatch(self, inputs: list[lit_types.JsonDict], **kw):
    return [
        {'score': i['num'] + (1 if i['cats'] == 'One' else 0)} for i in inputs
    ]
class TestClassificationPdp(lit_model.BatchedModel):
  def input_spec(self):
    return _MODEL_INPUT_SPEC
  def output_spec(self):
    return {'probas': lit_types.MulticlassPreds(vocab=['0', '1'])}
  def predict_minibatch(self, inputs: list[lit_types.JsonDict], **kw):
    def pred(i):
      val = (i['num'] / 100) + (.5 if i['cats'] == 'One' else 0)
      return {'probas': [1 - val, val]}
    return [pred(i) for i in inputs]
class PdpTest(absltest.TestCase):
  def setUp(self):
    super(PdpTest, self).setUp()
    self.pdp = pdp.PdpInterpreter()
    self.reg_model = TestRegressionPdp()
    self.class_model = TestClassificationPdp()
    examples: list[lit_types.JsonDict] = [
        {
            'num': 1,
            'cats': 'One',
        },
        {
            'num': 10,
            'cats': 'None',
        },
        {
            'num': 5,
            'cats': 'One',
        },
    ]
    self.dataset = lit_dataset.IndexedDataset(
        spec=_MODEL_INPUT_SPEC, examples=examples
    )
  def test_regression_num(self):
    config = {'feature': 'num'}
    result = self.pdp.run(
        self.dataset.examples[:1], self.reg_model, self.dataset, config=config
    )
    expected = {1.0: 2.0, 2.0: 3.0, 3.0: 4.0, 4.0: 5.0, 5.0: 6.0, 6.0: 7.0,
                7.0: 8.0, 8.0: 9.0, 9.0: 10.0, 10.0: 11.0}
    testing_utils.assert_deep_almost_equal(self, result['score'], expected)
  def test_provided_range(self):
    config = {'feature': 'num', 'range': [0, 9]}
    result = self.pdp.run(
        self.dataset.examples[:1], self.reg_model, self.dataset, config=config
    )
    expected = {0.0: 1.0, 1.0: 2.0, 2.0: 3.0, 3.0: 4.0, 4.0: 5.0, 5.0: 6.0,
                6.0: 7.0, 7.0: 8.0, 8.0: 9.0, 9.0: 10.0}
    testing_utils.assert_deep_almost_equal(self, result['score'], expected)
  def test_regression_cat(self):
    config = {'feature': 'cats'}
    result = self.pdp.run(
        self.dataset.examples[:1], self.reg_model, self.dataset, config=config
    )
    expected = {'One': 2.0, 'None': 1.0}
    testing_utils.assert_deep_almost_equal(self, result['score'], expected)
  def test_class_num(self):
    config = {'feature': 'num'}
    result = self.pdp.run(
        self.dataset.examples[:1], self.class_model, self.dataset, config=config
    )
    expected = {1.0: [0.49, 0.51], 2.0: [0.48, 0.52], 3.0: [0.47, 0.53],
                4.0: [0.46, 0.54], 5.0: [0.45, 0.55], 6.0: [0.44, 0.56],
                7.0: [0.43, 0.57], 8.0: [0.42, 0.58], 9.0: [0.41, 0.59],
                10.0: [0.4, 0.6]}
    testing_utils.assert_deep_almost_equal(self, result['probas'], expected)
  def test_classification_cat(self):
    config = {'feature': 'cats'}
    result = self.pdp.run(
        self.dataset.examples[:1], self.class_model, self.dataset, config=config
    )
    expected = {'One': [0.49, 0.51], 'None': [0.99, 0.01]}
    testing_utils.assert_deep_almost_equal(self, result['probas'], expected)
  def test_multiple_inputs(self):
    config = {'feature': 'num'}
    result = self.pdp.run(
        self.dataset.examples[:2], self.reg_model, self.dataset, config=config
    )
    expected = {1.0: 1.5, 2.0: 2.5, 3.0: 3.5, 4.0: 4.5, 5.0: 5.5, 6.0: 6.5,
                7.0: 7.5, 8.0: 8.5, 9.0: 9.5, 10.0: 10.5}
    testing_utils.assert_deep_almost_equal(self, result['score'], expected)
if __name__ == '__main__':
  absltest.main()

================
File: lit_nlp/components/pdp.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Partial dependence plot interpreter.
Runs a model on a set of edited examples to see the effect of changing a
specified feature on a set of examples. Returns a dict of prediction results for
different feature values, for each classification and regression head.
The front-end can display these as charts.
"""
import functools
from typing import cast, Optional
from absl import logging
from lit_nlp.api import components as lit_components
from lit_nlp.api import dataset as lit_dataset
from lit_nlp.api import model as lit_model
from lit_nlp.api import types
from lit_nlp.lib import utils
import numpy as np
_SUPPORTED_PRED_TYPES = (types.MulticlassPreds, types.RegressionScore)
class PdpInterpreter(lit_components.Interpreter):
  """Partial Dependence Plot interpreter."""
  def is_compatible(self, model: lit_model.Model,
                    dataset: lit_dataset.Dataset) -> bool:
    del dataset  # Unused by PDP
    return utils.spec_contains(model.output_spec(), _SUPPORTED_PRED_TYPES)
  @functools.lru_cache()
  def get_vals_to_test(self, feat, dataset: lit_dataset.IndexedDataset):
    """Returns all values to try for a given feature."""
    numeric = isinstance(dataset.spec()[feat], types.Scalar)
    # Get categorical values to test from the spec vocab.
    if not numeric:
      return cast(types.MulticlassPreds, dataset.spec()[feat]).vocab or []
    # Create 10 values from min to max of a numeric feature to test and store
    # it for later use as well.
    nums = [ex[feat] for ex in dataset.examples]
    min_val = np.min(nums)
    max_val = np.max(nums)
    return np.linspace(min_val, max_val, 10)
  def run(self,
          inputs: list[types.JsonDict],
          model: lit_model.Model,
          dataset: lit_dataset.Dataset,
          model_outputs: Optional[list[types.JsonDict]] = None,
          config: Optional[types.JsonDict] = None):
    """Create PDP chart info using provided inputs.
    Args:
      inputs: sequence of inputs, following model.input_spec()
      model: optional model to use to generate new examples.
      dataset: dataset which the current examples belong to.
      model_outputs: optional precomputed model outputs
      config: optional runtime config.
    Returns:
      A dict of alternate feature values to model outputs. The model outputs
      will be a number for regression models and a list of numbers for
      multiclass models.
    Raises:
      KeyError: `config` does not have a value for `feature`
      TypeError: `config` is missing
    """
    if not config:
      raise TypeError('config must be provided')
    feature = config.get('feature')
    if not feature:
      raise KeyError('Config must have a "feature" field')
    pred_keys = utils.find_spec_keys(model.output_spec(), _SUPPORTED_PRED_TYPES)
    if not pred_keys:
      logging.warning('PDP did not find any supported output fields.')
      return None
    provided_range = config.get('range', [])
    edited_outputs = {pred_key: {} for pred_key in pred_keys}
    # If a range was provided, use that to create the possible values.
    vals_to_test = (
        np.linspace(provided_range[0], provided_range[1], 10)
        if len(provided_range) == 2
        else self.get_vals_to_test(feature, dataset))
    # If no specific inputs provided, use the entire dataset.
    inputs_to_use = inputs if inputs else dataset.examples
    # For each alternate value for a given feature.
    for new_val in vals_to_test:
      # Create copies of all provided inputs with the value replaced.
      edited_inputs = [
          utils.make_modified_input(inp, {feature: new_val}, 'PDP')
          for inp in inputs_to_use
      ]
      # Run prediction on the altered inputs.
      outputs = list(model.predict(edited_inputs))
      # Store the mean of the prediction for the alternate value.
      for pred_key in pred_keys:
        field_spec = model.output_spec().get(pred_key)
        if isinstance(field_spec, types.RegressionScore):
          edited_outputs[pred_key][new_val] = np.mean(
              [output[pred_key] for output in outputs])
        else:
          edited_outputs[pred_key][new_val] = np.mean(
              [output[pred_key] for output in outputs], axis=0)
    return edited_outputs

================
File: lit_nlp/components/projection.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Server-side dimensionality-reduction implementation.
This file implements machinery to manage dimensionality reduction models, such
as UMAP or PCA. Unlike most other LIT components, these projections need to be
learned on a particular dataset. This can take from a few seconds to a few
minutes, so for interactive use we want to cache both the projection and the
projected datapoints.
We implement this two-level caching with three classes:
- ProjectorModel simply wraps the projection model into a LIT Model, and
  provides training methods.
- ProjectionInterpreter implements the LIT Interpreter interface, and provides
  example-level caching.
- ProjectionManager implements the LIT Interpreter interface, and provides
  config-based caching of ProjectionInterpreter instances.
In most cases, the LIT server should interface with ProjectionManager so that
new configurations can be explored interactively (at the cost of re-training
projections).
"""
from collections.abc import Hashable, Sequence
import threading
from typing import Optional
from absl import logging
from lit_nlp.api import components as lit_components
from lit_nlp.api import dataset as lit_dataset
from lit_nlp.api import model as lit_model
from lit_nlp.api import types
from lit_nlp.lib import caching
JsonDict = types.JsonDict
IndexedInput = types.IndexedInput
Spec = types.Spec
class ProjectionInterpreter(lit_components.Interpreter):
  """Interpreter API implementation for dimensionality reduction model."""
  def __init__(
      self,
      model: lit_model.Model,
      inputs: Sequence[JsonDict],
      model_outputs: Optional[list[JsonDict]],
      projector: lit_model.ProjectorModel,
      field_name: str,
      name: str,
  ):
    self._projector = caching.CachingModelWrapper(projector, name=name)
    self._field_name = field_name
    # Train on the given examples
    self._run(model, inputs, model_outputs, do_fit=True)
  def is_compatible(
      self, model: lit_model.Model, dataset: lit_dataset.Dataset
  ) -> bool:
    del dataset, model  # Unused as field and model come through constructor
    return self._field_name in self._projector.output_spec()
  def convert_input(self, inp: JsonDict, model_output: JsonDict) -> JsonDict:
    """Convert inputs, preserving metadata for caching."""
    return {"x": model_output[self._field_name], "_id": inp.get("_id")}
  def _run(self,
           model: lit_model.Model,
           inputs: Sequence[JsonDict],
           model_outputs: Optional[Sequence[JsonDict]] = None,
           do_fit=False):
    # Run model, if needed.
    if model_outputs is None:
      model_outputs = list(model.predict(inputs))
    assert len(model_outputs) == len(inputs)
    converted_inputs = list(map(self.convert_input, inputs, model_outputs))
    if do_fit:
      return self._projector.fit_transform(converted_inputs)
    else:
      return self._projector.predict(converted_inputs)
  def run(self,
          inputs: list[JsonDict],
          model: lit_model.Model,
          dataset: lit_dataset.Dataset,
          model_outputs: Optional[list[JsonDict]] = None,
          config: Optional[JsonDict] = None):
    """Run this component, given a model and input(s)."""
    del dataset   # Unused - Examples passed to constructor instead.
    # If using input values, then treat inputs as outputs instead of running
    # the model.
    if config and config.get("use_input"):
      model_outputs = inputs
    return self._run(model, inputs, model_outputs, do_fit=False)
def _key_from_dict(d) -> Hashable:
  """Convert nested dict into a frozen, hashable structure usable as a key."""
  if isinstance(d, dict):
    return frozenset((k, _key_from_dict(v)) for k, v in d.items())
  elif isinstance(d, (list, tuple)):
    return tuple(map(_key_from_dict, d))
  else:
    return d
class ProjectionManager(lit_components.Interpreter):
  """Manager for multiple ProjectionInterpreter instances.
  Presents a standard "Interpreter" interface so that client code can treat
  this as an ordinary stateless component.
  The config is used to uniquely identify the projection instance, and a new
  instance is created and fit if not found.
  Config must contain the following fields:
  - field_name: name of embedding field (in model output)
  - (optional) proj_kw: config for the underlying ProjectorModel
  We also recommend including the model name and dataset name in the key, but
  this is not explicitly enforced.
  """
  def __init__(self, model_class: type[lit_model.ProjectorModel]):
    self._lock = threading.RLock()
    self._instances: dict[Hashable, ProjectionInterpreter] = {}
    # Used to construct new instances, given config['proj_kw']
    self._model_factory = model_class
  def _train_instance(
      self,
      model: lit_model.Model,
      dataset: lit_dataset.Dataset,
      config: JsonDict,
      name: str
  ) -> ProjectionInterpreter:
    # Ignore pytype warning about abstract methods, since this should always
    # be a subclass of ProjectorModel which has these implemented.
    projector = self._model_factory(**config.get("proj_kw", {}))  # pytype: disable=not-instantiable
    train_inputs = dataset.examples
    # If using input values, then treat inputs as outputs instead of running
    # the model.
    if config.get("use_input"):
      train_outputs = train_inputs
    else:
      train_outputs = list(model.predict(train_inputs))
    logging.info("Creating new projection instance on %d points",
                 len(train_inputs))
    return ProjectionInterpreter(
        model,
        train_inputs,
        train_outputs,
        projector=projector,
        field_name=config["field_name"],
        name=name)
  def run(self,
          inputs: list[JsonDict],
          model: lit_model.Model,
          dataset: lit_dataset.Dataset,
          model_outputs: Optional[list[JsonDict]] = None,
          config: Optional[JsonDict] = None):
    # UMAP code is not threadsafe and will throw strange 'index-out-of-bounds'
    # errors if multiple instances are accessed concurrently.
    with self._lock:
      instance_key = _key_from_dict(config)
      logging.info("Projection request: instance key: %s", instance_key)
      # Fit a new instance if necessary
      if instance_key not in self._instances:
        self._instances[instance_key] = self._train_instance(
            model, dataset, config, name=str(instance_key))
      proj_instance = self._instances[instance_key]
      # If projector was just trained, points should be cached.
      return proj_instance.run(inputs, model, dataset, model_outputs, config)

================
File: lit_nlp/components/regression_results_test.py
================
# Copyright 2022 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Tests for lit_nlp.components.regression_results."""
from absl.testing import absltest
from absl.testing import parameterized
from lit_nlp.api import dataset as lit_dataset
from lit_nlp.api import dtypes
from lit_nlp.api import model as lit_model
from lit_nlp.components import regression_results
from lit_nlp.lib import testing_utils
class RegressionResultsTest(parameterized.TestCase):
  def setUp(self):
    super(RegressionResultsTest, self).setUp()
    self.interpreter = regression_results.RegressionInterpreter()
  @parameterized.named_parameters(
      ('classification', testing_utils.ClassificationModelForTesting(), False),
      ('regression', testing_utils.RegressionModelForTesting({}), True),
  )
  def test_is_compatible(self, model: lit_model.Model, epxected: bool):
    compat = self.interpreter.is_compatible(
        model, lit_dataset.NoneDataset({'test': model}))
    self.assertEqual(compat, epxected)
  def test_run_with_label(self):
    dataset = lit_dataset.Dataset(None, None)
    inputs = [
        {'label': 2}, {'label': -1}, {'label': 0}
    ]
    results = self.interpreter.run(
        inputs, testing_utils.RegressionModelForTesting({}), dataset
    )
    expected = [
        {'scores': dtypes.RegressionResult(0, -2, 4)},
        {'scores': dtypes.RegressionResult(0, 1, 1)},
        {'scores': dtypes.RegressionResult(0, 0, 0)},
    ]
    self.assertEqual(expected, results)
  def test_run_with_no_label(self):
    dataset = lit_dataset.Dataset(None, None)
    inputs = [
        {}, {}, {}
    ]
    results = self.interpreter.run(
        inputs, testing_utils.RegressionModelForTesting({}), dataset
    )
    expected = [
        {'scores': dtypes.RegressionResult(0, None, None)},
        {'scores': dtypes.RegressionResult(0, None, None)},
        {'scores': dtypes.RegressionResult(0, None, None)},
    ]
    self.assertEqual(expected, results)
if __name__ == '__main__':
  absltest.main()

================
File: lit_nlp/components/regression_results.py
================
# Copyright 2022 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""An interpreter for analyzing regression results."""
from typing import cast, Optional
from lit_nlp.api import components as lit_components
from lit_nlp.api import dataset as lit_dataset
from lit_nlp.api import dtypes
from lit_nlp.api import model as lit_model
from lit_nlp.api import types
from lit_nlp.lib import utils as lit_utils
JsonDict = types.JsonDict
IndexedInput = types.IndexedInput
Spec = types.Spec
class RegressionInterpreter(lit_components.Interpreter):
  """Calculates and returns regression results from model outputs."""
  def run(  # pytype: disable=signature-mismatch  # overriding-parameter-type-checks
      self,
      inputs: list[JsonDict],
      model: lit_model.Model,
      dataset: lit_dataset.IndexedDataset,
      model_outputs: Optional[list[JsonDict]] = None,
      config: Optional[JsonDict] = None):
    # Find the prediction field key in the model output to use for calculations.
    output_spec = model.output_spec()
    supported_keys = self._find_supported_pred_keys(output_spec)
    results: list[dict[str, dtypes.RegressionResult]] = []
    # Run prediction if needed:
    if model_outputs is None:
      model_outputs = list(model.predict(inputs))
    for i, inp in enumerate(inputs):
      input_result: dict[str, dtypes.RegressionResult] = {}
      for key in supported_keys:
        field_spec = cast(types.RegressionScore, output_spec[key])
        score = model_outputs[i][key]
        error = None
        sq_error = None
        # If there is ground truth information, calculate error and squared
        # error.
        if (field_spec.parent and field_spec.parent in inp):
          ground_truth = inp[field_spec.parent]
          error = score - ground_truth
          sq_error = error * error
        result = dtypes.RegressionResult(score, error, sq_error)
        input_result[key] = result
      results.append(input_result)
    return results
  def is_compatible(self, model: lit_model.Model,
                    dataset: lit_dataset.Dataset) -> bool:
    del dataset  # Unused as regressions depend on model only
    return lit_utils.spec_contains(model.output_spec(), types.RegressionScore)
  def _find_supported_pred_keys(self, output_spec: types.Spec) -> list[str]:
    return lit_utils.find_spec_keys(output_spec, types.RegressionScore)

================
File: lit_nlp/components/remote_model.py
================
"""Client code for querying remote models hosted by a LIT server."""
from typing import Any, Optional
import urllib
from absl import logging
from lit_nlp.api import model as lit_model
from lit_nlp.api import types
from lit_nlp.lib import serialize
import requests
import six
urlopen = urllib.urlopen
JsonDict = types.JsonDict
def query_lit_server(
    url: str,
    endpoint: str,
    params: Optional[dict[str, str]] = None,
    inputs: Optional[Any] = None,
    config: Optional[Any] = None,
) -> Any:
  """Query a LIT server from Python."""
  # Pack data for LIT request
  data = {'inputs': inputs, 'config': config}
  # TODO(lit-dev): for open source, require HTTPS.
  if not url.startswith('http://'):
    url = 'http://' + url
  full_url = urllib.parse.urljoin(url, endpoint)
  # Use requests to handle URL params.
  rq = requests.Request(
      'POST',
      full_url,
      params=params,
      data=serialize.to_json(data),
      headers={'Content-Type': 'application/json'})
  rq = rq.prepare()
  # Convert to urllib request
  request = urllib.request.Request(
      url=rq.url,
      data=six.ensure_binary(rq.body) if rq.body else None,
      headers=rq.headers,
      method=rq.method)
  response = urlopen(request)
  if response.code != 200:
    raise IOError(f'Failed to query {rq.url}; response code {response.code}')
  # TODO(iftenney): handle bad server response, e.g. if corplogin is required
  # and the server sends a login page instead of a JSON response.
  response_bytes = response.read()
  return serialize.from_json(six.ensure_text(response_bytes))
class RemoteModel(lit_model.BatchedModel):
  """LIT model backed by a remote LIT server."""
  def __init__(self, url: str, name: str, max_minibatch_size: int = 256):
    """Initialize model wrapper from remote server.
    Args:
      url: url of LIT server
      name: name of model on the remote server
      max_minibatch_size: batch size used for remote requests
    """
    self._url = url
    self._name = name
    # Get specs
    server_info = query_lit_server(self._url, 'get_info')
    model_spec = server_info['models'][self._name]['spec']
    self._input_spec = model_spec['input']
    self._output_spec = model_spec['output']
    self._max_minibatch_size = max_minibatch_size
  def input_spec(self):
    return self._input_spec
  def output_spec(self):
    return self._output_spec
  def max_minibatch_size(self):
    return self._max_minibatch_size
  def predict_minibatch(self, inputs: list[JsonDict]) -> list[JsonDict]:
    # Package data as IndexedInput with dummy ids.
    indexed_inputs = [{'id': None, 'data': d} for d in inputs]
    logging.info('Querying remote model: /get_preds on %d examples',
                 len(indexed_inputs))
    preds = query_lit_server(
        self._url,
        'get_preds',
        params={
            'model': self._name,
            'response_simple_json': False
        },
        inputs=indexed_inputs)
    logging.info('Received %d predictions from remote model.', len(preds))
    return preds
def models_from_server(url: str, **model_kw) -> dict[str, RemoteModel]:
  """Wrap all the models on a given LIT server."""
  server_info = query_lit_server(url, 'get_info')
  models = {}
  for name in server_info['models']:
    models[name] = RemoteModel(url, name, **model_kw)
  return models

================
File: lit_nlp/components/salience_clustering_test.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Tests for lit_nlp.components.gradient_maps."""
from absl.testing import absltest
from absl.testing import parameterized
from lit_nlp.api import dataset as lit_dataset
from lit_nlp.api import dtypes
from lit_nlp.components import gradient_maps
from lit_nlp.components import lime_explainer
from lit_nlp.components import salience_clustering
from lit_nlp.lib import testing_utils
import numpy as np
class SalienceClusteringTest(parameterized.TestCase):
  def setUp(self):
    super(SalienceClusteringTest, self).setUp()
    self.salience_mappers = {
        'Grad L2 Norm': gradient_maps.GradientNorm(),
        'Grad  Input': gradient_maps.GradientDotInput(),
        'LIME': lime_explainer.LIME()
    }
  def _call_classification_model_on_standard_input(self, config, grad_key):
    inputs = [
        {'segment': 'a b c d'},
        {'segment': 'a b c d'},
        {'segment': 'e f e f'},
        {'segment': 'e f e f'},
        {'segment': 'e f e f'},
    ]
    model = testing_utils.ClassificationModelForTesting()
    dataset = lit_dataset.Dataset(None, None)
    model_outputs = [{
        grad_key:
            np.array([[0, 0, 1, 1], [0, 1, 0, 0], [1, 1, 1, 1], [1, 0, 1, 1]]),
        'tokens': ['a', 'b', 'c', 'd'],
        'grad_class':
            '1'
    }, {
        grad_key:
            np.array([[0, 0, 1, 1], [0, 1, 0, 0], [1, 1, 1, 1], [1, 0, 1, 1]]),
        'tokens': ['a', 'b', 'c', 'd'],
        'grad_class':
            '1'
    }, {
        grad_key:
            np.array([[1, 1, 1, 1], [1, 0, 1, 1], [1, 1, 1, 1], [0, 0, 1, 1]]),
        'tokens': ['e', 'f', 'e', 'g'],
        'grad_class':
            '1'
    }, {
        grad_key:
            np.array([[1, 1, 1, 1], [1, 0, 1, 1], [1, 1, 1, 1], [0, 0, 1, 1]]),
        'tokens': ['e', 'f', 'e', 'g'],
        'grad_class':
            '1'
    }, {
        grad_key:
            np.array([[1, 1, 1, 1], [1, 0, 1, 1], [1, 1, 1, 1], [0, 0, 1, 1]]),
        'tokens': ['e', 'f', 'e', 'g'],
        'grad_class':
            '1'
    }]
    clustering_component = salience_clustering.SalienceClustering(
        self.salience_mappers)
    result = clustering_component.run(inputs, model, dataset, model_outputs,
                                      config)
    return result, clustering_component, inputs, model, dataset, model_outputs
  def test_build_vocab(self):
    token_saliencies = [
        {
            'token_grad_sentence':
                dtypes.TokenSalience(
                    ['a', 'b', 'c'],
                    np.array([0, 0, 0]),
                )
        },
        {
            'token_grad_sentence':
                dtypes.TokenSalience(
                    ['d', 'e', 'f'],
                    np.array([0, 0, 0]),
                )
        },
    ]
    clustering_component = salience_clustering.SalienceClustering(
        self.salience_mappers)
    vocab_lookup, vocab = clustering_component._build_vocab(token_saliencies)
    expected_vocab_lookup = {'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5}
    expected_vocab = ['a', 'b', 'c', 'd', 'e', 'f']
    self.assertEqual(expected_vocab_lookup, vocab_lookup)
    self.assertEqual(expected_vocab, vocab)
  def test_convert_to_bow_vector(self):
    token_saliencies = [
        {
            'token_grad_sentence':
                dtypes.TokenSalience(
                    ['a', 'b', 'c'],
                    np.array([0.1, 0.2, 0.3]),
                )
        },
        # Checks that for 2 equal tokens the one with the largest absolute value
        # is preserved.
        {
            'token_grad_sentence':
                dtypes.TokenSalience(
                    ['d', 'e', 'd'],
                    np.array([0.4, 0.5, -0.6]),
                )
        },
    ]
    clustering_component = salience_clustering.SalienceClustering(
        self.salience_mappers)
    vocab_lookup = {'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5}
    top_k = 2
    representations = clustering_component._compute_fixed_length_representation(
        token_saliencies, vocab_lookup, top_k)
    expected = [
        {
            'token_grad_sentence':
                np.array([0.0, 0.2, 0.3, 0.0, 0.0, 0.0]) / np.sqrt(0.13)
        },
        {
            'token_grad_sentence':
                np.array([0.0, 0.0, 0.0, -0.6, 0.5, 0.0]) / np.sqrt(0.61)
        },
    ]
    np.testing.assert_equal(expected, representations)
  @parameterized.named_parameters(
      ('lit_internal_salience', 'Grad L2 Norm', 'input_embs_grad'),
      ('lime', 'LIME', 'segment'),
  )
  def test_clustering(self, salience_mapper, grad_key):
    """Tests clustering on LIT-internal gradient methods."""
    config = {
        salience_clustering.SALIENCE_MAPPER_KEY: salience_mapper,
        salience_clustering.N_CLUSTERS_KEY: 2,
        salience_clustering.TOP_K_TOKENS_KEY: 2
    }
    result, clustering_component, *_ = (
        self._call_classification_model_on_standard_input(config, grad_key))
    # Cluster id assignment is random, so in one run the first 2 examples may
    # be cluster 0, in the next run they may be in cluster 1.
    cluster_id_of_first = result[
        salience_clustering.CLUSTER_ID_KEY][grad_key][0]
    cluster_id_of_last = result[
        salience_clustering.CLUSTER_ID_KEY][grad_key][-1]
    np.testing.assert_equal(
        result[salience_clustering.CLUSTER_ID_KEY][grad_key], [
            cluster_id_of_first, cluster_id_of_first, cluster_id_of_last,
            cluster_id_of_last, cluster_id_of_last
        ])
    np.testing.assert_allclose(
        result[salience_clustering.REPRESENTATION_KEY][grad_key][0],
        result[salience_clustering.REPRESENTATION_KEY][grad_key][1])
    np.testing.assert_allclose(
        result[salience_clustering.REPRESENTATION_KEY][grad_key][2],
        result[salience_clustering.REPRESENTATION_KEY][grad_key][3])
    np.testing.assert_allclose(
        result[salience_clustering.REPRESENTATION_KEY][grad_key][2],
        result[salience_clustering.REPRESENTATION_KEY][grad_key][4])
    self.assertIn(grad_key, clustering_component.kmeans)
    self.assertIsNotNone(clustering_component.kmeans[grad_key])
  def test_clustering_create_new_kmeans(self):
    """Tests that the kmeans component is created with every run."""
    config = {
        salience_clustering.SALIENCE_MAPPER_KEY: 'Grad L2 Norm',
        salience_clustering.N_CLUSTERS_KEY: 2,
        salience_clustering.TOP_K_TOKENS_KEY: 2
    }
    grad_key = 'input_embs_grad'
    _, clustering_component, inputs, model, dataset, model_outputs = (
        self._call_classification_model_on_standard_input(config, grad_key))
    kmeans_call_1 = clustering_component.kmeans[grad_key]
    clustering_component.run(inputs, model, dataset, model_outputs,
                             config)
    kmeans_call_2 = clustering_component.kmeans[grad_key]
    self.assertIsNot(kmeans_call_1, kmeans_call_2)
  def test_clustering_reuse_kmeans(self):
    """Tests that the kmeans component is reused."""
    config = {
        salience_clustering.SALIENCE_MAPPER_KEY: 'Grad L2 Norm',
        salience_clustering.N_CLUSTERS_KEY: 2,
        salience_clustering.TOP_K_TOKENS_KEY: 2
    }
    grad_key = 'input_embs_grad'
    _, clustering_component, inputs, model, dataset, model_outputs = (
        self._call_classification_model_on_standard_input(config, grad_key))
    kmeans_call_1 = clustering_component.kmeans[grad_key]
    config[salience_clustering.REUSE_CLUSTERING] = True
    clustering_component.run(inputs, model, dataset, model_outputs,
                             config)
    kmeans_call_2 = clustering_component.kmeans[grad_key]
    self.assertIs(kmeans_call_1, kmeans_call_2)
  def test_top_tokens(self):
    """Tests top token results (doesn't apply for LIME with a test model)."""
    config = {
        salience_clustering.SALIENCE_MAPPER_KEY: 'Grad L2 Norm',
        salience_clustering.N_CLUSTERS_KEY: 2,
        salience_clustering.TOP_K_TOKENS_KEY: 2,
        salience_clustering.N_TOKENS_TO_DISPLAY_KEY: 2
    }
    result, *_ = self._call_classification_model_on_standard_input(
        config, 'input_embs_grad')
    # Clustering isn't deterministic so we don't know if examples 1 and 2 are
    # in cluster 0 or 1.
    top_token_results = result[
        salience_clustering.TOP_TOKEN_KEY]['input_embs_grad']
    for cluster_id in range(config[salience_clustering.N_CLUSTERS_KEY]):
      top_tokens_with_weights = top_token_results[cluster_id][:config[
          salience_clustering.N_TOKENS_TO_DISPLAY_KEY]]
      top_tokens = [token_with_weight[0] for token_with_weight in
                    top_tokens_with_weights]
      subset_cd = ['c', 'd']
      subset_ef = ['e', 'f']
      top_tokens_are_set_cd = subset_cd == top_tokens
      top_tokens_are_set_ef = subset_ef == top_tokens
      self.assertTrue(top_tokens_are_set_cd or top_tokens_are_set_ef)
  def test_string_config_item(self):
    """Tests clustering when config contains a string value."""
    config = {
        salience_clustering.SALIENCE_MAPPER_KEY: 'Grad L2 Norm',
        salience_clustering.N_CLUSTERS_KEY: '2',
        salience_clustering.TOP_K_TOKENS_KEY: 2
    }
    result, *_ = self._call_classification_model_on_standard_input(
        config, 'input_embs_grad')
    self.assertIsNotNone(result)
  @parameterized.named_parameters(
      ('n_tokens_to_display = 1', 1),
      ('n_tokens_to_display = 2', 2),
      ('n_tokens_to_display = 10', 10),
  )
  def test_n_tokens_to_display(self, n_tokens_to_display):
    """Tests results contain n or less tokens per cluster."""
    config = {
        salience_clustering.SALIENCE_MAPPER_KEY: 'Grad L2 Norm',
        salience_clustering.N_CLUSTERS_KEY: 2,
        salience_clustering.TOP_K_TOKENS_KEY: 2,
        salience_clustering.N_TOKENS_TO_DISPLAY_KEY: n_tokens_to_display
    }
    result, *_ = self._call_classification_model_on_standard_input(
        config, 'input_embs_grad')
    top_token_results = result[
        salience_clustering.TOP_TOKEN_KEY]['input_embs_grad']
    for results_for_cluster in top_token_results:
      self.assertTrue(results_for_cluster)
      self.assertLessEqual(len(results_for_cluster), n_tokens_to_display)
if __name__ == '__main__':
  absltest.main()

================
File: lit_nlp/components/salience_clustering.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""kmeans clustering of salience weights."""
from collections.abc import Sequence
from typing import Optional
from lit_nlp.api import components as lit_components
from lit_nlp.api import dataset as lit_dataset
from lit_nlp.api import model as lit_model
from lit_nlp.api import types
import numpy as np
from sklearn import cluster
IndexedInput = types.IndexedInput
JsonDict = types.JsonDict
Spec = types.Spec
# Result keys.
CLUSTER_ID_KEY = 'cluster_ids'
REPRESENTATION_KEY = 'representations'
TOP_TOKEN_KEY = 'top_tokens'
# Config items.
N_CLUSTERS_KEY = 'Number of clusters'
TOP_K_TOKENS_KEY = 'Number of top salient tokens to consider per data point'
N_TOKENS_TO_DISPLAY_KEY = 'Number of tokens to display per cluster'
SALIENCE_MAPPER_KEY = 'Salience Mapper'
SEED_KEY = 'Clustering seed'
# Config items not to need to be surfaced in the controls for this interpreter.
REUSE_CLUSTERING = 'reuse_clustering'
class SalienceClustering(lit_components.Interpreter):
  """Salience map clustering."""
  def __init__(self, salience_mappers: dict[str, lit_components.Interpreter]):
    self.salience_mappers = salience_mappers
    self.kmeans = {}
    self.vocab_lookup = {}
    self.vocab = []
  def _build_vocab(
      self,
      token_saliencies: list[JsonDict]) -> tuple[dict[str, int], list[str]]:
    """Build a vocabulary from the given token saliencies.
    This creates a mapping from word type to index in the vocabulary taken from
    all tokens in `token_saliencies`. The order of word types in the vocabulary
    depends on the order of the tokens in the input.
    Args:
      token_saliencies: List of mappings from salience field to TokenSaliency
        objects. This is the result of a post-hoc explanation method, such as
        gradient l2.
    Returns:
      1. Mapping from word type to its index in the vocabulary.
      2. Ordered list of word types.
    """
    vocab_lookup = {}
    for instance in token_saliencies:
      for token_salience in instance.values():
        for token in token_salience.tokens:
          vocab_lookup[token] = vocab_lookup.get(token, len(vocab_lookup))
    vocab = [''] * len(vocab_lookup)
    for token, idx in vocab_lookup.items():
      vocab[idx] = token
    return vocab_lookup, vocab
  def _convert_to_bow_vector(self, token_weights: JsonDict,
                             vocab_lookup: dict[str, int],
                             top_k: int) -> np.ndarray:
    """Converts the given variable length-vector into a fixed-length vector.
    This function creates a zero vector of the length of the vocabulary and
    fills the positions of the given tokens by their salience.
    Args:
      token_weights: Mapping from tokens to their salience weights.
      vocab_lookup: Mapping from word type to its index in the vocabulary.
      top_k: Number of tokens to be considered to represent each sentence.
    Returns:
      Vector of the size of the vocabulary that contains salience weights at
      the vocabulary indexes of tokens in `token_weights` and zeros elsewhere.
    """
    vocab_vector = np.zeros((len(vocab_lookup),))
    # Find k-th largest weight value
    sorted_token_weights = list(
        sorted(token_weights.items(), key=lambda x: abs(x[1]), reverse=True))
    index_end = min(top_k, len(sorted_token_weights))
    weight_threshold = abs(sorted_token_weights[index_end - 1][1])
    # Check if there exist ties
    for token_weight in sorted_token_weights[index_end:]:
      if abs(token_weight[1]) == weight_threshold:
        index_end += 1
      else:
        break
    # Consider only top-k (including ties)
    for token_weight in sorted_token_weights[:index_end]:
      if token_weight[0] in vocab_lookup:
        vocab_vector[vocab_lookup[token_weight[0]]] = token_weight[1]
    return vocab_vector
  def _compute_fixed_length_representation(
      self, token_saliencies: list[JsonDict],
      vocab_lookup: dict[str, int],
      top_k: int) -> list[dict[str, np.ndarray]]:
    """Compute a fixed-length representation from the variable-length salience.
    The representation is a simple vocabulary vector with salience weights as
    values. Every resulting vector is normalized to unit length.
    When a token occurs multiple times in the input we keep the value whose
    absolute value is largest.
    Args:
      token_saliencies: List of mappings from salience field to TokenSaliency
        objects. This is the result of a post-hoc explanation method, such as
        gradient l2.
      vocab_lookup: Mapping from word type to its index in the vocabulary.
      top_k: Number of tokens to be considered to represent each sentence. They
        will be determined based on their saliency scores.
    Returns:
      List of one mapping per example. Every element maps a salience field to
      its fixed-length representation.
    """
    representations = []
    for instance in token_saliencies:
      per_field_results = {}
      for salience_field, token_salience in instance.items():
        token_weights = {}
        for token, score in zip(token_salience.tokens, token_salience.salience):
          token_weights[token] = max([token_weights.get(token, score), score],
                                     key=abs)
        representation = self._convert_to_bow_vector(token_weights,
                                                     vocab_lookup, top_k)
        # Normalize to unit length.
        representation = np.asarray(representation) / np.linalg.norm(
            representation)
        per_field_results[salience_field] = representation
      representations.append(per_field_results)
    return representations
  def run(
      self,
      inputs: Sequence[JsonDict],
      model: lit_model.Model,
      dataset: lit_dataset.Dataset,
      model_outputs: Optional[list[JsonDict]] = None,
      config: Optional[JsonDict] = None) -> Optional[JsonDict]:
    """Run this component, given a model and input(s).
    Note that when `config['REUSE_CLUSTERING'] == True` we reuse the previously
    computed kmeans objects and vocabulary. Tokens that do not exist in the
    vocabulary will be ignored.
    Args:
      inputs: Inputs to cluster.
      model: Model that provides salience maps.
      dataset: Dataset to compute salience maps for.
      model_outputs: Precomputed model outputs.
      config: Config for clustering and salience computation
    Returns:
      Dict with 2 keys:
        `CLUSTER_ID_KEY`: Contains the cluster assignments. One cluster id per
          dataset example.
        `REPRESENTATION_KEY`: Contains the representations of all examples in
          the dataset that were used in the clustering.
        `TOP_TOKEN_KEY`: Top tokens are the tokens that have highest mean
          salience over all examples. They are sorted according to their
          salience.
    Raises:
      RuntimeError: Salience interpreter incompatible with model and dataset
      TypeError: config is not provided
      ValueError: config["Salience Mapper"] is not provided
    """
    if not config:
      raise TypeError('config must be provided')
    salience_key: Optional[str] = config.get(SALIENCE_MAPPER_KEY)
    if not salience_key:
      raise ValueError(f'config[{SALIENCE_MAPPER_KEY}] must be provided')
    if not (salience_interpreter := self.salience_mappers.get(salience_key)):
      raise ValueError(f'No interpreter registered for ${salience_key}.')
    if not salience_interpreter.is_compatible(model=model, dataset=dataset):
      raise RuntimeError(
          f'The {salience_key} interpreter is incompatible with this model and'
          ' dataset pair.'
      )
    # If no specific inputs provided, use the entire dataset.
    inputs_to_use = inputs or dataset.examples
    token_saliencies = salience_interpreter.run(
        inputs_to_use, model, dataset, model_outputs, config)
    if not token_saliencies:
      return None
    salience_fields = list(token_saliencies[0].keys())
    reuse_clustering = self.kmeans and config.get(REUSE_CLUSTERING, False)
    if not reuse_clustering:
      try:
        self.vocab = model.get_embedding_table()[0]
        self.vocab_lookup = {
            word_type: i for i, word_type in enumerate(self.vocab)
        }
      except NotImplementedError:
        self.vocab_lookup, self.vocab = self._build_vocab(token_saliencies)
    top_k = int(config.get(TOP_K_TOKENS_KEY,
                           self.config_spec()[TOP_K_TOKENS_KEY].default))
    representations = self._compute_fixed_length_representation(
        token_saliencies, self.vocab_lookup, top_k)
    cluster_ids = {}
    salience_field_to_representations = {}
    salience_field_to_top_tokens = {}
    for salience_field in salience_fields:
      weight_matrix = np.vstack([
          representation[salience_field] for representation in representations
      ])
      n_clusters = int(
          config.get(N_CLUSTERS_KEY,
                     self.config_spec()[N_CLUSTERS_KEY].default))
      seed = int(
          config.get(SEED_KEY,
                     self.config_spec()[SEED_KEY].default))
      if not reuse_clustering:
        self.kmeans[salience_field] = cluster.KMeans(
            n_clusters=n_clusters, random_state=seed)
        cluster_ids[salience_field] = self.kmeans[salience_field].fit_predict(
            weight_matrix).tolist()
      else:
        cluster_ids[salience_field] = self.kmeans[salience_field].predict(
            weight_matrix).tolist()
      salience_field_to_representations[salience_field] = weight_matrix
      salience_field_to_top_tokens[salience_field] = []
      for cluster_id in range(n_clusters):
        weight_matrix_of_cluster = weight_matrix[np.asarray(
            cluster_ids[salience_field]) == cluster_id]
        # If this is empty, we don't have any data points in the current
        # cluster. This may happen when `reuse_clustering` is true and we get
        # only a single example.
        if not weight_matrix_of_cluster.size:
          continue
        # <float32>[vocab size]
        mean_weight_matrix = weight_matrix_of_cluster.mean(axis=0)
        num_tokens_to_display = int(
            config.get(N_TOKENS_TO_DISPLAY_KEY,
                       self.config_spec()[N_TOKENS_TO_DISPLAY_KEY].default))
        top_indices = (
            np.abs(mean_weight_matrix).argsort()[::-1][:num_tokens_to_display])
        top_tokens = [
            (self.vocab[i], mean_weight_matrix[i])
            for i in top_indices if mean_weight_matrix[i] != 0.0
        ]
        salience_field_to_top_tokens[salience_field].append(top_tokens)
    return {
        CLUSTER_ID_KEY: cluster_ids,
        REPRESENTATION_KEY: salience_field_to_representations,
        TOP_TOKEN_KEY: salience_field_to_top_tokens,
    }
  def is_compatible(self, model: lit_model.Model,
                    dataset: lit_dataset.Dataset) -> bool:
    return any(
        interp.is_compatible(model=model, dataset=dataset)
        for interp in self.salience_mappers.values())
  def config_spec(self) -> types.Spec:
    return {
        SALIENCE_MAPPER_KEY:
            types.CategoryLabel(
                required=True, vocab=list(self.salience_mappers.keys())),
        N_CLUSTERS_KEY:
            types.Scalar(min_val=2, max_val=100, default=10, step=1),
        TOP_K_TOKENS_KEY:
            types.Scalar(min_val=1, max_val=20, default=5, step=1),
        N_TOKENS_TO_DISPLAY_KEY:
            types.Scalar(min_val=5, max_val=50, default=10, step=1),
        SEED_KEY:
            types.TextSegment(default='0'),
    }
  def meta_spec(self) -> types.Spec:
    return {
        CLUSTER_ID_KEY: types.CategoryLabel(),
        REPRESENTATION_KEY: types.Embeddings(),
        TOP_TOKEN_KEY: types.TopTokens(),
    }

================
File: lit_nlp/components/scrambler_test.py
================
# Copyright 2023 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Tests for lit_nlp.components.scrambler."""
from absl.testing import absltest
from lit_nlp.api import dataset as lit_dataset
from lit_nlp.api import types as lit_types
from lit_nlp.components import scrambler
from lit_nlp.lib import testing_utils
class ScramblerTest(absltest.TestCase):
  def setUp(self):
    super().setUp()
    test_spec: lit_types.Spec = {
        'sentence': lit_types.TextSegment(),
        '_id': lit_types.TextSegment(),
        '_meta': lit_types.JsonDict,
    }
    self.model = testing_utils.RegressionModelForTesting(test_spec)
    # Dataset is only used for spec in Scrambler so define once
    self.dataset = lit_dataset.Dataset(
        spec=test_spec,
        examples=[{'text': 'blank', '_id': 'a1b2c3', '_meta': {}}],
    )
    self.generator = scrambler.Scrambler()
  def test_scrambler_id_changes(self):
    example = {
        'sentence': 'this is the sentence to be scrambled',
        '_id': 'a1b2c3',
        '_meta': {'parentId': '000000'},
    }
    config = {scrambler.FIELDS_TO_SCRAMBLE_KEY: ['sentence']}
    generated_example = self.generator.generate(
        example, self.model, self.dataset, config
    )[0]
    self.assertNotEqual(generated_example, example)
    self.assertNotEqual(generated_example['_id'], example['_id'])
    self.assertEqual(
        set(generated_example['sentence'].split()),
        set(example['sentence'].split()),
    )
    self.assertEqual(generated_example['_meta']['parentId'], example['_id'])
if __name__ == '__main__':
  absltest.main()

================
File: lit_nlp/components/scrambler.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Simple scrambling test generator."""
import copy
import random
from typing import Optional
from lit_nlp.api import components as lit_components
from lit_nlp.api import dataset as lit_dataset
from lit_nlp.api import model as lit_model
from lit_nlp.api import types
from lit_nlp.lib import utils
JsonDict = types.JsonDict
FIELDS_TO_SCRAMBLE_KEY = 'Fields to scramble'
def _scramble(val: str) -> str:
  words = val.split(' ')
  shuffled = copy.deepcopy(words)
  while True:
    random.shuffle(shuffled)
    if shuffled != words:
      break
  return ' '.join(shuffled)
class Scrambler(lit_components.Generator):
  """Scramble all words in an example to generate a new example."""
  def config_spec(self) -> types.Spec:
    return {
        FIELDS_TO_SCRAMBLE_KEY:
            types.MultiFieldMatcher(
                spec='input',
                types=['TextSegment'],
                select_all=True),
    }
  def is_compatible(self, model: lit_model.Model,
                    dataset: lit_dataset.Dataset) -> bool:
    del model  # Unused by Scrambler
    return utils.spec_contains(dataset.spec(), types.TextSegment)
  def generate(self,
               example: JsonDict,
               model: lit_model.Model,
               dataset: lit_dataset.Dataset,
               config: Optional[JsonDict] = None) -> list[JsonDict]:
    """Naively scramble all words in an example.
    Note: Even if more than one field is to be scrambled, only a single example
    will be produced, unlike other generators which will produce multiple
    examples, one per field.
    Args:
      example: the example used for basis of generated examples.
      model: the model.
      dataset: the dataset.
      config: user-provided config properties.
    Returns:
      examples: a list of generated examples.
    """
    del model  # Unused.
    config = config or {}
    # If config key is missing, generate no examples.
    fields_to_scramble = list(config.get(FIELDS_TO_SCRAMBLE_KEY, []))
    if not fields_to_scramble:
      return []
    # TODO(lit-dev): move this to generate_all(), so we read the spec once
    # instead of on every example.
    text_keys = [
        key for key in utils.find_spec_keys(dataset.spec(), types.TextSegment)
        if key in fields_to_scramble
    ]
    if not text_keys:
      return []
    updates = {
        text_key: _scramble(example[text_key])
        for text_key in text_keys
    }
    new_example = utils.make_modified_input(example, updates, 'Scrambler')
    return [new_example]

================
File: lit_nlp/components/sequence_salience.py
================
"""Interpreter components for seq2seq salience."""
from typing import Optional
import Levenshtein  # TEMPORARY; for dummy salience
from lit_nlp.api import components
from lit_nlp.api import dataset as lit_dataset
from lit_nlp.api import dtypes
from lit_nlp.api import model as lit_model
from lit_nlp.api import types
from lit_nlp.lib import utils
import numpy as np  # TEMPORARY; for dummy salience
JsonDict = types.JsonDict
_SUPPORTED_OUTPUTS = (types.GeneratedText, types.GeneratedTextCandidates)
class DummySequenceSalience(components.Interpreter):
  """Dummy-valued seq2seq salience, for testing."""
  def find_fields(self, model: lit_model.Model) -> dict[str, list[str]]:
    src_fields = utils.find_spec_keys(model.input_spec(), types.TextSegment)
    gen_fields = utils.find_spec_keys(model.output_spec(), _SUPPORTED_OUTPUTS)
    return {f: src_fields for f in gen_fields}
  def _dummy_sequence_salience(
      self, source_tokens: list[str], target_tokens: list[str]):
    """Compute salience matrix based on Levenshtein similarity."""
    all_input_tokens = source_tokens + target_tokens
    smat = np.zeros([len(target_tokens), len(all_input_tokens)])
    for i, out_tok in enumerate(target_tokens):
      for j, in_tok in enumerate(all_input_tokens[:len(source_tokens) + i]):
        smat[i, j] = 1.0 / (Levenshtein.distance(out_tok, in_tok) + 1.0)
    return smat
  def _run_single(
      self, ex: JsonDict, mo: JsonDict,
      field_map: dict[str, str]) -> dict[str, dtypes.SequenceSalienceMap]:
    result = {}  # dict[target_field name -> interpretations]
    for (target_field, source_fields) in field_map.items():
      source_tokens = []
      for sf in source_fields:
        source_tokens.extend(ex.get(sf, '').split())
      target = mo[target_field]
      if isinstance(target, list):
        target = target[0][0]  # text for first candidate
      target_tokens = target.split()
      salience = self._dummy_sequence_salience(source_tokens, target_tokens)
      result[target_field] = dtypes.SequenceSalienceMap(source_tokens,
                                                        target_tokens, salience)
    return result
  def run(self,
          inputs: list[JsonDict],
          model: lit_model.Model,
          dataset: lit_dataset.Dataset,
          model_outputs: Optional[list[JsonDict]] = None,
          config: Optional[JsonDict] = None) -> Optional[list[JsonDict]]:
    del dataset
    del config
    field_map = self.find_fields(model)
    # Run model, if needed.
    if model_outputs is None:
      model_outputs = list(model.predict(inputs))
    assert len(model_outputs) == len(inputs)
    return [
        self._run_single(ex, mo, field_map)  # pytype: disable=wrong-arg-types
        for ex, mo in zip(inputs, model_outputs)
    ]
  def is_compatible(self, model: lit_model.Model,
                    dataset: lit_dataset.Dataset) -> bool:
    del dataset  # Unused as salience comes from the model.
    has_inputs = utils.spec_contains(model.input_spec(), types.TextSegment)
    has_outputs = utils.spec_contains(model.output_spec(), _SUPPORTED_OUTPUTS)
    return has_inputs and has_outputs
  def meta_spec(self) -> types.Spec:
    return {'saliency': types.SequenceSalience(autorun=True, signed=False)}

================
File: lit_nlp/components/shap_explainer_test.py
================
# Copyright 2022 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Tests for lit_nlp.components.shap_interpreter."""
from typing import Optional
from absl.testing import absltest
from absl.testing import parameterized
from lit_nlp.api import dataset as lit_dataset
from lit_nlp.api import model as lit_model
from lit_nlp.api import types as lit_types
from lit_nlp.components import shap_explainer
from lit_nlp.lib import testing_utils
_BAD_DATASET = lit_dataset.Dataset(
    spec={
        'segment': lit_types.TextSegment(),
        'scalar': lit_types.Scalar(required=False),
        'grad_class': lit_types.CategoryLabel(vocab=['0', '1'], required=False)
    },
    examples=[])
_GOOD_DATASET = lit_dataset.Dataset(
    spec={'val': lit_types.Scalar()},
    examples=[{
        'val': 0.8675309
    }] * 10
)
class _EmptyTestModel(lit_model.Model):
  def input_spec(self) -> lit_types.Spec:
    return {}
  def output_spec(self) -> lit_types.Spec:
    return {}
  def predict(self, inputs, **kw):
    return None
class _SparseMultilabelTestModel(testing_utils.ClassificationModelForTesting):
  def output_spec(self) -> lit_types.Spec:
    return {'preds': lit_types.SparseMultilabelPreds()}
  def predict_minibatch(self, inputs, **kw):
    self.predict(inputs, **kw)
  def predict(self, inputs, **kw):
    return [{'preds': [('label', 0.8675309)]} for _ in inputs]
class TabularShapExplainerTest(parameterized.TestCase):
  def setUp(self):
    super(TabularShapExplainerTest, self).setUp()
    self.dataset = _GOOD_DATASET
    self.regression_model = testing_utils.IdentityRegressionModelForTesting()
    self.shap = shap_explainer.TabularShapExplainer()
    self.sparse_model = _SparseMultilabelTestModel()
  @parameterized.named_parameters(
      # Empty model always fails
      ('empty_model_bad_dataset', _EmptyTestModel(), _BAD_DATASET, False),
      ('empty_model_good_dataset', _EmptyTestModel(), _GOOD_DATASET, False),
      # Classification model never matches dataset
      (
          'cls_model_bad_dataset',
          testing_utils.ClassificationModelForTesting(),
          _BAD_DATASET,
          False,
      ),
      (
          'cls_model_good_dataset',
          testing_utils.ClassificationModelForTesting(),
          _GOOD_DATASET,
          False,
      ),
      # Incompatible dataset and model inut specs
      (
          'reg_model_bad_dataset',
          testing_utils.IdentityRegressionModelForTesting(),
          _BAD_DATASET,
          False,
      ),
      # Compatible dataset and model
      (
          'reg_model_good_dataset',
          testing_utils.IdentityRegressionModelForTesting(),
          _GOOD_DATASET,
          True,
      ),
      # Sparse model never matches dataset
      (
          'sparse_model_bad_dataset',
          _SparseMultilabelTestModel(),
          _BAD_DATASET,
          False,
      ),
      (
          'sparse_model_good_dataset',
          _SparseMultilabelTestModel(),
          _GOOD_DATASET,
          False,
      ),
  )
  def test_compatibility(self, model: lit_model.Model,
                         dataset: lit_dataset.Dataset, expected: bool):
    self.assertEqual(self.shap.is_compatible(model, dataset), expected)
  def test_config_errors(self):
    with self.assertRaises(ValueError):
      self.shap.run([],
                    self.regression_model,
                    _GOOD_DATASET,
                    config={shap_explainer.EXPLAIN_KEY: 'not_in_mdl'})
  @parameterized.named_parameters(
      ('default', None, None),
      ('pred_key_only', 'score', None),
      ('sample_size_only', None, 3),
      ('all_fields', 'score', 3),
  )
  def test_run(self, pred_key: Optional[str], sample_size: Optional[int]):
    config = {}
    if pred_key:
      config[shap_explainer.EXPLAIN_KEY] = pred_key
    if sample_size:
      config[shap_explainer.SAMPLE_KEY] = sample_size
      expected_length = sample_size
    else:
      expected_length = len(self.dataset.examples)
    regress_salience = self.shap.run(
        self.dataset.examples,
        self.regression_model,
        self.dataset,
        config=config)
    self.assertLen(regress_salience, expected_length)
if __name__ == '__main__':
  absltest.main()

================
File: lit_nlp/components/shap_explainer.py
================
# Copyright 2022 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""SHAP explanations for datasets and models."""
from typing import Optional, Union
from lit_nlp.api import components as lit_components
from lit_nlp.api import dataset as lit_dataset
from lit_nlp.api import dtypes
from lit_nlp.api import model as lit_model
from lit_nlp.api import types
from lit_nlp.lib import utils
import numpy as np
import pandas as pd
import shap
JsonDict = types.JsonDict
Spec = types.Spec
EXPLAIN_KEY = 'Prediction key'
SAMPLE_KEY = 'Sample size'
_SUPPORTED_INPUT_TYPES = (types.Scalar, types.CategoryLabel)
_SUPPORTED_OUTPUT_TYPES = (types.MulticlassPreds, types.RegressionScore,
                           types.Scalar, types.SparseMultilabelPreds)
class TabularShapExplainer(lit_components.Interpreter):
  """SHAP explanations for model predictions over tabular data.
  Kernel SHAP is used to determine the influence that each input features has
  on the prediction of an output feature, given a dataset and model.
  This explainer takes two inputs in its call configuration:
  1.  [Required] The "Prediction key" tells the interpreter which feature in
      the output spec to explain. This interpreter returns None if a
      "Prediction key" is not provided.
  2.  [Optional] "Sample size" is used to extract a random sample from the
      inputs provided to the run() function. If "Sample size" is 0, run() will
      explain the entire inputs list. Due to the performance characteristics of
      SHAP, it is recommended that you use a sample size  50, otherwise it is
      very likely that the HTTP request will timeout.
  This explainer outputs a list of FeatureSalience objects with the influence of
  each input feature on the predicted value for that input. The size of this
  list is equal to either the length of the inputs or the "Sample size", if the
  latter is > 0. Influence values are normalized in the range of [-1, 1].
  """
  def description(self) -> str:
    return (
        'Kernel SHAP explanations of input feature influence on model '
        'predictions over tabular data. Influence values are normalized in '
        'the range of [-1, 1].'
    )
  def is_compatible(
      self, model: lit_model.Model, dataset: lit_dataset.Dataset
  ) -> bool:
    # Tabular models require all dataset features are present for each datapoint
    if not model.is_compatible_with_dataset(dataset):
      return False
    input_spec_keys = model.input_spec().keys()
    dataset_features = [
        feature
        for name, feature in dataset.spec().items()
        if name in input_spec_keys
    ]
    is_tabular = all(
        feature.required and isinstance(feature, _SUPPORTED_INPUT_TYPES)
        for feature in dataset_features
    )
    has_outputs = utils.spec_contains(
        model.output_spec(), _SUPPORTED_OUTPUT_TYPES
    )
    return is_tabular and has_outputs
  def config_spec(self) -> types.Spec:
    return {
        EXPLAIN_KEY: types.SingleFieldMatcher(
            spec='output',
            types=[
                'MulticlassPreds',
                'RegressionScore',
                'Scalar',
                'SparseMultilabelPreds',
            ],
        ),
        SAMPLE_KEY: types.Scalar(min_val=0, max_val=50, default=30, step=1),
    }
  def meta_spec(self) -> types.Spec:
    return {'saliency': types.FeatureSalience(autorun=False, signed=True)}
  def run(
      self,
      inputs: list[JsonDict],
      model: lit_model.Model,
      dataset: lit_dataset.Dataset,
      model_outputs: Optional[list[JsonDict]] = None,
      config: Optional[JsonDict] = None
  ) -> Optional[list[dict[str, dtypes.FeatureSalience]]]:
    """Generate SHAP explanations for model predictions given a set of inputs.
    Args:
      inputs: The subset of inputs from the dataset to get prediction for. If
        empty, falls back to dataset.examples.
      model: The model making the predictions that get explained.
      dataset: The dataset from which the inputs originated.
      model_outputs: Unused, but reqired by the base class.
      config: A dictionary containing the key of the feature to explain, and the
        optional sample size if taking a random sample from the inputs.
    Returns:
      A list of FeatureSalience objects, one for each (randomly sampled) input,
      containing per-input feature salience values in the range of [-1, 1].
    Raises:
      ValueError: if the value of `config[EXPLAIN_KEY]` is not found in the
        model's output spec.
    """
    del model_outputs   # Unused. SHAP calls the model directly
    config_defaults = {k: v.default for k, v in self.config_spec().items()}
    config = dict(config_defaults, **(config or {}))
    default_pred_key = utils.find_spec_keys(
        model.output_spec(), _SUPPORTED_OUTPUT_TYPES)[0]
    pred_key = config.get(EXPLAIN_KEY) or default_pred_key
    pred_spec = model.output_spec().get(pred_key)
    if not pred_spec:
      raise ValueError('SHAP requires a prediction field to explain. Could not '
                       f'find {pred_key} in spec, {str(model.output_spec())}.')
    input_feats = [key for key in model.input_spec() if key in dataset.spec()]
    example_data = inputs or dataset.examples
    examples: pd.DataFrame = pd.DataFrame(example_data)[input_feats]
    sample_size = int(config.get(SAMPLE_KEY, 0))
    if sample_size and len(examples) > sample_size:
      inputs_to_use: pd.DataFrame = examples.sample(sample_size)
    else:
      inputs_to_use: pd.DataFrame = examples
    random_baseline = dataset.sample(1).examples
    background = pd.DataFrame(random_baseline)[input_feats]
    def prediction_fn(examples):
      dict_examples: list[JsonDict] = [
          dict(zip(input_feats, feature_values)) for feature_values in examples
      ]
      preds: list[Union[int, float]] = []
      for pred in model.predict(dict_examples):
        if isinstance(pred_spec, types.MulticlassPreds):
          pred_list: list[float] = list(pred[pred_key])
          max_value: float = max(pred_list)
          index: int = pred_list.index(max_value)
          preds.append(index)
        elif isinstance(pred_spec, types.SparseMultilabelPreds):
          pred_tuples: types.ScoredTextCandidates = pred[pred_key]
          pred_list = list(map(lambda pred: pred[1], pred_tuples))
          max_value: float = max(pred_list)
          index: int = pred_list.index(max_value)
          preds.append(index)
        else:
          preds.append(pred[pred_key])
      return np.array(preds)
    explainer = shap.KernelExplainer(prediction_fn, background)
    shap_values_by_example = explainer.shap_values(inputs_to_use)
    salience = [
        dict(zip(input_feats, example_shap_values))
        for example_shap_values in shap_values_by_example
    ]
    return [{'saliency': dtypes.FeatureSalience(s)} for s in salience]

================
File: lit_nlp/components/static_preds_test.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Tests for lit_nlp.lib.model."""
from absl.testing import absltest
from lit_nlp.api import dataset as lit_dataset
from lit_nlp.api import types as lit_types
from lit_nlp.components import static_preds
class StaticPredictionsTest(absltest.TestCase):
  def setUp(self):
    super().setUp()
    number_names = [
        "zero", "one", "two", "three", "four", "five", "six", "seven", "eight",
        "nine"
    ]
    # pylint: disable=g-complex-comprehension
    dummy_inputs = [{
        "name": name,
        "val": i
    } for i, name in enumerate(number_names)]
    # pylint: enable=g-complex-comprehension
    input_spec = {"name": lit_types.TextSegment(), "val": lit_types.Scalar()}
    self.input_ds = lit_dataset.Dataset(input_spec, dummy_inputs)
    dummy_preds = [{
        "pred": "{name:s}={val:d}".format(**d)
    } for d in self.input_ds.examples]
    self.preds_ds = lit_dataset.Dataset({"pred": lit_types.TextSegment()},
                                        dummy_preds)
  def test_all_identifiers(self):
    """Test using all fields as identifier keys."""
    model = static_preds.StaticPredictions(
        self.input_ds, self.preds_ds, input_identifier_keys=["name", "val"])
    self.assertEqual(self.input_ds.spec(), model.input_spec())
    self.assertEqual(self.preds_ds.spec(), model.output_spec())
    # Test on whole dataset
    self.assertEqual(
        list(model.predict(self.input_ds.examples)),
        list(self.preds_ds.examples))
    # Test on a slice
    self.assertEqual(
        list(model.predict(self.input_ds.examples[2:5])),
        list(self.preds_ds.examples[2:5]))
    # Test on unknown examples
    with self.assertRaises(KeyError):
      # Should raise an exception on the second input.
      inputs = [{"name": "nine", "val": 9}, {"name": "twenty", "val": 20}]
      # Use list() to force the generator to run.
      _ = list(model.predict(inputs))
  def test_partial_identifiers(self):
    """Test using only some fields as identifier keys."""
    model = static_preds.StaticPredictions(
        self.input_ds, self.preds_ds, input_identifier_keys=["name"])
    self.assertEqual({"name": lit_types.TextSegment()}, model.input_spec())
    self.assertEqual(self.preds_ds.spec(), model.output_spec())
    # Test on whole dataset
    self.assertEqual(
        list(model.predict(self.input_ds.examples)),
        list(self.preds_ds.examples))
    # Test on a slice
    self.assertEqual(
        list(model.predict(self.input_ds.examples[2:5])),
        list(self.preds_ds.examples[2:5]))
    # Test on unknown examples
    with self.assertRaises(KeyError):
      # Should raise an exception on the second input.
      inputs = [{"name": "nine", "val": 9}, {"name": "twenty", "val": 20}]
      # Use list() to force the generator to run.
      _ = list(model.predict(inputs))
if __name__ == "__main__":
  absltest.main()

================
File: lit_nlp/components/static_preds.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""LIT model wrapper for pre-computed (offline) predictions."""
from collections.abc import Iterable, Iterator
from typing import Optional
from lit_nlp.api import dataset as lit_dataset
from lit_nlp.api import model as lit_model
from lit_nlp.api import types as lit_types
from lit_nlp.lib import caching
JsonDict = lit_types.JsonDict
class StaticPredictions(lit_model.BatchedModel):
  """Implements lit.Model interface for a set of pre-computed predictions."""
  def key_fn(self, example: JsonDict) -> str:
    reduced_example: JsonDict = {
        k: example[k]
        for k in self.input_identifier_keys
    }
    return caching.input_hash(reduced_example)
  def description(self):
    return self._description
  @property
  def input_dataset(self):
    return self._all_inputs
  def __init__(self,
               inputs: lit_dataset.Dataset,
               preds: lit_dataset.Dataset,
               input_identifier_keys: Optional[list[str]] = None):
    """Build a static index.
    Args:
      inputs: a lit Dataset
      preds: a lit Dataset, parallel to inputs
      input_identifier_keys: (optional), list of keys to treat as identifiers
        for matching inputs. If None, will use all fields in inputs.spec()
    """
    self._all_inputs = inputs
    self._input_spec = inputs.spec()
    self._output_spec = preds.spec()
    self._description = preds.description()
    self.input_identifier_keys = input_identifier_keys or self._input_spec.keys(
    )
    # Filter to only the identifier keys
    self._input_spec = {
        k: self._input_spec[k] for k in self.input_identifier_keys
    }
    # Build the index for prediction lookups
    self._index = {
        self.key_fn(ex): pred
        for ex, pred in zip(inputs.examples, preds.examples)
    }
  def _predict_single(self, example: JsonDict):
    key = self.key_fn(example)
    if key not in self._index:
      raise KeyError(
          f'Example {key} not found in stored predictions: {str(example)}')
    return self._index[key]
  ##
  # LIT API implementation
  def input_spec(self):
    return self._input_spec
  def output_spec(self):
    return self._output_spec
  def predict_minibatch(self, inputs: list[JsonDict], **kw):
    return list(self.predict(inputs))
  def predict(self, inputs: Iterable[JsonDict], **kw) -> Iterator[JsonDict]:
    """Predict on known inputs.
    Args:
      inputs: input examples
      **kw: unused
    Returns:
      predictions
    Raises:
      KeyError if input not recognized
    """
    # Implement predict() directly, since there's no need to batch.
    return map(self._predict_single, inputs)

================
File: lit_nlp/components/tcav_int_test.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Tests for lit_nlp.components.gradient_maps."""
import random
from absl.testing import absltest
from absl.testing import parameterized
from lit_nlp.api import dataset as lit_dataset
from lit_nlp.components import tcav
from lit_nlp.examples.glue import models as glue_models
from lit_nlp.lib import caching  # for hash id fn
from lit_nlp.lib import testing_utils
from lit_nlp.lib import file_cache
BERT_TINY_PATH = file_cache.cached_path(
    'https://storage.googleapis.com/what-if-tool-resources/lit-models/sst2_tiny.tar.gz',  # pylint: disable=line-too-long
    extract_compressed_file=True,
)
_ALPHABET_EXAMPLES = [
    {'sentence': 'a', '_id': 'a'},
    {'sentence': 'b', '_id': 'b'},
    {'sentence': 'c', '_id': 'c'},
    {'sentence': 'd', '_id': 'd'},
    {'sentence': 'e', '_id': 'e'},
    {'sentence': 'f', '_id': 'f'},
    {'sentence': 'g', '_id': 'g'},
    {'sentence': 'h', '_id': 'h'},
    {'sentence': 'i', '_id': 'i'},
]
_EMOTION_EXAMPLES = [
    {'sentence': 'happy', '_id': 'happy'},  # 0
    {'sentence': 'sad', '_id': 'sad'},  # 1
    {'sentence': 'good', '_id': 'good'},  # 2
    {'sentence': 'bad', '_id': 'bad'},  # 3
    {'sentence': 'pretty', '_id': 'pretty'},  # 4
    {'sentence': 'ugly', '_id': 'ugly'},  # 5
    {'sentence': 'sweet', '_id': 'sweet'},  # 6
    {'sentence': 'bitter', '_id': 'bitter'},  # 7
    {'sentence': 'well', '_id': 'well'},  # 8
    {'sentence': 'poor', '_id': 'poor'},  # 9
    {'sentence': 'compelling', '_id': 'compelling'},  # 10
    {'sentence': 'boring', '_id': 'boring'},  # 11
    {'sentence': 'pleasing', '_id': 'pleasing'},  # 12
    {'sentence': 'gross', '_id': 'gross'},  # 13
    {'sentence': 'blue', '_id': 'blue'},  # 14
    {'sentence': 'red', '_id': 'red'},  # 15
    {'sentence': 'flower', '_id': 'flower'},  # 16
    {'sentence': 'bee', '_id': 'bee'},  # 17
    {'sentence': 'snake', '_id': 'snake'},  # 18
    {'sentence': 'windshield', '_id': 'windshield'},  # 19
    {'sentence': 'plant', '_id': 'plant'},  # 20
    {'sentence': 'scary', '_id': 'scary'},  # 21
    {'sentence': 'pencil', '_id': 'pencil'},  # 22
    {'sentence': 'hello', '_id': 'hello'}  # 23
]
class ModelBasedTCAVTest(parameterized.TestCase):
  @classmethod
  def setUpClass(cls):
    super().setUpClass()
    cls.alphabet_dataset = lit_dataset.IndexedDataset(
        id_fn=caching.input_hash, examples=_ALPHABET_EXAMPLES
    )
    cls.emotion_dataset = lit_dataset.IndexedDataset(
        id_fn=caching.input_hash, examples=_EMOTION_EXAMPLES
    )
    cls.model = caching.CachingModelWrapper(
        glue_models.SST2Model(BERT_TINY_PATH), 'test')
  def setUp(self):
    super().setUp()
    self.tcav = tcav.TCAV()
  def test_tcav(self):
    random.seed(0)  # Sets seed since create_comparison_splits() uses random.
    config = {
        'concept_set_ids': [
            _ALPHABET_EXAMPLES[0]['_id'],
            _ALPHABET_EXAMPLES[2]['_id'],
            _ALPHABET_EXAMPLES[3]['_id'],
            _ALPHABET_EXAMPLES[7]['_id']
        ],
        'class_to_explain': '1',
        'grad_layer': 'cls_grad',
        'random_state': 0,
    }
    result = self.tcav.run(
        _ALPHABET_EXAMPLES,
        self.model,
        self.alphabet_dataset,
        config=config,
    )
    self.assertLen(result, 1)
    expected = {
        'p_val': 0.13311,
        'random_mean': 0.56667,
        'result': {
            'score':
                0.33333,
            'cos_sim': [
                0.088691, -0.12179, 0.16013, 0.24840, -0.09793, 0.05166,
                -0.21578, -0.06560, -0.14759
            ],
            'dot_prods': [
                189.085096, -266.36317, 344.350498, 547.144949, -211.663965,
                112.502439, -472.72066, -144.529598, -323.31888
            ],
            'accuracy':
                0.66667,
            'cav': [[
                0.91326976, -13.67988429, -1.33965892, 12.23702997, 4.83590182,
                -20.6276055, 19.07518597, 4.9641991, -5.52570226, 10.37834134,
                0.53685622, -5.21721739, 12.00234068, -26.36114057, 6.20088358,
                4.76729567, 26.4359945, 16.30961659, 14.70620605, -13.47771528,
                5.29218365, 0.87290488, 10.19441762, 2.96687215, -0.70745918,
                -12.201803, -10.65010904, -5.90814342, -25.17510006,
                -5.90629019, 26.53638293, 27.44054429, -11.75430314,
                -3.72779609, 10.28197421, -11.58444132, -18.09351946,
                -22.09520524, -30.04023056, -16.7551763, -12.34913637,
                30.27897114, -13.98790656, 0.65758253, 3.75770261, 4.81132118,
                -11.18005293, 5.85445303, 18.88336475, -7.34885733,
                -35.76094848, -3.39495953, 5.52774132, -8.38126488, 10.07413613,
                -1.96825956, 17.97041089, -2.47085774, 19.5700424, -49.46295186,
                0.12541183, -6.95592842, -0.33562953, 12.9269603, -13.6288284,
                -9.51211543, 21.22778867, -2.81344371, -9.66434107,
                -15.41551695, -30.15406401, 14.65903841, 2.51729041,
                17.70711379, 13.21615045, 12.55899318, 11.45749114,
                -25.67659992, 13.00876054, -1.52499005, 27.45026658,
                -4.36110401, 10.01664277, -11.24470769, -6.79411522,
                -1.67106503, -2.59389537, 11.72310319, -0.84126818, 2.03886137,
                -11.25047383, -8.60679631, -23.1676229, 22.83532544,
                -25.2657054, -15.49795527, -1.62890474, -15.49504251,
                27.26973702, -8.00652979, 6.87541861, -3.61878753, 0.82889822,
                -2.88891667, 6.13730358, 12.55884424, -2.24121286, 45.90285087,
                34.43108722, -13.32567113, 19.0988537, 2.16242269, 17.45654791,
                18.17472208, 18.28023357, -20.34869744, -11.21275755, 6.0583063,
                -10.38857432, -7.30056744, -8.85395997, -17.19779724,
                -8.12087822, 2.46058364, 29.35061395, -6.53820405, 10.3522653,
                8.54478485
            ]]
        }
    }
    testing_utils.assert_deep_almost_equal(self, expected, result[0])
  def test_tcav_ignores_labels(self):
    random.seed(0)  # Sets seed since create_comparison_splits() uses random.
    config = {
        'concept_set_ids': [
            _ALPHABET_EXAMPLES[0]['_id'],
            _ALPHABET_EXAMPLES[2]['_id'],
            _ALPHABET_EXAMPLES[3]['_id'],
            _ALPHABET_EXAMPLES[7]['_id'],
        ],
        'class_to_explain': '1',
        'grad_layer': 'cls_grad',
        'random_state': 0,
    }
    # Don't use utils.make_modified_input here, because we need to match the
    # example IDs above.
    labeled_examples = [
        dict(ex, label=('0' if i % 2 == 0 else '1'))
        for i, ex in enumerate(_ALPHABET_EXAMPLES)
    ]
    result = self.tcav.run(
        labeled_examples,
        self.model,
        self.alphabet_dataset,
        config=config,
    )
    self.assertLen(result, 1)
    expected = {
        'p_val': 0.13311,
        'random_mean': 0.56667,
        'result': {
            'score': 0.33333,
            'cos_sim': [
                0.088691,
                -0.12179,
                0.16013,
                0.24840,
                -0.09793,
                0.05166,
                -0.21578,
                -0.06560,
                -0.14759,
            ],
            'dot_prods': [
                189.085096,
                -266.36317,
                344.350498,
                547.144949,
                -211.663965,
                112.502439,
                -472.72066,
                -144.529598,
                -323.31888,
            ],
            'accuracy': 0.66667,
            'cav': [[
                0.91326976,
                -13.67988429,
                -1.33965892,
                12.23702997,
                4.83590182,
                -20.6276055,
                19.07518597,
                4.9641991,
                -5.52570226,
                10.37834134,
                0.53685622,
                -5.21721739,
                12.00234068,
                -26.36114057,
                6.20088358,
                4.76729567,
                26.4359945,
                16.30961659,
                14.70620605,
                -13.47771528,
                5.29218365,
                0.87290488,
                10.19441762,
                2.96687215,
                -0.70745918,
                -12.201803,
                -10.65010904,
                -5.90814342,
                -25.17510006,
                -5.90629019,
                26.53638293,
                27.44054429,
                -11.75430314,
                -3.72779609,
                10.28197421,
                -11.58444132,
                -18.09351946,
                -22.09520524,
                -30.04023056,
                -16.7551763,
                -12.34913637,
                30.27897114,
                -13.98790656,
                0.65758253,
                3.75770261,
                4.81132118,
                -11.18005293,
                5.85445303,
                18.88336475,
                -7.34885733,
                -35.76094848,
                -3.39495953,
                5.52774132,
                -8.38126488,
                10.07413613,
                -1.96825956,
                17.97041089,
                -2.47085774,
                19.5700424,
                -49.46295186,
                0.12541183,
                -6.95592842,
                -0.33562953,
                12.9269603,
                -13.6288284,
                -9.51211543,
                21.22778867,
                -2.81344371,
                -9.66434107,
                -15.41551695,
                -30.15406401,
                14.65903841,
                2.51729041,
                17.70711379,
                13.21615045,
                12.55899318,
                11.45749114,
                -25.67659992,
                13.00876054,
                -1.52499005,
                27.45026658,
                -4.36110401,
                10.01664277,
                -11.24470769,
                -6.79411522,
                -1.67106503,
                -2.59389537,
                11.72310319,
                -0.84126818,
                2.03886137,
                -11.25047383,
                -8.60679631,
                -23.1676229,
                22.83532544,
                -25.2657054,
                -15.49795527,
                -1.62890474,
                -15.49504251,
                27.26973702,
                -8.00652979,
                6.87541861,
                -3.61878753,
                0.82889822,
                -2.88891667,
                6.13730358,
                12.55884424,
                -2.24121286,
                45.90285087,
                34.43108722,
                -13.32567113,
                19.0988537,
                2.16242269,
                17.45654791,
                18.17472208,
                18.28023357,
                -20.34869744,
                -11.21275755,
                6.0583063,
                -10.38857432,
                -7.30056744,
                -8.85395997,
                -17.19779724,
                -8.12087822,
                2.46058364,
                29.35061395,
                -6.53820405,
                10.3522653,
                8.54478485,
            ]],
        },
    }
    testing_utils.assert_deep_almost_equal(self, expected, result[0])
  def test_tcav_sample_from_positive(self):
    random.seed(0)  # Sets seed since create_comparison_splits() uses random.
    # Tests the case where more concept examples are passed than non-concept
    # examples, so the concept set is sampled from the concept examples.
    config = {
        'concept_set_ids': [
            _ALPHABET_EXAMPLES[0]['_id'],
            _ALPHABET_EXAMPLES[2]['_id'],
            _ALPHABET_EXAMPLES[3]['_id'],
            _ALPHABET_EXAMPLES[4]['_id'],
            _ALPHABET_EXAMPLES[7]['_id']
        ],
        'class_to_explain': '1',
        'grad_layer': 'cls_grad',
        'random_state': 0
    }
    result = self.tcav.run(
        _ALPHABET_EXAMPLES[:8],
        self.model,
        self.alphabet_dataset,
        config=config
    )
    self.assertLen(result, 1)
    expected = {
        'p_val': 0.80489,
        'random_mean': 0.53333,
        'result': {
            'score':
                0.8,
            'cos_sim': [
                0.09527, -0.20442, 0.05141, 0.14985, 0.06750, -0.28244,
                -0.11022, -0.14479
            ],
            'dot_prods': [
                152.48776, -335.64998, 82.99588, 247.80113, 109.53684,
                -461.81805, -181.29095, -239.47817
            ],
            'accuracy':
                1.0,
            'cav': [[
                -9.18092006e+00, -3.25423105e+00, -1.95599351e+01,
                7.90544869e+00, 2.71945760e-01, 9.21655507e+00, -7.10360094e-01,
                1.06792122e+01, -4.22297728e+00, -5.88364071e+00,
                -5.90468259e+00, 3.02462186e-01, 2.73947077e+01,
                -4.21289488e+00, 4.10535370e+00, 2.90032257e+00,
                -1.15293947e+01, -6.05013625e+00, -9.63492785e+00,
                -1.24673936e+01, -7.16846202e+00, 1.65560561e+01,
                3.91797282e+00, 9.43190766e+00, 1.13459987e+01, -3.52568932e+00,
                9.13124442e-01, 9.91573080e-01, -3.25439546e+00,
                -9.93854182e-01, -1.07992122e+01, 3.11628229e+00,
                6.59499537e+00, -1.36178363e+01, 1.29858952e+01, 1.03866086e+01,
                1.59031300e+01, -5.56392889e+00, -3.77058407e+00,
                -9.32599755e+00, -9.47891226e-01, 2.05803580e+01,
                -1.37954357e+01, -7.73649342e+00, -9.07151538e+00,
                9.29099926e-01, -1.08088474e+01, -7.80737270e-01,
                2.40018316e+00, 1.09211064e+01, -3.27414948e+01,
                -2.34077057e+01, 1.17713587e+01, 1.81635854e+00, 2.31977099e+01,
                1.61744777e+00, 1.17686967e+00, -3.88545806e+00, 2.46277754e+00,
                -4.51580108e+01, 9.32061903e+00, -1.73902286e+01,
                -3.66235470e+00, 3.26925824e+01, 3.21295843e+00,
                -1.79316338e+01, 2.01011182e+01, 3.49235823e+00, 1.52195300e+01,
                -5.04835360e+00, 1.45131574e+01, 1.59716750e+01,
                -2.96747872e+00, 5.25282201e+00, -1.98576797e+01,
                -1.46852052e+00, 1.58219867e+00, -5.74221070e-01,
                1.16208072e+01, 5.11250274e-01, 5.52448443e+00, 1.11949046e+01,
                -6.93072443e-02, -7.17318663e+00, -8.48821209e+00,
                -8.71016927e+00, -1.05849366e+01, 8.71665351e+00,
                1.24408289e+01, 4.21549502e+00, -2.07208098e+01, 1.56304334e+01,
                -4.81732341e+00, 3.20792625e+01, -1.20141017e+01,
                -1.73165977e+01, -6.54369967e+00, 3.16826359e+00,
                -1.19574116e+01, -5.90525906e+00, 1.08200571e-01,
                -9.32849433e+00, -1.29775955e+00, -1.20217051e+01,
                1.30472736e+01, 2.25283409e+01, 7.04978975e-02, 5.09369848e-01,
                1.27449879e+01, -2.16621823e+00, -8.19007901e+00,
                -4.16839353e+00, 9.41498786e+00, 6.79635008e+00, 4.88519035e+00,
                2.63718274e+00, -5.28336147e-03, 1.26470921e+00,
                -3.35551546e-01, 8.26617183e+00, 2.58916933e+00,
                -8.54040920e+00, -1.18897963e+00, -5.09214401e+00,
                7.85278007e+00, -1.26074616e+01, -3.05422845e+00, 3.79508590e+00
            ]]
        }
    }
    testing_utils.assert_deep_almost_equal(self, expected, result[0])
  # TODO(b/254110131): Paramaterize this test. the values returned by the
  # current implementaiton change when naively paramterized because of the
  # single call to random.seed() at the beginning of the funciton, and these
  # changes need to be accounted for in the parmertized version.
  def test_relative_tcav(self):
    # Tests passing in a negative set.
    random.seed(0)  # Sets seed since create_comparison_splits() uses random.
    # This first example doesn't have enough examples for statistical testing,
    # so the returned p-value is None.
    config = {
        'concept_set_ids': [
            _EMOTION_EXAMPLES[0]['_id'],
            _EMOTION_EXAMPLES[2]['_id'],
            _EMOTION_EXAMPLES[4]['_id']
        ],
        'negative_set_ids': [
            _EMOTION_EXAMPLES[1]['_id'],
            _EMOTION_EXAMPLES[3]['_id'],
            _EMOTION_EXAMPLES[5]['_id']
        ],
        'class_to_explain': '1',
        'grad_layer': 'cls_grad',
        'random_state': 0
    }
    result = self.tcav.run(
        _EMOTION_EXAMPLES,
        self.model,
        self.emotion_dataset,
        config=config,
    )
    self.assertLen(result, 1)
    expected = {
        'result': {
            'score': 1.0,
            'cos_sim': [
                0.9999999581246426, 0.049332143689572144, 0.8987945047547466,
                -0.41858423757857954, 0.6908297036543664, -0.5167857909664919,
                0.8423017503220364, -0.005793079244916016, 0.8334491603894322,
                -0.4054645113448612, 0.7616102123736647, -0.4578596155267783,
                0.8366905563807711, -0.27390786544756535, 0.7325538474066896,
                0.5190287630768531, 0.8145227936096425, 0.02005592868363552,
                -0.1143256029298114, -0.1221480700842533, 0.6852995739227957,
                0.3984620730733816, 0.5211149530112407, 0.5909723902471223
            ],
            'dot_prods': [
                1385.1480610241554, 69.95638452724207, 1239.4947646060161,
                -595.253135700978, 971.5880156862692, -725.0749813217176,
                1182.8641913758102, -8.149647641120662, 1146.5803071544124,
                -576.4043054391316, 1038.3510704649307, -648.097269442522,
                1154.4720122394317, -378.32103870822493, 1024.066390571124,
                738.6959135414066, 1139.7963358416857, 28.691395032352318,
                -167.37808507284706, -176.4474746971391, 959.5159619261449,
                562.8772536987927, 716.7270332848395, 840.7031847912738
            ],
            'accuracy': 0.5,
            'cav': [[
                2.03451865, -13.17604027, -8.97477873, 14.2175882, 1.75463727,
                -0.46571316, 15.63798371, 9.93142109, -16.8851253, 9.11427972,
                -11.06386059, 7.19105954, -17.18586585, -4.17196936,
                29.00569093, 4.33784787, 12.33925142, -15.43126877,
                -24.48932802, 9.00534947, -2.19966647, -0.43898864, 10.17245833,
                -1.75776027, 5.32707314, -10.89541682, -2.57798881, 4.57338285,
                -1.9818453, 8.84294916, -5.4750157, 4.17024913, -19.01907373,
                -1.23878054, 0.78451806, -11.60109751, 6.45912611, 1.2580972,
                -11.36715828, 3.96784638, 5.01414527, -8.80780738, -17.95446165,
                6.28686056, 11.37040026, -9.99612493, -5.1080606, 9.22049438,
                -6.30251447, 3.63926025, 13.14918974, 14.03763789, 5.7383655,
                -8.20873775, 1.60465614, -17.91190497, -13.33602912,
                -4.49996902, 28.75901011, -3.47341192, -3.82988803, 7.55923052,
                -8.09407339, -16.11918298, -1.71507548, -1.36150357,
                -5.86025439, 15.68441846, -3.51353798, 2.43790252, -2.19348517,
                5.0211597, 2.37646679, 13.41654008, 0.55570348, -6.05668506,
                -8.42704919, -5.02851713, 14.84420539, -7.34185174, 4.03091355,
                -5.09645307, 8.2008516, 1.08925377, -0.58413633, 9.92903691,
                2.14726806, 13.30179969, 13.57605882, -2.90356308, -5.45706524,
                10.79901632, -7.80617599, 17.51044709, -7.5189762, -16.52918401,
                -0.52797854, -2.44961364, 10.13560444, -13.19007029,
                -0.93666233, 0.54694589, 1.92814453, -4.7547722, -1.39708064,
                6.24580764, -1.41235513, -2.47243578, 10.31137604, -11.16312467,
                -10.42280945, -13.08010871, 17.36711388, 5.96436519,
                -11.97783485, 18.54090287, 7.91701915, -17.52516079,
                -13.20368127, -26.3674698, -2.45510052, 5.56414756, 10.80934702,
                -8.21923153, -11.8954632, -4.44263104, -3.42691711, -4.79007185
            ]]
        },
        'p_val': None,
        'random_mean': 0.9285714285714286,
        'split_size': 3,
        'num_runs': 1
    }
    testing_utils.assert_deep_almost_equal(self, expected, result[0], places=3)
    # This example has enough inputs for two runs of size 3.
    config = {
        'concept_set_ids': [
            _EMOTION_EXAMPLES[1]['_id'],
            _EMOTION_EXAMPLES[2]['_id'],
            _EMOTION_EXAMPLES[4]['_id'],
            _EMOTION_EXAMPLES[5]['_id'],
            _EMOTION_EXAMPLES[10]['_id'],
            _EMOTION_EXAMPLES[9]['_id']
        ],
        'negative_set_ids': [
            _EMOTION_EXAMPLES[0]['_id'],
            _EMOTION_EXAMPLES[3]['_id'],
            _EMOTION_EXAMPLES[12]['_id'],
            _EMOTION_EXAMPLES[6]['_id'],
            _EMOTION_EXAMPLES[7]['_id'],
            _EMOTION_EXAMPLES[8]['_id']
        ],
        'class_to_explain': '0',
        'grad_layer': 'cls_grad',
        'random_state': 0
    }
    result = self.tcav.run(
        _EMOTION_EXAMPLES,
        self.model,
        self.emotion_dataset,
        config=config,
    )
    self.assertLen(result, 1)
    expected = {
        'result': {
            'score': 0.0,
            'cos_sim': [
                0.2731987606830683, 0.427838045403812, 0.3166440584420665,
                -0.1358964965831398, 0.5616614702946262, -0.16511808390168164,
                -0.05103355252438478, -0.16945565920473257, 0.28148962348967155,
                -0.18169036476392003, 0.33244873698665106, -0.13316476546155087,
                0.15226772288202886, -0.05534469666649352, 0.2886150002073456,
                0.33888135113008555, 0.12875301375254147, 0.046908665182593096,
                -0.052445114502024985, 0.088858405172313, 0.219517174438115,
                0.35833013079793435, 0.2291162415605806, 0.3635686086637199
            ],
            'dot_prods': [
                452.17220644153525, 724.9460578876271, 521.776546745851,
                -230.9170522777958, 943.8754747127095, -276.8190148523963,
                -85.63511897570154, -284.8487792023684, 462.71830216201926,
                -308.62790255581496, 541.5830529968077, -225.2299308998058,
                251.04716264718752, -91.33998249705493, 482.0991668852444,
                576.3029773313335, 215.28329927312336, 80.18458502795752,
                -91.74640483442752, 153.37559992294862, 367.2562273288043,
                604.8378479001944, 376.53473821563625, 618.003311205616
            ],
            'accuracy': 0.5,
            'cav': [[
                7.55329281, -2.62460237, -9.0024543, 0.59818666, -14.58093322,
                -15.42218557, -5.5354822, 12.364417, 21.63543734, -12.10592188,
                -28.13137179, 14.31976269, -31.59852801, -9.5688074,
                12.95670217, -22.25037747, 16.35172203, 1.05301289, -31.3269959,
                -2.59048782, 19.77513966, 10.23254963, 4.18399631, 4.87700827,
                -16.33114267, 2.79754082, 10.54735581, 11.78229815,
                -12.25723096, 4.70124727, -11.24257086, -5.43199386,
                -7.69344958, 14.56370698, -5.75688441, -5.26137455, 23.44984724,
                -6.31077994, 17.08448569, 9.05777822, 1.6895136, -17.90790926,
                -14.34180658, 10.53637, 20.52315338, 13.25800162, 10.93816225,
                -3.75636305, -7.45849454, -3.97231903, -2.56363646, 7.11917039,
                8.16617295, -7.89912694, 12.99981985, -21.34898036,
                -17.63337856, -2.63946913, -4.40439706, 6.52652447, 3.80778434,
                -9.19669592, -6.7804855, -1.69738751, 14.07850725, -19.72428768,
                -4.39987548, 0.52300402, -1.32919268, -1.17373043, 25.24466652,
                1.10433416, 2.28887964, 2.73226767, 11.38988286, -15.36477422,
                1.72328432, -10.98681852, 4.09940512, 1.35212746, -11.08208377,
                15.02775212, 24.25939889, 2.5819625, -4.82504369, -2.8454034,
                24.2562686, -4.11429068, -1.74594319, -19.37503033, 2.29546159,
                6.19942707, 3.62587758, 17.79947087, 1.53031934, 2.38564874,
                -16.59565939, -5.20520209, -4.84197479, -9.78838634,
                -12.3678921, 16.95331818, 7.66526629, 20.39393112, 2.87282507,
                -17.29492882, -7.67759675, 2.78484882, 7.79825779, -5.36932978,
                12.74328979, -5.22601154, 3.36537017, -16.2269691, 5.08010908,
                8.33516097, 6.93697789, -30.79130942, 8.65766065, 14.05914846,
                1.90387642, 7.06612428, 13.16682386, -7.97255194, -15.33183742,
                2.61200359, -2.05912859, 10.68784471
            ]]
        },
        'p_val': 0.42264973081037427,
        'random_mean': 0.0,
        'split_size': 3,
        'num_runs': 2
    }
    testing_utils.assert_deep_almost_equal(self, expected, result[0], places=3)
    # This example has enough examples for three runs of size 3 and two runs of
    # size 5, and returns results with p-value < 0.05.
    config = {
        'concept_set_ids': [
            _EMOTION_EXAMPLES[0]['_id'],
            _EMOTION_EXAMPLES[1]['_id'],
            _EMOTION_EXAMPLES[2]['_id'],
            _EMOTION_EXAMPLES[3]['_id'],
            _EMOTION_EXAMPLES[4]['_id'],
            _EMOTION_EXAMPLES[5]['_id'],
            _EMOTION_EXAMPLES[6]['_id'],
            _EMOTION_EXAMPLES[7]['_id'],
            _EMOTION_EXAMPLES[8]['_id'],
            _EMOTION_EXAMPLES[9]['_id']
        ],
        'negative_set_ids': [
            _EMOTION_EXAMPLES[10]['_id'],
            _EMOTION_EXAMPLES[11]['_id'],
            _EMOTION_EXAMPLES[12]['_id'],
            _EMOTION_EXAMPLES[13]['_id'],
            _EMOTION_EXAMPLES[14]['_id'],
            _EMOTION_EXAMPLES[15]['_id'],
            _EMOTION_EXAMPLES[16]['_id'],
            _EMOTION_EXAMPLES[17]['_id'],
            _EMOTION_EXAMPLES[18]['_id'],
            _EMOTION_EXAMPLES[19]['_id']
        ],
        'class_to_explain': '1',
        'grad_layer': 'cls_grad',
        'random_state': 0
    }
    result = self.tcav.run(
        _EMOTION_EXAMPLES,
        self.model,
        self.emotion_dataset,
        config=config,
    )
    self.assertLen(result, 1)
    expected = [{
        'result': {
            'score': 0.42857142857142855,
            'cos_sim': [
                -0.1107393877916321, -0.0993967046974328, -0.2214985917242054,
                0.08132588965575606, -0.3590211572508748, 0.18708109817461333,
                0.000724498781128839, 0.09700473783330398, -0.25015742815240055,
                0.16108236033785076, -0.10283274286140846, 0.0972663321478731,
                -0.05924679176256152, -0.048499696342091746,
                -0.4357117016074766, -0.593245752003111, -0.3645147796989344,
                -0.5507605083253673, -0.27914997949782694, -0.30908550968594417,
                -0.5584676299422896, -0.16983339994284577, -0.42587740852240746,
                -0.37482298817032594
            ],
            'dot_prods': [
                -261.4389298435066, -240.23776409902007, -520.6275907607769,
                197.11495117497446, -860.6035066083074, 447.3775519523981,
                1.7341104803878409, 232.59170976304426, -586.5576327736542,
                390.2961568516803, -238.95427152619726, 234.6617547723058,
                -139.3334215524385, -114.17392512371171, -1038.149036709951,
                -1439.0663895591745, -869.3828698612926, -1342.899780229334,
                -696.569760699206, -760.9907977738051, -1332.7284530349625,
                -408.90435403478875, -998.3360993150825, -908.8111404537224
            ],
            'accuracy': 0.75,
            'cav': [[
                -2.01128835e+01, -2.53418963e+01, 3.02074307e+01,
                3.78726319e+00, -6.30370696e+00, 1.74700275e+00, 2.26769890e+01,
                -1.06487415e+01, -1.76710295e+00, 1.53911930e+01,
                6.54822833e+00, 2.02141240e+00, 1.16919236e+00, -4.59410912e-01,
                4.54187735e+00, 1.98659207e+01, 1.01712167e-01, 1.18509678e+01,
                3.11218189e+01, 1.60491967e+01, -2.84655620e+01,
                -1.08942472e+01, -1.64852881e+01, 4.79670684e+00,
                3.08027345e+01, -2.64932823e+01, 1.14056950e+00,
                -1.45201048e+01, -8.41149151e+00, -1.26598740e+01,
                -5.87020201e+00, -2.87027220e+00, 2.13724773e+01,
                3.16488669e+01, -1.38671518e+01, 2.25443206e+01,
                -1.93658542e+01, 1.47272427e+01, 1.42369662e+01,
                -7.98238134e+00, -2.34073887e+01, 3.63425744e+00,
                3.20426643e+00, -1.57109214e+01, -1.87085664e+01,
                2.04525909e-02, 1.75499187e+01, -1.89476242e+00, 1.22602038e+01,
                -8.30212717e-01, 6.89138456e+00, -4.76848738e+01,
                -1.02832284e+01, 1.25879647e+01, -2.34436812e+01,
                3.46227988e+01, 3.98466501e+00, -4.18899781e+01, 2.31382829e+01,
                9.44899112e+00, -5.55529692e+00, 2.99535098e+01, 1.91656336e+01,
                1.90906964e+01, -1.97222943e+01, -8.81455291e+00,
                -4.31148519e-01, 1.35055378e+01, -2.93433908e+01,
                1.41053039e+01, 7.31250277e+00, 1.84754657e+01, -3.59438637e+00,
                -1.94830172e+01, 1.82690513e+00, 1.11006786e+01,
                -1.98780467e+01, 9.70483509e+00, 3.10464830e+01,
                -1.58164540e+00, 3.92983591e+01, -1.94148650e+01,
                -1.78218017e+01, 1.68395976e+01, 1.88509800e+01,
                -4.67428605e+00, -4.11605219e+01, 4.20027328e+00,
                -1.78356055e+01, 2.93033407e+01, 3.67970865e+00, 4.43803533e+00,
                -5.86478123e+00, -1.89515809e+01, -4.15883069e+00,
                -2.53725359e+01, -2.06732149e+01, 3.51060797e+00,
                7.32087133e+00, 7.98146613e-01, 3.38068707e+00, -2.48928693e+01,
                1.99907796e+00, 2.12458621e+00, -1.33852676e+01, 1.42904487e+01,
                1.33286404e-02, -3.82017252e+00, -1.04635257e+01,
                1.00280360e+01, 8.93273804e+00, 5.26520675e+00, -1.28270015e+01,
                3.54992057e+00, 2.32608393e+00, -1.84115520e+01,
                -1.74357882e+00, 2.53797871e+01, -1.75258907e+01,
                -4.22843710e+01, -1.49635329e+01, -3.77852872e+00,
                -1.25404291e+01, 1.17741455e+01, 3.35462077e+01, 1.65145075e+01,
                4.27775903e-01, 8.26403983e+00
            ]]
        },
        'p_val': 0.04400624968940752,
        'random_mean': 0.9642857142857143,
        'split_size': 5,
        'num_runs': 2
    }]
    testing_utils.assert_deep_almost_equal(self, expected, result, places=3)
if __name__ == '__main__':
  absltest.main()

================
File: lit_nlp/components/tcav_test.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Tests for lit_nlp.components.gradient_maps."""
from absl.testing import absltest
from absl.testing import parameterized
from lit_nlp.api import dataset as lit_dataset
from lit_nlp.api import model as lit_model
from lit_nlp.api import types as lit_types
from lit_nlp.components import tcav
import numpy as np
_TEST_VOCAB = ['0', '1']
class VariableOutputSpecModel(lit_model.BatchedModel):
  """A dummy model used for testing interpreter compatibility."""
  def __init__(self, output_spec: lit_types.Spec):
    self._output_spec = output_spec
  def output_spec(self) -> lit_types.Spec:
    return self._output_spec
  def input_spec(self) -> lit_types.Spec:
    return {}
  def predict_minibatch(
      self, inputs: list[lit_types.JsonDict]
  ) -> list[lit_types.JsonDict]:
    return []
class TCAVTest(parameterized.TestCase):
  def setUp(self):
    super(TCAVTest, self).setUp()
    self.tcav = tcav.TCAV()
  @parameterized.named_parameters(
      dict(
          testcase_name='compatible',
          output_spec={
              'probas': lit_types.MulticlassPreds(vocab=_TEST_VOCAB),
              'embs': lit_types.Embeddings(),
              'grads': lit_types.Gradients(
                  align='probas',
                  grad_for='embs',
                  grad_target_field_key='grad_target',
              ),
              'grad_target': lit_types.CategoryLabel(vocab=_TEST_VOCAB),
          },
          expected=True,
      ),
      dict(
          testcase_name='incompatible_align_not_in_spec',
          output_spec={
              'embs': lit_types.Embeddings(),
              'grads': lit_types.Gradients(
                  align='probas',
                  grad_for='embs',
                  grad_target_field_key='grad_target',
              ),
              'grad_target': lit_types.CategoryLabel(vocab=_TEST_VOCAB),
          },
          expected=False,
      ),
      dict(
          testcase_name='incompatible_align_undefined',
          output_spec={
              'probas': lit_types.MulticlassPreds(vocab=_TEST_VOCAB),
              'embs': lit_types.Embeddings(),
              'grads': lit_types.Gradients(
                  grad_for='embs',
                  grad_target_field_key='grad_target',
              ),
              'grad_target': lit_types.CategoryLabel(vocab=_TEST_VOCAB),
          },
          expected=False,
      ),
      dict(
          testcase_name='incompatible_align_wrong_type',
          output_spec={
              'probas': lit_types.RegressionScore(),
              'embs': lit_types.Scalar(),
              'grads': lit_types.Gradients(
                  align='probas',
                  grad_for='embs',
                  grad_target_field_key='grad_target',
              ),
              'grad_target': lit_types.CategoryLabel(vocab=_TEST_VOCAB),
          },
          expected=False,
      ),
      dict(
          testcase_name='incompatible_embeddings_not_in_spec',
          output_spec={
              'probas': lit_types.MulticlassPreds(vocab=_TEST_VOCAB),
              'grads': lit_types.Gradients(
                  align='probas',
                  grad_for='embs',
                  grad_target_field_key='grad_target',
              ),
              'grad_target': lit_types.CategoryLabel(vocab=_TEST_VOCAB),
          },
          expected=False,
      ),
      dict(
          testcase_name='incompatible_embeddings_undefined',
          output_spec={
              'probas': lit_types.MulticlassPreds(vocab=_TEST_VOCAB),
              'embs': lit_types.Embeddings(),
              'grads': lit_types.Gradients(
                  align='probas',
                  grad_target_field_key='grad_target',
              ),
              'grad_target': lit_types.CategoryLabel(vocab=_TEST_VOCAB),
          },
          expected=False,
      ),
      dict(
          testcase_name='incompatible_embeddings_wrong_type',
          output_spec={
              'probas': lit_types.MulticlassPreds(vocab=_TEST_VOCAB),
              'embs': lit_types.Scalar(),
              'grads': lit_types.Gradients(
                  align='probas',
                  grad_for='embs',
                  grad_target_field_key='grad_target',
              ),
              'grad_target': lit_types.CategoryLabel(vocab=_TEST_VOCAB),
          },
          expected=False,
      ),
      dict(
          testcase_name='incompatible_gradients_not_in_spec',
          output_spec={
              'probas': lit_types.MulticlassPreds(vocab=_TEST_VOCAB),
              'embs': lit_types.Embeddings(),
              'grad_target': lit_types.CategoryLabel(vocab=_TEST_VOCAB),
          },
          expected=False,
      ),
      dict(
          testcase_name='incompatible_gradient_target_not_in_spec',
          output_spec={
              'probas': lit_types.MulticlassPreds(vocab=_TEST_VOCAB),
              'embs': lit_types.Embeddings(),
              'grads': lit_types.Gradients(
                  align='probas',
                  grad_for='embs',
                  grad_target_field_key='grad_target',
              ),
          },
          expected=False,
      ),
      dict(
          testcase_name='incompatible_gradient_target_undefined',
          output_spec={
              'probas': lit_types.MulticlassPreds(vocab=_TEST_VOCAB),
              'embs': lit_types.Embeddings(),
              'grads': lit_types.Gradients(
                  align='probas',
                  grad_for='embs',
              ),
              'grad_target': lit_types.CategoryLabel(vocab=_TEST_VOCAB),
          },
          expected=False,
      ),
      dict(
          testcase_name='incompatible_gradient_target_wrong_type',
          output_spec={
              'probas': lit_types.MulticlassPreds(vocab=_TEST_VOCAB),
              'embs': lit_types.Embeddings(),
              'grads': lit_types.Gradients(
                  align='probas',
                  grad_for='embs',
                  grad_target_field_key='grad_target',
              ),
              'grad_target': lit_types.Scalar(),
          },
          expected=False,
      ),
  )
  def test_is_comaptible(self, output_spec: lit_types.Spec, expected: bool):
    test_model = VariableOutputSpecModel(output_spec)
    test_dataset = lit_dataset.NoneDataset({'test': test_model})
    is_compatible = self.tcav.is_compatible(test_model, test_dataset)
    self.assertEqual(is_compatible, expected)
  def test_hyp_test(self):
    # t-test where p-value != 1.
    scores = [0, 0, 0.5, 0.5, 1, 1]
    random_scores = [3, 5, -8, -100, 0, -90]
    result = self.tcav.hyp_test(scores, random_scores)
    self.assertAlmostEqual(0.1415165926492605, result)
    # t-test where p-value = 1.
    scores = [0.1, 0.13, 0.19, 0.09, 0.12, 0.1]
    random_scores = [0.1, 0.13, 0.19, 0.09, 0.12, 0.1]
    result = self.tcav.hyp_test(scores, random_scores)
    self.assertEqual(1.0, result)
  def test_compute_tcav_score(self):
    dir_deriv_positive_class = [1]
    result = self.tcav.compute_tcav_score(dir_deriv_positive_class)
    self.assertAlmostEqual(1, result)
    dir_deriv_positive_class = [0]
    result = self.tcav.compute_tcav_score(dir_deriv_positive_class)
    self.assertAlmostEqual(0, result)
    dir_deriv_positive_class = [1, -5, 4, 6.5, -3, -2.5, 0, 2]
    result = self.tcav.compute_tcav_score(dir_deriv_positive_class)
    self.assertAlmostEqual(0.5, result)
  def test_get_trained_cav(self):
    # 1D inputs.
    x = [[1], [1], [1], [2], [1], [1], [-1], [-1], [-2], [-1], [-1]]
    y = [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]
    cav, accuracy = self.tcav.get_trained_cav(x, y, 0.33, random_state=0)
    np.testing.assert_almost_equal(np.array([[19.08396947]]), cav)
    self.assertAlmostEqual(1.0, accuracy)
    # 2D inputs.
    x = [[-8, 1], [5, 3], [3, 6], [-2, 5], [-8, 10], [10, -5]]
    y = [1, 0, 0, 1, 1, 0]
    cav, accuracy = self.tcav.get_trained_cav(x, y, 0.33, random_state=0)
    np.testing.assert_almost_equal(np.array([[-77.89678676, 9.73709834]]), cav)
    self.assertAlmostEqual(1.0, accuracy)
  def test_compute_local_scores(self):
    cav = np.array([[0, 1]])
    dataset_outputs = [
        {
            'probas': [0.2, 0.8],
            'cls_emb': [5, 12]
        },
        {
            'probas': [0.6, 0.4],
            'cls_emb': [3, 4]
        }
    ]
    cos_sim, dot_prods = self.tcav.compute_local_scores(
        cav, dataset_outputs, 'cls_emb')
    self.assertListEqual([12, 4], dot_prods)
    # Magnitude of cav is 1, magnitude of cls_embs are [13, 5].
    # Cosine similarity is dot / (cav_mag * cls_embs_mag),
    # which is [12/13, 4/5].
    self.assertListEqual([0.9230769230769231, 0.8], cos_sim)
    cav = np.array([[1, 2, 3]])
    dataset_outputs = [
        {
            'probas': [0.2, 0.8],
            'cls_emb': [3, 2, 1]
        },
        {
            'probas': [0.6, 0.4],
            'cls_emb': [1, 2, 0]
        }
    ]
    cos_sim, dot_prods = self.tcav.compute_local_scores(
        cav, dataset_outputs, 'cls_emb')
    self.assertListEqual([10, 5], dot_prods)
    self.assertListEqual([0.7142857142857143, 0.5976143046671968],
                         cos_sim)
  def test_get_dir_derivs(self):
    cav = np.array([[1, 2, 3]])
    dataset_outputs = [
        {
            'probas': [0.2, 0.8],
            'cls_grad': [3, 2, 1],
            'grad_class': '1'
        },
        {
            'probas': [0.6, 0.4],
            'cls_grad': [1, 2, 0],
            'grad_class': '0'
        }
    ]
    # Example where only the first output is in class_to_explain 1.
    dir_derivs = self.tcav.get_dir_derivs(
        cav, dataset_outputs, 'cls_grad', 'grad_class',
        class_to_explain='1')
    self.assertListEqual([10], dir_derivs)
if __name__ == '__main__':
  absltest.main()

================
File: lit_nlp/components/tcav.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Quantitative Testing with Concept Activation Vectors (TCAV)."""
from collections.abc import Sequence
import dataclasses
import math
import random
from typing import Any, Optional, cast
from lit_nlp.api import components as lit_components
from lit_nlp.api import dataset as lit_dataset
from lit_nlp.api import model as lit_model
from lit_nlp.api import types
from lit_nlp.lib import utils
import numpy as np
import scipy.stats
import sklearn.linear_model
import sklearn.model_selection
JsonDict = types.JsonDict
IndexedInput = types.IndexedInput
Spec = types.Spec
NUM_SPLITS = 15  # TODO(lit-dev): Make this configurable in the UI.
RELATIVE_TCAV_SPLITS = [3, 5, 7, 10, 15]  # split sizes to try for relative TCAV
MIN_SPLIT_SIZE = 3
MIN_SPLITS = 2
@dataclasses.dataclass
class TCAVConfig(object):
  """Config options for TCAV component."""
  concept_set_ids: list[str] = dataclasses.field(default_factory=list)
  class_to_explain: str = ''
  grad_layer: str = ''
  # Percentage of the example set to use in the test set when training the LM.
  test_size: Optional[float] = 0.33
  random_state: Optional[int] = 42
  negative_set_ids: list[str] = dataclasses.field(default_factory=list)
  # Optional pre-computed CAV to use by interpreter.
  cav: Optional[Any] = None
_TCAV_CONFIG_FIELDS = [field.name for field in dataclasses.fields(TCAVConfig)]
class TCAV(lit_components.Interpreter):
  """Quantitative Testing with Concept Activation Vectors (TCAV).
  TCAV is an interpretability method which allows users to define a concept
  by selecting a set of representative examples and trains a classifier at
  different model layers to determine whether that concept has any influence
  over the predictions of the model (https://arxiv.org/pdf/1711.11279.pdf).
  If so, it can be measured as an aggregate score, and also individual points
  can be measured by how much of that concept they contain, at a given layer.
  The original implementation can be found at:
  https://github.com/tensorflow/tcav
  This component requires that the following fields in the model spec. Field
  names like `layer` are placeholders; you can call them whatever you like,
  and as with other LIT components multiple segments are supported.
    Output:
      - Embeddings (`emb_layer`) to return the input embeddings
          for a layer
      - Gradients (`grad_layer`) to return the gradients w.r.t.
          `emb_layer`
      - Gradients class (`grad_class`) to return the label that gradients
        were computed for. This is usually a CategoryLabel, but can be anything
        since it will just be fed back into the model.
      - MulticlassPreds (`probas`)
  **Note: TCAV calls the model multiple times. It is _strongly_ recommended that
  you use a caching model (i.e., wrap your model in a `CachingModelWrapper`) to
  optimize performance.**
  """
  def hyp_test(self, scores, random_scores):
    """Returns the p-value for a two-sided t-test on the TCAV score."""
    # The null hypothesis is 0.5, since a TCAV score of 0.5 would indicate
    # the concept does not affect the prediction positively or negatively.
    _, p_val = scipy.stats.ttest_ind(scores, random_scores)
    return p_val
  def is_compatible(self, model: lit_model.Model,
                    dataset: lit_dataset.Dataset) -> bool:
    del dataset  # Unused by TCAV
    output_spec = model.output_spec()
    gradient_fields = utils.find_spec_keys(output_spec, types.Gradients)
    if not gradient_fields:
      return False
    for grad_field in gradient_fields:
      field_spec = cast(types.Gradients, output_spec.get(grad_field))
      preds = output_spec.get(field_spec.align)
      compat_preds = isinstance(preds, types.MulticlassPreds)
      grad_for = output_spec.get(field_spec.grad_for)
      compat_grad_for = isinstance(grad_for, types.Embeddings)
      # TODO(b/205996131, b/294613507): remove grad_target_field_key and just
      # use the target labels from the input, similar to salience methods.
      grad_target = output_spec.get(field_spec.grad_target_field_key)
      compat_grad_target = isinstance(grad_target, types.CategoryLabel)
      if compat_preds and compat_grad_for and compat_grad_target:
        return True
    return False
  def get_predictions(
      self,
      model: lit_model.Model,
      inputs: Sequence[JsonDict],
      precomputed_outputs: Optional[Sequence[JsonDict]] = None,
  ) -> list[JsonDict]:
    """Get predictions with gradients w.r.t. argmax class."""
    # Get outputs using model.predict().
    if precomputed_outputs is None:
      predictions = list(model.predict(inputs))
    else:
      predictions = precomputed_outputs
    output_spec = model.output_spec()
    # TCAV always operates on the predicted class, but LIT models typically
    # compute gradients w.r.t. a specified target label.
    # Make new examples here with this set.
    target_fields = utils.find_spec_keys(output_spec, types.MulticlassPreds)
    valid_target_fields = [
        t for t in target_fields if getattr(output_spec[t], 'parent')
    ]
    modified_inputs = []
    for ex, preds in zip(inputs, predictions):
      overrides = {}
      for field in valid_target_fields:
        label_idx = np.argmax(preds[field])
        label = cast(types.MulticlassPreds, output_spec[field]).vocab[label_idx]
        parent_field = getattr(output_spec[field], 'parent')
        overrides[parent_field] = label
      modified_inputs.append(utils.make_modified_input(ex, overrides, 'TCAV'))
    # TODO(b/294613507): enable caching here for better performance?
    # Any examples that were unmodified will already be cached, so this is only
    # for cases where argmax pred != target label.
    predictions = list(model.predict(modified_inputs))
    return predictions
  def run(
      self,
      inputs: Sequence[JsonDict],
      model: lit_model.Model,
      dataset: lit_dataset.Dataset,
      model_outputs: Optional[list[JsonDict]] = None,
      config: Optional[JsonDict] = None) -> Optional[list[JsonDict]]:
    """Runs the TCAV method given the params in the inputs and config.
    Args:
      inputs: all examples in the dataset.
      model: the model being explained.
      dataset: the dataset which the current examples belong to.
      model_outputs: optional model outputs from calling model.predict(inputs).
      config: a config which should specify: {
          'concept_set_ids': [list of ids to use in concept set]
          'class_to_explain': [gradient class to explain],
          'grad_layer': [the Gradient field key of the layer to explain],
          'random_state': [an optional seed to make outputs deterministic]
          'test_size': [Percentage of the example set to use in the LM test set]
          'negative_set_ids': [optional list of ids to use as negative set] }
    Returns:
      A JsonDict containing the TCAV scores, directional derivatives,
      statistical test p-values, and LM accuracies.
    Raises:
      ValueError: configured `grad_layer` is not actually of type `Gradients`
    """
    if not config:
      raise TypeError('config must be provided')
    tcav_config = TCAVConfig(**{
        k: v for k, v in config.items() if k in _TCAV_CONFIG_FIELDS
    })
    # TODO(b/171513556): get these from the Dataset object once indices are
    # available there.
    dataset_examples = inputs
    # Get this layer's output spec keys for gradients and embeddings.
    grad_layer = tcav_config.grad_layer
    output_spec = model.output_spec()
    field_spec = output_spec.get(grad_layer)
    if not isinstance(field_spec, types.Gradients):
      raise ValueError(f'Configured grad_layer, {grad_layer}, must be a '
                       'Gradients field')
    field_spec = cast(types.Gradients, field_spec)
    emb_layer = field_spec.grad_for
    # Get the class that the gradients were computed for.
    grad_class_key = field_spec.grad_target_field_key
    predictions = self.get_predictions(model, dataset_examples, model_outputs)
    # If CAV is provided in config, then only calculate CAV similarity for
    # provided datapoints.
    if tcav_config.cav is not None:
      return [{
          'cos_sim':
              self._get_cos_sim(
                  np.array(tcav_config.cav), predictions, emb_layer)
      }]
    ids_set = set(tcav_config.concept_set_ids)
    concept_set_preds = [
        preds
        for ex, preds in zip(dataset_examples, predictions)
        if ex['_id'] in ids_set
    ]
    if tcav_config.negative_set_ids:
      negative_ids_set = set(tcav_config.negative_set_ids)
      negative_set_preds = [
          preds
          for ex, preds in zip(dataset_examples, predictions)
          if ex['_id'] in negative_ids_set
      ]
      return self._run_relative_tcav(
          grad_layer,
          emb_layer,
          grad_class_key,
          concept_set_preds,
          negative_set_preds,
          predictions,
          tcav_config,
      )
    else:
      non_concept_set_preds = [
          preds
          for ex, preds in zip(dataset_examples, predictions)
          if ex['_id'] not in ids_set
      ]
      return self._run_default_tcav(
          grad_layer,
          emb_layer,
          grad_class_key,
          concept_set_preds,
          non_concept_set_preds,
          predictions,
          tcav_config,
      )
  def _subsample(self, examples, n):
    return random.sample(examples, n) if n < len(examples) else examples
  def _run_default_tcav(
      self,
      grad_layer,
      emb_layer,
      grad_class_key,
      concept_outputs,
      non_concept_outputs,
      dataset_outputs,
      config,
  ):
    concept_results = []
    # If there are more concept set examples than non-concept set examples, we
    # use random splits of the concept examples as the concept set and use the
    # remainder of the dataset as the comparison set. Otherwise, we use random
    # splits of the non-concept examples as the comparison set.
    n = min(len(concept_outputs), len(non_concept_outputs))
    # If there are equal numbers of concept and non-concept examples, we
    # decrease n by one so that we also sample a different set in each TCAV run.
    if len(concept_outputs) == len(non_concept_outputs):
      n -= 1
    for _ in range(NUM_SPLITS):
      concept_split_outputs = self._subsample(concept_outputs, n)
      comparison_split_outputs = self._subsample(non_concept_outputs, n)
      concept_results.append(
          self._run_tcav(concept_split_outputs, comparison_split_outputs,
                         dataset_outputs, config.class_to_explain, emb_layer,
                         grad_layer, grad_class_key, config.test_size,
                         config.random_state))
    random_results = []
    # Get tcav scores on random splits.
    for _ in range(NUM_SPLITS):
      concept_split_outputs = self._subsample(dataset_outputs, n)
      comparison_split_outputs = self._subsample(non_concept_outputs, n)
      random_results.append(
          self._run_tcav(concept_split_outputs, comparison_split_outputs,
                         dataset_outputs, config.class_to_explain, emb_layer,
                         grad_layer, grad_class_key, config.test_size,
                         config.random_state))
    cav_scores = [res['score'] for res in concept_results]
    random_scores = [res['score'] for res in random_results]
    p_val = self.hyp_test(cav_scores, random_scores)
    random_mean = np.mean(random_scores)
    # Get index of CAV result with the highest accuracy.
    accuracies = [res['accuracy'] for res in concept_results]
    index = np.argmax(accuracies)
    # Many CAVS are trained and checked for statistical testing to calculate
    # the p-value. The values of the first CAV are returned.
    results = {
        'result': concept_results[index],
        'p_val': p_val,
        'random_mean': random_mean
    }
    return [results]
  def _run_relative_tcav(
      self,
      grad_layer,
      emb_layer,
      grad_class_key,
      positive_outputs,
      negative_outputs,
      dataset_outputs,
      config,
  ):
    # Ideally, for relative TCAV, users would test concepts with at least ~100
    # examples each so we can perform ~15 runs on unique subsets.
    # In practice, users may not pass in this many examples, so to accommodate
    # this, we use a cross-validation approach, where we will try different
    # subset split sizes, and return one with a statistically significant
    # result.
    splits = RELATIVE_TCAV_SPLITS
    min_length = min(len(positive_outputs), len(negative_outputs))
    # We set the minimum number of examples to run TCAV at 3 examples, and
    # need at least 2 runs for statistical testing. If there are too few
    # examples for this, we will perform 1 run of size
    # min(concept set length, negative set length), and return the result
    # without statistical testing.
    if (len(positive_outputs) < MIN_SPLIT_SIZE * MIN_SPLITS or
        len(negative_outputs) < MIN_SPLIT_SIZE * MIN_SPLITS):
      splits = [min_length]
    results = []
    for split in splits:
      num_runs = math.floor(min_length / split)
      # Exit if there are not enough examples for a run of this split size.
      if num_runs < 1:
        break
      concept_results = []
      # The i-th run will use the i-th (non-overlapping) subset of this split
      # size of examples.
      for i in range(num_runs):
        positive_split_outputs = positive_outputs[i * split: (i+1) * split]
        negative_split_outputs = negative_outputs[i * split: (i+1) * split]
        concept_results.append(
            self._run_tcav(positive_split_outputs, negative_split_outputs,
                           dataset_outputs, config.class_to_explain, emb_layer,
                           grad_layer, grad_class_key, config.test_size,
                           config.random_state))
      random_results = []
      # Get tcav scores on random splits.
      for _ in range(num_runs):
        positive_split_outputs = self._subsample(dataset_outputs, split)
        negative_split_outputs = self._subsample(dataset_outputs, split)
        random_results.append(
            self._run_tcav(positive_split_outputs, negative_split_outputs,
                           dataset_outputs, config.class_to_explain, emb_layer,
                           grad_layer, grad_class_key, config.test_size,
                           config.random_state))
      cav_scores = [res['score'] for res in concept_results]
      random_scores = [res['score'] for res in random_results]
      p_val = None
      if num_runs > 1:
        p_val = self.hyp_test(cav_scores, random_scores)
      random_mean = np.mean(random_scores)
      # Get index of CAV result with the highest accuracy.
      accuracies = [res['accuracy'] for res in concept_results]
      index = np.argmax(accuracies)
      # Many CAVS are trained and checked for statistical testing to calculate
      # the p-value. The values of the CAV with the highest accuracy for this
      # split is appended to the results.
      results.append({
          'result': concept_results[index],
          'p_val': p_val,
          'random_mean': random_mean,
          'split_size': split,
          'num_runs': num_runs,
      })
    tested_results = [
        result for result in results if result['p_val'] is not None
    ]
    # If there weren't enough runs for any t-testing, just return non-t-tested
    # results.
    if not tested_results:
      return results
    significant_tested_results = [
        result for result in tested_results if result['p_val'] < 0.05
    ]
    # If there were statistically significant results, return results from those
    # runs; otherwise, just return the (non-significant) t-tested results.
    if significant_tested_results:
      return significant_tested_results
    else:
      return tested_results
  def _get_training_data(self, comparison_outputs, concept_outputs, emb_layer):
    """Formats activations from model outputs as training data for the LM."""
    x = np.concatenate([[o[emb_layer] for o in comparison_outputs],
                        [o[emb_layer] for o in concept_outputs]])
    y = np.concatenate(
        [np.zeros(len(comparison_outputs)),
         np.ones(len(concept_outputs))])
    return x, y
  def _get_cos_sim(self, cav, datapoints_output: list[JsonDict],
                   emb_layer: str):
    cos_sim, _ = self.compute_local_scores(cav, datapoints_output, emb_layer)
    return cos_sim
  def _run_tcav(self,
                concept_outputs: list[JsonDict],
                comparison_outputs: list[JsonDict],
                dataset_outputs: list[JsonDict],
                class_to_explain: Any,
                emb_layer: str,
                grad_layer: str,
                grad_class_key: str,
                test_size: float,
                random_state=None):
    """Returns directional derivatives, tcav score, and LM accuracy."""
    x, y = self._get_training_data(comparison_outputs, concept_outputs,
                                   emb_layer)
    # Get CAV vector and accuracy of the trained linear model.
    cav, accuracy = self.get_trained_cav(x, y, test_size, random_state)
    # Compute directional derivatives for class to explain.
    dir_derivs = self.get_dir_derivs(cav, dataset_outputs, grad_layer,
                                     grad_class_key, class_to_explain)
    # Calculate the TCAV score using the gradient class directional derivatives.
    tcav_score = self.compute_tcav_score(dir_derivs)
    # Compute cosine similarity and dot product between CAV and activations.
    cos_sim, dot_prods = self.compute_local_scores(cav, dataset_outputs,
                                                   emb_layer)
    return {
        'score': tcav_score,
        'cos_sim': cos_sim,
        'dot_prods': dot_prods,
        'accuracy': accuracy,
        'cav': cav
    }
  def get_trained_cav(self, x, y, test_size, random_state=None):
    """Trains linear model on activations, returns weights (CAV) and accuracy."""
    x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(
        x, y, test_size=test_size, stratify=y, random_state=random_state)
    # Train linear model on training set.
    # TODO(b/177005822): Include additional linear classifier options
    # (e.g. L-BFGS, logistic regression, etc.)
    lm = sklearn.linear_model.SGDClassifier(random_state=random_state)
    lm.fit(x_train, y_train)
    cav = lm.coef_  # the weights of the LM are the CAV.
    # Compute accuracy on test set.
    y_pred = lm.predict(x_test)
    correct_count = 0
    for pred_val, test_val in zip(y_pred, y_test):
      if pred_val == test_val:
        correct_count += 1
    accuracy = correct_count / len(y_test)
    return cav, accuracy
  def get_dir_derivs(self, cav, dataset_outputs, grad_layer, grad_class_key,
                     class_to_explain):
    """Returns directional derivatives for class_to_explain examples."""
    dir_derivs = []
    for o in dataset_outputs:
      if o[grad_class_key] == class_to_explain:
        grad = o[grad_layer]
        # Multiplies the dataset_outputs gradients with the models weights
        # to get the directional derivative.
        dir_deriv = np.dot(grad, cav.flatten())
        dir_derivs.append(dir_deriv)
    return dir_derivs
  def compute_tcav_score(self, dir_derivs):
    """Returns the tcav score given the class to explain directional derivs."""
    # Maps positive derivatives to 1 and non-positive derivatives to 0.
    positive_dirs = [int(dir > 0) for dir in dir_derivs]
    # Divides the number of examples in the class_to_explain with directional
    # derivative > 0 by the total number of examples in the class_to_explain
    # to compute TCAV score.
    num_positive_dirs = sum(positive_dirs)
    return num_positive_dirs / len(dir_derivs)
  def compute_local_scores(self, cav, dataset_outputs, emb_layer):
    """Compute cosine similarity and dot product between CAV and activations."""
    flattened_cav = cav.flatten()
    dot_prods = [np.dot(flattened_cav, o[emb_layer]) for o in dataset_outputs]
    cav_magnitude = np.linalg.norm(flattened_cav)
    emb_magnitudes = [np.linalg.norm(o[emb_layer]) for o in dataset_outputs]
    cos_sim = [
        dot_prod / (emb_magnitude * cav_magnitude)
        for dot_prod, emb_magnitude in zip(dot_prods, emb_magnitudes)
    ]
    return cos_sim, dot_prods

================
File: lit_nlp/components/thresholder_int_test.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Tests for lit_nlp.components.thresholder."""
from absl.testing import absltest
from absl.testing import parameterized
from lit_nlp.api import dataset as lit_dataset
from lit_nlp.api import types as lit_types
from lit_nlp.components import thresholder
from lit_nlp.examples.glue import models as glue_models
from lit_nlp.lib import caching  # for hash id fn
from lit_nlp.lib import file_cache
BERT_TINY_PATH = file_cache.cached_path(
    'https://storage.googleapis.com/what-if-tool-resources/lit-models/sst2_tiny.tar.gz',  # pylint: disable=line-too-long
    extract_compressed_file=True,
)
_EXAMPLES = [
    {'sentence': 'a', 'label': '1', '_id': 'a'},
    {'sentence': 'b', 'label': '1', '_id': 'b'},
    {'sentence': 'c', 'label': '1', '_id': 'c'},
    {'sentence': 'd', 'label': '1', '_id': 'd'},
    {'sentence': 'e', 'label': '1', '_id': 'e'},
    {'sentence': 'f', 'label': '0', '_id': 'f'},
    {'sentence': 'g', 'label': '0', '_id': 'g'},
    {'sentence': 'h', 'label': '0', '_id': 'h'},
    {'sentence': 'i', 'label': '0', '_id': 'i'},
]
class ThresholderTest(parameterized.TestCase):
  @classmethod
  def setUpClass(cls):
    super().setUpClass()
    cls.model = caching.CachingModelWrapper(
        glue_models.SST2Model(BERT_TINY_PATH), 'test'
    )
    cls.dataset = lit_dataset.IndexedDataset(
        id_fn=caching.input_hash,
        spec={
            'sentence': lit_types.TextSegment(),
            'label': lit_types.CategoryLabel(vocab=['0', '1'])
        },
        examples=_EXAMPLES,
    )
    cls.model_outputs = list(cls.model.predict(_EXAMPLES))
  def setUp(self):
    super().setUp()
    self.thresholder = thresholder.Thresholder()
  @parameterized.named_parameters(
      ('default', None, 0.71),
      ('cost_ratio_high', {'cost_ratio': 5, 'facets': {'': {}}}, 0.86),
      ('cost_ratio_low', {'cost_ratio': 0.2, 'facets': {'': {}}}, 0),
  )
  def test_thresholder(self, config: lit_types.JsonDict, expected: float):
    # Test with default options.
    result = self.thresholder.run(
        _EXAMPLES,
        self.model,
        self.dataset,
        self.model_outputs,
        config=config
    )
    self.assertLen(result, 1)
    self.assertEqual('probas', result[0]['pred_key'])
    self.assertEqual(expected, result[0]['thresholds']['']['Single'])
  def test_thresholder_with_facets(self):
    config = {
        'cost_ratio': 1,
        'facets': {
            'label:1': {'data': _EXAMPLES[0:5]},
            'label:0': {'data': _EXAMPLES[5:9]},
        }
    }
    result = self.thresholder.run(
        _EXAMPLES,
        self.model,
        self.dataset,
        self.model_outputs,
        config=config,
    )
    thresholds = result[0]['thresholds']
    self.assertEqual(0.71, thresholds['label:0']['Single'])
    self.assertEqual(0.71, thresholds['label:1']['Single'])
    self.assertEqual(0.86, thresholds['label:0']['Individual'])
    self.assertEqual(0, thresholds['label:1']['Individual'])
    self.assertEqual(0, thresholds['label:0']['Demographic parity'])
if __name__ == '__main__':
  absltest.main()

================
File: lit_nlp/components/thresholder.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Threshold setter for binary classifiers."""
from collections.abc import Mapping, Sequence
import math
from typing import Optional
import attr
from lit_nlp.api import components as lit_components
from lit_nlp.api import dataset as lit_dataset
from lit_nlp.api import model as lit_model
from lit_nlp.api import types
from lit_nlp.components import metrics
from lit_nlp.lib import utils
import numpy as np
JsonDict = types.JsonDict
IndexedInput = types.IndexedInput
Spec = types.Spec
@attr.s(auto_attribs=True, kw_only=True)
class ThresholderConfig(object):
  """Config options for Thresholder component."""
  # Ratio of cost of a false negative to a false positive.
  cost_ratio: Optional[float] = 1
  # Facets of datapoints to calculate individual thresholds for.
  facets: Optional[Mapping[str, JsonDict]] = {'': {}}
class Thresholder(lit_components.Interpreter):
  """Determines optimal thresholds for classifiers given constraints."""
  def __init__(self):
    self.metrics_gen = metrics.BinaryConfusionMetrics()
    # Set up the fairness measure calculators, which take confusion matrix
    # metrics and calculates a score from them.
    def demo_parity_metric(stats):
      return ((stats['TP'] + stats['FP']) /
              (stats['TP'] + stats['FP'] + stats['TN'] + stats['FN']))
    def equal_acc_metric(stats):
      return ((stats['TP'] + stats['TN']) /
              (stats['TP'] + stats['FP'] + stats['TN'] + stats['FN']))
    def equal_opp_metric(stats):
      return stats['TP'] / (stats['TP'] + stats['FN'])
    self.fairness_measures = {
        'Demographic parity': demo_parity_metric,
        'Equal accuracy': equal_acc_metric,
        'Equal opporitunity': equal_opp_metric,
    }
  def is_compatible(self, model: lit_model.Model,
                    dataset: lit_dataset.Dataset) -> bool:
    del dataset  # Unused by Thresholder
    return utils.spec_contains(model.output_spec(), types.MulticlassPreds)
  def threshold_to_margin(self, thresh):
    # Convert between margin and classification threshold when displaying
    # margin as a threshold, as is done for binary classifiers.
    # Threshold is between 0 and 1 and represents the minimum score of the
    # positive (non-null) class before a datapoint is classified as positive.
    # A margin of 0 is the same as a threshold of .5 - meaning we take the
    # argmax class. A negative margin is a threshold below .5. Margin ranges
    # from -5 to 5, and can be converted the threshold through the equation
    # margin = ln(threshold / (1 - threshold)).
    if thresh == 0:
      return -5
    if thresh == 1:
      return 5
    return math.log(thresh / (1 - thresh))
  def get_cost(self, metrics_output, cost_ratio):
    return metrics_output['FP'] * cost_ratio + metrics_output['FN']
  def get_thresholds_for_pred_key(self, pred_key, i, margins_to_try,
                                  dataset_results, faceted_results, config):
    # Find the optimal threshold for the entire dataset
    metrics_list = [result[i]['metrics'] for result in dataset_results]
    costs = [self.get_cost(metrics_for_threshold, config.cost_ratio)
             for metrics_for_threshold in metrics_list]
    single_threshold = np.argmin(costs) / 100
    if not (facets := config.facets):
      return {'pred_key': pred_key, 'thresholds': {}}
    faceted_thresholds = {}
    faceted_costs = {}
    faceted_measures = {}
    # If there is only a single facet, return the single best threshold for
    # this prediction key.
    facets_keys = list(facets.keys())
    if len(facets_keys) == 1:
      faceted_thresholds[facets_keys[0]] = {
          'Single': single_threshold,
      }
      return {'pred_key': pred_key, 'thresholds': faceted_thresholds}
    # Loop through all facets of the dataset.
    for facet_key in facets_keys:
      # Find the optimal threshold for each facet individually.
      faceted_metrics_list = [
          result[i]['metrics'] for result in faceted_results[facet_key]]
      costs = [self.get_cost(metrics_for_threshold, config.cost_ratio)
               for metrics_for_threshold in faceted_metrics_list]
      ind_threshold = np.argmin(costs) / 100
      faceted_thresholds[facet_key] = {
          'Single': single_threshold,
          'Individual': ind_threshold
      }
      # Store the error costs and fairness measure scores for each measure for
      # the current facet across all possible margin values.
      measures = {}
      for measure_key in self.fairness_measures:
        measures[measure_key] = [
            self.fairness_measures[measure_key](metrics_for_threshold)
            for metrics_for_threshold in faceted_metrics_list]
      faceted_costs[facet_key] = costs
      faceted_measures[facet_key] = measures
    # Follows the same logic used in https://github.com/PAIR-code/what-if-tool
    # for calculating these thresholds.
    #
    # For all fairness measures:
    #   For all margins for first facet:
    #     For all other facets:
    #        Find margin with closest fairness measure of first facet at
    #        current margin
    #     Calculate overall cost for these margin settings settings across the
    #     facets.
    #   Save the margin settings that correspond to the lowest overall cost
    #   across faceted results.
    first_facet = facets_keys[0]
    for measure_key in self.fairness_measures:
      first_facet_costs = []
      first_facet_thresholds = []
      for threshold_idx in range(len(margins_to_try)):
        first_facet_measure = faceted_measures[
            first_facet][measure_key][threshold_idx]
        cost = faceted_costs[first_facet][threshold_idx]
        cur_thresholds = [threshold_idx / 100]
        for facet_to_check in facets_keys[1:]:
          distances_to_first_facet_measure = [
              abs(measure - first_facet_measure)
              for measure in faceted_measures[facet_to_check][measure_key]]
          threhold_idx_for_facet = np.argmin(distances_to_first_facet_measure)
          cost += faceted_costs[facet_to_check][threhold_idx_for_facet]
          cur_thresholds.append(threhold_idx_for_facet / 100)
        first_facet_costs.append(cost)
        first_facet_thresholds.append(cur_thresholds)
      min_measure_thresholds_idx = np.argmin(first_facet_costs)
      measure_thresholds = first_facet_thresholds[min_measure_thresholds_idx]
      for t_idx, facet_key in enumerate(facets_keys):
        faceted_thresholds[facet_key][measure_key] = measure_thresholds[t_idx]
    return {'pred_key': pred_key, 'thresholds': faceted_thresholds}
  def run(
      self,
      inputs: Sequence[JsonDict],
      model: lit_model.Model,
      dataset: lit_dataset.Dataset,
      model_outputs: Optional[list[JsonDict]] = None,
      config: Optional[JsonDict] = None) -> Optional[list[JsonDict]]:
    """Calculates optimal thresholds on the provided data.
    Args:
      inputs: all examples in the dataset, in the indexed input format.
      model: the model being explained.
      dataset: the dataset which the current examples belong to.
      model_outputs: optional model outputs from calling model.predict(inputs).
      config: a config which should specify TresholderConfig options.
    Returns:
      A JsonDict containing the calcuated thresholds
    """
    # Convert any facets from using IndexedInputs to using JsonDicts prior to
    # passing to self.run().
    if config and (facets := config.get('facets')):
      for facet_dict in facets.values():
        if (facet_data := facet_dict.get('data')):
          facet_dict['data'] = [ex.get('data', ex) for ex in facet_data]
    config = ThresholderConfig(**(config or {}))
    pred_keys = []
    for pred_key, pred_spec in model.output_spec().items():
      if not isinstance(pred_spec, types.MulticlassPreds):
        continue
      if not (parent_key := pred_spec.parent):
        continue
      parent_spec: Optional[types.LitType] = dataset.spec().get(parent_key)
      if self.metrics_gen.is_field_compatible(pred_spec, parent_spec):
        pred_keys.append(pred_key)
    indexed_outputs = {
        ex['_id']: output for (ex, output) in zip(inputs, model_outputs)
    }
    # Try all margins for thresholds from 0 to 1, by hundreths.
    margins_to_try = [
        self.threshold_to_margin(t) for t in np.linspace(0, 1, 101)]
    # Get binary classification metrics for all margins, for the entire
    # dataset, and also for each facet specified in the config.
    dataset_results = []
    faceted_results = {}
    # Loop over each margin/threshold to check.
    for margin in margins_to_try:
      # Set up an empty config to pass to the metrics generator.
      metrics_config = {}
      for pred_key in pred_keys:
        metrics_config[pred_key] = {'': {'margin': margin}}
      # Get and store the metrics for the entire dataset for this margin.
      dataset_results.append(self.metrics_gen.run(
          list(inputs), model, dataset, model_outputs, metrics_config))
      # Get and store the metrics for each facet of the dataset for this margin.
      if not (facets := config.facets):
        continue
      for facet_key, facet_dict in facets.items():
        if not (facet_data := facet_dict.get('data')):
          continue
        if facet_key not in faceted_results:
          faceted_results[facet_key] = []
        faceted_outputs = [indexed_outputs[ex['_id']] for ex in facet_data]
        faceted_results[facet_key].append(
            self.metrics_gen.run(
                facet_data, model, dataset, faceted_outputs, metrics_config
            )
        )
    pred_keys = [result['pred_key'] for result in dataset_results[0]]
    ret = []
    # Find threshold information for each prediction key.
    for i, pred_key in enumerate(pred_keys):
      ret.append(self.get_thresholds_for_pred_key(
          pred_key, i, margins_to_try, dataset_results, faceted_results,
          config))
    return ret

================
File: lit_nlp/components/umap_test.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Tests for lit_nlp.components.umap."""
from absl.testing import absltest
from lit_nlp.components import umap
from lit_nlp.lib import testing_utils
import numpy as np
class UmapTest(absltest.TestCase):
  def test_fit_transform(self):
    umap_model = umap.UmapModel(n_components=3)
    # Make fake embeddings.
    n = 100
    inputs = testing_utils.fake_projection_input(n, 10)
    outputs = umap_model.fit_transform(inputs)
    outputs_list = list(outputs)
    # Check that the output dict keys are correct.
    self.assertIn('z', outputs_list[0])
    # Check that the _fitted flag has been flipped.
    self.assertTrue(umap_model._fitted)
    # Check that the output shape is correct.
    output_np = np.array([o['z'] for o in outputs_list])
    shape = output_np.shape
    expected_shape = (n, 3)
    self.assertEqual(shape, expected_shape)
    self.assertFalse(np.any(np.iscomplex(output_np)))
  def test_predict_minibatch(self):
    umap_model = umap.UmapModel(n_components=3)
    # Test falsy return value when umap hasn't been initialized.
    num_dims = 10
    inputs = testing_utils.fake_projection_input(1, num_dims)
    output = umap_model.predict_minibatch(inputs)
    self.assertEqual(list(output)[0]['z'], [0, 0, 0])
    # Make dummy embeddings to warm start.
    umap_model.fit_transform(testing_utils.fake_projection_input(100, num_dims))
    # Test that we can now predict a minibatch.
    output = umap_model.predict_minibatch(inputs)
    output_shape = np.array(list(output)[0]['z']).shape
    self.assertFalse(np.any(np.iscomplex(output)))
    self.assertEqual(output_shape, (3,))
if __name__ == '__main__':
  absltest.main()

================
File: lit_nlp/components/umap.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Implementation of UMAP as a dimensionality reduction model."""
from absl import logging
from lit_nlp.api import model
from lit_nlp.lib import utils
import numpy as np
import umap
class UmapModel(model.ProjectorModel):
  """LIT model API implementation for UMAP."""
  def __init__(self, **umap_kw):
    self._umap = umap.UMAP(**umap_kw)
    self._fitted = False
  ##
  # Training methods
  def fit_transform(self, inputs):
    x_input = [i["x"] for i in inputs]
    if not x_input:
      return []
    x_train = np.stack(x_input)
    logging.info("UMAP input x_train: %s", str(x_train.shape))
    zs = self._umap.fit_transform(x_train)
    self._fitted = True
    return ({"z": utils.coerce_real(z)} for z in zs)
  ##
  # LIT model API
  def predict_minibatch(self, inputs, **unused_kw):
    if not self._fitted:
      return ({"z": [0, 0, 0]} for _ in inputs)
    x = np.stack([i["x"] for i in inputs])
    zs = self._umap.transform(x)
    return ({"z": utils.coerce_real(z)} for z in zs)

================
File: lit_nlp/components/word_replacer_test.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Tests for lit_nlp.generators.word_replacer."""
from absl.testing import absltest
from absl.testing import parameterized
from lit_nlp.api import dataset as lit_dataset
from lit_nlp.api import types as lit_types
from lit_nlp.components import word_replacer
from lit_nlp.lib import testing_utils
class WordReplacerTest(parameterized.TestCase):
  def setUp(self):
    super().setUp()
    test_spec: lit_types.Spec = {'text': lit_types.TextSegment()}
    self.model = testing_utils.RegressionModelForTesting(test_spec)
    # Dataset is only used for spec in word_replacer so define once
    self.dataset = lit_dataset.Dataset(
        spec=test_spec, examples=[{'text': 'blank'}]
    )
    self.generator = word_replacer.WordReplacer()
  def test_default_replacements(self):
    example = {'text': 'xyz yzy zzz.'}
    config = {word_replacer.FIELDS_TO_REPLACE_KEY: ['text']}
    generated = self.generator.generate(
        example, self.model, self.dataset, config
    )
    self.assertEqual(generated, [])
  def test_init_replacements(self):
    generator = word_replacer.WordReplacer(replacements={'tree': ['car']})
    example = {'text': 'black truck hit the tree'}
    config = {word_replacer.FIELDS_TO_REPLACE_KEY: ['text']}
    generated = generator.generate(example, self.model, self.dataset, config)
    self.assertEqual(generated, [{'text': 'black truck hit the car'}])
  @parameterized.named_parameters(
      dict(
          testcase_name='ascii_to_ascii_ignore_caps',
          example={'text': 'Capitalization is ignored.'},
          config={
              word_replacer.SUBSTITUTIONS_KEY: 'capitalization -> blank',
              word_replacer.FIELDS_TO_REPLACE_KEY: ['text'],
          },
          expected=[{'text': 'blank is ignored.'}],
      ),
      dict(
          testcase_name='ascii_to_ascii_respect_caps_change',
          example={'text': 'Capitalization is ignored.'},
          config={
              word_replacer.SUBSTITUTIONS_KEY: 'Capitalization -> blank',
              word_replacer.FIELDS_TO_REPLACE_KEY: ['text'],
              word_replacer.IGNORE_CASING_KEY: False,
          },
          expected=[{'text': 'blank is ignored.'}],
      ),
      dict(
          testcase_name='ascii_to_ascii_respect_caps_no_change',
          example={'text': 'Capitalization is ignored.'},
          config={
              word_replacer.SUBSTITUTIONS_KEY: 'capitalization -> blank',
              word_replacer.FIELDS_TO_REPLACE_KEY: ['text'],
              word_replacer.IGNORE_CASING_KEY: False,
          },
          expected=[],
      ),
      dict(
          testcase_name='deletion',
          example={'text': 'A storm is raging.'},
          config={
              word_replacer.SUBSTITUTIONS_KEY: 'storm -> ',
              word_replacer.FIELDS_TO_REPLACE_KEY: ['text'],
          },
          expected=[{'text': 'A  is raging.'}],
      ),
      dict(
          testcase_name='multiple_targets',
          example={'text': 'It`s raining cats and dogs.'},
          config={
              word_replacer.SUBSTITUTIONS_KEY: 'dogs -> horses|donkeys',
              word_replacer.FIELDS_TO_REPLACE_KEY: ['text'],
          },
          expected=[
              {'text': 'It`s raining cats and horses.'},
              {'text': 'It`s raining cats and donkeys.'}
          ],
      ),
      dict(
          testcase_name='multiple_words',
          example={'text': 'A red cat is coming.'},
          config={
              word_replacer.SUBSTITUTIONS_KEY: 'red cat -> black dog',
              word_replacer.FIELDS_TO_REPLACE_KEY: ['text'],
          },
          expected=[{'text': 'A black dog is coming.'}],
      ),
      dict(
          testcase_name='no_partial_match',
          example={'text': 'A catastrophic storm'},
          config={
              word_replacer.SUBSTITUTIONS_KEY: 'cat -> blank',
              word_replacer.FIELDS_TO_REPLACE_KEY: ['text'],
          },
          expected=[],
      ),
      dict(
          testcase_name='special_chars_punctuation',
          example={'text': 'A catastrophic storm .'},
          config={
              word_replacer.SUBSTITUTIONS_KEY: '. -> -',
              word_replacer.FIELDS_TO_REPLACE_KEY: ['text'],
          },
          expected=[{'text': 'A catastrophic storm -'}],
      ),
      dict(
          testcase_name='special_chars_repeated_punctuation',
          example={'text': 'A.catastrophic. storm'},
          config={
              word_replacer.SUBSTITUTIONS_KEY: '. -> -',
              word_replacer.FIELDS_TO_REPLACE_KEY: ['text'],
          },
          expected=[
              {'text': 'A-catastrophic. storm'},
              {'text': 'A.catastrophic- storm'},
          ],
      ),
      dict(
          testcase_name='special_chars_repeated_multichar_punctuation',
          example={'text': 'A...catastrophic.... storm'},
          config={
              word_replacer.SUBSTITUTIONS_KEY: '.. -> --',
              word_replacer.FIELDS_TO_REPLACE_KEY: ['text'],
          },
          expected=[
              {'text': 'A--.catastrophic.... storm'},
              {'text': 'A...catastrophic--.. storm'},
              {'text': 'A...catastrophic..-- storm'},
          ],
      ),
      dict(
          testcase_name='special_chars_underscore',
          example={'text': 'A nasty_storm is raging.'},
          config={
              word_replacer.SUBSTITUTIONS_KEY: 'nasty_storm -> nice_storm',
              word_replacer.FIELDS_TO_REPLACE_KEY: ['text'],
          },
          expected=[{'text': 'A nice_storm is raging.'}],
      ),
      dict(
          testcase_name='two_repetitions_yields_two_examples',
          example={'text': 'maybe repetition repetition maybe'},
          config={
              word_replacer.SUBSTITUTIONS_KEY: 'repetition -> blank',
              word_replacer.FIELDS_TO_REPLACE_KEY: ['text'],
          },
          expected=[
              {'text': 'maybe blank repetition maybe'},
              {'text': 'maybe repetition blank maybe'},
          ],
      ),
      dict(
          testcase_name='unicode_latin_to_ascii',
          example={'text': 'Is rpertoire a unicode word?'},
          config={
              word_replacer.SUBSTITUTIONS_KEY: 'rpertoire -> repertoire',
              word_replacer.FIELDS_TO_REPLACE_KEY: ['text'],
          },
          expected=[{'text': 'Is repertoire a unicode word?'}],
      ),
      dict(
          testcase_name='unicode_pictograph_to_unicode_pictograph',
          example={'text': ' is a black chess knight.'},
          config={
              word_replacer.SUBSTITUTIONS_KEY: ' -> ',
              word_replacer.FIELDS_TO_REPLACE_KEY: ['text'],
          },
          expected=[{'text': ' is a black chess knight.'}],
      ),
      dict(
          testcase_name='words_with_and_near_punctuation',
          example={'text': 'It`s raining cats and dogs.'},
          config={
              word_replacer.SUBSTITUTIONS_KEY: 'dogs -> blank',
              word_replacer.FIELDS_TO_REPLACE_KEY: ['text'],
          },
          expected=[{'text': 'It`s raining cats and blank.'}],
      ),
  )
  def test_replacement(
      self,
      example: lit_types.JsonDict,
      config: lit_types.JsonDict,
      expected: list[lit_types.JsonDict]
  ):
    generated = self.generator.generate(
        example, self.model, self.dataset, config
    )
    self.assertCountEqual(generated, expected)
  @parameterized.named_parameters(
      dict(
          testcase_name='multiple_replacements_all_valid',
          query_string='foo -> bar, spam -> eggs',
          expected={'foo': ['bar'], 'spam': ['eggs']},
      ),
      dict(
          testcase_name='multiple_replacements_ignore_malformed',
          query_string='foo -> bar, spam eggs',
          expected={'foo': ['bar']},
      ),
      dict(
          testcase_name='multiple_replacements_mulitple_targets',
          query_string='foo -> bar, spam -> eggs|donuts | cream',
          expected={'foo': ['bar'], 'spam': ['eggs', 'donuts', 'cream']},
      ),
      dict(
          testcase_name='empty',
          query_string='',
          expected={},
      ),
      dict(
          testcase_name='single_unicode_replacement',
          query_string=' -> ',
          expected={'': ['']},
      )
  )
  def test_parse_sub_string(
      self, query_string: str, expected: dict[str, list[str]]
  ):
    parsed = self.generator.parse_subs_string(query_string)
    self.assertEqual(parsed, expected)
if __name__ == '__main__':
  absltest.main()

================
File: lit_nlp/components/word_replacer.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Word replacement generator."""
from collections.abc import Iterator, Sequence
import re
from typing import Optional
from absl import logging
from lit_nlp.api import components as lit_components
from lit_nlp.api import dataset as lit_dataset
from lit_nlp.api import model as lit_model
from lit_nlp.api import types
from lit_nlp.lib import utils
_JsonDict = types.JsonDict
IGNORE_CASING_KEY = 'ignore_casing'
SUBSTITUTIONS_KEY = 'Substitutions'
FIELDS_TO_REPLACE_KEY = 'Fields to replace'
class WordReplacer(lit_components.Generator):
  """Generate new examples by replacing words in examples.
  Substitutions must be of the form 'foo -> bar, spam -> eggs'.
  """
  def __init__(self, replacements: Optional[dict[str, list[str]]] = None):
    # Populate dictionary with replacement options.
    if replacements is not None:
      assert isinstance(replacements, dict), 'Replacements must be a dict.'
      assert all([isinstance(tgt, list) for tgt in replacements.values()
                 ]), 'Replacement dict must be str->list[str]'
      self.default_replacements = replacements
    else:
      self.default_replacements = {}
  def parse_subs_string(self, subs_string: str,
                        ignore_casing: bool = True) -> dict[str, list[str]]:
    """Parse a substitutions list of the form 'foo -> bar, spam -> eggs' ."""
    replacements = {}
    # TODO(lit-dev) Use pyparsing if the pattern gets more complicated.
    rules = subs_string.split(',')
    for rule in rules:
      segments = re.split(r'\s*->\s*', rule)
      if len(segments) != 2:
        logging.warning("Malformed rule: '%s'", rule)
        continue
      src, tgt_str = segments
      tgts = re.split(r'\s*\|\s*', tgt_str)
      if ignore_casing:
        src = src.lower()
      replacements[src.strip()] = [tgt.strip() for tgt in tgts]
    return replacements
  def _get_replacement_pattern(self,
                               replacements: dict[str, list[str]],
                               ignore_casing: bool = True) -> re.Pattern[str]:
    r"""Generate replacement pattern for whole word match.
    If the source word does not end or begin with non-word characters
    (e.g. punctuation) then do a whole word match by using the special word
    boundary character "\b". This allows us to replace whole words ignoring
    punctuation around them (e.g. "cat." becomes "dog." with rule "cat"->"dog").
    However, if the source word has a non-word character at its edges this
    fails. For example for the rule "."-> "," it would not find "cat. " as there
    is no boundary between "." and " ". Therefore, for patterns with punctuation
    at the word boundaries, we ignore the whole word match and replace all
    instances. So "cat.dog" will become "dogdog" for "cat."->"dog" instead of
    being ignored. Also "." -> "," will replace all instances of "." with ",".
    Args:
      replacements: A dict of word replacements
      ignore_casing: Ignore casing for source words if True.
    Returns:
      regexp_pattern: Compiled regexp pattern used to find source words in
                      replacements.
    """
    re_strings = []
    for s in replacements:
      pattern_str = r'\b%s\b' % re.escape(s)
      # If the source word ends or begins with a non-word character (see above.)
      if not re.search(pattern_str, s):
        pattern_str = r'%s' % re.escape(s)
      re_strings.append(pattern_str)
    casing_flag = re.IGNORECASE if ignore_casing else 0
    return re.compile('|'.join(re_strings), casing_flag)
  def is_compatible(self, model: lit_model.Model,
                    dataset: lit_dataset.Dataset) -> bool:
    del model  # Unused by WordReplacer
    return utils.spec_contains(dataset.spec(), types.TextSegment)
  def generate_counterfactuals(
      self, text: str,
      replacement_regex: re.Pattern[str],
      replacements: dict[str, list[str]],
      ignore_casing: bool = True) -> Iterator[str]:
    """Replace each token and yield a new string each time that succeeds.
    Note: ignores casing.
    Args:
      text: input sentence
      replacement_regex: The regexp string used to find source words.
      replacements: Dictionary of source and replacement tokens.
      ignore_casing: Ignore casing if this is true.
    Yields:
      counterfactual: a string
    """
    # If replacement_regex is empty do not attempt to match.
    if not replacement_regex.pattern:
      return
    for s in replacement_regex.finditer(text):
      start, end = s.span()
      token = text[start:end]
      if ignore_casing:
        token = token.lower()
      # Yield one output for each target.
      for tgt in replacements[token]:
        yield text[:start] + tgt + text[end:]
  def generate_all(self,
                   inputs: list[_JsonDict],
                   model: lit_model.Model,
                   dataset: lit_dataset.Dataset,
                   config: Optional[_JsonDict] = None) -> list[list[_JsonDict]]:
    """Run generation on a set of inputs.
    Args:
      inputs: sequence of inputs, following model.input_spec()
      model: optional model to use to generate new examples.
      dataset: optional dataset which the current examples belong to.
      config: optional runtime config.
    Returns:
      list of list of new generated inputs, following model.input_spec()
    """
    config: _JsonDict = config or {}
    text_fields_to_replace = self._compute_fields_to_replace(dataset, config)
    return [
        self.generate(ex, model, dataset, config, text_fields_to_replace)
        for ex in inputs
    ]
  def generate(
      self,
      example: _JsonDict,
      model: lit_model.Model,
      dataset: lit_dataset.Dataset,
      config: Optional[_JsonDict] = None,
      text_fields_to_replace: Optional[Sequence[str]] = None,
  ) -> list[_JsonDict]:
    """Replace words based on replacement list.
    Note: If multiple fields are selected for replacement, this method will
    generate an example per field. For example, if there are two fields on which
    to perform replacement, the method will perform replacement first on one
    field to produce an example (other fields left intact), and then perform
    replacement on the second field (again copying all other fields from the
    original datum).
    Args:
      example: the example used for basis of generated examples.
      model: unused.
      dataset: the dataset.
      config: user-provided config properties.
      text_fields_to_replace: Opionally the fields over which the replacer runs.
    Returns:
      examples: a list of generated examples.
    """
    del model  # Unused.
    config: _JsonDict = config or {}
    ignore_casing = config.get(IGNORE_CASING_KEY, True)
    subs_string = config.get(SUBSTITUTIONS_KEY, None)
    if subs_string:
      replacements = self.parse_subs_string(
          subs_string, ignore_casing=ignore_casing)
    else:
      replacements = self.default_replacements
    # If replacements dictionary is empty, do not attempt to match.
    if not replacements:
      return []
    replacement_regex = self._get_replacement_pattern(
        replacements, ignore_casing=ignore_casing)
    # If config key is missing, generate no examples.
    if text_fields_to_replace is None:
      text_fields_to_replace = self._compute_fields_to_replace(
          dataset, config
      )
    if not text_fields_to_replace:
      return []
    new_examples = []
    for text_field in text_fields_to_replace:
      for new_val in self.generate_counterfactuals(
          example[text_field],
          replacement_regex,
          replacements,
          ignore_casing=ignore_casing,
      ):
        new_examples.append(
            utils.make_modified_input(
                example, {text_field: new_val}, 'WordReplacer'
            )
        )
    return new_examples
  def _compute_fields_to_replace(
      self, dataset: lit_dataset.Dataset, config: _JsonDict
  ) -> Sequence[str]:
    fields_to_replace: tuple[str] = tuple(config.get(FIELDS_TO_REPLACE_KEY, []))
    if not fields_to_replace:
      return []
    text_fields = utils.find_spec_keys(dataset.spec(), types.TextSegment)
    return [key for key in text_fields if key in fields_to_replace]
  def config_spec(self) -> types.Spec:
    return {
        # Requires a substitution string. Include a default.
        SUBSTITUTIONS_KEY: types.TextSegment(default='great -> terrible'),
        FIELDS_TO_REPLACE_KEY: types.MultiFieldMatcher(
            spec='input', types=['TextSegment'], select_all=True
        ),
    }

================
File: lit_nlp/dev_server.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Development wrapper for LIT server."""
import inspect
from typing import Any, Optional, Union
from absl import logging
from lit_nlp import app as lit_app
from lit_nlp.lib import wsgi_serving
import termcolor
LitServerType = Union[lit_app.LitApp, wsgi_serving.BasicDevServer,
                      wsgi_serving.NotebookWsgiServer]
WSGI_SERVERS = {}
WSGI_SERVERS['basic'] = wsgi_serving.BasicDevServer
WSGI_SERVERS['default'] = wsgi_serving.BasicDevServer
WSGI_SERVERS['notebook'] = wsgi_serving.NotebookWsgiServer
def get_lit_logo():
  """Prints the LIT logo as ASCII art."""
  # pyformat: disable
  logo = ('\n'
          r' (    (           ' '\n'
          r' )\ ) )\ )  *   ) ' '\n'
          r'(()/((()/(` )  /( ' '\n'
          r' /(_))/(_))( )(_))' '\n'
          r'(_)) (_)) (_(_()) ' '\n'
          r'| |  |_ _||_   _| ' '\n'
          r'| |__ | |   | |   ' '\n'
          r'|____|___|  |_|   ' '\n\n')
  # pyformat: enable
  return logo
def get_available_keywords(func):
  """Get names of keyword arguments to a function."""
  sig = inspect.signature(func)
  return [
      p.name
      for p in sig.parameters.values()
      if p.kind == p.POSITIONAL_OR_KEYWORD or p.kind == p.KEYWORD_ONLY
  ]
class Server(object):
  """Development version of LIT server.
  This wraps the real LIT server and allows for quick reloading of the server
  code without reloading models or datasets.
  """
  def __init__(self, *args, server_type='default', **kw):
    # We expose a single Server class to simplify client use, but internally
    # this is factored into a WSGI app (LitApp) and a webserver.
    # Positional arguments and some keywords passed to the LitApp,
    # which contains the LIT backend logic.
    self._app_args = args  # models, datasets, etc.
    self._app_kw = {
        k: kw.pop(k) for k in get_available_keywords(lit_app.LitApp) if k in kw
    }
    # Remaining keywords passed to the webserver class.
    self._server_kw = kw
    self._server_type = server_type
  def serve(self) -> Optional[LitServerType]:
    """Run server, with optional reload loop and cache saving.
    If the server type is 'external', then the app is returned instead of
    served by this module.
    Returns:
      WSGI app if the server type is 'external', server if the server type
      is 'notebook', otherwise None when serving is complete.
    """
    logging.info(termcolor.colored(get_lit_logo(), 'red', attrs=['bold']))
    logging.info(
        termcolor.colored('Starting LIT server...', 'green', attrs=['bold'])
    )
    app = lit_app.LitApp(*self._app_args, **self._app_kw)
    # If using a separate server program to serve the app, such as gunicorn,
    # then just return the WSGI app instead of serving it directly.
    if self._server_type == 'external':
      return app
    # Pre-bake mode runs any warm-start functions, saves the cache,
    # and exits. Designed to be used in container setup for faster launching.
    if self._server_type == 'prebake':
      app.save_cache()
      logging.info('Pre-bake completed; exiting server.')
      return
    server_fn = WSGI_SERVERS[self._server_type]
    server = server_fn(app, **self._server_kw)
    # server.serve isn't blocking for notebook server type.
    # For other types, the underlying server registers a SIGINT handler,
    # so if you hit Ctrl+C it will return.
    server.serve()
    if self._server_type == 'notebook':
      return server
    app.save_cache()

================
File: lit_nlp/examples/__init__.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

================
File: lit_nlp/examples/blank_slate_demo.py
================
r"""A blank demo ready to load models and datasets.
The currently supported models and datasets are:
- classification model on SST-2, with the Stanford Sentiment Treebank dataset.
- regression model on STS-B, with Semantic Textual Similarit Benchmark dataset.
- classification model on MultiNLI, with the MultiNLI dataset.
- TensorFlow Keras model for penguin classification, with the Penguin tabular
  dataset from TFDS.
To run:
  python -m lit_nlp.examples.blank_slate_demo --port=5432
Then navigate to localhost:5432 to access the demo UI.
"""
from collections.abc import Sequence
import sys
from typing import Optional
from absl import app
from absl import flags
from absl import logging
from lit_nlp import app as lit_app
from lit_nlp import dev_server
from lit_nlp import server_flags
from lit_nlp.examples.glue import data as glue_data
from lit_nlp.examples.glue import models as glue_models
from lit_nlp.examples.penguin import data as penguin_data
from lit_nlp.examples.penguin import model as penguin_model
# NOTE: additional flags defined in server_flags.py
FLAGS = flags.FLAGS
FLAGS.set_default("development_demo", True)
def get_wsgi_app() -> Optional[dev_server.LitServerType]:
  """Return WSGI app for container-hosted demos."""
  FLAGS.set_default("server_type", "external")
  FLAGS.set_default("demo_mode", True)
  # Parse flags without calling app.run(main), to avoid conflict with
  # gunicorn command line flags.
  unused = flags.FLAGS(sys.argv, known_only=True)
  if unused:
    logging.info(
        "blank_slate_demo:get_wsgi_app() called with unused args: %s", unused
    )
  return main([])
def main(argv: Sequence[str]) -> Optional[dev_server.LitServerType]:
  if len(argv) > 1:
    raise app.UsageError("Too many command-line arguments.")
  models = {}
  model_loaders: lit_app.ModelLoadersMap = {}
  # glue demo model loaders.
  model_loaders["sst2"] = (
      glue_models.SST2Model,
      glue_models.GlueModelConfig.init_spec(),
  )
  model_loaders["stsb"] = (
      glue_models.STSBModel,
      glue_models.GlueModelConfig.init_spec(),
  )
  model_loaders["mnli"] = (
      glue_models.MNLIModel,
      glue_models.GlueModelConfig.init_spec(),
  )
  # penguin demo model loaders.
  model_loaders["penguin"] = (
      penguin_model.PenguinModel,
      penguin_model.PenguinModel.init_spec(),
  )
  datasets = {}
  dataset_loaders: lit_app.DatasetLoadersMap = {}
  # glue demo dataset loaders.
  dataset_loaders["sst2"] = (glue_data.SST2Data, glue_data.SST2Data.init_spec())
  dataset_loaders["stsb"] = (glue_data.STSBData, glue_data.STSBData.init_spec())
  dataset_loaders["mnli"] = (glue_data.MNLIData, glue_data.MNLIData.init_spec())
  # penguin demo dataset loaders.
  dataset_loaders["penguin"] = (
      penguin_data.PenguinDataset,
      penguin_data.PenguinDataset.init_spec(),
  )
  # lm demo dataset loaders.
  dataset_loaders["sst (lm)"] = (
      glue_data.SST2DataForLM,
      glue_data.SST2DataForLM.init_spec(),
  )
  # Start the LIT server. See server_flags.py for server options.
  lit_demo = dev_server.Server(
      models,
      datasets,
      model_loaders=model_loaders,
      dataset_loaders=dataset_loaders,
      **server_flags.get_flags(),
  )
  return lit_demo.serve()
if __name__ == "__main__":
  app.run(main)

================
File: lit_nlp/examples/custom_module/__init__.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

================
File: lit_nlp/examples/custom_module/potato_demo.py
================
r"""Demo for a sentiment analysis model with a custom frontend build.
This demo loads a small BERT model trained on a sentiment analysis task.
It also uses a custom frontend build, which has a fun potato module!
To run locally:
  python -m lit_nlp.examples.custom_module.potato_demo --port=5432
Once you see the ASCII-art LIT logo, navigate to localhost:5432 to access the
demo UI.
"""
from collections.abc import Sequence
import os
import pathlib
import sys
from typing import Optional
from absl import app
from absl import flags
from absl import logging
from lit_nlp import dev_server
from lit_nlp import server_flags
from lit_nlp.api import layout
from lit_nlp.examples.glue import data as glue_data
from lit_nlp.examples.glue import models as glue_models
from lit_nlp.lib import file_cache
# NOTE: additional flags defined in server_flags.py
FLAGS = flags.FLAGS
FLAGS.set_default("development_demo", True)
FLAGS.set_default("default_layout", "potato")
_MODEL = flags.DEFINE_string(
    "model",
    "https://storage.googleapis.com/what-if-tool-resources/lit-models/sst2_tiny.tar.gz",
    "Path to model, as in examples/glue/demo.py")
# Use our custom frontend build from this directory.
FLAGS.set_default(
    "client_root",
    os.path.join(pathlib.Path(__file__).parent.absolute(), "build")
)
# Custom frontend layout; see api/layout.py
modules = layout.LitModuleName
POTATO_LAYOUT = layout.LitCanonicalLayout(
    upper={
        "Main": [modules.DatapointEditorModule, modules.ClassificationModule],
    },
    lower={
        "Data": [modules.DataTableModule, "potato-module"],
    },
    description="Custom layout with our spud-tastic potato module.",
)
CUSTOM_LAYOUTS = layout.DEFAULT_LAYOUTS | {"potato": POTATO_LAYOUT}
def get_wsgi_app() -> Optional[dev_server.LitServerType]:
  """Returns a LitApp instance for consumption by gunicorn."""
  FLAGS.set_default("server_type", "external")
  FLAGS.set_default("demo_mode", True)
  # Parse flags without calling app.run(main), to avoid conflict with
  # gunicorn command line flags.
  unused = flags.FLAGS(sys.argv, known_only=True)
  if unused:
    logging.info("potato_demo:get_wsgi_app() called with unused args: %s",
                 unused)
  return main([])
def main(argv: Sequence[str]) -> Optional[dev_server.LitServerType]:
  if len(argv) > 1:
    raise app.UsageError("Too many command-line arguments.")
  # Load our trained model.
  model = _MODEL.value
  if model.endswith(".tar.gz"):
    model = file_cache.cached_path(
        model, extract_compressed_file=True)
  models = {"sst": glue_models.SST2Model(model)}
  datasets = {"sst_dev": glue_data.SST2Data("validation")}
  # Start the LIT server. See server_flags.py for server options.
  lit_demo = dev_server.Server(
      models, datasets, layouts=CUSTOM_LAYOUTS, **server_flags.get_flags()
  )
  return lit_demo.serve()
if __name__ == "__main__":
  app.run(main)

================
File: lit_nlp/examples/gcp/constants.py
================
# Copyright 2024 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Constants used across the Model Server and LIT Server code surfaces."""
import enum
class LlmHTTPEndpoints(enum.Enum):
  """Names of HTTP endpoints provided by the Model Server conainer."""
  GENERATE = 'predict'
  SALIENCE = 'salience'
  TOKENIZE = 'tokenize'

================
File: lit_nlp/examples/gcp/model_server_gunicorn_config.py
================
# Copyright 2024 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""gunicorn configuration for cloud-hosted demos."""
import os
_PORT = os.getenv('PORT', '8080')
bind = f'0.0.0.0:{_PORT}'
timeout = 3600
threads = 8
worker_class = 'gthread'
wsgi_app = 'lit_nlp.examples.gcp.model_server:get_wsgi_app()'

================
File: lit_nlp/examples/gcp/model_server_test.py
================
# Copyright 2024 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
import os
from unittest import mock
from absl.testing import absltest
from absl.testing import parameterized
from lit_nlp.examples.gcp import constants as lit_gcp_constants
from lit_nlp.examples.gcp import model_server
from lit_nlp.examples.prompt_debugging import utils as pd_utils
import webtest
class TestWSGIApp(parameterized.TestCase):
  @classmethod
  def setUpClass(cls):
    super().setUpClass()
    test_model_name = 'lit_on_gcp_test_model'
    sal_name, tok_name = pd_utils.generate_model_group_names(test_model_name)
    test_model_config = f'{test_model_name}:test_model_path'
    os.environ['MODEL_CONFIG'] = test_model_config
    generation_model = mock.MagicMock()
    generation_model.predict.side_effect = [[{'response': 'test output text'}]]
    salience_model = mock.MagicMock()
    salience_model.predict.side_effect = [[{
        'tokens': ['test', 'output', 'text'],
        'grad_l2': [0.1234, 0.3456, 0.5678],
        'grad_dot_input': [0.1234, -0.3456, 0.5678],
    }]]
    tokenize_model = mock.MagicMock()
    tokenize_model.predict.side_effect = [
        [{'tokens': ['test', 'output', 'text']}]
    ]
    cls.mock_models = {
        test_model_name: generation_model,
        sal_name: salience_model,
        tok_name: tokenize_model,
    }
  @parameterized.named_parameters(
      dict(
          testcase_name=lit_gcp_constants.LlmHTTPEndpoints.GENERATE.value,
          endpoint=f'/{lit_gcp_constants.LlmHTTPEndpoints.GENERATE.value}',
          expected=[{'response': 'test output text'}],
      ),
      dict(
          testcase_name=lit_gcp_constants.LlmHTTPEndpoints.SALIENCE.value,
          endpoint=f'/{lit_gcp_constants.LlmHTTPEndpoints.SALIENCE.value}',
          expected=[{
              'tokens': ['test', 'output', 'text'],
              'grad_l2': [0.1234, 0.3456, 0.5678],
              'grad_dot_input': [0.1234, -0.3456, 0.5678],
          }],
      ),
      dict(
          testcase_name=lit_gcp_constants.LlmHTTPEndpoints.TOKENIZE.value,
          endpoint=f'/{lit_gcp_constants.LlmHTTPEndpoints.TOKENIZE.value}',
          expected=[{'tokens': ['test', 'output', 'text']}],
      ),
  )
  @mock.patch('lit_nlp.examples.prompt_debugging.models.get_models')
  def test_endpoint(self, mock_get_models, endpoint, expected):
    mock_get_models.return_value = self.mock_models
    app = webtest.TestApp(model_server.get_wsgi_app())
    response = app.post_json(endpoint, {'inputs': [{'prompt': 'test input'}]})
    self.assertEqual(response.status_code, 200)
    self.assertEqual(response.json, expected)
if __name__ == '__main__':
  absltest.main()

================
File: lit_nlp/examples/gcp/model_server.py
================
# Copyright 2024 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
r"""A model server for serving models on GCP via Gunicorn."""
from collections.abc import Sequence
import functools
import os
from absl import app
from lit_nlp.examples.gcp import constants as lit_gcp_constants
from lit_nlp.examples.prompt_debugging import models as pd_models
from lit_nlp.examples.prompt_debugging import utils as pd_utils
from lit_nlp.lib import serialize
from lit_nlp.lib import wsgi_app
DEFAULT_MODELS = 'gemma_1.1_2b_IT:gemma_1.1_instruct_2b_en'
_LlmHTTPEndpoints = lit_gcp_constants.LlmHTTPEndpoints
def get_wsgi_app() -> wsgi_app.App:
  """Return WSGI app for an LLM server."""
  def wrap_handler(predict_fn):
    @functools.wraps(predict_fn)
    def _handler(wsgiapp: wsgi_app.App, request, unused_environ):
      data = serialize.from_json(request.data) if len(request.data) else None
      inputs = data['inputs']
      outputs = predict_fn(inputs)
      response_body = serialize.to_json(list(outputs), simple=True)
      return wsgiapp.respond(request, response_body, 'application/json', 200)
    return _handler
  if not (model_config := os.getenv('MODEL_CONFIG', DEFAULT_MODELS).split(',')):
    raise ValueError('No model configuration was provided')
  elif (num_configs := len(model_config)) > 1:
    raise ValueError(
        f'Only 1 model configuration can be provided, got {num_configs}'
    )
  dl_framework = os.getenv('DL_FRAMEWORK', pd_models.DEFAULT_DL_FRAMEWORK)
  dl_runtime = os.getenv('DL_RUNTIME', pd_models.DEFAULT_DL_RUNTIME)
  precision = os.getenv('PRECISION', pd_models.DEFAULT_PRECISION)
  batch_size = int(os.getenv('BATCH_SIZE', pd_models.DEFAULT_BATCH_SIZE))
  sequence_length = int(
      os.getenv('SEQUENCE_LENGTH', pd_models.DEFAULT_SEQUENCE_LENGTH)
  )
  models = pd_models.get_models(
      models_config=model_config,
      dl_framework=dl_framework,
      dl_runtime=dl_runtime,
      precision=precision,
      batch_size=batch_size,
      max_length=sequence_length,
  )
  gen_name = model_config[0].split(':')[0]
  sal_name, tok_name = pd_utils.generate_model_group_names(gen_name)
  handlers = {
      f'/{_LlmHTTPEndpoints.GENERATE.value}': models[gen_name].predict,
      f'/{_LlmHTTPEndpoints.SALIENCE.value}': models[sal_name].predict,
      f'/{_LlmHTTPEndpoints.TOKENIZE.value}': models[tok_name].predict,
  }
  wrapped_handlers = {
      endpoint: wrap_handler(endpoint_fn)
      for endpoint, endpoint_fn in handlers.items()
  }
  return wsgi_app.App(
      wrapped_handlers, project_root='gcp', index_file='index.html'
  )
def main(argv: Sequence[str]) -> wsgi_app.App:
  if len(argv) > 1:
    raise app.UsageError('Too many command-line arguments.')
  return get_wsgi_app()
if __name__ == '__main__':
  app.run(main)

================
File: lit_nlp/examples/gcp/model.py
================
# Copyright 2024 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Wrapper for connecting to LLMs on GCP via the model_server HTTP API."""
from lit_nlp import app as lit_app
from lit_nlp.api import model as lit_model
from lit_nlp.api import types as lit_types
from lit_nlp.examples.gcp import constants as lit_gcp_constants
from lit_nlp.examples.prompt_debugging import constants as pd_constants
from lit_nlp.examples.prompt_debugging import utils as pd_utils
from lit_nlp.lib import serialize
import requests
_LlmHTTPEndpoints = lit_gcp_constants.LlmHTTPEndpoints
LLM_ON_GCP_INIT_SPEC: lit_types.Spec = {
    # Note that `new_name` is not actually passed to LlmOverHTTP but the
    # `/create_model` API will validate the config with a `new_name` in it.
    'new_name': lit_types.String(required=False),
    'base_url': lit_types.String(),
    'identity_token': lit_types.String(default=''),
    'max_concurrent_requests': lit_types.Integer(default=1),
    'max_qps': lit_types.Integer(default=25, required=False),
}
class LlmOverHTTP(lit_model.BatchedRemoteModel):
  """Model wrapper LLMs hosted in a Model Server container."""
  def __init__(
      self,
      base_url: str,
      identity_token: str,
      endpoint: str | _LlmHTTPEndpoints,
      max_concurrent_requests: int = 4,
      max_qps: int | float = 25,
  ):
    super().__init__(max_concurrent_requests, max_qps)
    self.endpoint = _LlmHTTPEndpoints(endpoint)
    self.url = f'{base_url}/{self.endpoint.value}'
    self.identity_token = identity_token
  def input_spec(self) -> lit_types.Spec:
    input_spec = pd_constants.INPUT_SPEC
    if self.endpoint == _LlmHTTPEndpoints.SALIENCE:
      input_spec |= pd_constants.INPUT_SPEC_SALIENCE
    return input_spec
  def output_spec(self) -> lit_types.Spec:
    if self.endpoint == _LlmHTTPEndpoints.GENERATE:
      return (
          pd_constants.OUTPUT_SPEC_GENERATION
          | pd_constants.OUTPUT_SPEC_GENERATION_EMBEDDINGS
      )
    elif self.endpoint == _LlmHTTPEndpoints.SALIENCE:
      return pd_constants.OUTPUT_SPEC_SALIENCE
    else:
      return pd_constants.OUTPUT_SPEC_TOKENIZER
  def predict_minibatch(
      self, inputs: list[lit_types.JsonDict]
  ) -> list[lit_types.JsonDict]:
    """Run prediction on a batch of inputs.
    Subclass should implement this.
    Args:
      inputs: sequence of inputs, following model.input_spec()
    Returns:
      list of outputs, following model.output_spec()
    Raises:
      RuntimeError: Received non-200 HTTP Status Codes in the response.
    """
    inputs = {'inputs': inputs}
    headers = {
        'Authorization': f'Bearer {self.identity_token}',
        'Content-Type': 'application/json',
    }
    response = requests.post(
        self.url, headers=headers, data=serialize.to_json(inputs, simple=True)
    )
    if not (200 <= response.status_code < 300):
      raise RuntimeError()
    outputs = serialize.from_json(response.text)
    return outputs
def initialize_model_group_for_salience(
    new_name: str, base_url: str, *args, **kw
) -> lit_model.ModelMap:
  """Creates '{name}' and '_{name}_salience' and '_{name}_tokenizer'."""
  salience_name, tokenizer_name = pd_utils.generate_model_group_names(new_name)
  generation_model = LlmOverHTTP(
      *args, base_url=base_url, endpoint=_LlmHTTPEndpoints.GENERATE, **kw
  )
  salience_model = LlmOverHTTP(
      *args, base_url=base_url, endpoint=_LlmHTTPEndpoints.SALIENCE, **kw
  )
  tokenizer_model = LlmOverHTTP(
      *args, base_url=base_url, endpoint=_LlmHTTPEndpoints.TOKENIZE, **kw
  )
  return {
      new_name: generation_model,
      salience_name: salience_model,
      tokenizer_name: tokenizer_model,
  }
def get_model_loaders() -> lit_app.ModelLoadersMap:
  return {
      'LLM (self hosted)': (
          initialize_model_group_for_salience,
          LLM_ON_GCP_INIT_SPEC,
      )
  }

================
File: lit_nlp/examples/gcp/README.md
================
# Using LLMs in LIT on Google Cloud Platform

## Developing

### Use a virtual environment

```shell
# Create and activate the virtual environment
python3 -m venv ~/.venvs/lit-on-gcp
source ~/.venvs/lit-on-gcp/bin/activate

# Install the requirements and LIT in editable mode
pip install -f ./lit_nlp/examples/gcp/requirements.txt
pip install -e .

# Optionally, install tetsing requirements
pip install -f ./requirements_test.txt
pytest pytest lit_nlp/examples/gcp
```

### Build the Docker image

```shell
docker build -f ./lit_nlp/examples/gcp/Dockerfile -t lit-app:gcp-dev .
```

### Run GPT-2 in a Docker container

```shell
# Runs GPT-2 in Keras on PyTorch
docker run --rm -p 5432:5432 -e MODEL_CONFIG=gpt2:gpt2_base_en lit-app:gcp-dev
```

================
File: lit_nlp/examples/gcp/server_gunicorn_config.py
================
# Copyright 2024 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""gunicorn configuration for cloud-hosted demos."""
import os
_PORT = os.getenv('PORT', '5432')
bind = f'0.0.0.0:{_PORT}'
timeout = 3600
threads = 8
worker_class = 'gthread'
wsgi_app = 'lit_nlp.examples.gcp.server:get_wsgi_app()'

================
File: lit_nlp/examples/gcp/server.py
================
# Copyright 2024 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Server for sequence salience with a left-to-right language model."""
from collections.abc import Sequence
import sys
from typing import Optional
from absl import app
from absl import flags
from absl import logging
from lit_nlp import dev_server
from lit_nlp import server_flags
from lit_nlp.components import scrambler
from lit_nlp.components import word_replacer
from lit_nlp.examples.gcp import model as lit_gcp_model
from lit_nlp.examples.gcp import vertexai_models
from lit_nlp.examples.prompt_debugging import datasets as pd_datasets
from lit_nlp.examples.prompt_debugging import layouts as pd_layouts
_FLAGS = flags.FLAGS
_SPLASH_SCREEN_DOC = """
# Language Model Salience
To begin, select an example, then click the segment(s) (tokens, words, etc.)
of the output that you would like to explain. Preceding segments(s) will be
highlighted according to their importance to the selected target segment(s),
with darker colors indicating a greater influence (salience) of that segment on
the model's likelihood of the target segment.
"""
def get_wsgi_app() -> Optional[dev_server.LitServerType]:
  """Return WSGI app for container-hosted demos."""
  _FLAGS.set_default("server_type", "external")
  _FLAGS.set_default("demo_mode", False)
  _FLAGS.set_default("page_title", "LM Prompt Debugging")
  _FLAGS.set_default("default_layout", pd_layouts.THREE_PANEL)
  # Parse flags without calling app.run(main), to avoid conflict with
  # gunicorn command line flags.
  unused = flags.FLAGS(sys.argv, known_only=True)
  if unused:
    logging.info("lm_demo:get_wsgi_app() called with unused args: %s", unused)
  return main([])
def main(argv: Sequence[str]) -> Optional[dev_server.LitServerType]:
  if len(argv) > 1:
    raise app.UsageError("Too many command-line arguments.")
  datasets = pd_datasets.get_datasets(
      datasets_config=pd_datasets.DEFAULT_DATASETS,
      max_examples=pd_datasets.DEFAULT_MAX_EXAMPLES,
  )
  model_loaders = lit_gcp_model.get_model_loaders()
  model_loaders["gemini"] = (
      vertexai_models.GeminiFoundationalModel,
      vertexai_models.GeminiFoundationalModel.init_spec(),
  )
  generators = {
      "word_replacer": word_replacer.WordReplacer(),
      "scrambler": scrambler.Scrambler(),
  }
  lit_demo = dev_server.Server(
      models={},
      datasets=datasets,
      layouts=pd_layouts.PROMPT_DEBUGGING_LAYOUTS,
      model_loaders=model_loaders,
      generators=generators,
      dataset_loaders=pd_datasets.get_dataset_loaders(),
      onboard_start_doc=_SPLASH_SCREEN_DOC,
      **server_flags.get_flags(),
  )
  return lit_demo.serve()
if __name__ == "__main__":
  app.run(main)

================
File: lit_nlp/examples/gcp/vertexai_models_test.py
================
# Copyright 2024 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
from unittest import mock
from absl.testing import absltest
from google.cloud import aiplatform
from vertexai import generative_models
from lit_nlp.examples.gcp import vertexai_models
class ModelsTest(absltest.TestCase):
  @mock.patch(
      "vertexai.generative_models.GenerativeModel.generate_content"
  )
  @mock.patch(
      "vertexai.generative_models.GenerativeModel.__init__",
      return_value=None,
  )
  def test_query_gemini_model(self, mock_init, mock_generate_content):
    response1 = generative_models.GenerationResponse.from_dict({
        "candidates": [{
            "content": {
                "parts": [
                    {"text": "I say yes you say no"},
                ],
                "role": "model",
            }
        }]
    })
    response2 = generative_models.GenerationResponse.from_dict({
        "candidates": [{
            "content": {
                "parts": [
                    {"text": "I have a dog"},
                ],
                "role": "model",
            }
        }]
    })
    mock_generate_content.side_effect = [response1, response2]
    model = vertexai_models.GeminiFoundationalModel(model_name="gemini-pro")
    model._model = mock.MagicMock()
    model._model.generate_content.side_effect = [response1, response2]
    output = model.predict(
        inputs=[{"prompt": "I say yes you say no"}, {"prompt": "I have a dog"}]
    )
    result = list(output)
    self.assertLen(result, 2)
    self.assertEqual(
        result,
        [
            {"response": [("I say yes you say no", None)]},
            {"response": [("I have a dog", None)]},
        ],
    )
    mock_init.assert_called_once_with("gemini-pro")
  @mock.patch("google.cloud.aiplatform.models.Endpoint.predict")
  @mock.patch(
      "google.cloud.aiplatform.models.Endpoint.__init__",
      return_value=None,
  )
  def test_query_self_hosted_generative_model(
      self, mock_init, mock_generate_content
  ):
    response1 = aiplatform.models.Prediction(
        predictions=["I say yes you say no"],
        deployed_model_id="",
    )
    response2 = aiplatform.models.Prediction(
        predictions=["I have a dog"],
        deployed_model_id="",
    )
    mock_generate_content.side_effect = [response1, response2]
    model = vertexai_models.SelfHostedGenerativeModel(
        aip_endpoint_name="endpoint_name"
    )
    model._endpoint = mock.MagicMock()
    model._endpoint.predict.side_effect = [response1, response2]
    output = model.predict(
        inputs=[{"prompt": "I say yes you say no"}, {"prompt": "I have a dog"}]
    )
    result = list(output)
    self.assertLen(result, 2)
    self.assertEqual(
        result,
        [
            {"response": [("I say yes you say no", None)]},
            {"response": [("I have a dog", None)]},
        ],
    )
    mock_init.assert_called_once_with("endpoint_name")
if __name__ == "__main__":
  absltest.main()

================
File: lit_nlp/examples/gcp/vertexai_models.py
================
# Copyright 2024 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Model Wrapper for generative models."""
from collections.abc import Iterable
import logging
import time
from typing import Optional, Union
from google.cloud import aiplatform
from vertexai import generative_models
from lit_nlp.api import model as lit_model
from lit_nlp.api import types as lit_types
_MAX_NUM_RETRIES = 5
_DEFAULT_CANDIDATE_COUNT = 1
_DEFAULT_MAX_OUTPUT_TOKENS = 256
_DEFAULT_TEMPERATURE = 0.7
class GeminiFoundationalModel(lit_model.BatchedRemoteModel):
  """GeminiFoundationalModel is a wrapper for foundatoinal Gemini models in Vertex AI Model Garden model.
  Attributes:
    model_name: The name of the model to load.
    max_concurrent_requests: The maximum number of concurrent requests to the
      model.
    max_qps: The maximum number of queries per second to the model.
    temperature: The temperature to use for the model.
    max_output_tokens: The maximum number of tokens to generate.
  Please note the model will predict all examples at a fixed temperature.
  """
  def __init__(
      self,
      model_name: str,
      max_concurrent_requests: int = 4,
      max_qps: Union[int, float] = 25,
      temperature: Optional[float] = _DEFAULT_TEMPERATURE,
      candidate_count: Optional[int] = _DEFAULT_CANDIDATE_COUNT,
      max_output_tokens: Optional[int] = _DEFAULT_MAX_OUTPUT_TOKENS,
  ):
    super().__init__(max_concurrent_requests, max_qps)
    # Connect to the remote model.
    self._generation_config = generative_models.GenerationConfig(
        temperature=temperature,
        candidate_count=candidate_count,
        max_output_tokens=max_output_tokens,
    )
    self._model = generative_models.GenerativeModel(model_name)
  def query_model(self, input_text: str) -> lit_types.ScoredTextCandidates:
    num_attempts = 0
    predictions = None
    exception = None
    while num_attempts < _MAX_NUM_RETRIES and predictions is None:
      num_attempts += 1
      try:
        predictions = self._model.generate_content(
            input_text,
            generation_config=self._generation_config,
        )
      except Exception as e:  # pylint: disable=broad-except
        wait_time = 2**num_attempts
        exception = e
        logging.warning('Waiting %ds to retry... (%s)', wait_time, e)
        time.sleep(2**num_attempts)
    if predictions is None:
      raise ValueError(
          f'Failed to get predictions. ({exception})'
      ) from exception
    if not isinstance(predictions, Iterable):
      predictions = [predictions]
    return [(prediction.text, None) for prediction in predictions]
  def predict_minibatch(
      self, inputs: list[lit_types.JsonDict]
  ) -> list[lit_types.JsonDict]:
    res = [
        {'response': self.query_model(input_dict['prompt'])}
        for input_dict in inputs
    ]
    return res
  @classmethod
  def init_spec(cls) -> lit_types.Spec:
    return {
        'model_name': lit_types.String(default='gemini-1.0-pro', required=True),
        'max_concurrent_requests': lit_types.Integer(default=4, required=False),
        'max_qps': lit_types.Integer(default=25, required=False),
        'temperature': lit_types.Scalar(
            default=_DEFAULT_TEMPERATURE, required=False
        ),
        'candidate_count': lit_types.Integer(default=1, required=False),
        'max_output_tokens': lit_types.Integer(default=256, required=False),
    }
  def input_spec(self) -> lit_types.Spec:
    return {
        'prompt': lit_types.TextSegment(),
    }
  def output_spec(self) -> lit_types.Spec:
    return {'response': lit_types.GeneratedTextCandidates(parent='prompt')}
class SelfHostedGenerativeModel(lit_model.BatchedRemoteModel):
  """SelfHostedGenerativeModel is a wrapper for self-hosted generative models.
  This model wrapper is used for self-hosted generative models that require
  self-deployment.
  The model deployment process is managed by the user, and described in
  https://cloud.google.com/vertex-ai/docs/pipelines/model-endpoint-component. It
  is recommended deploy the model in Vertex AI. After the model is deployed,
  an aip_endpoint_name will be provided, and can be used to query the
  model.
  Attributes:
    aip_endpoint_name: A fully-qualified VertexAI depolyed model endpoint
      resource name or endpoint ID.
    max_concurrent_requests: The maximum number of concurrent requests to the
      model.
    max_qps: The maximum number of queries per second to the model.
    temperature: The temperature to use for the model.
    candidate_count: The number of candidates to generate.
    max_output_tokens: The maximum number of tokens to generate.
  Please note the model will predict all examples at a fixed temperature.
  """
  def __init__(
      self,
      aip_endpoint_name: str,
      max_concurrent_requests: int = 4,
      max_qps: Union[int, float] = 25,
      temperature: Optional[float] = _DEFAULT_TEMPERATURE,
      max_output_tokens: Optional[int] = _DEFAULT_MAX_OUTPUT_TOKENS,
  ):
    super().__init__(
        max_concurrent_requests=max_concurrent_requests, max_qps=max_qps
    )
    self.temperature = temperature
    self.max_output_tokens = max_output_tokens
    self._endpoint = aiplatform.models.Endpoint(aip_endpoint_name)
  def query_model(self, input_text: str) -> lit_types.ScoredTextCandidates:
    num_attempts = 0
    predictions = None
    exception = None
    instances = [
        {
            'prompt': input_text,
            'max_tokens': self.max_output_tokens,
            'temperature': self.temperature,
        },
    ]
    while num_attempts < _MAX_NUM_RETRIES and predictions is None:
      num_attempts += 1
      try:
        predictions = self._endpoint.predict(instances).predictions
      except Exception as e:  # pylint: disable=broad-except
        wait_time = 2**num_attempts
        exception = e
        logging.warning('Waiting %ds to retry... (%s)', wait_time, e)
        time.sleep(2**num_attempts)
    if predictions is None:
      raise ValueError(
          'Failed to get predictions with endpoint %s, after %d attempts.'
          % (self._endpoint.name, _MAX_NUM_RETRIES)
      ) from exception
    if not isinstance(predictions, Iterable):
      predictions = [predictions]
    return [(prediction, None) for prediction in predictions]
  def predict_minibatch(
      self, inputs: list[lit_types.JsonDict]
  ) -> list[lit_types.JsonDict]:
    res = [
        {'response': self.query_model(input_dict['prompt'])}
        for input_dict in inputs
    ]
    return res
  @classmethod
  def init_spec(cls) -> lit_types.Spec:
    return {
        'aip_endpoint_name': lit_types.String(default='', required=True),
        'max_concurrent_requests': lit_types.Integer(default=4, required=False),
        'max_qps': lit_types.Integer(default=25, required=False),
        'temperature': lit_types.Scalar(
            default=_DEFAULT_TEMPERATURE, required=False
        ),
        'max_output_tokens': lit_types.Integer(default=256, required=False),
    }
  def input_spec(self) -> lit_types.Spec:
    return {
        'prompt': lit_types.TextSegment(),
    }
  def output_spec(self) -> lit_types.Spec:
    return {'response': lit_types.GeneratedTextCandidates(parent='prompt')}

================
File: lit_nlp/examples/glue/data.py
================
"""GLUE benchmark datasets, using TFDS or from CSV.
See https://gluebenchmark.com/ and
https://www.tensorflow.org/datasets/catalog/glue
Note that this requires the TensorFlow Datasets package, but the resulting LIT
datasets just contain regular Python/NumPy data.
"""
from typing import Optional
from lit_nlp.api import dataset as lit_dataset
from lit_nlp.api import types as lit_types
from lit_nlp.lib import file_cache
from lit_nlp.lib import utils
import pandas as pd
import tensorflow_datasets as tfds
def load_tfds(*args, do_sort=True, **kw):
  """Load from TFDS, with optional sorting."""
  # Materialize to NumPy arrays.
  # This also ensures compatibility with TF1.x non-eager mode, which doesn't
  # support direct iteration over a tf.data.Dataset.
  ret = list(tfds.as_numpy(tfds.load(*args, download=True, try_gcs=True, **kw)))
  if do_sort:
    # Recover original order, as if you loaded from a TSV file.
    ret.sort(key=lambda ex: ex['idx'])
  return ret
class CoLAData(lit_dataset.Dataset):
  """Corpus of Linguistic Acceptability.
  See
  https://www.tensorflow.org/datasets/catalog/glue#gluecola_default_config.
  """
  LABELS = ['0', '1']
  def __init__(self, split: str):
    self._examples = []
    for ex in load_tfds('glue/cola', split=split):
      self._examples.append({
          'sentence': ex['sentence'].decode('utf-8'),
          'label': self.LABELS[ex['label']],
      })
  def spec(self):
    return {
        'sentence': lit_types.TextSegment(),
        'label': lit_types.CategoryLabel(vocab=self.LABELS)
    }
class SST2Data(lit_dataset.Dataset):
  """Stanford Sentiment Treebank, binary version (SST-2).
  See https://www.tensorflow.org/datasets/catalog/glue#gluesst2.
  """
  LABELS = ['0', '1']
  TFDS_SPLITS = ['test', 'train', 'validation']
  def load_from_csv(self, path: str):
    path = file_cache.cached_path(path)
    with open(path) as fd:
      df = pd.read_csv(fd)
    if set(df.columns) != set(self.spec().keys()):
      raise ValueError(
          f'CSV columns {list(df.columns)} do not match expected'
          f' {list(self.spec().keys())}.'
      )
    df['label'] = df.label.map(str)
    return df.to_dict(orient='records')
  def load_from_tfds(self, split: str):
    if split not in self.TFDS_SPLITS:
      raise ValueError(
          f"Unsupported split '{split}'. Allowed values: {self.TFDS_SPLITS}"
      )
    ret = []
    for ex in load_tfds('glue/sst2', split=split):
      ret.append({
          'sentence': ex['sentence'].decode('utf-8'),
          'label': self.LABELS[ex['label']],
      })
    return ret
  def __init__(
      self, path_or_splitname: str, max_examples: Optional[int] = None
  ):
    if path_or_splitname.endswith('.csv'):
      self._examples = self.load_from_csv(path_or_splitname)[:max_examples]
    else:
      self._examples = self.load_from_tfds(path_or_splitname)[:max_examples]
  @classmethod
  def init_spec(cls) -> lit_types.Spec:
    return {
        'path_or_splitname': lit_types.String(
            default='validation', required=True
        ),
        'max_examples': lit_types.Integer(
            default=1000, min_val=0, max_val=10_000, required=False
        ),
    }
  def spec(self):
    return {
        'sentence': lit_types.TextSegment(),
        'label': lit_types.CategoryLabel(vocab=self.LABELS)
    }
class SST2DataForLM(SST2Data):
  """Stanford Sentiment Treebank, binary version (SST-2).
  See https://www.tensorflow.org/datasets/catalog/glue#gluesst2.
  This data is reformatted to serve the language models.
  """
  def __init__(self, path_or_splitname: str, max_examples: int = -1):
    super().__init__(path_or_splitname, max_examples)
    self._examples = [
        utils.remap_dict(ex, {'sentence': 'text'}) for ex in self._examples
    ]
  def spec(self):
    return {
        'text': lit_types.TextSegment(),
        'label': lit_types.CategoryLabel(vocab=self.LABELS),
    }
  @classmethod
  def init_spec(cls) -> lit_types.Spec:
    return {
        'path_or_splitname': lit_types.String(
            default='validation', required=True
        ),
        'max_examples': lit_types.Integer(
            default=1000, min_val=0, max_val=10_000, required=False
        ),
    }
class MRPCData(lit_dataset.Dataset):
  """Microsoft Research Paraphrase Corpus.
  See https://www.tensorflow.org/datasets/catalog/glue#gluemrpc.
  """
  LABELS = ['0', '1']
  def __init__(self, split: str):
    self._examples = []
    for ex in load_tfds('glue/mrpc', split=split):
      self._examples.append({
          'sentence1': ex['sentence1'].decode('utf-8'),
          'sentence2': ex['sentence2'].decode('utf-8'),
          'label': self.LABELS[ex['label']],
      })
  def spec(self):
    return {
        'sentence1': lit_types.TextSegment(),
        'sentence2': lit_types.TextSegment(),
        'label': lit_types.CategoryLabel(vocab=self.LABELS)
    }
class QQPData(lit_dataset.Dataset):
  """Quora Question Pairs.
  See https://www.tensorflow.org/datasets/catalog/glue#glueqqp.
  """
  LABELS = ['0', '1']
  def __init__(self, split: str):
    self._examples = []
    for ex in load_tfds('glue/qqp', split=split):
      self._examples.append({
          'question1': ex['question1'].decode('utf-8'),
          'question2': ex['question2'].decode('utf-8'),
          'label': self.LABELS[ex['label']],
      })
  def spec(self):
    return {
        'question1': lit_types.TextSegment(),
        'question2': lit_types.TextSegment(),
        'label': lit_types.CategoryLabel(vocab=self.LABELS)
    }
class STSBData(lit_dataset.Dataset):
  """Semantic Textual Similarity Benchmark (STS-B).
  Unlike the other GLUE tasks, this is formulated as a regression problem.
  See https://www.tensorflow.org/datasets/catalog/glue#gluestsb.
  """
  TFDS_SPLITS = ['test', 'train', 'validation']
  def load_from_csv(self, path: str):
    path = file_cache.cached_path(path)
    with open(path) as fd:
      df = pd.read_csv(fd)
    if set(df.columns) != set(self.spec().keys()):
      raise ValueError(
          f'CSV columns {list(df.columns)} do not match expected'
          f' {list(self.spec().keys())}.'
      )
    df['label'] = df.label.map(float)
    return df.to_dict(orient='records')
  def load_from_tfds(self, split: str):
    if split not in self.TFDS_SPLITS:
      raise ValueError(
          f"Unsupported split '{split}'. Allowed values: {self.TFDS_SPLITS}"
      )
    ret = []
    for ex in load_tfds('glue/stsb', split=split):
      ret.append({
          'sentence1': ex['sentence1'].decode('utf-8'),
          'sentence2': ex['sentence2'].decode('utf-8'),
          'label': ex['label'],
      })
    return ret
  def __init__(
      self, path_or_splitname: str, max_examples: Optional[int] = None
  ):
    if path_or_splitname.endswith('.csv'):
      self._examples = self.load_from_csv(path_or_splitname)[:max_examples]
    else:
      self._examples = self.load_from_tfds(path_or_splitname)[:max_examples]
  @classmethod
  def init_spec(cls) -> lit_types.Spec:
    return {
        'path_or_splitname': lit_types.String(
            default='validation', required=True
        ),
        'max_examples': lit_types.Integer(
            default=1000, min_val=0, max_val=10_000, required=False
        ),
    }
  def spec(self):
    return {
        'sentence1': lit_types.TextSegment(),
        'sentence2': lit_types.TextSegment(),
        'label': lit_types.Scalar(min_val=0, max_val=5),
    }
class MNLIData(lit_dataset.Dataset):
  """MultiNLI dataset.
  See https://www.tensorflow.org/datasets/catalog/glue#gluemnli.
  """
  LABELS = ['entailment', 'neutral', 'contradiction']
  TFDS_SPLITS = [
      'test_matched',
      'test_mismatched',
      'train',
      'validation_matched',
      'validation_mismatched',
  ]
  def load_from_csv(self, path: str):
    path = file_cache.cached_path(path)
    with open(path) as fd:
      df = pd.read_csv(fd)
    if set(df.columns) != set(self.spec().keys()):
      raise ValueError(
          f'CSV columns {list(df.columns)} do not match expected'
          f' {list(self.spec().keys())}.'
      )
    df['label'] = df.label.map(str)
    return df.to_dict(orient='records')
  def load_from_tfds(self, split: str):
    if split not in self.TFDS_SPLITS:
      raise ValueError(
          f"Unsupported split '{split}'. Allowed values: {self.TFDS_SPLITS}"
      )
    ret = []
    for ex in load_tfds('glue/mnli', split=split):
      ret.append({
          'premise': ex['premise'].decode('utf-8'),
          'hypothesis': ex['hypothesis'].decode('utf-8'),
          'label': self.LABELS[ex['label']],
      })
    return ret
  def __init__(
      self, path_or_splitname: str, max_examples: Optional[int] = None
  ):
    if path_or_splitname.endswith('.csv'):
      self._examples = self.load_from_csv(path_or_splitname)[:max_examples]
    else:
      self._examples = self.load_from_tfds(path_or_splitname)[:max_examples]
  @classmethod
  def init_spec(cls) -> lit_types.Spec:
    return {
        'path_or_splitname': lit_types.String(
            default='validation_matched', required=True
        ),
        'max_examples': lit_types.Integer(
            default=1000, min_val=0, max_val=10_000, required=False
        ),
    }
  def spec(self):
    return {
        'premise': lit_types.TextSegment(),
        'hypothesis': lit_types.TextSegment(),
        'label': lit_types.CategoryLabel(vocab=self.LABELS)
    }
class QNLIData(lit_dataset.Dataset):
  """NLI examples derived from SQuAD.
  See https://www.tensorflow.org/datasets/catalog/glue#glueqnli.
  """
  LABELS = ['entailment', 'not_entailment']
  def __init__(self, split: str):
    self._examples = []
    for ex in load_tfds('glue/qnli', split=split):
      self._examples.append({
          'question': ex['question'].decode('utf-8'),
          'sentence': ex['sentence'].decode('utf-8'),
          'label': self.LABELS[ex['label']],
      })
  def spec(self):
    return {
        'question': lit_types.TextSegment(),
        'sentence': lit_types.TextSegment(),
        'label': lit_types.CategoryLabel(vocab=self.LABELS)
    }
class RTEData(lit_dataset.Dataset):
  """Recognizing Textual Entailment.
  See https://www.tensorflow.org/datasets/catalog/glue#gluerte.
  """
  LABELS = ['entailment', 'not_entailment']
  def __init__(self, split: str):
    self._examples = []
    for ex in load_tfds('glue/rte', split=split):
      self._examples.append({
          'sentence1': ex['sentence1'].decode('utf-8'),
          'sentence2': ex['sentence2'].decode('utf-8'),
          'label': self.LABELS[ex['label']],
      })
  def spec(self):
    return {
        'sentence1': lit_types.TextSegment(),
        'sentence2': lit_types.TextSegment(),
        'label': lit_types.CategoryLabel(vocab=self.LABELS)
    }
class WNLIData(lit_dataset.Dataset):
  """Winograd schema challenge.
  See https://www.tensorflow.org/datasets/catalog/glue#gluewnli.
  """
  LABELS = ['0', '1']
  def __init__(self, split: str):
    self._examples = []
    for ex in load_tfds('glue/wnli', split=split):
      self._examples.append({
          'sentence1': ex['sentence1'].decode('utf-8'),
          'sentence2': ex['sentence2'].decode('utf-8'),
          'label': self.LABELS[ex['label']],
      })
  def spec(self):
    return {
        'sentence1': lit_types.TextSegment(),
        'sentence2': lit_types.TextSegment(),
        'label': lit_types.CategoryLabel(vocab=self.LABELS)
    }
class DiagnosticNLIData(lit_dataset.Dataset):
  """NLI diagnostic set; use to evaluate models trained on MultiNLI.
  See https://www.tensorflow.org/datasets/catalog/glue#glueax.
  """
  LABELS = ['entailment', 'neutral', 'contradiction']
  def __init__(self, split: str):
    self._examples = []
    for ex in load_tfds('glue/ax', split=split):
      self._examples.append({
          'premise': ex['premise'].decode('utf-8'),
          'hypothesis': ex['hypothesis'].decode('utf-8'),
          'label': self.LABELS[ex['label']],
      })
  def spec(self):
    return {
        'premise': lit_types.TextSegment(),
        'hypothesis': lit_types.TextSegment(),
        'label': lit_types.CategoryLabel(vocab=self.LABELS)
    }

================
File: lit_nlp/examples/glue/demo.py
================
r"""Example demo loading a handful of GLUE models.
For a quick-start set of models, run:
  python -m lit_nlp.examples.glue.demo \
    --quickstart --port=5432
To run with the 'normal' defaults, including full-size BERT models:
  python -m lit_nlp.examples.glue.demo --port=5432
Then navigate to localhost:5432 to access the demo UI.
"""
from collections.abc import Sequence
import sys
from typing import Optional
from absl import app
from absl import flags
from absl import logging
from lit_nlp import app as lit_app
from lit_nlp import dev_server
from lit_nlp import server_flags
from lit_nlp.examples.glue import data as glue_data
from lit_nlp.examples.glue import models as glue_models
# NOTE: additional flags defined in server_flags.py
FLAGS = flags.FLAGS
FLAGS.set_default("development_demo", True)
_QUICKSTART = flags.DEFINE_bool(
    "quickstart",
    False,
    "Quick-start mode, loads smaller models and a subset of the full data.",
)
_MODELS = flags.DEFINE_list(
    "models",
    [
        "sst2-tiny:sst2:https://storage.googleapis.com/what-if-tool-resources/lit-models/sst2_tiny.tar.gz",
        "sst2-base:sst2:https://storage.googleapis.com/what-if-tool-resources/lit-models/sst2_base.tar.gz",
        "stsb:stsb:https://storage.googleapis.com/what-if-tool-resources/lit-models/stsb_base.tar.gz",
        "mnli:mnli:https://storage.googleapis.com/what-if-tool-resources/lit-models/mnli_base.tar.gz",
    ],
    (
        "List of models to load, as <name>:<task>:<path>. See MODELS_BY_TASK"
        " for available tasks. Path should be the output of saving a"
        " transformers model, e.g. model.save_pretrained(path) and"
        " tokenizer.save_pretrained(path). Remote .tar.gz files will be"
        " downloaded and cached locally."
    ),
)
_MAX_EXAMPLES = flags.DEFINE_integer(
    "max_examples",
    None,
    "Maximum number of examples to load into LIT. "
    "Note: MNLI eval set is 10k examples, so will take a while to run and may "
    "be slow on older machines. Set --max_examples=200 for a quick start.",
)
MODELS_BY_TASK = {
    "sst2": glue_models.SST2Model,
    "stsb": glue_models.STSBModel,
    "mnli": glue_models.MNLIModel,
}
# Pre-specified set of small models, which will load and run much faster.
QUICK_START_MODELS = (
    "sst2-tiny:sst2:https://storage.googleapis.com/what-if-tool-resources/lit-models/sst2_tiny.tar.gz",
    "sst2-small:sst2:https://storage.googleapis.com/what-if-tool-resources/lit-models/sst2_small.tar.gz",
    "stsb-tiny:stsb:https://storage.googleapis.com/what-if-tool-resources/lit-models/stsb_tiny.tar.gz",
    "mnli-small:mnli:https://storage.googleapis.com/what-if-tool-resources/lit-models/mnli_small.tar.gz",
)
def get_wsgi_app() -> Optional[dev_server.LitServerType]:
  """Return WSGI app for container-hosted demos."""
  FLAGS.set_default("server_type", "external")
  FLAGS.set_default("demo_mode", True)
  # Parse flags without calling app.run(main), to avoid conflict with
  # gunicorn command line flags.
  unused = flags.FLAGS(sys.argv, known_only=True)
  if unused:
    logging.info("glue_demo:get_wsgi_app() called with unused args: %s", unused)
  return main([])
def main(argv: Sequence[str]) -> Optional[dev_server.LitServerType]:
  if len(argv) > 1:
    raise app.UsageError("Too many command-line arguments.")
  # Quick-start mode.
  if _QUICKSTART.value:
    FLAGS.models = QUICK_START_MODELS  # smaller, faster models
    if _MAX_EXAMPLES.value is None or _MAX_EXAMPLES.value > 1000:
      FLAGS.max_examples = 1000  # truncate larger eval sets
    logging.info("Quick-start mode; overriding --models and --max_examples.")
  models = {}
  model_loaders: lit_app.ModelLoadersMap = {}
  datasets = {}
  dataset_loaders: lit_app.DatasetLoadersMap = {}
  tasks_to_load = set()
  for model_string in _MODELS.value:
    # Only split on the first two ':', because path may be a URL
    # containing 'https://'
    name, task, path = model_string.split(":", 2)
    logging.info("Loading model '%s' for task '%s' from '%s'", name, task, path)
    models[name] = MODELS_BY_TASK[task](path)  # load model weights
    tasks_to_load.add(task)
    if task not in model_loaders:
      # Adds the model loader info. Since task-specific GLUE models set specific
      # __init__() values, we use the GlueModelConfig.init_spec() here because
      # it is limited to only those paramaters that will not override or
      # interfere with the parameters set by task-specific model subclasses.
      model_loaders[task] = (
          MODELS_BY_TASK[task],
          glue_models.GlueModelConfig.init_spec(),
      )
  ##
  # Load datasets for each task that we have a model for
  if "sst2" in tasks_to_load:
    logging.info("Loading data for SST-2 task.")
    # split = 'validation' will also work, but this will cause TDFS to download
    # the entire dataset which can be very slow.
    split = "https://storage.googleapis.com/what-if-tool-resources/lit-data/sst2.validation.csv"
    datasets["sst_dev"] = glue_data.SST2Data(split)
    dataset_loaders["sst2"] = (
        glue_data.SST2Data,
        glue_data.SST2Data.init_spec(),
    )
  if "stsb" in tasks_to_load:
    logging.info("Loading data for STS-B task.")
    # split = 'validation' will also work, but this will cause TDFS to download
    # the entire dataset which can be very slow.
    split = "https://storage.googleapis.com/what-if-tool-resources/lit-data/stsb.validation.csv"
    datasets["stsb_dev"] = glue_data.STSBData(split)
    dataset_loaders["stsb"] = (
        glue_data.STSBData,
        glue_data.STSBData.init_spec(),
    )
  if "mnli" in tasks_to_load:
    logging.info("Loading data for MultiNLI task.")
    # split = 'validation_matched' will also work, but this will cause TDFS to
    # download the entire dataset which can be very slow.
    split = "https://storage.googleapis.com/what-if-tool-resources/lit-data/mnli.validation_matched.csv"
    datasets["mnli_dev"] = glue_data.MNLIData(split)
    dataset_loaders["mnli"] = (
        glue_data.MNLIData,
        glue_data.MNLIData.init_spec(),
    )
  # Truncate datasets if --max_examples is set.
  if _MAX_EXAMPLES.value is not None:
    for name in datasets:
      logging.info("Dataset: '%s' with %d examples", name, len(datasets[name]))
      datasets[name] = datasets[name].slice[: _MAX_EXAMPLES.value]
      logging.info("  truncated to %d examples", len(datasets[name]))
  # Start the LIT server. See server_flags.py for server options.
  lit_demo = dev_server.Server(
      models,
      datasets,
      model_loaders=model_loaders,
      dataset_loaders=dataset_loaders,
      **server_flags.get_flags(),
  )
  return lit_demo.serve()
if __name__ == "__main__":
  app.run(main)

================
File: lit_nlp/examples/glue/model_int_test.py
================
r"""Integration tests for lit_nlp.examples.glue.models.
Test locally with:
blaze test //third_party/py/lit_nlp/examples/glue:integration_tests \
    --guitar_cluster=LOCAL \
    --test_output=streamed \
    --guitar_detach
"""
from typing import Any
from absl.testing import absltest
from absl.testing import parameterized
from lit_nlp.examples.glue import models as glue_models
from lit_nlp.lib import file_cache
# TODO(b/254110131): Fix test flakiness. Expand to SST-2, STS-B, and MNLI
class GlueModelsIntTest(parameterized.TestCase):
  def __init__(self, *args: Any, **kwargs: Any):
    super().__init__(*args, **kwargs)
    # Create the SST-2 Model
    model_path = "https://storage.googleapis.com/what-if-tool-resources/lit-models/sst2_tiny.tar.gz"  # pylint: disable=line-too-long
    if model_path.endswith(".tar.gz"):
      model_path = file_cache.cached_path(
          model_path, extract_compressed_file=True
      )
    self.sst2_model = glue_models.SST2Model(model_path)
  @parameterized.named_parameters(
      dict(
          testcase_name="default",
          config={},
      ),
      # Common individual cases
      dict(
          testcase_name="no_attention",
          config={"output_attention": False},
      ),
      dict(
          testcase_name="no_embeddings",
          config={"output_embeddings": False},
      ),
      dict(
          testcase_name="no_gradients",
          config={"compute_grads": False},
      ),
      # Common multiple cases
      dict(
          testcase_name="no_attention_or_embeddings",
          config={"output_attention": False, "output_embeddings": False},
      ),
      dict(
          testcase_name="no_attention_or_embeddings_or_gradients",
          config={
              "compute_grads": False,
              "output_attention": False,
              "output_embeddings": False,
          },
      ),
  )
  def test_sst2_model_predict(self, config: dict[str, bool]):
    # Configure model.
    if config:
      self.sst2_model.config = glue_models.GlueModelConfig(
          # Include the SST-2 default config options
          text_a_name="sentence",
          text_b_name=None,
          labels=["0", "1"],
          null_label_idx=0,
          # Add the output-affecting config options
          **config
      )
    # Run prediction to ensure no failure.
    model_in = [{"sentence": "test sentence"}]
    model_out = list(self.sst2_model.predict(model_in))
    # Sanity-check output vs output spec.
    self.assertLen(model_out, 1)
    for key in self.sst2_model.output_spec().keys():
      self.assertIn(key, model_out[0])
if __name__ == "__main__":
  absltest.main()

================
File: lit_nlp/examples/glue/model_utils_test.py
================
"""Tests for model_utils."""
import os
from absl.testing import absltest
from lit_nlp.examples.glue import model_utils
import numpy as np
import transformers
TESTDATA_PATH = os.path.join(os.path.dirname(__file__), 'testdata')
class BatchEncodePretokenizedTest(absltest.TestCase):
  def setUp(self):
    super().setUp()
    tokenizer_path = os.path.join(TESTDATA_PATH, 'bert_tokenizer')
    self.tokenizer = transformers.AutoTokenizer.from_pretrained(tokenizer_path)
  def test_tokenizer(self):
    sentences = [
        'Hello World!', 'Pineapple is not in the BERT vocabulary.', 'foobar',
        'spam and eggs'
    ]
    expected_tokens = [['hello', 'world', '!'],
                       [
                           'pine', '##apple', 'is', 'not', 'in', 'the', 'bert',
                           'vocabulary', '.'
                       ], ['foo', '##bar'], ['spa', '##m', 'and', 'eggs']]
    tokens = [self.tokenizer.tokenize(s) for s in sentences]
    self.assertEqual(tokens, expected_tokens)
  def test_tokenized_single(self):
    tokens = [['hello', 'world', '!'],
              [
                  'pine', '##apple', 'is', 'not', 'in', 'the', 'bert',
                  'vocabulary', '.'
              ]]
    encoded_input = model_utils.batch_encode_pretokenized(
        self.tokenizer, tokens)
    # We don't care about the raw IDs, but let's detokenize and make sure we get
    # something reasonable.
    recovered_tokens = [
        self.tokenizer.convert_ids_to_tokens(ids[:ntok]) for ids, ntok in zip(
            encoded_input['input_ids'].numpy(),
            encoded_input['attention_mask'].numpy().sum(axis=1))
    ]
    expected_tokens = [['[CLS]'] + ts + ['[SEP]'] for ts in tokens]
    self.assertEqual(recovered_tokens, expected_tokens)
  def test_tokenized_pairs(self):
    input_pairs = [[['hello', 'world', '!'], ['pine', '##apple']],
                   [['foo', '##bar'], ['spa', '##m', 'and', 'eggs']]]
    encoded_input = model_utils.batch_encode_pretokenized(
        self.tokenizer, *zip(*input_pairs))
    # We don't care about the raw IDs, but let's detokenize and make sure we get
    # something reasonable.
    recovered_tokens = [
        self.tokenizer.convert_ids_to_tokens(ids[:ntok]) for ids, ntok in zip(
            encoded_input['input_ids'].numpy(),
            encoded_input['attention_mask'].numpy().sum(axis=1))
    ]
    expected_tokens = [
        ['[CLS]'] + ip[0] + ['[SEP]'] + ip[1] + ['[SEP]'] for ip in input_pairs
    ]
    self.assertEqual(recovered_tokens, expected_tokens)
  def test_untokenized_single(self):
    """Check that this matches batch_encode_plus on the original text."""
    sentences = [
        'Hello World!', 'Pineapple is not in the BERT vocabulary.', 'foobar',
        'spam and eggs'
    ]
    tokens = [self.tokenizer.tokenize(s) for s in sentences]
    encoded = model_utils.batch_encode_pretokenized(self.tokenizer, tokens)
    expected_encoded = self.tokenizer.batch_encode_plus(
        sentences,
        return_tensors='tf',
        add_special_tokens=True,
        padding='longest',
        truncation='longest_first')
    for key in expected_encoded.keys():
      np.testing.assert_array_equal(encoded[key].numpy(),
                                    expected_encoded[key].numpy())
  def test_untokenized_pairs(self):
    """Check that this matches batch_encode_plus on the original text."""
    sentence_pairs = [('Hello World!',
                       'Pineapple is not in the BERT vocabulary.'),
                      ('foobar', 'spam and eggs')]
    input_tokens = [
        [self.tokenizer.tokenize(s) for s in pair] for pair in sentence_pairs
    ]
    encoded = model_utils.batch_encode_pretokenized(self.tokenizer,
                                                    *zip(*input_tokens))
    expected_encoded = self.tokenizer.batch_encode_plus(
        sentence_pairs,
        return_tensors='tf',
        add_special_tokens=True,
        padding='longest',
        truncation='longest_first')
    for key in expected_encoded.keys():
      np.testing.assert_array_equal(encoded[key].numpy(),
                                    expected_encoded[key].numpy())
  def test_extra_kw(self):
    """Check that this matches batch_encode_plus on the original text."""
    kw = dict(max_length=6)
    sentences = ['This is a somewhat longer sentence that should be truncated.']
    tokens = [self.tokenizer.tokenize(s) for s in sentences]
    encoded = model_utils.batch_encode_pretokenized(self.tokenizer, tokens,
                                                    **kw)
    expected_encoded = self.tokenizer.batch_encode_plus(
        sentences,
        return_tensors='tf',
        add_special_tokens=True,
        padding='longest',
        truncation='longest_first',
        **kw)
    for key in expected_encoded.keys():
      np.testing.assert_array_equal(encoded[key].numpy(),
                                    expected_encoded[key].numpy())
if __name__ == '__main__':
  absltest.main()

================
File: lit_nlp/examples/glue/model_utils.py
================
"""Helpers for working with transformers models."""
from typing import Optional
from absl import logging
import transformers
def load_pretrained(cls, *args, **kw):
  """Load a transformers model in TF2, with fallback to PyTorch weights."""
  try:
    return cls.from_pretrained(*args, **kw)
  except OSError as e:
    logging.warning("Caught OSError loading model: %s", e)
    logging.warning(
        "Re-trying to convert from PyTorch checkpoint (from_pt=True)")
    return cls.from_pretrained(*args, from_pt=True, **kw)
def batch_encode_pretokenized(
    tokenizer: transformers.PreTrainedTokenizerBase,
    tokenized_inputs: list[list[str]],
    tokenized_pair_inputs: Optional[list[list[str]]] = None,
    tensor_type="tf",
    **kw
) -> transformers.BatchEncoding:
  """Batch encode pre-tokenized text, without further splitting.
  This is necessary because tokenizer(..., is_split_into_words=True) doesn't
  guarantee that tokens will stay intact - only that the final tokens will not
  span the given boundaries. If the tokenizer is called directly, you'll get
  things like: "foo" "##bar" -> "foo" "#" "#" "bar"
  Based on the implementation of batch_encode_plus in
  https://github.com/huggingface/transformers/blob/v4.1.1/src/transformers/tokenization_utils_base.py#L2489
  Args:
    tokenizer: Transformers tokenizer
    tokenized_inputs: list of tokenized inputs
    tokenized_pair_inputs: (optional) list of tokenized second-segment inputs
    tensor_type: tensor type to return
    **kw: additional args, forwarded to tokenizer.prepare_for_model
  Returns:
    BatchEncoding, suitable for model input
  """
  encoded_input = {}
  tokenized_pair_inputs = (
      tokenized_pair_inputs or [None] * len(tokenized_inputs))
  for tokens, pair_tokens in zip(tokenized_inputs, tokenized_pair_inputs):
    ids = tokenizer.convert_tokens_to_ids(tokens)
    pair_ids = (
        tokenizer.convert_tokens_to_ids(pair_tokens)
        if pair_tokens is not None else None)
    encoded = tokenizer.prepare_for_model(
        ids,
        pair_ids=pair_ids,
        add_special_tokens=True,
        padding="do_not_pad",
        truncation="longest_first",
        return_attention_mask=False,
        pad_to_multiple_of=False,
        **kw)
    for k, v in encoded.items():
      encoded_input.setdefault(k, []).append(v)
  encoded_input = tokenizer.pad(
      encoded_input, padding="longest", return_attention_mask=True)
  return transformers.BatchEncoding(encoded_input, tensor_type=tensor_type)

================
File: lit_nlp/examples/glue/models_test.py
================
"""Tests for lit_nlp.examples.models.glue_models."""
from absl.testing import absltest
from absl.testing import parameterized
import attr
from lit_nlp.examples.glue import models as glue_models
import numpy as np
@attr.s(auto_attribs=True, kw_only=True)
class GlueModelConfigForTesting(object):
  num_hidden_layers: int = 3
@attr.s(auto_attribs=True, kw_only=True)
class GlueModelInternalForTesting(object):
  config = GlueModelConfigForTesting()
class GlueModelForTesting(glue_models.GlueModel):
  """Glue model for testing, which skips Huggingface initializations."""
  def _load_model(self, model_name_or_path):
    del model_name_or_path  # unused
    self.model = GlueModelInternalForTesting()
class GlueModelsTest(parameterized.TestCase):
  @parameterized.named_parameters(
      dict(
          testcase_name="default",
          config={},
          expect_attention=True,
          expect_embs=True,
          expect_grads=True,
      ),
      # Common individual cases
      dict(
          testcase_name="no_attention",
          config={"output_attention": False},
          expect_attention=False,
          expect_embs=True,
          expect_grads=True,
      ),
      dict(
          testcase_name="no_embeddings",
          config={"output_embeddings": False},
          expect_attention=True,
          expect_embs=False,
          expect_grads=True,
      ),
      dict(
          testcase_name="no_gradients",
          config={"compute_grads": False},
          expect_attention=True,
          expect_embs=True,
          expect_grads=False,
      ),
      # Common multiple cases
      dict(
          testcase_name="no_attention_or_embeddings",
          config={"output_attention": False, "output_embeddings": False},
          expect_attention=False,
          expect_embs=False,
          expect_grads=True,
      ),
      dict(
          testcase_name="no_attention_or_embeddings_or_gradients",
          config={
              "compute_grads": False,
              "output_attention": False,
              "output_embeddings": False,
          },
          expect_attention=False,
          expect_embs=False,
          expect_grads=False,
      ),
  )
  def test_spec_affecting_config_options(
      self,
      config: dict[str, bool],
      expect_attention: bool,
      expect_embs: bool,
      expect_grads: bool,
  ):
    model = GlueModelForTesting(
        model_name_or_path="bert-base-uncased", **config
    )
    input_spec = model.input_spec()
    output_spec = model.output_spec()
    attention_fields = [key for key in output_spec if "/attention" in key]
    avg_emb_fields = [key for key in output_spec if "/avg_emb" in key]
    text_a_embs = "input_embs_" + model.config.text_a_name
    text_b_embs = "input_embs_" + model.config.text_b_name
    text_a_token_grads = "token_grad_" + model.config.text_a_name
    text_b_token_grads = "token_grad_" + model.config.text_b_name
    self.assertEqual(model.config.compute_grads, expect_grads)
    self.assertEqual(model.config.output_attention, expect_attention)
    self.assertEqual(model.config.output_embeddings, expect_embs)
    # Check required fields in input spec, should only be the text inputs.
    for key, field_spec in input_spec.items():
      if key == model.config.text_a_name or key == model.config.text_b_name:
        self.assertTrue(field_spec.required)
      else:
        self.assertFalse(field_spec.required)
    # Check required fields in output spec.
    for key, field_spec in output_spec.items():
      self.assertTrue(field_spec.required)
    if expect_attention:
      self.assertLen(attention_fields, model.model.config.num_hidden_layers)
    else:
      self.assertEmpty(attention_fields)
    if expect_embs:
      self.assertLen(avg_emb_fields, model.model.config.num_hidden_layers + 1)
      self.assertIn(text_a_embs, input_spec)
      self.assertIn(text_b_embs, input_spec)
      self.assertIn("cls_emb", output_spec)
      self.assertIn(text_a_embs, output_spec)
      self.assertIn(text_b_embs, output_spec)
    else:
      self.assertEmpty(avg_emb_fields)
      self.assertNotIn(text_a_embs, input_spec)
      self.assertNotIn(text_b_embs, input_spec)
      self.assertNotIn("cls_emb", output_spec)
      self.assertNotIn(text_a_embs, output_spec)
      self.assertNotIn(text_b_embs, output_spec)
    if expect_embs and expect_grads:
      self.assertIn(text_a_token_grads, output_spec)
      self.assertIn(text_b_token_grads, output_spec)
    else:
      self.assertNotIn(text_a_token_grads, output_spec)
      self.assertNotIn(text_b_token_grads, output_spec)
  def test_scatter_all_embeddings_single_input(self):
    glue_model = GlueModelForTesting(
        model_name_or_path="bert-base-uncased", text_a_name="sentence1"
    )
    emb_size = 10
    # We'll inject zeros for the embeddings of 'hi',
    # while special tokens get vectors of 1s.
    embs_a = np.zeros((1, emb_size))
    input_embs = np.ones((1, 3, emb_size))
    # Scatter embs_a into input_embs
    result = glue_model.scatter_all_embeddings(
        [{
            "sentence1": "hi",
            "input_embs_sentence1": embs_a,
        }],
        input_embs,
    )
    target = [[
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
    ]]
    np.testing.assert_almost_equal(result, target)
  def test_scatter_all_embeddings_both_inputs(self):
    glue_model = GlueModelForTesting(
        model_name_or_path="bert-base-uncased",
        text_a_name="sentence1",
        text_b_name="sentence2",
    )
    emb_size = 10
    # Inject zeros at positions corresponding to real tokens
    # in each segment. Special tokens get vectors of 1s.
    embs_a = np.zeros((1, emb_size))
    embs_b = np.zeros((3, emb_size))
    input_embs = np.ones((1, 7, emb_size))
    # Scatter embs_a and embs_b into input_embs
    result = glue_model.scatter_all_embeddings(
        [{
            "sentence1": "hi",
            "input_embs_sentence1": embs_a,
            "sentence2": "how are you",
            "input_embs_sentence2": embs_b,
        }],
        input_embs,
    )
    target = [[
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
    ]]
    np.testing.assert_almost_equal(result, target)
  def test_scatter_all_embeddings_multi_batch(self):
    glue_model = GlueModelForTesting(
        model_name_or_path="bert-base-uncased", text_a_name="sentence1"
    )
    emb_size = 4
    embs_a = np.zeros((1, emb_size))
    embs_b = np.zeros((2, emb_size))
    input_embs = np.ones((2, 4, emb_size))
    # Scatter embs_a and embs_b into input_embs
    result = glue_model.scatter_all_embeddings(
        [
            {
                "sentence1": "hi",
                "input_embs_sentence1": embs_a,
            },
            {
                "sentence1": "hi there",
                "input_embs_sentence1": embs_b,
            },
        ],
        input_embs,
    )
    target = [
        [[1, 1, 1, 1], [0, 0, 0, 0], [1, 1, 1, 1], [1, 1, 1, 1]],
        [[1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0], [1, 1, 1, 1]],
    ]
    np.testing.assert_almost_equal(result, target)
    # Scatter only embs_a into input_embs
    result = glue_model.scatter_all_embeddings(
        [
            {
                "sentence1": "hi",
                "input_embs_sentence1": embs_a,
            },
            {"sentence1": "hi there"},
        ],
        input_embs,
    )
    target = [
        [[1, 1, 1, 1], [0, 0, 0, 0], [1, 1, 1, 1], [1, 1, 1, 1]],
        [[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]],
    ]
    np.testing.assert_almost_equal(result, target)
    # Scatter only embs_b into input_embs
    result = glue_model.scatter_all_embeddings(
        [
            {"sentence1": "hi"},
            {
                "sentence1": "hi there",
                "input_embs_sentence1": embs_b,
            },
        ],
        input_embs,
    )
    target = [
        [[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]],
        [[1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0], [1, 1, 1, 1]],
    ]
    np.testing.assert_almost_equal(result, target)
if __name__ == "__main__":
  absltest.main()

================
File: lit_nlp/examples/glue/models.py
================
"""Wrapper for fine-tuned HuggingFace models in LIT."""
# TODO(b/261736863): Update to PEP 585 typings, consider using f-strings, and
# make common substrings into module CONSTANTS.
from collections.abc import Iterable, Sequence
import os
import re
import threading
from typing import Any, Optional
import attr
from lit_nlp.api import model as lit_model
from lit_nlp.api import types as lit_types
from lit_nlp.examples.glue import model_utils
from lit_nlp.lib import file_cache
from lit_nlp.lib import utils
import numpy as np
import tensorflow as tf
import tf_keras as keras
import transformers
os.environ["TF_USE_LEGACY_KERAS"] = "1"
JsonDict = lit_types.JsonDict
Spec = lit_types.Spec
TFSequenceClassifierOutput = (
    transformers.modeling_tf_outputs.TFSequenceClassifierOutput
)
@attr.s(auto_attribs=True, kw_only=True)
class GlueModelConfig(object):
  """Config options for a GlueModel."""
  # Preprocessing options
  max_seq_length: int = 128
  inference_batch_size: int = 32
  # Input options
  text_a_name: str = "sentence1"
  text_b_name: Optional[str] = "sentence2"  # set to None for single-segment
  label_name: str = "label"
  # Output options
  labels: Optional[list[str]] = None  # set to None for regression
  null_label_idx: Optional[int] = None
  compute_grads: bool = True  # if True, compute and return gradients.
  output_attention: bool = True
  output_embeddings: bool = True
  @classmethod
  def init_spec(cls) -> lit_types.Spec:
    return {
        "model_name_or_path": lit_types.String(
            default="bert-base-uncased",
            required=False,
        ),
        "max_seq_length": lit_types.Integer(
            default=128,
            max_val=512,
            min_val=1,
            required=False,
        ),
        "inference_batch_size": lit_types.Integer(
            default=32,
            max_val=64,
            min_val=1,
            required=False,
        ),
        "compute_grads": lit_types.Boolean(default=True, required=False),
        "output_attention": lit_types.Boolean(default=True, required=False),
        "output_embeddings": lit_types.Boolean(default=True, required=False),
    }
class GlueModel(lit_model.BatchedModel):
  """GLUE benchmark model, using Keras/TF2 and Huggingface Transformers.
  This is a general-purpose classification or regression model. It works for
  one- or two-segment input, and predicts either a multiclass label or
  a regression score. See GlueModelConfig for available options.
  This implements the LIT API for inference (e.g. input_spec(), output_spec(),
  and predict()), but also provides a train() method to run fine-tuning.
  This is a full-featured implementation, which includes embeddings, attention,
  gradients, as well as support for the different input and output types above.
  """
  def _verify_num_layers(self, hidden_states: Sequence[Any]):
    """Verify correct # of layer activations returned."""
    # First entry is embeddings, then output from each transformer layer.
    expected_hidden_states_len = self.model.config.num_hidden_layers + 1
    actual_hidden_states_len = len(hidden_states)
    if actual_hidden_states_len != expected_hidden_states_len:
      raise ValueError(
          "Unexpected size of hidden_states. Should be one "
          "more than the number of hidden layers to account "
          "for the embeddings. Expected "
          f"{expected_hidden_states_len}, got "
          f"{actual_hidden_states_len}."
      )
  @property
  def is_regression(self) -> bool:
    return self.config.labels is None
  # TODO(b/254110131): Move file_cache.cached_path() call inside this __init__
  # function to reduce boilerplate in other locations (e.g., TCAV tests).
  def __init__(self, model_name_or_path="bert-base-uncased", **config_kw):
    self.config = GlueModelConfig(**config_kw)
    self._load_model(model_name_or_path)
    self._lock = threading.Lock()
  def _load_model(self, model_name_or_path):
    """Load model. Can be overridden for testing."""
    # Normally path is a directory; if it's an archive file, download and
    # extract to the transformers cache.
    if model_name_or_path.endswith(".tar.gz"):
      model_name_or_path = file_cache.cached_path(
          model_name_or_path, extract_compressed_file=True
      )
    self.tokenizer = transformers.AutoTokenizer.from_pretrained(
        model_name_or_path
    )
    self.vocab = self.tokenizer.convert_ids_to_tokens(
        range(len(self.tokenizer))
    )
    model_config = transformers.AutoConfig.from_pretrained(
        model_name_or_path,
        num_labels=1 if self.is_regression else len(self.config.labels),
        return_dict=False,  # default for training; overridden for predict
        output_attentions=self.config.output_attention,
    )
    self.model = model_utils.load_pretrained(
        transformers.TFAutoModelForSequenceClassification,
        model_name_or_path,
        config=model_config,
    )
  def _get_tokens(self, ex: JsonDict, field_name: str) -> list[str]:
    with self._lock:
      return ex.get("tokens_" + field_name) or self.tokenizer.tokenize(
          ex[field_name]
      )
  def _preprocess(self, inputs: Iterable[JsonDict]) -> dict[str, tf.Tensor]:
    # Use pretokenized input if available.
    tokens_a = [self._get_tokens(ex, self.config.text_a_name) for ex in inputs]
    tokens_b = None
    if self.config.text_b_name:
      tokens_b = [
          self._get_tokens(ex, self.config.text_b_name) for ex in inputs
      ]
    # Use custom tokenizer call to make sure we don't mangle pre-split
    # wordpieces in pretokenized input.
    encoded_input = model_utils.batch_encode_pretokenized(
        self.tokenizer,
        tokens_a,
        tokens_b,
        max_length=self.config.max_seq_length,
    )
    return encoded_input  # pytype: disable=bad-return-type
  def _make_dataset(self, inputs: Iterable[JsonDict]) -> tf.data.Dataset:
    """Make a tf.data.Dataset from inputs in LIT format."""
    encoded_input = self._preprocess(inputs)
    if self.is_regression:
      labels = tf.constant(
          [ex[self.config.label_name] for ex in inputs], dtype=tf.float32
      )
    else:
      indexes = []
      if self.config.labels is not None:
        for ex in inputs:
          indexes.append(self.config.labels.index(ex[self.config.label_name]))
      labels = tf.constant(
          indexes,
          dtype=tf.int64,
      )
    # encoded_input is actually a transformers.BatchEncoding
    # object, which tf.data.Dataset doesn't like. Convert to a regular dict.
    return tf.data.Dataset.from_tensor_slices((dict(encoded_input), labels))
  def train(
      self,
      train_inputs: list[JsonDict],
      validation_inputs: list[JsonDict],
      learning_rate=2e-5,
      batch_size=32,
      num_epochs=3,
      keras_callbacks=None,
  ):
    """Run fine-tuning."""
    train_dataset = (
        self._make_dataset(train_inputs)
        .shuffle(128)
        .batch(batch_size)
        .repeat(-1)
    )
    # Use larger batch for validation since inference is about 1/2 memory usage
    # of backprop.
    eval_batch_size = 2 * batch_size
    validation_dataset = self._make_dataset(validation_inputs).batch(
        eval_batch_size
    )
    # Prepare model for training.
    opt = keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08)
    if self.is_regression:
      loss = keras.losses.MeanSquaredError()
      metric = keras.metrics.RootMeanSquaredError("rmse")
    else:
      loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)
      metric = keras.metrics.SparseCategoricalAccuracy("accuracy")
    self.model.compile(optimizer=opt, loss=loss, metrics=[metric])
    steps_per_epoch = len(train_inputs) // batch_size
    validation_steps = len(validation_inputs) // eval_batch_size
    history = self.model.fit(
        train_dataset,
        epochs=num_epochs,
        steps_per_epoch=steps_per_epoch,
        validation_data=validation_dataset,
        validation_steps=validation_steps,
        callbacks=keras_callbacks,
        verbose=2,
    )
    return history
  def save(self, path: str):
    """Save model weights and tokenizer info.
    To re-load, pass the path to the constructor instead of the name of a
    base model.
    Args:
      path: directory to save to. Will write several files here.
    """
    if not os.path.isdir(path):
      os.mkdir(path)
    self.tokenizer.save_pretrained(path)
    self.model.save_pretrained(path)
  def _segment_slicers(self, tokens: list[str]):
    """Slicers along the tokens dimension for each segment.
    For tokens ['[CLS]', a0, a1, ..., '[SEP]', b0, b1, ..., '[SEP]'],
    we want to get the slices [a0, a1, ...] and [b0, b1, ...]
    Args:
      tokens: <string>[num_tokens], including special tokens
    Returns:
      (slicer_a, slicer_b), slice objects
    """
    try:
      split_point = tokens.index(self.tokenizer.sep_token)
    except ValueError:
      split_point = len(tokens) - 1
    slicer_a = slice(1, split_point)  # start after [CLS]
    slicer_b = slice(split_point + 1, len(tokens) - 1)  # end before last [SEP]
    return slicer_a, slicer_b
  def _postprocess(self, output: dict[str, Any]):
    """Per-example postprocessing, on NumPy output."""
    ntok = output.pop("ntok")
    output["tokens"] = self.tokenizer.convert_ids_to_tokens(
        output.pop("input_ids")[:ntok]
    )
    # Tokens for each segment, individually.
    slicer_a, slicer_b = self._segment_slicers(output["tokens"])
    output["tokens_" + self.config.text_a_name] = output["tokens"][slicer_a]
    if self.config.text_b_name:
      output["tokens_" + self.config.text_b_name] = output["tokens"][slicer_b]
    # Embeddings for each segment, individually.
    if self.config.output_embeddings:
      output["input_embs_" + self.config.text_a_name] = output["input_embs"][
          slicer_a
      ]
      if self.config.text_b_name:
        output["input_embs_" + self.config.text_b_name] = output["input_embs"][
            slicer_b
        ]
    # Gradients for each segment, individually.
    if self.config.compute_grads:
      # Gradients for the CLS token.
      output["cls_grad"] = output["input_emb_grad"][0]
      output["token_grad_" + self.config.text_a_name] = output[
          "input_emb_grad"
      ][slicer_a]
      if self.config.text_b_name:
        output["token_grad_" + self.config.text_b_name] = output[
            "input_emb_grad"
        ][slicer_b]
      # TODO(b/294613507): remove output[self.config.label_name] once TCAV
      # is updated.
      if not self.is_regression:
        # Return the label corresponding to the class index used for gradients.
        output[self.config.label_name] = self.config.labels[
            output[self.config.label_name]
        ]  # pytype: disable=container-type-mismatch
      # Remove "input_emb_grad" since it's not in the output spec.
      del output["input_emb_grad"]
    if not self.config.output_attention:
      return output
    # Process attention.
    for key in output:
      if not re.match(r"layer_(\d+)/attention", key):
        continue
      # Select only real tokens, since most of this matrix is padding.
      # <float32>[num_heads, max_seq_length, max_seq_length]
      # -> <float32>[num_heads, num_tokens, num_tokens]
      output[key] = output[key][:, :ntok, :ntok].transpose((0, 2, 1))
      # Make a copy of this array to avoid memory leaks, since NumPy otherwise
      # keeps a pointer around that prevents the source array from being GCed.
      output[key] = output[key].copy()  # pytype: disable=attribute-error
    return output
  def _scatter_embs(
      self, passed_input_embs, input_embs, batch_indices, offsets
  ):
    """Scatters custom passed embeddings into the default model embeddings.
    Args:
      passed_input_embs: <tf.float32>[num_scatter_tokens], the custom passed
        embeddings to be scattered into the default model embeddings.
      input_embs: the default model embeddings.
      batch_indices: the indices of the embeddings to replace in the format
        (batch_index, sequence_index).
      offsets: the offset from which to scatter the custom embedding (number of
        tokens from the start of the sequence).
    Returns:
      The default model embeddings with scattered custom embeddings.
    """
    # <float32>[scatter_batch_size, num_tokens, emb_size]
    filtered_embs = [emb for emb in passed_input_embs if emb is not None]
    # Prepares update values that should be scattered in, i.e. one for each
    # of the (scatter_batch_size * num_tokens) word embeddings.
    # <np.float32>[scatter_batch_size * num_tokens, emb_size]
    updates = np.concatenate(filtered_embs)
    # Prepares indices in format (batch_index, sequence_index) for all
    # values that should be scattered in, i.e. one for each of the
    # (scatter_batch_size * num_tokens) word embeddings.
    scatter_indices = []
    for batch_index, sentence_embs, offset in zip(
        batch_indices, filtered_embs, offsets
    ):
      for token_index, _ in enumerate(sentence_embs):
        scatter_indices.append([batch_index, token_index + offset])
    # Scatters passed word embeddings into embeddings gathered from tokens.
    # <tf.float32>[batch_size, num_tokens + num_special_tokens, emb_size]
    return tf.tensor_scatter_nd_update(input_embs, scatter_indices, updates)
  def scatter_all_embeddings(self, inputs, input_embs):
    """Scatters custom passed embeddings for text segment inputs.
    Args:
      inputs: the model inputs, which contain any custom embeddings to scatter.
      input_embs: the default model embeddings.
    Returns:
      The default model embeddings with scattered custom embeddings.
    """
    # Gets batch indices of any word embeddings that were passed for text_a.
    passed_input_embs_a = [
        ex.get("input_embs_" + self.config.text_a_name) for ex in inputs
    ]
    batch_indices_a = [
        index
        for (index, emb) in enumerate(passed_input_embs_a)
        if emb is not None
    ]
    # If word embeddings were passed in for text_a, scatter them into the
    # embeddings, gathered from the input ids. 1 is passed in as the offset
    # for each, since text_a starts at index 1, after the [CLS] token.
    if batch_indices_a:
      input_embs = self._scatter_embs(
          passed_input_embs_a,
          input_embs,
          batch_indices_a,
          offsets=np.ones(len(batch_indices_a), dtype=np.int64),
      )
    if self.config.text_b_name:
      # Gets batch indices of any word embeddings that were passed for text_b.
      passed_input_embs_b = [
          ex.get("input_embs_" + self.config.text_b_name) for ex in inputs
      ]
      batch_indices_b = [
          index
          for (index, emb) in enumerate(passed_input_embs_b)
          if emb is not None
      ]
      # If word embeddings were also passed in for text_b, scatter them into the
      # embeddings gathered from the input ids. The offsets are the [lengths
      # of the corresponding text_a embeddings] + 2, since text_b starts after
      # [CLS] [text_a tokens] [SEP]. (This assumes that text_b embeddings
      # will only be passed together with text_a embeddings.)
      if batch_indices_b:
        lengths = np.array(
            [len(embed) for embed in passed_input_embs_a if embed is not None]
        )
        input_embs = self._scatter_embs(
            passed_input_embs_b,
            input_embs,
            batch_indices_b,
            offsets=(lengths + 2),
        )
    return input_embs
  def get_target_scores(self, inputs: Iterable[JsonDict], scores):
    """Get target-class scores, as a 1D tensor.
    Args:
      inputs: list of input examples
      scores: <tf.float32>[batch_size, num_classes], either logits or probas
    Returns:
      <tf.float32>[batch_size] target scores for each input
    """
    arg_max = tf.math.argmax(scores, axis=-1).numpy()
    grad_classes = [
        ex.get(self.config.label_name, arg_max[i])
        for (i, ex) in enumerate(inputs)
    ]
    # Convert the class names to indices if needed.
    grad_idxs = []
    for label in grad_classes:
      if isinstance(label, str) and self.config.labels is not None:
        grad_idxs.append(self.config.labels.index(label))
      else:
        grad_idxs.append(label)
    # list of tuples (batch idx, label idx)
    gather_indices = list(enumerate(grad_idxs))
    # <tf.float32>[batch_size]
    return tf.gather_nd(scores, gather_indices), grad_idxs
  ##
  # LIT API implementation
  def max_minibatch_size(self):
    return self.config.inference_batch_size
  def get_embedding_table(self):
    # TODO(b/236276775): Unify on the TFBertEmbeddings.weight API after
    # transformers is updated to v4.25.1 (or newer).
    if hasattr(self.model.bert.embeddings, "word_embeddings"):
      return self.vocab, self.model.bert.embeddings.word_embeddings.numpy()
    else:
      return self.vocab, self.model.bert.embeddings.weight.numpy()
  def predict_minibatch(self, inputs: Iterable[JsonDict]):
    # Use watch_accessed_variables to save memory by having the tape do nothing
    # if we don't need gradients.
    with tf.GradientTape(
        watch_accessed_variables=self.config.compute_grads
    ) as tape:
      encoded_input = self._preprocess(inputs)
      # Gathers word embeddings from BERT model embedding layer using input ids
      # of the tokens.
      input_ids = encoded_input["input_ids"]
      word_embeddings = self.model.bert.embeddings.weight
      # <tf.float32>[batch_size, num_tokens, emb_size]
      input_embs = tf.gather(word_embeddings, input_ids)
      # Scatter in any passed in embeddings.
      # <tf.float32>[batch_size, num_tokens, emb_size]
      input_embs = self.scatter_all_embeddings(inputs, input_embs)
      tape.watch(input_embs)  # Watch input_embs for gradient calculation.
      model_inputs = encoded_input.copy()
      model_inputs["input_ids"] = None
      out: TFSequenceClassifierOutput = self.model(
          model_inputs,
          inputs_embeds=input_embs,
          training=False,
          output_hidden_states=True,
          output_attentions=True,
          return_dict=True,
      )
      batched_outputs = {
          "input_ids": encoded_input["input_ids"],
          "ntok": tf.reduce_sum(encoded_input["attention_mask"], axis=1),
          "cls_emb": out.hidden_states[-1][:, 0],  # last layer, first token
      }
      if self.config.output_embeddings:
        batched_outputs["input_embs"] = input_embs
        self._verify_num_layers(out.hidden_states)
        # <float32>[batch_size, num_tokens, 1]
        token_mask = tf.expand_dims(
            tf.cast(encoded_input["attention_mask"], tf.float32), axis=2
        )
        # <float32>[batch_size, 1]
        denom = tf.reduce_sum(token_mask, axis=1)
        for i, layer_output in enumerate(out.hidden_states):
          # layer_output is <float32>[batch_size, num_tokens, emb_dim]
          # average over tokens to get <float32>[batch_size, emb_dim]
          batched_outputs[f"layer_{i}/avg_emb"] = (
              tf.reduce_sum(layer_output * token_mask, axis=1) / denom
          )
      if self.config.output_attention:
        if len(out.attentions) != self.model.config.num_hidden_layers:
          raise ValueError(
              "Unexpected size of attentions. Should be the same "
              "size as the number of hidden layers. Expected "
              f"{self.model.config.num_hidden_layers}, got "
              f"{len(out.attentions)}."
          )
        for i, layer_attention in enumerate(out.attentions):
          batched_outputs[f"layer_{i+1}/attention"] = layer_attention
      if self.is_regression:
        # <tf.float32>[batch_size]
        batched_outputs["score"] = tf.squeeze(out.logits, axis=-1)
        # <tf.float32>[batch_size], a single target per example
        scalar_targets = batched_outputs["score"]
      else:
        # <tf.float32>[batch_size, num_labels]
        batched_outputs["probas"] = tf.nn.softmax(out.logits, axis=-1)
        # <tf.float32>[batch_size], a single target per example
        scalar_targets, grad_idxs = self.get_target_scores(
            inputs, batched_outputs["probas"]
        )
        # TODO(b/294613507): remove once TCAV updated.
        if self.config.compute_grads:
          batched_outputs[self.config.label_name] = tf.convert_to_tensor(
              grad_idxs
          )
    # Request gradients after the tape is run.
    # Note: embs[0] includes position and segment encodings, as well as subword
    # embeddings.
    if self.config.compute_grads:
      # <tf.float32>[batch_size, num_tokens, emb_dim]
      batched_outputs["input_emb_grad"] = tape.gradient(
          scalar_targets, input_embs
      )
    detached_outputs = {
        k: v.numpy()
        for k, v in batched_outputs.items()
        if v is not None
    }
    # Sequence of dicts, one per example.
    unbatched_outputs = utils.unbatch_preds(detached_outputs)
    return map(self._postprocess, unbatched_outputs)
  def input_spec(self) -> Spec:
    ret = {}
    ret[self.config.text_a_name] = lit_types.TextSegment()
    ret["tokens_" + self.config.text_a_name] = lit_types.Tokens(
        parent=self.config.text_a_name, required=False
    )
    if self.config.text_b_name:
      ret[self.config.text_b_name] = lit_types.TextSegment()
      ret["tokens_" + self.config.text_b_name] = lit_types.Tokens(
          parent=self.config.text_b_name, required=False
      )
    if self.is_regression:
      ret[self.config.label_name] = lit_types.Scalar(required=False)
    else:
      ret[self.config.label_name] = lit_types.CategoryLabel(
          required=False, vocab=self.config.labels
      )
    if self.config.output_embeddings:
      # The input_embs_ fields are used for Integrated Gradients.
      text_a_embs = "input_embs_" + self.config.text_a_name
      ret[text_a_embs] = lit_types.TokenEmbeddings(
          align="tokens", required=False
      )
      if self.config.text_b_name:
        text_b_embs = "input_embs_" + self.config.text_b_name
        ret[text_b_embs] = lit_types.TokenEmbeddings(
            align="tokens", required=False
        )
    return ret
  def output_spec(self) -> Spec:
    ret = {"tokens": lit_types.Tokens()}
    ret["tokens_" + self.config.text_a_name] = lit_types.Tokens(
        parent=self.config.text_a_name
    )
    if self.config.text_b_name:
      ret["tokens_" + self.config.text_b_name] = lit_types.Tokens(
          parent=self.config.text_b_name
      )
    if self.is_regression:
      ret["score"] = lit_types.RegressionScore(parent=self.config.label_name)
    else:
      ret["probas"] = lit_types.MulticlassPreds(
          parent=self.config.label_name,
          vocab=self.config.labels,
          null_idx=self.config.null_label_idx,
      )
    if self.config.output_embeddings:
      ret["cls_emb"] = lit_types.Embeddings()
      # Average embeddings, one per layer including embeddings.
      for i in range(1 + self.model.config.num_hidden_layers):
        ret[f"layer_{i}/avg_emb"] = lit_types.Embeddings()
      # The input_embs_ fields are used for Integrated Gradients.
      ret["input_embs_" + self.config.text_a_name] = lit_types.TokenEmbeddings(
          align="tokens_" + self.config.text_a_name
      )
      if self.config.text_b_name:
        text_b_embs = "input_embs_" + self.config.text_b_name
        ret[text_b_embs] = lit_types.TokenEmbeddings(
            align="tokens_" + self.config.text_b_name
        )
    # Gradients, if requested.
    if self.config.compute_grads:
      ret["cls_grad"] = lit_types.Gradients(
          align=("score" if self.is_regression else "probas"),
          grad_for="cls_emb",
          grad_target_field_key=self.config.label_name,
      )
      if not self.is_regression:
        ret[self.config.label_name] = lit_types.CategoryLabel(
            required=False, vocab=self.config.labels
        )
      if self.config.output_embeddings:
        text_a_token_grads = "token_grad_" + self.config.text_a_name
        ret[text_a_token_grads] = lit_types.TokenGradients(
            align="tokens_" + self.config.text_a_name,
            grad_for="input_embs_" + self.config.text_a_name,
            grad_target_field_key=self.config.label_name,
        )
        if self.config.text_b_name:
          text_b_token_grads = "token_grad_" + self.config.text_b_name
          ret[text_b_token_grads] = lit_types.TokenGradients(
              align="tokens_" + self.config.text_b_name,
              grad_for="input_embs_" + self.config.text_b_name,
              grad_target_field_key=self.config.label_name,
          )
    if self.config.output_attention:
      # Attention heads, one field for each layer.
      for i in range(self.model.config.num_hidden_layers):
        ret[f"layer_{i+1}/attention"] = lit_types.AttentionHeads(
            align_in="tokens", align_out="tokens"
        )
    return ret
class SST2Model(GlueModel):
  """Classification model on SST-2."""
  def __init__(self, *args, **kw):
    super().__init__(
        *args,
        text_a_name="sentence",
        text_b_name=None,
        labels=["0", "1"],
        null_label_idx=0,
        **kw,
    )
class MNLIModel(GlueModel):
  """Classification model on MultiNLI."""
  def __init__(self, *args, **kw):
    super().__init__(
        *args,
        text_a_name="premise",
        text_b_name="hypothesis",
        labels=["entailment", "neutral", "contradiction"],
        **kw,
    )
class STSBModel(GlueModel):
  """Regression model on STS-B."""
  def __init__(self, *args, **kw):
    super().__init__(
        *args,
        text_a_name="sentence1",
        text_b_name="sentence2",
        labels=None,
        **kw,
    )
  def input_spec(self):
    ret = super().input_spec()
    ret[self.config.label_name] = lit_types.Scalar(min_val=0, max_val=5)
    return ret

================
File: lit_nlp/examples/glue/README.md
================
Glue Demo

* LIT team hosts a glue demo which can be accessible at https://pair-code.github.io/lit/demos/glue.html.
* For more details, check out the documentations for Sentiment and NLI / Textual Similarity at https://pair-code.github.io/lit/documentation/demos.html.

================
File: lit_nlp/examples/glue/testdata/bert_tokenizer/vocab.txt
================
[PAD]
[unused0]
[unused1]
[unused2]
[unused3]
[unused4]
[unused5]
[unused6]
[unused7]
[unused8]
[unused9]
[unused10]
[unused11]
[unused12]
[unused13]
[unused14]
[unused15]
[unused16]
[unused17]
[unused18]
[unused19]
[unused20]
[unused21]
[unused22]
[unused23]
[unused24]
[unused25]
[unused26]
[unused27]
[unused28]
[unused29]
[unused30]
[unused31]
[unused32]
[unused33]
[unused34]
[unused35]
[unused36]
[unused37]
[unused38]
[unused39]
[unused40]
[unused41]
[unused42]
[unused43]
[unused44]
[unused45]
[unused46]
[unused47]
[unused48]
[unused49]
[unused50]
[unused51]
[unused52]
[unused53]
[unused54]
[unused55]
[unused56]
[unused57]
[unused58]
[unused59]
[unused60]
[unused61]
[unused62]
[unused63]
[unused64]
[unused65]
[unused66]
[unused67]
[unused68]
[unused69]
[unused70]
[unused71]
[unused72]
[unused73]
[unused74]
[unused75]
[unused76]
[unused77]
[unused78]
[unused79]
[unused80]
[unused81]
[unused82]
[unused83]
[unused84]
[unused85]
[unused86]
[unused87]
[unused88]
[unused89]
[unused90]
[unused91]
[unused92]
[unused93]
[unused94]
[unused95]
[unused96]
[unused97]
[unused98]
[UNK]
[CLS]
[SEP]
[MASK]
[unused99]
[unused100]
[unused101]
[unused102]
[unused103]
[unused104]
[unused105]
[unused106]
[unused107]
[unused108]
[unused109]
[unused110]
[unused111]
[unused112]
[unused113]
[unused114]
[unused115]
[unused116]
[unused117]
[unused118]
[unused119]
[unused120]
[unused121]
[unused122]
[unused123]
[unused124]
[unused125]
[unused126]
[unused127]
[unused128]
[unused129]
[unused130]
[unused131]
[unused132]
[unused133]
[unused134]
[unused135]
[unused136]
[unused137]
[unused138]
[unused139]
[unused140]
[unused141]
[unused142]
[unused143]
[unused144]
[unused145]
[unused146]
[unused147]
[unused148]
[unused149]
[unused150]
[unused151]
[unused152]
[unused153]
[unused154]
[unused155]
[unused156]
[unused157]
[unused158]
[unused159]
[unused160]
[unused161]
[unused162]
[unused163]
[unused164]
[unused165]
[unused166]
[unused167]
[unused168]
[unused169]
[unused170]
[unused171]
[unused172]
[unused173]
[unused174]
[unused175]
[unused176]
[unused177]
[unused178]
[unused179]
[unused180]
[unused181]
[unused182]
[unused183]
[unused184]
[unused185]
[unused186]
[unused187]
[unused188]
[unused189]
[unused190]
[unused191]
[unused192]
[unused193]
[unused194]
[unused195]
[unused196]
[unused197]
[unused198]
[unused199]
[unused200]
[unused201]
[unused202]
[unused203]
[unused204]
[unused205]
[unused206]
[unused207]
[unused208]
[unused209]
[unused210]
[unused211]
[unused212]
[unused213]
[unused214]
[unused215]
[unused216]
[unused217]
[unused218]
[unused219]
[unused220]
[unused221]
[unused222]
[unused223]
[unused224]
[unused225]
[unused226]
[unused227]
[unused228]
[unused229]
[unused230]
[unused231]
[unused232]
[unused233]
[unused234]
[unused235]
[unused236]
[unused237]
[unused238]
[unused239]
[unused240]
[unused241]
[unused242]
[unused243]
[unused244]
[unused245]
[unused246]
[unused247]
[unused248]
[unused249]
[unused250]
[unused251]
[unused252]
[unused253]
[unused254]
[unused255]
[unused256]
[unused257]
[unused258]
[unused259]
[unused260]
[unused261]
[unused262]
[unused263]
[unused264]
[unused265]
[unused266]
[unused267]
[unused268]
[unused269]
[unused270]
[unused271]
[unused272]
[unused273]
[unused274]
[unused275]
[unused276]
[unused277]
[unused278]
[unused279]
[unused280]
[unused281]
[unused282]
[unused283]
[unused284]
[unused285]
[unused286]
[unused287]
[unused288]
[unused289]
[unused290]
[unused291]
[unused292]
[unused293]
[unused294]
[unused295]
[unused296]
[unused297]
[unused298]
[unused299]
[unused300]
[unused301]
[unused302]
[unused303]
[unused304]
[unused305]
[unused306]
[unused307]
[unused308]
[unused309]
[unused310]
[unused311]
[unused312]
[unused313]
[unused314]
[unused315]
[unused316]
[unused317]
[unused318]
[unused319]
[unused320]
[unused321]
[unused322]
[unused323]
[unused324]
[unused325]
[unused326]
[unused327]
[unused328]
[unused329]
[unused330]
[unused331]
[unused332]
[unused333]
[unused334]
[unused335]
[unused336]
[unused337]
[unused338]
[unused339]
[unused340]
[unused341]
[unused342]
[unused343]
[unused344]
[unused345]
[unused346]
[unused347]
[unused348]
[unused349]
[unused350]
[unused351]
[unused352]
[unused353]
[unused354]
[unused355]
[unused356]
[unused357]
[unused358]
[unused359]
[unused360]
[unused361]
[unused362]
[unused363]
[unused364]
[unused365]
[unused366]
[unused367]
[unused368]
[unused369]
[unused370]
[unused371]
[unused372]
[unused373]
[unused374]
[unused375]
[unused376]
[unused377]
[unused378]
[unused379]
[unused380]
[unused381]
[unused382]
[unused383]
[unused384]
[unused385]
[unused386]
[unused387]
[unused388]
[unused389]
[unused390]
[unused391]
[unused392]
[unused393]
[unused394]
[unused395]
[unused396]
[unused397]
[unused398]
[unused399]
[unused400]
[unused401]
[unused402]
[unused403]
[unused404]
[unused405]
[unused406]
[unused407]
[unused408]
[unused409]
[unused410]
[unused411]
[unused412]
[unused413]
[unused414]
[unused415]
[unused416]
[unused417]
[unused418]
[unused419]
[unused420]
[unused421]
[unused422]
[unused423]
[unused424]
[unused425]
[unused426]
[unused427]
[unused428]
[unused429]
[unused430]
[unused431]
[unused432]
[unused433]
[unused434]
[unused435]
[unused436]
[unused437]
[unused438]
[unused439]
[unused440]
[unused441]
[unused442]
[unused443]
[unused444]
[unused445]
[unused446]
[unused447]
[unused448]
[unused449]
[unused450]
[unused451]
[unused452]
[unused453]
[unused454]
[unused455]
[unused456]
[unused457]
[unused458]
[unused459]
[unused460]
[unused461]
[unused462]
[unused463]
[unused464]
[unused465]
[unused466]
[unused467]
[unused468]
[unused469]
[unused470]
[unused471]
[unused472]
[unused473]
[unused474]
[unused475]
[unused476]
[unused477]
[unused478]
[unused479]
[unused480]
[unused481]
[unused482]
[unused483]
[unused484]
[unused485]
[unused486]
[unused487]
[unused488]
[unused489]
[unused490]
[unused491]
[unused492]
[unused493]
[unused494]
[unused495]
[unused496]
[unused497]
[unused498]
[unused499]
[unused500]
[unused501]
[unused502]
[unused503]
[unused504]
[unused505]
[unused506]
[unused507]
[unused508]
[unused509]
[unused510]
[unused511]
[unused512]
[unused513]
[unused514]
[unused515]
[unused516]
[unused517]
[unused518]
[unused519]
[unused520]
[unused521]
[unused522]
[unused523]
[unused524]
[unused525]
[unused526]
[unused527]
[unused528]
[unused529]
[unused530]
[unused531]
[unused532]
[unused533]
[unused534]
[unused535]
[unused536]
[unused537]
[unused538]
[unused539]
[unused540]
[unused541]
[unused542]
[unused543]
[unused544]
[unused545]
[unused546]
[unused547]
[unused548]
[unused549]
[unused550]
[unused551]
[unused552]
[unused553]
[unused554]
[unused555]
[unused556]
[unused557]
[unused558]
[unused559]
[unused560]
[unused561]
[unused562]
[unused563]
[unused564]
[unused565]
[unused566]
[unused567]
[unused568]
[unused569]
[unused570]
[unused571]
[unused572]
[unused573]
[unused574]
[unused575]
[unused576]
[unused577]
[unused578]
[unused579]
[unused580]
[unused581]
[unused582]
[unused583]
[unused584]
[unused585]
[unused586]
[unused587]
[unused588]
[unused589]
[unused590]
[unused591]
[unused592]
[unused593]
[unused594]
[unused595]
[unused596]
[unused597]
[unused598]
[unused599]
[unused600]
[unused601]
[unused602]
[unused603]
[unused604]
[unused605]
[unused606]
[unused607]
[unused608]
[unused609]
[unused610]
[unused611]
[unused612]
[unused613]
[unused614]
[unused615]
[unused616]
[unused617]
[unused618]
[unused619]
[unused620]
[unused621]
[unused622]
[unused623]
[unused624]
[unused625]
[unused626]
[unused627]
[unused628]
[unused629]
[unused630]
[unused631]
[unused632]
[unused633]
[unused634]
[unused635]
[unused636]
[unused637]
[unused638]
[unused639]
[unused640]
[unused641]
[unused642]
[unused643]
[unused644]
[unused645]
[unused646]
[unused647]
[unused648]
[unused649]
[unused650]
[unused651]
[unused652]
[unused653]
[unused654]
[unused655]
[unused656]
[unused657]
[unused658]
[unused659]
[unused660]
[unused661]
[unused662]
[unused663]
[unused664]
[unused665]
[unused666]
[unused667]
[unused668]
[unused669]
[unused670]
[unused671]
[unused672]
[unused673]
[unused674]
[unused675]
[unused676]
[unused677]
[unused678]
[unused679]
[unused680]
[unused681]
[unused682]
[unused683]
[unused684]
[unused685]
[unused686]
[unused687]
[unused688]
[unused689]
[unused690]
[unused691]
[unused692]
[unused693]
[unused694]
[unused695]
[unused696]
[unused697]
[unused698]
[unused699]
[unused700]
[unused701]
[unused702]
[unused703]
[unused704]
[unused705]
[unused706]
[unused707]
[unused708]
[unused709]
[unused710]
[unused711]
[unused712]
[unused713]
[unused714]
[unused715]
[unused716]
[unused717]
[unused718]
[unused719]
[unused720]
[unused721]
[unused722]
[unused723]
[unused724]
[unused725]
[unused726]
[unused727]
[unused728]
[unused729]
[unused730]
[unused731]
[unused732]
[unused733]
[unused734]
[unused735]
[unused736]
[unused737]
[unused738]
[unused739]
[unused740]
[unused741]
[unused742]
[unused743]
[unused744]
[unused745]
[unused746]
[unused747]
[unused748]
[unused749]
[unused750]
[unused751]
[unused752]
[unused753]
[unused754]
[unused755]
[unused756]
[unused757]
[unused758]
[unused759]
[unused760]
[unused761]
[unused762]
[unused763]
[unused764]
[unused765]
[unused766]
[unused767]
[unused768]
[unused769]
[unused770]
[unused771]
[unused772]
[unused773]
[unused774]
[unused775]
[unused776]
[unused777]
[unused778]
[unused779]
[unused780]
[unused781]
[unused782]
[unused783]
[unused784]
[unused785]
[unused786]
[unused787]
[unused788]
[unused789]
[unused790]
[unused791]
[unused792]
[unused793]
[unused794]
[unused795]
[unused796]
[unused797]
[unused798]
[unused799]
[unused800]
[unused801]
[unused802]
[unused803]
[unused804]
[unused805]
[unused806]
[unused807]
[unused808]
[unused809]
[unused810]
[unused811]
[unused812]
[unused813]
[unused814]
[unused815]
[unused816]
[unused817]
[unused818]
[unused819]
[unused820]
[unused821]
[unused822]
[unused823]
[unused824]
[unused825]
[unused826]
[unused827]
[unused828]
[unused829]
[unused830]
[unused831]
[unused832]
[unused833]
[unused834]
[unused835]
[unused836]
[unused837]
[unused838]
[unused839]
[unused840]
[unused841]
[unused842]
[unused843]
[unused844]
[unused845]
[unused846]
[unused847]
[unused848]
[unused849]
[unused850]
[unused851]
[unused852]
[unused853]
[unused854]
[unused855]
[unused856]
[unused857]
[unused858]
[unused859]
[unused860]
[unused861]
[unused862]
[unused863]
[unused864]
[unused865]
[unused866]
[unused867]
[unused868]
[unused869]
[unused870]
[unused871]
[unused872]
[unused873]
[unused874]
[unused875]
[unused876]
[unused877]
[unused878]
[unused879]
[unused880]
[unused881]
[unused882]
[unused883]
[unused884]
[unused885]
[unused886]
[unused887]
[unused888]
[unused889]
[unused890]
[unused891]
[unused892]
[unused893]
[unused894]
[unused895]
[unused896]
[unused897]
[unused898]
[unused899]
[unused900]
[unused901]
[unused902]
[unused903]
[unused904]
[unused905]
[unused906]
[unused907]
[unused908]
[unused909]
[unused910]
[unused911]
[unused912]
[unused913]
[unused914]
[unused915]
[unused916]
[unused917]
[unused918]
[unused919]
[unused920]
[unused921]
[unused922]
[unused923]
[unused924]
[unused925]
[unused926]
[unused927]
[unused928]
[unused929]
[unused930]
[unused931]
[unused932]
[unused933]
[unused934]
[unused935]
[unused936]
[unused937]
[unused938]
[unused939]
[unused940]
[unused941]
[unused942]
[unused943]
[unused944]
[unused945]
[unused946]
[unused947]
[unused948]
[unused949]
[unused950]
[unused951]
[unused952]
[unused953]
[unused954]
[unused955]
[unused956]
[unused957]
[unused958]
[unused959]
[unused960]
[unused961]
[unused962]
[unused963]
[unused964]
[unused965]
[unused966]
[unused967]
[unused968]
[unused969]
[unused970]
[unused971]
[unused972]
[unused973]
[unused974]
[unused975]
[unused976]
[unused977]
[unused978]
[unused979]
[unused980]
[unused981]
[unused982]
[unused983]
[unused984]
[unused985]
[unused986]
[unused987]
[unused988]
[unused989]
[unused990]
[unused991]
[unused992]
[unused993]
!
"
#
$
%
&
'
(
)
*
+
,
-
.
/
0
1
2
3
4
5
6
7
8
9
:
;
<
=
>
?
@
[
\
]
^
_
`
a
b
c
d
e
f
g
h
i
j
k
l
m
n
o
p
q
r
s
t
u
v
w
x
y
z
{
|
}
~

































































































































































































































































































































































































































































































































































































































































































































































































































































































































































the
of
and
in
to
was
he
is
as
for
on
with
that
it
his
by
at
from
her
##s
she
you
had
an
were
but
be
this
are
not
my
they
one
which
or
have
him
me
first
all
also
their
has
up
who
out
been
when
after
there
into
new
two
its
##a
time
would
no
what
about
said
we
over
then
other
so
more
##e
can
if
like
back
them
only
some
could
##i
where
just
##ing
during
before
##n
do
##o
made
school
through
than
now
years
most
world
may
between
down
well
three
##d
year
while
will
##ed
##r
##y
later
##t
city
under
around
did
such
being
used
state
people
part
know
against
your
many
second
university
both
national
##er
these
don
known
off
way
until
re
how
even
get
head
...
didn
##ly
team
american
because
de
##l
born
united
film
since
still
long
work
south
us
became
any
high
again
day
family
see
right
man
eyes
house
season
war
states
including
took
life
north
same
each
called
name
much
place
however
go
four
group
another
found
won
area
here
going
10
away
series
left
home
music
best
make
hand
number
company
several
never
last
john
000
very
album
take
end
good
too
following
released
game
played
little
began
district
##m
old
want
those
side
held
own
early
county
ll
league
use
west
##u
face
think
##es
2010
government
##h
march
came
small
general
town
june
##on
line
based
something
##k
september
thought
looked
along
international
2011
air
july
club
went
january
october
our
august
april
york
12
few
2012
2008
east
show
member
college
2009
father
public
##us
come
men
five
set
station
church
##c
next
former
november
room
party
located
december
2013
age
got
2007
##g
system
let
love
2006
though
every
2014
look
song
water
century
without
body
black
night
within
great
women
single
ve
building
large
population
river
named
band
white
started
##an
once
15
20
should
18
2015
service
top
built
british
open
death
king
moved
local
times
children
february
book
why
11
door
need
president
order
final
road
wasn
although
due
major
died
village
third
knew
2016
asked
turned
st
wanted
say
##p
together
received
main
son
served
different
##en
behind
himself
felt
members
power
football
law
voice
play
##in
near
park
history
30
having
2005
16
##man
saw
mother
##al
army
point
front
help
english
street
art
late
hands
games
award
##ia
young
14
put
published
country
division
across
told
13
often
ever
french
london
center
six
red
2017
led
days
include
light
25
find
tell
among
species
really
according
central
half
2004
form
original
gave
office
making
enough
lost
full
opened
must
included
live
given
german
player
run
business
woman
community
cup
might
million
land
2000
court
development
17
short
round
ii
km
seen
class
story
always
become
sure
research
almost
director
council
la
##2
career
things
using
island
##z
couldn
car
##is
24
close
force
##1
better
free
support
control
field
students
2003
education
married
##b
nothing
worked
others
record
big
inside
level
anything
continued
give
james
##3
military
established
non
returned
feel
does
title
written
thing
feet
william
far
co
association
hard
already
2002
##ra
championship
human
western
100
##na
department
hall
role
various
production
21
19
heart
2001
living
fire
version
##ers
##f
television
royal
##4
produced
working
act
case
society
region
present
radio
period
looking
least
total
keep
england
wife
program
per
brother
mind
special
22
##le
am
works
soon
##6
political
george
services
taken
created
##7
further
able
reached
david
union
joined
upon
done
important
social
information
either
##ic
##x
appeared
position
ground
lead
rock
dark
election
23
board
france
hair
course
arms
site
police
girl
instead
real
sound
##v
words
moment
##te
someone
##8
summer
project
announced
san
less
wrote
past
followed
##5
blue
founded
al
finally
india
taking
records
america
##ne
1999
design
considered
northern
god
stop
battle
toward
european
outside
described
track
today
playing
language
28
call
26
heard
professional
low
australia
miles
california
win
yet
green
##ie
trying
blood
##ton
southern
science
maybe
everything
match
square
27
mouth
video
race
recorded
leave
above
##9
daughter
points
space
1998
museum
change
middle
common
##0
move
tv
post
##ta
lake
seven
tried
elected
closed
ten
paul
minister
##th
months
start
chief
return
canada
person
sea
release
similar
modern
brought
rest
hit
formed
mr
##la
1997
floor
event
doing
thomas
1996
robert
care
killed
training
star
week
needed
turn
finished
railway
rather
news
health
sent
example
ran
term
michael
coming
currently
yes
forces
despite
gold
areas
50
stage
fact
29
dead
says
popular
2018
originally
germany
probably
developed
result
pulled
friend
stood
money
running
mi
signed
word
songs
child
eventually
met
tour
average
teams
minutes
festival
current
deep
kind
1995
decided
usually
eastern
seemed
##ness
episode
bed
added
table
indian
private
charles
route
available
idea
throughout
centre
addition
appointed
style
1994
books
eight
construction
press
mean
wall
friends
remained
schools
study
##ch
##um
institute
oh
chinese
sometimes
events
possible
1992
australian
type
brown
forward
talk
process
food
debut
seat
performance
committee
features
character
arts
herself
else
lot
strong
russian
range
hours
peter
arm
##da
morning
dr
sold
##ry
quickly
directed
1993
guitar
china
##w
31
list
##ma
performed
media
uk
players
smile
##rs
myself
40
placed
coach
province
towards
wouldn
leading
whole
boy
official
designed
grand
census
##el
europe
attack
japanese
henry
1991
##re
##os
cross
getting
alone
action
lower
network
wide
washington
japan
1990
hospital
believe
changed
sister
##ar
hold
gone
sir
hadn
ship
##ka
studies
academy
shot
rights
below
base
bad
involved
kept
largest
##ist
bank
future
especially
beginning
mark
movement
section
female
magazine
plan
professor
lord
longer
##ian
sat
walked
hill
actually
civil
energy
model
families
size
thus
aircraft
completed
includes
data
captain
##or
fight
vocals
featured
richard
bridge
fourth
1989
officer
stone
hear
##ism
means
medical
groups
management
self
lips
competition
entire
lived
technology
leaving
federal
tournament
bit
passed
hot
independent
awards
kingdom
mary
spent
fine
doesn
reported
##ling
jack
fall
raised
itself
stay
true
studio
1988
sports
replaced
paris
systems
saint
leader
theatre
whose
market
capital
parents
spanish
canadian
earth
##ity
cut
degree
writing
bay
christian
awarded
natural
higher
bill
##as
coast
provided
previous
senior
ft
valley
organization
stopped
onto
countries
parts
conference
queen
security
interest
saying
allowed
master
earlier
phone
matter
smith
winning
try
happened
moving
campaign
los
##ley
breath
nearly
mid
1987
certain
girls
date
italian
african
standing
fell
artist
##ted
shows
deal
mine
industry
1986
##ng
everyone
republic
provide
collection
library
student
##ville
primary
owned
older
via
heavy
1st
makes
##able
attention
anyone
africa
##ri
stated
length
ended
fingers
command
staff
skin
foreign
opening
governor
okay
medal
kill
sun
cover
job
1985
introduced
chest
hell
feeling
##ies
success
meet
reason
standard
meeting
novel
1984
trade
source
buildings
##land
rose
guy
goal
##ur
chapter
native
husband
previously
unit
limited
entered
weeks
producer
operations
mountain
takes
covered
forced
related
roman
complete
successful
key
texas
cold
##ya
channel
1980
traditional
films
dance
clear
approximately
500
nine
van
prince
question
active
tracks
ireland
regional
silver
author
personal
sense
operation
##ine
economic
1983
holding
twenty
isbn
additional
speed
hour
edition
regular
historic
places
whom
shook
movie
km
secretary
prior
report
chicago
read
foundation
view
engine
scored
1982
units
ask
airport
property
ready
immediately
lady
month
listed
contract
##de
manager
themselves
lines
##ki
navy
writer
meant
##ts
runs
##ro
practice
championships
singer
glass
commission
required
forest
starting
culture
generally
giving
access
attended
test
couple
stand
catholic
martin
caught
executive
##less
eye
##ey
thinking
chair
quite
shoulder
1979
hope
decision
plays
defeated
municipality
whether
structure
offered
slowly
pain
ice
direction
##ion
paper
mission
1981
mostly
200
noted
individual
managed
nature
lives
plant
##ha
helped
except
studied
computer
figure
relationship
issue
significant
loss
die
smiled
gun
ago
highest
1972
##am
male
bring
goals
mexico
problem
distance
commercial
completely
location
annual
famous
drive
1976
neck
1978
surface
caused
italy
understand
greek
highway
wrong
hotel
comes
appearance
joseph
double
issues
musical
companies
castle
income
review
assembly
bass
initially
parliament
artists
experience
1974
particular
walk
foot
engineering
talking
window
dropped
##ter
miss
baby
boys
break
1975
stars
edge
remember
policy
carried
train
stadium
bar
sex
angeles
evidence
##ge
becoming
assistant
soviet
1977
upper
step
wing
1970
youth
financial
reach
##ll
actor
numerous
##se
##st
nodded
arrived
##ation
minute
##nt
believed
sorry
complex
beautiful
victory
associated
temple
1968
1973
chance
perhaps
metal
##son
1945
bishop
##et
lee
launched
particularly
tree
le
retired
subject
prize
contains
yeah
theory
empire
##ce
suddenly
waiting
trust
recording
##to
happy
terms
camp
champion
1971
religious
pass
zealand
names
2nd
port
ancient
tom
corner
represented
watch
legal
anti
justice
cause
watched
brothers
45
material
changes
simply
response
louis
fast
##ting
answer
60
historical
1969
stories
straight
create
feature
increased
rate
administration
virginia
el
activities
cultural
overall
winner
programs
basketball
legs
guard
beyond
cast
doctor
mm
flight
results
remains
cost
effect
winter
##ble
larger
islands
problems
chairman
grew
commander
isn
1967
pay
failed
selected
hurt
fort
box
regiment
majority
journal
35
edward
plans
##ke
##ni
shown
pretty
irish
characters
directly
scene
likely
operated
allow
spring
##j
junior
matches
looks
mike
houses
fellow
##tion
beach
marriage
##ham
##ive
rules
oil
65
florida
expected
nearby
congress
sam
peace
recent
iii
wait
subsequently
cell
##do
variety
serving
agreed
please
poor
joe
pacific
attempt
wood
democratic
piece
prime
##ca
rural
mile
touch
appears
township
1964
1966
soldiers
##men
##ized
1965
pennsylvania
closer
fighting
claimed
score
jones
physical
editor
##ous
filled
genus
specific
sitting
super
mom
##va
therefore
supported
status
fear
cases
store
meaning
wales
minor
spain
tower
focus
vice
frank
follow
parish
separate
golden
horse
fifth
remaining
branch
32
presented
stared
##id
uses
secret
forms
##co
baseball
exactly
##ck
choice
note
discovered
travel
composed
truth
russia
ball
color
kiss
dad
wind
continue
ring
referred
numbers
digital
greater
##ns
metres
slightly
direct
increase
1960
responsible
crew
rule
trees
troops
##no
broke
goes
individuals
hundred
weight
creek
sleep
memory
defense
provides
ordered
code
value
jewish
windows
1944
safe
judge
whatever
corps
realized
growing
pre
##ga
cities
alexander
gaze
lies
spread
scott
letter
showed
situation
mayor
transport
watching
workers
extended
##li
expression
normal
##ment
chart
multiple
border
##ba
host
##ner
daily
mrs
walls
piano
##ko
heat
cannot
##ate
earned
products
drama
era
authority
seasons
join
grade
##io
sign
difficult
machine
1963
territory
mainly
##wood
stations
squadron
1962
stepped
iron
19th
##led
serve
appear
sky
speak
broken
charge
knowledge
kilometres
removed
ships
article
campus
simple
##ty
pushed
britain
##ve
leaves
recently
cd
soft
boston
latter
easy
acquired
poland
##sa
quality
officers
presence
planned
nations
mass
broadcast
jean
share
image
influence
wild
offer
emperor
electric
reading
headed
ability
promoted
yellow
ministry
1942
throat
smaller
politician
##by
latin
spoke
cars
williams
males
lack
pop
80
##ier
acting
seeing
consists
##ti
estate
1961
pressure
johnson
newspaper
jr
chris
olympics
online
conditions
beat
elements
walking
vote
##field
needs
carolina
text
featuring
global
block
shirt
levels
francisco
purpose
females
et
dutch
duke
ahead
gas
twice
safety
serious
turning
highly
lieutenant
firm
maria
amount
mixed
daniel
proposed
perfect
agreement
affairs
3rd
seconds
contemporary
paid
1943
prison
save
kitchen
label
administrative
intended
constructed
academic
nice
teacher
races
1956
formerly
corporation
ben
nation
issued
shut
1958
drums
housing
victoria
seems
opera
1959
graduated
function
von
mentioned
picked
build
recognized
shortly
protection
picture
notable
exchange
elections
1980s
loved
percent
racing
fish
elizabeth
garden
volume
hockey
1941
beside
settled
##ford
1940
competed
replied
drew
1948
actress
marine
scotland
steel
glanced
farm
steve
1957
risk
tonight
positive
magic
singles
effects
gray
screen
dog
##ja
residents
bus
sides
none
secondary
literature
polish
destroyed
flying
founder
households
1939
lay
reserve
usa
gallery
##ler
1946
industrial
younger
approach
appearances
urban
ones
1950
finish
avenue
powerful
fully
growth
page
honor
jersey
projects
advanced
revealed
basic
90
infantry
pair
equipment
visit
33
evening
search
grant
effort
solo
treatment
buried
republican
primarily
bottom
owner
1970s
israel
gives
jim
dream
bob
remain
spot
70
notes
produce
champions
contact
ed
soul
accepted
ways
del
##ally
losing
split
price
capacity
basis
trial
questions
##ina
1955
20th
guess
officially
memorial
naval
initial
##ization
whispered
median
engineer
##ful
sydney
##go
columbia
strength
300
1952
tears
senate
00
card
asian
agent
1947
software
44
draw
warm
supposed
com
pro
##il
transferred
leaned
##at
candidate
escape
mountains
asia
potential
activity
entertainment
seem
traffic
jackson
murder
36
slow
product
orchestra
haven
agency
bbc
taught
website
comedy
unable
storm
planning
albums
rugby
environment
scientific
grabbed
protect
##hi
boat
typically
1954
1953
damage
principal
divided
dedicated
mount
ohio
##berg
pick
fought
driver
##der
empty
shoulders
sort
thank
berlin
prominent
account
freedom
necessary
efforts
alex
headquarters
follows
alongside
des
simon
andrew
suggested
operating
learning
steps
1949
sweet
technical
begin
easily
34
teeth
speaking
settlement
scale
##sh
renamed
ray
max
enemy
semi
joint
compared
##rd
scottish
leadership
analysis
offers
georgia
pieces
captured
animal
deputy
guest
organized
##lin
tony
combined
method
challenge
1960s
huge
wants
battalion
sons
rise
crime
types
facilities
telling
path
1951
platform
sit
1990s
##lo
tells
assigned
rich
pull
##ot
commonly
alive
##za
letters
concept
conducted
wearing
happen
bought
becomes
holy
gets
ocean
defeat
languages
purchased
coffee
occurred
titled
##q
declared
applied
sciences
concert
sounds
jazz
brain
##me
painting
fleet
tax
nick
##ius
michigan
count
animals
leaders
episodes
##line
content
##den
birth
##it
clubs
64
palace
critical
refused
fair
leg
laughed
returning
surrounding
participated
formation
lifted
pointed
connected
rome
medicine
laid
taylor
santa
powers
adam
tall
shared
focused
knowing
yards
entrance
falls
##wa
calling
##ad
sources
chosen
beneath
resources
yard
##ite
nominated
silence
zone
defined
##que
gained
thirty
38
bodies
moon
##ard
adopted
christmas
widely
register
apart
iran
premier
serves
du
unknown
parties
##les
generation
##ff
continues
quick
fields
brigade
quiet
teaching
clothes
impact
weapons
partner
flat
theater
supreme
1938
37
relations
##tor
plants
suffered
1936
wilson
kids
begins
##age
1918
seats
armed
internet
models
worth
laws
400
communities
classes
background
knows
thanks
quarter
reaching
humans
carry
killing
format
kong
hong
setting
75
architecture
disease
railroad
inc
possibly
wish
arthur
thoughts
harry
doors
density
##di
crowd
illinois
stomach
tone
unique
reports
anyway
##ir
liberal
der
vehicle
thick
dry
drug
faced
largely
facility
theme
holds
creation
strange
colonel
##mi
revolution
bell
politics
turns
silent
rail
relief
independence
combat
shape
write
determined
sales
learned
4th
finger
oxford
providing
1937
heritage
fiction
situated
designated
allowing
distribution
hosted
##est
sight
interview
estimated
reduced
##ria
toronto
footballer
keeping
guys
damn
claim
motion
sport
sixth
stayed
##ze
en
rear
receive
handed
twelve
dress
audience
granted
brazil
##well
spirit
##ated
noticed
etc
olympic
representative
eric
tight
trouble
reviews
drink
vampire
missing
roles
ranked
newly
household
finals
wave
critics
##ee
phase
massachusetts
pilot
unlike
philadelphia
bright
guns
crown
organizations
roof
42
respectively
clearly
tongue
marked
circle
fox
korea
bronze
brian
expanded
sexual
supply
yourself
inspired
labour
fc
##ah
reference
vision
draft
connection
brand
reasons
1935
classic
driving
trip
jesus
cells
entry
1920
neither
trail
claims
atlantic
orders
labor
nose
afraid
identified
intelligence
calls
cancer
attacked
passing
stephen
positions
imperial
grey
jason
39
sunday
48
swedish
avoid
extra
uncle
message
covers
allows
surprise
materials
fame
hunter
##ji
1930
citizens
figures
davis
environmental
confirmed
shit
titles
di
performing
difference
acts
attacks
##ov
existing
votes
opportunity
nor
shop
entirely
trains
opposite
pakistan
##pa
develop
resulted
representatives
actions
reality
pressed
##ish
barely
wine
conversation
faculty
northwest
ends
documentary
nuclear
stock
grace
sets
eat
alternative
##ps
bag
resulting
creating
surprised
cemetery
1919
drop
finding
sarah
cricket
streets
tradition
ride
1933
exhibition
target
ear
explained
rain
composer
injury
apartment
municipal
educational
occupied
netherlands
clean
billion
constitution
learn
1914
maximum
classical
francis
lose
opposition
jose
ontario
bear
core
hills
rolled
ending
drawn
permanent
fun
##tes
##lla
lewis
sites
chamber
ryan
##way
scoring
height
1934
##house
lyrics
staring
55
officials
1917
snow
oldest
##tic
orange
##ger
qualified
interior
apparently
succeeded
thousand
dinner
lights
existence
fans
heavily
41
greatest
conservative
send
bowl
plus
enter
catch
##un
economy
duty
1929
speech
authorities
princess
performances
versions
shall
graduate
pictures
effective
remembered
poetry
desk
crossed
starring
starts
passenger
sharp
##ant
acres
ass
weather
falling
rank
fund
supporting
check
adult
publishing
heads
cm
southeast
lane
##burg
application
bc
##ura
les
condition
transfer
prevent
display
ex
regions
earl
federation
cool
relatively
answered
besides
1928
obtained
portion
##town
mix
##ding
reaction
liked
dean
express
peak
1932
##tte
counter
religion
chain
rare
miller
convention
aid
lie
vehicles
mobile
perform
squad
wonder
lying
crazy
sword
##ping
attempted
centuries
weren
philosophy
category
##ize
anna
interested
47
sweden
wolf
frequently
abandoned
kg
literary
alliance
task
entitled
##ay
threw
promotion
factory
tiny
soccer
visited
matt
fm
achieved
52
defence
internal
persian
43
methods
##ging
arrested
otherwise
cambridge
programming
villages
elementary
districts
rooms
criminal
conflict
worry
trained
1931
attempts
waited
signal
bird
truck
subsequent
programme
##ol
ad
49
communist
details
faith
sector
patrick
carrying
laugh
##ss
controlled
korean
showing
origin
fuel
evil
1927
##ent
brief
identity
darkness
address
pool
missed
publication
web
planet
ian
anne
wings
invited
##tt
briefly
standards
kissed
##be
ideas
climate
causing
walter
worse
albert
articles
winners
desire
aged
northeast
dangerous
gate
doubt
1922
wooden
multi
##ky
poet
rising
funding
46
communications
communication
violence
copies
prepared
ford
investigation
skills
1924
pulling
electronic
##ak
##ial
##han
containing
ultimately
offices
singing
understanding
restaurant
tomorrow
fashion
christ
ward
da
pope
stands
5th
flow
studios
aired
commissioned
contained
exist
fresh
americans
##per
wrestling
approved
kid
employed
respect
suit
1925
angel
asking
increasing
frame
angry
selling
1950s
thin
finds
##nd
temperature
statement
ali
explain
inhabitants
towns
extensive
narrow
51
jane
flowers
images
promise
somewhere
object
fly
closely
##ls
1912
bureau
cape
1926
weekly
presidential
legislative
1921
##ai
##au
launch
founding
##ny
978
##ring
artillery
strike
un
institutions
roll
writers
landing
chose
kevin
anymore
pp
##ut
attorney
fit
dan
billboard
receiving
agricultural
breaking
sought
dave
admitted
lands
mexican
##bury
charlie
specifically
hole
iv
howard
credit
moscow
roads
accident
1923
proved
wear
struck
hey
guards
stuff
slid
expansion
1915
cat
anthony
##kin
melbourne
opposed
sub
southwest
architect
failure
plane
1916
##ron
map
camera
tank
listen
regarding
wet
introduction
metropolitan
link
ep
fighter
inch
grown
gene
anger
fixed
buy
dvd
khan
domestic
worldwide
chapel
mill
functions
examples
##head
developing
1910
turkey
hits
pocket
antonio
papers
grow
unless
circuit
18th
concerned
attached
journalist
selection
journey
converted
provincial
painted
hearing
aren
bands
negative
aside
wondered
knight
lap
survey
ma
##ow
noise
billy
##ium
shooting
guide
bedroom
priest
resistance
motor
homes
sounded
giant
##mer
150
scenes
equal
comic
patients
hidden
solid
actual
bringing
afternoon
touched
funds
wedding
consisted
marie
canal
sr
kim
treaty
turkish
recognition
residence
cathedral
broad
knees
incident
shaped
fired
norwegian
handle
cheek
contest
represent
##pe
representing
beauty
##sen
birds
advantage
emergency
wrapped
drawing
notice
pink
broadcasting
##ong
somehow
bachelor
seventh
collected
registered
establishment
alan
assumed
chemical
personnel
roger
retirement
jeff
portuguese
wore
tied
device
threat
progress
advance
##ised
banks
hired
manchester
nfl
teachers
structures
forever
##bo
tennis
helping
saturday
sale
applications
junction
hip
incorporated
neighborhood
dressed
ceremony
##ds
influenced
hers
visual
stairs
decades
inner
kansas
hung
hoped
gain
scheduled
downtown
engaged
austria
clock
norway
certainly
pale
protected
1913
victor
employees
plate
putting
surrounded
##ists
finishing
blues
tropical
##ries
minnesota
consider
philippines
accept
54
retrieved
1900
concern
anderson
properties
institution
gordon
successfully
vietnam
##dy
backing
outstanding
muslim
crossing
folk
producing
usual
demand
occurs
observed
lawyer
educated
##ana
kelly
string
pleasure
budget
items
quietly
colorado
philip
typical
##worth
derived
600
survived
asks
mental
##ide
56
jake
jews
distinguished
ltd
1911
sri
extremely
53
athletic
loud
thousands
worried
shadow
transportation
horses
weapon
arena
importance
users
tim
objects
contributed
dragon
douglas
aware
senator
johnny
jordan
sisters
engines
flag
investment
samuel
shock
capable
clark
row
wheel
refers
session
familiar
biggest
wins
hate
maintained
drove
hamilton
request
expressed
injured
underground
churches
walker
wars
tunnel
passes
stupid
agriculture
softly
cabinet
regarded
joining
indiana
##ea
##ms
push
dates
spend
behavior
woods
protein
gently
chase
morgan
mention
burning
wake
combination
occur
mirror
leads
jimmy
indeed
impossible
singapore
paintings
covering
##nes
soldier
locations
attendance
sell
historian
wisconsin
invasion
argued
painter
diego
changing
egypt
##don
experienced
inches
##ku
missouri
vol
grounds
spoken
switzerland
##gan
reform
rolling
ha
forget
massive
resigned
burned
allen
tennessee
locked
values
improved
##mo
wounded
universe
sick
dating
facing
pack
purchase
user
##pur
moments
##ul
merged
anniversary
1908
coal
brick
understood
causes
dynasty
queensland
establish
stores
crisis
promote
hoping
views
cards
referee
extension
##si
raise
arizona
improve
colonial
formal
charged
##rt
palm
lucky
hide
rescue
faces
95
feelings
candidates
juan
##ell
goods
6th
courses
weekend
59
luke
cash
fallen
##om
delivered
affected
installed
carefully
tries
swiss
hollywood
costs
lincoln
responsibility
##he
shore
file
proper
normally
maryland
assistance
jump
constant
offering
friendly
waters
persons
realize
contain
trophy
800
partnership
factor
58
musicians
cry
bound
oregon
indicated
hero
houston
medium
##ure
consisting
somewhat
##ara
57
cycle
##che
beer
moore
frederick
gotten
eleven
worst
weak
approached
arranged
chin
loan
universal
bond
fifteen
pattern
disappeared
##ney
translated
##zed
lip
arab
capture
interests
insurance
##chi
shifted
cave
prix
warning
sections
courts
coat
plot
smell
feed
golf
favorite
maintain
knife
vs
voted
degrees
finance
quebec
opinion
translation
manner
ruled
operate
productions
choose
musician
discovery
confused
tired
separated
stream
techniques
committed
attend
ranking
kings
throw
passengers
measure
horror
fan
mining
sand
danger
salt
calm
decade
dam
require
runner
##ik
rush
associate
greece
##ker
rivers
consecutive
matthew
##ski
sighed
sq
documents
steam
edited
closing
tie
accused
1905
##ini
islamic
distributed
directors
organisation
bruce
7th
breathing
mad
lit
arrival
concrete
taste
08
composition
shaking
faster
amateur
adjacent
stating
1906
twin
flew
##ran
tokyo
publications
##tone
obviously
ridge
storage
1907
carl
pages
concluded
desert
driven
universities
ages
terminal
sequence
borough
250
constituency
creative
cousin
economics
dreams
margaret
notably
reduce
montreal
mode
17th
ears
saved
jan
vocal
##ica
1909
andy
##jo
riding
roughly
threatened
##ise
meters
meanwhile
landed
compete
repeated
grass
czech
regularly
charges
tea
sudden
appeal
##ung
solution
describes
pierre
classification
glad
parking
##ning
belt
physics
99
rachel
add
hungarian
participate
expedition
damaged
gift
childhood
85
fifty
##red
mathematics
jumped
letting
defensive
mph
##ux
##gh
testing
##hip
hundreds
shoot
owners
matters
smoke
israeli
kentucky
dancing
mounted
grandfather
emma
designs
profit
argentina
##gs
truly
li
lawrence
cole
begun
detroit
willing
branches
smiling
decide
miami
enjoyed
recordings
##dale
poverty
ethnic
gay
##bi
gary
arabic
09
accompanied
##one
##ons
fishing
determine
residential
acid
##ary
alice
returns
starred
mail
##ang
jonathan
strategy
##ue
net
forty
cook
businesses
equivalent
commonwealth
distinct
ill
##cy
seriously
##ors
##ped
shift
harris
replace
rio
imagine
formula
ensure
##ber
additionally
scheme
conservation
occasionally
purposes
feels
favor
##and
##ore
1930s
contrast
hanging
hunt
movies
1904
instruments
victims
danish
christopher
busy
demon
sugar
earliest
colony
studying
balance
duties
##ks
belgium
slipped
carter
05
visible
stages
iraq
fifa
##im
commune
forming
zero
07
continuing
talked
counties
legend
bathroom
option
tail
clay
daughters
afterwards
severe
jaw
visitors
##ded
devices
aviation
russell
kate
##vi
entering
subjects
##ino
temporary
swimming
forth
smooth
ghost
audio
bush
operates
rocks
movements
signs
eddie
##tz
ann
voices
honorary
06
memories
dallas
pure
measures
racial
promised
66
harvard
ceo
16th
parliamentary
indicate
benefit
flesh
dublin
louisiana
1902
1901
patient
sleeping
1903
membership
coastal
medieval
wanting
element
scholars
rice
62
limit
survive
makeup
rating
definitely
collaboration
obvious
##tan
boss
ms
baron
birthday
linked
soil
diocese
##lan
ncaa
##mann
offensive
shell
shouldn
waist
##tus
plain
ross
organ
resolution
manufacturing
adding
relative
kennedy
98
whilst
moth
marketing
gardens
crash
72
heading
partners
credited
carlos
moves
cable
##zi
marshall
##out
depending
bottle
represents
rejected
responded
existed
04
jobs
denmark
lock
##ating
treated
graham
routes
talent
commissioner
drugs
secure
tests
reign
restored
photography
##gi
contributions
oklahoma
designer
disc
grin
seattle
robin
paused
atlanta
unusual
##gate
praised
las
laughing
satellite
hungary
visiting
##sky
interesting
factors
deck
poems
norman
##water
stuck
speaker
rifle
domain
premiered
##her
dc
comics
actors
01
reputation
eliminated
8th
ceiling
prisoners
script
##nce
leather
austin
mississippi
rapidly
admiral
parallel
charlotte
guilty
tools
gender
divisions
fruit
##bs
laboratory
nelson
fantasy
marry
rapid
aunt
tribe
requirements
aspects
suicide
amongst
adams
bone
ukraine
abc
kick
sees
edinburgh
clothing
column
rough
gods
hunting
broadway
gathered
concerns
##ek
spending
ty
12th
snapped
requires
solar
bones
cavalry
##tta
iowa
drinking
waste
index
franklin
charity
thompson
stewart
tip
flash
landscape
friday
enjoy
singh
poem
listening
##back
eighth
fred
differences
adapted
bomb
ukrainian
surgery
corporate
masters
anywhere
##more
waves
odd
sean
portugal
orleans
dick
debate
kent
eating
puerto
cleared
96
expect
cinema
97
guitarist
blocks
electrical
agree
involving
depth
dying
panel
struggle
##ged
peninsula
adults
novels
emerged
vienna
metro
debuted
shoes
tamil
songwriter
meets
prove
beating
instance
heaven
scared
sending
marks
artistic
passage
superior
03
significantly
shopping
##tive
retained
##izing
malaysia
technique
cheeks
##ola
warren
maintenance
destroy
extreme
allied
120
appearing
##yn
fill
advice
alabama
qualifying
policies
cleveland
hat
battery
smart
authors
10th
soundtrack
acted
dated
lb
glance
equipped
coalition
funny
outer
ambassador
roy
possibility
couples
campbell
dna
loose
ethan
supplies
1898
gonna
88
monster
##res
shake
agents
frequency
springs
dogs
practices
61
gang
plastic
easier
suggests
gulf
blade
exposed
colors
industries
markets
pan
nervous
electoral
charts
legislation
ownership
##idae
mac
appointment
shield
copy
assault
socialist
abbey
monument
license
throne
employment
jay
93
replacement
charter
cloud
powered
suffering
accounts
oak
connecticut
strongly
wright
colour
crystal
13th
context
welsh
networks
voiced
gabriel
jerry
##cing
forehead
mp
##ens
manage
schedule
totally
remix
##ii
forests
occupation
print
nicholas
brazilian
strategic
vampires
engineers
76
roots
seek
correct
instrumental
und
alfred
backed
hop
##des
stanley
robinson
traveled
wayne
welcome
austrian
achieve
67
exit
rates
1899
strip
whereas
##cs
sing
deeply
adventure
bobby
rick
jamie
careful
components
cap
useful
personality
knee
##shi
pushing
hosts
02
protest
ca
ottoman
symphony
##sis
63
boundary
1890
processes
considering
considerable
tons
##work
##ft
##nia
cooper
trading
dear
conduct
91
illegal
apple
revolutionary
holiday
definition
harder
##van
jacob
circumstances
destruction
##lle
popularity
grip
classified
liverpool
donald
baltimore
flows
seeking
honour
approval
92
mechanical
till
happening
statue
critic
increasingly
immediate
describe
commerce
stare
##ster
indonesia
meat
rounds
boats
baker
orthodox
depression
formally
worn
naked
claire
muttered
sentence
11th
emily
document
77
criticism
wished
vessel
spiritual
bent
virgin
parker
minimum
murray
lunch
danny
printed
compilation
keyboards
false
blow
belonged
68
raising
78
cutting
##board
pittsburgh
##up
9th
shadows
81
hated
indigenous
jon
15th
barry
scholar
ah
##zer
oliver
##gy
stick
susan
meetings
attracted
spell
romantic
##ver
ye
1895
photo
demanded
customers
##ac
1896
logan
revival
keys
modified
commanded
jeans
##ious
upset
raw
phil
detective
hiding
resident
vincent
##bly
experiences
diamond
defeating
coverage
lucas
external
parks
franchise
helen
bible
successor
percussion
celebrated
il
lift
profile
clan
romania
##ied
mills
##su
nobody
achievement
shrugged
fault
1897
rhythm
initiative
breakfast
carbon
700
69
lasted
violent
74
wound
ken
killer
gradually
filmed
c
dollars
processing
94
remove
criticized
guests
sang
chemistry
##vin
legislature
disney
##bridge
uniform
escaped
integrated
proposal
purple
denied
liquid
karl
influential
morris
nights
stones
intense
experimental
twisted
71
84
##ld
pace
nazi
mitchell
ny
blind
reporter
newspapers
14th
centers
burn
basin
forgotten
surviving
filed
collections
monastery
losses
manual
couch
description
appropriate
merely
tag
missions
sebastian
restoration
replacing
triple
73
elder
julia
warriors
benjamin
julian
convinced
stronger
amazing
declined
versus
merchant
happens
output
finland
bare
barbara
absence
ignored
dawn
injuries
##port
producers
##ram
82
luis
##ities
kw
admit
expensive
electricity
nba
exception
symbol
##ving
ladies
shower
sheriff
characteristics
##je
aimed
button
ratio
effectively
summit
angle
jury
bears
foster
vessels
pants
executed
evans
dozen
advertising
kicked
patrol
1889
competitions
lifetime
principles
athletics
##logy
birmingham
sponsored
89
rob
nomination
1893
acoustic
##sm
creature
longest
##tra
credits
harbor
dust
josh
##so
territories
milk
infrastructure
completion
thailand
indians
leon
archbishop
##sy
assist
pitch
blake
arrangement
girlfriend
serbian
operational
hence
sad
scent
fur
dj
sessions
hp
refer
rarely
##ora
exists
1892
##ten
scientists
dirty
penalty
burst
portrait
seed
79
pole
limits
rival
1894
stable
alpha
grave
constitutional
alcohol
arrest
flower
mystery
devil
architectural
relationships
greatly
habitat
##istic
larry
progressive
remote
cotton
##ics
##ok
preserved
reaches
##ming
cited
86
vast
scholarship
decisions
cbs
joy
teach
1885
editions
knocked
eve
searching
partly
participation
gap
animated
fate
excellent
##ett
na
87
alternate
saints
youngest
##ily
climbed
##ita
##tors
suggest
##ct
discussion
staying
choir
lakes
jacket
revenue
nevertheless
peaked
instrument
wondering
annually
managing
neil
1891
signing
terry
##ice
apply
clinical
brooklyn
aim
catherine
fuck
farmers
figured
ninth
pride
hugh
evolution
ordinary
involvement
comfortable
shouted
tech
encouraged
taiwan
representation
sharing
##lia
##em
panic
exact
cargo
competing
fat
cried
83
1920s
occasions
pa
cabin
borders
utah
marcus
##isation
badly
muscles
##ance
victorian
transition
warner
bet
permission
##rin
slave
terrible
similarly
shares
seth
uefa
possession
medals
benefits
colleges
lowered
perfectly
mall
transit
##ye
##kar
publisher
##ened
harrison
deaths
elevation
##ae
asleep
machines
sigh
ash
hardly
argument
occasion
parent
leo
decline
1888
contribution
##ua
concentration
1000
opportunities
hispanic
guardian
extent
emotions
hips
mason
volumes
bloody
controversy
diameter
steady
mistake
phoenix
identify
violin
##sk
departure
richmond
spin
funeral
enemies
1864
gear
literally
connor
random
sergeant
grab
confusion
1865
transmission
informed
op
leaning
sacred
suspended
thinks
gates
portland
luck
agencies
yours
hull
expert
muscle
layer
practical
sculpture
jerusalem
latest
lloyd
statistics
deeper
recommended
warrior
arkansas
mess
supports
greg
eagle
1880
recovered
rated
concerts
rushed
##ano
stops
eggs
files
premiere
keith
##vo
delhi
turner
pit
affair
belief
paint
##zing
mate
##ach
##ev
victim
##ology
withdrew
bonus
styles
fled
##ud
glasgow
technologies
funded
nbc
adaptation
##ata
portrayed
cooperation
supporters
judges
bernard
justin
hallway
ralph
##ick
graduating
controversial
distant
continental
spider
bite
##ho
recognize
intention
mixing
##ese
egyptian
bow
tourism
suppose
claiming
tiger
dominated
participants
vi
##ru
nurse
partially
tape
##rum
psychology
##rn
essential
touring
duo
voting
civilian
emotional
channels
##king
apparent
hebrew
1887
tommy
carrier
intersection
beast
hudson
##gar
##zo
lab
nova
bench
discuss
costa
##ered
detailed
behalf
drivers
unfortunately
obtain
##lis
rocky
##dae
siege
friendship
honey
##rian
1861
amy
hang
posted
governments
collins
respond
wildlife
preferred
operator
##po
laura
pregnant
videos
dennis
suspected
boots
instantly
weird
automatic
businessman
alleged
placing
throwing
ph
mood
1862
perry
venue
jet
remainder
##lli
##ci
passion
biological
boyfriend
1863
dirt
buffalo
ron
segment
fa
abuse
##era
genre
thrown
stroke
colored
stress
exercise
displayed
##gen
struggled
##tti
abroad
dramatic
wonderful
thereafter
madrid
component
widespread
##sed
tale
citizen
todd
monday
1886
vancouver
overseas
forcing
crying
descent
##ris
discussed
substantial
ranks
regime
1870
provinces
switch
drum
zane
ted
tribes
proof
lp
cream
researchers
volunteer
manor
silk
milan
donated
allies
venture
principle
delivery
enterprise
##ves
##ans
bars
traditionally
witch
reminded
copper
##uk
pete
inter
links
colin
grinned
elsewhere
competitive
frequent
##oy
scream
##hu
tension
texts
submarine
finnish
defending
defend
pat
detail
1884
affiliated
stuart
themes
villa
periods
tool
belgian
ruling
crimes
answers
folded
licensed
resort
demolished
hans
lucy
1881
lion
traded
photographs
writes
craig
##fa
trials
generated
beth
noble
debt
percentage
yorkshire
erected
ss
viewed
grades
confidence
ceased
islam
telephone
retail
##ible
chile
m
roberts
sixteen
##ich
commented
hampshire
innocent
dual
pounds
checked
regulations
afghanistan
sung
rico
liberty
assets
bigger
options
angels
relegated
tribute
wells
attending
leaf
##yan
butler
romanian
forum
monthly
lisa
patterns
gmina
##tory
madison
hurricane
rev
##ians
bristol
##ula
elite
valuable
disaster
democracy
awareness
germans
freyja
##ins
loop
absolutely
paying
populations
maine
sole
prayer
spencer
releases
doorway
bull
##ani
lover
midnight
conclusion
##sson
thirteen
lily
mediterranean
##lt
nhl
proud
sample
##hill
drummer
guinea
##ova
murphy
climb
##ston
instant
attributed
horn
ain
railways
steven
##ao
autumn
ferry
opponent
root
traveling
secured
corridor
stretched
tales
sheet
trinity
cattle
helps
indicates
manhattan
murdered
fitted
1882
gentle
grandmother
mines
shocked
vegas
produces
##light
caribbean
##ou
belong
continuous
desperate
drunk
historically
trio
waved
raf
dealing
nathan
bat
murmured
interrupted
residing
scientist
pioneer
harold
aaron
##net
delta
attempting
minority
mini
believes
chorus
tend
lots
eyed
indoor
load
shots
updated
jail
##llo
concerning
connecting
wealth
##ved
slaves
arrive
rangers
sufficient
rebuilt
##wick
cardinal
flood
muhammad
whenever
relation
runners
moral
repair
viewers
arriving
revenge
punk
assisted
bath
fairly
breathe
lists
innings
illustrated
whisper
nearest
voters
clinton
ties
ultimate
screamed
beijing
lions
andre
fictional
gathering
comfort
radar
suitable
dismissed
hms
ban
pine
wrist
atmosphere
voivodeship
bid
timber
##ned
##nan
giants
##ane
cameron
recovery
uss
identical
categories
switched
serbia
laughter
noah
ensemble
therapy
peoples
touching
##off
locally
pearl
platforms
everywhere
ballet
tables
lanka
herbert
outdoor
toured
derek
1883
spaces
contested
swept
1878
exclusive
slight
connections
##dra
winds
prisoner
collective
bangladesh
tube
publicly
wealthy
thai
##ys
isolated
select
##ric
insisted
pen
fortune
ticket
spotted
reportedly
animation
enforcement
tanks
110
decides
wider
lowest
owen
##time
nod
hitting
##hn
gregory
furthermore
magazines
fighters
solutions
##ery
pointing
requested
peru
reed
chancellor
knights
mask
worker
eldest
flames
reduction
1860
volunteers
##tis
reporting
##hl
wire
advisory
endemic
origins
settlers
pursue
knock
consumer
1876
eu
compound
creatures
mansion
sentenced
ivan
deployed
guitars
frowned
involves
mechanism
kilometers
perspective
shops
maps
terminus
duncan
alien
fist
bridges
##pers
heroes
fed
derby
swallowed
##ros
patent
sara
illness
characterized
adventures
slide
hawaii
jurisdiction
##op
organised
##side
adelaide
walks
biology
se
##ties
rogers
swing
tightly
boundaries
##rie
prepare
implementation
stolen
##sha
certified
colombia
edwards
garage
##mm
recalled
##ball
rage
harm
nigeria
breast
##ren
furniture
pupils
settle
##lus
cuba
balls
client
alaska
21st
linear
thrust
celebration
latino
genetic
terror
##cia
##ening
lightning
fee
witness
lodge
establishing
skull
##ique
earning
hood
##ei
rebellion
wang
sporting
warned
missile
devoted
activist
porch
worship
fourteen
package
1871
decorated
##shire
housed
##ock
chess
sailed
doctors
oscar
joan
treat
garcia
harbour
jeremy
##ire
traditions
dominant
jacques
##gon
##wan
relocated
1879
amendment
sized
companion
simultaneously
volleyball
spun
acre
increases
stopping
loves
belongs
affect
drafted
tossed
scout
battles
1875
filming
shoved
munich
tenure
vertical
romance
pc
##cher
argue
##ical
craft
ranging
www
opens
honest
tyler
yesterday
virtual
##let
muslims
reveal
snake
immigrants
radical
screaming
speakers
firing
saving
belonging
ease
lighting
prefecture
blame
farmer
hungry
grows
rubbed
beam
sur
subsidiary
##cha
armenian
sao
dropping
conventional
##fer
microsoft
reply
qualify
spots
1867
sweat
festivals
##ken
immigration
physician
discover
exposure
sandy
explanation
isaac
implemented
##fish
hart
initiated
connect
stakes
presents
heights
householder
pleased
tourist
regardless
slip
closest
##ction
surely
sultan
brings
riley
preparation
aboard
slammed
baptist
experiment
ongoing
interstate
organic
playoffs
##ika
1877
130
##tar
hindu
error
tours
tier
plenty
arrangements
talks
trapped
excited
sank
ho
athens
1872
denver
welfare
suburb
athletes
trick
diverse
belly
exclusively
yelled
1868
##med
conversion
##ette
1874
internationally
computers
conductor
abilities
sensitive
hello
dispute
measured
globe
rocket
prices
amsterdam
flights
tigers
inn
municipalities
emotion
references
3d
##mus
explains
airlines
manufactured
pm
archaeological
1873
interpretation
devon
comment
##ites
settlements
kissing
absolute
improvement
suite
impressed
barcelona
sullivan
jefferson
towers
jesse
julie
##tin
##lu
grandson
hi
gauge
regard
rings
interviews
trace
raymond
thumb
departments
burns
serial
bulgarian
scores
demonstrated
##ix
1866
kyle
alberta
underneath
romanized
##ward
relieved
acquisition
phrase
cliff
reveals
han
cuts
merger
custom
##dar
nee
gilbert
graduation
##nts
assessment
cafe
difficulty
demands
swung
democrat
jennifer
commons
1940s
grove
##yo
completing
focuses
sum
substitute
bearing
stretch
reception
##py
reflected
essentially
destination
pairs
##ched
survival
resource
##bach
promoting
doubles
messages
tear
##down
##fully
parade
florence
harvey
incumbent
partial
framework
900
pedro
frozen
procedure
olivia
controls
##mic
shelter
personally
temperatures
##od
brisbane
tested
sits
marble
comprehensive
oxygen
leonard
##kov
inaugural
iranian
referring
quarters
attitude
##ivity
mainstream
lined
mars
dakota
norfolk
unsuccessful
##
explosion
helicopter
congressional
##sing
inspector
bitch
seal
departed
divine
##ters
coaching
examination
punishment
manufacturer
sink
columns
unincorporated
signals
nevada
squeezed
dylan
dining
photos
martial
manuel
eighteen
elevator
brushed
plates
ministers
ivy
congregation
##len
slept
specialized
taxes
curve
restricted
negotiations
likes
statistical
arnold
inspiration
execution
bold
intermediate
significance
margin
ruler
wheels
gothic
intellectual
dependent
listened
eligible
buses
widow
syria
earn
cincinnati
collapsed
recipient
secrets
accessible
philippine
maritime
goddess
clerk
surrender
breaks
playoff
database
##ified
##lon
ideal
beetle
aspect
soap
regulation
strings
expand
anglo
shorter
crosses
retreat
tough
coins
wallace
directions
pressing
##oon
shipping
locomotives
comparison
topics
nephew
##mes
distinction
honors
travelled
sierra
ibn
##over
fortress
sa
recognised
carved
1869
clients
##dan
intent
##mar
coaches
describing
bread
##ington
beaten
northwestern
##ona
merit
youtube
collapse
challenges
em
historians
objective
submitted
virus
attacking
drake
assume
##ere
diseases
marc
stem
leeds
##cus
##ab
farming
glasses
##lock
visits
nowhere
fellowship
relevant
carries
restaurants
experiments
101
constantly
bases
targets
shah
tenth
opponents
verse
territorial
##ira
writings
corruption
##hs
instruction
inherited
reverse
emphasis
##vic
employee
arch
keeps
rabbi
watson
payment
uh
##ala
nancy
##tre
venice
fastest
sexy
banned
adrian
properly
ruth
touchdown
dollar
boards
metre
circles
edges
favour
comments
ok
travels
liberation
scattered
firmly
##ular
holland
permitted
diesel
kenya
den
originated
##ral
demons
resumed
dragged
rider
##rus
servant
blinked
extend
torn
##ias
##sey
input
meal
everybody
cylinder
kinds
camps
##fe
bullet
logic
##wn
croatian
evolved
healthy
fool
chocolate
wise
preserve
pradesh
##ess
respective
1850
##ew
chicken
artificial
gross
corresponding
convicted
cage
caroline
dialogue
##dor
narrative
stranger
mario
br
christianity
failing
trent
commanding
buddhist
1848
maurice
focusing
yale
bike
altitude
##ering
mouse
revised
##sley
veteran
##ig
pulls
theology
crashed
campaigns
legion
##ability
drag
excellence
customer
cancelled
intensity
excuse
##lar
liga
participating
contributing
printing
##burn
variable
##rk
curious
bin
legacy
renaissance
##my
symptoms
binding
vocalist
dancer
##nie
grammar
gospel
democrats
ya
enters
sc
diplomatic
hitler
##ser
clouds
mathematical
quit
defended
oriented
##heim
fundamental
hardware
impressive
equally
convince
confederate
guilt
chuck
sliding
##ware
magnetic
narrowed
petersburg
bulgaria
otto
phd
skill
##ama
reader
hopes
pitcher
reservoir
hearts
automatically
expecting
mysterious
bennett
extensively
imagined
seeds
monitor
fix
##ative
journalism
struggling
signature
ranch
encounter
photographer
observation
protests
##pin
influences
##hr
calendar
##all
cruz
croatia
locomotive
hughes
naturally
shakespeare
basement
hook
uncredited
faded
theories
approaches
dare
phillips
filling
fury
obama
##ain
efficient
arc
deliver
min
raid
breeding
inducted
leagues
efficiency
axis
montana
eagles
##ked
supplied
instructions
karen
picking
indicating
trap
anchor
practically
christians
tomb
vary
occasional
electronics
lords
readers
newcastle
faint
innovation
collect
situations
engagement
160
claude
mixture
##feld
peer
tissue
logo
lean
##ration
f
floors
##ven
architects
reducing
##our
##ments
rope
1859
ottawa
##har
samples
banking
declaration
proteins
resignation
francois
saudi
advocate
exhibited
armor
twins
divorce
##ras
abraham
reviewed
jo
temporarily
matrix
physically
pulse
curled
##ena
difficulties
bengal
usage
##ban
annie
riders
certificate
##pi
holes
warsaw
distinctive
jessica
##mon
mutual
1857
customs
circular
eugene
removal
loaded
mere
vulnerable
depicted
generations
dame
heir
enormous
lightly
climbing
pitched
lessons
pilots
nepal
ram
google
preparing
brad
louise
renowned
##
liam
##ably
plaza
shaw
sophie
brilliant
bills
##bar
##nik
fucking
mainland
server
pleasant
seized
veterans
jerked
fail
beta
brush
radiation
stored
warmth
southeastern
nate
sin
raced
berkeley
joke
athlete
designation
trunk
##low
roland
qualification
archives
heels
artwork
receives
judicial
reserves
##bed
woke
installation
abu
floating
fake
lesser
excitement
interface
concentrated
addressed
characteristic
amanda
saxophone
monk
auto
##bus
releasing
egg
dies
interaction
defender
ce
outbreak
glory
loving
##bert
sequel
consciousness
http
awake
ski
enrolled
##ress
handling
rookie
brow
somebody
biography
warfare
amounts
contracts
presentation
fabric
dissolved
challenged
meter
psychological
lt
elevated
rally
accurate
##tha
hospitals
undergraduate
specialist
venezuela
exhibit
shed
nursing
protestant
fluid
structural
footage
jared
consistent
prey
##ska
succession
reflect
exile
lebanon
wiped
suspect
shanghai
resting
integration
preservation
marvel
variant
pirates
sheep
rounded
capita
sailing
colonies
manuscript
deemed
variations
clarke
functional
emerging
boxing
relaxed
curse
azerbaijan
heavyweight
nickname
editorial
rang
grid
tightened
earthquake
flashed
miguel
rushing
##ches
improvements
boxes
brooks
180
consumption
molecular
felix
societies
repeatedly
variation
aids
civic
graphics
professionals
realm
autonomous
receiver
delayed
workshop
militia
chairs
trump
canyon
##point
harsh
extending
lovely
happiness
##jan
stake
eyebrows
embassy
wellington
hannah
##ella
sony
corners
bishops
swear
cloth
contents
xi
namely
commenced
1854
stanford
nashville
courage
graphic
commitment
garrison
##bin
hamlet
clearing
rebels
attraction
literacy
cooking
ruins
temples
jenny
humanity
celebrate
hasn
freight
sixty
rebel
bastard
##art
newton
##ada
deer
##ges
##ching
smiles
delaware
singers
##ets
approaching
assists
flame
##ph
boulevard
barrel
planted
##ome
pursuit
##sia
consequences
posts
shallow
invitation
rode
depot
ernest
kane
rod
concepts
preston
topic
chambers
striking
blast
arrives
descendants
montgomery
ranges
worlds
##lay
##ari
span
chaos
praise
##ag
fewer
1855
sanctuary
mud
fbi
##ions
programmes
maintaining
unity
harper
bore
handsome
closure
tournaments
thunder
nebraska
linda
facade
puts
satisfied
argentine
dale
cork
dome
panama
##yl
1858
tasks
experts
##ates
feeding
equation
##las
##ida
##tu
engage
bryan
##ax
um
quartet
melody
disbanded
sheffield
blocked
gasped
delay
kisses
maggie
connects
##non
sts
poured
creator
publishers
##we
guided
ellis
extinct
hug
gaining
##ord
complicated
##bility
poll
clenched
investigate
##use
thereby
quantum
spine
cdp
humor
kills
administered
semifinals
##du
encountered
ignore
##bu
commentary
##maker
bother
roosevelt
140
plains
halfway
flowing
cultures
crack
imprisoned
neighboring
airline
##ses
##view
##mate
##ec
gather
wolves
marathon
transformed
##ill
cruise
organisations
carol
punch
exhibitions
numbered
alarm
ratings
daddy
silently
##stein
queens
colours
impression
guidance
liu
tactical
##rat
marshal
della
arrow
##ings
rested
feared
tender
owns
bitter
advisor
escort
##ides
spare
farms
grants
##ene
dragons
encourage
colleagues
cameras
##und
sucked
pile
spirits
prague
statements
suspension
landmark
fence
torture
recreation
bags
permanently
survivors
pond
spy
predecessor
bombing
coup
##og
protecting
transformation
glow
##lands
##book
dug
priests
andrea
feat
barn
jumping
##chen
##ologist
##con
casualties
stern
auckland
pipe
serie
revealing
ba
##bel
trevor
mercy
spectrum
yang
consist
governing
collaborated
possessed
epic
comprises
blew
shane
##ack
lopez
honored
magical
sacrifice
judgment
perceived
hammer
mtv
baronet
tune
das
missionary
sheets
350
neutral
oral
threatening
attractive
shade
aims
seminary
##master
estates
1856
michel
wounds
refugees
manufacturers
##nic
mercury
syndrome
porter
##iya
##din
hamburg
identification
upstairs
purse
widened
pause
cared
breathed
affiliate
santiago
prevented
celtic
fisher
125
recruited
byzantine
reconstruction
farther
##mp
diet
sake
au
spite
sensation
##ert
blank
separation
105
##hon
vladimir
armies
anime
##lie
accommodate
orbit
cult
sofia
archive
##ify
##box
founders
sustained
disorder
honours
northeastern
mia
crops
violet
threats
blanket
fires
canton
followers
southwestern
prototype
voyage
assignment
altered
moderate
protocol
pistol
##eo
questioned
brass
lifting
1852
math
authored
##ual
doug
dimensional
dynamic
##san
1851
pronounced
grateful
quest
uncomfortable
boom
presidency
stevens
relating
politicians
chen
barrier
quinn
diana
mosque
tribal
cheese
palmer
portions
sometime
chester
treasure
wu
bend
download
millions
reforms
registration
##osa
consequently
monitoring
ate
preliminary
brandon
invented
ps
eaten
exterior
intervention
ports
documented
log
displays
lecture
sally
favourite
##itz
vermont
lo
invisible
isle
breed
##ator
journalists
relay
speaks
backward
explore
midfielder
actively
stefan
procedures
cannon
blond
kenneth
centered
servants
chains
libraries
malcolm
essex
henri
slavery
##hal
facts
fairy
coached
cassie
cats
washed
cop
##fi
announcement
item
2000s
vinyl
activated
marco
frontier
growled
curriculum
##das
loyal
accomplished
leslie
ritual
kenny
##00
vii
napoleon
hollow
hybrid
jungle
stationed
friedrich
counted
##ulated
platinum
theatrical
seated
col
rubber
glen
1840
diversity
healing
extends
id
provisions
administrator
columbus
##oe
tributary
te
assured
org
##uous
prestigious
examined
lectures
grammy
ronald
associations
bailey
allan
essays
flute
believing
consultant
proceedings
travelling
1853
kit
kerala
yugoslavia
buddy
methodist
##ith
burial
centres
batman
##nda
discontinued
bo
dock
stockholm
lungs
severely
##nk
citing
manga
##ugh
steal
mumbai
iraqi
robot
celebrity
bride
broadcasts
abolished
pot
joel
overhead
franz
packed
reconnaissance
johann
acknowledged
introduce
handled
doctorate
developments
drinks
alley
palestine
##nis
##aki
proceeded
recover
bradley
grain
patch
afford
infection
nationalist
legendary
##ath
interchange
virtually
gen
gravity
exploration
amber
vital
wishes
powell
doctrine
elbow
screenplay
##bird
contribute
indonesian
pet
creates
##com
enzyme
kylie
discipline
drops
manila
hunger
##ien
layers
suffer
fever
bits
monica
keyboard
manages
##hood
searched
appeals
##bad
testament
grande
reid
##war
beliefs
congo
##ification
##dia
si
requiring
##via
casey
1849
regret
streak
rape
depends
syrian
sprint
pound
tourists
upcoming
pub
##xi
tense
##els
practiced
echo
nationwide
guild
motorcycle
liz
##zar
chiefs
desired
elena
bye
precious
absorbed
relatives
booth
pianist
##mal
citizenship
exhausted
wilhelm
##ceae
##hed
noting
quarterback
urge
hectares
##gue
ace
holly
##tal
blonde
davies
parked
sustainable
stepping
twentieth
airfield
galaxy
nest
chip
##nell
tan
shaft
paulo
requirement
##zy
paradise
tobacco
trans
renewed
vietnamese
##cker
##ju
suggesting
catching
holmes
enjoying
md
trips
colt
holder
butterfly
nerve
reformed
cherry
bowling
trailer
carriage
goodbye
appreciate
toy
joshua
interactive
enabled
involve
##kan
collar
determination
bunch
facebook
recall
shorts
superintendent
episcopal
frustration
giovanni
nineteenth
laser
privately
array
circulation
##ovic
armstrong
deals
painful
permit
discrimination
##wi
aires
retiring
cottage
ni
##sta
horizon
ellen
jamaica
ripped
fernando
chapters
playstation
patron
lecturer
navigation
behaviour
genes
georgian
export
solomon
rivals
swift
seventeen
rodriguez
princeton
independently
sox
1847
arguing
entity
casting
hank
criteria
oakland
geographic
milwaukee
reflection
expanding
conquest
dubbed
##tv
halt
brave
brunswick
doi
arched
curtis
divorced
predominantly
somerset
streams
ugly
zoo
horrible
curved
buenos
fierce
dictionary
vector
theological
unions
handful
stability
chan
punjab
segments
##lly
altar
ignoring
gesture
monsters
pastor
##stone
thighs
unexpected
operators
abruptly
coin
compiled
associates
improving
migration
pin
##ose
compact
collegiate
reserved
##urs
quarterfinals
roster
restore
assembled
hurry
oval
##cies
1846
flags
martha
##del
victories
sharply
##rated
argues
deadly
neo
drawings
symbols
performer
##iel
griffin
restrictions
editing
andrews
java
journals
arabia
compositions
dee
pierce
removing
hindi
casino
runway
civilians
minds
nasa
hotels
##zation
refuge
rent
retain
potentially
conferences
suburban
conducting
##tto
##tions
##tle
descended
massacre
##cal
ammunition
terrain
fork
souls
counts
chelsea
durham
drives
cab
##bank
perth
realizing
palestinian
finn
simpson
##dal
betty
##ule
moreover
particles
cardinals
tent
evaluation
extraordinary
##oid
inscription
##works
wednesday
chloe
maintains
panels
ashley
trucks
##nation
cluster
sunlight
strikes
zhang
##wing
dialect
canon
##ap
tucked
##ws
collecting
##mas
##can
##sville
maker
quoted
evan
franco
aria
buying
cleaning
eva
closet
provision
apollo
clinic
rat
##ez
necessarily
ac
##gle
##ising
venues
flipped
cent
spreading
trustees
checking
authorized
##sco
disappointed
##ado
notion
duration
trumpet
hesitated
topped
brussels
rolls
theoretical
hint
define
aggressive
repeat
wash
peaceful
optical
width
allegedly
mcdonald
strict
copyright
##illa
investors
mar
jam
witnesses
sounding
miranda
michelle
privacy
hugo
harmony
##pp
valid
lynn
glared
nina
102
headquartered
diving
boarding
gibson
##ncy
albanian
marsh
routine
dealt
enhanced
er
intelligent
substance
targeted
enlisted
discovers
spinning
observations
pissed
smoking
rebecca
capitol
visa
varied
costume
seemingly
indies
compensation
surgeon
thursday
arsenal
westminster
suburbs
rid
anglican
##ridge
knots
foods
alumni
lighter
fraser
whoever
portal
scandal
##ray
gavin
advised
instructor
flooding
terrorist
##ale
teenage
interim
senses
duck
teen
thesis
abby
eager
overcome
##ile
newport
glenn
rises
shame
##cc
prompted
priority
forgot
bomber
nicolas
protective
360
cartoon
katherine
breeze
lonely
trusted
henderson
richardson
relax
banner
candy
palms
remarkable
##rio
legends
cricketer
essay
ordained
edmund
rifles
trigger
##uri
##away
sail
alert
1830
audiences
penn
sussex
siblings
pursued
indianapolis
resist
rosa
consequence
succeed
avoided
1845
##ulation
inland
##tie
##nna
counsel
profession
chronicle
hurried
##una
eyebrow
eventual
bleeding
innovative
cure
##dom
committees
accounting
con
scope
hardy
heather
tenor
gut
herald
codes
tore
scales
wagon
##oo
luxury
tin
prefer
fountain
triangle
bonds
darling
convoy
dried
traced
beings
troy
accidentally
slam
findings
smelled
joey
lawyers
outcome
steep
bosnia
configuration
shifting
toll
brook
performers
lobby
philosophical
construct
shrine
aggregate
boot
cox
phenomenon
savage
insane
solely
reynolds
lifestyle
##ima
nationally
holdings
consideration
enable
edgar
mo
mama
##tein
fights
relegation
chances
atomic
hub
conjunction
awkward
reactions
currency
finale
kumar
underwent
steering
elaborate
gifts
comprising
melissa
veins
reasonable
sunshine
chi
solve
trails
inhabited
elimination
ethics
huh
ana
molly
consent
apartments
layout
marines
##ces
hunters
bulk
##oma
hometown
##wall
##mont
cracked
reads
neighbouring
withdrawn
admission
wingspan
damned
anthology
lancashire
brands
batting
forgive
cuban
awful
##lyn
104
dimensions
imagination
##ade
dante
##ship
tracking
desperately
goalkeeper
##yne
groaned
workshops
confident
burton
gerald
milton
circus
uncertain
slope
copenhagen
sophia
fog
philosopher
portraits
accent
cycling
varying
gripped
larvae
garrett
specified
scotia
mature
luther
kurt
rap
##kes
aerial
750
ferdinand
heated
es
transported
##shan
safely
nonetheless
##orn
##gal
motors
demanding
##sburg
startled
##brook
ally
generate
caps
ghana
stained
demo
mentions
beds
ap
afterward
diary
##bling
utility
##iro
richards
1837
conspiracy
conscious
shining
footsteps
observer
cyprus
urged
loyalty
developer
probability
olive
upgraded
gym
miracle
insects
graves
1844
ourselves
hydrogen
amazon
katie
tickets
poets
##pm
planes
##pan
prevention
witnessed
dense
jin
randy
tang
warehouse
monroe
bang
archived
elderly
investigations
alec
granite
mineral
conflicts
controlling
aboriginal
carlo
##zu
mechanics
stan
stark
rhode
skirt
est
##berry
bombs
respected
##horn
imposed
limestone
deny
nominee
memphis
grabbing
disabled
##als
amusement
aa
frankfurt
corn
referendum
varies
slowed
disk
firms
unconscious
incredible
clue
sue
##zhou
twist
##cio
joins
idaho
chad
developers
computing
destroyer
103
mortal
tucker
kingston
choices
yu
carson
1800
os
whitney
geneva
pretend
dimension
staged
plateau
maya
##une
freestyle
##bc
rovers
hiv
##ids
tristan
classroom
prospect
##hus
honestly
diploma
lied
thermal
auxiliary
feast
unlikely
iata
##tel
morocco
pounding
treasury
lithuania
considerably
1841
dish
1812
geological
matching
stumbled
destroying
marched
brien
advances
cake
nicole
belle
settling
measuring
directing
##mie
tuesday
bassist
capabilities
stunned
fraud
torpedo
##list
##phone
anton
wisdom
surveillance
ruined
##ulate
lawsuit
healthcare
theorem
halls
trend
aka
horizontal
dozens
acquire
lasting
swim
hawk
gorgeous
fees
vicinity
decrease
adoption
tactics
##ography
pakistani
##ole
draws
##hall
willie
burke
heath
algorithm
integral
powder
elliott
brigadier
jackie
tate
varieties
darker
##cho
lately
cigarette
specimens
adds
##ree
##ensis
##inger
exploded
finalist
cia
murders
wilderness
arguments
nicknamed
acceptance
onwards
manufacture
robertson
jets
tampa
enterprises
blog
loudly
composers
nominations
1838
ai
malta
inquiry
automobile
hosting
viii
rays
tilted
grief
museums
strategies
furious
euro
equality
cohen
poison
surrey
wireless
governed
ridiculous
moses
##esh
##room
vanished
##ito
barnes
attract
morrison
istanbul
##iness
absent
rotation
petition
janet
##logical
satisfaction
custody
deliberately
observatory
comedian
surfaces
pinyin
novelist
strictly
canterbury
oslo
monks
embrace
ibm
jealous
photograph
continent
dorothy
marina
doc
excess
holden
allegations
explaining
stack
avoiding
lance
storyline
majesty
poorly
spike
dos
bradford
raven
travis
classics
proven
voltage
pillow
fists
butt
1842
interpreted
##car
1839
gage
telegraph
lens
promising
expelled
casual
collector
zones
##min
silly
nintendo
##kh
##bra
downstairs
chef
suspicious
afl
flies
vacant
uganda
pregnancy
condemned
lutheran
estimates
cheap
decree
saxon
proximity
stripped
idiot
deposits
contrary
presenter
magnus
glacier
im
offense
edwin
##ori
upright
##long
bolt
##ois
toss
geographical
##izes
environments
delicate
marking
abstract
xavier
nails
windsor
plantation
occurring
equity
saskatchewan
fears
drifted
sequences
vegetation
revolt
##stic
1843
sooner
fusion
opposing
nato
skating
1836
secretly
ruin
lease
##oc
edit
##nne
flora
anxiety
ruby
##ological
##mia
tel
bout
taxi
emmy
frost
rainbow
compounds
foundations
rainfall
assassination
nightmare
dominican
##win
achievements
deserve
orlando
intact
armenia
##nte
calgary
valentine
106
marion
proclaimed
theodore
bells
courtyard
thigh
gonzalez
console
troop
minimal
monte
everyday
##ence
##if
supporter
terrorism
buck
openly
presbyterian
activists
carpet
##iers
rubbing
uprising
##yi
cute
conceived
legally
##cht
millennium
cello
velocity
ji
rescued
cardiff
1835
rex
concentrate
senators
beard
rendered
glowing
battalions
scouts
competitors
sculptor
catalogue
arctic
ion
raja
bicycle
wow
glancing
lawn
##woman
gentleman
lighthouse
publish
predicted
calculated
##val
variants
##gne
strain
##ui
winston
deceased
##nus
touchdowns
brady
caleb
sinking
echoed
crush
hon
blessed
protagonist
hayes
endangered
magnitude
editors
##tine
estimate
responsibilities
##mel
backup
laying
consumed
sealed
zurich
lovers
frustrated
##eau
ahmed
kicking
mit
treasurer
1832
biblical
refuse
terrified
pump
agrees
genuine
imprisonment
refuses
plymouth
##hen
lou
##nen
tara
trembling
antarctic
ton
learns
##tas
crap
crucial
faction
atop
##borough
wrap
lancaster
odds
hopkins
erik
lyon
##eon
bros
##ode
snap
locality
tips
empress
crowned
cal
acclaimed
chuckled
##ory
clara
sends
mild
towel
##fl
##day
##
wishing
assuming
interviewed
##bal
##die
interactions
eden
cups
helena
##lf
indie
beck
##fire
batteries
filipino
wizard
parted
##lam
traces
##born
rows
idol
albany
delegates
##ees
##sar
discussions
##ex
notre
instructed
belgrade
highways
suggestion
lauren
possess
orientation
alexandria
abdul
beats
salary
reunion
ludwig
alright
wagner
intimate
pockets
slovenia
hugged
brighton
merchants
cruel
stole
trek
slopes
repairs
enrollment
politically
underlying
promotional
counting
boeing
##bb
isabella
naming
##
keen
bacteria
listing
separately
belfast
ussr
450
lithuanian
anybody
ribs
sphere
martinez
cock
embarrassed
proposals
fragments
nationals
##fs
##wski
premises
fin
1500
alpine
matched
freely
bounded
jace
sleeve
##af
gaming
pier
populated
evident
##like
frances
flooded
##dle
frightened
pour
trainer
framed
visitor
challenging
pig
wickets
##fold
infected
email
##pes
arose
##aw
reward
ecuador
oblast
vale
ch
shuttle
##usa
bach
rankings
forbidden
cornwall
accordance
salem
consumers
bruno
fantastic
toes
machinery
resolved
julius
remembering
propaganda
iceland
bombardment
tide
contacts
wives
##rah
concerto
macdonald
albania
implement
daisy
tapped
sudan
helmet
angela
mistress
##lic
crop
sunk
finest
##craft
hostile
##ute
##tsu
boxer
fr
paths
adjusted
habit
ballot
supervision
soprano
##zen
bullets
wicked
sunset
regiments
disappear
lamp
performs
app
##gia
##oa
rabbit
digging
incidents
entries
##cion
dishes
##oi
introducing
##ati
##fied
freshman
slot
jill
tackles
baroque
backs
##iest
lone
sponsor
destiny
altogether
convert
##aro
consensus
shapes
demonstration
basically
feminist
auction
artifacts
##bing
strongest
twitter
halifax
2019
allmusic
mighty
smallest
precise
alexandra
viola
##los
##ille
manuscripts
##illo
dancers
ari
managers
monuments
blades
barracks
springfield
maiden
consolidated
electron
##end
berry
airing
wheat
nobel
inclusion
blair
payments
geography
bee
cc
eleanor
react
##hurst
afc
manitoba
##yu
su
lineup
fitness
recreational
investments
airborne
disappointment
##dis
edmonton
viewing
##row
renovation
##cast
infant
bankruptcy
roses
aftermath
pavilion
##yer
carpenter
withdrawal
ladder
##hy
discussing
popped
reliable
agreements
rochester
##abad
curves
bombers
220
rao
reverend
decreased
choosing
107
stiff
consulting
naples
crawford
tracy
ka
ribbon
cops
##lee
crushed
deciding
unified
teenager
accepting
flagship
explorer
poles
sanchez
inspection
revived
skilled
induced
exchanged
flee
locals
tragedy
swallow
loading
hanna
demonstrate
##ela
salvador
flown
contestants
civilization
##ines
wanna
rhodes
fletcher
hector
knocking
considers
##ough
nash
mechanisms
sensed
mentally
walt
unclear
##eus
renovated
madame
##cks
crews
governmental
##hin
undertaken
monkey
##ben
##ato
fatal
armored
copa
caves
governance
grasp
perception
certification
froze
damp
tugged
wyoming
##rg
##ero
newman
##lor
nerves
curiosity
graph
115
##ami
withdraw
tunnels
dull
meredith
moss
exhibits
neighbors
communicate
accuracy
explored
raiders
republicans
secular
kat
superman
penny
criticised
##tch
freed
update
conviction
wade
ham
likewise
delegation
gotta
doll
promises
technological
myth
nationality
resolve
convent
##mark
sharon
dig
sip
coordinator
entrepreneur
fold
##dine
capability
councillor
synonym
blown
swan
cursed
1815
jonas
haired
sofa
canvas
keeper
rivalry
##hart
rapper
speedway
swords
postal
maxwell
estonia
potter
recurring
##nn
##ave
errors
##oni
cognitive
1834
##
claws
nadu
roberto
bce
wrestler
ellie
##ations
infinite
ink
##tia
presumably
finite
staircase
108
noel
patricia
nacional
##cation
chill
eternal
tu
preventing
prussia
fossil
limbs
##logist
ernst
frog
perez
rene
##ace
pizza
prussian
##ios
##vy
molecules
regulatory
answering
opinions
sworn
lengths
supposedly
hypothesis
upward
habitats
seating
ancestors
drank
yield
hd
synthesis
researcher
modest
##var
mothers
peered
voluntary
homeland
##the
acclaim
##igan
static
valve
luxembourg
alto
carroll
fe
receptor
norton
ambulance
##tian
johnston
catholics
depicting
jointly
elephant
gloria
mentor
badge
ahmad
distinguish
remarked
councils
precisely
allison
advancing
detection
crowded
##10
cooperative
ankle
mercedes
dagger
surrendered
pollution
commit
subway
jeffrey
lesson
sculptures
provider
##fication
membrane
timothy
rectangular
fiscal
heating
teammate
basket
particle
anonymous
deployment
##ple
missiles
courthouse
proportion
shoe
sec
##ller
complaints
forbes
blacks
abandon
remind
sizes
overwhelming
autobiography
natalie
##awa
risks
contestant
countryside
babies
scorer
invaded
enclosed
proceed
hurling
disorders
##cu
reflecting
continuously
cruiser
graduates
freeway
investigated
ore
deserved
maid
blocking
phillip
jorge
shakes
dove
mann
variables
lacked
burden
accompanying
que
consistently
organizing
provisional
complained
endless
##rm
tubes
juice
georges
krishna
mick
labels
thriller
##uch
laps
arcade
sage
snail
##table
shannon
fi
laurence
seoul
vacation
presenting
hire
churchill
surprisingly
prohibited
savannah
technically
##oli
170
##lessly
testimony
suited
speeds
toys
romans
mlb
flowering
measurement
talented
kay
settings
charleston
expectations
shattered
achieving
triumph
ceremonies
portsmouth
lanes
mandatory
loser
stretching
cologne
realizes
seventy
cornell
careers
webb
##ulating
americas
budapest
ava
suspicion
##ison
yo
conrad
##hai
sterling
jessie
rector
##az
1831
transform
organize
loans
christine
volcanic
warrant
slender
summers
subfamily
newer
danced
dynamics
rhine
proceeds
heinrich
gastropod
commands
sings
facilitate
easter
ra
positioned
responses
expense
fruits
yanked
imported
25th
velvet
vic
primitive
tribune
baldwin
neighbourhood
donna
rip
hay
pr
##uro
1814
espn
welcomed
##aria
qualifier
glare
highland
timing
##cted
shells
eased
geometry
louder
exciting
slovakia
##sion
##iz
##lot
savings
prairie
##ques
marching
rafael
tonnes
##lled
curtain
preceding
shy
heal
greene
worthy
##pot
detachment
bury
sherman
##eck
reinforced
seeks
bottles
contracted
duchess
outfit
walsh
##sc
mickey
##ase
geoffrey
archer
squeeze
dawson
eliminate
invention
##enberg
neal
##eth
stance
dealer
coral
maple
retire
polo
simplified
##ht
1833
hid
watts
backwards
jules
##oke
genesis
mt
frames
rebounds
burma
woodland
moist
santos
whispers
drained
subspecies
##aa
streaming
ulster
burnt
correspondence
maternal
gerard
denis
stealing
##load
genius
duchy
##oria
inaugurated
momentum
suits
placement
sovereign
clause
thames
##hara
confederation
reservation
sketch
yankees
lets
rotten
charm
hal
verses
ultra
commercially
dot
salon
citation
adopt
winnipeg
mist
allocated
cairo
##boy
jenkins
interference
objectives
##wind
1820
portfolio
armoured
sectors
##eh
initiatives
##world
integrity
exercises
robe
tap
ab
gazed
##tones
distracted
rulers
111
favorable
jerome
tended
cart
factories
##eri
diplomat
valued
gravel
charitable
##try
calvin
exploring
chang
shepherd
terrace
pdf
pupil
##ural
reflects
ups
##rch
governors
shelf
depths
##nberg
trailed
crest
tackle
##nian
##ats
hatred
##kai
clare
makers
ethiopia
longtime
detected
embedded
lacking
slapped
rely
thomson
anticipation
iso
morton
successive
agnes
screenwriter
straightened
philippe
playwright
haunted
licence
iris
intentions
sutton
112
logical
correctly
##weight
branded
licked
tipped
silva
ricky
narrator
requests
##ents
greeted
supernatural
cow
##wald
lung
refusing
employer
strait
gaelic
liner
##piece
zoe
sabha
##mba
driveway
harvest
prints
bates
reluctantly
threshold
algebra
ira
wherever
coupled
240
assumption
picks
##air
designers
raids
gentlemen
##ean
roller
blowing
leipzig
locks
screw
dressing
strand
##lings
scar
dwarf
depicts
##nu
nods
##mine
differ
boris
##eur
yuan
flip
##gie
mob
invested
questioning
applying
##ture
shout
##sel
gameplay
blamed
illustrations
bothered
weakness
rehabilitation
##of
##zes
envelope
rumors
miners
leicester
subtle
kerry
##ico
ferguson
##fu
premiership
ne
##cat
bengali
prof
catches
remnants
dana
##rily
shouting
presidents
baltic
ought
ghosts
dances
sailors
shirley
fancy
dominic
##bie
madonna
##rick
bark
buttons
gymnasium
ashes
liver
toby
oath
providence
doyle
evangelical
nixon
cement
carnegie
embarked
hatch
surroundings
guarantee
needing
pirate
essence
##bee
filter
crane
hammond
projected
immune
percy
twelfth
##ult
regent
doctoral
damon
mikhail
##ichi
lu
critically
elect
realised
abortion
acute
screening
mythology
steadily
##fc
frown
nottingham
kirk
wa
minneapolis
##rra
module
algeria
mc
nautical
encounters
surprising
statues
availability
shirts
pie
alma
brows
munster
mack
soup
crater
tornado
sanskrit
cedar
explosive
bordered
dixon
planets
stamp
exam
happily
##bble
carriers
kidnapped
##vis
accommodation
emigrated
##met
knockout
correspondent
violation
profits
peaks
lang
specimen
agenda
ancestry
pottery
spelling
equations
obtaining
ki
linking
1825
debris
asylum
##20
buddhism
teddy
##ants
gazette
##nger
##sse
dental
eligibility
utc
fathers
averaged
zimbabwe
francesco
coloured
hissed
translator
lynch
mandate
humanities
mackenzie
uniforms
lin
##iana
##gio
asset
mhz
fitting
samantha
genera
wei
rim
beloved
shark
riot
entities
expressions
indo
carmen
slipping
owing
abbot
neighbor
sidney
##av
rats
recommendations
encouraging
squadrons
anticipated
commanders
conquered
##oto
donations
diagnosed
##mond
divide
##iva
guessed
decoration
vernon
auditorium
revelation
conversations
##kers
##power
herzegovina
dash
alike
protested
lateral
herman
accredited
mg
##gent
freeman
mel
fiji
crow
crimson
##rine
livestock
##pped
humanitarian
bored
oz
whip
##lene
##ali
legitimate
alter
grinning
spelled
anxious
oriental
wesley
##nin
##hole
carnival
controller
detect
##ssa
bowed
educator
kosovo
macedonia
##sin
occupy
mastering
stephanie
janeiro
para
unaware
nurses
noon
135
cam
hopefully
ranger
combine
sociology
polar
rica
##eer
neill
##sman
holocaust
##ip
doubled
lust
1828
109
decent
cooling
unveiled
##card
1829
nsw
homer
chapman
meyer
##gin
dive
mae
reagan
expertise
##gled
darwin
brooke
sided
prosecution
investigating
comprised
petroleum
genres
reluctant
differently
trilogy
johns
vegetables
corpse
highlighted
lounge
pension
unsuccessfully
elegant
aided
ivory
beatles
amelia
cain
dubai
sunny
immigrant
babe
click
##nder
underwater
pepper
combining
mumbled
atlas
horns
accessed
ballad
physicians
homeless
gestured
rpm
freak
louisville
corporations
patriots
prizes
rational
warn
modes
decorative
overnight
din
troubled
phantom
##ort
monarch
sheer
##dorf
generals
guidelines
organs
addresses
##zon
enhance
curling
parishes
cord
##kie
linux
caesar
deutsche
bavaria
##bia
coleman
cyclone
##eria
bacon
petty
##yama
##old
hampton
diagnosis
1824
throws
complexity
rita
disputed
##
pablo
##sch
marketed
trafficking
##ulus
examine
plague
formats
##oh
vault
faithful
##bourne
webster
##ox
highlights
##ient
##ann
phones
vacuum
sandwich
modeling
##gated
bolivia
clergy
qualities
isabel
##nas
##ars
wears
screams
reunited
annoyed
bra
##ancy
##rate
differential
transmitter
tattoo
container
poker
##och
excessive
resides
cowboys
##tum
augustus
trash
providers
statute
retreated
balcony
reversed
void
storey
preceded
masses
leap
laughs
neighborhoods
wards
schemes
falcon
santo
battlefield
pad
ronnie
thread
lesbian
venus
##dian
beg
sandstone
daylight
punched
gwen
analog
stroked
wwe
acceptable
measurements
dec
toxic
##kel
adequate
surgical
economist
parameters
varsity
##sberg
quantity
ella
##chy
##rton
countess
generating
precision
diamonds
expressway
ga
##
1821
uruguay
talents
galleries
expenses
scanned
colleague
outlets
ryder
lucien
##ila
paramount
##bon
syracuse
dim
fangs
gown
sweep
##sie
toyota
missionaries
websites
##nsis
sentences
adviser
val
trademark
spells
##plane
patience
starter
slim
##borg
toe
incredibly
shoots
elliot
nobility
##wyn
cowboy
endorsed
gardner
tendency
persuaded
organisms
emissions
kazakhstan
amused
boring
chips
themed
##hand
llc
constantinople
chasing
systematic
guatemala
borrowed
erin
carey
##hard
highlands
struggles
1810
##ifying
##ced
wong
exceptions
develops
enlarged
kindergarten
castro
##ern
##rina
leigh
zombie
juvenile
##most
consul
##nar
sailor
hyde
clarence
intensive
pinned
nasty
useless
jung
clayton
stuffed
exceptional
ix
apostolic
230
transactions
##dge
exempt
swinging
cove
religions
##ash
shields
dairy
bypass
190
pursuing
bug
joyce
bombay
chassis
southampton
chat
interact
redesignated
##pen
nascar
pray
salmon
rigid
regained
malaysian
grim
publicity
constituted
capturing
toilet
delegate
purely
tray
drift
loosely
striker
weakened
trinidad
mitch
itv
defines
transmitted
ming
scarlet
nodding
fitzgerald
fu
narrowly
sp
tooth
standings
virtue
##
##wara
##cting
chateau
gloves
lid
##nel
hurting
conservatory
##pel
sinclair
reopened
sympathy
nigerian
strode
advocated
optional
chronic
discharge
##rc
suck
compatible
laurel
stella
shi
fails
wage
dodge
128
informal
sorts
levi
buddha
villagers
##aka
chronicles
heavier
summoned
gateway
3000
eleventh
jewelry
translations
accordingly
seas
##ency
fiber
pyramid
cubic
dragging
##ista
caring
##ops
android
contacted
lunar
##dt
kai
lisbon
patted
1826
sacramento
theft
madagascar
subtropical
disputes
ta
holidays
piper
willow
mare
cane
itunes
newfoundland
benny
companions
dong
raj
observe
roar
charming
plaque
tibetan
fossils
enacted
manning
bubble
tina
tanzania
##eda
##hir
funk
swamp
deputies
cloak
ufc
scenario
par
scratch
metals
anthem
guru
engaging
specially
##boat
dialects
nineteen
cecil
duet
disability
messenger
unofficial
##lies
defunct
eds
moonlight
drainage
surname
puzzle
honda
switching
conservatives
mammals
knox
broadcaster
sidewalk
cope
##ried
benson
princes
peterson
##sal
bedford
sharks
eli
wreck
alberto
gasp
archaeology
lgbt
teaches
securities
madness
compromise
waving
coordination
davidson
visions
leased
possibilities
eighty
jun
fernandez
enthusiasm
assassin
sponsorship
reviewer
kingdoms
estonian
laboratories
##fy
##nal
applies
verb
celebrations
##zzo
rowing
lightweight
sadness
submit
mvp
balanced
dude
##vas
explicitly
metric
magnificent
mound
brett
mohammad
mistakes
irregular
##hing
##ass
sanders
betrayed
shipped
surge
##enburg
reporters
termed
georg
pity
verbal
bulls
abbreviated
enabling
appealed
##are
##atic
sicily
sting
heel
sweetheart
bart
spacecraft
brutal
monarchy
##tter
aberdeen
cameo
diane
##ub
survivor
clyde
##aries
complaint
##makers
clarinet
delicious
chilean
karnataka
coordinates
1818
panties
##rst
pretending
ar
dramatically
kiev
bella
tends
distances
113
catalog
launching
instances
telecommunications
portable
lindsay
vatican
##eim
angles
aliens
marker
stint
screens
bolton
##rne
judy
wool
benedict
plasma
europa
spark
imaging
filmmaker
swiftly
##een
contributor
##nor
opted
stamps
apologize
financing
butter
gideon
sophisticated
alignment
avery
chemicals
yearly
speculation
prominence
professionally
##ils
immortal
institutional
inception
wrists
identifying
tribunal
derives
gains
##wo
papal
preference
linguistic
vince
operative
brewery
##ont
unemployment
boyd
##ured
##outs
albeit
prophet
1813
bi
##rr
##face
##rad
quarterly
asteroid
cleaned
radius
temper
##llen
telugu
jerk
viscount
menu
##ote
glimpse
##aya
yacht
hawaiian
baden
##rl
laptop
readily
##gu
monetary
offshore
scots
watches
##yang
##arian
upgrade
needle
xbox
lea
encyclopedia
flank
fingertips
##pus
delight
teachings
confirm
roth
beaches
midway
winters
##iah
teasing
daytime
beverly
gambling
bonnie
##backs
regulated
clement
hermann
tricks
knot
##shing
##uring
##vre
detached
ecological
owed
specialty
byron
inventor
bats
stays
screened
unesco
midland
trim
affection
##ander
##rry
jess
thoroughly
feedback
##uma
chennai
strained
heartbeat
wrapping
overtime
pleaded
##sworth
mon
leisure
oclc
##tate
##ele
feathers
angelo
thirds
nuts
surveys
clever
gill
commentator
##dos
darren
rides
gibraltar
##nc
##mu
dissolution
dedication
shin
meals
saddle
elvis
reds
chaired
taller
appreciation
functioning
niece
favored
advocacy
robbie
criminals
suffolk
yugoslav
passport
constable
congressman
hastings
vera
##rov
consecrated
sparks
ecclesiastical
confined
##ovich
muller
floyd
nora
1822
paved
1827
cumberland
ned
saga
spiral
##flow
appreciated
yi
collaborative
treating
similarities
feminine
finishes
##ib
jade
import
##nse
##hot
champagne
mice
securing
celebrities
helsinki
attributes
##gos
cousins
phases
ache
lucia
gandhi
submission
vicar
spear
shine
tasmania
biting
detention
constitute
tighter
seasonal
##gus
terrestrial
matthews
##oka
effectiveness
parody
philharmonic
##onic
1816
strangers
encoded
consortium
guaranteed
regards
shifts
tortured
collision
supervisor
inform
broader
insight
theaters
armour
emeritus
blink
incorporates
mapping
##50
##ein
handball
flexible
##nta
substantially
generous
thief
##own
carr
loses
1793
prose
ucla
romeo
generic
metallic
realization
damages
mk
commissioners
zach
default
##ther
helicopters
lengthy
stems
spa
partnered
spectators
rogue
indication
penalties
teresa
1801
sen
##tric
dalton
##wich
irving
photographic
##vey
dell
deaf
peters
excluded
unsure
##vable
patterson
crawled
##zio
resided
whipped
latvia
slower
ecole
pipes
employers
maharashtra
comparable
va
textile
pageant
##gel
alphabet
binary
irrigation
chartered
choked
antoine
offs
waking
supplement
##wen
quantities
demolition
regain
locate
urdu
folks
alt
114
##mc
scary
andreas
whites
##ava
classrooms
mw
aesthetic
publishes
valleys
guides
cubs
johannes
bryant
conventions
affecting
##itt
drain
awesome
isolation
prosecutor
ambitious
apology
captive
downs
atmospheric
lorenzo
aisle
beef
foul
##onia
kidding
composite
disturbed
illusion
natives
##ffer
emi
rockets
riverside
wartime
painters
adolf
melted
##ail
uncertainty
simulation
hawks
progressed
meantime
builder
spray
breach
unhappy
regina
russians
##urg
determining
##tation
tram
1806
##quin
aging
##12
1823
garion
rented
mister
diaz
terminated
clip
1817
depend
nervously
disco
owe
defenders
shiva
notorious
disbelief
shiny
worcester
##gation
##yr
trailing
undertook
islander
belarus
limitations
watershed
fuller
overlooking
utilized
raphael
1819
synthetic
breakdown
klein
##nate
moaned
memoir
lamb
practicing
##erly
cellular
arrows
exotic
##graphy
witches
117
charted
rey
hut
hierarchy
subdivision
freshwater
giuseppe
aloud
reyes
qatar
marty
sideways
utterly
sexually
jude
prayers
mccarthy
softball
blend
damien
##gging
##metric
wholly
erupted
lebanese
negro
revenues
tasted
comparative
teamed
transaction
labeled
maori
sovereignty
parkway
trauma
gran
malay
121
advancement
descendant
2020
buzz
salvation
inventory
symbolic
##making
antarctica
mps
##gas
##bro
mohammed
myanmar
holt
submarines
tones
##lman
locker
patriarch
bangkok
emerson
remarks
predators
kin
afghan
confession
norwich
rental
emerge
advantages
##zel
rca
##hold
shortened
storms
aidan
##matic
autonomy
compliance
##quet
dudley
atp
##osis
1803
motto
documentation
summary
professors
spectacular
christina
archdiocese
flashing
innocence
remake
##dell
psychic
reef
scare
employ
rs
sticks
meg
gus
leans
##ude
accompany
bergen
tomas
##iko
doom
wages
pools
##nch
##bes
breasts
scholarly
alison
outline
brittany
breakthrough
willis
realistic
##cut
##boro
competitor
##stan
pike
picnic
icon
designing
commercials
washing
villain
skiing
micro
costumes
auburn
halted
executives
##hat
logistics
cycles
vowel
applicable
barrett
exclaimed
eurovision
eternity
ramon
##umi
##lls
modifications
sweeping
disgust
##uck
torch
aviv
ensuring
rude
dusty
sonic
donovan
outskirts
cu
pathway
##band
##gun
##lines
disciplines
acids
cadet
paired
##40
sketches
##sive
marriages
##
folding
peers
slovak
implies
admired
##beck
1880s
leopold
instinct
attained
weston
megan
horace
##ination
dorsal
ingredients
evolutionary
##its
complications
deity
lethal
brushing
levy
deserted
institutes
posthumously
delivering
telescope
coronation
motivated
rapids
luc
flicked
pays
volcano
tanner
weighed
##nica
crowds
frankie
gifted
addressing
granddaughter
winding
##rna
constantine
gomez
##front
landscapes
rudolf
anthropology
slate
werewolf
##lio
astronomy
circa
rouge
dreaming
sack
knelt
drowned
naomi
prolific
tracked
freezing
herb
##dium
agony
randall
twisting
wendy
deposit
touches
vein
wheeler
##bbled
##bor
batted
retaining
tire
presently
compare
specification
daemon
nigel
##grave
merry
recommendation
czechoslovakia
sandra
ng
roma
##sts
lambert
inheritance
sheikh
winchester
cries
examining
##yle
comeback
cuisine
nave
##iv
ko
retrieve
tomatoes
barker
polished
defining
irene
lantern
personalities
begging
tract
swore
1809
175
##gic
omaha
brotherhood
##rley
haiti
##ots
exeter
##ete
##zia
steele
dumb
pearson
210
surveyed
elisabeth
trends
##ef
fritz
##rf
premium
bugs
fraction
calmly
viking
##birds
tug
inserted
unusually
##ield
confronted
distress
crashing
brent
turks
resign
##olo
cambodia
gabe
sauce
##kal
evelyn
116
extant
clusters
quarry
teenagers
luna
##lers
##ister
affiliation
drill
##ashi
panthers
scenic
libya
anita
strengthen
inscriptions
##cated
lace
sued
judith
riots
##uted
mint
##eta
preparations
midst
dub
challenger
##vich
mock
cf
displaced
wicket
breaths
enables
schmidt
analyst
##lum
ag
highlight
automotive
axe
josef
newark
sufficiently
resembles
50th
##pal
flushed
mum
traits
##ante
commodore
incomplete
warming
titular
ceremonial
ethical
118
celebrating
eighteenth
cao
lima
medalist
mobility
strips
snakes
##city
miniature
zagreb
barton
escapes
umbrella
automated
doubted
differs
cooled
georgetown
dresden
cooked
fade
wyatt
rna
jacobs
carlton
abundant
stereo
boost
madras
inning
##hia
spur
ip
malayalam
begged
osaka
groan
escaping
charging
dose
vista
##aj
bud
papa
communists
advocates
edged
tri
##cent
resemble
peaking
necklace
fried
montenegro
saxony
goose
glances
stuttgart
curator
recruit
grocery
sympathetic
##tting
##fort
127
lotus
randolph
ancestor
##rand
succeeding
jupiter
1798
macedonian
##heads
hiking
1808
handing
fischer
##itive
garbage
node
##pies
prone
singular
papua
inclined
attractions
italia
pouring
motioned
grandma
garnered
jacksonville
corp
ego
ringing
aluminum
##hausen
ordering
##foot
drawer
traders
synagogue
##play
##kawa
resistant
wandering
fragile
fiona
teased
var
hardcore
soaked
jubilee
decisive
exposition
mercer
poster
valencia
hale
kuwait
1811
##ises
##wr
##eed
tavern
gamma
122
johan
##uer
airways
amino
gil
##ury
vocational
domains
torres
##sp
generator
folklore
outcomes
##keeper
canberra
shooter
fl
beams
confrontation
##lling
##gram
feb
aligned
forestry
pipeline
jax
motorway
conception
decay
##tos
coffin
##cott
stalin
1805
escorted
minded
##nam
sitcom
purchasing
twilight
veronica
additions
passive
tensions
straw
123
frequencies
1804
refugee
cultivation
##iate
christie
clary
bulletin
crept
disposal
##rich
##zong
processor
crescent
##rol
bmw
emphasized
whale
nazis
aurora
##eng
dwelling
hauled
sponsors
toledo
mega
ideology
theatres
tessa
cerambycidae
saves
turtle
cone
suspects
kara
rusty
yelling
greeks
mozart
shades
cocked
participant
##tro
shire
spit
freeze
necessity
##cos
inmates
nielsen
councillors
loaned
uncommon
omar
peasants
botanical
offspring
daniels
formations
jokes
1794
pioneers
sigma
licensing
##sus
wheelchair
polite
1807
liquor
pratt
trustee
##uta
forewings
balloon
##zz
kilometre
camping
explicit
casually
shawn
foolish
teammates
nm
hassan
carrie
judged
satisfy
vanessa
knives
selective
cnn
flowed
##lice
eclipse
stressed
eliza
mathematician
cease
cultivated
##roy
commissions
browns
##ania
destroyers
sheridan
meadow
##rius
minerals
##cial
downstream
clash
gram
memoirs
ventures
baha
seymour
archie
midlands
edith
fare
flynn
invite
canceled
tiles
stabbed
boulder
incorporate
amended
camden
facial
mollusk
unreleased
descriptions
yoga
grabs
550
raises
ramp
shiver
##rose
coined
pioneering
tunes
qing
warwick
tops
119
melanie
giles
##rous
wandered
##inal
annexed
nov
30th
unnamed
##ished
organizational
airplane
normandy
stoke
whistle
blessing
violations
chased
holders
shotgun
##ctic
outlet
reactor
##vik
tires
tearing
shores
fortified
mascot
constituencies
nc
columnist
productive
tibet
##rta
lineage
hooked
oct
tapes
judging
cody
##gger
hansen
kashmir
triggered
##eva
solved
cliffs
##tree
resisted
anatomy
protesters
transparent
implied
##iga
injection
mattress
excluding
##mbo
defenses
helpless
devotion
##elli
growl
liberals
weber
phenomena
atoms
plug
##iff
mortality
apprentice
howe
convincing
aaa
swimmer
barber
leone
promptly
sodium
def
nowadays
arise
##oning
gloucester
corrected
dignity
norm
erie
##ders
elders
evacuated
sylvia
compression
##yar
hartford
pose
backpack
reasoning
accepts
24th
wipe
millimetres
marcel
##oda
dodgers
albion
1790
overwhelmed
aerospace
oaks
1795
showcase
acknowledge
recovering
nolan
ashe
hurts
geology
fashioned
disappearance
farewell
swollen
shrug
marquis
wimbledon
124
rue
1792
commemorate
reduces
experiencing
inevitable
calcutta
intel
##court
murderer
sticking
fisheries
imagery
bloom
280
brake
##inus
gustav
hesitation
memorable
po
viral
beans
accidents
tunisia
antenna
spilled
consort
treatments
aye
perimeter
##gard
donation
hostage
migrated
banker
addiction
apex
lil
trout
##ously
conscience
##nova
rams
sands
genome
passionate
troubles
##lets
##set
amid
##ibility
##ret
higgins
exceed
vikings
##vie
payne
##zan
muscular
##ste
defendant
sucking
##wal
ibrahim
fuselage
claudia
vfl
europeans
snails
interval
##garh
preparatory
statewide
tasked
lacrosse
viktor
##lation
angola
##hra
flint
implications
employs
teens
patrons
stall
weekends
barriers
scrambled
nucleus
tehran
jenna
parsons
lifelong
robots
displacement
5000
##bles
precipitation
##gt
knuckles
clutched
1802
marrying
ecology
marx
accusations
declare
scars
kolkata
mat
meadows
bermuda
skeleton
finalists
vintage
crawl
coordinate
affects
subjected
orchestral
mistaken
##tc
mirrors
dipped
relied
260
arches
candle
##nick
incorporating
wildly
fond
basilica
owl
fringe
rituals
whispering
stirred
feud
tertiary
slick
goat
honorable
whereby
skip
ricardo
stripes
parachute
adjoining
submerged
synthesizer
##gren
intend
positively
ninety
phi
beaver
partition
fellows
alexis
prohibition
carlisle
bizarre
fraternity
##bre
doubts
icy
cbc
aquatic
sneak
sonny
combines
airports
crude
supervised
spatial
merge
alfonso
##bic
corrupt
scan
undergo
##ams
disabilities
colombian
comparing
dolphins
perkins
##lish
reprinted
unanimous
bounced
hairs
underworld
midwest
semester
bucket
paperback
miniseries
coventry
demise
##leigh
demonstrations
sensor
rotating
yan
##hler
arrange
soils
##idge
hyderabad
labs
##dr
brakes
grandchildren
##nde
negotiated
rover
ferrari
continuation
directorate
augusta
stevenson
counterpart
gore
##rda
nursery
rican
ave
collectively
broadly
pastoral
repertoire
asserted
discovering
nordic
styled
fiba
cunningham
harley
middlesex
survives
tumor
tempo
zack
aiming
lok
urgent
##rade
##nto
devils
##ement
contractor
turin
##wl
##ool
bliss
repaired
simmons
moan
astronomical
cr
negotiate
lyric
1890s
lara
bred
clad
angus
pbs
##ience
engineered
posed
##lk
hernandez
possessions
elbows
psychiatric
strokes
confluence
electorate
lifts
campuses
lava
alps
##ep
##ution
##date
physicist
woody
##page
##ographic
##itis
juliet
reformation
sparhawk
320
complement
suppressed
jewel
##
floated
##kas
continuity
sadly
##ische
inability
melting
scanning
paula
flour
judaism
safer
vague
##lm
solving
curb
##stown
financially
gable
bees
expired
miserable
cassidy
dominion
1789
cupped
145
robbery
facto
amos
warden
resume
tallest
marvin
ing
pounded
usd
declaring
gasoline
##aux
darkened
270
650
sophomore
##mere
erection
gossip
televised
risen
dial
##eu
pillars
##link
passages
profound
##tina
arabian
ashton
silicon
nail
##ead
##lated
##wer
##hardt
fleming
firearms
ducked
circuits
blows
waterloo
titans
##lina
atom
fireplace
cheshire
financed
activation
algorithms
##zzi
constituent
catcher
cherokee
partnerships
sexuality
platoon
tragic
vivian
guarded
whiskey
meditation
poetic
##late
##nga
##ake
porto
listeners
dominance
kendra
mona
chandler
factions
22nd
salisbury
attitudes
derivative
##ido
##haus
intake
paced
javier
illustrator
barrels
bias
cockpit
burnett
dreamed
ensuing
##anda
receptors
someday
hawkins
mattered
##lal
slavic
1799
jesuit
cameroon
wasted
tai
wax
lowering
victorious
freaking
outright
hancock
librarian
sensing
bald
calcium
myers
tablet
announcing
barack
shipyard
pharmaceutical
##uan
greenwich
flush
medley
patches
wolfgang
pt
speeches
acquiring
exams
nikolai
##gg
hayden
kannada
##type
reilly
##pt
waitress
abdomen
devastated
capped
pseudonym
pharmacy
fulfill
paraguay
1796
clicked
##trom
archipelago
syndicated
##hman
lumber
orgasm
rejection
clifford
lorraine
advent
mafia
rodney
brock
##ght
##used
##elia
cassette
chamberlain
despair
mongolia
sensors
developmental
upstream
##eg
##alis
spanning
165
trombone
basque
seeded
interred
renewable
rhys
leapt
revision
molecule
##ages
chord
vicious
nord
shivered
23rd
arlington
debts
corpus
sunrise
bays
blackburn
centimetres
##uded
shuddered
gm
strangely
gripping
cartoons
isabelle
orbital
##ppa
seals
proving
##lton
refusal
strengthened
bust
assisting
baghdad
batsman
portrayal
mara
pushes
spears
og
##cock
reside
nathaniel
brennan
1776
confirmation
caucus
##worthy
markings
yemen
nobles
ku
lazy
viewer
catalan
encompasses
sawyer
##fall
sparked
substances
patents
braves
arranger
evacuation
sergio
persuade
dover
tolerance
penguin
cum
jockey
insufficient
townships
occupying
declining
plural
processed
projection
puppet
flanders
introduces
liability
##yon
gymnastics
antwerp
taipei
hobart
candles
jeep
wes
observers
126
chaplain
bundle
glorious
##hine
hazel
flung
sol
excavations
dumped
stares
sh
bangalore
triangular
icelandic
intervals
expressing
turbine
##vers
songwriting
crafts
##igo
jasmine
ditch
rite
##ways
entertaining
comply
sorrow
wrestlers
basel
emirates
marian
rivera
helpful
##some
caution
downward
networking
##atory
##tered
darted
genocide
emergence
replies
specializing
spokesman
convenient
unlocked
fading
augustine
concentrations
resemblance
elijah
investigator
andhra
##uda
promotes
bean
##rrell
fleeing
wan
simone
announcer
##ame
##bby
lydia
weaver
132
residency
modification
##fest
stretches
##ast
alternatively
nat
lowe
lacks
##ented
pam
tile
concealed
inferior
abdullah
residences
tissues
vengeance
##ided
moisture
peculiar
groove
zip
bologna
jennings
ninja
oversaw
zombies
pumping
batch
livingston
emerald
installations
1797
peel
nitrogen
rama
##fying
##star
schooling
strands
responding
werner
##ost
lime
casa
accurately
targeting
##rod
underway
##uru
hemisphere
lester
##yard
occupies
2d
griffith
angrily
reorganized
##owing
courtney
deposited
##dd
##30
estadio
##ifies
dunn
exiled
##ying
checks
##combe
##
##fly
successes
unexpectedly
blu
assessed
##flower
##
observing
sacked
spiders
kn
##tail
mu
nodes
prosperity
audrey
divisional
155
broncos
tangled
adjust
feeds
erosion
paolo
surf
directory
snatched
humid
admiralty
screwed
gt
reddish
##nese
modules
trench
lamps
bind
leah
bucks
competes
##nz
##form
transcription
##uc
isles
violently
clutching
pga
cyclist
inflation
flats
ragged
unnecessary
##hian
stubborn
coordinated
harriet
baba
disqualified
330
insect
wolfe
##fies
reinforcements
rocked
duel
winked
embraced
bricks
##raj
hiatus
defeats
pending
brightly
jealousy
##xton
##hm
##uki
lena
gdp
colorful
##dley
stein
kidney
##shu
underwear
wanderers
##haw
##icus
guardians
m
roared
habits
##wise
permits
gp
uranium
punished
disguise
bundesliga
elise
dundee
erotic
partisan
pi
collectors
float
individually
rendering
behavioral
bucharest
ser
hare
valerie
corporal
nutrition
proportional
##isa
immense
##kis
pavement
##zie
##eld
sutherland
crouched
1775
##lp
suzuki
trades
endurance
operas
crosby
prayed
priory
rory
socially
##urn
gujarat
##pu
walton
cube
pasha
privilege
lennon
floods
thorne
waterfall
nipple
scouting
approve
##lov
minorities
voter
dwight
extensions
assure
ballroom
slap
dripping
privileges
rejoined
confessed
demonstrating
patriotic
yell
investor
##uth
pagan
slumped
squares
##cle
##kins
confront
bert
embarrassment
##aid
aston
urging
sweater
starr
yuri
brains
williamson
commuter
mortar
structured
selfish
exports
##jon
cds
##him
unfinished
##rre
mortgage
destinations
##nagar
canoe
solitary
buchanan
delays
magistrate
fk
##pling
motivation
##lier
##vier
recruiting
assess
##mouth
malik
antique
1791
pius
rahman
reich
tub
zhou
smashed
airs
galway
xii
conditioning
honduras
discharged
dexter
##pf
lionel
129
debates
lemon
tiffany
volunteered
dom
dioxide
procession
devi
sic
tremendous
advertisements
colts
transferring
verdict
hanover
decommissioned
utter
relate
pac
racism
##top
beacon
limp
similarity
terra
occurrence
ant
##how
becky
capt
updates
armament
richie
pal
##graph
halloween
mayo
##ssen
##bone
cara
serena
fcc
dolls
obligations
##dling
violated
lafayette
jakarta
exploitation
##ime
infamous
iconic
##lah
##park
kitty
moody
reginald
dread
spill
crystals
olivier
modeled
bluff
equilibrium
separating
notices
ordnance
extinction
onset
cosmic
attachment
sammy
expose
privy
anchored
##bil
abbott
admits
bending
baritone
emmanuel
policeman
vaughan
winged
climax
dresses
denny
polytechnic
mohamed
burmese
authentic
nikki
genetics
grandparents
homestead
gaza
postponed
metacritic
una
##sby
##bat
unstable
dissertation
##rial
##cian
curls
obscure
uncovered
bronx
praying
disappearing
##hoe
prehistoric
coke
turret
mutations
nonprofit
pits
monaco
##
##usion
prominently
dispatched
podium
##mir
uci
##uation
133
fortifications
birthplace
kendall
##lby
##oll
preacher
rack
goodman
##rman
persistent
##ott
countless
jaime
recorder
lexington
persecution
jumps
renewal
wagons
##11
crushing
##holder
decorations
##lake
abundance
wrath
laundry
1
garde
##rp
jeanne
beetles
peasant
##sl
splitting
caste
sergei
##rer
##ema
scripts
##ively
rub
satellites
##vor
inscribed
verlag
scrapped
gale
packages
chick
potato
slogan
kathleen
arabs
##culture
counterparts
reminiscent
choral
##tead
rand
retains
bushes
dane
accomplish
courtesy
closes
##oth
slaughter
hague
krakow
lawson
tailed
elias
ginger
##ttes
canopy
betrayal
rebuilding
turf
##hof
frowning
allegiance
brigades
kicks
rebuild
polls
alias
nationalism
td
rowan
audition
bowie
fortunately
recognizes
harp
dillon
horrified
##oro
renault
##tics
ropes
##
presumed
rewarded
infrared
wiping
accelerated
illustration
##rid
presses
practitioners
badminton
##iard
detained
##tera
recognizing
relates
misery
##sies
##tly
reproduction
piercing
potatoes
thornton
esther
manners
hbo
##aan
ours
bullshit
ernie
perennial
sensitivity
illuminated
rupert
##jin
##iss
##ear
rfc
nassau
##dock
staggered
socialism
##haven
appointments
nonsense
prestige
sharma
haul
##tical
solidarity
gps
##ook
##rata
igor
pedestrian
##uit
baxter
tenants
wires
medication
unlimited
guiding
impacts
diabetes
##rama
sasha
pas
clive
extraction
131
continually
constraints
##bilities
sonata
hunted
sixteenth
chu
planting
quote
mayer
pretended
abs
spat
##hua
ceramic
##cci
curtains
pigs
pitching
##dad
latvian
sore
dayton
##sted
##qi
patrols
slice
playground
##nted
shone
stool
apparatus
inadequate
mates
treason
##ija
desires
##liga
##croft
somalia
laurent
mir
leonardo
oracle
grape
obliged
chevrolet
thirteenth
stunning
enthusiastic
##ede
accounted
concludes
currents
basil
##kovic
drought
##rica
mai
##aire
shove
posting
##shed
pilgrimage
humorous
packing
fry
pencil
wines
smells
144
marilyn
aching
newest
clung
bon
neighbours
sanctioned
##pie
mug
##stock
drowning
##mma
hydraulic
##vil
hiring
reminder
lilly
investigators
##ncies
sour
##eous
compulsory
packet
##rion
##graphic
##elle
cannes
##inate
depressed
##rit
heroic
importantly
theresa
##tled
conway
saturn
marginal
rae
##xia
corresponds
royce
pact
jasper
explosives
packaging
aluminium
##ttered
denotes
rhythmic
spans
assignments
hereditary
outlined
originating
sundays
lad
reissued
greeting
beatrice
##dic
pillar
marcos
plots
handbook
alcoholic
judiciary
avant
slides
extract
masculine
blur
##eum
##force
homage
trembled
owens
hymn
trey
omega
signaling
socks
accumulated
reacted
attic
theo
lining
angie
distraction
primera
talbot
##key
1200
ti
creativity
billed
##hey
deacon
eduardo
identifies
proposition
dizzy
gunner
hogan
##yam
##pping
##hol
ja
##chan
jensen
reconstructed
##berger
clearance
darius
##nier
abe
harlem
plea
dei
circled
emotionally
notation
fascist
neville
exceeded
upwards
viable
ducks
##fo
workforce
racer
limiting
shri
##lson
possesses
1600
kerr
moths
devastating
laden
disturbing
locking
##cture
gal
fearing
accreditation
flavor
aide
1870s
mountainous
##baum
melt
##ures
motel
texture
servers
soda
##mb
herd
##nium
erect
puzzled
hum
peggy
examinations
gould
testified
geoff
ren
devised
sacks
##law
denial
posters
grunted
cesar
tutor
ec
gerry
offerings
byrne
falcons
combinations
ct
incoming
pardon
rocking
26th
avengers
flared
mankind
seller
uttar
loch
nadia
stroking
exposing
##hd
fertile
ancestral
instituted
##has
noises
prophecy
taxation
eminent
vivid
pol
##bol
dart
indirect
multimedia
notebook
upside
displaying
adrenaline
referenced
geometric
##iving
progression
##ddy
blunt
announce
##far
implementing
##lav
aggression
liaison
cooler
cares
headache
plantations
gorge
dots
impulse
thickness
ashamed
averaging
kathy
obligation
precursor
137
fowler
symmetry
thee
225
hears
##rai
undergoing
ads
butcher
bowler
##lip
cigarettes
subscription
goodness
##ically
browne
##hos
##tech
kyoto
donor
##erty
damaging
friction
drifting
expeditions
hardened
prostitution
152
fauna
blankets
claw
tossing
snarled
butterflies
recruits
investigative
coated
healed
138
communal
hai
xiii
academics
boone
psychologist
restless
lahore
stephens
mba
brendan
foreigners
printer
##pc
ached
explode
27th
deed
scratched
dared
##pole
cardiac
1780
okinawa
proto
commando
compelled
oddly
electrons
##base
replica
thanksgiving
##rist
sheila
deliberate
stafford
tidal
representations
hercules
ou
##path
##iated
kidnapping
lenses
##tling
deficit
samoa
mouths
consuming
computational
maze
granting
smirk
razor
fixture
ideals
inviting
aiden
nominal
##vs
issuing
julio
pitt
ramsey
docks
##oss
exhaust
##owed
bavarian
draped
anterior
mating
ethiopian
explores
noticing
##nton
discarded
convenience
hoffman
endowment
beasts
cartridge
mormon
paternal
probe
sleeves
interfere
lump
deadline
##rail
jenks
bulldogs
scrap
alternating
justified
reproductive
nam
seize
descending
secretariat
kirby
coupe
grouped
smash
panther
sedan
tapping
##18
lola
cheer
germanic
unfortunate
##eter
unrelated
##fan
subordinate
##sdale
suzanne
advertisement
##ility
horsepower
##lda
cautiously
discourse
luigi
##mans
##fields
noun
prevalent
mao
schneider
everett
surround
governorate
kira
##avia
westward
##take
misty
rails
sustainability
134
unused
##rating
packs
toast
unwilling
regulate
thy
suffrage
nile
awe
assam
definitions
travelers
affordable
##rb
conferred
sells
undefeated
beneficial
torso
basal
repeating
remixes
##pass
bahrain
cables
fang
##itated
excavated
numbering
statutory
##rey
deluxe
##lian
forested
ramirez
derbyshire
zeus
slamming
transfers
astronomer
banana
lottery
berg
histories
bamboo
##uchi
resurrection
posterior
bowls
vaguely
##thi
thou
preserving
tensed
offence
##inas
meyrick
callum
ridden
watt
langdon
tying
lowland
snorted
daring
truman
##hale
##girl
aura
overly
filing
weighing
goa
infections
philanthropist
saunders
eponymous
##owski
latitude
perspectives
reviewing
mets
commandant
radial
##kha
flashlight
reliability
koch
vowels
amazed
ada
elaine
supper
##rth
##encies
predator
debated
soviets
cola
##boards
##nah
compartment
crooked
arbitrary
fourteenth
##ctive
havana
majors
steelers
clips
profitable
ambush
exited
packers
##tile
nude
cracks
fungi
##
limb
trousers
josie
shelby
tens
frederic
##
definite
smoothly
constellation
insult
baton
discs
lingering
##nco
conclusions
lent
staging
becker
grandpa
shaky
##tron
einstein
obstacles
sk
adverse
elle
economically
##moto
mccartney
thor
dismissal
motions
readings
nostrils
treatise
##pace
squeezing
evidently
prolonged
1783
venezuelan
je
marguerite
beirut
takeover
shareholders
##vent
denise
digit
airplay
norse
##bbling
imaginary
pills
hubert
blaze
vacated
eliminating
##ello
vine
mansfield
##tty
retrospective
barrow
borne
clutch
bail
forensic
weaving
##nett
##witz
desktop
citadel
promotions
worrying
dorset
ieee
subdivided
##iating
manned
expeditionary
pickup
synod
chuckle
185
barney
##rz
##ffin
functionality
karachi
litigation
meanings
uc
lick
turbo
anders
##ffed
execute
curl
oppose
ankles
typhoon
##
##ache
##asia
linguistics
compassion
pressures
grazing
perfection
##iting
immunity
monopoly
muddy
backgrounds
136
namibia
francesca
monitors
attracting
stunt
tuition
##
vegetable
##mates
##quent
mgm
jen
complexes
forts
##ond
cellar
bites
seventeenth
royals
flemish
failures
mast
charities
##cular
peruvian
capitals
macmillan
ipswich
outward
frigate
postgraduate
folds
employing
##ouse
concurrently
fiery
##tai
contingent
nightmares
monumental
nicaragua
##kowski
lizard
mal
fielding
gig
reject
##pad
harding
##ipe
coastline
##cin
##nos
beethoven
humphrey
innovations
##tam
##nge
norris
doris
solicitor
huang
obey
141
##lc
niagara
##tton
shelves
aug
bourbon
curry
nightclub
specifications
hilton
##ndo
centennial
dispersed
worm
neglected
briggs
sm
font
kuala
uneasy
plc
##nstein
##bound
##aking
##burgh
awaiting
pronunciation
##bbed
##quest
eh
optimal
zhu
raped
greens
presided
brenda
worries
##life
venetian
marxist
turnout
##lius
refined
braced
sins
grasped
sunderland
nickel
speculated
lowell
cyrillic
communism
fundraising
resembling
colonists
mutant
freddie
usc
##mos
gratitude
##run
mural
##lous
chemist
wi
reminds
28th
steals
tess
pietro
##ingen
promoter
ri
microphone
honoured
rai
sant
##qui
feather
##nson
burlington
kurdish
terrorists
deborah
sickness
##wed
##eet
hazard
irritated
desperation
veil
clarity
##rik
jewels
xv
##gged
##ows
##cup
berkshire
unfair
mysteries
orchid
winced
exhaustion
renovations
stranded
obe
infinity
##nies
adapt
redevelopment
thanked
registry
olga
domingo
noir
tudor
ole
##atus
commenting
behaviors
##ais
crisp
pauline
probable
stirling
wigan
##bian
paralympics
panting
surpassed
##rew
luca
barred
pony
famed
##sters
cassandra
waiter
carolyn
exported
##orted
andres
destructive
deeds
jonah
castles
vacancy
suv
##glass
1788
orchard
yep
famine
belarusian
sprang
##forth
skinny
##mis
administrators
rotterdam
zambia
zhao
boiler
discoveries
##ride
##physics
lucius
disappointing
outreach
spoon
##frame
qualifications
unanimously
enjoys
regency
##iidae
stade
realism
veterinary
rodgers
dump
alain
chestnut
castile
censorship
rumble
gibbs
##itor
communion
reggae
inactivated
logs
loads
##houses
homosexual
##iano
ale
informs
##cas
phrases
plaster
linebacker
ambrose
kaiser
fascinated
850
limerick
recruitment
forge
mastered
##nding
leinster
rooted
threaten
##strom
borneo
##hes
suggestions
scholarships
propeller
documentaries
patronage
coats
constructing
invest
neurons
comet
entirety
shouts
identities
annoying
unchanged
wary
##antly
##ogy
neat
oversight
##kos
phillies
replay
constance
##kka
incarnation
humble
skies
minus
##acy
smithsonian
##chel
guerrilla
jar
cadets
##plate
surplus
audit
##aru
cracking
joanna
louisa
pacing
##lights
intentionally
##iri
diner
nwa
imprint
australians
tong
unprecedented
bunker
naive
specialists
ark
nichols
railing
leaked
pedal
##uka
shrub
longing
roofs
v8
captains
neural
tuned
##ntal
##jet
emission
medina
frantic
codex
definitive
sid
abolition
intensified
stocks
enrique
sustain
genoa
oxide
##written
clues
cha
##gers
tributaries
fragment
venom
##rity
##ente
##sca
muffled
vain
sire
laos
##ingly
##hana
hastily
snapping
surfaced
sentiment
motive
##oft
contests
approximate
mesa
luckily
dinosaur
exchanges
propelled
accord
bourne
relieve
tow
masks
offended
##ues
cynthia
##mmer
rains
bartender
zinc
reviewers
lois
##sai
legged
arrogant
rafe
rosie
comprise
handicap
blockade
inlet
lagoon
copied
drilling
shelley
petals
##inian
mandarin
obsolete
##inated
onward
arguably
productivity
cindy
praising
seldom
busch
discusses
raleigh
shortage
ranged
stanton
encouragement
firstly
conceded
overs
temporal
##uke
cbe
##bos
woo
certainty
pumps
##pton
stalked
##uli
lizzie
periodic
thieves
weaker
##night
gases
shoving
chooses
wc
##chemical
prompting
weights
##kill
robust
flanked
sticky
hu
tuberculosis
##eb
##eal
christchurch
resembled
wallet
reese
inappropriate
pictured
distract
fixing
fiddle
giggled
burger
heirs
hairy
mechanic
torque
apache
obsessed
chiefly
cheng
logging
##tag
extracted
meaningful
numb
##vsky
gloucestershire
reminding
##bay
unite
##lit
breeds
diminished
clown
glove
1860s
##
##ug
archibald
focal
freelance
sliced
depiction
##yk
organism
switches
sights
stray
crawling
##ril
lever
leningrad
interpretations
loops
anytime
reel
alicia
delighted
##ech
inhaled
xiv
suitcase
bernie
vega
licenses
northampton
exclusion
induction
monasteries
racecourse
homosexuality
##right
##sfield
##rky
dimitri
michele
alternatives
ions
commentators
genuinely
objected
pork
hospitality
fencing
stephan
warships
peripheral
wit
drunken
wrinkled
quentin
spends
departing
chung
numerical
spokesperson
##zone
johannesburg
caliber
killers
##udge
assumes
neatly
demographic
abigail
bloc
##vel
mounting
##lain
bentley
slightest
xu
recipients
##jk
merlin
##writer
seniors
prisons
blinking
hindwings
flickered
kappa
##hel
80s
strengthening
appealing
brewing
gypsy
mali
lashes
hulk
unpleasant
harassment
bio
treaties
predict
instrumentation
pulp
troupe
boiling
mantle
##ffe
ins
##vn
dividing
handles
verbs
##onal
coconut
senegal
340
thorough
gum
momentarily
##sto
cocaine
panicked
destined
##turing
teatro
denying
weary
captained
mans
##hawks
##code
wakefield
bollywood
thankfully
##16
cyril
##wu
amendments
##bahn
consultation
stud
reflections
kindness
1787
internally
##ovo
tex
mosaic
distribute
paddy
seeming
143
##hic
piers
##15
##mura
##verse
popularly
winger
kang
sentinel
mccoy
##anza
covenant
##bag
verge
fireworks
suppress
thrilled
dominate
##jar
swansea
##60
142
reconciliation
##ndi
stiffened
cue
dorian
##uf
damascus
amor
ida
foremost
##aga
porsche
unseen
dir
##had
##azi
stony
lexi
melodies
##nko
angular
integer
podcast
ants
inherent
jaws
justify
persona
##olved
josephine
##nr
##ressed
customary
flashes
gala
cyrus
glaring
backyard
ariel
physiology
greenland
html
stir
avon
atletico
finch
methodology
ked
##lent
mas
catholicism
townsend
branding
quincy
fits
containers
1777
ashore
aragon
##19
forearm
poisoning
##sd
adopting
conquer
grinding
amnesty
keller
finances
evaluate
forged
lankan
instincts
##uto
guam
bosnian
photographed
workplace
desirable
protector
##dog
allocation
intently
encourages
willy
##sten
bodyguard
electro
brighter
##
bihar
##chev
lasts
opener
amphibious
sal
verde
arte
##cope
captivity
vocabulary
yields
##tted
agreeing
desmond
pioneered
##chus
strap
campaigned
railroads
##
emblem
##dre
stormed
501
##ulous
marijuana
northumberland
##gn
##nath
bowen
landmarks
beaumont
##qua
danube
##bler
attorneys
th
ge
flyers
critique
villains
cass
mutation
acc
##0s
colombo
mckay
motif
sampling
concluding
syndicate
##rell
neon
stables
ds
warnings
clint
mourning
wilkinson
##tated
merrill
leopard
evenings
exhaled
emil
sonia
ezra
discrete
stove
farrell
fifteenth
prescribed
superhero
##rier
worms
helm
wren
##duction
##hc
expo
##rator
hq
unfamiliar
antony
prevents
acceleration
fiercely
mari
painfully
calculations
cheaper
ign
clifton
irvine
davenport
mozambique
##np
pierced
##evich
wonders
##wig
##cate
##iling
crusade
ware
##uel
enzymes
reasonably
mls
##coe
mater
ambition
bunny
eliot
kernel
##fin
asphalt
headmaster
torah
aden
lush
pins
waived
##care
##yas
joao
substrate
enforce
##grad
##ules
alvarez
selections
epidemic
tempted
##bit
bremen
translates
ensured
waterfront
29th
forrest
manny
malone
kramer
reigning
cookies
simpler
absorption
205
engraved
##ffy
evaluated
1778
haze
146
comforting
crossover
##abe
thorn
##rift
##imo
##pop
suppression
fatigue
cutter
##tr
201
wurttemberg
##orf
enforced
hovering
proprietary
gb
samurai
syllable
ascent
lacey
tick
lars
tractor
merchandise
rep
bouncing
defendants
##yre
huntington
##ground
##oko
standardized
##hor
##hima
assassinated
nu
predecessors
rainy
liar
assurance
lyrical
##uga
secondly
flattened
ios
parameter
undercover
##mity
bordeaux
punish
ridges
markers
exodus
inactive
hesitate
debbie
nyc
pledge
savoy
nagar
offset
organist
##tium
hesse
marin
converting
##iver
diagram
propulsion
pu
validity
reverted
supportive
##dc
ministries
clans
responds
proclamation
##inae
##
##rea
ein
pleading
patriot
sf
birch
islanders
strauss
hates
##dh
brandenburg
concession
rd
##ob
1900s
killings
textbook
antiquity
cinematography
wharf
embarrassing
setup
creed
farmland
inequality
centred
signatures
fallon
370
##ingham
##uts
ceylon
gazing
directive
laurie
##tern
globally
##uated
##dent
allah
excavation
threads
##cross
148
frantically
icc
utilize
determines
respiratory
thoughtful
receptions
##dicate
merging
chandra
seine
147
builders
builds
diagnostic
dev
visibility
goddamn
analyses
dhaka
cho
proves
chancel
concurrent
curiously
canadians
pumped
restoring
1850s
turtles
jaguar
sinister
spinal
traction
declan
vows
1784
glowed
capitalism
swirling
install
universidad
##lder
##oat
soloist
##genic
##oor
coincidence
beginnings
nissan
dip
resorts
caucasus
combustion
infectious
##eno
pigeon
serpent
##itating
conclude
masked
salad
jew
##gr
surreal
toni
##wc
harmonica
151
##gins
##etic
##coat
fishermen
intending
bravery
##wave
klaus
titan
wembley
taiwanese
ransom
40th
incorrect
hussein
eyelids
jp
cooke
dramas
utilities
##etta
##print
eisenhower
principally
granada
lana
##rak
openings
concord
##bl
bethany
connie
morality
sega
##mons
##nard
earnings
##kara
##cine
wii
communes
##rel
coma
composing
softened
severed
grapes
##17
nguyen
analyzed
warlord
hubbard
heavenly
behave
slovenian
##hit
##ony
hailed
filmmakers
trance
caldwell
skye
unrest
coward
likelihood
##aging
bern
sci
taliban
honolulu
propose
##wang
1700
browser
imagining
cobra
contributes
dukes
instinctively
conan
violinist
##ores
accessories
gradual
##amp
quotes
sioux
##dating
undertake
intercepted
sparkling
compressed
139
fungus
tombs
haley
imposing
rests
degradation
lincolnshire
retailers
wetlands
tulsa
distributor
dungeon
nun
greenhouse
convey
atlantis
aft
exits
oman
dresser
lyons
##sti
joking
eddy
judgement
omitted
digits
##cts
##game
juniors
##rae
cents
stricken
une
##ngo
wizards
weir
breton
nan
technician
fibers
liking
royalty
##cca
154
persia
terribly
magician
##rable
##unt
vance
cafeteria
booker
camille
warmer
##static
consume
cavern
gaps
compass
contemporaries
foyer
soothing
graveyard
maj
plunged
blush
##wear
cascade
demonstrates
ordinance
##nov
boyle
##lana
rockefeller
shaken
banjo
izzy
##ense
breathless
vines
##32
##eman
alterations
chromosome
dwellings
feudal
mole
153
catalonia
relics
tenant
mandated
##fm
fridge
hats
honesty
patented
raul
heap
cruisers
accusing
enlightenment
infants
wherein
chatham
contractors
zen
affinity
hc
osborne
piston
156
traps
maturity
##rana
lagos
##zal
peering
##nay
attendant
dealers
protocols
subset
prospects
biographical
##cre
artery
##zers
insignia
nuns
endured
##eration
recommend
schwartz
serbs
berger
cromwell
crossroads
##ctor
enduring
clasped
grounded
##bine
marseille
twitched
abel
choke
https
catalyst
moldova
italians
##tist
disastrous
wee
##oured
##nti
wwf
nope
##piration
##asa
expresses
thumbs
167
##nza
coca
1781
cheating
##ption
skipped
sensory
heidelberg
spies
satan
dangers
semifinal
202
bohemia
whitish
confusing
shipbuilding
relies
surgeons
landings
ravi
baku
moor
suffix
alejandro
##yana
litre
upheld
##unk
rajasthan
##rek
coaster
insists
posture
scenarios
etienne
favoured
appoint
transgender
elephants
poked
greenwood
defences
fulfilled
militant
somali
1758
chalk
potent
##ucci
migrants
wink
assistants
nos
restriction
activism
niger
##ario
colon
shaun
##sat
daphne
##erated
swam
congregations
reprise
considerations
magnet
playable
xvi
##
overthrow
tobias
knob
chavez
coding
##mers
propped
katrina
orient
newcomer
##suke
temperate
##pool
farmhouse
interrogation
##vd
committing
##vert
forthcoming
strawberry
joaquin
macau
ponds
shocking
siberia
##cellular
chant
contributors
##nant
##ologists
sped
absorb
hail
1782
spared
##hore
barbados
karate
opus
originates
saul
##xie
evergreen
leaped
##rock
correlation
exaggerated
weekday
unification
bump
tracing
brig
afb
pathways
utilizing
##ners
mod
mb
disturbance
kneeling
##stad
##guchi
100th
pune
##thy
decreasing
168
manipulation
miriam
academia
ecosystem
occupational
rbi
##lem
rift
##14
rotary
stacked
incorporation
awakening
generators
guerrero
racist
##omy
cyber
derivatives
culminated
allie
annals
panzer
sainte
wikipedia
pops
zu
austro
##vate
algerian
politely
nicholson
mornings
educate
tastes
thrill
dartmouth
##gating
db
##jee
regan
differing
concentrating
choreography
divinity
##media
pledged
alexandre
routing
gregor
madeline
##idal
apocalypse
##hora
gunfire
culminating
elves
fined
liang
lam
programmed
tar
guessing
transparency
gabrielle
##gna
cancellation
flexibility
##lining
accession
shea
stronghold
nets
specializes
##rgan
abused
hasan
sgt
ling
exceeding
##
admiration
supermarket
##ark
photographers
specialised
tilt
resonance
hmm
perfume
380
sami
threatens
garland
botany
guarding
boiled
greet
puppy
russo
supplier
wilmington
vibrant
vijay
##bius
paralympic
grumbled
paige
faa
licking
margins
hurricanes
##gong
fest
grenade
ripping
##uz
counseling
weigh
##sian
needles
wiltshire
edison
costly
##not
fulton
tramway
redesigned
staffordshire
cache
gasping
watkins
sleepy
candidacy
##group
monkeys
timeline
throbbing
##bid
##sos
berth
uzbekistan
vanderbilt
bothering
overturned
ballots
gem
##iger
sunglasses
subscribers
hooker
compelling
ang
exceptionally
saloon
stab
##rdi
carla
terrifying
rom
##vision
coil
##oids
satisfying
vendors
31st
mackay
deities
overlooked
ambient
bahamas
felipe
olympia
whirled
botanist
advertised
tugging
##dden
disciples
morales
unionist
rites
foley
morse
motives
creepy
##
soo
##sz
bargain
highness
frightening
turnpike
tory
reorganization
##cer
depict
biographer
##walk
unopposed
manifesto
##gles
institut
emile
accidental
kapoor
##dam
kilkenny
cortex
lively
##13
romanesque
jain
shan
cannons
##ood
##ske
petrol
echoing
amalgamated
disappears
cautious
proposes
sanctions
trenton
##
flotilla
aus
contempt
tor
canary
cote
theirs
##hun
conceptual
deleted
fascinating
paso
blazing
elf
honourable
hutchinson
##eiro
##outh
##zin
surveyor
tee
amidst
wooded
reissue
intro
##ono
cobb
shelters
newsletter
hanson
brace
encoding
confiscated
dem
caravan
marino
scroll
melodic
cows
imam
##adi
##aneous
northward
searches
biodiversity
cora
310
roaring
##bers
connell
theologian
halo
compose
pathetic
unmarried
dynamo
##oot
az
calculation
toulouse
deserves
humour
nr
forgiveness
tam
undergone
martyr
pamela
myths
whore
counselor
hicks
290
heavens
battleship
electromagnetic
##bbs
stellar
establishments
presley
hopped
##chin
temptation
90s
wills
nas
##yuan
nhs
##nya
seminars
##yev
adaptations
gong
asher
lex
indicator
sikh
tobago
cites
goin
##yte
satirical
##gies
characterised
correspond
bubbles
lure
participates
##vid
eruption
skate
therapeutic
1785
canals
wholesale
defaulted
sac
460
petit
##zzled
virgil
leak
ravens
256
portraying
##yx
ghetto
creators
dams
portray
vicente
##rington
fae
namesake
bounty
##arium
joachim
##ota
##iser
aforementioned
axle
snout
depended
dismantled
reuben
480
##ibly
gallagher
##lau
##pd
earnest
##ieu
##iary
inflicted
objections
##llar
asa
gritted
##athy
jericho
##sea
##was
flick
underside
ceramics
undead
substituted
195
eastward
undoubtedly
wheeled
chimney
##iche
guinness
cb
##ager
siding
##bell
traitor
baptiste
disguised
inauguration
149
tipperary
choreographer
perched
warmed
stationary
eco
##ike
##ntes
bacterial
##aurus
flores
phosphate
##core
attacker
invaders
alvin
intersects
a1
indirectly
immigrated
businessmen
cornelius
valves
narrated
pill
sober
ul
nationale
monastic
applicants
scenery
##jack
161
motifs
constitutes
cpu
##osh
jurisdictions
sd
tuning
irritation
woven
##uddin
fertility
gao
##erie
antagonist
impatient
glacial
hides
boarded
denominations
interception
##jas
cookie
nicola
##tee
algebraic
marquess
bahn
parole
buyers
bait
turbines
paperwork
bestowed
natasha
renee
oceans
purchases
157
vaccine
215
##tock
fixtures
playhouse
integrate
jai
oswald
intellectuals
##cky
booked
nests
mortimer
##isi
obsession
sept
##gler
##sum
440
scrutiny
simultaneous
squinted
##shin
collects
oven
shankar
penned
remarkably
##
slips
luggage
spectral
1786
collaborations
louie
consolidation
##ailed
##ivating
420
hoover
blackpool
harness
ignition
vest
tails
belmont
mongol
skinner
##nae
visually
mage
derry
##tism
##unce
stevie
transitional
##rdy
redskins
drying
prep
prospective
##21
annoyance
oversee
##loaded
fills
##books
##iki
announces
fda
scowled
respects
prasad
mystic
tucson
##vale
revue
springer
bankrupt
1772
aristotle
salvatore
habsburg
##geny
dal
natal
nut
pod
chewing
darts
moroccan
walkover
rosario
lenin
punjabi
##e
grossed
scattering
wired
invasive
hui
polynomial
corridors
wakes
gina
portrays
##cratic
arid
retreating
erich
irwin
sniper
##dha
linen
lindsey
maneuver
butch
shutting
socio
bounce
commemorative
postseason
jeremiah
pines
275
mystical
beads
bp
abbas
furnace
bidding
consulted
assaulted
empirical
rubble
enclosure
sob
weakly
cancel
polly
yielded
##emann
curly
prediction
battered
70s
vhs
jacqueline
render
sails
barked
detailing
grayson
riga
sloane
raging
##yah
herbs
bravo
##athlon
alloy
giggle
imminent
suffers
assumptions
waltz
##itate
accomplishments
##ited
bathing
remixed
deception
prefix
##emia
deepest
##tier
##eis
balkan
frogs
##rong
slab
##pate
philosophers
peterborough
grains
imports
dickinson
rwanda
##atics
1774
dirk
lan
tablets
##rove
clone
##rice
caretaker
hostilities
mclean
##gre
regimental
treasures
norms
impose
tsar
tango
diplomacy
variously
complain
192
recognise
arrests
1779
celestial
pulitzer
##dus
bing
libretto
##moor
adele
splash
##rite
expectation
lds
confronts
##izer
spontaneous
harmful
wedge
entrepreneurs
buyer
##ope
bilingual
translate
rugged
conner
circulated
uae
eaton
##gra
##zzle
lingered
lockheed
vishnu
reelection
alonso
##oom
joints
yankee
headline
cooperate
heinz
laureate
invading
##sford
echoes
scandinavian
##dham
hugging
vitamin
salute
micah
hind
trader
##sper
radioactive
##ndra
militants
poisoned
ratified
remark
campeonato
deprived
wander
prop
##dong
outlook
##tani
##rix
##eye
chiang
darcy
##oping
mandolin
spice
statesman
babylon
182
walled
forgetting
afro
##cap
158
giorgio
buffer
##polis
planetary
##gis
overlap
terminals
kinda
centenary
##bir
arising
manipulate
elm
ke
1770
ak
##tad
chrysler
mapped
moose
pomeranian
quad
macarthur
assemblies
shoreline
recalls
stratford
##rted
noticeable
##evic
imp
##rita
##sque
accustomed
supplying
tents
disgusted
vogue
sipped
filters
khz
reno
selecting
luftwaffe
mcmahon
tyne
masterpiece
carriages
collided
dunes
exercised
flare
remembers
muzzle
##mobile
heck
##rson
burgess
lunged
middleton
boycott
bilateral
##sity
hazardous
lumpur
multiplayer
spotlight
jackets
goldman
liege
porcelain
rag
waterford
benz
attracts
hopeful
battling
ottomans
kensington
baked
hymns
cheyenne
lattice
levine
borrow
polymer
clashes
michaels
monitored
commitments
denounced
##25
##von
cavity
##oney
hobby
akin
##holders
futures
intricate
cornish
patty
##oned
illegally
dolphin
##lag
barlow
yellowish
maddie
apologized
luton
plagued
##puram
nana
##rds
sway
fanny
odz
##rino
psi
suspicions
hanged
##eding
initiate
charlton
##por
nak
competent
235
analytical
annex
wardrobe
reservations
##rma
sect
162
fairfax
hedge
piled
buckingham
uneven
bauer
simplicity
snyder
interpret
accountability
donors
moderately
byrd
continents
##cite
##max
disciple
hr
jamaican
ping
nominees
##uss
mongolian
diver
attackers
eagerly
ideological
pillows
miracles
apartheid
revolver
sulfur
clinics
moran
163
##enko
ile
katy
rhetoric
##icated
chronology
recycling
##hrer
elongated
mughal
pascal
profiles
vibration
databases
domination
##fare
##rant
matthias
digest
rehearsal
polling
weiss
initiation
reeves
clinging
flourished
impress
ngo
##hoff
##ume
buckley
symposium
rhythms
weed
emphasize
transforming
##taking
##gence
##yman
accountant
analyze
flicker
foil
priesthood
voluntarily
decreases
##80
##hya
slater
sv
charting
mcgill
##lde
moreno
##iu
besieged
zur
robes
##phic
admitting
api
deported
turmoil
peyton
earthquakes
##ares
nationalists
beau
clair
brethren
interrupt
welch
curated
galerie
requesting
164
##ested
impending
steward
viper
##vina
complaining
beautifully
brandy
foam
nl
1660
##cake
alessandro
punches
laced
explanations
##lim
attribute
clit
reggie
discomfort
##cards
smoothed
whales
##cene
adler
countered
duffy
disciplinary
widening
recipe
reliance
conducts
goats
gradient
preaching
##shaw
matilda
quasi
striped
meridian
cannabis
cordoba
certificates
##agh
##tering
graffiti
hangs
pilgrims
repeats
##ych
revive
urine
etat
##hawk
fueled
belts
fuzzy
susceptible
##hang
mauritius
salle
sincere
beers
hooks
##cki
arbitration
entrusted
advise
sniffed
seminar
junk
donnell
processors
principality
strapped
celia
mendoza
everton
fortunes
prejudice
starving
reassigned
steamer
##lund
tuck
evenly
foreman
##ffen
dans
375
envisioned
slit
##xy
baseman
liberia
rosemary
##weed
electrified
periodically
potassium
stride
contexts
sperm
slade
mariners
influx
bianca
subcommittee
##rane
spilling
icao
estuary
##nock
delivers
iphone
##ulata
isa
mira
bohemian
dessert
##sbury
welcoming
proudly
slowing
##chs
musee
ascension
russ
##vian
waits
##psy
africans
exploit
##morphic
gov
eccentric
crab
peck
##ull
entrances
formidable
marketplace
groom
bolted
metabolism
patton
robbins
courier
payload
endure
##ifier
andes
refrigerator
##pr
ornate
##uca
ruthless
illegitimate
masonry
strasbourg
bikes
adobe
##
apples
quintet
willingly
niche
bakery
corpses
energetic
##cliffe
##sser
##ards
177
centimeters
centro
fuscous
cretaceous
rancho
##yde
andrei
telecom
tottenham
oasis
ordination
vulnerability
presiding
corey
cp
penguins
sims
##pis
malawi
piss
##48
correction
##cked
##ffle
##ryn
countdown
detectives
psychiatrist
psychedelic
dinosaurs
blouse
##get
choi
vowed
##oz
randomly
##pol
49ers
scrub
blanche
bruins
dusseldorf
##using
unwanted
##ums
212
dominique
elevations
headlights
om
laguna
##oga
1750
famously
ignorance
shrewsbury
##aine
ajax
breuning
che
confederacy
greco
overhaul
##screen
paz
skirts
disagreement
cruelty
jagged
phoebe
shifter
hovered
viruses
##wes
mandy
##lined
##gc
landlord
squirrel
dashed
##
ornamental
gag
wally
grange
literal
spurs
undisclosed
proceeding
yin
##text
billie
orphan
spanned
humidity
indy
weighted
presentations
explosions
lucian
##tary
vaughn
hindus
##anga
##hell
psycho
171
daytona
protects
efficiently
rematch
sly
tandem
##oya
rebranded
impaired
hee
metropolis
peach
godfrey
diaspora
ethnicity
prosperous
gleaming
dar
grossing
playback
##rden
stripe
pistols
##tain
births
labelled
##cating
172
rudy
alba
##onne
aquarium
hostility
##gb
##tase
shudder
sumatra
hardest
lakers
consonant
creeping
demos
homicide
capsule
zeke
liberties
expulsion
pueblo
##comb
trait
transporting
##ddin
##neck
##yna
depart
gregg
mold
ledge
hangar
oldham
playboy
termination
analysts
gmbh
romero
##itic
insist
cradle
filthy
brightness
slash
shootout
deposed
bordering
##truct
isis
microwave
tumbled
sheltered
cathy
werewolves
messy
andersen
convex
clapped
clinched
satire
wasting
edo
vc
rufus
##jak
mont
##etti
poznan
##keeping
restructuring
transverse
##rland
azerbaijani
slovene
gestures
roommate
choking
shear
##quist
vanguard
oblivious
##hiro
disagreed
baptism
##lich
coliseum
##aceae
salvage
societe
cory
locke
relocation
relying
versailles
ahl
swelling
##elo
cheerful
##word
##edes
gin
sarajevo
obstacle
diverted
##nac
messed
thoroughbred
fluttered
utrecht
chewed
acquaintance
assassins
dispatch
mirza
##wart
nike
salzburg
swell
yen
##gee
idle
ligue
samson
##nds
##igh
playful
spawned
##cise
tease
##case
burgundy
##bot
stirring
skeptical
interceptions
marathi
##dies
bedrooms
aroused
pinch
##lik
preferences
tattoos
buster
digitally
projecting
rust
##ital
kitten
priorities
addison
pseudo
##guard
dusk
icons
sermon
##psis
##iba
bt
##lift
##xt
ju
truce
rink
##dah
##wy
defects
psychiatry
offences
calculate
glucose
##iful
##rized
##unda
francaise
##hari
richest
warwickshire
carly
1763
purity
redemption
lending
##cious
muse
bruises
cerebral
aero
carving
##name
preface
terminology
invade
monty
##int
anarchist
blurred
##iled
rossi
treats
guts
shu
foothills
ballads
undertaking
premise
cecilia
affiliates
blasted
conditional
wilder
minors
drone
rudolph
buffy
swallowing
horton
attested
##hop
rutherford
howell
primetime
livery
penal
##bis
minimize
hydro
wrecked
wrought
palazzo
##gling
cans
vernacular
friedman
nobleman
shale
walnut
danielle
##ection
##tley
sears
##kumar
chords
lend
flipping
streamed
por
dracula
gallons
sacrifices
gamble
orphanage
##iman
mckenzie
##gible
boxers
daly
##balls
##
208
##ific
##rative
##iq
exploited
slated
##uity
circling
hillary
pinched
goldberg
provost
campaigning
lim
piles
ironically
jong
mohan
successors
usaf
##tem
##ught
autobiographical
haute
preserves
##ending
acquitted
comparisons
203
hydroelectric
gangs
cypriot
torpedoes
rushes
chrome
derive
bumps
instability
fiat
pets
##mbe
silas
dye
reckless
settler
##itation
info
heats
##writing
176
canonical
maltese
fins
mushroom
stacy
aspen
avid
##kur
##loading
vickers
gaston
hillside
statutes
wilde
gail
kung
sabine
comfortably
motorcycles
##rgo
169
pneumonia
fetch
##sonic
axel
faintly
parallels
##oop
mclaren
spouse
compton
interdisciplinary
miner
##eni
181
clamped
##chal
##llah
separates
versa
##mler
scarborough
labrador
##lity
##osing
rutgers
hurdles
como
166
burt
divers
##100
wichita
cade
coincided
##erson
bruised
mla
##pper
vineyard
##ili
##brush
notch
mentioning
jase
hearted
kits
doe
##acle
pomerania
##ady
ronan
seizure
pavel
problematic
##zaki
domenico
##ulin
catering
penelope
dependence
parental
emilio
ministerial
atkinson
##bolic
clarkson
chargers
colby
grill
peeked
arises
summon
##aged
fools
##grapher
faculties
qaeda
##vial
garner
refurbished
##hwa
geelong
disasters
nudged
bs
shareholder
lori
algae
reinstated
rot
##ades
##nous
invites
stainless
183
inclusive
##itude
diocesan
til
##icz
denomination
##xa
benton
floral
registers
##ider
##erman
##kell
absurd
brunei
guangzhou
hitter
retaliation
##uled
##eve
blanc
nh
consistency
contamination
##eres
##rner
dire
palermo
broadcasters
diaries
inspire
vols
brewer
tightening
ky
mixtape
hormone
##tok
stokes
##color
##dly
##ssi
pg
##ometer
##lington
sanitation
##tility
intercontinental
apps
##adt

cylinders
economies
favourable
unison
croix
gertrude
odyssey
vanity
dangling
##logists
upgrades
dice
middleweight
practitioner
##ight
206
henrik
parlor
orion
angered
lac
python
blurted
##rri
sensual
intends
swings
angled
##phs
husky
attain
peerage
precinct
textiles
cheltenham
shuffled
dai
confess
tasting
bhutan
##riation
tyrone
segregation
abrupt
ruiz
##rish
smirked
blackwell
confidential
browning
amounted
##put
vase
scarce
fabulous
raided
staple
guyana
unemployed
glider
shay
##tow
carmine
troll
intervene
squash
superstar
##uce
cylindrical
len
roadway
researched
handy
##rium
##jana
meta
lao
declares
##rring
##tadt
##elin
##kova
willem
shrubs
napoleonic
realms
skater
qi
volkswagen
##
tad
hara
archaeologist
awkwardly
eerie
##kind
wiley
##heimer
##24
titus
organizers
cfl
crusaders
lama
usb
vent
enraged
thankful
occupants
maximilian
##gaard
possessing
textbooks
##oran
collaborator
quaker
##ulo
avalanche
mono
silky
straits
isaiah
mustang
surged
resolutions
potomac
descend
cl
kilograms
plato
strains
saturdays
##olin
bernstein
##ype
holstein
ponytail
##watch
belize
conversely
heroine
perpetual
##ylus
charcoal
piedmont
glee
negotiating
backdrop
prologue
##jah
##mmy
pasadena
climbs
ramos
sunni
##holm
##tner
##tri
anand
deficiency
hertfordshire
stout
##avi
aperture
orioles
##irs
doncaster
intrigued
bombed
coating
otis
##mat
cocktail
##jit
##eto
amir
arousal
sar
##proof
##act
##ories
dixie
pots
##bow
whereabouts
159
##fted
drains
bullying
cottages
scripture
coherent
fore
poe
appetite
##uration
sampled
##ators
##dp
derrick
rotor
jays
peacock
installment
##rro
advisors
##coming
rodeo
scotch
##mot
##db
##fen
##vant
ensued
rodrigo
dictatorship
martyrs
twenties
##
towed
incidence
marta
rainforest
sai
scaled
##cles
oceanic
qualifiers
symphonic
mcbride
dislike
generalized
aubrey
colonization
##iation
##lion
##ssing
disliked
lublin
salesman
##ulates
spherical
whatsoever
sweating
avalon
contention
punt
severity
alderman
atari
##dina
##grant
##rop
scarf
seville
vertices
annexation
fairfield
fascination
inspiring
launches
palatinate
regretted
##rca
feral
##iom
elk
nap
olsen
reddy
yong
##leader
##iae
garment
transports
feng
gracie
outrage
viceroy
insides
##esis
breakup
grady
organizer
softer
grimaced
222
murals
galicia
arranging
vectors
##rsten
bas
##sb
##cens
sloan
##eka
bitten
ara
fender
nausea
bumped
kris
banquet
comrades
detector
persisted
##llan
adjustment
endowed
cinemas
##shot
sellers
##uman
peek
epa
kindly
neglect
simpsons
talon
mausoleum
runaway
hangul
lookout
##cic
rewards
coughed
acquainted
chloride
##ald
quicker
accordion
neolithic
##qa
artemis
coefficient
lenny
pandora
tx
##xed
ecstasy
litter
segunda
chairperson
gemma
hiss
rumor
vow
nasal
antioch
compensate
patiently
transformers
##eded
judo
morrow
penis
posthumous
philips
bandits
husbands
denote
flaming
##any
##phones
langley
yorker
1760
walters
##uo
##kle
gubernatorial
fatty
samsung
leroy
outlaw
##nine
unpublished
poole
jakob
##
##
crete
distorted
superiority
##dhi
intercept
crust
mig
claus
crashes
positioning
188
stallion
301
frontal
armistice
##estinal
elton
aj
encompassing
camel
commemorated
malaria
woodward
calf
cigar
penetrate
##oso
willard
##rno
##uche
illustrate
amusing
convergence
noteworthy
##lma
##rva
journeys
realise
manfred
##sable
410
##vocation
hearings
fiance
##posed
educators
provoked
adjusting
##cturing
modular
stockton
paterson
vlad
rejects
electors
selena
maureen
##tres
uber
##rce
swirled
##num
proportions
nanny
pawn
naturalist
parma
apostles
awoke
ethel
wen
##bey
monsoon
overview
##inating
mccain
rendition
risky
adorned
##ih
equestrian
germain
nj
conspicuous
confirming
##yoshi
shivering
##imeter
milestone
rumours
flinched
bounds
smacked
token
##bei
lectured
automobiles
##shore
impacted
##iable
nouns
nero
##leaf
ismail
prostitute
trams
##lace
bridget
sud
stimulus
impressions
reins
revolves
##oud
##gned
giro
honeymoon
##swell
criterion
##sms
##uil
libyan
prefers
##osition
211
preview
sucks
accusation
bursts
metaphor
diffusion
tolerate
faye
betting
cinematographer
liturgical
specials
bitterly
humboldt
##ckle
flux
rattled
##itzer
archaeologists
odor
authorised
marshes
discretion
##
alarmed
archaic
inverse
##leton
explorers
##pine
drummond
tsunami
woodlands
##minate
##tland
booklet
insanity
owning
insert
crafted
calculus
##tore
receivers
##bt
stung
##eca
##nched
prevailing
travellers
eyeing
lila
graphs
##borne
178
julien
##won
morale
adaptive
therapist
erica
cw
libertarian
bowman
pitches
vita
##ional
crook
##ads
##entation
caledonia
mutiny
##sible
1840s
automation
##
flock
##pia
ironic
pathology
##imus
remarried
##22
joker
withstand
energies
##att
shropshire
hostages
madeleine
tentatively
conflicting
mateo
recipes
euros
ol
mercenaries
nico
##ndon
albuquerque
augmented
mythical
bel
freud
##child
cough
##lica
365
freddy
lillian
genetically
nuremberg
calder
209
bonn
outdoors
paste
suns
urgency
vin
restraint
tyson
##cera
##selle
barrage
bethlehem
kahn
##par
mounts
nippon
barony
happier
ryu
makeshift
sheldon
blushed
castillo
barking
listener
taped
bethel
fluent
headlines
pornography
rum
disclosure
sighing
mace
doubling
gunther
manly
##plex
rt
interventions
physiological
forwards
emerges
##tooth
##gny
compliment
rib
recession
visibly
barge
faults
connector
exquisite
prefect
##rlin
patio
##cured
elevators
brandt
italics
pena
173
wasp
satin
ea
botswana
graceful
respectable
##jima
##rter
##oic
franciscan
generates
##dl
alfredo
disgusting
##olate
##iously
sherwood
warns
cod
promo
cheryl
sino
##
##escu
twitch
##zhi
brownish
thom
ortiz
##dron
densely
##beat
carmel
reinforce
##bana
187
anastasia
downhill
vertex
contaminated
remembrance
harmonic
homework
##sol
fiancee
gears
olds
angelica
loft
ramsay
quiz
colliery
sevens
##cape
autism
##hil
walkway
##boats
ruben
abnormal
ounce
khmer
##bbe
zachary
bedside
morphology
punching
##olar
sparrow
convinces
##35
hewitt
queer
remastered
rods
mabel
solemn
notified
lyricist
symmetric
##xide
174
encore
passports
wildcats
##uni
baja
##pac
mildly
##ease
bleed
commodity
mounds
glossy
orchestras
##omo
damian
prelude
ambitions
##vet
awhile
remotely
##aud
asserts
imply
##iques
distinctly
modelling
remedy
##dded
windshield
dani
xiao
##endra
audible
powerplant
1300
invalid
elemental
acquisitions
##hala
immaculate
libby
plata
smuggling
ventilation
denoted
minh
##morphism
430
differed
dion
kelley
lore
mocking
sabbath
spikes
hygiene
drown
runoff
stylized
tally
liberated
aux
interpreter
righteous
aba
siren
reaper
pearce
millie
##cier
##yra
gaius
##iso
captures
##ttering
dorm
claudio
##sic
benches
knighted
blackness
##ored
discount
fumble
oxidation
routed
##
novak
perpendicular
spoiled
fracture
splits
##urt
pads
topology
##cats
axes
fortunate
offenders
protestants
esteem
221
broadband
convened
frankly
hound
prototypes
isil
facilitated
keel
##sher
sahara
awaited
bubba
orb
prosecutors
186
hem
520
##xing
relaxing
remnant
romney
sorted
slalom
stefano
ulrich
##active
exemption
folder
pauses
foliage
hitchcock
epithet
204
criticisms
##aca
ballistic
brody
hinduism
chaotic
youths
equals
##pala
pts
thicker
analogous
capitalist
improvised
overseeing
sinatra
ascended
beverage
##tl
straightforward
##kon
curran
##west
bois
325
induce
surveying
emperors
sax
unpopular
##kk
cartoonist
fused
##mble
unto
##yuki
localities
##cko
##ln
darlington
slain
academie
lobbying
sediment
puzzles
##grass
defiance
dickens
manifest
tongues
alumnus
arbor
coincide
184
appalachian
mustafa
examiner
cabaret
traumatic
yves
bracelet
draining
heroin
magnum
baths
odessa
consonants
mitsubishi
##gua
kellan
vaudeville
##fr
joked
null
straps
probation
##aw
ceded
interfaces
##pas
##zawa
blinding
viet
224
rothschild
museo
640
huddersfield
##vr
tactic
##storm
brackets
dazed
incorrectly
##vu
reg
glazed
fearful
manifold
benefited
irony
##sun
stumbling
##rte
willingness
balkans
mei
wraps
##aba
injected
##lea
gu
syed
harmless
##hammer
bray
takeoff
poppy
timor
cardboard
astronaut
purdue
weeping
southbound
cursing
stalls
diagonal
##neer
lamar
bryce
comte
weekdays
harrington
##uba
negatively
##see
lays
grouping
##cken
##henko
affirmed
halle
modernist
##lai
hodges
smelling
aristocratic
baptized
dismiss
justification
oilers
##now
coupling
qin
snack
healer
##qing
gardener
layla
battled
formulated
stephenson
gravitational
##gill
##jun
1768
granny
coordinating
suites
##cd
##ioned
monarchs
##cote
##hips
sep
blended
apr
barrister
deposition
fia
mina
policemen
paranoid
##pressed
churchyard
covert
crumpled
creep
abandoning
tr
transmit
conceal
barr
understands
readiness
spire
##cology
##enia
##erry
610
startling
unlock
vida
bowled
slots
##nat
##islav
spaced
trusting
admire
rig
##ink
slack
##70
mv
207
casualty
##wei
classmates
##odes
##rar
##rked
amherst
furnished
evolve
foundry
menace
mead
##lein
flu
wesleyan
##kled
monterey
webber
##vos
wil
##mith
##
bartholomew
justices
restrained
##cke
amenities
191
mediated
sewage
trenches
ml
mainz
##thus
1800s
##cula
##inski
caine
bonding
213
converts
spheres
superseded
marianne
crypt
sweaty
ensign
historia
##br
spruce
##post
##ask
forks
thoughtfully
yukon
pamphlet
ames
##uter
karma
##yya
bryn
negotiation
sighs
incapable
##mbre
##ntial
actresses
taft
##mill
luce
prevailed
##amine
1773
motionless
envoy
testify
investing
sculpted
instructors
provence
kali
cullen
horseback
##while
goodwin
##jos
gaa
norte
##ldon
modify
wavelength
abd
214
skinned
sprinter
forecast
scheduling
marries
squared
tentative
##chman
boer
##isch
bolts
swap
fisherman
assyrian
impatiently
guthrie
martins
murdoch
194
tanya
nicely
dolly
lacy
med
##45
syn
decks
fashionable
millionaire
##ust
surfing
##ml
##ision
heaved
tammy
consulate
attendees
routinely
197
fuse
saxophonist
backseat
malaya
##lord
scowl
tau
##ishly
193
sighted
steaming
##rks
303
911
##holes
##hong
ching
##wife
bless
conserved
jurassic
stacey
unix
zion
chunk
rigorous
blaine
198
peabody
slayer
dismay
brewers
nz
##jer
det
##glia
glover
postwar
int
penetration
sylvester
imitation
vertically
airlift
heiress
knoxville
viva
##uin
390
macon
##rim
##fighter
##gonal
janice
##orescence
##wari
marius
belongings
leicestershire
196
blanco
inverted
preseason
sanity
sobbing
##due
##elt
##dled
collingwood
regeneration
flickering
shortest
##mount
##osi
feminism
##lat
sherlock
cabinets
fumbled
northbound
precedent
snaps
##mme
researching
##akes
guillaume
insights
manipulated
vapor
neighbour
sap
gangster
frey
f1
stalking
scarcely
callie
barnett
tendencies
audi
doomed
assessing
slung
panchayat
ambiguous
bartlett
##etto
distributing
violating
wolverhampton
##hetic
swami
histoire
##urus
liable
pounder
groin
hussain
larsen
popping
surprises
##atter
vie
curt
##station
mute
relocate
musicals
authorization
richter
##sef
immortality
tna
bombings
##press
deteriorated
yiddish
##acious
robbed
colchester
cs
pmid
ao
verified
balancing
apostle
swayed
recognizable
oxfordshire
retention
nottinghamshire
contender
judd
invitational
shrimp
uhf
##icient
cleaner
longitudinal
tanker
##mur
acronym
broker
koppen
sundance
suppliers
##gil
4000
clipped
fuels
petite
##anne
landslide
helene
diversion
populous
landowners
auspices
melville
quantitative
##xes
ferries
nicky
##llus
doo
haunting
roche
carver
downed
unavailable
##pathy
approximation
hiroshima
##hue
garfield
valle
comparatively
keyboardist
traveler
##eit
congestion
calculating
subsidiaries
##bate
serb
modernization
fairies
deepened
ville
averages
##lore
inflammatory
tonga
##itch
co
squads
##hea
gigantic
serum
enjoyment
retailer
verona
35th
cis
##phobic
magna
technicians
##vati
arithmetic
##sport
levin
##dation
amtrak
chow
sienna
##eyer
backstage
entrepreneurship
##otic
learnt
tao
##udy
worcestershire
formulation
baggage
hesitant
bali
sabotage
##kari
barren
enhancing
murmur
pl
freshly
putnam
syntax
aces
medicines
resentment
bandwidth
##sier
grins
chili
guido
##sei
framing
implying
gareth
lissa
genevieve
pertaining
admissions
geo
thorpe
proliferation
sato
bela
analyzing
parting
##gor
awakened
##isman
huddled
secrecy
##kling
hush
gentry
540
dungeons
##ego
coasts
##utz
sacrificed
##chule
landowner
mutually
prevalence
programmer
adolescent
disrupted
seaside
gee
trusts
vamp
georgie
##nesian
##iol
schedules
sindh
##market
etched
hm
sparse
bey
beaux
scratching
gliding
unidentified
216
collaborating
gems
jesuits
oro
accumulation
shaping
mbe
anal
##xin
231
enthusiasts
newscast
##egan
janata
dewey
parkinson
179
ankara
biennial
towering
dd
inconsistent
950
##chet
thriving
terminate
cabins
furiously
eats
advocating
donkey
marley
muster
phyllis
leiden
##user
grassland
glittering
iucn
loneliness
217
memorandum
armenians
##ddle
popularized
rhodesia
60s
lame
##illon
sans
bikini
header
orbits
##xx
##finger
##ulator
sharif
spines
biotechnology
strolled
naughty
yates
##wire
fremantle
milo
##mour
abducted
removes
##atin
humming
wonderland
##chrome
##ester
hume
pivotal
##rates
armand
grams
believers
elector
rte
apron
bis
scraped
##yria
endorsement
initials
##llation
eps
dotted
hints
buzzing
emigration
nearer
##tom
indicators
##ulu
coarse
neutron
protectorate
##uze
directional
exploits
pains
loire
1830s
proponents
guggenheim
rabbits
ritchie
305
hectare
inputs
hutton
##raz
verify
##ako
boilers
longitude
##lev
skeletal
yer
emilia
citrus
compromised
##gau
pokemon
prescription
paragraph
eduard
cadillac
attire
categorized
kenyan
weddings
charley
##bourg
entertain
monmouth
##lles
nutrients
davey
mesh
incentive
practised
ecosystems
kemp
subdued
overheard
##rya
bodily
maxim
##nius
apprenticeship
ursula
##fight
lodged
rug
silesian
unconstitutional
patel
inspected
coyote
unbeaten
##hak
34th
disruption
convict
parcel
##cl
##nham
collier
implicated
mallory
##iac
##lab
susannah
winkler
##rber
shia
phelps
sediments
graphical
robotic
##sner
adulthood
mart
smoked
##isto
kathryn
clarified
##aran
divides
convictions
oppression
pausing
burying
##mt
federico
mathias
eileen
##tana
kite
hunched
##acies
189
##atz
disadvantage
liza
kinetic
greedy
paradox
yokohama
dowager
trunks
ventured
##gement
gupta
vilnius
olaf
##thest
crimean
hopper
##ej
progressively
arturo
mouthed
arrondissement
##fusion
rubin
simulcast
oceania
##orum
##stra
##rred
busiest
intensely
navigator
cary
##vine
##hini
##bies
fife
rowe
rowland
posing
insurgents
shafts
lawsuits
activate
conor
inward
culturally
garlic
265
##eering
eclectic
##hui
##kee
##nl
furrowed
vargas
meteorological
rendezvous
##aus
culinary
commencement
##dition
quota
##notes
mommy
salaries
overlapping
mule
##iology
##mology
sums
wentworth
##isk
##zione
mainline
subgroup
##illy
hack
plaintiff
verdi
bulb
differentiation
engagements
multinational
supplemented
bertrand
caller
regis
##naire
##sler
##arts
##imated
blossom
propagation
kilometer
viaduct
vineyards
##uate
beckett
optimization
golfer
songwriters
seminal
semitic
thud
volatile
evolving
ridley
##wley
trivial
distributions
scandinavia
jiang
##ject
wrestled
insistence
##dio
emphasizes
napkin
##ods
adjunct
rhyme
##ricted
##eti
hopeless
surrounds
tremble
32nd
smoky
##ntly
oils
medicinal
padded
steer
wilkes
219
255
concessions
hue
uniquely
blinded
landon
yahoo
##lane
hendrix
commemorating
dex
specify
chicks
##ggio
intercity
1400
morley
##torm
highlighting
##oting
pang
oblique
stalled
##liner
flirting
newborn
1769
bishopric
shaved
232
currie
##ush
dharma
spartan
##ooped
favorites
smug
novella
sirens
abusive
creations
espana
##lage
paradigm
semiconductor
sheen
##rdo
##yen
##zak
nrl
renew
##pose
##tur
adjutant
marches
norma
##enity
ineffective
weimar
grunt
##gat
lordship
plotting
expenditure
infringement
lbs
refrain
av
mimi
mistakenly
postmaster
1771
##bara
ras
motorsports
tito
199
subjective
##zza
bully
stew
##kaya
prescott
1a
##raphic
##zam
bids
styling
paranormal
reeve
sneaking
exploding
katz
akbar
migrant
syllables
indefinitely
##ogical
destroys
replaces
applause
##phine
pest
##fide
218
articulated
bertie
##thing
##cars
##ptic
courtroom
crowley
aesthetics
cummings
tehsil
hormones
titanic
dangerously
##ibe
stadion
jaenelle
auguste
ciudad
##chu
mysore
partisans
##sio
lucan
philipp
##aly
debating
henley
interiors
##rano
##tious
homecoming
beyonce
usher
henrietta
prepares
weeds
##oman
ely
plucked
##pire
##dable
luxurious
##aq
artifact
password
pasture
juno
maddy
minsk
##dder
##ologies
##rone
assessments
martian
royalist
1765
examines
##mani
##rge
nino
223
parry
scooped
relativity
##eli
##uting
##cao
congregational
noisy
traverse
##agawa
strikeouts
nickelodeon
obituary
transylvania
binds
depictions
polk
trolley
##yed
##lard
breeders
##under
dryly
hokkaido
1762
strengths
stacks
bonaparte
connectivity
neared
prostitutes
stamped
anaheim
gutierrez
sinai
##zzling
bram
fresno
madhya
##86
proton
##lena
##llum
##phon
reelected
wanda
##anus
##lb
ample
distinguishing
##yler
grasping
sermons
tomato
bland
stimulation
avenues
##eux
spreads
scarlett
fern
pentagon
assert
baird
chesapeake
ir
calmed
distortion
fatalities
##olis
correctional
pricing
##astic
##gina
prom
dammit
ying
collaborate
##chia
welterweight
33rd
pointer
substitution
bonded
umpire
communicating
multitude
paddle
##obe
federally
intimacy
##insky
betray
ssr
##lett
##lean
##lves
##therapy
airbus
##tery
functioned
ud
bearer
biomedical
netflix
##hire
##nca
condom
brink
ik
##nical
macy
##bet
flap
gma
experimented
jelly
lavender
##icles
##ulia
munro
##mian
##tial
rye
##rle
60th
gigs
hottest
rotated
predictions
fuji
bu
##erence
##omi
barangay
##fulness
##sas
clocks
##rwood
##liness
cereal
roe
wight
decker
uttered
babu
onion
xml
forcibly
##df
petra
sarcasm
hartley
peeled
storytelling
##42
##xley
##ysis
##ffa
fibre
kiel
auditor
fig
harald
greenville
##berries
geographically
nell
quartz
##athic
cemeteries
##lr
crossings
nah
holloway
reptiles
chun
sichuan
snowy
660
corrections
##ivo
zheng
ambassadors
blacksmith
fielded
fluids
hardcover
turnover
medications
melvin
academies
##erton
ro
roach
absorbing
spaniards
colton
##founded
outsider
espionage
kelsey
245
edible
##ulf
dora
establishes
##sham
##tries
contracting
##tania
cinematic
costello
nesting
##uron
connolly
duff
##nology
mma
##mata
fergus
sexes
gi
optics
spectator
woodstock
banning
##hee
##fle
differentiate
outfielder
refinery
226
312
gerhard
horde
lair
drastically
##udi
landfall
##cheng
motorsport
odi
##achi
predominant
quay
skins
##ental
edna
harshly
complementary
murdering
##aves
wreckage
##90
ono
outstretched
lennox
munitions
galen
reconcile
470
scalp
bicycles
gillespie
questionable
rosenberg
guillermo
hostel
jarvis
kabul
volvo
opium
yd
##twined
abuses
decca
outpost
##cino
sensible
neutrality
##64
ponce
anchorage
atkins
turrets
inadvertently
disagree
libre
vodka
reassuring
weighs
##yal
glide
jumper
ceilings
repertory
outs
stain
##bial
envy
##ucible
smashing
heightened
policing
hyun
mixes
lai
prima
##ples
celeste
##bina
lucrative
intervened
kc
manually
##rned
stature
staffed
bun
bastards
nairobi
priced
##auer
thatcher
##kia
tripped
comune
##ogan
##pled
brasil
incentives
emanuel
hereford
musica
##kim
benedictine
biennale
##lani
eureka
gardiner
rb
knocks
sha
##ael
##elled
##onate
efficacy
ventura
masonic
sanford
maize
leverage
##feit
capacities
santana
##aur
novelty
vanilla
##cter
##tour
benin
##oir
##rain
neptune
drafting
tallinn
##cable
humiliation
##boarding
schleswig
fabian
bernardo
liturgy
spectacle
sweeney
pont
routledge
##tment
cosmos
ut
hilt
sleek
universally
##eville
##gawa
typed
##dry
favors
allegheny
glaciers
##rly
recalling
aziz
##log
parasite
requiem
auf
##berto
##llin
illumination
##breaker
##issa
festivities
bows
govern
vibe
vp
333
sprawled
larson
pilgrim
bwf
leaping
##rts
##ssel
alexei
greyhound
hoarse
##dler
##oration
seneca
##cule
gaping
##ulously
##pura
cinnamon
##gens
##rricular
craven
fantasies
houghton
engined
reigned
dictator
supervising
##oris
bogota
commentaries
unnatural
fingernails
spirituality
tighten
##tm
canadiens
protesting
intentional
cheers
sparta
##ytic
##iere
##zine
widen
belgarath
controllers
dodd
iaaf
navarre
##ication
defect
squire
steiner
whisky
##mins
560
inevitably
tome
##gold
chew
##uid
##lid
elastic
##aby
streaked
alliances
jailed
regal
##ined
##phy
czechoslovak
narration
absently
##uld
bluegrass
guangdong
quran
criticizing
hose
hari
##liest
##owa
skier
streaks
deploy
##lom
raft
bose
dialed
huff
##eira
haifa
simplest
bursting
endings
ib
sultanate
##titled
franks
whitman
ensures
sven
##ggs
collaborators
forster
organising
ui
banished
napier
injustice
teller
layered
thump
##otti
roc
battleships
evidenced
fugitive
sadie
robotics
##roud
equatorial
geologist
##iza
yielding
##bron
##sr
internationale
mecca
##diment
sbs
skyline
toad
uploaded
reflective
undrafted
lal
leafs
bayern
##dai
lakshmi
shortlisted
##stick
##wicz
camouflage
donate
af
christi
lau
##acio
disclosed
nemesis
1761
assemble
straining
northamptonshire
tal
##asi
bernardino
premature
heidi
42nd
coefficients
galactic
reproduce
buzzed
sensations
zionist
monsieur
myrtle
##eme
archery
strangled
musically
viewpoint
antiquities
bei
trailers
seahawks
cured
pee
preferring
tasmanian
lange
sul
##mail
##working
colder
overland
lucivar
massey
gatherings
haitian
##smith
disapproval
flaws
##cco
##enbach
1766
npr
##icular
boroughs
creole
forums
techno
1755
dent
abdominal
streetcar
##eson
##stream
procurement
gemini
predictable
##tya
acheron
christoph
feeder
fronts
vendor
bernhard
jammu
tumors
slang
##uber
goaltender
twists
curving
manson
vuelta
mer
peanut
confessions
pouch
unpredictable
allowance
theodor
vascular
##factory
bala
authenticity
metabolic
coughing
nanjing
##cea
pembroke
##bard
splendid
36th
ff
hourly
##ahu
elmer
handel
##ivate
awarding
thrusting
dl
experimentation
##hesion
##46
caressed
entertained
steak
##rangle
biologist
orphans
baroness
oyster
stepfather
##dridge
mirage
reefs
speeding
##31
barons
1764
227
inhabit
preached
repealed
##tral
honoring
boogie
captives
administer
johanna
##imate
gel
suspiciously
1767
sobs
##dington
backbone
hayward
garry
##folding
##nesia
maxi
##oof
##ppe
ellison
galileo
##stand
crimea
frenzy
amour
bumper
matrices
natalia
baking
garth
palestinians
##grove
smack
conveyed
ensembles
gardening
##manship
##rup
##stituting
1640
harvesting
topography
jing
shifters
dormitory
##carriage
##lston
ist
skulls
##stadt
dolores
jewellery
sarawak
##wai
##zier
fences
christy
confinement
tumbling
credibility
fir
stench
##bria
##plication
##nged
##sam
virtues
##belt
marjorie
pba
##eem
##made
celebrates
schooner
agitated
barley
fulfilling
anthropologist
##pro
restrict
novi
regulating
##nent
padres
##rani
##hesive
loyola
tabitha
milky
olson
proprietor
crambidae
guarantees
intercollegiate
ljubljana
hilda
##sko
ignorant
hooded
##lts
sardinia
##lidae
##vation
frontman
privileged
witchcraft
##gp
jammed
laude
poking
##than
bracket
amazement
yunnan
##erus
maharaja
linnaeus
264
commissioning
milano
peacefully
##logies
akira
rani
regulator
##36
grasses
##rance
luzon
crows
compiler
gretchen
seaman
edouard
tab
buccaneers
ellington
hamlets
whig
socialists
##anto
directorial
easton
mythological
##kr
##vary
rhineland
semantic
taut
dune
inventions
succeeds
##iter
replication
branched
##pired
jul
prosecuted
kangaroo
penetrated
##avian
middlesbrough
doses
bleak
madam
predatory
relentless
##vili
reluctance
##vir
hailey
crore
silvery
1759
monstrous
swimmers
transmissions
hawthorn
informing
##eral
toilets
caracas
crouch
kb
##sett
295
cartel
hadley
##aling
alexia
yvonne
##biology
cinderella
eton
superb
blizzard
stabbing
industrialist
maximus
##gm
##orus
groves
maud
clade
oversized
comedic
##bella
rosen
nomadic
fulham
montane
beverages
galaxies
redundant
swarm
##rot
##folia
##llis
buckinghamshire
fen
bearings
bahadur
##rom
gilles
phased
dynamite
faber
benoit
vip
##ount
##wd
booking
fractured
tailored
anya
spices
westwood
cairns
auditions
inflammation
steamed
##rocity
##acion
##urne
skyla
thereof
watford
torment
archdeacon
transforms
lulu
demeanor
fucked
serge
##sor
mckenna
minas
entertainer
##icide
caress
originate
residue
##sty
1740
##ilised
##org
beech
##wana
subsidies
##ghton
emptied
gladstone
ru
firefighters
voodoo
##rcle
het
nightingale
tamara
edmond
ingredient
weaknesses
silhouette
285
compatibility
withdrawing
hampson
##mona
anguish
giggling
##mber
bookstore
##jiang
southernmost
tilting
##vance
bai
economical
rf
briefcase
dreadful
hinted
projections
shattering
totaling
##rogate
analogue
indicted
periodical
fullback
##dman
haynes
##tenberg
##ffs
##ishment
1745
thirst
stumble
penang
vigorous
##ddling
##kor
##lium
octave
##ove
##enstein
##inen
##ones
siberian
##uti
cbn
repeal
swaying
##vington
khalid
tanaka
unicorn
otago
plastered
lobe
riddle
##rella
perch
##ishing
croydon
filtered
graeme
tripoli
##ossa
crocodile
##chers
sufi
mined
##tung
inferno
lsu
##phi
swelled
utilizes
2
cale
periodicals
styx
hike
informally
coop
lund
##tidae
ala
hen
qui
transformations
disposed
sheath
chickens
##cade
fitzroy
sas
silesia
unacceptable
odisha
1650
sabrina
pe
spokane
ratios
athena
massage
shen
dilemma
##drum
##riz
##hul
corona
doubtful
niall
##pha
##bino
fines
cite
acknowledging
bangor
ballard
bathurst
##resh
huron
mustered
alzheimer
garments
kinase
tyre
warship
##cp
flashback
pulmonary
braun
cheat
kamal
cyclists
constructions
grenades
ndp
traveller
excuses
stomped
signalling
trimmed
futsal
mosques
relevance
##wine
wta
##23
##vah
##lter
hoc
##riding
optimistic
##s
deco
sim
interacting
rejecting
moniker
waterways
##ieri
##oku
mayors
gdansk
outnumbered
pearls
##ended
##hampton
fairs
totals
dominating
262
notions
stairway
compiling
pursed
commodities
grease
yeast
##jong
carthage
griffiths
residual
amc
contraction
laird
sapphire
##marine
##ivated
amalgamation
dissolve
inclination
lyle
packaged
altitudes
suez
canons
graded
lurched
narrowing
boasts
guise
wed
enrico
##ovsky
rower
scarred
bree
cub
iberian
protagonists
bargaining
proposing
trainers
voyages
vans
fishes
##aea
##ivist
##verance
encryption
artworks
kazan
sabre
cleopatra
hepburn
rotting
supremacy
mecklenburg
##brate
burrows
hazards
outgoing
flair
organizes
##ctions
scorpion
##usions
boo
234
chevalier
dunedin
slapping
##34
ineligible
pensions
##38
##omic
manufactures
emails
bismarck
238
weakening
blackish
ding
mcgee
quo
##rling
northernmost
xx
manpower
greed
sampson
clicking
##ange
##horpe
##inations
##roving
torre
##eptive
##moral
symbolism
38th
asshole
meritorious
outfits
splashed
biographies
sprung
astros
##tale
302
737
filly
raoul
nw
tokugawa
linden
clubhouse
##apa
tracts
romano
##pio
putin
tags
##note
chained
dickson
gunshot
moe
gunn
rashid
##tails
zipper
##bas
##nea
contrasted
##ply
##udes
plum
pharaoh
##pile
aw
comedies
ingrid
sandwiches
subdivisions
1100
mariana
nokia
kamen
hz
delaney
veto
herring
##words
possessive
outlines
##roup
siemens
stairwell
rc
gallantry
messiah
palais
yells
233
zeppelin
##dm
bolivar
##cede
smackdown
mckinley
##mora
##yt
muted
geologic
finely
unitary
avatar
hamas
maynard
rees
bog
contrasting
##rut
liv
chico
disposition
pixel
##erate
becca
dmitry
yeshiva
narratives
##lva
##ulton
mercenary
sharpe
tempered
navigate
stealth
amassed
keynes
##lini
untouched
##rrie
havoc
lithium
##fighting
abyss
graf
southward
wolverine
balloons
implements
ngos
transitions
##icum
ambushed
concacaf
dormant
economists
##dim
costing
csi
rana
universite
boulders
verity
##llon
collin
mellon
misses
cypress
fluorescent
lifeless
spence
##ulla
crewe
shepard
pak
revelations
##
jolly
gibbons
paw
##dro
##quel
freeing
##test
shack
fries
palatine
##51
##hiko
accompaniment
cruising
recycled
##aver
erwin
sorting
synthesizers
dyke
realities
sg
strides
enslaved
wetland
##ghan
competence
gunpowder
grassy
maroon
reactors
objection
##oms
carlson
gearbox
macintosh
radios
shelton
##sho
clergyman
prakash
254
mongols
trophies
oricon
228
stimuli
twenty20
cantonese
cortes
mirrored
##saurus
bhp
cristina
melancholy
##lating
enjoyable
nuevo
##wny
downfall
schumacher
##ind
banging
lausanne
rumbled
paramilitary
reflex
ax
amplitude
migratory
##gall
##ups
midi
barnard
lastly
sherry
##hp
##nall
keystone
##kra
carleton
slippery
##53
coloring
foe
socket
otter
##rgos
mats
##tose
consultants
bafta
bison
topping
##km
490
primal
abandonment
transplant
atoll
hideous
mort
pained
reproduced
tae
howling
##turn
unlawful
billionaire
hotter
poised
lansing
##chang
dinamo
retro
messing
nfc
domesday
##mina
blitz
timed
##athing
##kley
ascending
gesturing
##izations
signaled
tis
chinatown
mermaid
savanna
jameson
##aint
catalina
##pet
##hers
cochrane
cy
chatting
##kus
alerted
computation
mused
noelle
majestic
mohawk
campo
octagonal
##sant
##hend
241
aspiring
##mart
comprehend
iona
paralyzed
shimmering
swindon
rhone
##eley
reputed
configurations
pitchfork
agitation
francais
gillian
lipstick
##ilo
outsiders
pontifical
resisting
bitterness
sewer
rockies
##edd
##ucher
misleading
1756
exiting
galloway
##nging
risked
##heart
246
commemoration
schultz
##rka
integrating
##rsa
poses
shrieked
##weiler
guineas
gladys
jerking
owls
goldsmith
nightly
penetrating
##unced
lia
##33
ignited
betsy
##aring
##thorpe
follower
vigorously
##rave
coded
kiran
knit
zoology
tbilisi
##28
##bered
repository
govt
deciduous
dino
growling
##bba
enhancement
unleashed
chanting
pussy
biochemistry
##eric
kettle
repression
toxicity
nrhp
##arth
##kko
##bush
ernesto
commended
outspoken
242
mca
parchment
sms
kristen
##aton
bisexual
raked
glamour
navajo
a2
conditioned
showcased
##hma
spacious
youthful
##esa
usl
appliances
junta
brest
layne
conglomerate
enchanted
chao
loosened
picasso
circulating
inspect
montevideo
##centric
##kti
piazza
spurred
##aith
bari
freedoms
poultry
stamford
lieu
##ect
indigo
sarcastic
bahia
stump
attach
dvds
frankenstein
lille
approx
scriptures
pollen
##script
nmi
overseen
##ivism
tides
proponent
newmarket
inherit
milling
##erland
centralized
##rou
distributors
credentials
drawers
abbreviation
##lco
##xon
downing
uncomfortably
ripe
##oes
erase
franchises
##ever
populace
##bery
##khar
decomposition
pleas
##tet
daryl
sabah
##stle
##wide
fearless
genie
lesions
annette
##ogist
oboe
appendix
nair
dripped
petitioned
maclean
mosquito
parrot
rpg
hampered
1648
operatic
reservoirs
##tham
irrelevant
jolt
summarized
##fp
medallion
##taff
##
clawed
harlow
narrower
goddard
marcia
bodied
fremont
suarez
altering
tempest
mussolini
porn
##isms
sweetly
oversees
walkers
solitude
grimly
shrines
hk
ich
supervisors
hostess
dietrich
legitimacy
brushes
expressive
##yp
dissipated
##rse
localized
systemic
##nikov
gettysburg
##js
##uaries
dialogues
muttering
251
housekeeper
sicilian
discouraged
##frey
beamed
kaladin
halftime
kidnap
##amo
##llet
1754
synonymous
depleted
instituto
insulin
reprised
##opsis
clashed
##ctric
interrupting
radcliffe
insisting
medici
1715
ejected
playfully
turbulent
##47
starvation
##rini
shipment
rebellious
petersen
verification
merits
##rified
cakes
##charged
1757
milford
shortages
spying
fidelity
##aker
emitted
storylines
harvested
seismic
##iform
cheung
kilda
theoretically
barbie
lynx
##rgy
##tius
goblin
mata
poisonous
##nburg
reactive
residues
obedience
##
conjecture
##rac
401
hating
sixties
kicker
moaning
motown
##bha
emancipation
neoclassical
##hering
consoles
ebert
professorship
##tures
sustaining
assaults
obeyed
affluent
incurred
tornadoes
##eber
##zow
emphasizing
highlanders
cheated
helmets
##ctus
internship
terence
bony
executions
legislators
berries
peninsular
tinged
##aco
1689
amplifier
corvette
ribbons
lavish
pennant
##lander
worthless
##chfield
##forms
mariano
pyrenees
expenditures
##icides
chesterfield
mandir
tailor
39th
sergey
nestled
willed
aristocracy
devotees
goodnight
raaf
rumored
weaponry
remy
appropriations
harcourt
burr
riaa
##lence
limitation
unnoticed
guo
soaking
swamps
##tica
collapsing
tatiana
descriptive
brigham
psalm
##chment
maddox
##lization
patti
caliph
##aja
akron
injuring
serra
##ganj
basins
##sari
astonished
launcher
##church
hilary
wilkins
sewing
##sf
stinging
##fia
##ncia
underwood
startup
##ition
compilations
vibrations
embankment
jurist
##nity
bard
juventus
groundwater
kern
palaces
helium
boca
cramped
marissa
soto
##worm
jae
princely
##ggy
faso
bazaar
warmly
##voking
229
pairing
##lite
##grate
##nets
wien
freaked
ulysses
rebirth
##alia
##rent
mummy
guzman
jimenez
stilled
##nitz
trajectory
tha
woken
archival
professions
##pts
##pta
hilly
shadowy
shrink
##bolt
norwood
glued
migrate
stereotypes
devoid
##pheus
625
evacuate
horrors
infancy
gotham
knowles
optic
downloaded
sachs
kingsley
parramatta
darryl
mor
##onale
shady
commence
confesses
kan
##meter
##placed
marlborough
roundabout
regents
frigates
io
##imating
gothenburg
revoked
carvings
clockwise
convertible
intruder
##sche
banged
##ogo
vicky
bourgeois
##mony
dupont
footing
##gum
pd
##real
buckle
yun
penthouse
sane
720
serviced
stakeholders
neumann
bb
##eers
comb
##gam
catchment
pinning
rallies
typing
##elles
forefront
freiburg
sweetie
giacomo
widowed
goodwill
worshipped
aspirations
midday
##vat
fishery
##trick
bournemouth
turk
243
hearth
ethanol
guadalajara
murmurs
sl
##uge
afforded
scripted
##hta
wah
##jn
coroner
translucent
252
memorials
puck
progresses
clumsy
##race
315
candace
recounted
##27
##slin
##uve
filtering
##mac
howl
strata
heron
leveled
##ays
dubious
##oja
##
##wheel
citations
exhibiting
##laya
##mics
##pods
turkic
##lberg
injunction
##ennial
##mit
antibodies
##44
organise
##rigues
cardiovascular
cushion
inverness
##zquez
dia
cocoa
sibling
##tman
##roid
expanse
feasible
tunisian
algiers
##relli
rus
bloomberg
dso
westphalia
bro
tacoma
281
downloads
##ours
konrad
duran
##hdi
continuum
jett
compares
legislator
secession
##nable
##gues
##zuka
translating
reacher
##gley
##a
aleppo
##agi
tc
orchards
trapping
linguist
versatile
drumming
postage
calhoun
superiors
##mx
barefoot
leary
##cis
ignacio
alfa
kaplan
##rogen
bratislava
mori
##vot
disturb
haas
313
cartridges
gilmore
radiated
salford
tunic
hades
##ulsive
archeological
delilah
magistrates
auditioned
brewster
charters
empowerment
blogs
cappella
dynasties
iroquois
whipping
##krishna
raceway
truths
myra
weaken
judah
mcgregor
##horse
mic
refueling
37th
burnley
bosses
markus
premio
query
##gga
dunbar
##economic
darkest
lyndon
sealing
commendation
reappeared
##mun
addicted
ezio
slaughtered
satisfactory
shuffle
##eves
##thic
##uj
fortification
warrington
##otto
resurrected
fargo
mane
##utable
##lei
##space
foreword
ox
##aris
##vern
abrams
hua
##mento
sakura
##alo
uv
sentimental
##skaya
midfield
##eses
sturdy
scrolls
macleod
##kyu
entropy
##lance
mitochondrial
cicero
excelled
thinner
convoys
perceive
##oslav
##urable
systematically
grind
burkina
287
##tagram
ops
##aman
guantanamo
##cloth
##tite
forcefully
wavy
##jou
pointless
##linger
##tze
layton
portico
superficial
clerical
outlaws
##hism
burials
muir
##inn
creditors
hauling
rattle
##leg
calais
monde
archers
reclaimed
dwell
wexford
hellenic
falsely
remorse
##tek
dough
furnishings
##uttered
gabon
neurological
novice
##igraphy
contemplated
pulpit
nightstand
saratoga
##istan
documenting
pulsing
taluk
##firmed
busted
marital
##rien
disagreements
wasps
##yes
hodge
mcdonnell
mimic
fran
pendant
dhabi
musa
##nington
congratulations
argent
darrell
concussion
losers
regrets
thessaloniki
reversal
donaldson
hardwood
thence
achilles
ritter
##eran
demonic
jurgen
prophets
goethe
eki
classmate
buff
##cking
yank
irrational
##inging
perished
seductive
qur
sourced
##crat
##typic
mustard
ravine
barre
horizontally
characterization
phylogenetic
boise
##dit
##runner
##tower
brutally
intercourse
seduce
##bbing
fay
ferris
ogden
amar
nik
unarmed
##inator
evaluating
kyrgyzstan
sweetness
##lford
##oki
mccormick
meiji
notoriety
stimulate
disrupt
figuring
instructional
mcgrath
##zoo
groundbreaking
##lto
flinch
khorasan
agrarian
bengals
mixer
radiating
##sov
ingram
pitchers
nad
tariff
##cript
tata
##codes
##emi
##ungen
appellate
lehigh
##bled
##giri
brawl
duct
texans
##ciation
##ropolis
skipper
speculative
vomit
doctrines
stresses
253
davy
graders
whitehead
jozef
timely
cumulative
haryana
paints
appropriately
boon
cactus
##ales
##pid
dow
legions
##pit
perceptions
1730
picturesque
##yse
periphery
rune
wr
##aha
celtics
sentencing
whoa
##erin
confirms
variance
425
moines
mathews
spade
rave
m1
fronted
fx
blending
alleging
reared
##gl
237
##paper
grassroots
eroded
##free
##physical
directs
ordeal
##saw
accelerate
hacker
rooftop
##inia
lev
buys
cebu
devote
##lce
specialising
##ulsion
choreographed
repetition
warehouses
##ryl
paisley
tuscany
analogy
sorcerer
hash
huts
shards
descends
exclude
nix
chaplin
gaga
ito
vane
##drich
causeway
misconduct
limo
orchestrated
glands
jana
##kot
u2
##mple
##sons
branching
contrasts
scoop
longed
##virus
chattanooga
##75
syrup
cornerstone
##tized
##mind
##iaceae
careless
precedence
frescoes
##uet
chilled
consult
modelled
snatch
peat
##thermal
caucasian
humane
relaxation
spins
temperance
##lbert
occupations
lambda
hybrids
moons
mp3
##oese
247
rolf
societal
yerevan
ness
##ssler
befriended
mechanized
nominate
trough
boasted
cues
seater
##hom
bends
##tangle
conductors
emptiness
##lmer
eurasian
adriatic
tian
##cie
anxiously
lark
propellers
chichester
jock
ev
2a
##holding
credible
recounts
tori
loyalist
abduction
##hoot
##redo
nepali
##mite
ventral
tempting
##ango
##crats
steered
##wice
javelin
dipping
laborers
prentice
looming
titanium
##
badges
emir
tensor
##ntation
egyptians
rash
denies
hawthorne
lombard
showers
wehrmacht
dietary
trojan
##reus
welles
executing
horseshoe
lifeboat
##lak
elsa
infirmary
nearing
roberta
boyer
mutter
trillion
joanne
##fine
##oked
sinks
vortex
uruguayan
clasp
sirius
##block
accelerator
prohibit
sunken
byu
chronological
diplomats
ochreous
510
symmetrical
1644
maia
##tology
salts
reigns
atrocities
##
hess
bared
issn
##vyn
cater
saturated
##cycle
##isse
sable
voyager
dyer
yusuf
##inge
fountains
wolff
##39
##nni
engraving
rollins
atheist
ominous
##ault
herr
chariot
martina
strung
##fell
##farlane
horrific
sahib
gazes
saetan
erased
ptolemy
##olic
flushing
lauderdale
analytic
##ices
530
navarro
beak
gorilla
herrera
broom
guadalupe
raiding
sykes
311
bsc
deliveries
1720
invasions
carmichael
tajikistan
thematic
ecumenical
sentiments
onstage
##rians
##brand
##sume
catastrophic
flanks
molten
##arns
waller
aimee
terminating
##icing
alternately
##oche
nehru
printers
outraged
##eving
empires
template
banners
repetitive
za
##oise
vegetarian
##tell
guiana
opt
cavendish
lucknow
synthesized
##hani
##mada
finalized
##ctable
fictitious
mayoral
unreliable
##enham
embracing
peppers
rbis
##chio
##neo
inhibition
slashed
togo
orderly
embroidered
safari
salty
236
barron
benito
totaled
##dak
pubs
simulated
caden
devin
tolkien
momma
welding
sesame
##ept
gottingen
hardness
630
shaman
temeraire
620
adequately
pediatric
##kit
ck
assertion
radicals
composure
cadence
seafood
beaufort
lazarus
mani
warily
cunning
kurdistan
249
cantata
##kir
ares
##41
##clusive
nape
townland
geared
insulted
flutter
boating
violate
draper
dumping
malmo
##hh
##romatic
firearm
alta
bono
obscured
##clave
exceeds
panorama
unbelievable
##train
preschool
##essed
disconnected
installing
rescuing
secretaries
accessibility
##castle
##drive
##ifice
##film
bouts
slug
waterway
mindanao
##buro
##ratic
halves
##
calming
liter
maternity
adorable
bragg
electrification
mcc
##dote
roxy
schizophrenia
##body
munoz
kaye
whaling
239
mil
tingling
tolerant
##ago
unconventional
volcanoes
##finder
deportivo
##llie
robson
kaufman
neuroscience
wai
deportation
masovian
scraping
converse
##bh
hacking
bulge
##oun
administratively
yao
580
amp
mammoth
booster
claremont
hooper
nomenclature
pursuits
mclaughlin
melinda
##sul
catfish
barclay
substrates
taxa
zee
originals
kimberly
packets
padma
##ality
borrowing
ostensibly
solvent
##bri
##genesis
##mist
lukas
shreveport
veracruz
##
##lou
##wives
cheney
tt
anatolia
hobbs
##zyn
cyclic
radiant
alistair
greenish
siena
dat
independents
##bation
conform
pieter
hyper
applicant
bradshaw
spores
telangana
vinci
inexpensive
nuclei
322
jang
nme
soho
spd
##ign
cradled
receptionist
pow
##43
##rika
fascism
##ifer
experimenting
##ading
##iec
##region
345
jocelyn
maris
stair
nocturnal
toro
constabulary
elgin
##kker
msc
##giving
##schen
##rase
doherty
doping
sarcastically
batter
maneuvers
##cano
##apple
##gai
##git
intrinsic
##nst
##stor
1753
showtime
cafes
gasps
lviv
ushered
##thed
fours
restart
astonishment
transmitting
flyer
shrugs
##sau
intriguing
cones
dictated
mushrooms
medial
##kovsky
##elman
escorting
gaped
##26
godfather
##door
##sell
djs
recaptured
timetable
vila
1710
3a
aerodrome
mortals
scientology
##orne
angelina
mag
convection
unpaid
insertion
intermittent
lego
##nated
endeavor
kota
pereira
##lz
304
bwv
glamorgan
insults
agatha
fey
##cend
fleetwood
mahogany
protruding
steamship
zeta
##arty
mcguire
suspense
##sphere
advising
urges
##wala
hurriedly
meteor
gilded
inline
arroyo
stalker
##oge
excitedly
revered
##cure
earle
introductory
##break
##ilde
mutants
puff
pulses
reinforcement
##haling
curses
lizards
stalk
correlated
##fixed
fallout
macquarie
##unas
bearded
denton
heaving
802
##ocation
winery
assign
dortmund
##lkirk
everest
invariant
charismatic
susie
##elling
bled
lesley
telegram
sumner
bk
##ogen
##
wilcox
needy
colbert
duval
##iferous
##mbled
allotted
attends
imperative
##hita
replacements
hawker
##inda
insurgency
##zee
##eke
casts
##yla
680
ives
transitioned
##pack
##powering
authoritative
baylor
flex
cringed
plaintiffs
woodrow
##skie
drastic
ape
aroma
unfolded
commotion
nt
preoccupied
theta
routines
lasers
privatization
wand
domino
ek
clenching
nsa
strategically
showered
bile
handkerchief
pere
storing
christophe
insulting
316
nakamura
romani
asiatic
magdalena
palma
cruises
stripping
405
konstantin
soaring
##berman
colloquially
forerunner
havilland
incarcerated
parasites
sincerity
##utus
disks
plank
saigon
##ining
corbin
homo
ornaments
powerhouse
##tlement
chong
fastened
feasibility
idf
morphological
usable
##nish
##zuki
aqueduct
jaguars
keepers
##flies
aleksandr
faust
assigns
ewing
bacterium
hurled
tricky
hungarians
integers
wallis
321
yamaha
##isha
hushed
oblivion
aviator
evangelist
friars
##eller
monograph
ode
##nary
airplanes
labourers
charms
##nee
1661
hagen
tnt
rudder
fiesta
transcript
dorothea
ska
inhibitor
maccabi
retorted
raining
encompassed
clauses
menacing
1642
lineman
##gist
vamps
##ape
##dick
gloom
##rera
dealings
easing
seekers
##nut
##pment
helens
unmanned
##anu
##isson
basics
##amy
##ckman
adjustments
1688
brutality
horne
##zell
sui
##55
##mable
aggregator
##thal
rhino
##drick
##vira
counters
zoom
##01
##rting
mn
montenegrin
packard
##unciation
##
##kki
reclaim
scholastic
thugs
pulsed
##icia
syriac
quan
saddam
banda
kobe
blaming
buddies
dissent
##lusion
##usia
corbett
jaya
delle
erratic
lexie
##hesis
435
amiga
hermes
##pressing
##leen
chapels
gospels
jamal
##uating
compute
revolving
warp
##sso
##thes
armory
##eras
##gol
antrim
loki
##kow
##asian
##good
##zano
braid
handwriting
subdistrict
funky
pantheon
##iculate
concurrency
estimation
improper
juliana
##his
newcomers
johnstone
staten
communicated
##oco
##alle
sausage
stormy
##stered
##tters
superfamily
##grade
acidic
collateral
tabloid
##oped
##rza
bladder
austen
##ellant
mcgraw
##hay
hannibal
mein
aquino
lucifer
wo
badger
boar
cher
christensen
greenberg
interruption
##kken
jem
244
mocked
bottoms
cambridgeshire
##lide
sprawling
##bbly
eastwood
ghent
synth
##buck
advisers
##bah
nominally
hapoel
qu
daggers
estranged
fabricated
towels
vinnie
wcw
misunderstanding
anglia
nothin
unmistakable
##dust
##lova
chilly
marquette
truss
##edge
##erine
reece
##lty
##chemist
##connected
272
308
41st
bash
raion
waterfalls
##ump
##main
labyrinth
queue
theorist
##istle
bharatiya
flexed
soundtracks
rooney
leftist
patrolling
wharton
plainly
alleviate
eastman
schuster
topographic
engages
immensely
unbearable
fairchild
1620
dona
lurking
parisian
oliveira
ia
indictment
hahn
bangladeshi
##aster
vivo
##uming
##ential
antonia
expects
indoors
kildare
harlan
##logue
##ogenic
##sities
forgiven
##wat
childish
tavi
##mide
##orra
plausible
grimm
successively
scooted
##bola
##dget
##rith
spartans
emery
flatly
azure
epilogue
##wark
flourish
##iny
##tracted
##overs
##oshi
bestseller
distressed
receipt
spitting
hermit
topological
##cot
drilled
subunit
francs
##layer
eel
##fk
##itas
octopus
footprint
petitions
ufo
##say
##foil
interfering
leaking
palo
##metry
thistle
valiant
##pic
narayan
mcpherson
##fast
gonzales
##ym
##enne
dustin
novgorod
solos
##zman
doin
##raph
##patient
##meyer
soluble
ashland
cuffs
carole
pendleton
whistling
vassal
##river
deviation
revisited
constituents
rallied
rotate
loomed
##eil
##nting
amateurs
augsburg
auschwitz
crowns
skeletons
##cona
bonnet
257
dummy
globalization
simeon
sleeper
mandal
differentiated
##crow
##mare
milne
bundled
exasperated
talmud
owes
segregated
##feng
##uary
dentist
piracy
props
##rang
devlin
##torium
malicious
paws
##laid
dependency
##ergy
##fers
##enna
258
pistons
rourke
jed
grammatical
tres
maha
wig
512
ghostly
jayne
##achal
##creen
##ilis
##lins
##rence
designate
##with
arrogance
cambodian
clones
showdown
throttle
twain
##ception
lobes
metz
nagoya
335
braking
##furt
385
roaming
##minster
amin
crippled
##37
##llary
indifferent
hoffmann
idols
intimidating
1751
261
influenza
memo
onions
1748
bandage
consciously
##landa
##rage
clandestine
observes
swiped
tangle
##ener
##jected
##trum
##bill
##lta
hugs
congresses
josiah
spirited
##dek
humanist
managerial
filmmaking
inmate
rhymes
debuting
grimsby
ur
##laze
duplicate
vigor
##tf
republished
bolshevik
refurbishment
antibiotics
martini
methane
newscasts
royale
horizons
levant
iain
visas
##ischen
paler
##around
manifestation
snuck
alf
chop
futile
pedestal
rehab
##kat
bmg
kerman
res
fairbanks
jarrett
abstraction
saharan
##zek
1746
procedural
clearer
kincaid
sash
luciano
##ffey
crunch
helmut
##vara
revolutionaries
##tute
creamy
leach
##mmon
1747
permitting
nes
plight
wendell
##lese
contra
ts
clancy
ipa
mach
staples
autopsy
disturbances
nueva
karin
pontiac
##uding
proxy
venerable
haunt
leto
bergman
expands
##helm
wal
##pipe
canning
celine
cords
obesity
##enary
intrusion
planner
##phate
reasoned
sequencing
307
harrow
##chon
##dora
marred
mcintyre
repay
tarzan
darting
248
harrisburg
margarita
repulsed
##hur
##lding
belinda
hamburger
novo
compliant
runways
bingham
registrar
skyscraper
ic
cuthbert
improvisation
livelihood
##corp
##elial
admiring
##dened
sporadic
believer
casablanca
popcorn
##29
asha
shovel
##bek
##dice
coiled
tangible
##dez
casper
elsie
resin
tenderness
rectory
##ivision
avail
sonar
##mori
boutique
##dier
guerre
bathed
upbringing
vaulted
sandals
blessings
##naut
##utnant
1680
306
foxes
pia
corrosion
hesitantly
confederates
crystalline
footprints
shapiro
tirana
valentin
drones
45th
microscope
shipments
texted
inquisition
wry
guernsey
unauthorized
resigning
760
ripple
schubert
stu
reassure
felony
##ardo
brittle
koreans
##havan
##ives
dun
implicit
tyres
##aldi
##lth
magnolia
##ehan
##puri
##poulos
aggressively
fei
gr
familiarity
##poo
indicative
##trust
fundamentally
jimmie
overrun
395
anchors
moans
##opus
britannia
armagh
##ggle
purposely
seizing
##vao
bewildered
mundane
avoidance
cosmopolitan
geometridae
quartermaster
caf
415
chatter
engulfed
gleam
purge
##icate
juliette
jurisprudence
guerra
revisions
##bn
casimir
brew
##jm
1749
clapton
cloudy
conde
hermitage
278
simulations
torches
vincenzo
matteo
##rill
hidalgo
booming
westbound
accomplishment
tentacles
unaffected
##sius
annabelle
flopped
sloping
##litz
dreamer
interceptor
vu
##loh
consecration
copying
messaging
breaker
climates
hospitalized
1752
torino
afternoons
winfield
witnessing
##teacher
breakers
choirs
sawmill
coldly
##ege
sipping
haste
uninhabited
conical
bibliography
pamphlets
severn
edict
##oca
deux
illnesses
grips
##pl
rehearsals
sis
thinkers
tame
##keepers
1690
acacia
reformer
##osed
##rys
shuffling
##iring
##shima
eastbound
ionic
rhea
flees
littered
##oum
rocker
vomiting
groaning
champ
overwhelmingly
civilizations
paces
sloop
adoptive
##tish
skaters
##vres
aiding
mango
##joy
nikola
shriek
##ignon
pharmaceuticals
##mg
tuna
calvert
gustavo
stocked
yearbook
##urai
##mana
computed
subsp
riff
hanoi
kelvin
hamid
moors
pastures
summons
jihad
nectar
##ctors
bayou
untitled
pleasing
vastly
republics
intellect
##
##ulio
##tou
crumbling
stylistic
sb
##
consolation
frequented
ho
walden
widows
##iens
404
##ignment
chunks
improves
288
grit
recited
##dev
snarl
sociological
##arte
##gul
inquired
##held
bruise
clube
consultancy
homogeneous
hornets
multiplication
pasta
prick
savior
##grin
##kou
##phile
yoon
##gara
grimes
vanishing
cheering
reacting
bn
distillery
##quisite
##vity
coe
dockyard
massif
##jord
escorts
voss
##valent
byte
chopped
hawke
illusions
workings
floats
##koto
##vac
kv
annapolis
madden
##onus
alvaro
noctuidae
##cum
##scopic
avenge
steamboat
forte
illustrates
erika
##trip
570
dew
nationalities
bran
manifested
thirsty
diversified
muscled
reborn
##standing
arson
##lessness
##dran
##logram
##boys
##kushima
##vious
willoughby
##phobia
286
alsace
dashboard
yuki
##chai
granville
myspace
publicized
tricked
##gang
adjective
##ater
relic
reorganisation
enthusiastically
indications
saxe
##lassified
consolidate
iec
padua
helplessly
ramps
renaming
regulars
pedestrians
accents
convicts
inaccurate
lowers
mana
##pati
barrie
bjp
outta
someplace
berwick
flanking
invoked
marrow
sparsely
excerpts
clothed
rei
##ginal
wept
##strae
##vish
alexa
excel
##ptive
membranes
aquitaine
creeks
cutler
sheppard
implementations
ns
##dur
fragrance
budge
concordia
magnesium
marcelo
##antes
gladly
vibrating
##rral
##ggles
montrose
##omba
lew
seamus
1630
cocky
##ament
##uen
bjorn
##rrick
fielder
fluttering
##lase
methyl
kimberley
mcdowell
reductions
barbed
##jic
##tonic
aeronautical
condensed
distracting
##promising
huffed
##cala
##sle
claudius
invincible
missy
pious
balthazar
ci
##lang
butte
combo
orson
##dication
myriad
1707
silenced
##fed
##rh
coco
netball
yourselves
##oza
clarify
heller
peg
durban
etudes
offender
roast
blackmail
curvature
##woods
vile
309
illicit
suriname
##linson
overture
1685
bubbling
gymnast
tucking
##mming
##ouin
maldives
##bala
gurney
##dda
##eased
##oides
backside
pinto
jars
racehorse
tending
##rdial
baronetcy
wiener
duly
##rke
barbarian
cupping
flawed
##thesis
bertha
pleistocene
puddle
swearing
##nob
##tically
fleeting
prostate
amulet
educating
##mined
##iti
##tler
75th
jens
respondents
analytics
cavaliers
papacy
raju
##iente
##ulum
##tip
funnel
271
disneyland
##lley
sociologist
##iam
2500
faulkner
louvre
menon
##dson
276
##ower
afterlife
mannheim
peptide
referees
comedians
meaningless
##anger
##laise
fabrics
hurley
renal
sleeps
##bour
##icle
breakout
kristin
roadside
animator
clover
disdain
unsafe
redesign
##urity
firth
barnsley
portage
reset
narrows
268
commandos
expansive
speechless
tubular
##lux
essendon
eyelashes
smashwords
##yad
##bang
##claim
craved
sprinted
chet
somme
astor
wrocaw
orton
266
bane
##erving
##uing
mischief
##amps
##sund
scaling
terre
##xious
impairment
offenses
undermine
moi
soy
contiguous
arcadia
inuit
seam
##tops
macbeth
rebelled
##icative
##iot
590
elaborated
frs
uniformed
##dberg
259
powerless
priscilla
stimulated
980
qc
arboretum
frustrating
trieste
bullock
##nified
enriched
glistening
intern
##adia
locus
nouvelle
ollie
ike
lash
starboard
ee
tapestry
headlined
hove
rigged
##vite
pollock
##yme
thrive
clustered
cas
roi
gleamed
olympiad
##lino
pressured
regimes
##hosis
##lick
ripley
##ophone
kickoff
gallon
rockwell
##arable
crusader
glue
revolutions
scrambling
1714
grover
##jure
englishman
aztec
263
contemplating
coven
ipad
preach
triumphant
tufts
##esian
rotational
##phus
328
falkland
##brates
strewn
clarissa
rejoin
environmentally
glint
banded
drenched
moat
albanians
johor
rr
maestro
malley
nouveau
shaded
taxonomy
v6
adhere
bunk
airfields
##ritan
1741
encompass
remington
tran
##erative
amelie
mazda
friar
morals
passions
##zai
breadth
vis
##hae
argus
burnham
caressing
insider
rudd
##imov
##mini
##rso
italianate
murderous
textual
wainwright
armada
bam
weave
timer
##taken
##nh
fra
##crest
ardent
salazar
taps
tunis
##ntino
allegro
gland
philanthropic
##chester
implication
##optera
esq
judas
noticeably
wynn
##dara
inched
indexed
crises
villiers
bandit
royalties
patterned
cupboard
interspersed
accessory
isla
kendrick
entourage
stitches
##esthesia
headwaters
##ior
interlude
distraught
draught
1727
##basket
biased
sy
transient
triad
subgenus
adapting
kidd
shortstop
##umatic
dimly
spiked
mcleod
reprint
nellie
pretoria
windmill
##cek
singled
##mps
273
reunite
##orous
747
bankers
outlying
##omp
##ports
##tream
apologies
cosmetics
patsy
##deh
##ocks
##yson
bender
nantes
serene
##nad
lucha
mmm
323
##cius
##gli
cmll
coinage
nestor
juarez
##rook
smeared
sprayed
twitching
sterile
irina
embodied
juveniles
enveloped
miscellaneous
cancers
dq
gulped
luisa
crested
swat
donegal
ref
##anov
##acker
hearst
mercantile
##lika
doorbell
ua
vicki
##alla
##som
bilbao
psychologists
stryker
sw
horsemen
turkmenistan
wits
##national
anson
mathew
screenings
##umb
rihanna
##agne
##nessy
aisles
##iani
##osphere
hines
kenton
saskatoon
tasha
truncated
##champ
##itan
mildred
advises
fredrik
interpreting
inhibitors
##athi
spectroscopy
##hab
##kong
karim
panda
##oia
##nail
##vc
conqueror
kgb
leukemia
##dity
arrivals
cheered
pisa
phosphorus
shielded
##riated
mammal
unitarian
urgently
chopin
sanitary
##mission
spicy
drugged
hinges
##tort
tipping
trier
impoverished
westchester
##caster
267
epoch
nonstop
##gman
##khov
aromatic
centrally
cerro
##tively
##vio
billions
modulation
sedimentary
283
facilitating
outrageous
goldstein
##eak
##kt
ld
maitland
penultimate
pollard
##dance
fleets
spaceship
vertebrae
##nig
alcoholism
als
recital
##bham
##ference
##omics
m2
##bm
trois
##tropical
##
commemorates
##meric
marge
##raction
1643
670
cosmetic
ravaged
##ige
catastrophe
eng
##shida
albrecht
arterial
bellamy
decor
harmon
##rde
bulbs
synchronized
vito
easiest
shetland
shielding
wnba
##glers
##ssar
##riam
brianna
cumbria
##aceous
##rard
cores
thayer
##nsk
brood
hilltop
luminous
carts
keynote
larkin
logos
##cta
##
##mund
##quay
lilith
tinted
277
wrestle
mobilization
##uses
sequential
siam
bloomfield
takahashi
274
##ieving
presenters
ringo
blazed
witty
##oven
##ignant
devastation
haydn
harmed
newt
therese
##peed
gershwin
molina
rabbis
sudanese
001
innate
restarted
##sack
##fus
slices
wb
##shah
enroll
hypothetical
hysterical
1743
fabio
indefinite
warped
##hg
exchanging
525
unsuitable
##sboro
gallo
1603
bret
cobalt
homemade
##hunter
mx
operatives
##dhar
terraces
durable
latch
pens
whorls
##ctuated
##eaux
billing
ligament
succumbed
##gly
regulators
spawn
##brick
##stead
filmfare
rochelle
##nzo
1725
circumstance
saber
supplements
##nsky
##tson
crowe
wellesley
carrot
##9th
##movable
primate
drury
sincerely
topical
##mad
##rao
callahan
kyiv
smarter
tits
undo
##yeh
announcements
anthologies
barrio
nebula
##islaus
##shaft
##tyn
bodyguards
2021
assassinate
barns
emmett
scully
##mah
##yd
##eland
##tino
##itarian
demoted
gorman
lashed
prized
adventist
writ
##gui
alla
invertebrates
##ausen
1641
amman
1742
align
healy
redistribution
##gf
##rize
insulation
##drop
adherents
hezbollah
vitro
ferns
yanking
269
php
registering
uppsala
cheerleading
confines
mischievous
tully
##ross
49th
docked
roam
stipulated
pumpkin
##bry
prompt
##ezer
blindly
shuddering
craftsmen
frail
scented
katharine
scramble
shaggy
sponge
helix
zaragoza
279
##52
43rd
backlash
fontaine
seizures
posse
cowan
nonfiction
telenovela
wwii
hammered
undone
##gpur
encircled
irs
##ivation
artefacts
oneself
searing
smallpox
##belle
##osaurus
shandong
breached
upland
blushing
rankin
infinitely
psyche
tolerated
docking
evicted
##col
unmarked
##lving
gnome
lettering
litres
musique
##oint
benevolent
##jal
blackened
##anna
mccall
racers
tingle
##ocene
##orestation
introductions
radically
292
##hiff
##
1610
1739
munchen
plead
##nka
condo
scissors
##sight
##tens
apprehension
##cey
##yin
hallmark
watering
formulas
sequels
##llas
aggravated
bae
commencing
##building
enfield
prohibits
marne
vedic
civilized
euclidean
jagger
beforehand
blasts
dumont
##arney
##nem
740
conversions
hierarchical
rios
simulator
##dya
##lellan
hedges
oleg
thrusts
shadowed
darby
maximize
1744
gregorian
##nded
##routed
sham
unspecified
##hog
emory
factual
##smo
##tp
fooled
##rger
ortega
wellness
marlon
##oton
##urance
casket
keating
ley
enclave
##ayan
char
influencing
jia
##chenko
412
ammonia
erebidae
incompatible
violins
cornered
##arat
grooves
astronauts
columbian
rampant
fabrication
kyushu
mahmud
vanish
##dern
mesopotamia
##lete
ict
##rgen
caspian
kenji
pitted
##vered
999
grimace
roanoke
tchaikovsky
twinned
##analysis
##awan
xinjiang
arias
clemson
kazakh
sizable
1662
##khand
##vard
plunge
tatum
vittorio
##nden
cholera
##dana
##oper
bracing
indifference
projectile
superliga
##chee
realises
upgrading
299
porte
retribution
##vies
nk
stil
##resses
ama
bureaucracy
blackberry
bosch
testosterone
collapses
greer
##pathic
ioc
fifties
malls
##erved
bao
baskets
adolescents
siegfried
##osity
##tosis
mantra
detecting
existent
fledgling
##cchi
dissatisfied
gan
telecommunication
mingled
sobbed
6000
controversies
outdated
taxis
##raus
fright
slams
##lham
##fect
##tten
detectors
fetal
tanned
##uw
fray
goth
olympian
skipping
mandates
scratches
sheng
unspoken
hyundai
tracey
hotspur
restrictive
##buch
americana
mundo
##bari
burroughs
diva
vulcan
##6th
distinctions
thumping
##ngen
mikey
sheds
fide
rescues
springsteen
vested
valuation
##ece
##ely
pinnacle
rake
sylvie
##edo
almond
quivering
##irus
alteration
faltered
##wad
51st
hydra
ticked
##kato
recommends
##dicated
antigua
arjun
stagecoach
wilfred
trickle
pronouns
##pon
aryan
nighttime
##anian
gall
pea
stitch
##hei
leung
milos
##dini
eritrea
nexus
starved
snowfall
kant
parasitic
cot
discus
hana
strikers
appleton
kitchens
##erina
##partisan
##itha
##vius
disclose
metis
##channel
1701
tesla
##vera
fitch
1735
blooded
##tila
decimal
##tang
##bai
cyclones
eun
bottled
peas
pensacola
basha
bolivian
crabs
boil
lanterns
partridge
roofed
1645
necks
##phila
opined
patting
##kla
##lland
chuckles
volta
whereupon
##nche
devout
euroleague
suicidal
##dee
inherently
involuntary
knitting
nasser
##hide
puppets
colourful
courageous
southend
stills
miraculous
hodgson
richer
rochdale
ethernet
greta
uniting
prism
umm
##haya
##itical
##utation
deterioration
pointe
prowess
##ropriation
lids
scranton
billings
subcontinent
##koff
##scope
brute
kellogg
psalms
degraded
##vez
stanisaw
##ructured
ferreira
pun
astonishing
gunnar
##yat
arya
prc
gottfried
##tight
excursion
##ographer
dina
##quil
##nare
huffington
illustrious
wilbur
gundam
verandah
##zard
naacp
##odle
constructive
fjord
kade
##naud
generosity
thrilling
baseline
cayman
frankish
plastics
accommodations
zoological
##fting
cedric
qb
motorized
##dome
##otted
squealed
tackled
canucks
budgets
situ
asthma
dail
gabled
grasslands
whimpered
writhing
judgments
##65
minnie
pv
##carbon
bananas
grille
domes
monique
odin
maguire
markham
tierney
##estra
##chua
libel
poke
speedy
atrium
laval
notwithstanding
##edly
fai
kala
##sur
robb
##sma
listings
luz
supplementary
tianjin
##acing
enzo
jd
ric
scanner
croats
transcribed
##49
arden
cv
##hair
##raphy
##lver
##uy
357
seventies
staggering
alam
horticultural
hs
regression
timbers
blasting
##ounded
montagu
manipulating
##cit
catalytic
1550
troopers
##meo
condemnation
fitzpatrick
##oire
##roved
inexperienced
1670
castes
##lative
outing
314
dubois
flicking
quarrel
ste
learners
1625
iq
whistled
##class
282
classify
tariffs
temperament
355
folly
liszt
##yles
immersed
jordanian
ceasefire
apparel
extras
maru
fished
##bio
harta
stockport
assortment
craftsman
paralysis
transmitters
##cola
blindness
##wk
fatally
proficiency
solemnly
##orno
repairing
amore
groceries
ultraviolet
##chase
schoolhouse
##tua
resurgence
nailed
##otype
##
ruse
saliva
diagrams
##tructing
albans
rann
thirties
1b
antennas
hilarious
cougars
paddington
stats
##eger
breakaway
ipod
reza
authorship
prohibiting
scoffed
##etz
##ttle
conscription
defected
trondheim
##fires
ivanov
keenan
##adan
##ciful
##fb
##slow
locating
##ials
##tford
cadiz
basalt
blankly
interned
rags
rattling
##tick
carpathian
reassured
sync
bum
guildford
iss
staunch
##onga
astronomers
sera
sofie
emergencies
susquehanna
##heard
duc
mastery
vh1
williamsburg
bayer
buckled
craving
##khan
##rdes
bloomington
##write
alton
barbecue
##bians
justine
##hri
##ndt
delightful
smartphone
newtown
photon
retrieval
peugeot
hissing
##monium
##orough
flavors
lighted
relaunched
tainted
##games
##lysis
anarchy
microscopic
hopping
adept
evade
evie
##beau
inhibit
sinn
adjustable
hurst
intuition
wilton
cisco
44th
lawful
lowlands
stockings
thierry
##dalen
##hila
##nai
fates
prank
tb
maison
lobbied
provocative
1724
4a
utopia
##qual
carbonate
gujarati
purcell
##rford
curtiss
##mei
overgrown
arenas
mediation
swallows
##rnik
respectful
turnbull
##hedron
##hope
alyssa
ozone
##i
ami
gestapo
johansson
snooker
canteen
cuff
declines
empathy
stigma
##ags
##iner
##raine
taxpayers
gui
volga
##wright
##copic
lifespan
overcame
tattooed
enactment
giggles
##ador
##camp
barrington
bribe
obligatory
orbiting
peng
##enas
elusive
sucker
##vating
cong
hardship
empowered
anticipating
estrada
cryptic
greasy
detainees
planck
sudbury
plaid
dod
marriott
kayla
##ears
##vb
##zd
mortally
##hein
cognition
radha
319
liechtenstein
meade
richly
argyle
harpsichord
liberalism
trumpets
lauded
tyrant
salsa
tiled
lear
promoters
reused
slicing
trident
##chuk
##gami
##lka
cantor
checkpoint
##points
gaul
leger
mammalian
##tov
##aar
##schaft
doha
frenchman
nirvana
##vino
delgado
headlining
##eron
##iography
jug
tko
1649
naga
intersections
##jia
benfica
nawab
##suka
ashford
gulp
##deck
##vill
##rug
brentford
frazier
pleasures
dunne
potsdam
shenzhen
dentistry
##tec
flanagan
##dorff
##hear
chorale
dinah
prem
quezon
##rogated
relinquished
sutra
terri
##pani
flaps
##rissa
poly
##rnet
homme
aback
##eki
linger
womb
##kson
##lewood
doorstep
orthodoxy
threaded
westfield
##rval
dioceses
fridays
subsided
##gata
loyalists
##biotic
##ettes
letterman
lunatic
prelate
tenderly
invariably
souza
thug
winslow
##otide
furlongs
gogh
jeopardy
##runa
pegasus
##umble
humiliated
standalone
tagged
##roller
freshmen
klan
##bright
attaining
initiating
transatlantic
logged
viz
##uance
1723
combatants
intervening
stephane
chieftain
despised
grazed
317
cdc
galveston
godzilla
macro
simulate
##planes
parades
##esses
960
##ductive
##unes
equator
overdose
##cans
##hosh
##lifting
joshi
epstein
sonora
treacherous
aquatics
manchu
responsive
##sation
supervisory
##christ
##llins
##ibar
##balance
##uso
kimball
karlsruhe
mab
##emy
ignores
phonetic
reuters
spaghetti
820
almighty
danzig
rumbling
tombstone
designations
lured
outset
##felt
supermarkets
##wt
grupo
kei
kraft
susanna
##blood
comprehension
genealogy
##aghan
##verted
redding
##ythe
1722
bowing
##pore
##roi
lest
sharpened
fulbright
valkyrie
sikhs
##unds
swans
bouquet
merritt
##tage
##venting
commuted
redhead
clerks
leasing
cesare
dea
hazy
##vances
fledged
greenfield
servicemen
##gical
armando
blackout
dt
sagged
downloadable
intra
potion
pods
##4th
##mism
xp
attendants
gambia
stale
##ntine
plump
asteroids
rediscovered
buds
flea
hive
##neas
1737
classifications
debuts
##eles
olympus
scala
##eurs
##gno
##mute
hummed
sigismund
visuals
wiggled
await
pilasters
clench
sulfate
##ances
bellevue
enigma
trainee
snort
##sw
clouded
denim
##rank
##rder
churning
hartman
lodges
riches
sima
##missible
accountable
socrates
regulates
mueller
##cr
1702
avoids
solids
himalayas
nutrient
pup
##jevic
squat
fades
nec
##lates
##pina
##rona
##
privateer
tequila
##gative
##mpton
apt
hornet
immortals
##dou
asturias
cleansing
dario
##rries
##anta
etymology
servicing
zhejiang
##venor
##nx
horned
erasmus
rayon
relocating
10
##bags
escalated
promenade
stubble
2010s
artisans
axial
liquids
mora
sho
yoo
##tsky
bundles
oldies
##nally
notification
bastion
##ths
sparkle
##lved
1728
leash
pathogen
highs
##hmi
immature
880
gonzaga
ignatius
mansions
monterrey
sweets
bryson
##loe
polled
regatta
brightest
pei
rosy
squid
hatfield
payroll
addict
meath
cornerback
heaviest
lodging
##mage
capcom
rippled
##sily
barnet
mayhem
ymca
snuggled
rousseau
##cute
blanchard
284
fragmented
leighton
chromosomes
risking
##md
##strel
##utter
corinne
coyotes
cynical
hiroshi
yeomanry
##ractive
ebook
grading
mandela
plume
agustin
magdalene
##rkin
bea
femme
trafford
##coll
##lun
##tance
52nd
fourier
upton
##mental
camilla
gust
iihf
islamabad
longevity
##kala
feldman
netting
##rization
endeavour
foraging
mfa
orr
##open
greyish
contradiction
graz
##ruff
handicapped
marlene
tweed
oaxaca
spp
campos
miocene
pri
configured
cooks
pluto
cozy
pornographic
##entes
70th
fairness
glided
jonny
lynne
rounding
sired
##emon
##nist
remade
uncover
##mack
complied
lei
newsweek
##jured
##parts
##enting
##pg
293
finer
guerrillas
athenian
deng
disused
stepmother
accuse
gingerly
seduction
521
confronting
##walker
##going
gora
nostalgia
sabres
virginity
wrenched
##minated
syndication
wielding
eyre
##56
##gnon
##igny
behaved
taxpayer
sweeps
##growth
childless
gallant
##ywood
amplified
geraldine
scrape
##ffi
babylonian
fresco
##rdan
##kney
##position
1718
restricting
tack
fukuoka
osborn
selector
partnering
##dlow
318
gnu
kia
tak
whitley
gables
##54
##mania
mri
softness
immersion
##bots
##evsky
1713
chilling
insignificant
pcs
##uis
elites
lina
purported
supplemental
teaming
##americana
##dding
##inton
proficient
rouen
##nage
##rret
niccolo
selects
##bread
fluffy
1621
gruff
knotted
mukherjee
polgara
thrash
nicholls
secluded
smoothing
thru
corsica
loaf
whitaker
inquiries
##rrier
##kam
indochina
289
marlins
myles
peking
##tea
extracts
pastry
superhuman
connacht
vogel
##ditional
##het
##udged
##lash
gloss
quarries
refit
teaser
##alic
##gaon
20s
materialized
sling
camped
pickering
tung
tracker
pursuant
##cide
cranes
soc
##cini
##typical
##viere
anhalt
overboard
workout
chores
fares
orphaned
stains
##logie
fenton
surpassing
joyah
triggers
##itte
grandmaster
##lass
##lists
clapping
fraudulent
ledger
nagasaki
##cor
##nosis
##tsa
eucalyptus
tun
##icio
##rney
##tara
dax
heroism
ina
wrexham
onboard
unsigned
##dates
moshe
galley
winnie
droplets
exiles
praises
watered
noodles
##aia
fein
adi
leland
multicultural
stink
bingo
comets
erskine
modernized
canned
constraint
domestically
chemotherapy
featherweight
stifled
##mum
darkly
irresistible
refreshing
hasty
isolate
##oys
kitchener
planners
##wehr
cages
yarn
implant
toulon
elects
childbirth
yue
##lind
##lone
cn
rightful
sportsman
junctions
remodeled
specifies
##rgh
291
##oons
complimented
##urgent
lister
ot
##logic
bequeathed
cheekbones
fontana
gabby
##dial
amadeus
corrugated
maverick
resented
triangles
##hered
##usly
nazareth
tyrol
1675
assent
poorer
sectional
aegean
##cous
296
nylon
ghanaian
##egorical
##weig
cushions
forbid
fusiliers
obstruction
somerville
##scia
dime
earrings
elliptical
leyte
oder
polymers
timmy
atm
midtown
piloted
settles
continual
externally
mayfield
##uh
enrichment
henson
keane
persians
1733
benji
braden
pep
324
##efe
contenders
pepsi
valet
##isches
298
##asse
##earing
goofy
stroll
##amen
authoritarian
occurrences
adversary
ahmedabad
tangent
toppled
dorchester
1672
modernism
marxism
islamist
charlemagne
exponential
racks
unicode
brunette
mbc
pic
skirmish
##bund
##lad
##powered
##yst
hoisted
messina
shatter
##ctum
jedi
vantage
##music
##neil
clemens
mahmoud
corrupted
authentication
lowry
nils
##washed
omnibus
wounding
jillian
##itors
##opped
serialized
narcotics
handheld
##arm
##plicity
intersecting
stimulating
##onis
crate
fellowships
hemingway
casinos
climatic
fordham
copeland
drip
beatty
leaflets
robber
brothel
madeira
##hedral
sphinx
ultrasound
##vana
valor
forbade
leonid
villas
##aldo
duane
marquez
##cytes
disadvantaged
forearms
kawasaki
reacts
consular
lax
uncles
uphold
##hopper
concepcion
dorsey
lass
##izan
arching
passageway
1708
researches
tia
internationals
##graphs
##opers
distinguishes
javanese
divert
##uven
plotted
##listic
##rwin
##erik
##tify
affirmative
signifies
validation
##bson
kari
felicity
georgina
zulu
##eros
##rained
##rath
overcoming
##dot
argyll
##rbin
1734
chiba
ratification
windy
earls
parapet
##marks
hunan
pristine
astrid
punta
##gart
brodie
##kota
##oder
malaga
minerva
rouse
##phonic
bellowed
pagoda
portals
reclamation
##gur
##odies
##
parentheses
quoting
allergic
palette
showcases
benefactor
heartland
nonlinear
##tness
bladed
cheerfully
scans
##ety
##hone
1666
girlfriends
pedersen
hiram
sous
##liche
##nator
1683
##nery
##orio
##umen
bobo
primaries
smiley
##cb
unearthed
uniformly
fis
metadata
1635
ind
##oted
recoil
##titles
##tura
##
406
hilbert
jamestown
mcmillan
tulane
seychelles
##frid
antics
coli
fated
stucco
##grants
1654
bulky
accolades
arrays
caledonian
carnage
optimism
puebla
##tative
##cave
enforcing
rotherham
seo
dunlop
aeronautics
chimed
incline
zoning
archduke
hellenistic
##oses
##sions
candi
thong
##ople
magnate
rustic
##rsk
projective
slant
##offs
danes
hollis
vocalists
##ammed
congenital
contend
gesellschaft
##ocating
##pressive
douglass
quieter
##cm
##kshi
howled
salim
spontaneously
townsville
buena
southport
##bold
kato
1638
faerie
stiffly
##vus
##rled
297
flawless
realising
taboo
##7th
bytes
straightening
356
jena
##hid
##rmin
cartwright
berber
bertram
soloists
411
noses
417
coping
fission
hardin
inca
##cen
1717
mobilized
vhf
##raf
biscuits
curate
##85
##anial
331
gaunt
neighbourhoods
1540
##abas
blanca
bypassed
sockets
behold
coincidentally
##bane
nara
shave
splinter
terrific
##arion
##erian
commonplace
juris
redwood
waistband
boxed
caitlin
fingerprints
jennie
naturalized
##ired
balfour
craters
jody
bungalow
hugely
quilt
glitter
pigeons
undertaker
bulging
constrained
goo
##sil
##akh
assimilation
reworked
##person
persuasion
##pants
felicia
##cliff
##ulent
1732
explodes
##dun
##inium
##zic
lyman
vulture
hog
overlook
begs
northwards
ow
spoil
##urer
fatima
favorably
accumulate
sargent
sorority
corresponded
dispersal
kochi
toned
##imi
##lita
internacional
newfound
##agger
##lynn
##rigue
booths
peanuts
##eborg
medicare
muriel
nur
##uram
crates
millennia
pajamas
worsened
##breakers
jimi
vanuatu
yawned
##udeau
carousel
##hony
hurdle
##ccus
##mounted
##pod
rv
##eche
airship
ambiguity
compulsion
recapture
##claiming
arthritis
##osomal
1667
asserting
ngc
sniffing
dade
discontent
glendale
ported
##amina
defamation
rammed
##scent
fling
livingstone
##fleet
875
##ppy
apocalyptic
comrade
lcd
##lowe
cessna
eine
persecuted
subsistence
demi
hoop
reliefs
710
coptic
progressing
stemmed
perpetrators
1665
priestess
##nio
dobson
ebony
rooster
itf
tortricidae
##bbon
##jian
cleanup
##jean
##y
1721
eighties
taxonomic
holiness
##hearted
##spar
antilles
showcasing
stabilized
##nb
gia
mascara
michelangelo
dawned
##uria
##vinsky
extinguished
fitz
grotesque
100
##fera
##loid
##mous
barges
neue
throbbed
cipher
johnnie
##a1
##mpt
outburst
##swick
spearheaded
administrations
c1
heartbreak
pixels
pleasantly
##enay
lombardy
plush
##nsed
bobbie
##hly
reapers
tremor
xiang
minogue
substantive
hitch
barak
##wyl
kwan
##encia
910
obscene
elegance
indus
surfer
bribery
conserve
##hyllum
##masters
horatio
##fat
apes
rebound
psychotic
##pour
iteration
##mium
##vani
botanic
horribly
antiques
dispose
paxton
##hli
##wg
timeless
1704
disregard
engraver
hounds
##bau
##version
looted
uno
facilitates
groans
masjid
rutland
antibody
disqualification
decatur
footballers
quake
slacks
48th
rein
scribe
stabilize
commits
exemplary
tho
##hort
##chison
pantry
traversed
##hiti
disrepair
identifiable
vibrated
baccalaureate
##nnis
csa
interviewing
##iensis
##rae
greaves
wealthiest
343
classed
jogged
5
##58
##atal
illuminating
knicks
respecting
##uno
scrubbed
##iji
##dles
kruger
moods
growls
raider
silvia
chefs
kam
vr
cree
percival
##terol
gunter
counterattack
defiant
henan
ze
##rasia
##riety
equivalence
submissions
##fra
##thor
bautista
mechanically
##heater
cornice
herbal
templar
##mering
outputs
ruining
ligand
renumbered
extravagant
mika
blockbuster
eta
insurrection
##ilia
darkening
ferocious
pianos
strife
kinship
##aer
melee
##anor
##iste
##may
##oue
decidedly
weep
##jad
##missive
##ppel
354
puget
unease
##gnant
1629
hammering
kassel
ob
wessex
##lga
bromwich
egan
paranoia
utilization
##atable
##idad
contradictory
provoke
##ols
##ouring
##tangled
knesset
##very
##lette
plumbing
##sden
##
greensboro
occult
sniff
338
zev
beaming
gamer
haggard
mahal
##olt
##pins
mendes
utmost
briefing
gunnery
##gut
##pher
##zh
##rok
1679
khalifa
sonya
##boot
principals
urbana
wiring
##liffe
##minating
##rrado
dahl
nyu
skepticism
np
townspeople
ithaca
lobster
somethin
##fur
##arina
##1
freighter
zimmerman
biceps
contractual
##herton
amend
hurrying
subconscious
##anal
336
meng
clermont
spawning
##eia
##lub
dignitaries
impetus
snacks
spotting
twigs
##bilis
##cz
##ouk
libertadores
nic
skylar
##aina
##firm
gustave
asean
##anum
dieter
legislatures
flirt
bromley
trolls
umar
##bbies
##tyle
blah
parc
bridgeport
crank
negligence
##nction
46th
constantin
molded
bandages
seriousness
00pm
siegel
carpets
compartments
upbeat
statehood
##dner
##edging
marko
730
platt
##hane
paving
##iy
1738
abbess
impatience
limousine
nbl
##talk
441
lucille
mojo
nightfall
robbers
##nais
karel
brisk
calves
replicate
ascribed
telescopes
##olf
intimidated
##reen
ballast
specialization
##sit
aerodynamic
caliphate
rainer
visionary
##arded
epsilon
##aday
##onte
aggregation
auditory
boosted
reunification
kathmandu
loco
robyn
402
acknowledges
appointing
humanoid
newell
redeveloped
restraints
##tained
barbarians
chopper
1609
italiana
##lez
##lho
investigates
wrestlemania
##anies
##bib
690
##falls
creaked
dragoons
gravely
minions
stupidity
volley
##harat
##week
musik
##eries
##uously
fungal
massimo
semantics
malvern
##ahl
##pee
discourage
embryo
imperialism
1910s
profoundly
##ddled
jiangsu
sparkled
stat
##holz
sweatshirt
tobin
##iction
sneered
##cheon
##oit
brit
causal
smyth
##neuve
diffuse
perrin
silvio
##ipes
##recht
detonated
iqbal
selma
##nism
##zumi
roasted
##riders
tay
##ados
##mament
##mut
##rud
840
completes
nipples
cfa
flavour
hirsch
##laus
calderon
sneakers
moravian
##ksha
1622
rq
294
##imeters
bodo
##isance
##pre
##ronia
anatomical
excerpt
##lke
dh
kunst
##tablished
##scoe
biomass
panted
unharmed
gael
housemates
montpellier
##59
coa
rodents
tonic
hickory
singleton
##taro
451
1719
aldo
breaststroke
dempsey
och
rocco
##cuit
merton
dissemination
midsummer
serials
##idi
haji
polynomials
##rdon
gs
enoch
prematurely
shutter
taunton
3
##grating
##inates
archangel
harassed
##asco
326
archway
dazzling
##ecin
1736
sumo
wat
##kovich
1086
honneur
##ently
##nostic
##ttal
##idon
1605
403
1716
blogger
rents
##gnan
hires
##ikh
##dant
howie
##rons
handler
retracted
shocks
1632
arun
duluth
kepler
trumpeter
##lary
peeking
seasoned
trooper
##mara
laszlo
##iciencies
##rti
heterosexual
##inatory
##ssion
indira
jogging
##inga
##lism
beit
dissatisfaction
malice
##ately
nedra
peeling
##rgeon
47th
stadiums
475
vertigo
##ains
iced
restroom
##plify
##tub
illustrating
pear
##chner
##sibility
inorganic
rappers
receipts
watery
##kura
lucinda
##oulos
reintroduced
##8th
##tched
gracefully
saxons
nutritional
wastewater
rained
favourites
bedrock
fisted
hallways
likeness
upscale
##lateral
1580
blinds
prequel
##pps
##tama
deter
humiliating
restraining
tn
vents
1659
laundering
recess
rosary
tractors
coulter
federer
##ifiers
##plin
persistence
##quitable
geschichte
pendulum
quakers
##beam
bassett
pictorial
buffet
koln
##sitor
drills
reciprocal
shooters
##57
##cton
##tees
converge
pip
dmitri
donnelly
yamamoto
aqua
azores
demographics
hypnotic
spitfire
suspend
wryly
roderick
##rran
sebastien
##asurable
mavericks
##fles
##200
himalayan
prodigy
##iance
transvaal
demonstrators
handcuffs
dodged
mcnamara
sublime
1726
crazed
##efined
##till
ivo
pondered
reconciled
shrill
sava
##duk
bal
cad
heresy
jaipur
goran
##nished
341
lux
shelly
whitehall
##hre
israelis
peacekeeping
##wled
1703
demetrius
ousted
##arians
##zos
beale
anwar
backstroke
raged
shrinking
cremated
##yck
benign
towing
wadi
darmstadt
landfill
parana
soothe
colleen
sidewalks
mayfair
tumble
hepatitis
ferrer
superstructure
##gingly
##urse
##wee
anthropological
translators
##mies
closeness
hooves
##pw
mondays
##roll
##vita
landscaping
##urized
purification
sock
thorns
thwarted
jalan
tiberius
##taka
saline
##rito
confidently
khyber
sculptors
##ij
brahms
hammersmith
inspectors
battista
fivb
fragmentation
hackney
##uls
arresting
exercising
antoinette
bedfordshire
##zily
dyed
##hema
1656
racetrack
variability
##tique
1655
austrians
deteriorating
madman
theorists
aix
lehman
weathered
1731
decreed
eruptions
1729
flaw
quinlan
sorbonne
flutes
nunez
1711
adored
downwards
fable
rasped
1712
moritz
mouthful
renegade
shivers
stunts
dysfunction
restrain
translit
327
pancakes
##avio
##cision
##tray
351
vial
##lden
bain
##maid
##oxide
chihuahua
malacca
vimes
##rba
##rnier
1664
donnie
plaques
##ually
337
bangs
floppy
huntsville
loretta
nikolay
##otte
eater
handgun
ubiquitous
##hett
eras
zodiac
1634
##omorphic
1820s
##zog
cochran
##bula
##lithic
warring
##rada
dalai
excused
blazers
mcconnell
reeling
bot
este
##abi
geese
hoax
taxon
##bla
guitarists
##icon
condemning
hunts
inversion
moffat
taekwondo
##lvis
1624
stammered
##rest
##rzy
sousa
fundraiser
marylebone
navigable
uptown
cabbage
daniela
salman
shitty
whimper
##kian
##utive
programmers
protections
rm
##rmi
##rued
forceful
##enes
fuss
##tao
##wash
brat
oppressive
reykjavik
spartak
ticking
##inkles
##kiewicz
adolph
horst
maui
protege
straighten
cpc
landau
concourse
clements
resultant
##ando
imaginative
joo
reactivated
##rem
##ffled
##uising
consultative
##guide
flop
kaitlyn
mergers
parenting
somber
##vron
supervise
vidhan
##imum
courtship
exemplified
harmonies
medallist
refining
##rrow
##
amara
##hum
780
goalscorer
sited
overshadowed
rohan
displeasure
secretive
multiplied
osman
##orth
engravings
padre
##kali
##veda
miniatures
mis
##yala
clap
pali
rook
##cana
1692
57th
antennae
astro
oskar
1628
bulldog
crotch
hackett
yucatan
##sure
amplifiers
brno
ferrara
migrating
##gree
thanking
turing
##eza
mccann
ting
andersson
onslaught
gaines
ganga
incense
standardization
##mation
sentai
scuba
stuffing
turquoise
waivers
alloys
##vitt
regaining
vaults
##clops
##gizing
digger
furry
memorabilia
probing
##iad
payton
rec
deutschland
filippo
opaque
seamen
zenith
afrikaans
##filtration
disciplined
inspirational
##merie
banco
confuse
grafton
tod
##dgets
championed
simi
anomaly
biplane
##ceptive
electrode
##para
1697
cleavage
crossbow
swirl
informant
##lars
##osta
afi
bonfire
spec
##oux
lakeside
slump
##culus
##lais
##qvist
##rrigan
1016
facades
borg
inwardly
cervical
xl
pointedly
050
stabilization
##odon
chests
1699
hacked
ctv
orthogonal
suzy
##lastic
gaulle
jacobite
rearview
##cam
##erted
ashby
##drik
##igate
##mise
##zbek
affectionately
canine
disperse
latham
##istles
##ivar
spielberg
##orin
##idium
ezekiel
cid
##sg
durga
middletown
##cina
customized
frontiers
harden
##etano
##zzy
1604
bolsheviks
##66
coloration
yoko
##bedo
briefs
slabs
debra
liquidation
plumage
##oin
blossoms
dementia
subsidy
1611
proctor
relational
jerseys
parochial
ter
##ici
esa
peshawar
cavalier
loren
cpi
idiots
shamrock
1646
dutton
malabar
mustache
##endez
##ocytes
referencing
terminates
marche
yarmouth
##sop
acton
mated
seton
subtly
baptised
beige
extremes
jolted
kristina
telecast
##actic
safeguard
waldo
##baldi
##bular
endeavors
sloppy
subterranean
##ensburg
##itung
delicately
pigment
tq
##scu
1626
##ound
collisions
coveted
herds
##personal
##meister
##nberger
chopra
##ricting
abnormalities
defective
galician
lucie
##dilly
alligator
likened
##genase
burundi
clears
complexion
derelict
deafening
diablo
fingered
champaign
dogg
enlist
isotope
labeling
mrna
##erre
brilliance
marvelous
##ayo
1652
crawley
ether
footed
dwellers
deserts
hamish
rubs
warlock
skimmed
##lizer
870
buick
embark
heraldic
irregularities
##ajan
kiara
##kulam
##ieg
antigen
kowalski
##lge
oakley
visitation
##mbit
vt
##suit
1570
murderers
##miento
##rites
chimneys
##sling
condemn
custer
exchequer
havre
##ghi
fluctuations
##rations
dfb
hendricks
vaccines
##tarian
nietzsche
biking
juicy
##duced
brooding
scrolling
selangor
##ragan
352
annum
boomed
seminole
sugarcane
##dna
departmental
dismissing
innsbruck
arteries
ashok
batavia
daze
kun
overtook
##rga
##tlan
beheaded
gaddafi
holm
electronically
faulty
galilee
fractures
kobayashi
##lized
gunmen
magma
aramaic
mala
eastenders
inference
messengers
bf
##qu
407
bathrooms
##vere
1658
flashbacks
ideally
misunderstood
##jali
##weather
mendez
##grounds
505
uncanny
##iii
1709
friendships
##nbc
sacrament
accommodated
reiterated
logistical
pebbles
thumped
##escence
administering
decrees
drafts
##flight
##cased
##tula
futuristic
picket
intimidation
winthrop
##fahan
interfered
339
afar
francoise
morally
uta
cochin
croft
dwarfs
##bruck
##dents
##nami
biker
##hner
##meral
nano
##isen
##ometric
##pres
##
brightened
meek
parcels
securely
gunners
##jhl
##zko
agile
hysteria
##lten
##rcus
bukit
champs
chevy
cuckoo
leith
sadler
theologians
welded
##section
1663
jj
plurality
xander
##rooms
##formed
shredded
temps
intimately
pau
tormented
##lok
##stellar
1618
charred
ems
essen
##mmel
alarms
spraying
ascot
blooms
twinkle
##abia
##apes
internment
obsidian
##chaft
snoop
##dav
##ooping
malibu
##tension
quiver
##itia
hays
mcintosh
travers
walsall
##ffie
1623
beverley
schwarz
plunging
structurally
m3
rosenthal
vikram
##tsk
770
ghz
##onda
##tiv
chalmers
groningen
pew
reckon
unicef
##rvis
55th
##gni
1651
sulawesi
avila
cai
metaphysical
screwing
turbulence
##mberg
augusto
samba
56th
baffled
momentary
toxin
##urian
##wani
aachen
condoms
dali
steppe
##3d
##app
##oed
##year
adolescence
dauphin
electrically
inaccessible
microscopy
nikita
##ega
atv
##cel
##enter
##oles
##oteric
##
accountants
punishments
wrongly
bribes
adventurous
clinch
flinders
southland
##hem
##kata
gough
##ciency
lads
soared
##
undergoes
deformation
outlawed
rubbish
##arus
##mussen
##nidae
##rzburg
arcs
##ingdon
##tituted
1695
wheelbase
wheeling
bombardier
campground
zebra
##lices
##oj
##bain
lullaby
##ecure
donetsk
wylie
grenada
##arding
##
squinting
eireann
opposes
##andra
maximal
runes
##broken
##cuting
##iface
##ror
##rosis
additive
britney
adultery
triggering
##drome
detrimental
aarhus
containment
jc
swapped
vichy
##ioms
madly
##oric
##rag
brant
##ckey
##trix
1560
1612
broughton
rustling
##stems
##uder
asbestos
mentoring
##nivorous
finley
leaps
##isan
apical
pry
slits
substitutes
##dict
intuitive
fantasia
insistent
unreasonable
##igen
##vna
domed
hannover
margot
ponder
##zziness
impromptu
jian
lc
rampage
stemming
##eft
andrey
gerais
whichever
amnesia
appropriated
anzac
clicks
modifying
ultimatum
cambrian
maids
verve
yellowstone
##mbs
conservatoire
##scribe
adherence
dinners
spectra
imperfect
mysteriously
sidekick
tatar
tuba
##aks
##ifolia
distrust
##athan
##zle
c2
ronin
zac
##pse
celaena
instrumentalist
scents
skopje
##mbling
comical
compensated
vidal
condor
intersect
jingle
wavelengths
##urrent
mcqueen
##izzly
carp
weasel
422
kanye
militias
postdoctoral
eugen
gunslinger
##
faux
hospice
##for
appalled
derivation
dwarves
##elis
dilapidated
##folk
astoria
philology
##lwyn
##otho
##saka
inducing
philanthropy
##bf
##itative
geek
markedly
sql
##yce
bessie
indices
rn
##flict
495
frowns
resolving
weightlifting
tugs
cleric
contentious
1653
mania
rms
##miya
##reate
##ruck
##tucket
bien
eels
marek
##ayton
##cence
discreet
unofficially
##ife
leaks
##bber
1705
332
dung
compressor
hillsborough
pandit
shillings
distal
##skin
381
##tat
##you
nosed
##nir
mangrove
undeveloped
##idia
textures
##inho
##500
##rise
ae
irritating
nay
amazingly
bancroft
apologetic
compassionate
kata
symphonies
##lovic
airspace
##lch
930
gifford
precautions
fulfillment
sevilla
vulgar
martinique
##urities
looting
piccolo
tidy
##dermott
quadrant
armchair
incomes
mathematicians
stampede
nilsson
##inking
##scan
foo
quarterfinal
##ostal
shang
shouldered
squirrels
##owe
344
vinegar
##bner
##rchy
##systems
delaying
##trics
ars
dwyer
rhapsody
sponsoring
##gration
bipolar
cinder
starters
##olio
##urst
421
signage
##nty
aground
figurative
mons
acquaintances
duets
erroneously
soyuz
elliptic
recreated
##cultural
##quette
##ssed
##tma
##zcz
moderator
scares
##itaire
##stones
##udence
juniper
sighting
##just
##nsen
britten
calabria
ry
bop
cramer
forsyth
stillness
##
airmen
gathers
unfit
##umber
##upt
taunting
##rip
seeker
streamlined
##bution
holster
schumann
tread
vox
##gano
##onzo
strive
dil
reforming
covent
newbury
predicting
##orro
decorate
tre
##puted
andover
ie
asahi
dept
dunkirk
gills
##tori
buren
huskies
##stis
##stov
abstracts
bets
loosen
##opa
1682
yearning
##glio
##sir
berman
effortlessly
enamel
napoli
persist
##peration
##uez
attache
elisa
b1
invitations
##kic
accelerating
reindeer
boardwalk
clutches
nelly
polka
starbucks
##kei
adamant
huey
lough
unbroken
adventurer
embroidery
inspecting
stanza
##ducted
naia
taluka
##pone
##roids
chases
deprivation
florian
##jing
##ppet
earthly
##lib
##ssee
colossal
foreigner
vet
freaks
patrice
rosewood
triassic
upstate
##pkins
dominates
ata
chants
ks
vo
##400
##bley
##raya
##rmed
555
agra
infiltrate
##ailing
##ilation
##tzer
##uppe
##werk
binoculars
enthusiast
fujian
squeak
##avs
abolitionist
almeida
boredom
hampstead
marsden
rations
##ands
inflated
334
bonuses
rosalie
patna
##rco
329
detachments
penitentiary
54th
flourishing
woolf
##dion
##etched
papyrus
##lster
##nsor
##toy
bobbed
dismounted
endelle
inhuman
motorola
tbs
wince
wreath
##ticus
hideout
inspections
sanjay
disgrace
infused
pudding
stalks
##urbed
arsenic
leases
##hyl
##rrard
collarbone
##waite
##wil
dowry
##bant
##edance
genealogical
nitrate
salamanca
scandals
thyroid
necessitated
##!
##"
###
##$
##%
##&
##'
##(
##)
##*
##+
##,
##-
##.
##/
##:
##;
##<
##=
##>
##?
##@
##[
##\
##]
##^
##_
##`
##{
##|
##}
##~
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##
##

================
File: lit_nlp/examples/gunicorn_config.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""gunicorn configuration for cloud-hosted demos."""
import os
_DEMO_NAME = os.getenv('DEMO_NAME', 'glue')
_DEMO_PORT = os.getenv('DEMO_PORT', '5432')
bind = f'0.0.0.0:{_DEMO_PORT}'
timeout = 3600
threads = 8
worker_class = 'gthread'
wsgi_app = f'lit_nlp.examples.{_DEMO_NAME}.demo:get_wsgi_app()'

================
File: lit_nlp/examples/penguin/data.py
================
""" Penguin tabular dataset from TFDS.
See https://www.tensorflow.org/datasets/catalog/penguins. for details.
"""
from collections.abc import Mapping
from typing import Optional, Union
from lit_nlp.api import dataset as lit_dataset
from lit_nlp.api import types as lit_types
import tensorflow_datasets as tfds
VOCABS = {
    'island': ['Biscoe', 'Dream', 'Torgersen'],
    'sex': ['Female', 'Male'],
    'species': ['Adelie', 'Chinstrap', 'Gentoo']
}
INPUT_SPEC: lit_types.Spec = {
    'body_mass_g': lit_types.Scalar(min_val=2700, max_val=6300),
    'culmen_depth_mm': lit_types.Scalar(min_val=13, max_val=22),
    'culmen_length_mm': lit_types.Scalar(min_val=32, max_val=60),
    'flipper_length_mm': lit_types.Scalar(min_val=172, max_val=231),
    'island': lit_types.CategoryLabel(vocab=VOCABS['island']),
    'sex': lit_types.CategoryLabel(vocab=VOCABS['sex']),
}
class PenguinDataset(lit_dataset.Dataset):
  """Dataset of penguin tabular data.
  From https://www.tensorflow.org/datasets/catalog/penguins.
  """
  @classmethod
  def lit_example_from_record(cls, rec: Mapping[str, Union[float, int]]):
    return {
        'body_mass_g': rec['body_mass_g'],
        'culmen_depth_mm': rec['culmen_depth_mm'],
        'culmen_length_mm': rec['culmen_length_mm'],
        'flipper_length_mm': rec['flipper_length_mm'],
        'island': VOCABS['island'][rec['island']],
        'sex': VOCABS['sex'][rec['sex']],
        'species': VOCABS['species'][rec['species']],
    }
  def __init__(self, max_examples: Optional[int] = None):
    peng = tfds.load('penguins/simple', download=True, try_gcs=True)
    dataset_df = tfds.as_dataframe(peng['train'])
    # Filter out invalid rows.
    dataset_df = dataset_df.loc[dataset_df['sex'] != 2]
    records = dataset_df.to_dict(orient='records')
    self._examples = [
        PenguinDataset.lit_example_from_record(rec) for rec in records
    ][:max_examples]
  @classmethod
  def init_spec(cls) -> lit_types.Spec:
    return {
        'max_examples': lit_types.Integer(
            default=1000, min_val=0, max_val=10_000, required=False
        ),
    }
  def spec(self):
    return INPUT_SPEC | {
        'species': lit_types.CategoryLabel(vocab=VOCABS['species'])
    }

================
File: lit_nlp/examples/penguin/demo.py
================
""" LIT demo for tabular data using penguin classification.
To run:
  python -m lit_nlp.examples.penguin.demo --port=5432
Then navigate to localhost:5432 to access the demo UI.
"""
from collections.abc import Sequence
import sys
from typing import Optional
from absl import app
from absl import flags
from absl import logging
from lit_nlp import dev_server
from lit_nlp import server_flags
from lit_nlp.api import layout
from lit_nlp.components import minimal_targeted_counterfactuals
from lit_nlp.examples.penguin import data as penguin_data
from lit_nlp.examples.penguin import model as penguin_model
MODEL_PATH = 'https://storage.googleapis.com/what-if-tool-resources/lit-models/penguin.h5'  # pylint: disable=line-too-long
FLAGS = flags.FLAGS
FLAGS.set_default('default_layout', 'penguins')
_MODEL_PATH = flags.DEFINE_string('model_path', MODEL_PATH,
                                  'Path to load trained model.')
_MAX_EXAMPLES = flags.DEFINE_integer(
    'max_examples',
    None,
    (
        'Maximum number of examples to load into LIT. '
        'Set --max_examples=200 for a quick start.'
    ),
)
# Custom frontend layout; see api/layout.py
modules = layout.LitModuleName
PENGUIN_LAYOUT = layout.LitCanonicalLayout(
    upper={
        'Main': [
            modules.DiveModule,
            modules.DataTableModule,
            modules.DatapointEditorModule,
        ]
    },
    lower=layout.STANDARD_LAYOUT.lower,
    description='Custom layout for the Palmer Penguins demo.',
)
CUSTOM_LAYOUTS = layout.DEFAULT_LAYOUTS | {'penguins': PENGUIN_LAYOUT}
# Function for running demo through gunicorn instead of the local dev server.
def get_wsgi_app() -> Optional[dev_server.LitServerType]:
  FLAGS.set_default('server_type', 'external')
  FLAGS.set_default('demo_mode', True)
  # Parse flags without calling app.run(main), to avoid conflict with
  # gunicorn command line flags.
  unused = flags.FLAGS(sys.argv, known_only=True)
  if unused:
    logging.info('penguin_demo:get_wsgi_app() called with unused args: %s',
                 unused)
  return main([])
def main(argv: Sequence[str]) -> Optional[dev_server.LitServerType]:
  if len(argv) > 1:
    raise app.UsageError('Too many command-line arguments.')
  models = {'species classifier': penguin_model.PenguinModel(_MODEL_PATH.value)}
  datasets = {'penguins': penguin_data.PenguinDataset()}
  # Truncate datasets if --max_examples is set.
  if _MAX_EXAMPLES.value is not None:
    for name in datasets:
      logging.info("Dataset: '%s' with %d examples", name, len(datasets[name]))
      datasets[name] = datasets[name].slice[: _MAX_EXAMPLES.value]
      logging.info('  truncated to %d examples', len(datasets[name]))
  generators = {
      'Minimal Targeted Counterfactuals':
          minimal_targeted_counterfactuals.TabularMTC()
  }
  lit_demo = dev_server.Server(
      models,
      datasets,
      generators=generators,
      layouts=CUSTOM_LAYOUTS,
      **server_flags.get_flags())
  return lit_demo.serve()
if __name__ == '__main__':
  app.run(main)

================
File: lit_nlp/examples/penguin/model_int_test.py
================
"""Integration tests for penguin_model."""
from absl.testing import absltest
from lit_nlp.examples.penguin import model as penguin_model
class PenguinModelIntTest(absltest.TestCase):
  """Test that model class can predict."""
  def test_model(self):
    # Create model.
    model_path = "https://storage.googleapis.com/what-if-tool-resources/lit-models/penguin.h5"  # pylint: disable=line-too-long
    model = penguin_model.PenguinModel(model_path)
    # Run prediction to ensure no failure.
    model_in = [{
        "body_mass_g": 4000,
        "culmen_depth_mm": 15,
        "culmen_length_mm": 50,
        "flipper_length_mm": 200,
        "island": "Biscoe",
        "sex": "Male",
    }]
    model_out = list(model.predict(model_in))
    # Sanity-check output vs output spec.
    self.assertLen(model_out, 1)
    for key in model.output_spec().keys():
      self.assertIn(key, model_out[0].keys())
if __name__ == "__main__":
  absltest.main()

================
File: lit_nlp/examples/penguin/model.py
================
""" TensorFlow Keras model for the Penguin dataset."""
import os
from lit_nlp.api import model as lit_model
from lit_nlp.api import types as lit_types
from lit_nlp.examples.penguin import data as penguin_data
from lit_nlp.lib import file_cache
import numpy as np
import tf_keras as keras
os.environ['TF_USE_LEGACY_KERAS'] = '1'
_VOCABS = penguin_data.VOCABS
class PenguinModel(lit_model.BatchedModel):
  """TensorFlow Keras model for penguin classification."""
  def __init__(self, path: str):
    path = file_cache.cached_path(path)
    self.model = keras.models.load_model(path)
    # Feature column means and variance to normalize values before
    # prediction.
    self.means = np.array([
        4.23795547e+03, 1.71222672e+01, 4.40004048e+01, 2.01587045e+02,
        5.14170040e-01, 3.40080972e-01, 1.45748988e-01, 4.89878543e-01,
        5.10121457e-01
    ])
    self.vars = np.array([
        6.29523451e+05, 4.17662973e+00, 2.76167220e+01, 1.97108820e+02,
        2.49799210e-01, 2.24425904e-01, 1.24506220e-01, 2.49897556e-01,
        2.49897556e-01
    ])
  def max_minibatch_size(self) -> int:
    return 32
  def predict_minibatch(self, inputs):
    def convert_input(inp):
      ex = np.array([
          inp['body_mass_g'], inp['culmen_depth_mm'], inp['culmen_length_mm'],
          inp['flipper_length_mm'], 0, 0, 0, 0, 0
      ])
      # Set one-hot encodings of categorical features.
      island_index = _VOCABS['island'].index(inp['island'])
      sex_index = _VOCABS['sex'].index(inp['sex'])
      # Island one-hot encodings start at input index 4.
      ex[island_index + 4] = 1
      # Sex one-hot encodings start at input index 7.
      ex[sex_index + 7] = 1
      # Normalize feature values.
      ex = (ex - self.means) / (self.vars**0.5)
      return ex.tolist()
    adjusted_inputs = [convert_input(inp) for inp in inputs]
    model_output = self.model.predict(adjusted_inputs)
    ret = [{'predicted_species': out} for out in model_output]
    return ret
  @classmethod
  def init_spec(cls) -> lit_types.Spec:
    default_model_path = ('https://storage.googleapis.com/'
                          'what-if-tool-resources/lit-models/penguin.h5')
    return {'path': lit_types.String(default=default_model_path)}
  def input_spec(self) -> lit_types.Spec:
    return penguin_data.INPUT_SPEC
  def output_spec(self) -> lit_types.Spec:
    return {
        'predicted_species': lit_types.MulticlassPreds(
            parent='species', vocab=_VOCABS['species']
        )
    }

================
File: lit_nlp/examples/penguin/README.md
================
Penguin Demo

* LIT team hosts a penguin demo which can be accessible at https://pair-code.github.io/lit/demos/penguins.html.
* For more details, check out the documented example for tabular data at https://pair-code.github.io/lit/documentation/demos.html.

================
File: lit_nlp/examples/prompt_debugging/constants.py
================
"""Constants used across parallel classes in the Prompt Debugging example."""
import types
from lit_nlp.api import types as lit_types
class FieldNames(types.SimpleNamespace):
  PROMPT = "prompt"
  RESPONSE = "response"
  PROMPT_EMBEDDINGS = "prompt_embeddings"
  RESPONSE_EMBEDDINGS = "response_embeddings"
  TARGET = "target"
  TOKENS = "tokens"
  TARGET_MASK = "target_mask"
  GRAD_DOT_INPUT = "grad_dot_input"
  GRAD_NORM = "grad_l2"
INPUT_SPEC: lit_types.Spec = {
    FieldNames.PROMPT: lit_types.TextSegment(),
    FieldNames.TARGET: lit_types.TextSegment(required=False),
}
INPUT_SPEC_SALIENCE: lit_types.Spec = {
    FieldNames.TARGET_MASK: lit_types.TokenScores(align="", required=False),
}
OUTPUT_SPEC_GENERATION: lit_types.Spec = {
    FieldNames.RESPONSE: lit_types.GeneratedText(parent=FieldNames.TARGET)
}
OUTPUT_SPEC_GENERATION_EMBEDDINGS: lit_types.Spec = {
    FieldNames.PROMPT_EMBEDDINGS: lit_types.Embeddings(required=False),
    FieldNames.RESPONSE_EMBEDDINGS: lit_types.Embeddings(required=False),
}
OUTPUT_SPEC_TOKENIZER: lit_types.Spec = {
    FieldNames.TOKENS: lit_types.Tokens(parent=""),
}
OUTPUT_SPEC_SALIENCE: lit_types.Spec = {
    FieldNames.GRAD_DOT_INPUT: lit_types.TokenScores(align=FieldNames.TOKENS),
    FieldNames.GRAD_NORM: lit_types.TokenScores(align=FieldNames.TOKENS),
} | OUTPUT_SPEC_TOKENIZER

================
File: lit_nlp/examples/prompt_debugging/datasets.py
================
"""Methods for configuring prompt debugging datasets."""
from collections.abc import Mapping, Sequence
import copy
import functools
import json
import os
import re
from typing import Optional
from absl import logging
from lit_nlp import app as lit_app
from lit_nlp.api import dataset as lit_dataset
from lit_nlp.api import types as lit_types
SAMPLE_DATA_DIR = os.path.dirname(__file__)
DEFAULT_DATASETS = ['sample_prompts']
DEFAULT_MAX_EXAMPLES = 1000
class PlaintextSents(lit_dataset.Dataset):
  """Load sentences from a flat text file."""
  def __init__(
      self,
      path_or_glob: str,
      skiplines: int = 0,
      max_examples: Optional[int] = None,
      field_name: str = 'text',
  ):
    self.field_name = field_name
    self._examples = self.load_datapoints(path_or_glob, skiplines=skiplines)[
        :max_examples
    ]
  @classmethod
  def init_spec(cls) -> lit_types.Spec:
    default_path = ''
    return {
        'path_or_glob': lit_types.String(default=default_path, required=False),
        'skiplines': lit_types.Integer(default=0, max_val=25),
        'max_examples': lit_types.Integer(
            default=1000, min_val=0, max_val=10_000, required=False
        ),
    }
  def load_datapoints(self, path_or_glob: str, skiplines: int = 0):
    examples = []
    for path in glob.glob(path_or_glob):
      with open(path) as fd:
        for i, line in enumerate(fd):
          if i < skiplines:  # skip header lines, if necessary
            continue
          line = line.strip()
          if line:  # skip blank lines, these are usually document breaks
            examples.append({self.field_name: line})
    return examples
  def load(self, path: str):
    return lit_dataset.Dataset(base=self, examples=self.load_datapoints(path))
  def spec(self) -> lit_types.Spec:
    """Should match MLM's input_spec()."""
    return {self.field_name: lit_types.TextSegment()}
class PromptExamples(lit_dataset.Dataset):
  """Prompt examples for modern LMs."""
  SAMPLE_DATA_PATH = os.path.join(SAMPLE_DATA_DIR, 'prompt_examples.jsonl')
  def load_datapoints(self, path: str):
    if not path:
      logging.warn(
          'Empty path to PromptExamples.load_datapoints(). Returning empty'
          ' dataset.'
      )
      return []
    default_ex_values = {
        k: copy.deepcopy(field_spec.default)
        for k, field_spec in self.spec().items()
    }
    examples = []
    with open(path) as fd:
      for line in fd:
        examples.append(default_ex_values | json.loads(line))
    return examples
  def __init__(self, path: str):
    self._examples = self.load_datapoints(path)
  def spec(self) -> lit_types.Spec:
    return {
        'source': lit_types.CategoryLabel(),
        'prompt': lit_types.TextSegment(),
        'target': lit_types.TextSegment(),
    }
  def load(self, path: str):
    return lit_dataset.Dataset(base=self, examples=self.load_datapoints(path))
_plaintext_prompts = functools.partial(  # pylint: disable=invalid-name
    PlaintextSents, field_name='prompt'
)
# Hack: normally dataset loaders are a class object which has a __name__,
# rather than a functools.partial
_plaintext_prompts.__name__ = 'PlaintextSents'
def get_datasets(
    datasets_config: Optional[Sequence[str]] = None,
    max_examples: int = DEFAULT_MAX_EXAMPLES,
) -> Mapping[str, lit_dataset.Dataset]:
  """Loads datasets from the provided configs.
  Args:
    datasets_config: A sequence of configs in the form of <name>:<path> where
      the path points to is either: 1) a JSON Lines file containing records with
      a required "prompt" field and optional "target" and "source" fields; or 2)
      a plain text file where each line is a prompt.
    max_examples: Maximum number of examples in each loaded dataset.
  Returns:
    A mapping from dataset name to the initialized LIT dataset.
  """
  if not datasets_config:
    return {}
  datasets: dict[str, lit_dataset.Dataset] = {}
  for dataset_string in datasets_config:
    if dataset_string == 'sample_prompts':
      dataset_name = 'sample_prompts'
      path = PromptExamples.SAMPLE_DATA_PATH
    else:
      # Only split on the first ':', because path may be a URL
      # containing 'https://'
      dataset_name, path = dataset_string.split(':', 1)
    logging.info("Loading dataset '%s' from '%s'", dataset_name, path)
    if path.endswith('.jsonl'):
      datasets[dataset_name] = PromptExamples(path)
    # .txt or .txt-#####-of-#####
    elif path.endswith('.txt') or re.match(r'.*\.txt-\d{5}-of-\d{5}$', path):
      datasets[dataset_name] = _plaintext_prompts(path)
    else:
      raise ValueError(f'Unsupported dataset format for {dataset_string}')
  for name in datasets:
    datasets[name] = datasets[name].slice[:max_examples]
    logging.info("Dataset: '%s' with %d examples", name, len(datasets[name]))
  return datasets
def get_dataset_loaders() -> lit_app.DatasetLoadersMap:
  return {
      'jsonl_examples': (
          PromptExamples,
          PromptExamples.init_spec(),
      ),
      'plaintext_inputs': (
          _plaintext_prompts,
          PlaintextSents.init_spec(),
      ),
  }

================
File: lit_nlp/examples/prompt_debugging/keras_lms.py
================
"""LIT model wrappers for generic instrumented Keras LMs."""
from collections.abc import Sequence
import functools
import inspect
from typing import Optional
from absl import logging
import keras
from keras_nlp import models as keras_models
from lit_nlp.api import model as lit_model
from lit_nlp.api import types as lit_types
from lit_nlp.examples.prompt_debugging import constants as pd_constants
from lit_nlp.examples.prompt_debugging import utils as pd_utils
from lit_nlp.lib import file_cache
from lit_nlp.lib import utils as lit_utils
# pylint: disable=g-import-not-at-top
# pytype: disable=import-error
# NOTE: The Keras backend must be set before loading the Keras library. You can
# set the backend using the KERAS_BACKEND environment variable or your
# ~/.keras/keras.json configuration file. For more information, see:
# https://keras.io/getting_started/#configuring-your-backend
if keras.backend.backend() == "tensorflow":
  import tensorflow as tf
elif keras.backend.backend() == "torch":
  import torch
else:
  # TODO(b/333373960): Update imports once a JAX salience is supported.
  raise ValueError(f"Unsupported backend: {keras.backend.backend()}")
# pytype: enable=import-error
# pylint: enable=g-import-not-at-top
_DEFAULT_MAX_LENGTH = 1024
class _KerasBaseModel(lit_model.BatchedModel):
  """Base LIT model wrapper class for Keras on TensorFlow."""
  # TODO(lit-dev): pytype annotations for model= ?
  # Should be keras_nlp.models.generative_task.GenerativeTask
  def __init__(
      self,
      model: Optional[keras_models.CausalLM] = None,
      model_name_or_path: Optional[str] = None,
      max_length: int = _DEFAULT_MAX_LENGTH,
      dynamic_sequence_length: bool = True,
      batch_size: int = 16,
  ):
    """Base wrapper for a Keras CausalLM supporting the layer_intercept_fn API.
    Model should support the following methods:
    - .generate()
    - .score()*
    - .preprocessor.generate_preprocess()
    . .preprocessor.tokenizer.id_to_token()
    . .backbone.token_embedding()
    * The score function should accept layer_intercept_fn= as a way to intercept
    and manipulate activations between layers. We use this for salience, below.
    Args:
      model: A pre-loaded Keras CausalLM, prioritized over model_name_or_path.
      model_name_or_path: A URL, path, or preset name for the model to load,
      max_length: max sequence length
      dynamic_sequence_length: if true, will trim padding to the length of the
        longest sequence in a batch. Recommended for CPU and GPU usage, but may
        be disabled for compilation where a fixed shape is required.
      batch_size: batch size
    """
    super().__init__()
    if model is not None:
      self.model = model
    elif model_name_or_path is not None:
      if (
          is_tar_gz := model_name_or_path.endswith(".tar.gz")
      ) or file_cache.is_remote(model_name_or_path):
        model_name_or_path = file_cache.cached_path(
            model_name_or_path,
            extract_compressed_file=is_tar_gz,
        )
      self.model = keras_models.CausalLM.from_preset(model_name_or_path)
    else:
      raise ValueError("Must provide either model or model_name_or_path.")
    self.batch_size = batch_size
    self.max_length = max_length
    self.dynamic_sequence_length = dynamic_sequence_length
    # map ids: <tf.int>[batch_size, num_tokens]
    # to embs: <tf.float>[batch_size, num_tokens, emb_dim]
    self.embedder = self.model.backbone.token_embedding
  def encode_inputs(self, texts: Sequence[str]):
    """Encode inputs, with optional dynamic trimming.
    By default, the model's generate_preprocess() pads to a fixed sequence
    length, either specified as sequence_length= or using an internal default.
    Here, we optionally trim this to remove extraneous padding positions based
    on the actual contents of the minibatch. This can greatly speed up
    performance when running on CPU or GPU.
    Args:
      texts: list of input strings
    Returns:
      A dict[str, Tensor] compatible with model.score(), etc. functions.
    """
    # First: pack to max_length
    encoded_inputs = self.model.preprocessor.generate_preprocess(
        texts, sequence_length=self.max_length
    )
    if not self.dynamic_sequence_length:
      return encoded_inputs
    # Trim to the maximum length needed to contain any non-padding tokens.
    mask = encoded_inputs["padding_mask"]
    if keras.backend.backend() == "tensorflow":
      max_indices = [tf.reduce_max(tf.where(row)) for row in mask]
    elif keras.backend.backend() == "torch":
      max_indices = [torch.max(torch.where(row)[0]) for row in mask]
    else:
      raise ValueError(f"Unsupported backend: {keras.backend.backend()}")
    # Find position of last 'True' in each row.
    seq_ends: Sequence[int] = [
        keras.ops.convert_to_numpy(i).tolist() + 1 for i in max_indices
    ]
    longest_sequence = max(seq_ends)
    # TODO(lit-dev): remove this line, or make it logging.debug ?
    logging.info(
        "Trimming batch to trimmed_length = %d based on sequence ends %s",
        longest_sequence,
        seq_ends,
    )
    # Actually trim the input tensors.
    return {k: v[:, :longest_sequence] for k, v in encoded_inputs.items()}
  def clean_subword_token(self, tok: str) -> str:
    """Clean up special subword token from the tokenizers if necessary.
    Args:
      tok: the token to clean up.
    Returns:
      The replaced token if the provided token matches the special subword token
      below; otherwise, the original token is returned.
    """
    # For GPT2 tokenizer.
    tok = tok.replace("", "\n")  # newlines
    tok = tok.replace("", "")  # start of word -> magic underscore
    # For SentencePiece Tokenizer.
    tok = tok.replace("<0x0A>", "\n")  # newlines
    return tok
  def ids_to_clean_tokens(self, ids: Sequence[int]) -> Sequence[str]:
    return [
        self.clean_subword_token(
            self.model.preprocessor.tokenizer.id_to_token(id)
        )
        for id in ids
    ]
  @classmethod
  def from_loaded(cls, existing: "_KerasBaseModel", *args, **kw):
    """Share weights and underlying Keras model with another instance."""
    return cls(model=existing.model, *args, **kw)
  def max_minibatch_size(self) -> int:
    return self.batch_size
  @classmethod
  def init_spec(cls):
    # Cannot initialize from spec, because we need a Keras model object.
    return None
  def input_spec(self):
    return pd_constants.INPUT_SPEC
class KerasGenerationModel(_KerasBaseModel):
  """LIT model wrapper for generating text with Keras on TensorFlow.
  This class accepts a loaded model and provides the LIT-required functions plus
  additional helper functions for generation tasks.
  This class supports generation and pass-through modes. If a dataset provides a
  pre-populated 'response' column then this model will return that text instead
  of generating new text from the 'prompt'. This allows the same model wrapper
  to be efficiently used to examine saved results from bulk-inference pipelines
  and new generations from, e.g., counterfactually generated examples, or novel
  evaluation datasets.
  """
  def __init__(self, *args, output_embeddings=True, **kw):
    super().__init__(*args, **kw)
    self.output_embeddings = output_embeddings
  def embed_texts(self, texts: Sequence[str]):
    processed_inputs = self.encode_inputs(texts)
    # <float>[batch_size, num_tokens, emb_dim]
    embs = self.embedder(processed_inputs["token_ids"])
    # <bool>[batch_size, num_tokens]
    mask = processed_inputs["padding_mask"]
    return embs, mask
  def embed_and_mean_pool(self, texts: Sequence[str]):
    """Return a single vector for each text."""
    embs, mask = self.embed_texts(texts)
    # <float>[batch_size, num_tokens, 1]
    cast_mask = keras.ops.cast(mask, dtype=embs.dtype)
    if keras.backend.backend() == "tensorflow":
      expanded_mask = tf.expand_dims(cast_mask, axis=2)
      pooled_embs = tf.reduce_sum(
          expanded_mask * embs, axis=1, keepdims=True
      ) / tf.reduce_sum(expanded_mask, axis=1, keepdims=True)
      return tf.squeeze(pooled_embs, axis=1)
    elif keras.backend.backend() == "torch":
      expanded_mask = torch.unsqueeze(cast_mask, dim=2)
      pooled_embs = torch.sum(
          expanded_mask * embs, dim=1, keepdim=True
      ) / torch.sum(expanded_mask, dim=1, keepdim=True)
      return torch.squeeze(pooled_embs, dim=1)
    else:
      raise ValueError(f"Unsupported backend: {keras.backend.backend()}")
  def predict_minibatch(
      self,
      inputs: list[lit_types.JsonDict],
  ) -> list[lit_types.JsonDict]:
    prompts: Sequence[str] = [
        ex[pd_constants.FieldNames.PROMPT] for ex in inputs
    ]
    # TODO(lit-dev): suppport loading cached responses here, since running
    # generation can be expensive.
    full_responses: Sequence[str] = list(
        self.model.generate(prompts, max_length=self.max_length)
    )
    # Model outputs include the prompt, so trim that off and just return the
    # generated portion.
    responses: Sequence[str] = [
        response[len(prompt) :]
        for response, prompt in zip(full_responses, prompts)
    ]
    outputs = [
        {pd_constants.FieldNames.RESPONSE: response} for response in responses
    ]
    if self.output_embeddings:
      prompt_embeddings = self.embed_and_mean_pool(prompts)
      # TODO(lit-dev): embed prompt + response and trim embedding instead?
      # Or just embed full_response.
      response_embeddings = self.embed_and_mean_pool(responses)
      for o, p, r in zip(outputs, prompt_embeddings, response_embeddings):
        o[pd_constants.FieldNames.PROMPT_EMBEDDINGS] = (
            keras.ops.convert_to_numpy(p)
        )
        o[pd_constants.FieldNames.RESPONSE_EMBEDDINGS] = (
            keras.ops.convert_to_numpy(r)
        )
    return outputs
  def output_spec(self) -> lit_types.Spec:
    ret = pd_constants.OUTPUT_SPEC_GENERATION
    if self.output_embeddings:
      return ret | pd_constants.OUTPUT_SPEC_GENERATION_EMBEDDINGS
    return ret
class KerasSalienceModel(_KerasBaseModel):
  """LIT model wrapper for computing salience with Keras on TensorFlow.
  This class accepts a loaded model and provides the LIT-required functions plus
  additional helper functions to convert and clean tokens and to compute
  sequence salience.
  This class does not support generation; use the KerasGenerationModel class to
  generate the text for which this class will compute salience.
  """
  def __init__(self, *args, **kw):
    super().__init__(*args, **kw)
    score_fn = getattr(self.model, "score", None)
    if score_fn is None or not inspect.ismethod(score_fn):
      raise TypeError(
          "Salience is computed via a .score() API, which is not supported by "
          "all KerasNLP CausalLM models. Please provide a model that "
          "supports this API."
      )
  def _pred(self, input_ids, padding_mask, target_masks):
    """Predict a batch of tokenized text.
    Args:
      input_ids: A Tensor with shape <int>[batch_size, num_tokens]
      padding_mask: A Tensor with shape <int>[batch_size, num_tokens]
      target_masks: A Numpy Array with shape <bool>[batch_size, num_tokens]
    Returns:
      Batched outputs for post-processing.
    """
    ##
    # Process target masks
    # It doesn't make sense to interpret the first token, since it is not ever
    # predicted. But we need to ensure that the mask[0] is zero, so it doesn't
    # cause problems when 'rolled' to the last position below.
    seq_len = keras.ops.shape(input_ids)[1]
    pad_fn = functools.partial(
        lit_utils.pad1d,
        min_len=seq_len,
        max_len=seq_len,
        pad_val=0,
        pad_left=False,
    )
    modified_masks = [[0] + list(mask[1:]) for mask in target_masks]
    stacked_padded_masks = keras.ops.stack(
        [pad_fn(mask) for mask in modified_masks],
        axis=0,
    )
    # Shift masks back so they align with the target_ids generated in the
    # backend-specific prediction functions.
    rolled_masks = keras.ops.roll(stacked_padded_masks, shift=-1, axis=1)
    loss_mask = keras.ops.convert_to_tensor(rolled_masks, dtype="bool")
    pred_kw_args = {
        "input_ids": input_ids,
        "padding_mask": padding_mask,
        "loss_mask": loss_mask,
    }
    if keras.backend.backend() == "tensorflow":
      grad_l2, grad_dot_input = self._pred_tf(**pred_kw_args)
    elif keras.backend.backend() == "jax":
      grad_l2, grad_dot_input = self._pred_jax(**pred_kw_args)
    elif keras.backend.backend() == "torch":
      grad_l2, grad_dot_input = self._pred_torch(**pred_kw_args)
    else:
      raise ValueError(f"Unsupported backend: {keras.backend.backend()}")
    batched_outputs = {
        "input_ids": input_ids,
        "padding_mask": padding_mask,
        pd_constants.FieldNames.GRAD_NORM: grad_l2,
        pd_constants.FieldNames.GRAD_DOT_INPUT: grad_dot_input,
    }
    return batched_outputs
  def _pred_tf(self, input_ids, padding_mask, loss_mask):
    # <int>[batch_size, num_tokens]; ignore the last one in each row.
    target_ids = tf.roll(input_ids, shift=-1, axis=1)
    embeddings = None
    with tf.GradientTape(watch_accessed_variables=False) as tape:
      def layer_intercept_fn(x, i):
        if i == -1:
          nonlocal embeddings, tape
          embeddings = x
          tape.watch(embeddings)
        return x
      # <float>[batch_size, num_tokens]
      per_token_loss = self.model.score(
          token_ids=input_ids,
          padding_mask=padding_mask,
          scoring_mode="loss",
          layer_intercept_fn=layer_intercept_fn,
          target_ids=target_ids,
      )
      masked_loss = per_token_loss * keras.ops.cast(
          loss_mask, per_token_loss.dtype
      )
    # <float>[batch_size, num_tokens, hdim]
    grads = tape.gradient(masked_loss, embeddings)
    # <float>[batch_size, num_tokens]
    grad_l2 = tf.norm(grads, axis=2)
    # <float>[batch_size, num_tokens]
    grad_dot_input = tf.reduce_sum(grads * embeddings, axis=2)
    return grad_l2, grad_dot_input
  # TODO(b/333373960): Implement salience computation for JAX.
  def _pred_jax(self, input_ids, padding_mask, loss_mask):
    # NOTE: JAX computes gradients automatically w.r.t function inputs and
    # outputs. The score function takes token_ids as its input but salience is
    # computed w.r.t. the embeddings, thus JAX cannot differentiate the loss
    # w.r.t. the embeddings and taking gradients w.r.t. the token_ids is not
    # equivalent. For now, we raise an error if using JAX.
    raise NotImplementedError("JAX backend not supported for salience.")
  def _pred_torch(self, input_ids, padding_mask, loss_mask):
    target_ids = torch.roll(input_ids, shifts=-1, dims=1)
    embeddings = None
    def layer_intercept_fn(x, i):
      if i == -1:
        nonlocal embeddings
        embeddings = x
      return x
    per_token_loss = self.model.score(
        token_ids=input_ids,
        padding_mask=padding_mask,
        scoring_mode="loss",
        layer_intercept_fn=layer_intercept_fn,
        target_ids=target_ids,
    )
    if embeddings is None:
      raise ValueError("Embeddings are None after scoring.")
    masked_loss = per_token_loss * keras.ops.cast(
        loss_mask, per_token_loss.dtype
    )
    # <float>[batch_size, num_tokens, hdim]
    grads = torch.autograd.grad(
        masked_loss, embeddings, grad_outputs=torch.ones_like(masked_loss)
    )[0]
    embeddings = embeddings.detach()
    # <float>[batch_size, num_tokens]
    grad_l2 = torch.norm(grads, dim=2)
    # <float>[batch_size, num_tokens]
    grad_dot_input = torch.sum(grads * embeddings, dim=2)
    return grad_l2, grad_dot_input
  def _postprocess(self, preds):
    """Post-process single-example preds. Operates on numpy arrays."""
    mask = preds.pop("padding_mask").astype(bool)
    ids = preds.pop("input_ids")[mask]
    preds[pd_constants.FieldNames.TOKENS] = self.ids_to_clean_tokens(ids)
    for key in lit_utils.find_spec_keys(
        self.output_spec(), lit_types.TokenScores
    ):
      preds[key] = preds[key][mask]
    # First token (<bos>) is not actually predicted, so return 0 for loss.
    # preds[FieldNames.TOKEN_LOSS][0] = 0
    return preds
  def predict_minibatch(self, inputs):
    """Predict on a single minibatch of examples."""
    texts: Sequence[str] = [
        ex[pd_constants.FieldNames.PROMPT]
        + ex.get(pd_constants.FieldNames.TARGET, "")
        for ex in inputs
    ]
    preprocessed_texts = self.encode_inputs(texts)
    sequence_ids = preprocessed_texts["token_ids"]
    padding_mask = preprocessed_texts["padding_mask"]
    target_masks = [
        ex.get(pd_constants.FieldNames.TARGET_MASK, []) for ex in inputs
    ]
    # Get the predictions.
    batched_outputs = self._pred(sequence_ids, padding_mask, target_masks)
    # Convert to numpy for post-processing.
    detached_outputs = {
        k: keras.ops.convert_to_numpy(v) for k, v in batched_outputs.items()
    }
    # Split up batched outputs, then post-process each example.
    unbatched_outputs = lit_utils.unbatch_preds(detached_outputs)
    return map(self._postprocess, unbatched_outputs)
  def input_spec(self):
    return super().input_spec() | pd_constants.INPUT_SPEC_SALIENCE
  def output_spec(self) -> lit_types.Spec:
    return pd_constants.OUTPUT_SPEC_SALIENCE
class KerasTokenizerModel(_KerasBaseModel):
  """LIT model wrapper for tokenizing text with Keras on TensorFlow.
  This class accepts a loaded model and provides the LIT-required functions plus
  additional helper functions to convert and clean tokens.
  """
  def _postprocess(self, preds):
    """Post-process single-example preds. Operates on numpy arrays."""
    # Be sure to cast to bool, otherwise this will select intger positions 0, 1
    # rather than acting as a boolean mask.
    mask = preds.pop("padding_mask").astype(bool)
    ids = preds.pop("token_ids")[mask]
    preds[pd_constants.FieldNames.TOKENS] = self.ids_to_clean_tokens(ids)
    return preds
  def predict_minibatch(self, inputs):
    """Tokenize a single minibatch of examples."""
    texts: Sequence[str] = [
        ex[pd_constants.FieldNames.PROMPT]
        + ex.get(pd_constants.FieldNames.TARGET, "")
        for ex in inputs
    ]
    preprocessed_texts = self.encode_inputs(texts)
    batched_outputs = {
        "token_ids": preprocessed_texts["token_ids"],
        "padding_mask": preprocessed_texts["padding_mask"],
    }
    # Convert to numpy for post-processing.
    detached_outputs = {
        k: keras.ops.convert_to_numpy(v) for k, v in batched_outputs.items()
    }
    # Split up batched outputs, then post-process each example.
    unbatched_outputs = lit_utils.unbatch_preds(detached_outputs)
    return map(self._postprocess, unbatched_outputs)
  def output_spec(self) -> lit_types.Spec:
    return pd_constants.OUTPUT_SPEC_TOKENIZER
def initialize_model_group_for_salience(
    new_name: str, **kw
) -> lit_model.ModelMap:
  """Creates '{name}' and '_{name}_salience' and '_{name}_tokenizer'."""
  salience_name, tokenizer_name = pd_utils.generate_model_group_names(new_name)
  generation_model = KerasGenerationModel(**kw)
  salience_model = KerasSalienceModel(model=generation_model.model, **kw)
  tokenizer_model = KerasTokenizerModel(model=generation_model.model, **kw)
  return {
      new_name: generation_model,
      salience_name: salience_model,
      tokenizer_name: tokenizer_model,
  }

================
File: lit_nlp/examples/prompt_debugging/layouts.py
================
"""Layouts for debugging language models in LIT."""
from lit_nlp.api import layout
_modules = layout.LitModuleName
LEFT_RIGHT_LAYOUT = layout.LitCanonicalLayout(
    left={
        "Examples": [_modules.DataTableModule],
        "Editor": [_modules.SingleDatapointEditorModule],
    },
    upper={  # if 'lower' not specified, this fills the right side
        "Salience": [_modules.SequenceSalienceModule],
    },
    layoutSettings=layout.LayoutSettings(leftWidth=40),
    description="Left/right layout for language model salience.",
)
TOP_BOTTOM_LAYOUT = layout.LitCanonicalLayout(
    upper={
        "Examples": [_modules.SimpleDataTableModule],
        "Editor": [_modules.SimpleDatapointEditorModule],
    },
    lower={
        "Salience": [_modules.SequenceSalienceModule],
    },
    layoutSettings=layout.LayoutSettings(
        hideToolbar=True,
        mainHeight=40,
        centerPage=True,
    ),
    description="Simplified layout for language model salience.",
)
THREE_PANEL_LAYOUT = layout.LitCanonicalLayout(
    left={
        "Data Table": [_modules.DataTableModule],
        "Embeddings": [_modules.EmbeddingsModule],
    },
    upper={
        "Datapoint Editor": [_modules.SingleDatapointEditorModule],
        "Datapoint Generators": [_modules.GeneratorModule],
    },
    lower={
        "Salience": [_modules.SequenceSalienceModule],
        "Metrics": [_modules.MetricsModule],
    },
    layoutSettings=layout.LayoutSettings(
        mainHeight=40,
        leftWidth=40,
    ),
    description="Custom layout for language model salience.",
)
LEFT_RIGHT = "left_right"
TOP_BOTTOM = "top_bottom"
THREE_PANEL = "three_panel"
PROMPT_DEBUGGING_LAYOUTS = {
    LEFT_RIGHT: LEFT_RIGHT_LAYOUT,
    TOP_BOTTOM: TOP_BOTTOM_LAYOUT,
    THREE_PANEL: THREE_PANEL_LAYOUT,
}

================
File: lit_nlp/examples/prompt_debugging/models.py
================
"""Methods for configuring models for prompt debugging."""
from collections.abc import Sequence
import os
from typing import Optional
from absl import logging
from lit_nlp import app as lit_app
from lit_nlp.api import model as lit_model
from lit_nlp.api import types as lit_types
DEFAULT_BATCH_SIZE = 1
DEFAULT_DL_FRAMEWORK = "kerasnlp"
DEFAULT_DL_RUNTIME = "torch"
DEFAULT_MODELS = ["gemma_1.1_instruct_2b_en:gemma_1.1_instruct_2b_en"]
DEFAULT_PRECISION = "bfloat16"
DEFAULT_SEQUENCE_LENGTH = 512
def _initialize_modeling_environment(
    dl_framework: str,
    dl_runtime: str,
    precision: str,
) -> None:
  """Configure the modeling environment."""
  if dl_framework == "kerasnlp":
    # NOTE: Keras requires that the KERAS_BACKEND variable is set before import.
    os.environ["KERAS_BACKEND"] = dl_runtime
    # NOTE: Imported here and not at the top of the file to avoid
    # initialization issues with the environment variables above.
    import keras  # pylint: disable=g-import-not-at-top
    keras.config.set_floatx(precision)
  elif dl_runtime == "torch":
    # NOTE: Keras sets precision for all backends with set_floatx(), but for
    # HuggingFace Transformers with PyTorch we need to set it explicitly.
    import torch  # pylint: disable=g-import-not-at-top # pytype: disable=import-error
    torch.set_default_dtype(
        torch.bfloat16 if precision == "bfloat16" else torch.float32
    )
def get_models(
    models_config: Optional[Sequence[str]] = None,
    dl_framework: str = DEFAULT_DL_FRAMEWORK,
    dl_runtime: str = DEFAULT_DL_RUNTIME,
    precision: str = DEFAULT_PRECISION,
    batch_size: int = DEFAULT_BATCH_SIZE,
    max_length: int = DEFAULT_SEQUENCE_LENGTH,
) -> lit_model.ModelMap:
  """Loads models from the given configs.
  Args:
    models_config: A list of model names and paths to load from, as
      "model:path", where path can be a URL, a local file path, or the name of a
      preset for the configured deep learning framework.
    dl_framework: The deep learning framework that loads and runs the model on
      the runtime, `models_config.path` incompatibilities will result in errors.
    dl_runtime: The deep learning runtime that the model runs on, either
      "tensorflow" or "torch". All loaded models will use the same runtime,
      incompatibilities will result in errors.
    precision: Floating point precision for the models, either `bfloat16` or
      `float32`.
    batch_size: The number of examples to process per batch.
    max_length: The maximum sequence length of the input.
  Returns:
    A mapping from model name to initialized LIT model.
  """
  if not models_config:
    return {}
  # NOTE: Always call this function before initializing models to ensure the
  # environment is properly configured.
  _initialize_modeling_environment(dl_framework, dl_runtime, precision)
  models: dict[str, lit_model.Model] = {}
  for model_string in models_config:
    # Only split on the first ':' as path may be a URL containing 'https://'
    model_name, path = model_string.split(":", 1)
    logging.info("Loading model '%s' from '%s'", model_name, path)
    if dl_framework == "kerasnlp":
      from lit_nlp.examples.prompt_debugging import keras_lms  # pylint: disable=g-import-not-at-top # pytype: disable=import-error
      models |= keras_lms.initialize_model_group_for_salience(
          model_name,
          model_name_or_path=path,
          max_length=max_length,
          batch_size=batch_size,
      )
    else:
      from lit_nlp.examples.prompt_debugging import transformers_lms  # pylint: disable=g-import-not-at-top # pytype: disable=import-error
      models |= transformers_lms.initialize_model_group_for_salience(
          model_name,
          model_name_or_path=path,
          batch_size=batch_size,
          framework=dl_runtime,
          max_length=max_length,
      )
  return models
def get_model_loaders(
    dl_framework: str = DEFAULT_DL_FRAMEWORK,
    dl_runtime: str = DEFAULT_DL_RUNTIME,
    batch_size: int = DEFAULT_BATCH_SIZE,
    max_length: int = DEFAULT_SEQUENCE_LENGTH,
) -> lit_app.ModelLoadersMap:
  """Get the model loader for the configured framework and runtime.
  Args:
    dl_framework: The deep learning framework that loads and runs the model on
      the runtime, all models are loaded with the same framework,
      `model_name_or_path` incompatibilities will result in errors.
    dl_runtime: The deep learning runtime that the model runs on, either
      "tensorflow" or "torch". All loaded models will use the same runtime,
      incompatibilities will result in errors.
    batch_size: The default batch size.
    max_length: The default maximum sequence length.
  Returns:
    A mapping from model name to initialized LIT model.
  """
  common_init_spec: lit_types.Spec = {
      "model_name_or_path": lit_types.String(),
      "batch_size": lit_types.Integer(
          default=batch_size, min_val=1, max_val=64, required=False
      ),
      "max_length": lit_types.Integer(
          default=max_length, min_val=1, max_val=2048, required=False
      ),
  }
  if dl_framework == "kerasnlp":
    from lit_nlp.examples.prompt_debugging import keras_lms  # pylint: disable=g-import-not-at-top # pytype: disable=import-error
    keras_init_spec: lit_types.Spec = {
        **common_init_spec,
        "dynamic_sequence_length": lit_types.Boolean(
            default=True, required=False
        ),
    }
    return {
        "Keras LLM": (
            keras_lms.initialize_model_group_for_salience,
            keras_init_spec,
        )
    }
  else:
    from lit_nlp.examples.prompt_debugging import transformers_lms  # pylint: disable=g-import-not-at-top # pytype: disable=import-error
    transformers_init_spec: lit_types.Spec = {
        **common_init_spec,
        "framework": lit_types.CategoryLabel(
            vocab=transformers_lms.SUPPORTED_ML_RUNTIMES, default=dl_runtime
        ),
    }
    return {
        "Transformers LLM": (
            transformers_lms.initialize_model_group_for_salience,
            transformers_init_spec,
        )
    }

================
File: lit_nlp/examples/prompt_debugging/notebook.py
================
"""Convenience functions for configuring LIT prompt debugging in a notebook."""
from collections.abc import Sequence
from lit_nlp import notebook as lit_notebook
from lit_nlp.examples.prompt_debugging import datasets
from lit_nlp.examples.prompt_debugging import layouts
from lit_nlp.examples.prompt_debugging import models
def make_notebook_widget(
    datasets_config: Sequence[str],
    models_config: Sequence[str],
    *,
    # keep-sorted start
    batch_size: int = models.DEFAULT_BATCH_SIZE,
    dl_framework: str = "kerasnlp",
    dl_runtime: str = "tensorflow",
    max_examples: int = datasets.DEFAULT_MAX_EXAMPLES,
    precision: str = "bfloat16",
    # keep-sorted end,
    **kwargs,
) -> lit_notebook.LitWidget:
  """Initializes a LIT widget for prompt debugging in a notebook.
  Args:
    datasets_config: A list of dataset names and paths to load from, as
      "dataset:path", where path can be a URL, a local file path, or the name of
      a preset for the configured deep learning framework.
    models_config: A list of model names and paths to load from, as
      "model:path", where path can be a URL, a local file path, or the name of a
      preset for the configured deep learning framework.
    batch_size: The number of examples the model will process per batch.
    dl_framework: The deep learning framework that loads and runs the model on
      the runtime, `models_config.path` incompatibilities will result in errors.
    dl_runtime: The deep learning runtime that the model runs on, either
      "tensorflow" or "torch". All loaded models will use the same runtime,
      incompatibilities will result in errors.
    max_examples: Maximum number of examples in each loaded dataset.
    precision: Floating point precision for the models, either `bfloat16` or
      `float32`.
    **kwargs: Additional keyword arguments passed to the LitWidget. See also
      LitApp for additinoal keyword arguments accepted by the LitWidget.
  Returns:
    A LitWidget with the configured models and datasets. Call `widget.render()`
    to load the data and render the UI.
  """
  return lit_notebook.LitWidget(
      models=models.get_models(
          models_config=models_config,
          dl_framework=dl_framework,
          dl_runtime=dl_runtime,
          precision=precision,
          batch_size=batch_size,
      ),
      datasets=datasets.get_datasets(
          datasets_config=datasets_config, max_examples=max_examples
      ),
      layouts=layouts.PROMPT_DEBUGGING_LAYOUTS,
      default_layout=layouts.LEFT_RIGHT,
      model_loaders=models.get_model_loaders(
          dl_framework=dl_framework,
          dl_runtime=dl_runtime,
          batch_size=batch_size,
          max_length=models.DEFAULT_SEQUENCE_LENGTH,
      ),
      dataset_loaders=datasets.get_dataset_loaders(),
      **kwargs,
  )

================
File: lit_nlp/examples/prompt_debugging/server.py
================
r"""Server for sequence salience with a left-to-right language model.
To use with the Gemma, Llama, or Mistral models, install the latest versions of
Keras, KerasNLP, and/or HuggingFace Transformers:
  pip install keras>=3.1.0 keras-nlp>=0.9.0 transformers>=4.38.0
To run with the default configuration (Gemma on TensorFlow via Keras):
  python3 -m lit_nlp.examples.prompt_debugging.server -- \
    --models=gemma:gemma_1.1_instruct_2b_en \
    --alsologtostderr
MODELS:
We strongly recommend a GPU or other accelerator to run this server with LLMs.
The table below shows the model names and presets for common models. Use these
to parameterize the --models flag with comma-separated `{model}:{preset}`
strings, and remember the number of models loaded will be limited by the memory
available on your accelerator.
| Model   | dl_framework | dl_backend=tensorflow Preset | dl_backend=torch Preset              |
| ------- | ------------ | ---------------------------- | ------------------------------------ |
| Gemma   | kerasnlp     | gemma_1.1_instruct_7b_en     | gemma_1.1_instruct_7b_en             |
| Gemma   | transformers | Unavailable                  | google/gemma-1.1-7b-it               |
| Llama 2 | kerasnlp     | llama2_instruct_7b_en        | llama2_instruct_7b_en                |
| Llama 2 | transformers | Unavailable                  | meta-llama/Llama-2-7b-hf             |
| Mistral | kerasnlp     | mistral_instruct_7b_en       | mistral_instruct_7b_en               |
| Mistral | transformers | Unavailable                  | mistralai/Mistral-7B-Instruct-v0.2   |
Additional model presets can be found at the following locations, though
compatibility with the LIT model wrappers is not guaranteed:
* KerasNLP: https://keras.io/api/keras_nlp/models/
* HuggingFace Transformers: https://huggingface.co/models
DATASETS:
By default this includes a small set of sample prompts. You can load your own
examples using the --datasets flag or through the "Configure" menu in the UI.
"""
from collections.abc import Sequence
import sys
from typing import Optional
from absl import app
from absl import flags
from absl import logging
from lit_nlp import dev_server
from lit_nlp import server_flags
from lit_nlp.examples.prompt_debugging import datasets
from lit_nlp.examples.prompt_debugging import layouts
from lit_nlp.examples.prompt_debugging import models
# The following flags enable command line configuration datasets.
_DATASETS = flags.DEFINE_list(
    "datasets",
    datasets.DEFAULT_DATASETS,
    "Datasets to load, as <name>:<path>. Format should be either .jsonl where"
    " each record contains 'prompt' and optional 'target' and 'source' fields,"
    " or a plain text file with one prompt per line.",
)
_MAX_EXAMPLES = flags.DEFINE_integer(
    "max_examples",
    datasets.DEFAULT_MAX_EXAMPLES,
    (
        "Maximum number of examples to load from each evaluation set. Set to"
        " None to load the full set."
    ),
)
# The following flags enable command line configuration of models.
_BATCH_SIZE = flags.DEFINE_integer(
    "batch_size",
    models.DEFAULT_BATCH_SIZE,
    "The number of examples to process per batch.",
)
_SUPPORTED_FRAMEWORKS = ("kerasnlp", "transformers")
_DL_FRAMEWORK = flags.DEFINE_enum(
    "dl_framework",
    models.DEFAULT_DL_FRAMEWORK,
    _SUPPORTED_FRAMEWORKS,
    "The deep learning framework that loads and runs the model on the backend."
    " This server will attempt to load all models specified by the --models"
    " flag with the configured framework, incompatibilities will result in"
    " errors.",
)
_DL_RUNTIME = flags.DEFINE_enum(
    "dl_runtime",
    models.DEFAULT_DL_RUNTIME,
    # TODO(b/333373960): Add "jax" once JAX salience is supported.
    ("tensorflow", "torch"),
    "The deep learning backend framework that the model runs on. All models"
    " loaded by this server will use the same backend, incompatibilities will"
    " result in errors.",
)
_MODELS = flags.DEFINE_list(
    "models",
    models.DEFAULT_MODELS,
    "Models to load, as <name>:<path>. Path can be a URL, a local file path, or"
    " the name of a preset for the configured Deep Learning framework (either"
    " KerasNLP or HuggingFace Transformers; see --dl_framework for more). This"
    " demo is tested with Gemma, GPT2, Llama, and Mistral on all supported"
    " --dl_framework values. Other models should work, but adjustments might be"
    " needed on their tokenizers (e.g., to define custom pad_token"
    " when eos_token is not available to use as pad_token).",
)
_PRECISION = flags.DEFINE_enum(
    "precision",
    models.DEFAULT_PRECISION,
    ("bfloat16", "float32"),
    "Floating point precision for the models, only `bfloat16` and `float32` are"
    " supported at this time.",
)
_SEQUENCE_LENGTH = flags.DEFINE_integer(
    "sequence_length",
    models.DEFAULT_SEQUENCE_LENGTH,
    "The maximum sequence length of the input prompt + generated text",
)
_FLAGS = flags.FLAGS
_FLAGS.set_default("development_demo", True)
_FLAGS.set_default("page_title", "LM Prompt Debugging")
_FLAGS.set_default("default_layout", layouts.THREE_PANEL)
_SPLASH_SCREEN_DOC = """
# Language Model Salience
To begin, select an example, then click the segment(s) (tokens, words, etc.)
of the output that you would like to explain. Preceding segments(s) will be
highlighted according to their importance to the selected target segment(s),
with darker colors indicating a greater influence (salience) of that segment on
the model's likelihood of the target segment.
"""
def get_wsgi_app() -> Optional[dev_server.LitServerType]:
  """Return WSGI app for container-hosted demos."""
  _FLAGS.set_default("server_type", "external")
  _FLAGS.set_default("demo_mode", True)
  # Parse flags without calling app.run(main), to avoid conflict with
  # gunicorn command line flags.
  unused = flags.FLAGS(sys.argv, known_only=True)
  if unused:
    logging.info("lm_demo:get_wsgi_app() called with unused args: %s", unused)
  return main([])
def main(argv: Sequence[str]) -> Optional[dev_server.LitServerType]:
  if len(argv) > 1:
    raise app.UsageError("Too many command-line arguments.")
  lit_demo = dev_server.Server(
      models=models.get_models(
          models_config=_MODELS.value,
          dl_framework=_DL_FRAMEWORK.value,
          dl_runtime=_DL_RUNTIME.value,
          precision=_PRECISION.value,
          batch_size=_BATCH_SIZE.value,
          max_length=_SEQUENCE_LENGTH.value,
      ),
      datasets=datasets.get_datasets(
          datasets_config=_DATASETS.value, max_examples=_MAX_EXAMPLES.value
      ),
      layouts=layouts.PROMPT_DEBUGGING_LAYOUTS,
      model_loaders=models.get_model_loaders(
          dl_framework=_DL_FRAMEWORK.value,
          dl_runtime=_DL_RUNTIME.value,
          batch_size=_BATCH_SIZE.value,
          max_length=_SEQUENCE_LENGTH.value,
      ),
      dataset_loaders=datasets.get_dataset_loaders(),
      onboard_start_doc=_SPLASH_SCREEN_DOC,
      **server_flags.get_flags(),
  )
  return lit_demo.serve()
if __name__ == "__main__":
  app.run(main)

================
File: lit_nlp/examples/prompt_debugging/transformers_lms_int_test.py
================
from absl.testing import absltest
from absl.testing import parameterized
from lit_nlp.examples.prompt_debugging import transformers_lms
import numpy as np
from transformers import tokenization_utils
_MAX_LENGTH = 32
def _tokenize_text(
    text: str, tokenizer: tokenization_utils.PreTrainedTokenizer, framework: str
) -> tokenization_utils.BatchEncoding:
  return_tensors_type = (
      transformers_lms._HF_PYTORCH
      if framework == transformers_lms.MLFramework.PT.value
      else transformers_lms._HF_TENSORFLOW
  )
  return tokenizer(
      text,
      return_tensors=return_tensors_type,
      add_special_tokens=True,
  )
def _get_text_mean_embeddings(
    text: str, model: transformers_lms.HFBaseModel, framework: str
) -> np.ndarray:
  tokens = _tokenize_text(
      text=text, tokenizer=model.tokenizer, framework=framework
  )
  embeddings = model.embedding_table(tokens["input_ids"])
  if framework == transformers_lms.MLFramework.PT.value:
    embeddings = embeddings.detach()
  mean_embeddings = np.mean(embeddings.numpy()[0], axis=0)
  return mean_embeddings
class TransformersLMSGeneration(parameterized.TestCase):
  """Test that model classes can predict."""
  @parameterized.named_parameters(
      dict(
          testcase_name="tensorflow_framework",
          framework=transformers_lms.MLFramework.TF.value,
          model_path="https://storage.googleapis.com/what-if-tool-resources/lit-models/gpt2.tar.gz",
      ),
      dict(
          testcase_name="pytorch_framework",
          framework=transformers_lms.MLFramework.PT.value,
          model_path="https://storage.googleapis.com/what-if-tool-resources/lit-models/gpt2-pt.tar.gz",
      ),
  )
  def test_gpt2_generation_output(self, framework, model_path):
    model = transformers_lms.HFGenerativeModel(
        model_name_or_path=model_path,
        framework=framework,
        max_length=_MAX_LENGTH,
    )
    model_in = [{"prompt": "Today is"}, {"prompt": "What is the color of"}]
    model_out = list(model.predict(model_in))
    with self.subTest(name="model_input_length_matches_output_length"):
      self.assertLen(model_out, 2)
    with self.subTest(name="model_output_has_expected_spec_keys"):
      expected_output_keys = sorted(model.output_spec().keys())
      for cur_output in model_out:
        self.assertSequenceEqual(
            sorted(cur_output.keys()), expected_output_keys
        )
    with self.subTest(
        name="model_output_prompt_and_response_embeddings_match_those_computed_from_embedding_table"
    ):
      for cur_input, cur_output in zip(model_in, model_out):
        expected_input_embeddings = _get_text_mean_embeddings(
            text=cur_input["prompt"], model=model, framework=framework
        )
        expected_output_embeddings = _get_text_mean_embeddings(
            text=cur_output["response"], model=model, framework=framework
        )
        np.testing.assert_array_almost_equal(
            expected_input_embeddings,
            cur_output["prompt_embeddings"],
        )
        np.testing.assert_array_almost_equal(
            expected_output_embeddings,
            cur_output["response_embeddings"],
        )
  @parameterized.named_parameters(
      dict(
          testcase_name="tensorflow_framework",
          framework=transformers_lms.MLFramework.TF.value,
          model_path="https://storage.googleapis.com/what-if-tool-resources/lit-models/gpt2.tar.gz",
      ),
      dict(
          testcase_name="pytorch_framework",
          framework=transformers_lms.MLFramework.PT.value,
          model_path="https://storage.googleapis.com/what-if-tool-resources/lit-models/gpt2-pt.tar.gz",
      ),
  )
  def test_gpt2_batched_generation_has_correct_input_and_output_token_lengths(
      self, framework, model_path
  ):
    model = transformers_lms.HFGenerativeModel(
        model_name_or_path=model_path,
        framework=framework,
        max_length=_MAX_LENGTH,
    )
    model_in = [{"prompt": "Today is"}, {"prompt": "What is the color of"}]
    batched_outputs = model._get_batched_outputs(model_in)
    tokenized_inputs = [
        _tokenize_text(
            text=input_dict["prompt"],
            tokenizer=model.tokenizer,
            framework=framework,
        )
        for input_dict in model_in
    ]
    expected_input_token_len = np.array([
        tokenized_input["input_ids"].shape[1]
        for tokenized_input in tokenized_inputs
    ])
    expected_output_token_len = np.full(
        (len(model_in),), _MAX_LENGTH - np.max(expected_input_token_len)
    )
    np.testing.assert_array_equal(
        expected_input_token_len, batched_outputs["ntok_in"]
    )
    np.testing.assert_array_equal(
        expected_output_token_len, batched_outputs["ntok_out"]
    )
if __name__ == "__main__":
  absltest.main()

================
File: lit_nlp/examples/prompt_debugging/transformers_lms.py
================
"""Wrapper for HuggingFace models in LIT.
Supported models include Gemma, GPT-2, Llama, Mistral, etc.
This wrapper loads a model into memory and implements the a number of helper
functions to predict a batch of examples and extract information such as
hidden states and attention.
"""
from collections.abc import Sequence
import enum
import functools
from typing import Any, Mapping
from absl import logging
from lit_nlp.api import model as lit_model
from lit_nlp.api import types as lit_types
from lit_nlp.examples.prompt_debugging import constants as pd_constants
from lit_nlp.examples.prompt_debugging import utils as pd_utils
from lit_nlp.lib import file_cache
from lit_nlp.lib import utils
import numpy as np
import transformers
# pylint: disable=g-import-not-at-top
# pytype: disable=import-error
try:
  import tensorflow as tf
except (ModuleNotFoundError, ImportError):
  logging.warning("TensorFlow is not available.")
try:
  import torch
except (ModuleNotFoundError, ImportError):
  logging.warning("PyTorch is not available.")
# pytype: enable=import-error
# pylint: enable=g-import-not-at-top
_PYTORCH = "torch"
_TENSORFLOW = "tensorflow"
# HuggingFace uses two letter abbreviations for pytorch and tensorflow.
_HF_PYTORCH = "pt"
_HF_TENSORFLOW = "tf"
@enum.unique
class MLFramework(enum.Enum):
  """The supported deep learning frameworks."""
  PT = _PYTORCH
  TF = _TENSORFLOW
SUPPORTED_ML_RUNTIMES = [framework.value for framework in MLFramework]
class HFBaseModel(lit_model.BatchedModel):
  """Base class for HF generative, salience, tokenizer model wrappers."""
  # Enum str values for entries in MLFramework, used for init_spec and logging.
  @property
  def num_layers(self):
    return self.model.config.n_layer
  @classmethod
  def init_spec(cls) -> lit_model.Spec:
    return {
        "model_name_or_path": lit_types.String(default="gpt2"),
        "batch_size": lit_types.Integer(default=6, min_val=1, max_val=64),
        "framework": lit_types.CategoryLabel(vocab=SUPPORTED_ML_RUNTIMES),
    }
  def __init__(
      self,
      model_name_or_path="gpt2",
      batch_size=6,
      framework=_PYTORCH,
      model=None,
      tokenizer=None,
      **unused_kw,
  ):
    """Constructor for HF base model wrappers.
    Note: args "model" and "tokenizer" take priority if both are specified.
    Otherwise, "model_name_or_path" is used to initialize the model and
    tokenizer.
    This class supports common HF transformer models such as GPT2, Llama,
    Mistral, etc.
    Args:
      model_name_or_path: gpt2, gpt2-medium, gpt2-large, distilgpt2,
        meta-llama/Llama-2-7b-hf, mistralai/Mistral-7B-v0.1, etc.
      batch_size: the number of items to process per `predict_minibatch` call.
      framework: the deep learning framework, only "tensorflow" and "torch"
        are supported.
      model: an initialized transformer model.
      tokenizer: an initialized tokenizer.
    """
    super().__init__()
    if model is not None and tokenizer is not None:
      self.model = model
      self.tokenizer = tokenizer
      # Check if the HF model object's framework is supported here.
      if model.framework == _HF_PYTORCH:
        self.framework = MLFramework.PT
      elif model.framework == _HF_TENSORFLOW:
        self.framework = MLFramework.TF
      else:
        raise ValueError(
            f"The HuggingFace model framework `{model.framework}` is not"
            " supported."
        )
    else:
      # Normally path is a directory; if it's an archive file, download and
      # extract to the transformers cache.
      if (
          is_tar_gz := model_name_or_path.endswith(".tar.gz")
      ) or file_cache.is_remote(model_name_or_path):
        model_name_or_path = file_cache.cached_path(
            model_name_or_path,
            extract_compressed_file=is_tar_gz,
        )
      # Note: we need to left-pad for generation to work properly.
      # Other modes such as scoring and salience should handle this as well;
      # see example in HFSalienceModel._postprocess().
      self.tokenizer = transformers.AutoTokenizer.from_pretrained(
          model_name_or_path,
          use_fast=False,
          padding_side="left",
      )
      # Set this after init, as if pad_token= is passed to
      # AutoTokenizer.from_pretrained() above it will create a new token with
      # with id = max_vocab_length and cause out-of-bounds errors in
      # the embedding lookup.
      if framework == _PYTORCH:
        auto_model = transformers.AutoModelForCausalLM
        self.framework = MLFramework.PT
      elif framework == _TENSORFLOW:
        auto_model = transformers.TFAutoModelForCausalLM
        self.framework = MLFramework.TF
      else:
        raise ValueError(
            f"The provided value `{framework}` for arg `framework` is not"
            f" supported, please choose from {SUPPORTED_ML_RUNTIMES}."
        )
      self.model = auto_model.from_pretrained(
          model_name_or_path,
          output_hidden_states=True,
          output_attentions=False,
      )
    if self.framework == MLFramework.PT:
      self.device = "cuda:0" if torch.cuda.is_available() else "cpu"
      self.model = self.model.to(self.device)
    self.embedding_table = self.model.get_input_embeddings()
    self.tokenizer.pad_token = self.tokenizer.eos_token
    self.batch_size = batch_size
  @property
  def pad_left(self):
    return self.tokenizer.padding_side == "left"
  @classmethod
  def from_loaded(cls, existing: "HFBaseModel", *args, **kw):
    """Share weights and underlying HF model with another instance."""
    return cls(model=existing.model, tokenizer=existing.tokenizer, *args, **kw)
  def clean_subword_token(self, tok):
    # For GPT2 tokenizer.
    tok = tok.replace("", "\n")  # newlines
    tok = tok.replace("", "")  # start of word -> magic underscore
    # For SentencePiece Tokenizer.
    tok = tok.replace("<0x0A>", "\n")  # newlines
    return tok
  def ids_to_clean_tokens(self, ids: Sequence[int]) -> list[str]:
    tokens = self.tokenizer.convert_ids_to_tokens(ids)
    return [self.clean_subword_token(t) for t in tokens]
  def max_minibatch_size(self) -> int:
    # The BatchedModel base class handles batching automatically in the
    # implementation of predict(), and uses this value as the batch size.
    return self.batch_size
  def input_spec(self):
    return pd_constants.INPUT_SPEC
class HFGenerativeModel(HFBaseModel):
  """Wrapper for a HF Transformer model that generates texts.
  This class loads a tokenizer and model using the Huggingface library and
  provides the LIT-required functions to generate text responses given input
  prompts.
  Note that the default model generation config is used such that the response
  is produced using multinomial sampling.
  """
  @classmethod
  def init_spec(cls) -> lit_model.Spec:
    return super().init_spec() | {
        "max_length": lit_types.Integer(default=512, min_val=1, max_val=2048)
    }
  def __init__(self, *args, max_length=512, **kw):
    """Constructor for HFGenerativeModel.
    Args:
      *args: as to HFBaseModel.__init__
      max_length: the maximum length the generated tokens can have. Corresponds
        to the length of the input prompt + max_new_tokens.
      **kw: as to HFBaseModel.__init__
    """
    super().__init__(*args, **kw)
    self.max_length = max_length
  def _postprocess(self, preds: Mapping[str, Any]) -> Mapping[str, Any]:
    """Post-process single-example preds. Operates on numpy arrays.
    Args:
      preds: a dict of the model outputs, including the response text, number of
        input and output tokens, and the embeddings of the input and output
        tokens (merged into a single array).
    Returns:
      a dict of the processed model outputs, including the response texts and
        embeddings of the input and output tokens (separated into two arrays).
    """
    # TODO(b/324957491): return actual decoder scores for each generation. For
    # now, we only output GeneratedText.
    processed_preds = {}
    processed_preds[pd_constants.FieldNames.RESPONSE] = preds[
        pd_constants.FieldNames.RESPONSE
    ]
    ntok_in = preds["ntok_in"]
    ntok_out = preds["ntok_out"]
    embs = preds["embs"]
    assert embs.shape[0] >= ntok_in + ntok_out
    # Mean-pool over input tokens.
    processed_preds[pd_constants.FieldNames.PROMPT_EMBEDDINGS] = np.mean(
        embs[-(ntok_out + ntok_in) : -ntok_out], axis=0
    )
    # Mean-pool over output (generated) tokens.
    # TODO(b/324957491): slice this to only "real" output tokens,
    # if generation length < max generation length.
    processed_preds[pd_constants.FieldNames.RESPONSE_EMBEDDINGS] = np.mean(
        embs[-ntok_out:], axis=0
    )
    return processed_preds
  def _get_batched_outputs(
      self, inputs: Sequence[Mapping[str, lit_types.TextSegment]]
  ) -> Mapping[str, Any]:
    """Returns the batched outputs generated by the model for the given inputs.
    Args:
      inputs: model inputs containing text prompts.
    Returns:
      a dict of the model outputs, including the generated texts and auxiliary
        data in numpy arrays (could come from torch or tensorflow, depending on
        the transformer backend).
    """
    encoded_inputs = self.tokenizer(
        [ex["prompt"] for ex in inputs],
        return_tensors=(
            _HF_PYTORCH if self.framework == MLFramework.PT else _HF_TENSORFLOW
        ),
        add_special_tokens=True,
        padding="longest",
        truncation="longest_first",
    )
    batch_size, ntok_in = encoded_inputs["input_ids"].shape
    if self.framework == MLFramework.PT:
      encoded_inputs = encoded_inputs.to(self.device)
    outputs = self.model.generate(**encoded_inputs, max_length=self.max_length)
    if isinstance(outputs, transformers.utils.ModelOutput):
      outputs = outputs.sequences
    ntok_out = outputs.shape[1] - ntok_in
    responses = self.tokenizer.batch_decode(
        outputs[:, -ntok_out:], skip_special_tokens=True
    )
    if self.framework == MLFramework.PT:
      with torch.no_grad():
        # Input embeddings: <float>[batch_size, num_tokens, emb_dim]
        embeddings = self.embedding_table(outputs).cpu().to(torch.float)
    else:
      embeddings = self.embedding_table(outputs)
    return {
        "embs": embeddings.numpy(),
        "ntok_in": np.array((batch_size, ntok_in)),
        "ntok_out": np.full((batch_size,), ntok_out),
        pd_constants.FieldNames.RESPONSE: responses,
    }
  ##
  # LIT API implementations
  def predict_minibatch(self, inputs):
    batched_outputs = self._get_batched_outputs(inputs)
    # Split up batched outputs, then post-process each example.
    unbatched_outputs = utils.unbatch_preds(batched_outputs)
    return map(self._postprocess, unbatched_outputs)
  def output_spec(self) -> lit_types.Spec:
    return (
        pd_constants.OUTPUT_SPEC_GENERATION
        | pd_constants.OUTPUT_SPEC_GENERATION_EMBEDDINGS
    )
class HFSalienceModel(HFBaseModel):
  """Wrapper for a HF Transformer model that computes input (token) salience."""
  def _left_pad_target_masks(self, seq_length, target_masks):
    """Pads target masks (from left) to the desired sequence length.
    Args:
      seq_length: desired length of the padded masks.
      target_masks: list(array_like) of binary (0/1) masks for each input.
    Returns:
      Numpy array of the padded masks at the desired sequence length.
    """
    # It doesn't make sense to interpret the first token, since it is not ever
    # predicted. But we need to ensure that the mask[0] is zero, so it doesn't
    # cause problems when 'rolled' to the last position below.
    modified_masks = [[0] + list(mask[1:]) for mask in target_masks]
    pad_fn = functools.partial(
        utils.pad1d,
        min_len=seq_length,
        max_len=seq_length,
        pad_val=0,
        pad_left=self.pad_left,
    )
    padded_target_masks = np.stack(
        [pad_fn(mask) for mask in modified_masks],
        axis=0,
    )
    return padded_target_masks
  def _pred_tf(self, encoded_inputs, target_masks):
    """Predicts one batch of tokenized text using TF.
    Also performs some batch-level post-processing in TF.
    Single-example postprocessing is done in _postprocess(), and operates on
    numpy arrays.
    Args:
      encoded_inputs: output of self.tokenizer()
      target_masks: list(array_like) of binary (0/1) masks for each input
    Returns:
      payload: Dictionary with items described above, each as single Tensor.
    """
    input_ids = encoded_inputs["input_ids"]
    # <tf.int32>[batch_size, num_tokens]; ignore the last one in each row.
    target_ids = tf.roll(input_ids, shift=-1, axis=1)
    ##
    # Process target masks
    padded_target_masks = tf.constant(
        self._left_pad_target_masks(target_ids.shape[1], target_masks),
        dtype=tf.bool,
    )
    # Shift masks back so they align with target_ids.
    loss_mask = tf.roll(padded_target_masks, shift=-1, axis=1)
    with tf.GradientTape(watch_accessed_variables=False) as tape:
      # We need to run the embedding layer ourselves so we can trace it.
      # See here for how the model normally does this:
      # https://github.com/huggingface/transformers/blob/v4.29.2/src/transformers/models/gpt2/modeling_tf_gpt2.py#L450
      embs = self.embedding_table(input_ids)
      tape.watch(embs)
      out = self.model(
          input_ids=None,
          inputs_embeds=embs,
          attention_mask=encoded_inputs["attention_mask"],
      )
      loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(
          from_logits=True, reduction="none"
      )
      # <tf.float>[batch_size, num_tokens]
      per_token_loss = loss_fn(target_ids, out.logits)
      masked_loss = per_token_loss * tf.cast(loss_mask, per_token_loss.dtype)
    grads = tape.gradient(
        masked_loss, embs
    )  # <tf.float>[batch_size, num_tokens, hdim]
    grad_l2 = tf.norm(grads, axis=2)  # <tf.float>[batch_size, num_tokens]
    grad_dot_input = tf.reduce_sum(
        grads * embs, axis=2
    )  # <tf.float>[batch_size, num_tokens]
    batched_outputs = {
        "input_ids": input_ids,
        "attention_mask": encoded_inputs["attention_mask"],
        pd_constants.FieldNames.GRAD_NORM: grad_l2,
        pd_constants.FieldNames.GRAD_DOT_INPUT: grad_dot_input,
    }
    return batched_outputs
  def _pred_pt(self, encoded_inputs, target_masks):
    """Predicts one batch of tokenized text using PyTorch.
    Also performs some batch-level post-processing in PyTorch.
    Single-example postprocessing is done in _postprocess(), and operates on
    numpy arrays.
    Args:
      encoded_inputs: output of self.tokenizer()
      target_masks: list(array_like) of binary (0/1) masks for each input
    Returns:
      payload: Dictionary with items described above, each as single Tensor.
    """
    encoded_inputs = encoded_inputs.to(self.device)
    input_ids = encoded_inputs["input_ids"]
    attention_mask = encoded_inputs["attention_mask"]
    # [batch_size, num_tokens]; ignore the last one in each row.
    target_ids = torch.roll(input_ids, shifts=-1, dims=1).to(self.device)
    ##
    # Process target masks
    padded_target_masks = torch.tensor(
        self._left_pad_target_masks(target_ids.shape[1], target_masks)
    ).bool()
    loss_mask = torch.roll(padded_target_masks, shifts=-1, dims=1).to(
        self.device
    )
    embs = self.embedding_table(input_ids)
    outs = self.model(
        input_ids=None,
        inputs_embeds=embs,
        attention_mask=attention_mask,
    )
    loss_func = torch.nn.CrossEntropyLoss(reduction="none")
    # Need to reshape outs.logits from [batch_size, num_tokens, vocab_size]
    # to [batch_size, vocab_size, num_tokens] so the last dimension matches that
    # of target_ids with dimension [batch_size, num_tokens].
    per_token_loss = loss_func(outs.logits.permute(0, 2, 1), target_ids)
    masked_loss = per_token_loss * loss_mask
    # returned gradients are wrapped in a single item tuple.
    grads = torch.autograd.grad(
        masked_loss, embs, grad_outputs=torch.ones_like(masked_loss)
    )[0]
    # Remove the grad function from embs.
    embs = embs.detach()
    grad_l2 = torch.norm(grads, dim=2)  # [batch_size, num_tokens]
    grad_dot_input = torch.sum(grads * embs, axis=2)  # [batch_size, num_tokens]
    batched_outputs = {
        "input_ids": input_ids.cpu().to(torch.int),
        "attention_mask": attention_mask.cpu().to(torch.int),
        pd_constants.FieldNames.GRAD_NORM: grad_l2.cpu().to(torch.float),
        pd_constants.FieldNames.GRAD_DOT_INPUT: grad_dot_input.cpu().to(
            torch.float
        ),
    }
    return batched_outputs
  def _postprocess(self, preds):
    """Post-process single-example preds. Operates on numpy arrays."""
    # Be sure to cast to bool, otherwise this will select integer positions 0, 1
    # rather than acting as a boolean mask.
    mask = preds.pop("attention_mask").astype(bool)
    ids = preds.pop("input_ids")[mask]
    preds[pd_constants.FieldNames.TOKENS] = self.ids_to_clean_tokens(ids)
    for key in utils.find_spec_keys(self.output_spec(), lit_types.TokenScores):
      preds[key] = preds[key][mask]
    # First token (usually <s>) is not actually predicted, so return 0 for loss.
    # preds["token_loss"][0] = 0
    return preds
  # LIT API implementations
  def predict_minibatch(self, inputs):
    """Predict on a single minibatch of examples."""
    # Preprocess inputs.
    texts = [
        ex[pd_constants.FieldNames.PROMPT]
        + ex.get(pd_constants.FieldNames.TARGET, "")
        for ex in inputs
    ]
    encoded_inputs = self.tokenizer(
        texts,
        return_tensors=_HF_PYTORCH
        if self.framework == MLFramework.PT
        else _HF_TENSORFLOW,
        add_special_tokens=True,
        padding="longest",
        truncation="longest_first",
    )
    target_masks = [
        ex.get(pd_constants.FieldNames.TARGET_MASK, []) for ex in inputs
    ]
    # Get the predictions.
    if self.framework == MLFramework.PT:
      batched_outputs = self._pred_pt(encoded_inputs, target_masks)
    else:
      batched_outputs = self._pred_tf(encoded_inputs, target_masks)
    # Convert to numpy for post-processing.
    detached_outputs = {k: v.numpy() for k, v in batched_outputs.items()}
    # Split up batched outputs, then post-process each example.
    unbatched_outputs = utils.unbatch_preds(detached_outputs)
    return map(self._postprocess, unbatched_outputs)
  def input_spec(self):
    return super().input_spec() | pd_constants.INPUT_SPEC_SALIENCE
  def output_spec(self) -> lit_types.Spec:
    return pd_constants.OUTPUT_SPEC_SALIENCE
class HFTokenizerModel(HFBaseModel):
  """Wrapper to run only the tokenizer.
  Should exactly match tokens from HFSalienceModel.
  """
  def _postprocess(self, preds):
    """Post-process single-example preds. Operates on numpy arrays."""
    # Be sure to cast to bool, otherwise this will select intger positions 0, 1
    # rather than acting as a boolean mask.
    mask = preds.pop("attention_mask").astype(bool)
    ids = preds.pop("input_ids")[mask]
    preds[pd_constants.FieldNames.TOKENS] = self.ids_to_clean_tokens(ids)
    return preds
  # LIT API implementations
  def predict_minibatch(self, inputs):
    """Predict on a single minibatch of examples."""
    # Preprocess inputs.
    texts = [
        ex[pd_constants.FieldNames.PROMPT]
        + ex.get(pd_constants.FieldNames.TARGET, "")
        for ex in inputs
    ]
    encoded_inputs = self.tokenizer(
        texts,
        return_tensors=_HF_PYTORCH
        if self.framework == MLFramework.PT
        else _HF_TENSORFLOW,
        add_special_tokens=True,
        padding="longest",
        truncation="longest_first",
    )
    batched_outputs = {
        "input_ids": encoded_inputs["input_ids"],
        "attention_mask": encoded_inputs["attention_mask"],
    }
    # Convert to numpy for post-processing.
    detached_outputs = {k: v.numpy() for k, v in batched_outputs.items()}
    # Split up batched outputs, then post-process each example.
    unbatched_outputs = utils.unbatch_preds(detached_outputs)
    return map(self._postprocess, unbatched_outputs)
  def output_spec(self) -> lit_types.Spec:
    return pd_constants.OUTPUT_SPEC_TOKENIZER
def initialize_model_group_for_salience(
    new_name: str, **kw
) -> lit_model.ModelMap:
  """Creates '{name}' and '_{name}_salience' and '_{name}_tokenizer'."""
  max_length = kw.pop("max_length", 512)
  salience_name, tokenizer_name = pd_utils.generate_model_group_names(new_name)
  generation_model = HFGenerativeModel(max_length=max_length, **kw)
  salience_model = HFSalienceModel.from_loaded(generation_model)
  tokenizer_model = HFTokenizerModel.from_loaded(generation_model)
  return {
      new_name: generation_model,
      salience_name: salience_model,
      tokenizer_name: tokenizer_model,
  }

================
File: lit_nlp/examples/prompt_debugging/utils_test.py
================
from absl.testing import absltest
from absl.testing import parameterized
from lit_nlp.examples.prompt_debugging import utils
class UtilsTest(parameterized.TestCase):
  @parameterized.named_parameters(
      dict(
          testcase_name="empty_name",
          name="",
          expected_salience_name="__salience",
          expected_tokenizer_name="__tokenizer",
      ),
      dict(
          testcase_name="known_name",
          name="gemma",
          expected_salience_name="_gemma_salience",
          expected_tokenizer_name="_gemma_tokenizer",
      ),
      dict(
          testcase_name="custom_name_with_spaces",
          name="my model",
          expected_salience_name="_my model_salience",
          expected_tokenizer_name="_my model_tokenizer",
      ),
  )
  def test_generate_model_group_names(
      self, name, expected_salience_name, expected_tokenizer_name
  ):
    salience_name, tokenizer_name = utils.generate_model_group_names(name)
    self.assertEqual(salience_name, expected_salience_name)
    self.assertEqual(tokenizer_name, expected_tokenizer_name)
if __name__ == "__main__":
  absltest.main()

================
File: lit_nlp/examples/prompt_debugging/utils.py
================
"""Utility functions for Prompt Debugging use cases."""
def generate_model_group_names(name: str) -> tuple[str, str]:
  return f"_{name}_salience", f"_{name}_tokenizer"

================
File: lit_nlp/examples/tools/__init__.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

================
File: lit_nlp/examples/tools/glue_trainer.py
================
r"""Lightweight trainer script to fine-tune on a GLUE or GLUE-like task.
Usage:
  python -m lit_nlp.examples.tools.glue_trainer \
    --encoder_name=bert-base-uncased --task=sst2 \
    --train_path=/path/to/save/model
For a quick start, use:
   --encoder_name="google/bert_uncased_L-2_H-128_A-2"
This will train a "bert-tiny" model from https://arxiv.org/abs/1908.08962,
which should run in under five minutes on a single GPU, and give validation
accuracy in the low 80s on SST-2.
Note: you don't have to use this trainer to use LIT; the classifier
implementation is just a wrapper around HuggingFace Transformers, using
AutoTokenizer, AutoConfig, and TFAutoModelForSequenceClassification, and can
load anything compatible with those classes.
"""
from collections.abc import Sequence
import os
from absl import app
from absl import flags
from absl import logging
from lit_nlp.examples.glue import data as glue_data
from lit_nlp.examples.glue import models as glue_models
from lit_nlp.lib import serialize
import tf_keras as keras
os.environ["TF_USE_LEGACY_KERAS"] = "1"
_ENCODER_NAME = flags.DEFINE_string(
    "encoder_name", "bert-base-uncased",
    "Model name or path to pretrained (base) encoder.")
_TASK = flags.DEFINE_string("task", "sst2", "Name of task to fine-tune on.")
_TRAIN_PATH = flags.DEFINE_string("train_path", "/tmp/hf_demo",
                                  "Path to save fine-tuned model.")
_NUM_EPOCHS = flags.DEFINE_integer(
    "num_epochs", 3, "Number of epochs to train for.", lower_bound=1)
_SAVE_INTERMEDIATES = flags.DEFINE_bool(
    "save_intermediates", False,
    "If true, save intermediate weights after each epoch.")
FLAGS = flags.FLAGS
def history_to_dict(keras_history):
  return {
      "epochs": keras_history.epoch,
      "history": keras_history.history,
      "params": keras_history.params,
      "optimizer_params": keras_history.model.optimizer.get_config(),
  }
class EpochSaverCallback(keras.callbacks.Callback):
  """Save model at the beginning of training and after every epoch.
  Similar to keras.callbacks.ModelCheckpoint, but this allows us to specify
  a custom save fn to call, such as the HuggingFace model.save() which writes
  .h5 files and config information.
  """
  def __init__(self, save_path_base: str, save_fn=None):
    super().__init__()
    self.save_path_base = save_path_base
    self.save_fn = save_fn or self.model.save
  def on_train_begin(self, logs=None):
    self.on_epoch_end(-1, logs=logs)  # write epoch-0
  def on_epoch_end(self, epoch, logs=None):
    # Save path 1-indexed = # of completed epochs.
    save_path = os.path.join(self.save_path_base, f"epoch-{epoch+1}")
    self.save_fn(save_path)
def train_and_save(model,
                   train_data,
                   val_data,
                   train_path,
                   save_intermediates=False,
                   **train_kw):
  """Run training and save model."""
  # Set up logging for TensorBoard. To view, run:
  #   tensorboard --log_dir=<train_path>/tensorboard
  keras_callbacks = [
      keras.callbacks.TensorBoard(
          log_dir=os.path.join(train_path, "tensorboard")
      )
  ]
  if save_intermediates:
    keras_callbacks.append(EpochSaverCallback(train_path, save_fn=model.save))
  history = model.train(
      train_data.examples,
      validation_inputs=val_data.examples,
      keras_callbacks=keras_callbacks,
      **train_kw)
  # Save training history too, since this is human-readable and more concise
  # than the TensorBoard log files.
  with open(os.path.join(train_path, "train.history.json"), "w") as fd:
    # Use LIT's custom JSON encoder to handle dicts containing NumPy data.
    fd.write(serialize.to_json(history_to_dict(history), simple=True, indent=2))
  model.save(train_path)
  logging.info("Saved model files: \n  %s",
               "\n  ".join(os.listdir(train_path)))
def main(argv: Sequence[str]) -> None:
  if len(argv) > 1:
    raise app.UsageError("Too many command-line arguments.")
  ##
  # Pick the model and datasets
  # TODO(lit-dev): add remaining GLUE tasks? These three cover all the major
  # features (single segment, two segment, classification, regression).
  if _TASK.value == "sst2":
    train_data = glue_data.SST2Data("train")
    val_data = glue_data.SST2Data("validation")
    model = glue_models.SST2Model(_ENCODER_NAME.value)
  elif _TASK.value == "mnli":
    train_data = glue_data.MNLIData("train")
    val_data = glue_data.MNLIData("validation_matched")
    model = glue_models.MNLIModel(_ENCODER_NAME.value)
  elif _TASK.value == "stsb":
    train_data = glue_data.STSBData("train")
    val_data = glue_data.STSBData("validation")
    model = glue_models.STSBModel(_ENCODER_NAME.value)
  else:
    raise ValueError(f"Unrecognized task name: '{_TASK.value:s}'")
  ##
  # Run training and save model.
  train_and_save(
      model,
      train_data,
      val_data,
      _TRAIN_PATH.value,
      save_intermediates=_SAVE_INTERMEDIATES.value,
      num_epochs=_NUM_EPOCHS.value)
if __name__ == "__main__":
  app.run(main)

================
File: lit_nlp/examples/tydi/data.py
================
"""Data loaders for Question answering model."""
import re
from lit_nlp.api import dataset as lit_dataset
from lit_nlp.api import dtypes
from lit_nlp.api import types as lit_types
import tensorflow_datasets as tfds
TYDI_LANG_VOCAB = [
    'english',
    'bengali',
    'russian',
    'telugu',
    'swahili',
    'korean',
    'indonesian',
    'arabic',
    'finnish',
]
class TyDiQA(lit_dataset.Dataset):
  """TyDiQA dataset."""
  def __init__(self, split: str, max_examples=-1):
    ds = tfds.load('tydi_qa', split=split)
    # populate this with data records
    self._examples = []
    for row in ds.take(max_examples):
      answers_text = row['answers']['text'].numpy()
      answers_start = [row['answers']['answer_start'].numpy()[0]]
      answers = []
      # gets language id example: finnish--9069599462862564793-0
      language_id = row['id'].numpy().decode('utf-8')
      alpha_chars_filter = re.findall(r'[a-z]', language_id)
      language = ''.join(str(r) for r in alpha_chars_filter)
      for label, start in zip(answers_text, answers_start):
        span = dtypes.SpanLabel(start, start + len(label), align='context')
        answers.append(
            dtypes.AnnotationCluster(label=label.decode('utf-8'), spans=[span])
        )
      self._examples.append({
          'answers_text': answers,
          'title': row['title'].numpy().decode('utf-8'),
          'context': row['context'].numpy().decode('utf-8'),
          'question': row['question'].numpy().decode('utf-8'),
          'language': language,
      })
  def spec(self) -> lit_types.Spec:
    return {
        'title': lit_types.TextSegment(),
        'context': lit_types.TextSegment(),
        'question': lit_types.TextSegment(),
        'answers_text': lit_types.MultiSegmentAnnotations(),
        'language': lit_types.CategoryLabel(
            required=False, vocab=TYDI_LANG_VOCAB
        )
    }

================
File: lit_nlp/examples/tydi/demo.py
================
r"""Example demo loading a TyDiModel.
To run locally with a small number of examples:
  python -m lit_nlp.examples.tydi.demo
Then navigate to localhost:5432 to access the demo UI.
"""
from collections.abc import Sequence
import os
import sys
from typing import Optional
from absl import app
from absl import flags
from absl import logging
from lit_nlp import dev_server
from lit_nlp import server_flags
from lit_nlp.components import word_replacer
from lit_nlp.examples.tydi import data as tydi_data
from lit_nlp.examples.tydi import model as tydi_model
# NOTE: additional flags defined in server_flags.py
_FLAGS = flags.FLAGS
_FLAGS.set_default("development_demo", True)
_MODELS = flags.DEFINE_list(
    "models", ["mrm8488/bert-multi-cased-finedtuned-xquad-tydiqa-goldp"],
    "Models to load")
_MAX_EXAMPLES = flags.DEFINE_integer(
    "max_examples", 1000,
    "Maximum number of examples to load from each evaluation set. Set to None "
    "to load the full set."
)
def get_wsgi_app() -> Optional[dev_server.LitServerType]:
  _FLAGS.set_default("server_type", "external")
  _FLAGS.set_default("demo_mode", True)
  # Parse flags without calling app.run(main), to avoid conflict with
  # gunicorn command line flags.
  unused = flags.FLAGS(sys.argv, known_only=True)
  if unused:
    logging.info("tydi_demo:get_wsgi_app() called with unused args: %s", unused)
  return main([])
def main(argv: Sequence[str]) -> Optional[dev_server.LitServerType]:
  if len(argv) > 1:
    raise app.UsageError("Too many command-line arguments.")
  ##
  # Load models, according to the --models flag.
  models = {}
  for model_name_or_path in _MODELS.value:
    # Ignore path prefix, if using /path/to/<model_name> to load from a
    # specific directory rather than the default shortcut.
    model_name = os.path.basename(model_name_or_path)
    models[model_name] = tydi_model.TyDiModel(model_name=model_name_or_path)
  max_examples: int = _MAX_EXAMPLES.value
  dataset_defs: tuple[tuple[str, str], ...] = (
      ("TyDiQA-Multilingual", "validation"),
      ("TyDiQA-English", "validation-en"),
      ("TyDiQA-Finnish", "validation-fi"),
      ("TyDiQA-Arabic", "validation-ar"),
      ("TyDiQA-Bengali", "validation-bn"),
      ("TyDiQA-Indonesian", "validation-id"),
      ("TyDiQA-Korean", "validation-ko"),
      ("TyDiQA-Russian", "validation-ru"),
      ("TyDiQA-Swahili", "validation-sw"),
      ("TyDiQA-Telugu", "validation-te"),
  )
  datasets = {
      name: tydi_data.TyDiQA(split=split, max_examples=max_examples)
      for name, split in dataset_defs
  }
  generators = {"word_replacer": word_replacer.WordReplacer()}
  lit_demo = dev_server.Server(
      models,
      datasets,
      generators=generators,
      **server_flags.get_flags(),
  )
  return lit_demo.serve()
if __name__ == "__main__":
  app.run(main)

================
File: lit_nlp/examples/tydi/model.py
================
"""LIT wrappers for TyDiModel."""
from collections.abc import Iterable
from lit_nlp.api import model as lit_model
from lit_nlp.api import types as lit_types
from lit_nlp.examples.tydi import data as tydi_data
import numpy as np
import transformers
_BertTokenizer = transformers.BertTokenizer
_FlaxBertForQuestionAnswering = transformers.FlaxBertForQuestionAnswering
_JsonDict = lit_types.JsonDict
class TyDiModel(lit_model.Model):
  """Question Answering Jax model based on TyDiQA Dataset."""
  def __init__(
      self,
      model_name: str,
      model=None,
      tokenizer=None,
      **unused_kw,
  ):
    super().__init__()
    self.tokenizer = tokenizer or _BertTokenizer.from_pretrained(model_name)
    self.model = model or _FlaxBertForQuestionAnswering.from_pretrained(
        model_name
    )
  def _segment_slicers(self, tokens: list[str]):
    """Slicers along the tokens dimension for each segment.
    For tokens ['[CLS]', a0, a1, ..., '[SEP]', b0, b1, ..., '[SEP]'],
    we want to get the slices [a0, a1, ...] and [b0, b1, ...]
    Args:
      tokens: <string>[num_tokens], including special tokens
    Returns:
      (slicer_a, slicer_b), slice objects
    """
    try:
      split_point = tokens.index(self.tokenizer.sep_token)
    except ValueError:
      split_point = len(tokens) - 1
    # Question starts after the [CLS] token
    slicer_question = slice(1, split_point)
    # Context ends before the last [SEP] token
    slicer_context = slice(split_point + 1, len(tokens) - 1)
    return slicer_question, slicer_context
  def max_minibatch_size(self) -> int:
    return 8
  def predict(self, inputs: Iterable[_JsonDict], **kw) -> Iterable[_JsonDict]:
    """Predict the answer given the question and context."""
    prediction_output: list[_JsonDict] = []
    for inp in inputs:
      tokenized_text = self.tokenizer(
          inp["question"], inp["context"], return_tensors="jax", padding=True
      )
      results = self.model(
          **tokenized_text, output_hidden_states=True
      )
      answer_start_index = results.start_logits.argmax()
      answer_end_index = results.end_logits.argmax()
      predict_answer_tokens = tokenized_text.input_ids[
          0, answer_start_index : answer_end_index + 1
      ]
      # get id's for question & context
      tokens = np.asarray(tokenized_text["input_ids"])
      # convert id's to tokens
      total_tokens = self.tokenizer.convert_ids_to_tokens(tokens[0])
      # split by question & context
      slicer_question, slicer_context = self._segment_slicers(total_tokens)
      # TODO(b/349177755): Gradients and embeddings are not implemented
      # correctly. Use lit_nlp/examples/prompt_debugging/transformers_lms.py
      # code as a reference for how to implement these correctly.
      # embeddings = results.hidden_states[0][0]
      # gradient = results.hidden_states[-1][0]
      prediction_output.append({
          "generated_text": self.tokenizer.decode(predict_answer_tokens),
          "answers_text": inp["answers_text"],
          "tokens_question": total_tokens[slicer_question],
          "tokens_context": total_tokens[slicer_context],
          # TODO(b/349177755): Re-enable these once the embeddings and gradients
          # are implemented correctly.
          # Embeddings come from the first token of the last layer.
          # "cls_emb": results.hidden_states[-1][:, 0][0],
          # "tokens_embs_question": np.asarray(embeddings[slicer_question]),
          # "token_grad_context": np.asarray(embeddings[slicer_context]),
          # "tokens_grad_question": np.asarray(gradient[slicer_question]),
          # "tokens_embs_context": np.asarray(gradient[slicer_context]),
      })
    return prediction_output
  def input_spec(self):
    return {
        "title": lit_types.TextSegment(),
        "context": lit_types.TextSegment(),
        "question": lit_types.TextSegment(),
        "answers_text": lit_types.MultiSegmentAnnotations(),
        "language": lit_types.CategoryLabel(
            required=False, vocab=tydi_data.TYDI_LANG_VOCAB
        ),
    }
  def output_spec(self):
    return {
        "answers_text": lit_types.MultiSegmentAnnotations(),
        "generated_text": lit_types.GeneratedText(parent="answers_text"),
        "tokens_context": lit_types.Tokens(parent="question"),
        "tokens_question": lit_types.Tokens(parent="question"),
        # TODO(b/349177755): Re-enable these once the embeddings and gradients
        # are implemented correctly.
        # "cls_emb": lit_types.Embeddings(),
        # "tokens_embs_question": lit_types.TokenEmbeddings(
        #     align="tokens_question"
        # ),
        # "tokens_grad_question": lit_types.TokenGradients(
        #     align="tokens_question", grad_for="tokens_embs_question"
        # ),
        # "tokens_embs_context": lit_types.TokenEmbeddings(
        #     align="tokens_context"
        # ),
        # "token_grad_context": lit_types.TokenGradients(
        #     align="tokens_context", grad_for="tokens_embs_context"
        # ),
    }

================
File: lit_nlp/examples/tydi/README.md
================
TyDi QA Demo for the Learning Interpretability Tool
=======================================================

This demo showcases how LIT can be used to a multilingual question-answering
model trained on the [TyDi QA dataset](https://doi.org/10.1162/tacl_a_00317)
using FLAX.

You will need a stand-alone virtual environment for the Python libraries, which you can set up using the following commands from the root of the LIT repo.

```sh
# Create the virtual environment. You may want to use python3 or python3.10
# depends on how many Python versions you have installed and their aliases.
python -m venv .tydi-venv
source .tydi-venv/bin/activate
# This requirements.txt file will also install the core LIT library deps.
pip install -r ./lit_nlp/examples/tydi/requirements.txt
# The LIT web app still needs to be built in the usual way.
(cd ./lit_nlp && yarn && yarn build)
```

Once your virtual environment is setup, you can launch the demo with the
following command.

```sh
python -m lit_nlp.examples.tydi.demo
```

================
File: lit_nlp/examples/tydi/requirements.txt
================
# Copyright 2023 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

-r ../../../requirements.txt

flax>=0.5.3
jax==0.4.6
jaxlib==0.4.6
tensorflow>=2.10.0,<2.16.0
tensorflow-datasets>=4.9.0
transformers>=4.27.1

================
File: lit_nlp/lib/__init__.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

================
File: lit_nlp/lib/caching_test.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Tests for lit_nlp.lib.model."""
from absl.testing import absltest
from absl.testing import parameterized
from lit_nlp.api import dataset
from lit_nlp.lib import caching
from lit_nlp.lib import testing_utils
class CachingModelWrapperTest(parameterized.TestCase):
  def test_caching_model_wrapper_use_cache(self):
    model = testing_utils.IdentityRegressionModelForTesting()
    wrapper = caching.CachingModelWrapper(model, "test")
    examples = [{"val": 1, "_id": "id_to_cache"}]
    results = wrapper.predict(examples)
    self.assertEqual(1, model.count)
    self.assertEqual({"score": 1}, results[0])
    results = wrapper.predict(examples)
    self.assertEqual(1, model.count)
    self.assertEqual({"score": 1}, results[0])
    self.assertEmpty(wrapper._cache._pred_locks)
  def test_caching_model_wrapper_not_cached(self):
    model = testing_utils.IdentityRegressionModelForTesting()
    wrapper = caching.CachingModelWrapper(model, "test")
    examples = [{"val": 1, "_id": "my_id"}]
    results = wrapper.predict(examples)
    self.assertEqual(1, model.count)
    self.assertEqual({"score": 1}, results[0])
    examples = [{"val": 2, "_id": "other_id"}]
    results = wrapper.predict(examples)
    self.assertEqual(2, model.count)
    self.assertEqual({"score": 2}, results[0])
  def test_caching_model_wrapper_uses_cached_subset(self):
    model = testing_utils.IdentityRegressionModelForTesting()
    wrapper = caching.CachingModelWrapper(model, "test")
    examples = [
        {"val": 0, "_id": "zeroth_id"},
        {"val": 1, "_id": "first_id"},
        {"val": 2, "_id": "second_id"},
    ]
    subset = examples[:1]
    # Run the CachingModelWrapper over a subset of examples
    results = wrapper.predict(subset)
    self.assertEqual(1, model.count)
    self.assertEqual({"score": 0}, results[0])
    # Now, run the CachingModelWrapper over all of the examples. This should
    # only pass the examples that were not in subset to the wrapped model, and
    # the total number of inputs processed by the wrapped model should be 3
    results = wrapper.predict(examples)
    self.assertEqual(3, model.count)
    self.assertEqual({"score": 0}, results[0])
    self.assertEqual({"score": 1}, results[1])
    self.assertEqual({"score": 2}, results[2])
  @parameterized.named_parameters(
      ("hash_fn=input_hash", caching.input_hash),
      ("hash_fn=custom_fn", lambda x: x["_id"]),
  )
  def test_caching_model_strict_id_validation(
      self, id_hash_fn: dataset.IdFnType
  ):
    model = testing_utils.IdentityRegressionModelForTesting()
    wrapper = caching.CachingModelWrapper(
        model, "test", strict_id_validation=True, id_hash_fn=id_hash_fn
    )
    examples = [{"val": 1, "_id": "b1d0ec818f8aeefdd0551cad96d58e75"}]
    results = wrapper.predict(examples)
    self.assertEqual(1, model.count)
    self.assertEqual({"score": 1}, results[0])
    self.assertEmpty(wrapper._cache._pred_locks)
  def test_caching_model_raises_strict_id_validation_no_id_hash_fn(self):
    model = testing_utils.IdentityRegressionModelForTesting()
    with self.assertRaises(ValueError):
      caching.CachingModelWrapper(model, "test", strict_id_validation=True)
  def test_caching_model_raises_strict_id_validation_differing_ids(self):
    model = testing_utils.IdentityRegressionModelForTesting()
    wrapper = caching.CachingModelWrapper(
        model, "test", strict_id_validation=True, id_hash_fn=caching.input_hash
    )
    examples = [{"val": 1, "_id": "my_id"}]
    with self.assertRaises(ValueError):
      wrapper.predict(examples)
class PredsCacheTest(absltest.TestCase):
  def test_preds_cache(self):
    """Test with an exact match."""
    cache = caching.PredsCache("test")
    self.assertEqual("0", cache.info())
    cache.put("test", None)
    self.assertEqual("0", cache.info())
    cache.put("test", ("a", "1"))
    self.assertEqual("1", cache.info())
    self.assertIsNone(None, cache.get(("a", "2")))
    self.assertEqual("test", cache.get(("a", "1")))
  def test_pred_lock_key(self):
    cache = caching.PredsCache("test")
    cache_key = [("a", "1"), ("a", "2")]
    self.assertIsNone(cache.pred_lock_key(cache_key))
    cache.get_pred_lock(cache_key)
    expected_cache_key = frozenset(cache_key)
    self.assertEqual(expected_cache_key, cache.pred_lock_key(cache_key))
    sub_cache_key = [("a", "1")]
    self.assertEqual(expected_cache_key, cache.pred_lock_key(sub_cache_key))
    mismatch_cache_key = [("b", "1")]
    self.assertIsNone(cache.pred_lock_key(mismatch_cache_key))
  def test_pred_lock_key_no_concurrent_predictions(self):
    cache = caching.PredsCache("test", False)
    cache_key = [("a", "1"), ("a", "2")]
    self.assertIsNone(cache.pred_lock_key(cache_key))
    cache.get_pred_lock(cache_key)
    expected_cache_key = frozenset(
        [caching.PRED_LOCK_KEY_WHEN_NO_CONCURRENT_ACCESS])
    self.assertEqual(expected_cache_key, cache.pred_lock_key(cache_key))
    sub_cache_key = [("a", "1")]
    self.assertEqual(expected_cache_key, cache.pred_lock_key(sub_cache_key))
    mismatch_cache_key = [("b", "1")]
    self.assertEqual(expected_cache_key, cache.pred_lock_key(
        mismatch_cache_key))
  def test_delete_pred_lock(self):
    cache = caching.PredsCache("test")
    cache_key = [("a", "1"), ("a", "2")]
    self.assertIsNone(cache.delete_pred_lock(cache_key))
    lock = cache.get_pred_lock(cache_key)
    self.assertEqual(lock, cache.delete_pred_lock(cache_key))
    self.assertIsNone(cache.delete_pred_lock(cache_key))
  def test_get_pred_lock(self):
    cache = caching.PredsCache("test")
    cache_key = [("a", "1"), ("a", "2")]
    lock = cache.get_pred_lock(cache_key)
    self.assertIsNotNone(lock)
    sub_cache_key = [("a", "2")]
    self.assertEqual(lock, cache.get_pred_lock(sub_cache_key))
    mismatch_cache_key = [("b", "2")]
    self.assertNotEqual(lock, cache.get_pred_lock(mismatch_cache_key))
if __name__ == "__main__":
  absltest.main()

================
File: lit_nlp/lib/caching.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Miscellaneous helper functions."""
from collections.abc import Callable, Iterable
import os
import pickle
import threading
from typing import Any, Optional, Union
from absl import logging
from lit_nlp.api import dataset as lit_dataset
from lit_nlp.api import model as lit_model
from lit_nlp.api import types
from lit_nlp.lib import serialize
JsonDict = types.JsonDict
Input = types.Input
IndexedInput = types.IndexedInput
ProgressIndicator = Callable[[Iterable], Iterable]
# Compound keys: (dataset_name, example_id)
# None is used as a sentinel to skip the cache.
CacheKey = Union[tuple[str, str], None]
# The keys to the prediction locks are frozen sets of CacheKeys.
PredLockKey = frozenset[CacheKey]
# Special CacheKey to use when a model doesn't allow concurrent predictions.
PRED_LOCK_KEY_WHEN_NO_CONCURRENT_ACCESS: CacheKey = (
    "NO_CONCURRENT_PREDICTION", "")
input_hash = lit_dataset.input_hash
class PickleCacheLoader(object):
  """For saving and loading cache to a pickle file."""
  def __init__(self, name: str, cache_dir: str):
    self._cache_path = os.path.join(cache_dir, name + ".cache.pkl")
  def save(self, data: dict[CacheKey, Any]):
    with open(self._cache_path, "wb") as fd:
      pickle.dump(data, fd)
  def load(self) -> dict[CacheKey, Any]:
    """Load data from pickle file."""
    try:
      with open(self._cache_path, "rb") as fd:
        data = pickle.load(fd)
    except FileNotFoundError:
      logging.info("No cache to load at %s.", self._cache_path)
      data = {}
    except EOFError:
      logging.error(
          "Failed loading cache, possibly due to malformed cache data."
          "Please remove %s and try again.", self._cache_path)
      data = {}
    except IOError:
      logging.error("Failed loading cache at %s.", self._cache_path)
      data = {}
    return data  # pytype: disable=name-error  # py310-upgrade
class PredsCache(object):
  """Cache for model outputs."""
  def __init__(self, name: str, allow_concurrent_predictions: bool = True,
               cache_dir: Optional[str] = None):
    # TODO(lit-team): consider using a read/write lock, or setting timeouts if
    # contention becomes an issue.
    self._lock = threading.RLock()
    self._d: dict[CacheKey, Any] = dict()
    self._num_persisted = 0
    self._allow_concurrent_predictions = allow_concurrent_predictions
    self._cache_dir = cache_dir
    self._cache_loader = None
    if cache_dir:
      self._cache_loader = PickleCacheLoader(name, cache_dir)
    # A map of keys needing predictions to a lock for that model predict call.
    # Used for not duplicating concurrent prediction calls on the same inputs.
    self._pred_locks: dict[PredLockKey, threading.RLock] = dict()
  @property
  def lock(self):
    return self._lock
  def put(self, data, key: CacheKey):
    if key is not None:
      self._d[key] = data
  def get(self, key: CacheKey) -> Optional[Any]:
    return self._d.get(key) if key is not None else None
  def info(self) -> str:
    """Print some info, for logging."""
    return str(len(self._d))
  def _construct_pred_lock_key(self, keys: list[CacheKey]) -> PredLockKey:
    # If this cache is set up to not allow concurrent predictions, then use the
    # same key to the predictions lock map regardless of example keys provided.
    fs = frozenset(keys) if self._allow_concurrent_predictions else frozenset(
        [PRED_LOCK_KEY_WHEN_NO_CONCURRENT_ACCESS])
    return fs
  def pred_lock_key(self, keys: list[CacheKey]) -> Optional[PredLockKey]:
    """Get the key for the predictions lock for the provided cache keys."""
    fs = self._construct_pred_lock_key(keys)
    # If the provided cache keys already have a lock, return the key.
    if fs in self._pred_locks:
      return fs
    # If there is a lock for a superset of the provided cache keys, return the
    # key to that lock.
    # This means that requests for subsets of data already being predicted will
    # wait for the larger set of predictions to complete. This can slow down
    # certain single-example requests but leads to more efficient use of the
    # model. We may have duplicate predict calls for an example to a model if
    # one example is part of separate but distinct predict calls with different
    # subsets of examples, but this is unlikely given how LIT predict requests
    # work.
    for key in self._pred_locks:
      if fs.issubset(key):
        return key
    # Otherwise, return None as there is no lock yet for the provided cache
    # keys.
    return None
  def get_pred_lock(self, keys: list[CacheKey]) -> threading.RLock:
    """Gets the lock for the provided cache keys, creating one if neccessary."""
    # If the lock already exists for the provided keys, return it.
    pl_key = self.pred_lock_key(keys)
    if pl_key:
      return self._pred_locks[pl_key]
    # If no such lock exists, create, store and return it.
    pred_lock = threading.RLock()
    self._pred_locks[self._construct_pred_lock_key(keys)] = pred_lock
    return pred_lock
  def delete_pred_lock(self, keys: list[CacheKey]) -> Optional[threading.RLock]:
    """Remove the lock from the map to clean up, returns it."""
    pl_key = self.pred_lock_key(keys)
    if pl_key is None:
      return None
    return self._pred_locks.pop(pl_key)
  def save_to_disk(self):
    """Save cache data to disk."""
    # No cache loader is created if no cache directory was provided, in which
    # case this is a no-op.
    cache_loader = self._cache_loader
    if not cache_loader:
      return
    if self._num_persisted == len(self._d):
      logging.info("No need to re-save cache to %s", self._cache_dir)
      return
    logging.info(
        "Saving cache (%d entries) to %s", len(self._d), self._cache_dir
    )
    cache_loader.save(self._d)
    self._num_persisted = len(self._d)
  def load_from_disk(self):
    """Load cache data from disk."""
    # No cache loader is created if no cache directory was provided, in which
    # case this is a no-op.
    cache_loader = self._cache_loader
    if not cache_loader:
      return
    self._d = cache_loader.load()
    self._num_persisted = len(self._d)
    logging.info(
        "Loaded cache (%d entries) from %s",
        self._num_persisted,
        self._cache_dir,
    )
class CachingModelWrapper(lit_model.ModelWrapper):
  """Wrapper to add per-example caching to a LIT model."""
  def __init__(
      self,
      model: lit_model.Model,
      name: str,
      cache_dir: Optional[str] = None,
      strict_id_validation: bool = False,
      id_hash_fn: Optional[lit_dataset.IdFnType] = None,
  ):
    """Wrap a model to add caching.
    Args:
      model: a LIT model
      name: name, used for logging and data files
      cache_dir: if given, will load/save data to disk
      strict_id_validation: if true, will re-compute hashes using id_hash_fn and
        verify that they match the provided IDs. See b/293984290.
      id_hash_fn: function of example --> string id, used by
        strict_id_validation mode.
    """
    if strict_id_validation and id_hash_fn is None:
      raise ValueError(
          "Must provide id_hash_fn to use strict_id_validation mode."
      )
    super().__init__(model)
    self._name = name
    self._log_prefix = f"CachingModelWrapper '{name:s}'"
    self._strict_id_validation = strict_id_validation
    self._id_hash_fn = id_hash_fn
    self._cache = PredsCache(
        name, model.supports_concurrent_predictions, cache_dir
    )
    self.load_cache()
  def load_cache(self):
    self._cache.load_from_disk()
  def save_cache(self):
    self._cache.save_to_disk()
  def key_fn(self, d) -> CacheKey:
    return (self._name, d_id) if (d_id := d.get("_id")) else None
  def _validate_ids(self, inputs: Iterable[JsonDict]):
    for ex in inputs:
      if not (given_id := ex.get("_id")):
        continue
      if (computed_id := self._id_hash_fn(types.Input(ex))) != given_id:
        raise ValueError(
            f"Given id '{given_id}' does not match computed id '{computed_id}'"
            f" for example {str(ex)}."
        )
  ##
  # For internal use
  def fit_transform(self, inputs: Iterable[JsonDict]):
    """Cache projections from ProjectorModel dimensionality reducers."""
    wrapped = self.wrapped
    if not isinstance(wrapped, lit_model.ProjectorModel):
      raise TypeError(
          "Attempted to call fit_transform() on a non-ProjectorModel."
      )
    inputs_as_list = list(inputs)
    cache_keys = [self.key_fn(d) for d in inputs_as_list]
    if (none_keys := [k for k in cache_keys if k is None]):
      logging.warning(
          "Attmepting to cache %d (of %d) where the cache key is None "
          "- this can be from a missing or empty example id. These"
          " will be recomputed on subsequent attempts.",
          len(none_keys),
          len(cache_keys),
      )
    outputs = list(wrapped.fit_transform(inputs_as_list))
    with self._cache.lock:
      for cache_key, output in zip(cache_keys, outputs):
        self._cache.put(output, cache_key)
    return outputs
  def predict(self,
              inputs: Iterable[JsonDict],
              progress_indicator: Optional[ProgressIndicator] = lambda x: x,
              **kw) -> list[JsonDict]:
    inputs_as_list = list(inputs)
    if self._strict_id_validation:
      self._validate_ids(inputs_as_list)
    # Try to get results from the cache.
    input_keys = [self.key_fn(d) for d in inputs_as_list]
    if (none_keys := [k for k in input_keys if k is None]):
      logging.warning(
          "Attmepting to retrieve %d (of %d) predictions from the cache where"
          " the cache key is None - this can be from a missing or empty example"
          " id. These will call model.predict() on this and subsequent calls.",
          len(none_keys),
          len(input_keys),
      )
    if self._cache.pred_lock_key(input_keys):
      with self._cache.get_pred_lock(input_keys):
        cached_results = self._get_results_from_cache(input_keys)
    else:
      cached_results = self._get_results_from_cache(input_keys)
    # Make a single list of everything that wasn't found in the cache,
    # to actually run the model on these inputs.
    miss_idxs = [i for i, v in enumerate(cached_results) if v is None]
    misses = [inputs_as_list[i] for i in miss_idxs]
    if misses:
      logging.info("%s: %d misses out of %d inputs", self._log_prefix,
                   len(miss_idxs), len(cached_results))
    else:
      # If all results were already cached, return them.
      return cached_results
    with self._cache.get_pred_lock(input_keys):
      model_preds = list(self.wrapped.predict(progress_indicator(misses)))
      logging.info("Received %d predictions from model", len(model_preds))
      if len(model_preds) != len(misses):
        raise ValueError(f"Received {len(model_preds)} predictions, which does "
                         f"not match {len(misses)}, the number of inputs.")
      # Merge results back into the output list.
      with self._cache.lock:
        for i, orig_idx in enumerate(miss_idxs):
          self._cache.put(model_preds[i], self.key_fn(inputs_as_list[orig_idx]))
          cached_results[orig_idx] = model_preds[i]
      # Remove the prediction lock from the cache as the request is complete
      self._cache.delete_pred_lock(input_keys)
    return cached_results
  def _get_results_from_cache(self, input_keys: list[CacheKey]):
    with self._cache.lock:
      return [self._cache.get(input_key) for input_key in input_keys]

================
File: lit_nlp/lib/file_cache_test.py
================
# Copyright 2023 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
from absl.testing import absltest
from absl.testing import parameterized
from lit_nlp.lib import file_cache
class FileCacheTest(parameterized.TestCase):
  # ETag can have strong (an ASCII character string) or weak (prefixed by 'W/)
  # validation. See MDN for more:
  # https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/ETag#directives
  @parameterized.named_parameters(
      dict(
          testcase_name='empty',
          etag='',
          expected='49e48471af489fc1.chkpt',
      ),
      dict(
          testcase_name='standard_validator',
          etag='a2c4e67',
          expected='49e48471af489fc1_043d7490.chkpt',
      ),
      dict(
          testcase_name='weak_validator',
          etag='W/a2c4e67',
          expected='49e48471af489fc1_9fee3ee2.chkpt',
      ),
  )
  def test_filename_fom_url_etag(self, etag: str, expected: str):
    url = 'https://example.com/testdata/model.chkpt'
    filename = file_cache.filename_fom_url(url, etag)
    self.assertEqual(filename, expected)
  @parameterized.named_parameters(
      dict(
          testcase_name='empty',
          url='',
          expected='e3b0c44298fc1c14',
      ),
      dict(
          testcase_name='extensionless',
          url='https://example.com/testdata/model',
          expected='adb48b9e4d4f2dfa',
      ),
      dict(
          testcase_name='HDF5_file',
          url='https://example.com/testdata/model.h5',
          expected='c56165097a137459.h5',
      ),
      dict(
          testcase_name='JSON_file',
          url='https://example.com/testdata/model.json',
          expected='56d84f3bc9b95492.json',
      ),
      dict(
          testcase_name='Tar_archive',
          url='https://example.com/testdata/model.tar',
          expected='5882fbfd78d7abb8.tar',
      ),
      dict(
          testcase_name='Zip_archive',
          url='https://example.com/testdata/model.zip',
          expected='94797e8635299d8a.zip',
      ),
  )
  def test_filename_fom_url_no_etag(self, url: str, expected: str):
    filename = file_cache.filename_fom_url(url)
    self.assertEqual(filename, expected)
  @parameterized.named_parameters(
      ('empty', '', False),
      ('Amazon_S3', 's3://testdata/model.chkpt', False),
      ('FTP', 'ftp://testdata/model.chkpt', False),
      ('Google_Cloud_Storage', 'gs://testdata/model.chkpt', False),
      ('HTTP', 'http://example.com/testdata/model.chkpt', True),
      ('HTTPS', 'https://example.com/testdata/model.chkpt', True),
      ('local_file', '/usr/local/testdata/model.chkpt', False),
  )
  def test_is_remote(self, url: str, expected: bool):
    is_remote = file_cache.is_remote(url)
    self.assertEqual(is_remote, expected)
  # TODO(b/285157349, b/254110131): Add UT/ITs for file_cache.cached_path().
  # Conditions should include:
  #
  # * File paths with lit_file_cache_path flag is set (UT).
  # * File paths with LIT_FILE_CACHE_PATH env var set (UT).
  # * File paths with lit_file_cache_path flag and LIT_FILE_CACHE_PATH env var
  #   are set (UT; flag should win).
  # * Local paths that include a file extension (UT)
  # * Local paths that do not include a file extension (UT)
  # * Local paths for TAR and Zip archives in the cache (UT)
  # * Local paths for TAR and Zip archives not in the cache (IT)
  # * URLs in cache for paths that include a file extension (UT)
  # * URLs in cache for paths that do not include a file extension (UT)
  # * URls in cache for TAR and Zip archives (UT)
  # * URLs not in cache for paths that include a file extension (IT?)
  # * URLs not in cache for paths that do not include a file extension (IT?)
  # * URls not in cache for TAR and Zip archives (IT?)
if __name__ == '__main__':
  absltest.main()

================
File: lit_nlp/lib/file_cache.py
================
# Copyright 2023 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""LIT-specific replcement for transformers.file_utils.cached_path."""
from collections.abc import Mapping
import functools
import hashlib
import os
import shutil
import sys
import tarfile
import tempfile
from typing import BinaryIO, Optional
from urllib import parse as urllib_parse
import zipfile
# TODO(b/254110131): Reenable this import once the integration tests in the TODO
# below have been updated for compatibility.
# from absl import flags
from absl import logging
import filelock
import requests
import tqdm
_HTTP_SESSION = requests
DEFAULT_FILE_CACHE_PATH = '/tmp/lit_nlp/file_cache'
FILE_CACHE_PATH_ENV = os.getenv('LIT_FILE_CACHE_PATH', DEFAULT_FILE_CACHE_PATH)
# TODO(b/254110131): Update model initialization code in
# ablation_flip_int_test.py, hotflip_int_test.py, tcav_test.py, and
# thresholder_test.py to be compatible with the use of absl.flags here (via
# absl.testing.flagsaver, see Python Tips #51)
# FILE_CACHE_PATH_FLAG = flags.DEFINE_string(
#     name='lit_file_cache_path',
#     default=None,
#     help=f'Path to the file cache. Defaults to {DEFAULT_FILE_CACHE_PATH}.'
#     ' Overrides the LIT_FILE_CACHE_PATH environment variable. This path is'
#     ' expanded using os.expanduser(), see expected expansion behavior at'
#     ' https://docs.python.org/3/library/os.path.html#os.path.expanduser.',
# )
def _fetch_content(
    url: str,
    temp_file: BinaryIO,
    headers: Mapping[str, str],
    progress_indicator: Optional[tqdm.tqdm] = None,
    use_default_progress_indicator: bool = True,
) -> None:
  """Fetches HTTP content and writes it to the provided file."""
  head_response = _HTTP_SESSION.head(url, headers=headers, stream=True)
  head_response.raise_for_status()
  content_length = head_response.headers.get('Content-Length')
  total = int(content_length) if content_length is not None else None
  if progress_indicator is not None:
    progress = progress_indicator
  elif use_default_progress_indicator:
    progress = tqdm.tqdm(
        unit='B',
        unit_scale=True,
        total=total,
        initial=0,
        desc='Downloading',
    )
  else:
    progress = None
  get_response = _HTTP_SESSION.get(url=url, stream=True, headers=headers)
  get_response.raise_for_status()
  for chunk in get_response.iter_content(chunk_size=1024):
    if chunk:
      if progress is not None:
        progress.update(len(chunk))
      temp_file.write(chunk)
  if progress is not None:
    progress.close()
def _get_extacted_dir(output_path: str) -> str:
  """Extracts and returns the directory containing the provided archive."""
  is_zip = zipfile.is_zipfile(output_path)
  if not (is_zip or tarfile.is_tarfile(output_path)):
    return output_path
  output_dir, output_file = os.path.split(output_path)
  output_extracted_dir_name = output_file.replace('.', '-') + '-extracted'
  output_extracted_path = os.path.join(output_dir, output_extracted_dir_name)
  if os.path.isdir(output_extracted_path) and os.listdir(output_extracted_path):
    return output_extracted_path
  lock_path = output_path + '.lock'
  with filelock.FileLock(lock_path):
    shutil.rmtree(output_extracted_path, ignore_errors=True)
    os.makedirs(output_extracted_path)
    if is_zip:
      with zipfile.ZipFile(output_path, 'r') as zip_file:
        zip_file.extractall(output_extracted_path)
        zip_file.close()
    else:
      tar_file = tarfile.open(output_path)
      tar_file.extractall(output_extracted_path)
      tar_file.close()
  return output_extracted_path
def _get_from_cache(
    url: str,
    local_files_only: bool = False,
    progress_indicator: Optional[tqdm.tqdm] = None,
) -> str:
  """Downloads and returns the cache path to the content at the provided URL."""
  # TODO(b/254110131): Update to use FILE_CACHE_PATH_FLAG.value once the
  # integration tests in the TODO above have been updated for compatibility.
  lit_cache_dir = os.path.expanduser(FILE_CACHE_PATH_ENV)
  headers = {'user-agent': f'lit_nlp; python/{sys.version.split()[0]}'}
  etag: Optional[str] = None
  url_to_download: str = url
  os.makedirs(lit_cache_dir, exist_ok=True)
  # Attempt to get the ETag for this resource.
  if not local_files_only:
    try:
      head_response = _HTTP_SESSION.head(
          url, headers=headers, stream=True, allow_redirects=False
      )
      head_response.raise_for_status()
      etag = str(
          head_response.headers.get('X-Linked-Etag')
          or head_response.headers.get('ETag')
      )
      if etag is None:
        raise OSError(
            'No ETag found for resource, reproducibility will be unreliable.'
        )
      # If the HEAD responds with a redirect, download from that URL if the
      # content is not in the cache, but use the original URL for the filename.
      # Otherwise, download from the original URL.
      if 300 <= head_response.status_code < 400:
        url_to_download = str(head_response.headers['Location'])
    except (requests.exceptions.ConnectionError, requests.exceptions.Timeout):
      pass  # etag is already None, url_to_download is already set.
  filename = filename_fom_url(url, etag=etag)
  cache_path = os.path.join(lit_cache_dir, filename)
  if os.path.exists(cache_path):
    logging.info('File %s exists in cache as %s', url, cache_path)
    return cache_path
  elif etag is None:
    raise ValueError(
        'Encountered a connection error and cannot find the requested files in'
        ' the cached path. Please try again and ensure your Internet connection'
        ' is on.'
    )
  lock_path = cache_path + '.lock'
  with filelock.FileLock(lock_path):
    if os.path.exists(cache_path):  # Download completed while activating lock.
      logging.info('File %s existing in cache as %s', url, cache_path)
      return cache_path  # Returning early will release the lock.
    temp_file_manager = functools.partial(
        tempfile.NamedTemporaryFile, mode='wb', dir=lit_cache_dir, delete=False
    )
    with temp_file_manager() as temp_file:
      logging.info('%s not found in cache.', url)
      _fetch_content(
          url_to_download,
          temp_file,
          headers=headers,
          progress_indicator=progress_indicator
      )
      logging.info('%s downloaded to %s', url, temp_file.name)
    logging.info('Storing %s in cache at %s', url, cache_path)
    os.replace(temp_file.name, cache_path)
  return cache_path
def filename_fom_url(url: str, etag: Optional[str] = None) -> str:
  """Converts a URL, and optional etag, to a filename using a SHA 256 hash."""
  url_as_bytes = url.encode('utf-8')
  filename = hashlib.sha256(url_as_bytes).hexdigest()[:16]
  if etag:
    etag_bytes = etag.encode('utf-8')
    etag_sha = hashlib.sha256(etag_bytes).hexdigest()[:8]
    filename += f'_{etag_sha}'
  # Preserve file extension for URLs that include it.
  parsed_url = urllib_parse.urlparse(url)
  url_path = parsed_url.path
  _, extension = os.path.splitext(url_path)
  if extension:
    filename += extension
  return filename
def is_remote(url_of_filepath: str) -> bool:
  """Check if a path represents a remote URL or non-local file."""
  parsed = urllib_parse.urlparse(url_of_filepath)
  return parsed.scheme in ('http', 'https')
def cached_path(
    url_or_filepath: str,
    extract_compressed_file: bool = False,
    local_files_only: bool = False,
    progress_indicator: Optional[tqdm.tqdm] = None,
) -> str:
  """Get the path to the locally cached resource."""
  if is_remote(url_or_filepath):
    logging.info('%s is remote', url_or_filepath)
    output_path = _get_from_cache(
        url_or_filepath,
        local_files_only=local_files_only,
        progress_indicator=progress_indicator,
    )
  elif os.path.exists(url_or_filepath):
    logging.info('%s is local', url_or_filepath)
    output_path = url_or_filepath
  elif not urllib_parse.urlparse(url_or_filepath).scheme:
    raise EnvironmentError(f'File not found: {url_or_filepath}')
  else:
    raise ValueError(
        f'Unable to parse as URL or local path: {url_or_filepath}'
    )
  if not extract_compressed_file:
    return output_path
  else:
    return _get_extacted_dir(output_path)

================
File: lit_nlp/lib/flag_helpers.py
================
"""Data to support runtime flags."""
import enum
@enum.unique
class ValidationMode(enum.Enum):
  """All the validation mode options."""
  OFF = 'off'  # Do not validate datasets and model outputs.
  FIRST = 'first'  # Validate the first datapoint.
  ALL = 'all'  # Validate all datapoints.
  SAMPLE = 'sample'  # Validate a sample of 5% of datapoints.

================
File: lit_nlp/lib/image_utils_test.py
================
# Copyright 2021 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Tests for lit_nlp.lib.image_utils."""
from absl.testing import absltest
from absl.testing import parameterized
from lit_nlp.lib import image_utils
import numpy as np
from PIL import Image as PILImage
def _create_image_array(shape: tuple[int, int, int]) -> np.ndarray[np.uint8]:
  height, width, channels = shape
  array = np.zeros(shape=shape, dtype=np.uint8)
  for i in range(1000):
    array[i % height, i % width, i % channels] = i % 256
  return array
_TEST_IMAGE_SHAPE = (100, 70, 3)
_TEST_IMAGE_ARRAY = _create_image_array(_TEST_IMAGE_SHAPE)
_TEST_IMAGE_RGB: PILImage.Image = PILImage.fromarray(_TEST_IMAGE_ARRAY)
_TEST_IMAGE_ARRAY_GREY: np.ndarray = np.asarray(_TEST_IMAGE_RGB.convert('L'))
class ImageUtilsTest(parameterized.TestCase):
  def test_pil_to_str(self):
    image_str = image_utils.convert_pil_to_image_str(_TEST_IMAGE_RGB)
    self.assertIsNotNone(image_str)
  def test_str_to_pil(self):
    image_str = image_utils.convert_pil_to_image_str(_TEST_IMAGE_RGB)
    pil_image = image_utils.convert_image_str_to_pil(image_str)
    image_array = np.asarray(pil_image)
    np.testing.assert_array_equal(_TEST_IMAGE_ARRAY, image_array)
  @parameterized.named_parameters(
      dict(
          testcase_name='greyscale_normalized',
          mode='L',
          normalize=True,
          expected=_TEST_IMAGE_ARRAY_GREY / 255,
      ),
      dict(
          testcase_name='greyscale_not_normalized',
          mode='L',
          normalize=False,
          expected=_TEST_IMAGE_ARRAY_GREY,
      ),
      dict(
          testcase_name='rgb_normalized',
          mode='RGB',
          normalize=True,
          expected=_TEST_IMAGE_ARRAY / 255,
      ),
      dict(
          testcase_name='rgb_not_normalized',
          mode='RGB',
          normalize=False,
          expected=_TEST_IMAGE_ARRAY,
      ),
  )
  def test_str_to_array(self, mode: str, normalize: bool, expected: np.ndarray):
    image_str = image_utils.convert_pil_to_image_str(_TEST_IMAGE_RGB)
    image_array = image_utils.convert_image_str_to_array(
        image_str, shape=_TEST_IMAGE_SHAPE, mode=mode, normalize=normalize
    )
    np.testing.assert_array_almost_equal(expected, image_array)
  def test_clip_unsigned_saliency(self):
    a = np.linspace(0, 100, num=101, endpoint=True)
    a_clipped = image_utils.clip_unsigned_saliency(a, fraction=0.1)
    self.assertEqual(a_clipped.min(), 0)
    self.assertEqual(a_clipped.max(), 90)
    self.assertLen(np.argwhere(a_clipped == 0), 1)
    self.assertLen(np.argwhere(a_clipped == 90), 11)
  def test_clip_signed_saliency(self):
    a = np.linspace(-50, 100, num=151, endpoint=True)
    a_clipped = image_utils.clip_signed_saliency(a, fraction=0.1)
    self.assertEqual(a_clipped.min(), -42)
    self.assertEqual(a_clipped.max(), 92)
    self.assertLen(np.argwhere(a_clipped == -42), 9)
    self.assertLen(np.argwhere(a_clipped == 92), 9)
  def test_normalize_unsigned_saliency(self):
    a = np.linspace(10, 100, num=101, endpoint=True)
    a_norm = image_utils.normalize_unsigned_saliency(a)
    self.assertAlmostEqual(a_norm.max(), 1.0)
    self.assertAlmostEqual(a_norm.min(), 0.0)
  def test_normalize_signed_saliency(self):
    # Test the case when the magnitude of positive numbers is higher.
    a = np.linspace(-10, 100, num=101, endpoint=True)
    a_norm = image_utils.normalize_signed_saliency(a)
    self.assertAlmostEqual(a_norm.max(), 1.0)
    self.assertAlmostEqual(a_norm.min(), 0.45)
    # Test the case when the magnitude of negative numbers is higher.
    a = np.linspace(-100, 10, num=101, endpoint=True)
    a_norm = image_utils.normalize_signed_saliency(a)
    self.assertAlmostEqual(a_norm.max(), 0.55)
    self.assertAlmostEqual(a_norm.min(), 0.0)
  def test_overlay_pixel_saliency(self):
    # Crate url encoded image representation.
    image_array = np.zeros(shape=(100, 70, 3), dtype=np.uint8)
    pil_image = PILImage.fromarray(image_array)
    image_str = image_utils.convert_pil_to_image_str(pil_image)
    # Create saliency.
    saliency = np.ones(shape=(50, 20), dtype=np.uint8)
    overlay_image = image_utils.overlay_pixel_saliency(
        image_str=image_str,
        saliency=saliency,
        cm_name='bwr',
        clip_fraction=0.01,
        alpha_mul=0.90,
        signed=True,
        pixel_saliency=True)
    self.assertIsNotNone(overlay_image)
    self.assertSequenceEqual((100, 70, 3), np.asarray(overlay_image).shape)
  def test_overlay_area_saliency(self):
    # Crate url encoded image representation.
    image_array = np.zeros(shape=(90, 70, 3), dtype=np.uint8)
    pil_image = PILImage.fromarray(image_array)
    image_str = image_utils.convert_pil_to_image_str(pil_image)
    # Create saliency.
    saliency = np.ones(shape=(50, 30), dtype=np.uint8)
    overlay_image = image_utils.overlay_pixel_saliency(
        image_str=image_str,
        saliency=saliency,
        cm_name='bwr',
        clip_fraction=0.01,
        alpha_mul=0.90,
        signed=False,
        pixel_saliency=False)
    self.assertIsNotNone(overlay_image)
    self.assertSequenceEqual((90, 70, 3), np.asarray(overlay_image).shape)
if __name__ == '__main__':
  absltest.main()

================
File: lit_nlp/lib/image_utils.py
================
"""Contains utility methods used by the image demo app."""
import base64
import io
from typing import Optional
import matplotlib.cm as plt_cm
import numpy as np
from PIL import Image as PILImage
from PIL import ImageEnhance as PILImageEnhance
def convert_image_str_to_pil(image_str: str) -> PILImage.Image:
  # Convert base64 string to PIL image.
  image_str = image_str[image_str.find(';base64,') + 8:]
  img_bytes = base64.b64decode(image_str.encode())
  return PILImage.open(io.BytesIO(img_bytes))
def convert_image_str_to_array(
    image_str: str,
    shape: tuple[int, int, Optional[int]],
    mode: str = 'RGB',
    normalize: bool = True,
) -> np.ndarray:
  """Converts a base64 encoded image to numpy array.
  Args:
    image_str: the base64 encoded image string to decode.
    shape: the (height, width, unused) of the image that will be decoded.
    mode: The PIL Mode to use when decoding. Defaults to RGB. See
      https://pillow.readthedocs.io/en/stable/handbook/concepts.html#concept-modes
        for more info on the modes supported by PIL.
    normalize: If true, normalizes the 8-bit channel values in the returned
      array to the range [0, 1].
  Returns:
    An ndarray of shape (shape[0], )
  """
  pil_image = convert_image_str_to_pil(image_str)
  # Resize image to match the model internal image size.
  pil_image = pil_image.resize(
      (shape[1], shape[0]), PILImage.Resampling.BILINEAR
  )
  # Convert image to the model format.
  pil_image = pil_image.convert(mode=mode)
  # Return image data as an array.
  if normalize:
    return np.asarray(pil_image, dtype=np.float32) / 255
  else:
    return np.asarray(pil_image, dtype=np.uint8)
def convert_pil_to_image_str(pil_image: PILImage.Image) -> str:
  """Converts PIL image to base64 URL encoded string."""
  buffered = io.BytesIO()
  pil_image.save(buffered, format='PNG')
  img_str = base64.b64encode(buffered.getvalue())
  return 'data:image/png;base64,' + img_str.decode('utf-8')
def normalize_signed_saliency(saliency: np.ndarray) -> np.ndarray:
  """Normalizes saliency map while preserving the sign and relative ratios.
  All result values are in interval [0, 1] but may assume a narrower interval.
  Value 0 in the original saliency array is mapped to 0.5 in the result
  saliency. E.g. the normalization of values in range [-10, 100] results in the
  output, which values belong to interval [0,45, 1.0].
  Args:
    saliency: a saliency map that should be normalized.
  Returns:
    The normalized saliency map.
  """
  saliency = saliency.astype(np.float32)
  max_abs_val = np.abs(saliency).max()
  if max_abs_val > 0:
    return (saliency / max_abs_val) / 2 + 0.5
  else:
    return saliency
def normalize_unsigned_saliency(saliency: np.ndarray) -> np.ndarray:
  """Normalizes positive only saliency map to range [0, 1]."""
  assert saliency.min() >= 0
  saliency = saliency.astype(np.float32)
  saliency = saliency - saliency.min()
  max_val = saliency.max()
  if max_val > 0:
    return saliency / max_val
  else:
    return saliency
def clip_signed_saliency(saliency: np.ndarray, fraction=0.01) -> np.ndarray:
  """Clips top and bottom parts if a signed saliency map."""
  b_value = np.quantile(saliency, fraction / 2, method='higher')
  t_value = np.quantile(saliency, 1 - fraction / 2, method='lower')
  return np.clip(saliency, min(0, b_value), max(0, t_value))
def clip_unsigned_saliency(saliency: np.ndarray, fraction=0.01) -> np.ndarray:
  """Clips the top part if an unsigned saliency map."""
  assert saliency.min() >= 0
  t_value = np.quantile(saliency, 1 - fraction, method='lower')
  return np.clip(saliency, 0, t_value)
def overlay_pixel_saliency(image_str: str, saliency: np.ndarray, cm_name: str,
                           clip_fraction: float, alpha_mul: float, signed: bool,
                           pixel_saliency: bool) -> PILImage.Image:
  """Overlays saliency data on top of the input image."""
  # Convert original image to PIL.
  img = convert_image_str_to_pil(image_str)
  img = img.convert(mode='RGBA')
  # Normalize saliency values.
  if signed:
    clipped_saliency = clip_signed_saliency(saliency, fraction=clip_fraction)
    norm_saliency = normalize_signed_saliency(clipped_saliency)
  else:
    saliency = np.abs(saliency)
    clipped_saliency = clip_unsigned_saliency(saliency, fraction=clip_fraction)
    norm_saliency = normalize_unsigned_saliency(clipped_saliency)
  # Map saliency to RGB values.
  cm = plt_cm.get_cmap(cm_name)
  saliency_bytes = cm(norm_saliency)
  # Assign alpha values.
  if pixel_saliency:
    if signed:
      alphas = map(lambda e: abs(e - 0.5) * 2, norm_saliency.flatten())
    else:
      alphas = map(lambda e: 1.0 - e, norm_saliency.flatten())
    alphas = np.reshape(list(alphas), newshape=norm_saliency.shape)
  else:
    alphas = 1.0
  alphas *= alpha_mul
  saliency_bytes[:, :, 3] = alphas
  # Adjust the original image brightness.
  brightness_enhancer = PILImageEnhance.Brightness(img)
  img = brightness_enhancer.enhance(0.5)
  color_enhancer = PILImageEnhance.Color(img)
  img = color_enhancer.enhance(0.0)
  # Overlay original image with the saliency heatmap.
  saliency_bytes = (saliency_bytes * 255).astype(np.uint8)
  saliency_img = PILImage.fromarray(saliency_bytes, mode='RGBA')
  saliency_img = saliency_img.resize(
      size=img.size, resample=PILImage.Resampling.BILINEAR
  )
  heatmap_img = PILImage.alpha_composite(img, saliency_img)
  heatmap_img = heatmap_img.convert(mode='RGB')
  return heatmap_img

================
File: lit_nlp/lib/serialize_test.py
================
"""Tests for serialize."""
import json
from absl.testing import absltest
from absl.testing import parameterized
from lit_nlp.api import dtypes
from lit_nlp.api import types
from lit_nlp.lib import serialize
import numpy as np
class SerializeTest(parameterized.TestCase):
  @parameterized.named_parameters(
      dict(
          testcase_name="data_tuple",
          json_dict={
              "__class__": "DataTuple",
              "__name__": "SpanLabel",
              "start": 0,
              "end": 1,
          },
          expected_type=dtypes.SpanLabel,
      ),
      dict(
          testcase_name="empty",
          json_dict={},
          expected_type=dict,
      ),
      dict(
          testcase_name="lit_type",
          json_dict={
              "required": False,
              "annotated": False,
              "default": "",
              "vocab": ["0", "1"],
              "__class__": "LitType",
              "__name__": "CategoryLabel",
          },
          expected_type=types.CategoryLabel,
      ),
      dict(
          testcase_name="nested",
          json_dict={
              "data_tuple": {
                  "__class__": "DataTuple",
                  "__name__": "SpanLabel",
                  "start": 0,
                  "end": 1,
              },
              "lit_type": {
                  "required": False,
                  "annotated": False,
                  "default": "",
                  "vocab": ["0", "1"],
                  "__name__": "CategoryLabel",
              },
              "np_ndarray": {
                  "__class__": "np.ndarray",
                  "__value__": [1, 2, 3],
              },
              "tuple": {
                  "__class__": "tuple",
                  "__value__": [1, 2, 3],
              },
              "vanilla": {
                  "a": 1,
                  "b": "2",
                  "c": [3, 4, 5],
                  "d": True,
              }
          },
          expected_type=dict,
      ),
      dict(
          testcase_name="np_ndarray",
          json_dict={
              "__class__": "np.ndarray",
              "__value__": [1, 2, 3],
          },
          expected_type=np.ndarray,
      ),
      dict(
          testcase_name="tuple",
          json_dict={
              "__class__": "tuple",
              "__value__": [1, 2, 3],
          },
          expected_type=tuple,
      ),
      dict(
          testcase_name="vanilla",
          json_dict={
              "a": 1,
              "b": "2",
              "c": [3, 4, 5],
              "d": True,
          },
          expected_type=dict,
      ),
  )
  def test_from_json(self, json_dict: types.JsonDict, expected_type):
    json_str = json.dumps(json_dict)
    parsed = serialize.from_json(json_str)
    self.assertIsInstance(parsed, expected_type)
  @parameterized.named_parameters(
      ("name_none", {"__name__": None}),
      ("name_number", {"__name__": 3.14159}),
      ("name_invalid", {"__name__": "not_a_lit_type"}),
  )
  def test_from_json_errors(self, json_dict: types.JsonDict):
    with self.assertRaises(serialize.LitJsonParseError):
      json_str = json.dumps(json_dict)
      _ = serialize.from_json(json_str)
if __name__ == "__main__":
  absltest.main()

================
File: lit_nlp/lib/serialize.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Miscellaneous utility functions."""
import json
from types import MappingProxyType  # pylint: disable=g-importing-member
from typing import Any, cast, Optional
from lit_nlp.api import dtypes
from lit_nlp.api import types
import numpy as np
class LitJsonParseError(ValueError):
  pass
def _obj_to_json(o: object):
  """JSON serialization helper."""
  if isinstance(o, np.ndarray):
    return {
        '__class__': 'np.ndarray',
        '__value__': o.tolist(),
    }
  elif isinstance(o, (np.number, np.bool_)):
    # Handle numpy scalar types, like np.float32
    # This discards some precision information, but is consistent with using
    # .tolist() on a NumPy array.
    return cast(np.number, o).tolist()  # to regular Python scalar
  elif isinstance(o, types.LitType):
    return o.to_json()  # pytype: disable=attribute-error  # enable-nested-classes
  elif isinstance(o, dtypes.DataTuple):
    return o.to_json()
  elif isinstance(o, tuple):
    return {
        '__class__': 'tuple',
        '__value__': list(o),
    }
  elif isinstance(o, MappingProxyType):
    return dict(o)
  else:
    raise TypeError(repr(o) + ' is not JSON serializable.')
# TODO(lit-dev): remove this once frontend can use the invertible versions.
def _obj_to_json_simple(o: object):
  """JSON serialization helper. Not invertible!"""
  if isinstance(o, np.ndarray):
    return o.tolist()
  elif isinstance(o, (np.number, np.bool_)):
    # Handle numpy scalar types, like np.float32
    # This discards some precision information, but is consistent with using
    # .tolist() on a NumPy array.
    return cast(np.number, o).tolist()  # to regular Python scalar
  elif isinstance(o, types.LitType):
    return o.to_json()  # pytype: disable=attribute-error  # enable-nested-classes
  elif isinstance(o, dtypes.DataTuple):
    return o.to_json()
  elif isinstance(o, dtypes.EnumSerializableAsValues):
    return o.value
  elif isinstance(o, tuple):
    return list(o)
  elif isinstance(o, MappingProxyType):
    return dict(o)
  else:
    raise TypeError(repr(o) + ' is not JSON serializable.')
def _obj_from_json(d: dict[str, Any]):
  """JSON deserialization helper.
  Args:
    d: The JSON Object-like dictionary to attempt to parse.
  Returns:
    The parsed JSON as a Python object or class instance.
  Raises:
    LitJsonParseError: If `LitType.from_json()` cannot parse the JSON.
  """
  obj_class = d.pop('__class__', None)
  if obj_class == 'np.ndarray':
    return np.array(d['__value__'])
  elif obj_class == 'DataTuple':
    return dtypes.DataTuple.from_json(d)
  elif obj_class == 'tuple':
    return tuple(d['__value__'])
  else:
    try:
      # The __class__ property was removed from JSON serialized LitTypes in
      # cl/464631365, therefore if obj_class is None try to parse to a LitType
      # TODO(b/260830384): Maybe bringing back __class__ is safer here?
      return types.LitType.from_json(d)
    except KeyError:
      # LitType.from_json() failing because of a KeyError means that the JSON
      # did no have a __name__ property, so it is probably not a LitType.
      # Return it as-is.
      return d
    except (NameError, TypeError) as e:
      # If parsing failed for reasons other than a KeyError, then the JSON has
      # a __name__ property, implying was intended to be a LitType but is
      # erroneously formatted, so we raise a RuntimeError
      raise LitJsonParseError(f'Failed to parse LitType from {d}') from e
##
# Custom encoder classes for using built-in Python3 json library.
# This is a bit clunkier than simplejson, but has the big advantage of
# preserving key order in Python 3.
class SimpleJSONEncoder(json.JSONEncoder):
  def default(self, o):
    return _obj_to_json_simple(o)
class CustomJSONEncoder(json.JSONEncoder):
  def default(self, o):
    return _obj_to_json(o)
def from_json(json_string: str) -> Optional[dict[str, Any]]:
  """Reconstruct from a JSON string."""
  if json_string:
    return json.loads(json_string, object_hook=_obj_from_json)
  return None
def to_json(obj, simple=False, **json_kw) -> str:
  """Serialize to a JSON string."""
  cls = SimpleJSONEncoder if simple else CustomJSONEncoder
  return json.dumps(obj, cls=cls, **json_kw)

================
File: lit_nlp/lib/testing_utils.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Testing utilities for python backend.
Contains things like dummy LIT Model so we don't have to define it in every
test.
"""
from collections.abc import Iterable, Iterator
from lit_nlp.api import model as lit_model
from lit_nlp.api import types as lit_types
import numpy as np
import numpy.testing as npt
JsonDict = lit_types.JsonDict
class RegressionModelForTesting(lit_model.BatchedModel):
  """Implements lit.Model interface for testing.
  This class allows flexible input spec to allow different testing scenarios.
  """
  def __init__(self, input_spec: lit_types.Spec):
    """Set input spec.
    Args:
      input_spec: An input spec.
    """
    self._input_spec = input_spec
  # LIT API implementation
  def input_spec(self):
    return self._input_spec
  def output_spec(self):
    return {'scores': lit_types.RegressionScore(parent='label')}
  def predict_minibatch(self, inputs: list[JsonDict], **kw):
    return self.predict(inputs)
  def predict(self, inputs: Iterable[JsonDict], *args,
              **kw) -> Iterator[JsonDict]:
    """Return 0.0 regression values for all examples.
    Args:
      inputs: input examples
      *args: unused
      **kw: unused
    Returns:
      predictions
    """
    return map(lambda x: {'scores': 0.0}, inputs)
class IdentityRegressionModelForTesting(lit_model.BatchedModel):
  """Implements lit.Model interface for testing.
  This class reflects the input in the prediction for simple testing.
  """
  def __init__(self):
    self._count = 0
  def input_spec(self):
    return {'val': lit_types.Scalar()}
  def output_spec(self):
    return {'score': lit_types.RegressionScore()}
  def predict_minibatch(self, inputs: list[JsonDict], **kw):
    return self.predict(inputs)
  def predict(self, inputs: Iterable[JsonDict], *args,
              **kw) -> Iterator[JsonDict]:
    """Return input value for all examples.
    Args:
      inputs: input examples
      *args: unused
      **kw: unused
    Returns:
      predictions
    """
    results = [{'score': input['val']} for input in inputs]
    self._count += len(results)
    return iter(results)
  @property
  def count(self):
    """Returns the number of times predict has been called."""
    return self._count
class ClassificationModelForTesting(lit_model.BatchedModel):
  """Implements lit.Model interface for testing classification models.
     Returns the same output for every input.
  """
  # LIT API implementation
  def input_spec(self):
    return {'input_embs': lit_types.TokenEmbeddings(align='tokens',
                                                    required=False),
            'segment': lit_types.TextSegment(),
            'grad_class': lit_types.CategoryLabel(vocab=['0', '1'])}
  def output_spec(self):
    return {
        'probas':
            lit_types.MulticlassPreds(
                parent='label', vocab=['0', '1'], null_idx=0),
        'input_embs':
            lit_types.TokenEmbeddings(align='tokens'),
        'input_embs_grad':
            lit_types.TokenGradients(
                align='tokens',
                grad_for='input_embs',
                grad_target_field_key='grad_class'),
        'tokens':
            lit_types.Tokens(),
        'grad_class':
            lit_types.CategoryLabel(vocab=['0', '1'])
    }
  def predict_minibatch(self, inputs: list[JsonDict], **kw):
    output = {
        'probas': np.array([0.2, 0.8]),
        'input_embs': np.array(
            [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]
        ),
        'input_embs_grad': np.array(
            [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]
        ),
        'tokens': ['test'],
        'grad_class': '1',
    }
    return map(lambda x: output, inputs)
def fake_projection_input(n, num_dims):
  """Generates random embeddings in the correct format."""
  rng = np.random.RandomState(42)
  return [{'x': rng.rand(num_dims)} for _ in range(n)]
def assert_deep_almost_equal(testcase, result, actual, places=4):
  """Checks if provided inputs are almost equal, recurses on dicts values."""
  if isinstance(result, (int, float)):
    testcase.assertAlmostEqual(result, actual, places=places)
  elif isinstance(result, (list)):
    if all(isinstance(n, (int, float)) for n in result):
      rtol = 10 ** (-1 * places)
      npt.assert_allclose(result, actual, rtol=rtol, atol=1e-4)
    elif all(isinstance(n, dict) for n in result):
      for i in range(len(result)):
        assert_deep_almost_equal(testcase, result[i], actual[i])
  elif isinstance(result, dict):
    if set(result.keys()) != set(actual.keys()):
      testcase.fail('results and actual have different keys')
    for key in result:
      assert_deep_almost_equal(testcase, result[key], actual[key])
class CustomOutputModelForTesting(lit_model.BatchedModel):
  """Implements lit.Model interface for testing.
  This class allows user-specified outputs for testing return values.
  """
  def __init__(
      self,
      input_spec: lit_types.Spec,
      output_spec: lit_types.Spec,
      results: list[JsonDict],
  ):
    """Set model internals.
    Args:
      input_spec: An input spec.
      output_spec: An output spec.
      results: Results to return.
    """
    self._input_spec = input_spec
    self._output_spec = output_spec
    self._predict_counter = 0
    self._results = results
  # LIT API implementation
  def input_spec(self):
    return self._input_spec
  def output_spec(self):
    return self._output_spec
  def predict_minibatch(self, inputs: list[JsonDict], **kw):
    def predict_single(_):
      output = self._results[self._predict_counter % len(self._results)]
      self._predict_counter += 1
      return output
    return map(predict_single, inputs)

================
File: lit_nlp/lib/ui_state.py
================
# Copyright 2022 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Selection state tracker.
This is a stateful component, intended for use in notebook/Colab contexts
to sync the UI selection state back to Python for further analysis.
"""
from typing import Optional
from absl import logging
import attr
from lit_nlp.api import dataset as lit_dataset
from lit_nlp.api import types
IndexedInput = types.IndexedInput
JsonDict = types.JsonDict
@attr.s(auto_attribs=True, kw_only=True)
class UIState(object):
  """UI state."""
  dataset_name: Optional[str] = None
  dataset: Optional[lit_dataset.IndexedDataset] = None
  primary: Optional[IndexedInput] = None
  selection: list[IndexedInput] = attr.Factory(list)
  pinned: Optional[IndexedInput] = None
class UIStateTracker(object):
  """UI state tracker; mirrors state from frontend SelectionService.
  WARNING: this component is _stateful_, and in current form implements no
  locking or access control. We recommend using this only in a single-user,
  single-threaded context such as IPython or Colab notebooks.
  """
  def __init__(self):
    self._state = UIState()
  @property
  def state(self):
    return self._state
  def update_state(self,
                   indexed_inputs: list[types.IndexedInput],
                   dataset: lit_dataset.IndexedDataset,
                   dataset_name: str,
                   primary_id: Optional[str] = None,
                   pinned_id: Optional[str] = None):
    """Update state from the UI."""
    self._state.dataset_name = dataset_name
    self._state.dataset = dataset
    # This may contain 'added' datapoints not in the base dataset.
    input_index = {ex["data"]["_id"]: ex for ex in indexed_inputs}
    def get_example(example_id):
      ex = input_index.get(example_id)
      if ex is None:
        ex = dataset.index.get(example_id)
      return ex
    if primary_id:
      self._state.primary = get_example(primary_id)
      if self._state.primary is None:
        logging.warn("State tracker: unable to find primary_id %s", primary_id)
    else:
      self._state.primary = None
    self._state.selection = indexed_inputs
    if pinned_id:
      self._state.pinned = get_example(pinned_id)
      if self._state.pinned is None:
        logging.warn("State tracker: unable to find pinned_id %s", pinned_id)
    else:
      self._state.pinned = None

================
File: lit_nlp/lib/utils_test.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Tests for lit_nlp.lib.utils."""
from collections.abc import Callable, Sequence
import copy
from typing import Any, Optional, TypeVar, Union
from absl.testing import absltest
from absl.testing import parameterized
from lit_nlp.api import types
from lit_nlp.lib import utils
import numpy as np
_BATCHING_RECORDS: Sequence[types.JsonDict] = [
    {"foo": 1, "bar": "one"},
    {"foo": 2, "bar": "two"},
    {"foo": 3, "bar": "three"},
]
_VALIDATION_SPEC: types.Spec = {
    "required_scalar": types.Scalar(),
    "required_text_segment": types.String(),
    "optional_boolean": types.Boolean(required=False),
}
T = TypeVar("T")
class UtilsTest(parameterized.TestCase):
  @parameterized.named_parameters(
      ("bool", True),
      ("bool_as_javascript_str", "true"),
      ("bool_as_str", "True"),
      ("non_zero_int", 1),
      ("non_zero_float", -2.2),
      ("non_empty_dict", {"a": "hi"}),
      ("non_empty_list", [0]),
      ("non_empty_string", "this is true"),
  )
  def test_coerce_bool_true(self, value: Any):
    self.assertTrue(utils.coerce_bool(value))
  @parameterized.named_parameters(
      ("bool", False),
      ("bool_as_javascript_str", "false"),
      ("bool_as_str", "False"),
      ("zero", 0),
      ("zero_as_str", "0"),
      ("empty_dict", {}),
      ("empty_list", []),
      ("empty_str", ""),
  )
  def test_coerce_bool_false(self, value: Any):
    self.assertFalse(utils.coerce_bool(value))
  @parameterized.named_parameters(
      dict(
          testcase_name="with_truthy_values",
          d={"a": True, "b": False, "c": True},
          predicate=lambda a: a,
          expected=["a", "c"],
      ),
      dict(
          testcase_name="with_specific_value_missing",
          d={"a": True, "b": False, "c": True},
          predicate=lambda a: a == "nothing",
          expected=[],
      ),
      dict(
          testcase_name="where_d_is_empty",
          d={},
          predicate=lambda a: a,
          expected=[],
      ),
  )
  def test_find_keys(
      self,
      d: dict[str, bool],
      predicate: Callable[[utils.V], bool],
      expected: Sequence[str],
  ):
    found = utils.find_keys(d, predicate)
    self.assertEqual(expected, found)
  @parameterized.named_parameters(
      dict(
          testcase_name="all",
          types_to_find=types.LitType,
          expected=[
              "score",
              "scalar_foo",
              "text",
              "emb_0",
              "emb_1",
              "tokens",
              "generated_text",
          ],
      ),
      dict(
          testcase_name="attention_heads",
          types_to_find=types.AttentionHeads,
          expected=[],
      ),
      dict(
          testcase_name="embeddings",
          types_to_find=types.Embeddings,
          expected=["emb_0", "emb_1"],
      ),
      dict(
          testcase_name="regression_only",
          types_to_find=types.RegressionScore,
          expected=["score"],
      ),
      dict(
          testcase_name="scalars",  # includes RegressionScore subclass
          types_to_find=types.Scalar,
          expected=["score", "scalar_foo"],
      ),
      dict(
          testcase_name="text_segement",  # Includes GeneratedText subclass
          types_to_find=types.TextSegment,
          expected=["text", "generated_text"],
      ),
      dict(
          testcase_name="text_like_tuple",
          types_to_find=(types.TextSegment, types.Tokens),
          expected=["text", "tokens", "generated_text"],
      ),
  )
  def test_find_spec_keys(
      self,
      types_to_find: Union[types.LitType, Sequence[types.LitType]],
      expected: Sequence[str],
  ):
    spec = {
        "score": types.RegressionScore(),
        "scalar_foo": types.Scalar(),
        "text": types.TextSegment(),
        "emb_0": types.Embeddings(),
        "emb_1": types.Embeddings(),
        "tokens": types.Tokens(),
        "generated_text": types.GeneratedText(),
    }
    found = utils.find_spec_keys(spec, types_to_find)
    self.assertEqual(expected, found)
  @parameterized.named_parameters(
      dict(
          testcase_name="d_contains_predicate_keys",
          d={"a": True, "b": False, "c": True},
          expected={"a": True, "b": False},
      ),
      dict(
          testcase_name="d_is_empty",
          d={},
          expected={},
      ),
      dict(
          testcase_name="d_missing_predicate_keys",
          d={"1": True, "2": False, "3": True},
          expected={},
      ),
  )
  def test_filter_by_keys(
      self, d: dict[utils.K, utils.V], expected: dict[utils.K, utils.V]
  ):
    predicate = lambda k: k in ("a", "b")
    filtered = utils.filter_by_keys(d, predicate)
    self.assertEqual(expected, filtered)
  @parameterized.named_parameters(
      dict(
          testcase_name="keys_is_empty",
          keys=[],
          expected={},
      ),
      dict(
          testcase_name="keys_is_None",
          keys=None,
          expected={"foo": [1, 2, 3], "bar": ["one", "two", "three"]},
      ),
      dict(
          testcase_name="keys_are_a_subset",
          keys=["bar"],
          expected={"bar": ["one", "two", "three"]},
      ),
      dict(
          testcase_name="keys_are_the_totality",
          keys=["foo", "bar"],
          expected={"foo": [1, 2, 3], "bar": ["one", "two", "three"]},
      ),
  )
  def test_batch_inputs(
      self, keys: Optional[list[str]], expected: dict[str, list[Any]]
  ):
    batched = utils.batch_inputs(_BATCHING_RECORDS, keys=keys)
    self.assertEqual(expected, batched)
  @parameterized.named_parameters(
      ("AssertionError_for_empty_inputs", [], None, AssertionError),
      ("KeyError_for_disjoint_keys", _BATCHING_RECORDS, ["baz"], KeyError),
  )
  def test_batch_inputs_raises(
      self,
      inputs: Sequence[types.JsonDict],
      keys: Optional[list[str]],
      expected: Exception,
  ):
    with self.assertRaises(expected):
      utils.batch_inputs(inputs, keys=keys)
  @parameterized.named_parameters(
      dict(
          testcase_name="pad_to_end",
          inputs=[1, 2, 3],
          min_len=5,
          pad_val=0,
          expected=[1, 2, 3, 0, 0],
      ),
      dict(
          testcase_name="pad_length_exact",
          inputs=[1, 2, 3],
          min_len=3,
          pad_val=0,
          expected=[1, 2, 3],
      ),
      dict(
          testcase_name="pad_too_long",
          inputs=[1, 2, 3, 4, 5],
          min_len=3,
          pad_val=0,
          expected=[1, 2, 3, 4, 5],
      ),
      dict(
          testcase_name="pad_with_strings",
          inputs=["one", "two", "three"],
          min_len=5,
          pad_val="",
          expected=["one", "two", "three", "", ""],
      ),
      dict(
          testcase_name="truncate_max_len",
          inputs=[1, 2, 3, 4, 5],
          min_len=3,
          pad_val=0,
          max_len=3,
          expected=[1, 2, 3],
      ),
      dict(
          testcase_name="pad_left",
          inputs=[1, 2, 3],
          min_len=5,
          pad_val=0,
          pad_left=True,
          expected=[0, 0, 1, 2, 3],
      ),
      dict(
          testcase_name="truncate_max_len_left",
          inputs=[1, 2, 3, 4, 5],
          min_len=3,
          pad_val=0,
          pad_left=True,
          max_len=3,
          expected=[3, 4, 5],
      ),
      dict(
          testcase_name="pad_left_with_strings",
          inputs=["one", "two", "three"],
          min_len=5,
          pad_val="",
          pad_left=True,
          expected=["", "", "one", "two", "three"],
      ),
  )
  def test_pad1d(
      self,
      inputs: list[T],
      min_len: T,
      pad_val: T,
      expected: list[T],
      pad_left: bool = False,
      max_len: Optional[int] = None,
  ):
    self.assertEqual(
        utils.pad1d(
            inputs, min_len, pad_val, pad_left=pad_left, max_len=max_len
        ),
        expected,
    )
  @parameterized.named_parameters(
      dict(
          testcase_name="remap_to_new_names",
          d={"a": True, "b": False, "c": True},
          keymap={"a": "a2", "b": "b2"},
          expected={"a2": True, "b2": False, "c": True},
      ),
      dict(
          testcase_name="remap_to_existing_name",
          d={"a": True, "b": False, "c": True},
          keymap={"a": "b"},
          expected={"b": False, "c": True},
      ),
      dict(
          testcase_name="keymap_is_empty",
          d={"a": True, "b": False, "c": True},
          keymap={},
          expected={"a": True, "b": False, "c": True},
      ),
      dict(
          testcase_name="dict_is_empty",
          d={},
          keymap={"a": "a2", "b": "b2"},
          expected={},
      ),
  )
  def test_remap_dict(
      self,
      d: dict[utils.K, utils.V],
      keymap: dict[utils.K, utils.K],
      expected: dict[utils.K, utils.V],
  ):
    remapped = utils.remap_dict(d, keymap)
    self.assertEqual(expected, remapped)
  @parameterized.named_parameters(
      dict(
          testcase_name="min_and_max_within_bounds",
          min_element_count=2,
          max_element_count=3,
          expected=[
              [1, 2],
              [1, 3],
              [1, 4],
              [2, 3],
              [2, 4],
              [3, 4],
              [1, 2, 3],
              [1, 2, 4],
              [1, 3, 4],
              [2, 3, 4],
          ],
      ),
      dict(
          testcase_name="max_is_greater_than_len",
          min_element_count=2,
          max_element_count=10,
          expected=[
              [1, 2],
              [1, 3],
              [1, 4],
              [2, 3],
              [2, 4],
              [3, 4],
              [1, 2, 3],
              [1, 2, 4],
              [1, 3, 4],
              [2, 3, 4],
              [1, 2, 3, 4],
          ],
      ),
      dict(
          testcase_name="min_is_greater_than_max",
          min_element_count=3,
          max_element_count=2,
          expected=[],
      ),
      dict(
          testcase_name="min_is_negative",
          min_element_count=-1,
          max_element_count=2,
          expected=[
              [1],
              [2],
              [3],
              [4],
              [1, 2],
              [1, 3],
              [1, 4],
              [2, 3],
              [2, 4],
              [3, 4],
          ],
      ),
  )
  def test_find_all_combinations(
      self,
      min_element_count: int,
      max_element_count: int,
      expected: list[list[int]],
  ):
    combinations = utils.find_all_combinations(
        [1, 2, 3, 4], min_element_count, max_element_count
    )
    self.assertEqual(combinations, expected)
  def test_get_real(self):
    l = np.array([1, 2, 3, 4])
    self.assertListEqual(utils.coerce_real(l).tolist(), l.tolist())
    l = np.array([1, 2 + 0.5j, 3, 4])
    self.assertListEqual(utils.coerce_real(l, 0.51).tolist(), [1, 2, 3, 4])
    with self.assertRaises(AssertionError):
      utils.coerce_real(l, 0.4)
  @parameterized.named_parameters(
      dict(
          testcase_name="with_all_params",
          config={
              "required_scalar": 0,
              "required_text_segment": "test",
              "optional_boolean": True,
          },
      ),
      dict(
          testcase_name="with_extra_params",
          config={
              "required_scalar": 0,
              "required_text_segment": "test",
              "optional_boolean": True,
              "param_not_in_spec": True,
          },
      ),
      dict(
          testcase_name="with_only_required_params",
          config={
              "required_scalar": 0,
              "required_text_segment": "test",
          },
      ),
  )
  def test_validate_config_against_spec(self, config: types.JsonDict):
    validated = utils.validate_config_against_spec(
        config, _VALIDATION_SPEC, "unittest"
    )
    self.assertIs(config, validated)
  @parameterized.named_parameters(
      dict(
          testcase_name="for_missing_params",
          config={
              "required_scalar": 0,
              "optional_boolean": True,
          },
      ),
      dict(
          testcase_name="for_unsupported_params",
          config={
              "required_scalar": 0,
              "required_text_segment": "test",
              "param_not_in_spec": True,
          },
      ),
  )
  def test_validate_config_against_spec_raises(self, config: types.JsonDict):
    with self.assertRaises(KeyError):
      utils.validate_config_against_spec(
          config, _VALIDATION_SPEC, "unittest", raise_for_unsupported=True
      )
  def test_combine_specs_not_overlapping(self):
    spec1 = {"string": types.String()}
    spec2 = {"int": types.Integer()}
    spec = utils.combine_specs(spec1, spec2)
    self.assertEqual(spec, {"string": types.String(), "int": types.Integer()})
  def test_combine_specs_overlapping_and_compatible(self):
    spec1 = {"string": types.String()}
    spec2 = {"int": types.Integer(), "string": types.String()}
    spec = utils.combine_specs(spec1, spec2)
    self.assertEqual(spec, {"string": types.String(), "int": types.Integer()})
  def test_combine_specs_conflicting(self):
    spec1 = {"string": types.String()}
    spec2 = {"string": types.TextSegment()}
    with self.assertRaises(ValueError):
      utils.combine_specs(spec1, spec2)
  def test_make_modified_input_one_field(self):
    ex = {
        "foo": 123,
        "bar": 234,
        "_id": "a1b2c3",
        "_meta": {"parentId": "000000"},
    }
    copy_of_original = copy.deepcopy(ex)
    new_ex = utils.make_modified_input(ex, {"bar": 345}, "testFn")
    expected = {
        "foo": 123,
        "bar": 345,
        "_id": "",
        "_meta": {"parentId": "a1b2c3", "added": True, "source": "testFn"},
    }
    self.assertEqual(new_ex, expected)
    # Check that original is unchanged
    self.assertEqual(ex, copy_of_original)
  def test_make_modified_input_two_fields(self):
    ex = {
        "foo": 123,
        "bar": 234,
        "_id": "a1b2c3",
        "_meta": {"parentId": "000000"},
    }
    new_ex = utils.make_modified_input(ex, {"foo": 234, "bar": 345}, "testFn")
    expected = {
        "foo": 234,
        "bar": 345,
        "_id": "",
        "_meta": {"parentId": "a1b2c3", "added": True, "source": "testFn"},
    }
    self.assertEqual(new_ex, expected)
  def test_make_modified_input_new_field(self):
    ex = {
        "foo": 123,
        "bar": 234,
        "_id": "a1b2c3",
        "_meta": {"parentId": "000000"},
    }
    new_ex = utils.make_modified_input(ex, {"baz": "spam and eggs"}, "testFn")
    expected = {
        "foo": 123,
        "bar": 234,
        "baz": "spam and eggs",
        "_id": "",
        "_meta": {"parentId": "a1b2c3", "added": True, "source": "testFn"},
    }
    self.assertEqual(new_ex, expected)
  def test_make_modified_input_unmodified(self):
    ex = {
        "foo": 123,
        "bar": 234,
        "_id": "a1b2c3",
        "_meta": {"parentId": "000000"},
    }
    copy_of_original = copy.deepcopy(ex)
    new_ex = utils.make_modified_input(ex, {"foo": 123, "bar": 234}, "testFn")
    self.assertEqual(new_ex, copy_of_original)
    self.assertIs(new_ex, ex)  # same object back
  def test_make_modified_input_empty_overrides(self):
    ex = {
        "foo": 123,
        "bar": 234,
        "_id": "a1b2c3",
        "_meta": {"parentId": "000000"},
    }
    copy_of_original = copy.deepcopy(ex)
    new_ex = utils.make_modified_input(ex, {}, "testFn")
    self.assertEqual(new_ex, copy_of_original)
    self.assertIs(new_ex, ex)  # same object back
  def test_make_modified_input_not_indexed(self):
    ex = {
        "foo": 123,
        "bar": 234,
    }
    copy_of_original = copy.deepcopy(ex)
    new_ex = utils.make_modified_input(ex, {"bar": 345}, "testFn")
    expected = {
        "foo": 123,
        "bar": 345,
    }
    self.assertEqual(new_ex, expected)
    # Check that original is unchanged
    self.assertEqual(ex, copy_of_original)
if __name__ == "__main__":
  absltest.main()

================
File: lit_nlp/lib/utils.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Miscellaneous helper functions."""
from collections.abc import Callable, Collection, Iterable, Iterator, Mapping, Sequence
import itertools
import queue
import threading
import time
from typing import Any, Optional, TypeVar, Union
import uuid
from lit_nlp.api import types as lit_types
import numpy as np
T = TypeVar('T')
K = TypeVar('K')
V = TypeVar('V')
def coerce_bool(value) -> bool:
  if isinstance(value, (bool, int, float, list, dict)):
    return bool(value)
  elif value is None:
    return False
  elif str(value).lower() in ['', '0', 'false']:
    return False
  else:
    return True
def find_keys(d: Mapping[K, V], predicate: Callable[[V], bool]) -> list[K]:
  """Find keys where values match predicate."""
  return [k for k, v in d.items() if predicate(v)]
def find_spec_keys(d: Mapping[K, Any], types) -> list[K]:
  """Find keys where values match one or more types."""
  return find_keys(d, lambda v: isinstance(v, types))
def filter_by_keys(
    d: Mapping[K, V], predicate: Callable[[K], bool]
) -> dict[K, V]:
  """Filter to keys matching predicate."""
  return {k: v for k, v in d.items() if predicate(k)}
def spec_contains(d: dict[str, Any], types) -> bool:
  """Returns true if the spec contains any field with one of these types."""
  return bool(find_spec_keys(d, types))
def remap_dict(d: Mapping[K, V], keymap: Mapping[K, K]) -> dict[K, V]:
  """Return a (shallow) copy of d with some fields renamed.
  Keys which are not in keymap are left alone.
  Args:
    d: dict to rename
    keymap: map of old key -> new key
  Returns:
    new dict with fields renamed
  """
  return {keymap.get(k, k): d[k] for k in d}
def _strict_numpy_equals(a, b):
  """Verify structural equality and type match."""
  # pylint: disable-next=unidiomatic-typecheck
  return np.array_equal(a, b) and type(a) == type(b)
def make_modified_input(
    ex: lit_types.JsonDict,
    overrides: lit_types.JsonDict,
    source: Optional[str] = None,
):
  """Make a modified (copy of) an input example.
  Prefer this to directly updating a dict, since this makes a copy and will
  reset the example ID if the values change.
  Args:
    ex: original example
    overrides: dict of new values
    source: optional source name (goes in _meta)
  Returns:
    ex or a modified copy
  """
  for k in overrides:
    if (k not in ex) or not _strict_numpy_equals(overrides[k], ex[k]):
      new_example = dict(ex, **overrides)
      # If example was indexed, update the index info (_id and _meta).
      if '_id' in ex:
        new_example['_id'] = ''
      if '_meta' in ex:
        new_example['_meta'] = lit_types.InputMetadata(
            added=True, parentId=ex.get('_id'), source=source
        )
      return new_example
  return ex  # unmodified
def rate_limit(iterable, qps: Union[int, float]):
  """Rate limit an iterator."""
  for item in iterable:
    yield item
    time.sleep(1.0 / qps)
def batch_iterator(
    items: Iterable[T], max_batch_size: int
) -> Iterator[list[T]]:
  """Create batches from an input stream.
  Use this to create batches, e.g. to feed to a model.
  The output can be easily flattened again using itertools.chain.from_iterable.
  Args:
    items: stream of items
    max_batch_size: maximum size of resulting batches
  Yields:
    batches of size <= max_batch_size
  """
  minibatch = []
  for item in items:
    if len(minibatch) < max_batch_size:
      minibatch.append(item)
    if len(minibatch) >= max_batch_size:
      yield minibatch
      minibatch = []
  if len(minibatch) > 0:  # pylint: disable=g-explicit-length-test
    yield minibatch
def batch_inputs(
    input_records: Sequence[Mapping[K, V]], keys: Optional[Collection[K]] = None
) -> dict[K, list[V]]:
  """Batch inputs from list-of-dicts to dict-of-lists."""
  assert input_records, 'Must have non-empty batch!'
  if keys is None:
    keys = input_records[0].keys()
  ret = {}
  for k in keys:
    ret[k] = [r[k] for r in input_records]
  return ret
def _extract_batch_length(preds):
  """Extracts batch length of predictions."""
  batch_length = None
  for key, value in preds.items():
    this_length = (
        len(value) if isinstance(value, (list, tuple)) else value.shape[0]
    )
    batch_length = batch_length or this_length
    if this_length != batch_length:
      raise ValueError('Batch length of predictions should be same. %s has '
                       'different batch length than others.' % key)
  return batch_length
def unbatch_preds(
    preds: Union[Mapping[K, Sequence[V]], Sequence[dict[K, V]]]
) -> Iterable[dict[K, V]]:
  """Unbatch predictions, as in estimator.predict().
  Args:
    preds: dict[str, np.ndarray], where all arrays have the same first
      dimension.
  Yields:
    sequence of dict[str, np.ndarray], with the same keys as preds.
  """
  if not isinstance(preds, dict):
    for pred in preds:
      yield pred
  else:
    for i in range(_extract_batch_length(preds)):
      yield {key: value[i] for key, value in preds.items()}
def pad1d(
    arr: list[T],
    min_len: int,
    pad_val: T,
    pad_left: bool = False,
    max_len: Optional[int] = None,
) -> list[T]:
  """Pad a list to the target length."""
  if pad_left:
    padded = [pad_val] * max(0, min_len - len(arr)) + arr
    return padded[-max_len:] if max_len is not None else padded
  else:
    padded = arr + [pad_val] * max(0, min_len - len(arr))
    return padded[:max_len] if max_len is not None else padded
def find_all_combinations(
    l: list[Any], min_element_count: int, max_element_count: int
) -> list[list[Any]]:
  """Finds all possible ways how elements of a list can be combined.
  E.g., all combinations of list [1, 2, 3] are
  [[1], [2], [3], [1, 2], [1, 3], [2, 3], [1, 2, 3]].
  Args:
    l: a list of arbitrary elements.
    min_element_count: the minimum number of elements that every combination
      should contain.
    max_element_count: the maximum number of elements that every combination
      should contain.
  Returns:
    The list of all possible combinations given the constraints.
  """
  result: list[list[Any]] = []
  min_element_count = max(1, min_element_count)
  max_element_count = min(max_element_count, len(l))
  for element_count in range(min_element_count, max_element_count + 1):
    result.extend(list(x) for x in itertools.combinations(l, element_count))
  return result
def coerce_real(vals: np.ndarray, limit=0.0001):
  """Return a copy of the array with only the real numbers, with a check.
  If any of the imaginary part of a value is greater than the provided limit,
  then assert an error.
  Args:
    vals: The array to convert
    limit: The limit above which any imaginary part of a value causes an error.
  Returns:
    The array with only the real portions of the numbers.
  """
  assert np.all(np.imag(vals) < limit), (
      'Array contains imaginary part out of acceptable limits.')
  return np.real(vals)
def get_uuid():
  """Return a randomly-generated UUID hex string."""
  return uuid.uuid4().hex
def validate_config_against_spec(
    config: lit_types.JsonDict,
    spec: lit_types.Spec,
    name: str,
    raise_for_unsupported: bool = False,
):
  """Validates that the provided config is compatible with the Spec.
  Args:
    config: The configuration parameters, such as extracted from the data of an
      HTTP Request, that are to be used in a function call.
    spec: A Spec defining the shape of allowed configuration parameters for the
      associated LIT component.
    name: The name of the endpoint, interpreter, etc. providing the Spec against
      which the config is valdiated.
    raise_for_unsupported: If true, raises a KeyError if the config contains
      keys that are not present in the Spec. Unsupported keys are assumed to be
      acceptable for subclasses of lit_nlp.api.components, but unacceptable for
      APIs that instantiate new instances of a class (e.g., /create_dataset).
  Returns:
    The config passed in as the first argument, if validation is successful.
  Raises:
    KeyError: Under two conditions: 1) the `config` is missing one or more
      required fields defined in the `spec`, or 2) the `config` contains fields
      not defined in the `spec`. Either of these conditions would likely result
      in a TypeError (for missing or unexpected arguments) if the `config` was
      used in a call.
  """
  missing_required_keys = [
      param_name for param_name, param_type in spec.items()
      if param_type.required and param_name not in config
  ]
  if missing_required_keys:
    raise KeyError(f'{name} missing required params: {missing_required_keys}')
  unsupported_keys = [
      param_name for param_name in config
      if param_name not in spec
  ]
  if raise_for_unsupported and unsupported_keys:
    raise KeyError(f'{name} received unsupported params: {unsupported_keys}')
  return config
def combine_specs(spec1: lit_types.Spec, spec2: lit_types.Spec):
  """Combine the fields in two specs.
  Args:
    spec1: the first spec.
    spec2: the second spec.
  Returns:
    A new spec with the combined fields of spec1 and spec2.
  Raises:
    ValueError, when these two specs have the same keys corresponding to
    different values.
  """
  # Ensure that there are no conflicting spec keys.
  conflicts = [k for k, v in spec1.items() if k in spec2 and spec2[k] != v]
  if conflicts:
    conflict_types: dict[str, tuple[lit_types.LitType, lit_types.LitType]] = {
        k: (spec1[k], spec2[k]) for k in conflicts
    }
    raise ValueError(f'Conflicting spec keys: {conflict_types}')
  combined_spec = {} | spec1 | spec2
  return combined_spec
class TaskQueue(queue.Queue):
  """A simple task queue for processing jobs in a thread pool."""
  def __init__(self, num_workers=1):
    # TODO(lit-dev): Could use QueueHandler and QueueListener for this.
    queue.Queue.__init__(self)
    self.num_workers = num_workers
    self.start_workers()
  def add_task(self, task, *args, **kwargs):
    args = args or ()
    kwargs = kwargs or {}
    self.put((task, args, kwargs))
  def start_workers(self):
    for _ in range(self.num_workers):
      t = threading.Thread(target=self.worker)
      t.daemon = True
      t.start()
  def worker(self):
    while True:
      item, args, kwargs = self.get()
      item(*args, **kwargs)
      self.task_done()

================
File: lit_nlp/lib/validation_test.py
================
"""Tests for validation."""
from absl.testing import absltest
from absl.testing import parameterized
from lit_nlp.api import dataset
from lit_nlp.api import types
from lit_nlp.lib import testing_utils
from lit_nlp.lib import validation
class ValidationTest(parameterized.TestCase):
  @parameterized.named_parameters(
      dict(
          testcase_name="all_required_all_present",
          spec={
              "score": types.Scalar(),
              "text": types.TextSegment(),
          },
          examples=[{"score": 0, "text": "a"}, {"score": 1, "text": "b"}],
      ),
      dict(
          testcase_name="some_required_all_present",
          spec={
              "score": types.Scalar(required=False),
              "text": types.TextSegment(),
          },
          examples=[{"score": 0, "text": "a"}, {"score": 1, "text": "b"}],
      ),
      dict(
          testcase_name="some_required_some_present",
          spec={
              "score": types.Scalar(required=False),
              "text": types.TextSegment(),
          },
          examples=[{"text": "a"}, {"text": "b"}],
      ),
  )
  def test_validate_dataset_with_base_dataset(
      self, spec: types.Spec, examples: list[types.JsonDict]
  ):
    ds = dataset.Dataset(spec=spec, examples=examples)
    try:
      validation.validate_dataset(ds, False)
    except ValueError:
      self.fail("Raised unexpected error.")
  @parameterized.named_parameters(
      dict(
          testcase_name="all_required_all_present",
          spec={
              "score": types.Scalar(),
              "text": types.TextSegment(),
          },
          examples=[
              {"data": {"score": 0, "text": "a"}, "id": 0, "meta": {}},
              {"data": {"score": 1, "text": "b"}, "id": 1, "meta": {}},
          ],
      ),
      dict(
          testcase_name="some_required_all_present",
          spec={
              "score": types.Scalar(required=False),
              "text": types.TextSegment(),
          },
          examples=[
              {"data": {"score": 0, "text": "a"}, "id": 0, "meta": {}},
              {"data": {"score": 1, "text": "b"}, "id": 1, "meta": {}},
          ],
      ),
      dict(
          testcase_name="some_required_some_present",
          spec={
              "score": types.Scalar(required=False),
              "text": types.TextSegment(),
          },
          examples=[
              {"data": {"text": "a"}, "id": 0, "meta": {}},
              {"data": {"text": "b"}, "id": 1, "meta": {}},
          ],
      ),
  )
  def test_validate_dataset_with_indexed_dataset(
      self, spec: types.Spec, examples: list[types.IndexedInput]
  ):
    ds = dataset.IndexedDataset(
        spec=spec, id_fn=lambda a: a, indexed_examples=examples
    )
    try:
      validation.validate_dataset(ds, False)
    except ValueError:
      self.fail("Raised unexpected error.")
  @parameterized.named_parameters(
      dict(
          testcase_name="first_field_not_required",
          spec={
              "score": types.Scalar(required=False),
              "text": types.TextSegment(),
          },
          examples=[{"score": 0, "text": "a"}, {"score": 1, "text": "b"}],
      ),
      dict(
          testcase_name="second_field_not_required",
          spec={
              "score": types.Scalar(),
              "text": types.TextSegment(required=False),
          },
          examples=[{"score": 0, "text": "a"}, {"score": 1, "text": "b"}],
      ),
  )
  def test_validate_dataset_raises_for_enforce_all_fields_required(
      self, spec: types.Spec, examples: list[types.JsonDict]
  ):
    ds = dataset.Dataset(spec=spec, examples=examples)
    with self.assertRaises(ValueError):
      validation.validate_dataset(ds, enforce_all_fields_required=True)
  @parameterized.named_parameters(
      dict(
          testcase_name="field_not_present_in_first_element",
          examples=[{"text": "a"}, {"score": 1, "text": "b"}],
      ),
      dict(
          testcase_name="field_not_present_in_second_element",
          examples=[{"score": 0, "text": "a"}, {"text": "b"}],
      ),
      dict(
          testcase_name="first_field_is_None_in_first_element",
          examples=[{"score": None, "text": "a"}, {"score": 1, "text": "b"}],
      ),
      dict(
          testcase_name="first_field_is_None_in_second_element",
          examples=[{"score": 0, "text": "a"}, {"score": None, "text": "b"}],
      ),
  )
  def test_validate_dataset_raises_for_required_fields(
      self, examples: list[types.JsonDict]
  ):
    spec = {"score": types.Scalar(), "text": types.TextSegment()}
    ds = dataset.Dataset(spec=spec, examples=examples)
    with self.assertRaises(ValueError):
      validation.validate_dataset(ds)
  @parameterized.named_parameters(
      dict(
          testcase_name="Scalar_is_str_in_first_element",
          examples=[{"score": "0", "text": "a"}, {"score": 1, "text": "b"}],
      ),
      dict(
          testcase_name="Scalar_is_str_in_second_element",
          examples=[{"score": 0, "text": "a"}, {"score": "1", "text": "b"}],
      ),
  )
  def test_validate_dataset_raises_for_malformed_fields(
      self, examples: list[types.JsonDict]
  ):
    spec = {"score": types.Scalar(), "text": types.TextSegment()}
    ds = dataset.Dataset(spec=spec, examples=examples)
    with self.assertRaises(ValueError):
      validation.validate_dataset(ds)
  @parameterized.named_parameters(
      dict(
          testcase_name="bad_everything_under_max_enforcement_logs_eight_times",
          spec={
              "score": types.Scalar(required=False),
              "text": types.TextSegment(required=False)
          },
          examples=[
              {},
              {"score": None, "text": None},
              {"score": "2", "text": True},
          ],
          enforce_all_fields_required=True,
          expected_log_count=8,
      ),
      dict(
          testcase_name="malformed_content_logs_twice",
          spec={"score": types.Scalar(), "text": types.TextSegment()},
          examples=[{"score": "0", "text": "a"}, {"score": 1, "text": True}],
          enforce_all_fields_required=False,
          expected_log_count=2,
      ),
      dict(
          testcase_name="missing_required_field_logs_twice",
          spec={"score": types.Scalar(), "text": types.TextSegment()},
          examples=[{"text": "a"}, {"score": 1}],
          enforce_all_fields_required=False,
          expected_log_count=2,
      ),
      dict(
          testcase_name="optional_fields_when_enforcing_required_logs_twice",
          spec={
              "score": types.Scalar(required=False),
              "text": types.TextSegment(required=False)
          },
          examples=[{"score": 0, "text": "a"}, {"score": 1, "text": "b"}],
          enforce_all_fields_required=True,
          expected_log_count=2,
      ),
  )
  def test_validate_dataset_report_all_log_counts(
      self,
      spec: types.Spec,
      examples: list[types.JsonDict],
      enforce_all_fields_required: bool,
      expected_log_count: int,
  ):
    ds = dataset.Dataset(spec=spec, examples=examples)
    with self.assertLogs(level="ERROR") as logs:
      with self.assertRaises(ValueError):
        validation.validate_dataset(
            ds,
            enforce_all_fields_required=enforce_all_fields_required,
            report_all=True,
        )
      self.assertLen(logs.output, expected_log_count)
  @parameterized.named_parameters(
      ("all_present", [{"res": 1, "grad": [1.0]}, {"res": 1, "grad": [1.0]}]),
      ("only_required", [{"res": 1}, {"res": 1}]),
  )
  def test_validate_model(self, results: list[types.JsonDict]):
    in_spec = {
        "score": types.Scalar(),
        "text": types.TextSegment(),
    }
    out_spec = {
        "res": types.RegressionScore(parent="score"),
        "grad": types.Gradients(required=False),
    }
    datapoints = [
        {"score": 0, "text": "a"},
        {"score": 1, "text": "b"},
    ]
    ds = dataset.Dataset(in_spec, datapoints)
    model = testing_utils.CustomOutputModelForTesting(
        in_spec, out_spec, results
    )
    try:
      validation.validate_model(model, ds)
    except ValueError:
      self.fail("Raised unexpected error.")
  @parameterized.named_parameters(
      ("required_output_is_missing", [{}]),
      ("required_output_is_None", [{"res": None}]),
      ("required_output_is_malformed", [{"res": "bad"}]),
      ("optional_output_is_malformed", [{"res": 1, "grad": "bad"}]),
  )
  def test_validate_model_raises(self, results: list[types.JsonDict]):
    in_spec: types.Spec = {
        "score": types.Scalar(),
        "text": types.TextSegment(),
    }
    out_spec: types.Spec = {
        "res": types.RegressionScore(parent="score"),
        "grad": types.Gradients(required=False),
    }
    datapoints: list[types.JsonDict] = [{"score": 0, "text": "a"}]
    ds = dataset.Dataset(in_spec, datapoints)
    model = testing_utils.CustomOutputModelForTesting(
        in_spec, out_spec, results
    )
    with self.assertRaises(ValueError):
      validation.validate_model(model, ds)
  def test_validate_model_report_all_log_counts(self):
    in_spec = {
        "score": types.Scalar(),
        "text": types.TextSegment(),
    }
    out_spec = {
        "res": types.RegressionScore(parent="score"),
    }
    datapoints = [
        {"score": 0, "text": "a"},
        {"score": 1, "text": "b"},
    ]
    results = [{"res": None}, {"res": "bad"}]
    ds = dataset.Dataset(in_spec, datapoints)
    model = testing_utils.CustomOutputModelForTesting(
        in_spec, out_spec, results
    )
    with self.assertLogs(level="ERROR") as logs:
      with self.assertRaises(ValueError):
        validation.validate_model(model, ds, True)
      self.assertLen(logs.output, 2)
if __name__ == "__main__":
  absltest.main()

================
File: lit_nlp/lib/validation.py
================
"""Validators for datasets and models."""
from typing import cast, Optional
from absl import logging
from lit_nlp.api import dataset
from lit_nlp.api import model
from lit_nlp.api import types
import termcolor
def validate_dataset(
    ds: dataset.Dataset,
    enforce_all_fields_required: bool = False,
    report_all: bool = False,
) -> None:
  """Validate dataset entries against spec.
  Args:
    ds: The Dataset being validated.
    enforce_all_fields_required: If `True`, require that every field in the
      Dataset referenced by `ds` has `required=True`.
    report_all: If `True`, log all errors before raising the first error
      encountered in the validation of the Dataset referenced by `ds`.
  Raises:
    ValueError: The first instance of one of the following conditions occurring
      during validation:
      * A field in the Dataset's Spec has `required=False` when enforcing all
        fields are required.
      * The value for a field in an example is `None` when that field is
        required (either explicitly or when enforcing all fields are required).
      * The value for a field fails valdiation via `LitType.validate_input()`.
  """
  # If report_all is True, first_error stores the first error encountered during
  # the validation process, which is then raised at the end of processing.
  first_error: Optional[ValueError] = None
  # If report_all is True, first_error_origin stores the ValueError raised by
  # LitType.validate_input() if a datapoint fails validation.
  first_error_origin: Optional[ValueError] = None
  def raise_or_log_error(
      msg: str, origin: Optional[ValueError] = None
  ) -> ValueError:
    """Raise (if report_all=False) or log (and return) a validation error."""
    if report_all:
      logging.error(termcolor.colored(msg, 'red'))
      return ValueError(msg)
    else:
      raise ValueError(msg) from origin
  for key, entry in ds.spec().items():
    if enforce_all_fields_required and not entry.required:
      err = raise_or_log_error(
          f'Encountered a field, "{key}", that has required=False while'
          ' enforcing that all fields in the Dataset.spec must be requred.'
      )
      first_error = first_error or err
    for example in ds.examples:
      value = example.get(key)
      if value is None:
        if enforce_all_fields_required or entry.required:
          err = raise_or_log_error(
              f'Required dataset feature "{key}" missing from datapoint.'
          )
          first_error = first_error or err
      else:
        try:
          entry.validate_input(value, ds.spec(), cast(types.Input, example))
        except ValueError as e:
          err = raise_or_log_error(
              f'Failed while validating dataset field "{key}" of type'
              f' "{type(entry)}".',
              e,
          )
          first_error = first_error or err
  if first_error:
    raise first_error from first_error_origin
def validate_model(
    mod: model.Model, ds: dataset.Dataset, report_all: bool = False
) -> None:
  """Validate model usage on dataset against specs.
  Args:
    mod: The Model providing the predictions that are being validated.
    ds: The Dataset providing the examples for which the model referenced by
      `mod` makes predictions.
    report_all: If `True`, log all errors before raising the first error
      encountered in the validation of the Model referenced by `mod`.
  Raises:
    ValueError: The first instance of one of the following conditions occurring
      during validation:
      * A required output field is missing from a prediction.
      * The value for a predicted field fails valdiation via
        `LitType.validate_output()`.
  """
  # If report_all is True, first_error stores the first error encountered during
  # the validation process, which is then raised at the end of processing.
  first_error: Optional[ValueError] = None
  # If report_all is True, first_error_origin stores the ValueError raised by
  # LitType.validate_output() if a datapoint fails validation.
  first_error_origin: Optional[ValueError] = None
  def raise_or_log_error(
      msg: str, origin: Optional[ValueError] = None
  ) -> ValueError:
    """Raise (if report_all=False) or log (and return) a validation error."""
    if report_all:
      logging.error(termcolor.colored(msg, 'red'))
      return ValueError(msg)
    else:
      raise ValueError(msg) from origin
  outputs = list(mod.predict(ds.examples))
  for ex, output in zip(ds.examples, outputs):
    for (key, entry) in mod.output_spec().items():
      value = output.get(key)
      if value is None:
        if entry.required:
          err = raise_or_log_error(
              f'Required model output "{key}" is missing from prediction.'
          )
          first_error = first_error or err
      else:
        try:
          entry.validate_output(
              value,
              mod.output_spec(),
              output,
              mod.input_spec(),
              ds.spec(),
              cast(types.Input, ex),
          )
        except ValueError as e:
          err = raise_or_log_error(
              f'Failed while validating model output field "{key}" of type'
              f' "{type(entry)}".',
              e,
          )
          first_error = first_error or err
  if first_error:
    raise first_error from first_error_origin

================
File: lit_nlp/lib/wsgi_app.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Simple WSGI app implementation.
This takes a list of handlers, and creates a WSGI application that can be served
through a variety of methods.
Why not use Flask or something? Historical reasons, and if it ain't broke, don't
fix it.
"""
import mimetypes
import os
import time
import traceback
import wsgiref.handlers
from absl import logging
import six
from six.moves.urllib.parse import urlparse
from werkzeug import wrappers
def _LoadResource(path):
  """Load the resource at given path.
  Args:
    path: a string resource path.
  Returns:
    The contents of that resource.
  Raises:
    ValueError: If the path is not set up correctly.
    IOError: If the path is not found, or the resource can't be opened.
  """
  try:
    with open(path, 'rb') as f:
      return f.read()
  except IOError as e:
    logging.warning('IOError %s on path %s', e, path)
    raise e
class App(object):
  """Standalone WSGI app that can serve files, etc."""
  _TEXTUAL_MIMETYPES = set([
      'application/javascript',
      'application/json',
      'application/json+protobuf',
      'image/svg+xml',
      'text/css',
      'text/csv',
      'text/html',
      'text/json',
      'text/plain',
      'text/tab-separated-values',
      'text/x-protobuf',
  ])
  def __init__(self, handlers, project_root, index_file='index.html'):
    self._handlers = handlers
    self._project_root = project_root
    self._index_file = index_file
  def respond(  # pylint: disable=invalid-name
      self,
      request,
      content,
      content_type,
      code=200,
      expires=0,
      content_encoding=None):
    """Construct a werkzeug WSGI response object.
    Args:
      request: A werkzeug Request object. Used mostly to check the
        Accept-Encoding header.
      content: Payload data as bytes or unicode string (will be UTF-8 encoded).
      content_type: Media type only - "charset=utf-8" will be added for text.
      code: Numeric HTTP status code to use.
      expires: Second duration for browser caching, default 0.
      content_encoding: Encoding if content is already encoded, e.g. 'gzip'.
    Returns:
      A werkzeug Response object (a WSGI application).
    """
    if isinstance(content, six.text_type):
      content = content.encode('utf-8')
    if content_type in self._TEXTUAL_MIMETYPES:
      content_type += '; charset=utf-8'
    headers = []
    headers.append(('Content-Length', str(len(content))))
    if content_encoding:
      headers.append(('Content-Encoding', content_encoding))
    if expires > 0:
      e = wsgiref.handlers.format_date_time(time.time() + float(expires))
      headers.append(('Expires', e))
      headers.append(('Cache-Control', 'private, max-age=%d' % expires))
    else:
      headers.append(('Expires', '0'))
      headers.append(('Cache-Control', 'no-cache, must-revalidate'))
    if request.method == 'HEAD':
      content = None
    return wrappers.Response(
        response=content,
        status=code,
        headers=headers,
        content_type=content_type)
  def _ServeStaticFile(self, request, path):
    """Serves the static file located at the given path.
    Args:
      request: A Werkzeug Request object.
      path: The path of the static file, relative to the current directory.
    Returns:
      A Werkzeug Response object.
    """
    if not self._PathIsSafe(path):
      logging.info('path %s not safe, sending 400', path)
      # Traversal attack, so 400.
      return self.respond(request, 'Path not safe', 'text/plain', 400)
    # Open the file and read it.
    try:
      contents = _LoadResource(path)
    except IOError:
      logging.info('path %s not found, sending 404', path)
      return self.respond(request, 'Not found', 'text/plain', code=404)
    mimetype, content_encoding = mimetypes.guess_type(path)
    mimetype = mimetype or 'application/octet-stream'
    return self.respond(
        request,
        contents,
        mimetype,
        expires=3600,
        content_encoding=content_encoding)
  def _PathIsSafe(self, path):
    """Check path is safe (stays within current directory).
    This is for preventing directory-traversal attacks.
    Args:
      path: The path to check for safety.
    Returns:
      True if the given path stays within the project directory, and false
      if it would escape to a higher directory. E.g. _path_is_safe('index.html')
      returns true, but _path_is_safe('../../../etc/password') returns false.
    """
    base = os.path.abspath(self._project_root)
    absolute_path = os.path.abspath(path)
    prefix = os.path.commonprefix([base, absolute_path])
    return prefix == base
  def _ServeCustomHandler(self, request, clean_path, environ):
    return self._handlers[clean_path](self, request, environ)
  def __call__(self, environ, start_response):
    """Implementation of the WSGI interface."""
    request = wrappers.Request(environ)
    try:
      parsed_url = urlparse(request.path)
      # Remove a trailing slash, if present.
      clean_path = parsed_url.path
      if clean_path.endswith('/'):
        clean_path = clean_path[:-1]
      if clean_path in self._handlers:
        return self._ServeCustomHandler(request, clean_path, environ)(
            environ, start_response)
      else:
        is_index = not clean_path or clean_path == '/index.html'
        if is_index:
          clean_path = os.path.join(self._project_root, self._index_file)
        else:
          # Strip off the leading forward slash. Don't do it for index because
          # in the vulcanized version we use an absolute path.
          clean_path = os.path.join(self._project_root, clean_path.lstrip('/'))
        response = self._ServeStaticFile(request, clean_path)
    except Exception as e:  # pylint: disable=broad-except
      errors = (str(e), str(traceback.format_exc()))
      html_response = (
          'Uncaught error: %s\n\nDetails: %s' % errors)
      logging.error('Uncaught error: %s \n\n %s', *errors)
      response = self.respond(request, html_response, 'text/html', 500)
    return response(environ, start_response)

================
File: lit_nlp/lib/wsgi_serving.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""WSGI servers to power the LIT backend."""
import socket
import threading
from typing import Optional
from wsgiref import validate
import wsgiref.simple_server
from absl import logging
import portpicker
import termcolor
from werkzeug import serving as werkzeug_serving
# TODO(b/231171830): Create abstract base class for the WSGI Server flavors.
# TODO(b/231171830): Update to inherit from WSGI Server ABC.
class BasicDevServer(object):
  """Basic development server; not recommended for deployment."""
  def __init__(
      self, wsgi_app, port: int = 4321, host: str = '127.0.0.1', **unused_kw
  ):
    self._port = port
    self._host = host
    self._app = wsgi_app
    self.can_act_as_model_server = True
  def serve(self):
    """Start serving."""
    logging.info(
        termcolor.colored(
            (
                f'\n\nStarting Server on port {self._port}'
                f'\nYou can navigate to http://{self._host}:{self._port}\n\n'
            ),
            'green',
            attrs=['bold'],
        )
    )
    werkzeug_serving.run_simple(
        self._host,
        self._port,
        self._app,
        use_debugger=False,
        use_reloader=False)
class WsgiServerIpv6(wsgiref.simple_server.WSGIServer):
  """IPv6 based extension of the simple WSGIServer."""
  address_family = socket.AF_INET6
# TODO(b/231171830): Update to inherit from WSGI Server ABC.
class NotebookWsgiServer(object):
  """WSGI server for notebook environments."""
  def __init__(
      self,
      wsgi_app,
      host: str = 'localhost',
      port: Optional[int] = None,
      **unused_kw,
  ):
    """Initialize the WSGI server.
    Args:
      wsgi_app: WSGI pep-333 application to run.
      host: Host to run on, defaults to 'localhost'.
      port: Port to run on. If not specified, then an unused one will be picked.
    """
    self._app = wsgi_app
    self._host = host
    self._port = port
    self._server_thread = None
    self.can_act_as_model_server = False
  @property
  def app(self):
    return self._app
  @property
  def port(self):
    """Returns the current port or error if the server is not started.
    Raises:
      RuntimeError: If server has not been started yet.
    Returns:
      The port being used by the server.
    """
    if self._server_thread is None:
      raise RuntimeError('Server not started.')
    return self._port
  def stop(self):
    """Stops the server thread."""
    if self._server_thread is None:
      return
    self._stopping.set()
    self._server_thread = None
    self._stopped.wait()
  def serve(self):
    """Starts a server in a thread using the WSGI application provided.
    Will wait until the thread has started calling with an already serving
    application will simple return.
    """
    if self._server_thread is not None:
      return
    if self._port is None:
      self._port = portpicker.pick_unused_port()
    started = threading.Event()
    self._stopped = threading.Event()
    self._stopping = threading.Event()
    def build_server(started, stopped, stopping):
      """Closure to build the server function to be passed to the thread.
      Args:
        started: Threading event to notify when started.
        stopped: Threading event to notify when stopped.
        stopping: Threading event to notify when stopping.
      Returns:
        A function that function that takes a port and WSGI app and notifies
          about its status via the threading events provided.
      """
      def server(port, wsgi_app):
        """Serve a WSGI application until stopped.
        Args:
          port: Port number to serve on.
          wsgi_app: WSGI application to serve.
        """
        try:
          httpd = wsgiref.simple_server.make_server(self._host, port, wsgi_app)
        except socket.error:
          # Try IPv6
          httpd = wsgiref.simple_server.make_server(
              self._host, port, wsgi_app, server_class=WsgiServerIpv6)
        started.set()
        httpd.timeout = 30
        while not stopping.is_set():
          httpd.handle_request()
        stopped.set()
      return server
    server = build_server(started, self._stopped, self._stopping)
    server_thread = threading.Thread(
        target=server, args=(self._port, self._app))
    self._server_thread = server_thread
    server_thread.start()
    started.wait()

================
File: lit_nlp/notebook.py
================
"""Notebook usage of LIT.
To use in LIT in colab or jupyter notebooks, create a LitWidget instance
with models and datasets to load. Optionally set the UI height and a proxy URL
if necessary. By default, the UI with render in the cell that creates the
instance. Set render=False to disable this, and manually render the UI in a cell
through the render() method. Use the stop() method to stop the server when done.
"""
from collections.abc import Mapping, Sequence
import html
import json
import os
import pathlib
import random
from typing import Any, Optional, cast
import urllib.parse
import attr
from IPython import display
from lit_nlp import dev_server
from lit_nlp import server_config
from lit_nlp.api import layout
from lit_nlp.lib import wsgi_serving
from tqdm import notebook
JsonDict = Mapping[str, Any]
try:
  import google.colab  # pylint: disable=g-import-not-at-top,unused-import
  from google.colab import output  # pylint: disable=g-import-not-at-top,unused-import # pytype: disable=import-error
  is_colab = True
except (ImportError, ModuleNotFoundError):
  is_colab = False
progress_indicator = notebook.tqdm
modules = layout.LitModuleName
LIT_NOTEBOOK_LAYOUT = layout.LitCanonicalLayout(
    upper={
        'Predictions': [
            modules.SimpleDataTableModule,
            *layout.MODEL_PREDS_MODULES,
        ],
        'Explanations': [
            modules.SimpleDatapointEditorModule,
            *layout.MODEL_PREDS_MODULES,
            modules.SalienceMapModule,
            modules.LegacySequenceSalienceModule,
        ],
        'Analysis': [
            modules.MetricsModule,
            modules.ConfusionMatrixModule,
            modules.ScalarModule,
        ],
    }
)
@attr.s(auto_attribs=True, kw_only=True)
class RenderConfig(object):
  """Config options for widget rendering."""
  tab: Optional[str] = None
  upper_tab: Optional[str] = None
  layout: Optional[str] = None
  dataset: Optional[str] = None
  models: Optional[Sequence[str]] = None
  datapoints: Optional[Sequence[JsonDict]] = None
  def get_query_str(self):
    """Convert config object to query string for LIT URL."""
    def _encode(v):
      if isinstance(v, (list, tuple)):
        return ','.join(v)
      return v
    string_params = {
        k: _encode(v)
        for k, v in attr.asdict(self).items()
        if (v is not None and k != 'datapoints')
    }
    if self.datapoints:
      for i, ex in enumerate(self.datapoints):
        for field in ex:
          string_params[f'data{i}_{field}'] = _encode(ex[field])
    return '?' + urllib.parse.urlencode(string_params)
class LitWidget(object):
  """Class for using LIT inside notebooks."""
  def __init__(
      self,
      *args,
      height=1000,
      render=False,
      proxy_url=None,
      layouts: Optional[layout.LitComponentLayouts] = None,
      warm_start: bool = False,
      **kw,
  ):
    """Start LIT server and optionally render the UI immediately.
    Args:
      *args: Positional arguments for the LitApp.
      height: Height to display the LIT UI in pixels. Defaults to 1000.
      render: Whether to render the UI when this object is constructed. Defaults
        to False.
      proxy_url: Optional proxy URL, if using in a notebook with a server proxy.
        Defaults to None.
      layouts: Optional custom UI layouts.
      warm_start: If true, run predictions for every model on every compatible
        dataset before returning a renderable widget.
      **kw: Keyword arguments for the LitApp.
    """
    app_flags = dict(server_config.get_flags())
    app_flags['server_type'] = 'notebook'
    app_flags['host'] = 'localhost'
    app_flags['port'] = None
    app_flags['warm_start'] = 1 if warm_start else 0
    app_flags['warm_start_progress_indicator'] = progress_indicator
    app_flags['sync_state'] = True
    layouts = dict(layouts or {})
    if 'notebook' not in layouts:
      layouts['notebook'] = LIT_NOTEBOOK_LAYOUT
    # This will be 'notebook' unless custom layouts are also given in Python.
    app_flags['default_layout'] = list(layouts.keys())[0]
    app_flags.update(kw)
    lit_demo = dev_server.Server(*args, layouts=layouts, **app_flags)
    self._server = cast(wsgi_serving.NotebookWsgiServer, lit_demo.serve())
    self._height = height
    self._proxy_url = proxy_url
    if render:
      self.render()
  @property
  def ui_state(self):
    return self._server.app.ui_state_tracker.state
  def stop(self):
    """Stop the LIT server."""
    self._server.stop()
  def render(
      self,
      height=None,
      open_in_new_tab=False,
      ui_params: Optional[RenderConfig] = None,
      data: Optional[Sequence[JsonDict]] = None,
  ):
    """Render the LIT UI in the output cell.
    To immediately analyze specifiic example(s), use the data= parameter:
      widget.render(..., data=[{"prompt": "Hello world "}])
    Args:
      height: Optional height to display the LIT UI in pixels. If not specified,
        then the height specified in the constructor is used.
      open_in_new_tab: Whether to show the UI in a new tab instead of in the
        output cell. Defaults to false.
      ui_params: Optional configuration options for the LIT UI's state.
      data: Optional examples to load directly to the UI (via URL params).
    """
    if not height:
      height = self._height
    if not ui_params:
      ui_params = RenderConfig()
    if data:
      ui_params.datapoints = data
    if is_colab:
      _display_colab(self._server.port, height, open_in_new_tab, ui_params)
    else:
      _display_jupyter(self._server.port, height, self._proxy_url,
                       open_in_new_tab, ui_params)
def _display_colab(port, height, open_in_new_tab, ui_params: RenderConfig):
  """Display the LIT UI in colab.
  Args:
    port: The port the LIT server is running on.
    height: The height of the LIT UI in pixels.
    open_in_new_tab: Whether to show the UI in a new tab instead of in the
      output cell.
    ui_params: RenderConfig of options for the LIT UI.
  """
  params = ui_params.get_query_str()
  path = f'/{params}'
  if open_in_new_tab:
    output.serve_kernel_port_as_window(port, path=path)
  else:
    output.serve_kernel_port_as_iframe(port, height=f'{height}', path=path)
def _display_jupyter(
    port, height, proxy_url, open_in_new_tab, ui_params: RenderConfig
):
  """Display the LIT UI in jupyter.
  Args:
    port: The port the LIT server is running on.
    height: The height of the LIT UI in pixels.
    proxy_url: Optional proxy URL, if using in a notebook with a server proxy.
        If not provided, LIT also checks to see if the environment variable
        LIT_PROXY_URL is set, and if so, it uses that value as the proxy URL.
    open_in_new_tab: Whether to show the UI in a new tab instead of in the
      output cell.
    ui_params: RenderConfig of options for the LIT UI.
  """
  # Add height to jupyter output_scroll div to fully contain LIT UI.
  output_scroll_height = height + 10
  params = ui_params.get_query_str()
  frame_id = 'lit-frame-{:08x}'.format(random.getrandbits(64))
  if open_in_new_tab:
    shell = """
      <a href="javascript:void(0);" id="%HTML_ID%"></a>
      <script>
        (function() {
          const urlStr = %URL% + '%PARAMS%'
          const url = new URL(urlStr, window.location);
          const port = %PORT%;
          if (port) {
            url.port = port;
          }
          const a = document.getElementById(%JSON_ID%);
          a.innerHTML = url;
          a.onclick = (e) => window.open(url, "_blank");
          window.open(url, "_blank");
        })();
      </script>
    """
  else:
    shell = """
      <style>div.output_scroll { height: %SCROLL_HEIGHT%px; }</style>
      <iframe id='%HTML_ID%' width='100%' height='%HEIGHT%' frameborder='0'>
      </iframe>
      <script>
        (function() {
          const frame = document.getElementById(%JSON_ID%);
          const urlStr = %URL% + '%PARAMS%'
          const url = new URL(urlStr, window.location);
          const port = %PORT%;
          if (port) {
            url.port = port;
          }
          frame.src = url;
        })();
      </script>
    """
  if proxy_url is None:
    proxy_url = os.environ.get('LIT_PROXY_URL')
  if proxy_url is not None:
    # Allow %PORT% in proxy_url.
    proxy_url = proxy_url.replace('%PORT%', '%d' % port)
    replacements = [
        ('%HTML_ID%', html.escape(frame_id, quote=True)),
        ('%JSON_ID%', json.dumps(frame_id)),
        ('%HEIGHT%', '%d' % height),
        ('%SCROLL_HEIGHT%', '%d' % output_scroll_height),
        ('%PORT%', '0'),
        ('%URL%', json.dumps(proxy_url)),
        ('%PARAMS%', '%s' % params),
    ]
  else:
    replacements = [
        ('%HTML_ID%', html.escape(frame_id, quote=True)),
        ('%JSON_ID%', json.dumps(frame_id)),
        ('%HEIGHT%', '%d' % height),
        ('%SCROLL_HEIGHT%', '%d' % output_scroll_height),
        ('%PORT%', '%d' % port),
        ('%URL%', json.dumps('/')),
        ('%PARAMS%', '%s' % params),
    ]
  for (k, v) in replacements:
    shell = shell.replace(k, v)
  iframe = display.HTML(shell)
  display.display(iframe)

================
File: lit_nlp/server_config.py
================
"""Common flags for the LIT server, for port, host, authentication, etc.
Not required to use LIT, but helpful as a convenience mixin.
Usage:
  server_kw = config_flags.get_flags()
  server = dev_server.Server(models, datasets, ..., **server_kw)
  server.serve()
  On the commandline: --lit.port=5432 (instead of --port=5432)
A fork of server_flags.py, which it will eventually replace.
This is because absl.FLAGS is global, so importing server_flags.py
may cause conflicts when LIT is run in a binary that uses the same flag names
through another library.
TODO(b/301004293): migrate demos over to use this instead of server_flags.py.
"""
import ml_collections
from ml_collections.config_dict import config_dict
from ml_collections.config_flags import config_flags
config = ml_collections.ConfigDict()
##
# Server flags, passed to the WSGI server.
# LINT.IfChange
# What port to serve on.
config.port = 5432
# Webserver to use; see dev_server.py for options. Use "external" when
# using an external webserver like gunicorn, or "prebake" to run start-up
# tasks (like warm start and caching data) without starting a server.
config.server_type = 'default'
# What host address to serve on. Use 127.0.0.1 for local development, or
# 0.0.0.0 to allow external connections.'
config.host = '127.0.0.1'
##
# LIT application flags, passed to app.LitApp constructor.
# Directory to store/lookup persisted data used by server,
# such as cached predictions. If empty, will cache in-memory only.
config.data_dir = ''
# If 1, will run all (model, dataset) on startup to populate the cache.
# If fractional, will only warm-start on a sample of each dataset,
# for development purposes.
config.warm_start = 0.0
# If true, will precompute server-side embedding projections such as PCA.
config.warm_projections = False
# If true, will disable capabilities not allowed in demo mode, such as
# saving generated datapoints to disk.
config.demo_mode = False
# Which layout to use by default (can be changed via url); see layout.ts
config.default_layout = 'default'
# What url base to use when copying the LIT url (e.g., something other
# than just a local server address.
config.canonical_url = config_dict.placeholder(str)
# Custom page title for this server.
config.page_title = config_dict.placeholder(str)
# Whether the LIT instance is a development demo.
config.development_demo = False
# Whether dataset and model validation will happen at startup.
config.validate = None
config.enforce_dataset_fields_required = False
config.report_all = False
# Whether to re-compute example hashes before checking the cache.
# See b/293984290.
config.strict_cache_id_validation = False
import os
import pathlib
config.client_root = os.path.join(
        pathlib.Path(__file__).parent.absolute(), 'client', 'build',
        'default')
config_flags.DEFINE_config_dict('lit', config)
# LINT.ThenChange(server_flags.py)
def get_flags():
  return config

================
File: lit_nlp/server_flags_test.py
================
"""Tests for server_flags."""
from absl import flags
from absl.testing import absltest
from lit_nlp import server_config
from lit_nlp import server_flags
FLAGS = flags.FLAGS
def _get_flags_for_module(module):
  """Get all of the flags defined in the specified module.
  This is very slow, but should be authoritative - so we use it in a test and
  rely on the SERVER_FLAGS list instead at runtime.
  Args:
    module: the module to get flags from
  Returns:
    dict mapping flag names (string) to values (various types).
  """
  ret = {}
  for name, value in FLAGS.flag_values_dict().items():
    if FLAGS.find_module_defining_flag(name) == module.__name__:
      ret[name] = value
  return ret
class ServerFlagsTest(absltest.TestCase):
  def setUp(self):
    super(ServerFlagsTest, self).setUp()
    FLAGS(['server_flags_test'])
  def test_matches_server_config(self):
    """Check that server_config and server_flags match."""
    server_config_dict = server_config.get_flags().to_dict()
    server_flags_dict = server_flags.get_flags()
    self.assertEqual(server_flags_dict, server_config_dict)
  def test_gets_all_flags(self):
    """Check that SERVER_FLAGS captures all the flags in server_flags.py."""
    all_server_flags = _get_flags_for_module(server_flags)
    server_flags_dict = server_flags.get_flags()
    self.assertEqual(server_flags_dict, all_server_flags)
if __name__ == '__main__':
  absltest.main()

================
File: lit_nlp/server_flags.py
================
# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
"""Common flags for the LIT server, for port, host, authentication, etc.
Not required to use LIT, but helpful as a convenience mixin.
Usage:
  server_kw = server_flags.get_flags()
  server = dev_server.Server(models, datasets, ..., **server_kw)
  server.serve()
TODO(lit-dev): consider defining a single ConfigDict instead of individual
flags.
"""
from collections.abc import Mapping
import os
import pathlib
from absl import flags
from lit_nlp.lib import flag_helpers
##
# Server flags, passed to the WSGI server.
# Wrap in a list to capture the FlagHolder objects returned by flags.DEFINE_*,
# so that we can access these all programmatically with get_flags()
_SERVER_FLAGS: tuple[flags.FlagHolder, ...] = (
    # LINT.IfChange
    flags.DEFINE_integer('port', 5432, 'What port to serve on.'),
    flags.DEFINE_string(
        'server_type',
        'default',
        'Webserver to use; see dev_server.py for options. Use "external" when'
        ' using an external webserver like gunicorn, or "prebake" to run'
        ' start-up tasks (like warm start and caching data) without starting a'
        ' server.',
    ),
    flags.DEFINE_string(
        'host',
        '127.0.0.1',
        'What host address to serve on. Use 127.0.0.1 for '
        'local development, or 0.0.0.0 to allow external connections.',
    ),
    ##
    # LIT application flags, passed to app.LitApp constructor.
    flags.DEFINE_string(
        'data_dir',
        '',
        'Directory to store/lookup persisted data used by server, '
        'such as cached predictions. If empty, will cache in-memory only.',
    ),
    flags.DEFINE_float(
        'warm_start',
        0.0,
        'If 1, will run all (model, dataset) on startup to populate the cache. '
        'If fractional, will only warm-start on a sample of each dataset, '
        'for development purposes.',
    ),
    flags.DEFINE_bool(
        'warm_projections',
        False,
        'If true, will precompute server-side embedding projections such as'
        ' PCA.',
    ),
    flags.DEFINE_bool(
        'demo_mode',
        False,
        'If true, will disable capabilities not allowed in demo mode, such as '
        'saving generated datapoints to disk.',
    ),
    flags.DEFINE_string(
        'default_layout',
        'default',
        'Which layout to use by default (can be changed via url); see'
        ' layout.ts',
    ),
    flags.DEFINE_string(
        'canonical_url',
        None,
        'What url base to use when copying the LIT url (e.g., something other '
        'than just a local server address.',
    ),
    flags.DEFINE_string(
        'page_title', None, 'Custom page title for this server.'
    ),
    flags.DEFINE_bool(
        'development_demo',
        False,
        'If true, signifies this LIT instance is a development demo.',
    ),
    flags.DEFINE_enum_class(
        'validate',
        None,
        flag_helpers.ValidationMode,
        'If not None or "off", will validate the datasets and model outputs '
        'according to the value set. By default, validation is disabled.',
    ),
    flags.DEFINE_bool(
        'enforce_dataset_fields_required',
        False,
        'If true and validate is true, this dataset validation will enforce'
        ' that all Spec fields have a required=True proeprty. A ValueError will'
        ' be logged and/or raised if a field does not meet this requirement.',
    ),
    flags.DEFINE_bool(
        'report_all',
        False,
        'If true, and validate is true, will report every issue in validation '
        'as opposed to just the first.',
    ),
    flags.DEFINE_bool(
        'strict_cache_id_validation',
        False,
        'If true, will re-compute hashes of all examples before checking the'
        ' cache, and raise an error if any do not match the provided _id'
        ' field. See b/293984290.',
    ),
    flags.DEFINE_string(
        'client_root',
        os.path.join(
            pathlib.Path(__file__).parent.absolute(), 'client', 'build',
            'default'),
        'Path to frontend client.'),
    # LINT.ThenChange(server_config.py)
)
def get_flags():
  """Get all of the flags in SERVER_FLAGS in this module."""
  return {entry.name: entry.value for entry in _SERVER_FLAGS}
def get_flag_holders() -> Mapping[str, flags.FlagHolder]:
  """Get all of the flags in SERVER_FLAGS in this module."""
  return {entry.name: entry for entry in _SERVER_FLAGS}

================
File: README.md
================
#  Learning Interpretability Tool (LIT)

<!--* freshness: { owner: 'lit-dev' reviewed: '2024-06-25' } *-->

The Learning Interpretability Tool (LIT, formerly known as the Language
Interpretability Tool) is a visual, interactive ML model-understanding tool that
supports text, image, and tabular data. It can be run as a standalone server, or
inside of notebook environments such as Colab, Jupyter, and Google Cloud Vertex
AI notebooks.

LIT is built to answer questions such as:

*   **What kind of examples** does my model perform poorly on?
*   **Why did my model make this prediction?** Can this prediction be attributed
    to adversarial behavior, or to undesirable priors in the training set?
*   **Does my model behave consistently** if I change things like textual style,
    verb tense, or pronoun gender?

![Example of LIT UI](https://pair-code.github.io/lit/assets/images/readme-fig-1.png)

LIT supports a variety of debugging workflows through a browser-based UI.
Features include:

*   **Local explanations** via salience maps and rich visualization of model
    predictions.
*   **Aggregate analysis** including custom metrics, slicing and binning, and
    visualization of embedding spaces.
*   **Counterfactual generation** via manual edits or generator plug-ins to
    dynamically create and evaluate new examples.
*   **Side-by-side mode** to compare two or more models, or one model on a pair
    of examples.
*   **Highly extensible** to new model types, including classification,
    regression, span labeling, seq2seq, and language modeling. Supports
    multi-head models and multiple input features out of the box.
*   **Framework-agnostic** and compatible with TensorFlow, PyTorch, and more.

LIT has a [website](https://pair-code.github.io/lit) with live demos, tutorials,
a setup guide and more.

Stay up to date on LIT by joining the
[lit-announcements mailing list](https://groups.google.com/g/lit-annoucements).

For a broader overview, check out [our paper](https://arxiv.org/abs/2008.05122) and the
[user guide](https://pair-code.github.io/lit/documentation/ui_guide).

## Documentation

*   [Documentation index](https://pair-code.github.io/lit/documentation/)
*   [FAQ](https://pair-code.github.io/lit/documentation/faq)
*   [Release notes](./RELEASE.md)

## Download and Installation

LIT can be installed via `pip` or built from source. Building from source is
necessary if you want to make code changes.

### Install from PyPI with pip

```sh
pip install lit-nlp
```

The default `pip` installation will install all required packages to use the LIT
Python API, built-in interpretability components, and web application. To
install dependencies for the provided demos or test suite, install LIT with the
appropriate optional dependencies.

```sh
# To install dependencies for the discriminative AI examples (GLUE, Penguin)
pip install 'lit-nlp[examples-discriminative-ai]'

# To install dependencies for the generative AI examples (Prompt Debugging)
pip install 'lit-nlp[examples-generative-ai]'

# To install dependencies for all examples plus the test suite
pip install 'lit-nlp[test]'
```

### Install from source

Clone the repo:

```sh
git clone https://github.com/PAIR-code/lit.git
cd lit
```

Note: be sure you are running Python 3.9+. If you have a different version on
your system, use the `conda` instructions below to set up a Python 3.9
environment.

Set up a Python environment with `venv` (or your preferred environment manager).
Note that these instructions assume you will be making code changes to LIT and
includes the full requirements for all examples and the test suite. See the
other optional dependency possibilities in the install with pip section.

```sh
python -m venv .venv
source .venv/bin/activate
python -m pip install -e '.[test]'
```

The LIT repo does not include a distributable version of the LIT app. You must
build it from source.

```sh
(cd lit_nlp; yarn && yarn build)
```

Note: if you see [an error](https://github.com/yarnpkg/yarn/issues/2821)
running `yarn` on Ubuntu/Debian, be sure you have the
[correct version installed](https://yarnpkg.com/en/docs/install#linux-tab).

## Running LIT

Explore a collection of hosted demos on the
[demos page](https://pair-code.github.io/lit/demos).

### Using container images

See the [containerization guide](https://pair-code.github.io/lit/documentation/docker) for instructions on using LIT
locally in Docker, Podman, etc.

LIT also provides pre-built images that can take advantage of accelerators,
making Generative AI and LLM use cases easier to work with. Check out the
[LIT on GCP docs](https://codelabs.developers.google.com/codelabs/responsible-ai/lit-on-gcp)
for more.

### Quick-start: classification and regression

To explore classification and regression models tasks from the popular
[GLUE benchmark](https://gluebenchmark.com/):

```sh
python -m lit_nlp.examples.glue.demo --port=5432 --quickstart
```

Navigate to http://localhost:5432 to access the LIT UI.

Your default view will be a
[small BERT-based model](https://arxiv.org/abs/1908.08962) fine-tuned on the
[Stanford Sentiment Treebank](https://nlp.stanford.edu/sentiment/treebank.html),
but you can switch to
[STS-B](http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark) or
[MultiNLI](https://cims.nyu.edu/~sbowman/multinli/) using the toolbar or the
gear icon in the upper right.

And navigate to http://localhost:5432 for the UI.

### Notebook usage

Colab notebooks showing the use of LIT inside of notebooks can be found at
[lit_nlp/examples/notebooks](./lit_nlp/examples/notebooks).

We provide a simple
[Colab demo](https://colab.research.google.com/github/PAIR-code/lit/blob/main/lit_nlp/examples/notebooks/LIT_sentiment_classifier.ipynb).
Run all the cells to see LIT on an example classification model in the notebook.

### More Examples

See [lit_nlp/examples](./lit_nlp/examples). Most are run similarly to the
quickstart example above:

```sh
python -m lit_nlp.examples.<example_name>.demo --port=5432 [optional --args]
```

## User Guide

To learn about LIT's features, check out the [user guide](https://pair-code.github.io/lit/documentation/ui_guide), or
watch this [video](https://www.youtube.com/watch?v=CuRI_VK83dU).

## Adding your own models or data

You can easily run LIT with your own model by creating a custom `demo.py`
launcher, similar to those in [lit_nlp/examples](./lit_nlp/examples). The
basic steps are:

*   Write a data loader which follows the [`Dataset` API](https://pair-code.github.io/lit/documentation/api#datasets)
*   Write a model wrapper which follows the [`Model` API](https://pair-code.github.io/lit/documentation/api#models)
*   Pass models, datasets, and any additional
    [components](https://pair-code.github.io/lit/documentation/api#interpretation-components) to the LIT server class

For a full walkthrough, see
[adding models and data](https://pair-code.github.io/lit/documentation/api#adding-models-and-data).

## Extending LIT with new components

LIT is easy to extend with new interpretability components, generators, and
more, both on the frontend or the backend. See our [documentation](https://pair-code.github.io/lit/documentation/) to get
started.

## Pull Request Process

To make code changes to LIT, please work off of the `dev` branch and
[create pull requests](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request)
(PRs) against that branch. The `main` branch is for stable releases, and it is
expected that the `dev` branch will always be ahead of `main`.

[Draft PRs](https://github.blog/2019-02-14-introducing-draft-pull-requests/) are
encouraged, especially for first-time contributors or contributors working on
complex tasks (e.g., Google Summer of Code contributors). Please use these to
communicate ideas and implementations with the LIT team, in addition to issues.

Prior to sending your PR or marking a Draft PR as "Ready for Review", please run
the Python and TypeScript linters on your code to ensure compliance with
Google's [Python](https://google.github.io/styleguide/pyguide.html) and
[TypeScript](https://google.github.io/styleguide/tsguide.html) Style Guides.

```sh
# Run Pylint on your code using the following command from the root of this repo
(cd lit_nlp; pylint)

# Run ESLint on your code using the following command from the root of this repo
(cd lit_nlp; yarn lint)
```

## Citing LIT

If you use LIT as part of your work, please cite the
[EMNLP paper](https://arxiv.org/abs/2008.05122) or the
[Sequence Salience paper](https://arxiv.org/abs/2404.07498)

```BibTeX
@misc{tenney2020language,
    title={The Language Interpretability Tool: Extensible, Interactive Visualizations and Analysis for {NLP} Models},
    author={Ian Tenney and James Wexler and Jasmijn Bastings and Tolga Bolukbasi and Andy Coenen and Sebastian Gehrmann and Ellen Jiang and Mahima Pushkarna and Carey Radebaugh and Emily Reif and Ann Yuan},
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    year = "2020",
    publisher = "Association for Computational Linguistics",
    pages = "107--118",
    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.15",
}
```

```BibTeX
@article{tenney2024interactive,
  title={Interactive prompt debugging with sequence salience},
  author={Tenney, Ian and Mullins, Ryan and Du, Bin and Pandya, Shree and Kahng, Minsuk and Dixon, Lucas},
  journal={arXiv preprint arXiv:2404.07498},
  year={2024}
}
```

## Disclaimer

This is not an official Google product.

LIT is a research project and under active development by a small team. We want
LIT to be an open platform, not a walled garden, and would love your suggestions
and feedback &ndash; please
[report any bugs](https://github.com/pair-code/lit/issues) and reach out on the
[Discussions page](https://github.com/PAIR-code/lit/discussions/landing).

================
File: RELEASE.md
================
# Learning Interpretability Tool Release Notes


## Release 1.3.1

This is a minor update to fix issues with running the [LIT Gemma Colab](https://colab.sandbox.google.com/github/google/generative-ai-docs/blob/main/site/en/gemma/docs/lit_gemma.ipynb).


## Release 1.3

This release updates how the Learning Interpretability Tool (LIT) can be
deployed on Google Cloud. You can now use LIT to interpret foundation
models&mdash;including
[Gemini](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/inference),
[Gemma](https://ai.google.dev/gemma), [Llama](https://www.llama.com/), and
[Mistral](https://mistral.ai/technology/#models)&mdash;using LIT's prompt
debugging workflows. LIT now provides public container images to make it easier
to deploy on your hosting platform of choice, with an updated
[tutorial](https://codelabs.developers.google.com/codelabs/responsible-ai/lit-on-gcp)
for deploying LIT with [Cloud Run](https://cloud.google.com/run).

### New Stuff
* LIT on GCP -
[1075325](https://github.com/PAIR-code/lit/commit/1075325c6a08d8fdef3bcf66f193b8d5aef673fb),
[1acc868](https://github.com/PAIR-code/lit/commit/1acc868d4a5fa0fd2a135f132f56bb4cb8ba3990),
[55bfc99](https://github.com/PAIR-code/lit/commit/55bfc993cc27fd25ae5089d58ae822bfeca296a3),
[180f68a](https://github.com/PAIR-code/lit/commit/180f68ad3774f8b276e262c0dcb7307ad87e42a3),
[64114d5](https://github.com/PAIR-code/lit/commit/64114d553ffd2c0ffd7bc674fb32a36e564ea0f4),
[2488aa7](https://github.com/PAIR-code/lit/commit/2488aa7cf8f8a112607ca0c8b40870efde73ec24),
[9baac29](https://github.com/PAIR-code/lit/commit/9baac29b96970ef7fa64f2f36ce2c79ff73707b7),
[60bdc7c](https://github.com/PAIR-code/lit/commit/60bdc7cf382bd0c5ead2576c119277230a6080c9),
[7681476](https://github.com/PAIR-code/lit/commit/7681476d5056d927905f24333b890501a36df040),
[4c81182](https://github.com/PAIR-code/lit/commit/4c81182a7db1fda7f8ba071a9542876f462a13fa),
[4e5e8e2](https://github.com/PAIR-code/lit/commit/4e5e8e25c2abb658dc141f0d9c6059dd41e14535),
[b9a0b82](https://github.com/PAIR-code/lit/commit/b9a0b8210263da9ee6d741e4e0f0444849e3a141),
[424adce](https://github.com/PAIR-code/lit/commit/424adce9cf8c9cbabdf5d89d485cdc5f3fd098ed),
[1d019c7](https://github.com/PAIR-code/lit/commit/1d019c7a1bf5f135ea42104889167b79c3f795cd),
[f4436a2](https://github.com/PAIR-code/lit/commit/f4436a26ed79f481e16e2c53c0551703e7ba8c4f),

### Non-breaking Changes, Bug Fixes, and Enhancements
* Upgrade LIT to MobX v6. - [c1f5055](https://github.com/PAIR-code/lit/commit/c1f5055eb7ee8b3671484c863a0967c05fa58338)
* Fix indexing issue in Sequence Salience module. - [58b1d2](https://github.com/PAIR-code/lit/commit/58b1d2b6d0d27c6dca086520cef45bf75466a101)
* Load multiple model wrappers with shared model. - [ba4d975](https://github.com/PAIR-code/lit/commit/ba4d975a90612b0c41a02b3dcb4dbb548261fdd7)
* Add the custom model and dataset loaders to prompt debugging notebook. - [338c6b](https://github.com/PAIR-code/lit/commit/338c6b12de98b61287a25650ad2c6ad7f7bb80cd)
* Convert hosted demos images to multi-stage builds. - [4bf1f8](https://github.com/PAIR-code/lit/commit/4bf1f81666fe546357f00c86a2315d2852346ebe)
* Adding testing instructions to README. - [f24b841](https://github.com/PAIR-code/lit/commit/f24b841959f0402498a056a5164a86ecae6dbb94)
* More LIT documentation updates. - [2e9d267](https://github.com/PAIR-code/lit/commit/2e9d26738d9344cde0eebd66d49dfc14cd800e74)

## Release 1.2

This release covers clean-ups on various obsolete demos, as well as improved
packaging and isolated dependencies on the GLUE, Penguin, Prompt Debugging with
Sequence Salience and TyDi demos for easier launch.

### New Stuff
* Improved packaging and instructions for launching Prompt Debugging with
Sequence Salience demo, as well as minor bug fixes -
[08289df](https://github.com/PAIR-code/lit/commit/08289df0dd9927dee7147e5aad6e8b51bbe74f9e),
[675ca2d](https://github.com/PAIR-code/lit/commit/675ca2de21b68dc62e4909c80a2cd57d8ee8b601),
[15eccb1](https://github.com/PAIR-code/lit/commit/15eccb1197366c925a5beff310fb5d7d369bde0c),
[e0e35c3](https://github.com/PAIR-code/lit/commit/e0e35c3ffcfd9ad5331d4154e7d33d0b1d0daf89),
[c7970fb](https://github.com/PAIR-code/lit/commit/c7970fb8c51d2a8bd3647cc7eedd15cca285ac08),
[cee3b58](https://github.com/PAIR-code/lit/commit/cee3b58baea2de27633109e6dd5b3e4211fa46ea)

* Clean up of obsolete demos -
[b16059f](https://github.com/PAIR-code/lit/commit/b16059fbd0320d411298009c0226489e1f548a69),
[f4c0990](https://github.com/PAIR-code/lit/commit/f4c099082f0e89986aad162cc3cd0ac9bc2214c7),
[6aa2eb6](https://github.com/PAIR-code/lit/commit/6aa2eb64eddb8ca154401bfd6a039762bc374d6d),
[c2fb41b](https://github.com/PAIR-code/lit/commit/c2fb41b4945edb91fac973cf0ddbca48c6257511),
[dd196e9](https://github.com/PAIR-code/lit/commit/dd196e941058a1d4246b3df3a3c37595f9791b18),
[72fd772](https://github.com/PAIR-code/lit/commit/72fd772fa02c7445f27fb517e667987ea8ab34d7),
[71d88fb](https://github.com/PAIR-code/lit/commit/71d88fb86eb88ffb80d665cf7571b21d7ae06bd2),
[aa49340](https://github.com/PAIR-code/lit/commit/aa493409c454a2ed269fdedd15353404c14b4936),
[fc7b0d0](https://github.com/PAIR-code/lit/commit/fc7b0d0624f6cc8e456ac0a1d75a4149927bef2f),
[2475b3b](https://github.com/PAIR-code/lit/commit/2475b3bb677c8685ab9a291c490783ae2ccce5b8),
[a59641c](https://github.com/PAIR-code/lit/commit/a59641c014b17409e8e5cfdac1cc1e6916d6da15),
[1ed82d4](https://github.com/PAIR-code/lit/commit/1ed82d4e81ff6a6ff5146b6198e35444960d326b),
[7d5ef58](https://github.com/PAIR-code/lit/commit/7d5ef5831427de71416c096a6dbcd46ea064457e),
[992823b](https://github.com/PAIR-code/lit/commit/992823b027fca8c60edabe837248a508ac04da22),
[3dad2b0](https://github.com/PAIR-code/lit/commit/3dad2b061b45cb44b1c3f9b9364660e907662069),
[0656386](https://github.com/PAIR-code/lit/commit/0656386188d6e4b6c83dab58fb4e6569ebea217e),
[27d7a84](https://github.com/PAIR-code/lit/commit/27d7a841cf6d514e67ebfb2af9f603398499f6e3),
[8863019](https://github.com/PAIR-code/lit/commit/886301972ec1e7ed274040b46ec0e0c3f34c8ace),
[71cbdba](https://github.com/PAIR-code/lit/commit/71cbdbaee0fee8e96f52cd4df7a269a0873b9259),
[416d573](https://github.com/PAIR-code/lit/commit/416d573d79f84b9a6964d36e498b850a249ef452)

* Python requirements update and isolated setup for individual demos -
[bcc481e](https://github.com/PAIR-code/lit/commit/bcc481e44185d04268f5f8bb4ba762ec2cd35907),
[bb29f43](https://github.com/PAIR-code/lit/commit/bb29f430ff7be55d74a82aec5dee1e54fa27bed0),
[fbd8874](https://github.com/PAIR-code/lit/commit/fbd88746263fec0f72f2f01bcc382e88e902ab50),
[b3c120b](https://github.com/PAIR-code/lit/commit/b3c120b22138fb03a712f11778197cf4966d0c3a),
[5188c8c](https://github.com/PAIR-code/lit/commit/5188c8c835328efcc9dff5a0a4cf4cd79fabe099),
[5639e3b](https://github.com/PAIR-code/lit/commit/5639e3b1b71b1c0ddf4a3c9e1bd25517fba18375)

* Documentation cleanup and updates -
[afd51fe](https://github.com/PAIR-code/lit/commit/afd51fe299c0070a19946a789984957f14a9b5bb),
[7dda659](https://github.com/PAIR-code/lit/commit/7dda659bec4e933d187b0d7afc04d954ae262cc2),
[79ada6e](https://github.com/PAIR-code/lit/commit/79ada6edf8b2e485ec6a6425d4c60720b4dab8d1),
[1c8d6a0](https://github.com/PAIR-code/lit/commit/1c8d6a0269ce5637e05e79ae435f770e2a0da147),
[2e9d267](https://github.com/PAIR-code/lit/commit/2e9d26738d9344cde0eebd66d49dfc14cd800e74)

### Non-breaking Changes, Bug Fixes, and Enhancements
* Refactor DataService reactions - [483082d](https://github.com/PAIR-code/lit/commit/483082dcb0beb39795c0fc093fe93036bb6a274c)
* Add warm_start option to LitWidget - [a5265a4](https://github.com/PAIR-code/lit/commit/a5265a4feeb701b878986f79665d5fdf9ddc244c)
* Pretty-printing of Model objects - [4fb3bde](https://github.com/PAIR-code/lit/commit/4fb3bde897c68fdeb3bd829f6e5a88223bc131a4)
* Avoid equivalent shuffles in Scrambler - [0d8c0d9](https://github.com/PAIR-code/lit/commit/0d8c0d948480e0835fd3f451b95b7ec306b6409d)
* Updated gunicorn config for demos running in Docker - [b14e3b1](https://github.com/PAIR-code/lit/commit/b14e3b1a81d7b6305063f778f46666a4d1326045)
* Disable embeddings for TyDi - [7ff377f](https://github.com/PAIR-code/lit/commit/7ff377f92820748476e796994fd207e1b5dba1d9)
* Cast embeddings to float32 before computing distances - [5456011](https://github.com/PAIR-code/lit/commit/5456011db8ead5d53db6f39bcdca3fc388802fbe)
* Update colab examples to include installation of the lit-nlp package - [48b029c](https://github.com/PAIR-code/lit/commit/48b029c3a1a3f25d4d2611a9b0e94355d41078ef)

## Release 1.1.1

This release covers various improvements for sequence salience, including new
features in the UI module, support of more LLMs, and detailed tutorial and
documentation on how to use the sequence salience module for prompt engineering.

### New stuff
* New features in the sequence salience UI module -
[62f18b2](https://github.com/PAIR-code/lit/commit/62f18b2ff62bf77fa47205cffddf0d072a73c366),
[f0417c9](https://github.com/PAIR-code/lit/commit/f0417c93da282a4699253f335c2643be5e50567f),
[fe5a705](https://github.com/PAIR-code/lit/commit/fe5a705bfb013ac782e87351af08bc5b03204e71),
[1ec8626](https://github.com/PAIR-code/lit/commit/1ec8626da0e2a1922fb7812913f2677b232043ef),
[15184a1](https://github.com/PAIR-code/lit/commit/15184a18da69dacfb657c238ef8f5bac79ed7863),
[84af141](https://github.com/PAIR-code/lit/commit/84af141c7cf8a6ddb4db6ececec787ac235ddd17),
[27cafd8](https://github.com/PAIR-code/lit/commit/27cafd85636b3d18f40d15a01ffd5d0857ff0daa),
[3591e61](https://github.com/PAIR-code/lit/commit/3591e614fb09264ee03ae0c73510f1d0a4b74cdf),
[d108b59](https://github.com/PAIR-code/lit/commit/d108b596658f456f43e0b19473ab1c70c59cc065),
[309c4f2](https://github.com/PAIR-code/lit/commit/309c4f283af559ca34570e044d77d5c4a7cce540),
[99821d3](https://github.com/PAIR-code/lit/commit/99821d3b5505d857f919fe2455830e6c2338fd68),
[c8ee224](https://github.com/PAIR-code/lit/commit/c8ee224a445f925a9a7d6d7dc4472436190d0174)

* Support of more models (GPT2, Gemma, Llama, Mistral) on deep learning frameworks (Tensorflow, Pytorch) for Keras and Hugging Face -
[b26256a](https://github.com/PAIR-code/lit/commit/b26256a7c339c9e0940eb7a806528da23098ed03),
[45887d3](https://github.com/PAIR-code/lit/commit/45887d35d3880289595613224524157b19481ac0),
[b9941ed](https://github.com/PAIR-code/lit/commit/b9941ed7aea315022426710ccd32e8e1c7ff6c04),
[5ee7064](https://github.com/PAIR-code/lit/commit/5ee7064ec23933c41b4233061f9cc65b851fa7bb),
[8ea325b](https://github.com/PAIR-code/lit/commit/8ea325b292b09aecbb074abc877525dcdf4f4cd0)

* A tutorial to use sequence salience at [our website](https://pair-code.github.io/lit/tutorials/) and documentation updates -
[962faaa](https://github.com/PAIR-code/lit/commit/962faaabcf209f9cf024df5cc9684d8d4e4e64d8),
[96eff29](https://github.com/PAIR-code/lit/commit/96eff29198e69a8a9f2203d88f9027f5596f1614),
[f731e6d](https://github.com/PAIR-code/lit/commit/f731e6dfdeeb26f959022ed5aeda71e4f1f377d0),
[f4d7cac](https://github.com/PAIR-code/lit/commit/f4d7cacda3399d3e474420facd1a75d5b6af4824),
[49e7736](https://github.com/PAIR-code/lit/commit/49e77369fabd820b3f213f2d846efb5e81dbeafe)

### Non-breaking Changes, Bug Fixes, and Enhancements
* Py typing fix -
[d70e3d3](https://github.com/PAIR-code/lit/commit/d70e3d3c64671dfd5da034d3a47a34aedeef6469)
* Improvements on the curves UI module -
[3d61a09](https://github.com/PAIR-code/lit/commit/3d61a09b684b3d57fc23b1362091d5293d8e6d19)
[2efe62b](https://github.com/PAIR-code/lit/commit/2efe62b77dbfddf698b0a79408c0227bd21dc959)
* Support model-column search in LIT Data Table -
[525bf5e](https://github.com/PAIR-code/lit/commit/525bf5e7c005fe1f931867bc1206b527544865b3)
* Obsolete code cleanup -
[82abec6](https://github.com/PAIR-code/lit/commit/82abec688836b8e6d136de83e56660ea055dc91d)


## Release 1.1

This release provides the capabilities to interpret and debug the behaviors of
Generative AI models in LIT. Specifically, we added sequence salience, which
explains the impact of the preceding tokens on the generated tokens produced by
the GenAI models. Major changes include:
* An `LM salience` module in the LIT UI that computes generations, tokenization,
and sequence salience on-demand;
* Computation of sequence salience at different granularities, from the smallest
possible level of tokens, to more interpretable larger spans, such as words,
sentences, lines, or paragraphs.
* Support of OSS modeling frameworks, including KerasNLP and Hugging Face
Transformers for sequence salience computation.
This release would not have been possible without the work of our contributors.
Many thanks to:
[Ryan Mullins](https://github.com/RyanMullins),
[Ian Tenney](https://github.com/iftenney),
[Bin Du](https://github.com/bdu91), and
[Cibi Arjun](https://github.com/cpka145).

### New Stuff
* LM salience module in the LIT UI -
[ab294bd](https://github.com/PAIR-code/lit/commit/ab294bd3e15675c0e63e5a16ffe4b8cd4941c94f)
[5cffc4d](https://github.com/PAIR-code/lit/commit/5cffc4d933e611587b00c25861c911d5f734fa22)
[40bb57a](https://github.com/PAIR-code/lit/commit/40bb57a2531257c38137188090a24e70d47581c8)
[d3980cc](https://github.com/PAIR-code/lit/commit/d3980cc5414e1f9be895defc4f967bee8a2480fc)
[406fbc7](https://github.com/PAIR-code/lit/commit/406fbc7690ee72f6f96ecf68f1238822ae8951c2)
[77583e7](https://github.com/PAIR-code/lit/commit/77583e74236aa443a21ad0779b0ab9c023821b93)
[a758f98](https://github.com/PAIR-code/lit/commit/a758f98c5153f23955b0190a75dc1258ba57b645)
* Sequence salience for decoder-only LM, with support for GPT-2 and KerasNLP -
[27e6901](https://github.com/PAIR-code/lit/commit/27e6901164044c0d33658603369a55600da0b202)
[80cf699](https://github.com/PAIR-code/lit/commit/80cf699f92cd77d58cb2a2a60b9314010b1f336c)
[1df3ba8](https://github.com/PAIR-code/lit/commit/1df3ba8449e865edb5806c10c8054c246d1e38e3)
[b6ab352](https://github.com/PAIR-code/lit/commit/b6ab3522b301810cab3c75723f3fe0dabf829577)
[c97a710](https://github.com/PAIR-code/lit/commit/c97a710416538906ea6b269f90264c0602a15593)
* Prompt examples for sequence salience -
[4f19891](https://github.com/PAIR-code/lit/commit/4f1989180ee570642285682f843242be5bffb9ef)
[000c844](https://github.com/PAIR-code/lit/commit/000c84486ed61439c98dbfdd92959bdbb6f5119f)
[34aa110](https://github.com/PAIR-code/lit/commit/34aa110c36fe0c7ec670f06662078d2f572c79c6)
[ca032ff](https://github.com/PAIR-code/lit/commit/ca032ffb3196e71fd0a7a09118635ca6dafc8153)


### Non-breaking Changes, Bug Fixes, and Enhancements
* Improvements to display various fields and their default ranges -
[8a3f366](https://github.com/PAIR-code/lit/commit/8a3f366816833ead164ecfca778b465ef6d074bb)
[e63b674](https://github.com/PAIR-code/lit/commit/e63b67484fc7f4dbfa3484126c355350d2127bf7)
[d274508](https://github.com/PAIR-code/lit/commit/d2745088966c4ac31a3755f55096eeb8193c5a91)
* Allow only displaying the UI layouts provided by users -
[a219863](https://github.com/PAIR-code/lit/commit/a21986342d83ae64d58607e337fab9db7736242a)
* Internal dependency changes -
[f254fa8](https://github.com/PAIR-code/lit/commit/f254fa8500d6267278fa3dc32fb4bbf56beb7cf7)
[724bdee](https://github.com/PAIR-code/lit/commit/724bdee1f9ea45ce998b9031eea4ad1169299efb)
[2138bd9](https://github.com/PAIR-code/lit/commit/2138bd920e72553f9c920ba489962c8649738574)
* Fix issues with adding more than one example from counterfactual generators -
[d4302bd](https://github.com/PAIR-code/lit/commit/d4302bd6bfc7e4c778ba0e96397ac620242a8d21)
* Fix issues with loading `SimpleSentimentModel` -
[ac8ed59](https://github.com/PAIR-code/lit/commit/ac8ed5902a2c96019ea1137b5138d48017fabf4e)
* Notebook widget improvements -
[cdf79eb](https://github.com/PAIR-code/lit/commit/cdf79eb9048be3e6798e916d5e1ac4cc294929b0)
* Docs updates

## Release 1.0

This is a major release, covering many new features and API changes from the
`dev` branch since the v0.5 release over 8 months ago. This release includes
a variety of breaking changes meant to simplify various aspects of the LIT API
and visual changes to improve usability. This release includes over 250 commits.
Major changes include:

* Refactored python code to remove `_with_metadata` methods from all component
  and model classes.
* Refactored Model and BatchedModel python classes to remove `predict_minibatch`
  method.
* Reworked UI and backend logic for dynamic loading of new datasets and models
  from the UI. This makes use of the new `init_spec` methods for datasets and
  model classes.
  * Added a blank demo with no models or datasets preloaded which allows for
    dynamic loading of models and datasets through the UI.
* Refactored to upgrade metrics calculation from a type of interpreter to its
  own top-level concept.
* Updated front-end layout code to default to a new layout that includes a
  full height side-panel on the left side to complement the existing top and
  bottom panels, providing for more customization of module layouts.
* Added automatic metrics calculations for multilabel models.
* Added target selector dropdown for saliency methods.
* A visual redesign of the Salience Clustering module.
* Improved searching capabilities in the Data Table module.
* Improved the Data Table module's display of long strings through a "Show more"
  capability.
* Updated to Python 3.10.
* Updated to Node 18 and Typescript 5.0.
* Improved documentation pages, now at https://pair-code.github.io/lit/documentation/


This release would not have been possible without the work of our new
contributors in 2023. Many thanks to
[Minsuk Kahng](https://github.com/minsukkahng),
[Nada Hussein](https://github.com/nadah09),
[Oscar Wahltinez](https://github.com/owahltinez),
[Bin Du](https://github.com/bdu91), and
[Cibi Arjun](https://github.com/cpka145)
for your support and contributions to this project!
A full list of contributors to this repo can be found at https://github.com/PAIR-code/lit/graphs/contributors.

### Breaking Changes
* Adds init_spec() capability to models and datasets for dynamic loading -
  [d28eec3](https://github.com/PAIR-code/lit/commit/d28eec3b00737282e353230c99a25cd656897958),
  [d624562](https://github.com/PAIR-code/lit/commit/d624562931b001902cbeda474b21ece9208fad66),
  [7bb60b2](https://github.com/PAIR-code/lit/commit/7bb60b24aa0ec7755d9e763f8689a3558409e5bc),
  [f74798a](https://github.com/PAIR-code/lit/commit/f74798aedf2d10c9fcda5309e302ff3581b73a95),
  [db51d9d](https://github.com/PAIR-code/lit/commit/db51d9d8e20705978337d13ef288e38e28f44b62),
  [f3b0d6e](https://github.com/PAIR-code/lit/commit/f3b0d6eb9746397a8b535379adedb8bfd728dead),
  [0f133cf](https://github.com/PAIR-code/lit/commit/0f133cfdf92a86c81761d82369d560b464b69790),
  [9eebe57](https://github.com/PAIR-code/lit/commit/9eebe5748d8973a778b7c93c80d7d522c2945185),
  [bcc6c09](https://github.com/PAIR-code/lit/commit/bcc6c090a29f832288b336a832ac0912ed9116e8),
  [99e78ff](https://github.com/PAIR-code/lit/commit/99e78ffd87758600652c6819f5a9416c642ea9fd)
* Simplify Model spec code -
  [16b72f7](https://github.com/PAIR-code/lit/commit/16b72f7b9923c09b195e0a5f62132b1baeb9cce1)
* Promote Metrics to top-level property of LitMetadata -
  [f019279](https://github.com/PAIR-code/lit/commit/f0192796ffc6630336aa5e4dfef99aeacfaa90c7),
  [6ba1db8](https://github.com/PAIR-code/lit/commit/6ba1db8e4c7fcfe59e7a42c6b9e02887d21bf658),
  [c1777ea](https://github.com/PAIR-code/lit/commit/c1777eadf34ac615c9f00c4cdd6ca13e8f91794b)
* Remove _with_metadata and batched methods from models and components -
  [cb4f6b0](https://github.com/PAIR-code/lit/commit/cb4f6b01717768e1050a2a8b8a7356265ad3e9fe),
  [e020faa](https://github.com/PAIR-code/lit/commit/e020faa9ec9a5c8755814e9a7fc707b640e73492),
  [e9ce692](https://github.com/PAIR-code/lit/commit/e9ce692980fc048b351a3fb31f59c9d3c3e3c5bf),
  [5f1a971](https://github.com/PAIR-code/lit/commit/5f1a97149a03e8fdea5f0082ad7446c99c40f756),
  [061973a](https://github.com/PAIR-code/lit/commit/061973aa2c197d7b13d787da33b2d0d33ed9dcda),
  [ad65fd9](https://github.com/PAIR-code/lit/commit/ad65fd9584735f51ec9bbfa6e32dcf69024d43b0),
  [bc6f82b](https://github.com/PAIR-code/lit/commit/bc6f82b2d8477140e8ede54be233fd012c6d53f0),
  [7888c66](https://github.com/PAIR-code/lit/commit/7888c6677081049111f1c3d51943dea2c9351c59),
  [9767670](https://github.com/PAIR-code/lit/commit/976767089fbbb56ab14056c298bd0e5480b20486),
  [0ec1527](https://github.com/PAIR-code/lit/commit/0ec152786c5858822222901c2bea09ce3e5af036),
  [e30e59a](https://github.com/PAIR-code/lit/commit/e30e59a6d560bd5102b61a6b90e8251b33931228),
  [b29d1f3](https://github.com/PAIR-code/lit/commit/b29d1f393b165a1f9b39bcbfe1e13caa36c075cd),
  [5ed93bd](https://github.com/PAIR-code/lit/commit/5ed93bd59e9b540d173a6f3048a2c4f6f993f642),
  [5047bdd](https://github.com/PAIR-code/lit/commit/5047bddd617bff6314986133590f6bd5b6845faf),
  [a15cc88](https://github.com/PAIR-code/lit/commit/a15cc88222325cd539c204c1df7be395d7a07814),
  [0146d5f](https://github.com/PAIR-code/lit/commit/0146d5f101391cf31df0756bca1494107f0e50f6),
  [50fc3a4](https://github.com/PAIR-code/lit/commit/50fc3a4397d7f3ba2f004886990c77b7f5523747),
  [6fdcbfe](https://github.com/PAIR-code/lit/commit/6fdcbfe09e521439991d6e487c6b7ef61c69a170),
  [ce38565](https://github.com/PAIR-code/lit/commit/ce38565ecd0370361c78f332c3ac8813cd416b63)
* Simplifications and refactors in layout system -
  [4a5c0cb](https://github.com/PAIR-code/lit/commit/4a5c0cb8836bce4f483df82e0d09b75868c487be),
  [5f6a46a](https://github.com/PAIR-code/lit/commit/5f6a46a8a15fe557b8ec16b5e2898817100b3bc4),
  [2551c2c](https://github.com/PAIR-code/lit/commit/2551c2ced40a58e17b7b2bc2fd4bc06530090cb8),
  [cc7bfd5](https://github.com/PAIR-code/lit/commit/cc7bfd54456c4a3ef4a3d9c832e2bf06b3c63947),
  [fb2467d](https://github.com/PAIR-code/lit/commit/fb2467d9152cc2a9e5ee113ff0f6796db9a71808)
* Update LIT to Node 18 and TypeScript 5.0 -
  [7b96a6d](https://github.com/PAIR-code/lit/commit/7b96a6d7b42785a184752381f4d684f8923bff9d)
* Update LIT to Python 3.10 -
  [8bce86a](https://github.com/PAIR-code/lit/commit/8bce86a27dfc27dad37d8d2ebcab96cb8cdfde5e)



### New Stuff
* Add three-panel layout configuration option -
  [a95ed67](https://github.com/PAIR-code/lit/commit/a95ed67100f24163624edb4bb659ccfa871dc9bf)
* Add output embeddings and attention options to GlueConfig -
  [6e0df41](https://github.com/PAIR-code/lit/commit/6e0df41636405b4ee5556cbf797fcce5887c6070)
* Allow downloading/copying data from the slice editor -
  [57fac3a](https://github.com/PAIR-code/lit/commit/57fac3aeb98fa49c508b20837eded3f4ec80e8f9)
* Use new custom tooltip elemement in various places -
  [d409900](https://github.com/PAIR-code/lit/commit/d409900984336d4f8ac73735b1fff57c92623ca4),
  [bd0f7fc](https://github.com/PAIR-code/lit/commit/bd0f7fc47682b16dd4c8e530e17b1a295def1433),
  [6c25619](https://github.com/PAIR-code/lit/commit/6c2561994db506586b63de46a5900dd5dc6c0078),
  [7d30408](https://github.com/PAIR-code/lit/commit/7d3040819cfda82fe5ac2ce5b9fc46556918da20),
  [6779a4b](https://github.com/PAIR-code/lit/commit/6779a4b1fcba64bc0d8174ea46e58e6a7684af53),
  [9179c73](https://github.com/PAIR-code/lit/commit/9179c730b73a7defc746fdd775c5a0ce78d40e84)
* Add multi-label metrics to LIT -
  [c0e3663](https://github.com/PAIR-code/lit/commit/c0e3663156991ae3639e1ee707d613705f60f6f8)
* Improved UI for dynamic loading of models and datasets -
  [abc8d1a](https://github.com/PAIR-code/lit/commit/abc8d1a37ae14626211467f72a129f35415a1887),
  [b7ce560](https://github.com/PAIR-code/lit/commit/b7ce56037c27880b6d1c2ed27dce449c6a8d26ad)
* Replace conda installation instructions with pip
  [de23ceb](https://github.com/PAIR-code/lit/commit/de23ceb7c6801c71c63c253da669aab694c6c2c3)
* Add a new Blank demo for dynamic loading of models and datasets -
  [22b0dea](https://github.com/PAIR-code/lit/commit/22b0dea22a8167db7965ef8aec0b5c7e8b7509da)
* Add target-selector dropdowns to salience map module -
  [4c9a7ec](https://github.com/PAIR-code/lit/commit/4c9a7ecfc1a3d7a9fd3247b125d2e9c0d30a11f0),
  [10926ea](https://github.com/PAIR-code/lit/commit/10926ea2759db7881264b0b21924899cfb39de23),
  [f635ea7](https://github.com/PAIR-code/lit/commit/f635ea7a8548c8934db583a8a8f45bd63d38bd0a),
  [fe121ca](https://github.com/PAIR-code/lit/commit/fe121cabd240aff0bd08a9ba4a030dbd7ce12193),
  [8cb965a](https://github.com/PAIR-code/lit/commit/8cb965a78616f9ec7de133871ecf01d92a71293e)


### Non-breaking Changes, Bug Fixes, and Enhancements
* Fixes Scalars Module resize bug affecting datasets with scalar fields -
  [453461a](https://github.com/PAIR-code/lit/commit/453461a06b73b982b2db778ce05db8199d89193a)
* Moves Model-Dataset compatibility checks to Model class instead of ModelSpec -
  [c268ce4](https://github.com/PAIR-code/lit/commit/c268ce4890a627bc7c85d9fc277785b2d9d8ed85)
* Updates to the Salience Clustering module -
  [3a3aad3](https://github.com/PAIR-code/lit/commit/3a3aad302fcb97a89f43645bc81e2dd8fdeb3bfd),
  [7d3f235](https://github.com/PAIR-code/lit/commit/7d3f235fc9aad221f4c24b34545511f81eab9223),
  [ff759ad](https://github.com/PAIR-code/lit/commit/ff759ad313852844479d3f395a6c291ed18d3dce),
  [20ec052](https://github.com/PAIR-code/lit/commit/20ec052af16cc46f9a6159e2ada3ce6d03eda6f0)
* Data Table module improvements -
  [c7fa619](https://github.com/PAIR-code/lit/commit/c7fa619d921af92e34195d17b969596101dd24e0),
  [7301b28](https://github.com/PAIR-code/lit/commit/7301b28bd4456e0b9a981c7fd1e0dbc405d2b318),
  [dd23083](https://github.com/PAIR-code/lit/commit/dd23083f945cb660af9214d304be3b5045c5231d),
  [42d189a](https://github.com/PAIR-code/lit/commit/42d189a49f9e1c7afc3a92eda4e365994ff454fc),
  [1cc6964](https://github.com/PAIR-code/lit/commit/1cc696465a24fb5aaeb8f35e25b62fd673488555),
  [ab7da61](https://github.com/PAIR-code/lit/commit/ab7da61ef7f74441dad2b62b6065d5b1ff6f4d4c),
  [ee54333](https://github.com/PAIR-code/lit/commit/ee543339ca89aeb88648f68dfb2b09c87ecea145),
  [d74f2d6](https://github.com/PAIR-code/lit/commit/d74f2d626c62d0b1f8a76416bf6e3cb65cdb9429),
  [35487fa](https://github.com/PAIR-code/lit/commit/35487fa93d1987fc9a7eb98e2d20e3372e24f469),
  [ea25e75](https://github.com/PAIR-code/lit/commit/ea25e75a65f143b5a8c0ca9e4e71003d9a88b46e),
  [8c4bf1f](https://github.com/PAIR-code/lit/commit/8c4bf1ff998867540ae14f551bff2b5df64effd7),
  [ddf8e52](https://github.com/PAIR-code/lit/commit/ddf8e522a55e1ee60042ff2c54bb234f5a87106f)
* Various styling fixes, bug fixes, and code cleanup efforts
* Docs, FAQ, and README updates

## Release 0.5

This is a major release, covering many new features from the `dev` branch since
the v0.4 release nearly 11 months ago. Most notably, we're renaming! It's still
LIT, but now the L stands for "Learning" instead of "Language", to better
reflect the scope of LIT and support for non-text modalities like images and
tabular data. Additionally, we've made lots of improvements, including:

* New modules including salience clustering, tabular feature attribution, and
  a new Dive module for data exploration (inspired by our prior work on
  [Facets Dive](https://pair-code.github.io/facets/)).
* New demos and tutorials for input salience comparison and tabular feature
  attribution.
* Many UI improvements, with better consistency across modules and shared
  functionality for colors, slicing, and faceting of data.
* Better performance on large datasets (up to 100k examples), as well as
  improvements to the type system and new validation routines (`--validate`) for
  models and datasets.
* Download data as CSV directly from tables in the UI, and in notebook mode
  access selected examples directly from Python.
* Update to Python 3.9 and TypeScript 4.7.

This release would not have been possible without the work of many new
contributors in 2022. Many thanks to
[Crystal Qian](https://github.com/cjqian),
[Shane Wong](https://github.com/jswong65),
[Anjishnu Mukherjee](https://github.com/iamshnoo),
[Aryan Chaurasia](https://github.com/aryan1107),
[Animesh Okhade](https://github.com/animeshokhade),
[Daniel Levenson](https://github.com/dleve123),
[Danila Sinopalnikov](https://github.com/sinopalnikov),
[Deepak Ramachandran](https://github.com/DeepakRamachandran),
[Rebecca Chen](https://github.com/rchen152),
[Sebastian Ebert](https://github.com/eberts-google), and
[Yilei Yang](https://github.com/yilei)
for your support and contributions to this project!

### Breaking Changes

* Upgraded to Python 3.9 
  [17bfabd](https://github.com/PAIR-code/lit/commit/17bfabd75959feae4d64e79db695fe38be7a14b0)
* Upgraded to Typescript 4.7 
  [10e2548](https://github.com/PAIR-code/lit/commit/10e25480d43ecfa1800ed77fd5e2b49b69723c39)
* Layout definitions moved to Python 
  [05824c8](https://github.com/PAIR-code/lit/commit/05824c88296e9fed48ed6757b2f459ff6cc29968),
  [d3d19d2](https://github.com/PAIR-code/lit/commit/d3d19d2fbada9c12ab06630494c7cc84f9b3a9c8),
  [2994d7e](https://github.com/PAIR-code/lit/commit/2994d7e00582cff528e3753b43ce81ced00a1b30),
  [b78c962](https://github.com/PAIR-code/lit/commit/b78c96227bc760bb5009a1ed119b8fd568076767),
  [0eacdd0](https://github.com/PAIR-code/lit/commit/0eacdd026d2a0933f67d8aa2b5a1ec9d37a0d2d6)
* Moving classification and regression results to Interpreters 
  [2b4e622](https://github.com/PAIR-code/lit/commit/2b4e622922ba35df79e538c3a157356b854a54c6),
  [bcdbb80](https://github.com/PAIR-code/lit/commit/bcdbb8050ed1cdcd6350a556bbc394e67d4113fe),
  [dad8edb](https://github.com/PAIR-code/lit/commit/dad8edb8f05af8e4c3e46c352ee689988ec5cc11)
* Use a Pinning construct instead of comparison mode 
  [05bfc90](https://github.com/PAIR-code/lit/commit/05bfc906c91b3b748ffc7f3b414a046629ca16b1),
  [d7bdc65](https://github.com/PAIR-code/lit/commit/d7bdc654f147f879dec97e96f25d95d963fb7caa),
  [6a4ca00](https://github.com/PAIR-code/lit/commit/6a4ca0018211ed52e7eb24ec3d01ca4c683f179a),
  [0fe3c79](https://github.com/PAIR-code/lit/commit/0fe3c79352832c594a82a8e52d853c1c29742910),
  [5b2b737](https://github.com/PAIR-code/lit/commit/5b2b73767a2fb81f90c222786ed2a73b9171969d)
* Parallel, class-based Specs and LitTypes in Python and TypeScript code
    * Prep work 
      [db1ef3d](https://github.com/PAIR-code/lit/commit/db1ef3ddc7bd35df8c75325b9662fa41facf4359),
      [c85e556](https://github.com/PAIR-code/lit/commit/c85e556eedddf555449cd8e92b3218503b46dbb4),
      [660b8ef](https://github.com/PAIR-code/lit/commit/660b8ef3d47430e71fc0f9fcfad32a0e7b360557),
      [db58fa4](https://github.com/PAIR-code/lit/commit/db58fa42d18e605d997dce84f0b08797cc2729dc),
      [c020d25](https://github.com/PAIR-code/lit/commit/c020d2535a10ea137e25ea5ba87fa6d3d4cecc58),
      [eb02465](https://github.com/PAIR-code/lit/commit/eb024651e3b09e8bcd836e3558b6cef7e7b70160),
      [72edd26](https://github.com/PAIR-code/lit/commit/72edd26ed4f71d6b8d81ecefa5d09b508a29861d),
      [65c5b8a](https://github.com/PAIR-code/lit/commit/65c5b8a93643d4735c51e6ded48dcb3434203e60),
      [abb8889](https://github.com/PAIR-code/lit/commit/abb88890898848bb5a8fbe84f184a4b2b3a244cf),
      [4c93b62](https://github.com/PAIR-code/lit/commit/4c93b62da400ae30a86b65e415bd495f3e611449),
      [40d14e5](https://github.com/PAIR-code/lit/commit/40d14e5985c8dcde384de0b9f5bc469239e269f0),
      [9ec5324](https://github.com/PAIR-code/lit/commit/9ec53248e8c7b0a2e1ba0996e6084709ce2080ea),
      [40a661e](https://github.com/PAIR-code/lit/commit/40a661edafc71e1a0ae4f2d88eeb529d04c1172a)
    * Breaking changes to front-end typing infrastructure 
      [8c6ac11](https://github.com/PAIR-code/lit/commit/8c6ac1174cd1020c00491736a3d0fa78e05e0eed),
      [2522e4f](https://github.com/PAIR-code/lit/commit/2522e4f72e96c09a019630623b9061e73b4dce54),
      [0f8ff8e](https://github.com/PAIR-code/lit/commit/0f8ff8e251aee27654a9e1590c50aa5f75598edc),
      [58970de](https://github.com/PAIR-code/lit/commit/58970de691dea2be533e9c80e52768b2eb7b8f07),
      [ef72bfc](https://github.com/PAIR-code/lit/commit/ef72bfc4fcfc2bde06db0db0a7f105e9401d4cd2),
      [ccbb72c](https://github.com/PAIR-code/lit/commit/ccbb72c60d1eefc71c1eeca50408613cb65e445c),
      [a5b9f65](https://github.com/PAIR-code/lit/commit/a5b9f658188339c11c28fd43dbe25ff167e06c0b),
      [ab1e06a](https://github.com/PAIR-code/lit/commit/ab1e06a016fd7b309ee77237adf35f52d43e52d6),
      [853edd0](https://github.com/PAIR-code/lit/commit/853edd0b03f695aaa5d708312325dc13758070da),
      [cb528f1](https://github.com/PAIR-code/lit/commit/cb528f1bd502edf9f6ed25734a1ef81cfbff007b),
      [a36a936](https://github.com/PAIR-code/lit/commit/a36a936689443b4ed2417299e17dcd5a0b49de39),
      [74b5dbb](https://github.com/PAIR-code/lit/commit/74b5dbbb23259df7c3233cfcedce588ef62def82),
      [e811359](https://github.com/PAIR-code/lit/commit/e811359cabd092bacf14799ab811c314f6a8bf84)
    * Build fixes 
      [948adb3](https://github.com/PAIR-code/lit/commit/948adb3d35894cbd78cc73ddbe2ea8da5a883ace)
* Minimizing duplication in modules
    * Classification Results 
      [4f2b53d](https://github.com/PAIR-code/lit/commit/4f2b53d94c73e210a1def9043623590e077ee1b8)
    * Scalars, including its migration to Megaplot 
      [353b96e](https://github.com/PAIR-code/lit/commit/353b96ea5fd0aca9ace2ac47491b99d58cbbbc67),
      [ed07199](https://github.com/PAIR-code/lit/commit/ed07199189bce50446e05506cdfb8260781977eb),
      [184c8c6](https://github.com/PAIR-code/lit/commit/184c8c684c1f497f8911a5e886cec604b46c12f9),
      [14f82d5](https://github.com/PAIR-code/lit/commit/14f82d53b2e41b2cc088db3c1df3ebac5aee193a),
      [764674a](https://github.com/PAIR-code/lit/commit/764674a0430fc8e55535e09ad4bae4dc1eac1234)
* Changes to component `is_compatible()` signature
    * Added checks to some generators 
      [9b2de92](https://github.com/PAIR-code/lit/commit/9b2de92101b0a0c4961007a0a37fa936ee708e29),
      [db94849](https://github.com/PAIR-code/lit/commit/db948496d7b040463328ce926499d79e9a4d434d)
    * Added Dataset parameter to all checks 
      [ecd3a66](https://github.com/PAIR-code/lit/commit/ecd3a6623f2a0d45ae26c74d0d72fb68b7bcb9aa)
* Adds `core` components library to encapsulate default interpreters,
  generators, and metrics 
  [9ea4ab2](https://github.com/PAIR-code/lit/commit/9ea4ab264f6d9b03ee19ab8af4309e97862c089a)
* Removed the Color module 
  [b18d887](https://github.com/PAIR-code/lit/commit/b18d8871ea7ab1d2b5e4c671d33653d32f87d952)
* Removed the Slice module 
  [7db22ae](https://github.com/PAIR-code/lit/commit/7db22ae197650935ab916b248ca3c06f8593afb5)
* Moved star button to Data Table module 
  [cd14f35](https://github.com/PAIR-code/lit/commit/cd14f355781500b07a433a5df58d2ca0ec8ed6f8)
* Salience Maps now inside of expansion panels with popup controls 
  [1994425](https://github.com/PAIR-code/lit/commit/199442552586fa48780a33166cd6927ba4ab3530)
* Metrics
    * Promotion to a major `component` type 
      [de7d8ba](https://github.com/PAIR-code/lit/commit/de7d8ba26e74ecf2fd8a7700352e0d6d469d22ac)
    * Improved compatibility checks 
      [0d8341d](https://github.com/PAIR-code/lit/commit/0d8341d9f120359bec86c983c5618dd59bb6f591)

### New Stuff

* Common Color Legend element 
  [f846772](https://github.com/PAIR-code/lit/commit/f8467720d33dd8ef3d0da5c5a12eed2db37bb4b0),
  [7a1e26a](https://github.com/PAIR-code/lit/commit/7a1e26a9759882e0bf697363298e68f969c24a84),
  [0cc934c](https://github.com/PAIR-code/lit/commit/0cc934c8980a6d4563319087fd7e9ee5201acd04)
* Common Expansion Panel element 
  [2d67ce](https://github.com/PAIR-code/lit/commit/2d670ce70a6e41d7c2fc1d4d9b8c37c2b3b8876b)
* Common Faceting Control 
  [0f46e16](https://github.com/PAIR-code/lit/commit/0f46e166595c83773611a715be694100d89cace0),
  [b109f9b](https://github.com/PAIR-code/lit/commit/b109f9b8cad9c26f328c1634122fe874309d5b53),
  [8993f9b](https://github.com/PAIR-code/lit/commit/8993f9b5cd92f0d4fdfcd1c9e654c2aa4e15fb98),
  [670abeb](https://github.com/PAIR-code/lit/commit/670abeb25dbdc747067fae725a50a873355eb368)
* Common Popup element 
  [1994425](https://github.com/PAIR-code/lit/commit/199442552586fa48780a33166cd6927ba4ab3530),
  [cca3511](https://github.com/PAIR-code/lit/commit/cca3511322189ddb49bb6a533576d01f532a6f23)
* A new Dive module for exploring your data 
  [155e0c4](https://github.com/PAIR-code/lit/commit/155e0c4f1fb8198a18186c432bdb1516e9910f9e),
  [1d17ca2](https://github.com/PAIR-code/lit/commit/1d17ca23245765d2ded6790902eb5c4b9af3c954),
  [a0da9cf](https://github.com/PAIR-code/lit/commit/a0da9cf0643c2468b06d964b942aa523cd06069c)
* Copy or download data from Table elements 
  [d23ecfc](https://github.com/PAIR-code/lit/commit/d23ecfc74993dc932d88e412170cbb3cf6998408)
* Training Data Attribution module 
  [5ff9102](https://github.com/PAIR-code/lit/commit/5ff91029b05bea2d47835b81e840387ce8e70294),
  [c7398f8](https://github.com/PAIR-code/lit/commit/c7398f82f845180192a76eba2c0caade05a5c0bc)
* Tabular Feature Attribution module with a heatmap mode and
  [SHAP](https://shap.readthedocs.io/en/latest/index.html) interpreter 
  [45e526c](https://github.com/PAIR-code/lit/commit/45e526c76c586ba3539f28c0e03ab4adb9825def),
  [76379ad](https://github.com/PAIR-code/lit/commit/76379adac37f7e284faf979673cbb0399a36d8ee)
* Salience Clustering module 
  [8f3c26c](https://github.com/PAIR-code/lit/commit/8f3c26c60b652ae22cbb8c64e4b2212747c40413),
  [fb795e8](https://github.com/PAIR-code/lit/commit/fb795e8949b4b430c96e6d02d001e0a9aedd6c42),
  [49faa00](https://github.com/PAIR-code/lit/commit/49faa002d648a4b128c862f63b8202bf739c75d2),
  [e35d8d8](https://github.com/PAIR-code/lit/commit/e35d8d84edb9bc1ced5c4bc5e7bbcd8307dc99ac),
  [7505861](https://github.com/PAIR-code/lit/commit/75058615bc46b53c82b8561ae2bf80ff4c0eb2aa),
  [f970958](https://github.com/PAIR-code/lit/commit/f970958024c821880b3238d7a2f293b947a4e1e7)
* Selection state syncing in Python notebooks 
  [08abc2c](https://github.com/PAIR-code/lit/commit/08abc2ca3a25f368823a4a9f3ba9d5b5ebeac7a6),
  [06613b9](https://github.com/PAIR-code/lit/commit/06613b909173978c1d4648c8b37c28269a783c14)
* Unified DataService 
  [9bdc23e](https://github.com/PAIR-code/lit/commit/9bdc23e7890e8afeb7ab6dcc89c8cb7730c10b26),
  [00749fc](https://github.com/PAIR-code/lit/commit/00749fc0d4f83cad204a69d792f602c63b1ff676)
* AUC ROC and AUC PR Curve interpreters and module  
  [51842ba](https://github.com/PAIR-code/lit/commit/51842babef63f9aa29d1d2add14633c4640627fc),
  [0f9fd4d](https://github.com/PAIR-code/lit/commit/0f9fd4dccc9e6375c577012191f89c3fb7067b01),
  [0558ef5](https://github.com/PAIR-code/lit/commit/0558ef52276ed6797a7a6f9d88721a50b6d6a792),
  [4efd58e](https://github.com/PAIR-code/lit/commit/4efd58e788a3cd38852961b136d8461f3b75b3d7)
* Splash screen documentation 
  [1f09ae9](https://github.com/PAIR-code/lit/commit/1f09ae9ca326dbaf0e5541f0f24370b56bcc6d1b),
  [cfabe78](https://github.com/PAIR-code/lit/commit/cfabe7865df5fd51ff8c483296f3fccc0fa30d28),
  [aca35d8](https://github.com/PAIR-code/lit/commit/aca35d832ad00a4bf35fd27adf35ba76f4d0d87f)
* Added a `GeneratedURL` type that displays in the Generated Text module 
  [bb06368](https://github.com/PAIR-code/lit/commit/bb06368602cfcca656746525de16a603e2359cb3)
* Added new built-in
  [ROUGE](https://en.wikipedia.org/wiki/ROUGE_(metric)) and Exact Match
  metrics 
  [6773927](https://github.com/PAIR-code/lit/commit/67739270434388a63627ee1bc405bc16923dd631),
  [eac9382](https://github.com/PAIR-code/lit/commit/eac9382cebbc9d1e974ec0e7b6bc1cd528a4df1a)
* Input Salience demo 
  [a98edce](https://github.com/PAIR-code/lit/commit/a98edce9caf8e8481f4105cb26b57d5d0429f963),
  [75ff835](https://github.com/PAIR-code/lit/commit/75ff835ed2e051899d3839ab0ca4360bbf0b9897),
  [55579de](https://github.com/PAIR-code/lit/commit/55579de33fd292b34fecbcd058686cab1f05fd74)
* Model and Dataset validation 
  [0fef77a](https://github.com/PAIR-code/lit/commit/0fef77a7835bbfc9a022a9b1c99b10fc9f5a55c7)
* Tutorials written by our Google Summer of Code contributor,
  [Anjishnu Mukherjee](https://github.com/iamshnoo)
    * Using LIT for Tabular Feature Attribution 
      [2c0703d](https://github.com/PAIR-code/lit/commit/2c0703d69b3c5d3f9ef5aa4c03fe3c6262e707c3)
    * Making Sense of Salience Maps 
      [4159527](https://github.com/PAIR-code/lit/commit/415952702893febbcea9d631ad1a289a3e43e27c)

### Non-breaking Changes, Bug Fixes, and Enhancements

* Added Dataset embeddings to Embeddings projector 
  [78e2e9c](https://github.com/PAIR-code/lit/commit/78e2e9c05c831fafd360da5f1c3b9b4e12054df9),
  [3c0929f](https://github.com/PAIR-code/lit/commit/3c0929f9bb293391471d5bc3c1219b6025946354),
  [e7ac98b](https://github.com/PAIR-code/lit/commit/e7ac98bbabb5b0bf40bd956724cc5a63aef10350)
* Added a sparse mode to Classification Results 
  [20a8f31](https://github.com/PAIR-code/lit/commit/20a8f316ec0b3d68cd131b785b8dfd6fa61ab3e5)
* Added Show only generated option to Data Table module 
  [4851c9d](https://github.com/PAIR-code/lit/commit/4851c9de8917d35e2e1cc66d8d33d52f78418acf)
* Added threshold property for `MulticlassPreds` that allows for default
  threshold values other than 0.5 
  [5e91b19](https://github.com/PAIR-code/lit/commit/5e91b1984700f6c1bb25b05d25e091d8d522c7e9)
* Added toggle for module duplication direction 
  [4e05a75](https://github.com/PAIR-code/lit/commit/4e05a759bca13afe857abd10abfdb5229d1ae622)
* Clickable links in the Generated Images module 
  [8cf8119](https://github.com/PAIR-code/lit/commit/8cf8119cbdaa5beea2b615d2eadb66630234af38)
* Constructor parameters for salience interpreters  [
  ab057b5](https://github.com/PAIR-code/lit/commit/ab057b55a938a59b87f08597050af5adfa2b8bcc)
* Image upload in Datapoint Editor 
  [a23b146](https://github.com/PAIR-code/lit/commit/a23b14676c7cb4fa7b82e42f9b6c036108801a54)
* Markdown support in LIT component descriptions 
  [0eaa00c](https://github.com/PAIR-code/lit/commit/0eaa00c1f58097c6e77354678f0b603eeabe74cd)
* Selection updates based on interactions in Metrics module 
  [c3b6a0c](https://github.com/PAIR-code/lit/commit/c3b6a0cceb300de5e18ff9bd68cf8c29b49b49b8)
* Support for many. new types of inputs in the Datapoint editor, including
  `GeneratedText`, `GeneratedTextCandidates`, `MultiSegmentAnnotation`,
  `Tokens`, `SparseMultilabel`, and `SparseMultilabelPreds`
* Various styling fixes and code cleanup efforts
* Docs, FAQ, and README updates

## Release 0.4.1

This is a bug fix release aimed at improving visual clarity and common
workflows.

The UI has been slightly revamped, bugs have been fixed, and new capabilities
have been added. Notable changes include:

- Adds "open in new tab" feature to LIT Notebook widget
- Adds support for `SparseMultilabelPreds` to LIME
- Improves color consistency across the UI
- Switching NumPy instead of SciKit Learn for PCA
- Ensuring all built-in demos are compatible with the Docker
- Updating the Dockerfile to support run-time `DEMO_NAME` and `DEMO_PORT` args
- Fixed a rendering bug in the Confusion Matrix related column and row spans
  when "hide empty labels" is turned on

## Release 0.4

This release adds a lot of new features. The website and documentation have
been updated accordingly.

The UI has been slightly revamped, bugs have been fixed, and new capabilities
have been added. Notable changes include:
- Support for Google Cloud Vertex AI notebooks.
- Preliminary support for tabular and image data, in addition to NLP models.
- Addition of TCAV global interpretability method.
- New counterfactual generators for ablating or flipping text tokens for
  minimal changes to flip predictions.
- New counterfactual generator for tabular data for minimal changes to flip
  predictions.
- Partial dependence plots for tabular input features.
- Ability to set binary classification thresholds separately for different
  facets of the dataset
- Controls to find optimal thresholds across facets given different fairness
  constraints, such as demographic parity or equal opportunity.

## Release 0.3

This release adds the ability to use LIT directly in colab and jupyter
notebooks. The website and documentation have been updated accordingly.

The UI has been slightly revamped, bugs have been fixed, and new capabilities
have been added. Notable changes include:
- Notebook mode added.
- New annotated text visualization module added.
- Allow saving/loading of generated datapoints, and dynamic adding of new
  datasets by path in the UI.
- Added synchronized scrolling between duplicated modules when comparing
  datapoints or models.
- Added a focus service for visually linking focus (i.e. hover) states between
  components.
- Allow layouts to be specified on LIT creation in python.

## Release 0.2

This release of LIT coincides with the EMNLP 2020 conference, where the LIT
paper was presented, and the publication of the LIT website, including tutorials
and hosted demos.

The UI has been slightly revamped, bugs have been fixed, and new capabilities
have been added.

## Release 0.1.1

This release of LIT adds a pip package for easy installation, cleans up some of
the code and documentation, and adds more examples.

## Release 0.1

This is the initial release of LIT.

================
File: requirements_examples_common.txt
================
# Copyright 2024 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================

# LINT.IfChange
-r requirements.txt

gunicorn>=20.1.0
tensorflow>=2.16.0
transformers>=4.27.1
# LINT.ThenChange(./pyproject.toml)

================
File: requirements_examples_discriminative_ai.txt
================
# Copyright 2024 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
# LINT.IfChange
-r requirements_examples_common.txt

tensorflow-datasets>=4.9.0
tf-keras>=2.16
# LINT.ThenChange(./pyproject.toml)

================
File: requirements_examples_generative_ai.txt
================
# Copyright 2024 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
# LINT.IfChange
-r requirements_examples_common.txt

google-cloud-aiplatform>=1.60.0
keras>=3.0.0
keras-nlp>=0.14.0
sentencepiece==0.1.99
tensorflow-text>=2.16.0
torch>=2.0.0
vertexai>=1.49.0
# LINT.ThenChange(./pyproject.toml)

================
File: requirements_test.txt
================
# Copyright 2023 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
# LINT.IfChange
-r requirements_examples_discriminative_ai.txt
-r requirements_examples_generative_ai.txt

lime==0.2.0.1
pytest>=7.4.0,<8.0.0
umap-learn==0.5.6
webtest>=2.0
# LINT.ThenChange(./pyproject.toml)

================
File: requirements.txt
================
# Copyright 2023 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
# LINT.IfChange
absl-py>=1.4.0
annoy>=1.17.3
attrs>=22.1.0
etils[epath]>=1.5.0
filelock>=3.12.3
google-cloud-translate>=3.11.1
ipython>=7.34.0
Levenshtein>=0.21.1
matplotlib>=3.6.0,<3.9.0
ml-collections>=0.1.1
numpy>=1.24.1,<2.0.0
pandas[output-formatting]>=2.0.3
Pillow>=10.0.0
portpicker>=1.5.2
requests>=2.31.0
rouge-score>=0.1.2
sacrebleu>=2.3.1
saliency>=0.1.3
scikit-learn>=1.0.2
scipy>=1.10.1
shap>=0.42.0,<0.46.0
six>=1.16.0
termcolor>=2.3.0
tqdm>=4.64.0
werkzeug>=2.2.3
# LINT.ThenChange(./pyproject.toml)

================
File: website/README.md
================
# Learning Interpretability Tool Website

This directory contains the code for the Learning Interpretability Tool website,
which is hosted on GitHub Pages at https://pair-code.github.io/lit.

The website consists of two parts: - A main site built with
[Eleventy](https://www.11ty.dev/), served at
`https://pair-code.github.io/lit/` - A documentation site built with
[Sphinx](https://www.sphinx-doc.org/), at
`https://pair-code.github.io/lit/documentation/`

If you wish to help make changes to our website, you've come to the right place.

**NOTE**, any updates should be made in a feature branch to the `/website/src/`
content. You can then make a pull request to the "dev" branch, and it will be
committed to the repo and deployed by a member of the LIT team.

## Dependencies

For Sphinx: `pip install sphinx myst-parser linkify-it-py furo`

For Eleventy (11ty): `npm install -g @11ty/eleventy`

Then from this directory, run `npm install`

## Directories

-   `website/src` - source for the Eleventy site
-   `website/sphinx_src` - source for the Sphinx site
-   `website/www` - source for compiled HTML site for local development
-   `docs/` compiled HTML site to be served by GitHub Pages

Don't edit `docs/` directly; rather edit the source and follow the instructions
below.

## Local Testing of the Homepage and Demos

Run `./local.sh` and navigate to the URL shown, or `http://localhost:8080`.

Note that this will auto-refresh if the 11ty source is updated, but only build
the Sphinx site once.

## Building the Site for Deployment to Github Pages

Run this any time `/website/src/` or `/website/sphinx_src/` has changed.

1.  Once you're ready to deploy, run `./deploy.sh` from the command-line while
    in the `/website` directory.
2.  This will build the site, remove the old `docs/` folder and replace it with
    the updates.
3.  Commit the docs folder to the Github repo, and wait a few minutes for Github
    CDNs to reflect your changes.

================
File: website/sphinx_src/api.md
================
# LIT Python API

<!--* freshness: { owner: 'lit-dev' reviewed: '2024-06-24' } *-->

<!-- [TOC] placeholder - DO NOT REMOVE -->

## Design Overview

LIT is a modular system, comprising a collection of backend components (written
in Python) and frontend modules (written in TypeScript). Most users will develop
against the Python API, which is documented below and allows LIT to be extended
with custom models, datasets, metrics, counterfactual generators, and more. The
LIT server and components are provided as a library which users can use through
their own demo binaries or via Colab.

The components can also be used as regular Python classes without starting a
server; see [below](#using-lit-components-outside-of-lit) for details.

![LIT system overview](./images/lit-system-diagram.svg)

The LIT backend serves models, data, and interpretability components, each of
which is a Python class implementing a minimal API and relying on the
[spec system](#type-system) to detect fields and verify compatibility. The
server is stateless, but implements a caching layer for model predictions - this
simplifies component design and allows interactive use of large models like BERT
or T5.

The frontend is a stateful single-page app, built using
[Lit](https://lit.dev/)[^1] for modularity and [MobX](https://mobx.js.org/) for
state management. It consists of a core UI framework, a set of shared "services"
which manage persistent state, and a set of independent modules which render
visualizations and support user interaction. For more details, see the
[UI guide](./ui_guide.md) and the
[frontend developer guide](./frontend_development.md).

[^1]: Naming is just a happy coincidence; the Learning Interpretability Tool is
      not related to the Lit projects.


## Adding Models and Data

To run LIT with your own models and data, you can create a custom `demo.py`
script that passes these to the LIT server. For example:

```py
def main(_):
  # MulitiNLIData implements the Dataset API
  datasets = {
      'mnli_matched': MultiNLIData('/path/to/dev_matched.tsv'),
      'mnli_mismatched': MultiNLIData('/path/to/dev_mismatched.tsv'),
  }

  # NLIModel implements the Model API
  models = {
      'model_foo': NLIModel('/path/to/model/foo/files'),
      'model_bar': NLIModel('/path/to/model/bar/files'),
  }

  lit_demo = lit_nlp.dev_server.Server(models, datasets, port=4321)
  lit_demo.serve()

if __name__ == '__main__':
  main()
```

Conceptually, a dataset is just a list of examples and a model is just a
function that takes examples and returns predictions. The [`Dataset`](#datasets)
and [`Model`](#models) classes implement this, and provide metadata (see the
[type system](#type-system)) to describe themselves to other components.

For pre-built `demo.py` examples, check out
https://github.com/PAIR-code/lit/tree/main/lit_nlp/examples

### Validating Models and Data

Datasets and models can optionally be validated by LIT to ensure that dataset
examples match their spec and that model output values match their spec.
This can be very helpful during development of new model and dataset wrappers
to ensure correct behavior in LIT.

At LIT server startup, the `validate` flag can be used to enable validation.
There are three modes:

*   `--validate=first` will check the first example in each dataset.
*   `--validate=sample` will validate a sample of 5% of each dataset.
*   `--validate=all` will run validation on all examples from all datasets.

Additionally, if using LIT datasets and models outside of the LIT server,
validation can be called directly through the
[`validation`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/lib/validation.py) module.

## Datasets

Datasets ([`Dataset`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/api/dataset.py)) are
just a list of examples, with associated type information following LIT's
[type system](#type-system).

*   `spec()` should return a flat dict that describes the fields in each example
*   `self._examples` should be a list of flat dicts

LIT operates on all examples loaded in the datasets you include in your LIT
server, therefore you should take care to use dataset sizes that can fit into
memory on your backend server and can be displayed in the browser.

NOTE: See the [FAQ](./faq.md) for more details on dataset size limitations.

Implementations should subclass
[`Dataset`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/api/dataset.py). Usually this
is just a few lines of code - for example, the following is a complete
implementation for the [MultiNLI](https://cims.nyu.edu/~sbowman/multinli/)
dataset:

```py
class MultiNLIData(Dataset):
  """Loader for MultiNLI development set."""

  NLI_LABELS = ['entailment', 'neutral', 'contradiction']

  def __init__(self, path: str):
    # Read the eval set from a .tsv file as distributed with the GLUE benchmark.
    df = pandas.read_csv(path, sep='\t')
    # Store as a list of dicts, conforming to self.spec()
    self._examples = [{
      'premise': row['sentence1'],
      'hypothesis': row['sentence2'],
      'label': row['gold_label'],
      'genre': row['genre'],
    } for _, row in df.iterrows()]

  def spec(self) -> types.Spec:
    return {
      'premise': lit_types.TextSegment(),
      'hypothesis': lit_types.TextSegment(),
      'label': lit_types.CategoryLabel(vocab=self.NLI_LABELS),
      # We can include additional fields, which don't have to be used by the model.
      'genre': lit_types.CategoryLabel(),
    }
```

In this example, all four fields (premise, hypothesis, label, and genre) have
string values, but the [semantic types](#type-system) tell LIT a bit more about
how to interpret them:

*   `premise` and `hypothesis` should be treated as natural-language text
    (`TextSegment`)
*   `label` should be treated as a categorical feature (`CategoryLabel`) with a
    fixed, known set of possible values (`vocab=self.NLI_LABELS`)
*   `genre` should be treated as a categorical feature, but with an unknown or
    open set of values.

This implementation uses Pandas to read a TSV file, but you can also use
services like [TensorFlow Datasets](https://www.tensorflow.org/datasets) -
simply wrap them in your `__init__()` function.

Note that you can freely add additional features - such as `genre` in the
example above - which the model may not be aware of. The LIT UI will recognize
these features for slicing, binning, etc., and they will also be available to
interpretation components such as custom metrics.

### Transformations

The `Dataset` class also supports a limited set of transformations, similar to
TensorFlow's
[tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)
but more limited in scope and aimed at supporting quick iteration:

*   `Dataset.slice[start:step:end]` will return a new `Dataset` with the same
    spec and a slice of the datapoints.
*   `Dataset.sample(n, seed=42)` will return a new `Dataset` with the same spec
    and a random sample of the datapoints.
*   `Dataset.remap(field_map: dict[str, str])` will return a new `Dataset` with
    renamed fields in both the examples and spec.

The latter is a shortcut to use datasets matching one model with another; for
example, a dataset with a `"document"` field can be used with a model expecting
a `"text"` input via `Dataset.remap({"document":
"text"})`.[^why-not-standardize-names]

[^why-not-standardize-names]: We could solve this particular case by
    standardizing names, but one still needs to be
    explicit if there are multiple segments available,
    such as `"question"` and `"document"` for a QA
    task.

## Models

Models ([`Model`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/api/model.py)) are
functions which take inputs and produce outputs, with associated type
information following LIT's [type system](#type-system). The core API consists
of three methods:

*   `input_spec()` should return a flat dict that describes necessary input
    fields
*   `output_spec()` should return a flat dict that describes the model's
    predictions and any additional outputs
*   `predict()` should take a sequence of inputs (satisfying `input_spec()`) and
    yields a parallel sequence of outputs matching `output_spec()`.

Implementations should subclass
[`Model`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/api/model.py). An example for
[MultiNLI](https://cims.nyu.edu/~sbowman/multinli/) might look something like:

```py
class NLIModel(Model):
  """Wrapper for a Natural Language Inference model."""

  NLI_LABELS = ['entailment', 'neutral', 'contradiction']

  def __init__(self, model_path: str, **kw):
    # Load the model into memory so we're ready for interactive use.
    self._model = _load_my_model(model_path, **kw)

  ##
  # LIT API implementations
  def predict(self, inputs: Iterable[Input]) -> Iterable[Preds]:
    """Predict on a stream of examples."""
    examples = [self._model.convert_dict_input(d) for d in inputs]  # any custom preprocessing
    return self._model.predict_examples(examples)  # returns a dict for each input

  def input_spec(self) -> types.Spec:
    """Describe the inputs to the model."""
    return {
        'premise': lit_types.TextSegment(),
        'hypothesis': lit_types.TextSegment(),
    }

  def output_spec(self) -> types.Spec:
    """Describe the model outputs."""
    return {
      # The 'parent' keyword tells LIT where to look for gold labels when computing metrics.
      'probas': lit_types.MulticlassPreds(vocab=NLI_LABELS, parent='label'),
    }
```

Unlike the dataset example, this model implementation is incomplete - you'll
need to customize `predict()` accordingly with any pre- or post-processing
needed, such as tokenization.

Many deep learning models support a batched prediction behavior. Thus, we
provide the `BatchedModel` class that implements simple batching. Users of this
class must implement the `predict_minibatch()` function, which should convert
a `Sequence` of `JsonDict` objects to the appropriate batch representation
(typically, a `Mapping` of strings to aligned `Sequences` or `Tensors`) before
calling the model. Optionally, you may want to override the
`max_minibatch_size()` function, which determines the batch size.

Note: there are a few additional methods in the model API - see
[`Model`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/api/model.py) for details.

If your model is on a remote server, consider using the `BatchedRemoteModel`
base class, which implements parallel batched requests using a thread pool.

### Adding more outputs

The above example defined a black-box model, with predictions but no access to
internals. If we want a richer view into the model's behavior, we can add
additional return fields corresponding to hidden-state activations, gradients,
word embeddings, attention, or more. For example, a BERT-based model with
several such features might have the following `output_spec()`:

```py
  def output_spec(self) -> types.Spec:
    """Describe the model outputs."""
    return {
      # The 'parent' keyword tells LIT where to look for gold labels when computing metrics.
      'probas': lit_types.MulticlassPreds(vocab=NLI_LABELS, parent='label'),
      # This model returns two different embeddings (activation vectors), but you can easily add more.
      'output_embs': lit_types.Embeddings(),      # from [CLS] token at top layer
      'mean_word_embs':  lit_types.Embeddings(),  # mean of input word embeddings
      # In LIT, we treat tokens as another model output. There can be more than one,
      # and the 'parent' field describes which input segment they correspond to.
      'premise_tokens': lit_types.Tokens(parent='premise'),
      'hypothesis_tokens': lit_types.Tokens(parent='hypothesis'),
      # Gradients are also returned by the model; 'align' here references a Tokens field.
      'premise_grad': lit_types.TokenGradients(align='premise_tokens'),
      'hypothesis_grad': lit_types.TokenGradients(align='hypothesis_tokens'),
      # Similarly, attention references a token field, but here we want the model's full "internal"
      # tokenization, which might be something like: [START] foo bar baz [SEP] spam eggs [END]
      'tokens': lit_types.Tokens(),
      'attention_layer0': lit_types.AttentionHeads(align=['tokens', 'tokens']),
      'attention_layer1': lit_types.AttentionHeads(align=['tokens', 'tokens']),
      'attention_layer2': lit_types.AttentionHeads(align=['tokens', 'tokens']),
      # ...and so on. Since the spec is just a dictionary of dataclasses, you can populate it
      # in a loop if you have many similar fields.
    }
```

The `predict()` function would return, for each example, additional dict entries
corresponding to each of these fields.

Note: Because tokenization is often tightly coupled with the model code, we
treat it as an intermediate state on the same level as embeddings or attention,
and thus return `Tokens` as a field in the model *output*. This also allows
models to expose different tokenizations for different inputs, such as
`premise_tokens` and `hypothesis_tokens` above.

LIT components and frontend modules will automatically detect these spec fields
and use them to support additional interpretation methods, such as the embedding
projector or gradient-based salience maps.

You can also implement multi-headed models this way: simply add additional
output fields for each prediction (such as another `MulticlassPreds`), and
they'll be automatically detected.

See the [type system documentation](#type-system) for more details on available
types and their semantics.

### Optional inputs

By default, LIT treats `input_spec` fields as required. However, this can be set
to false if you wish to define optional model inputs. For example, a model that
can accept pre-tokenized inputs might have the following spec:

```python
    def input_spec(self) -> types.Spec:
      return {
          "text": lit_types.TextSegment(),
          "tokens": lit_types.Tokens(parent='text', required=False),
      }
```

And in the model's `predict()`, you would have logic to use these and bypass the
tokenizer:

```python
    def predict(self, inputs: Iterable[Input]) -> Iterable[Preds]:
      input_tokens = [ex.get('tokens') or self.tokenizer.tokenize(ex['text'])
                      for ex in inputs]
      # ...rest of your predict logic...
```

`required=False` can also be used for label fields (such as `"label":
lit_types.CategoryLabel(required=False)`), though these can also be omitted from
the input spec entirely if they are not needed to compute model outputs.

## Interpretation Components

Backend interpretation components include metrics, salience maps, visualization
aids like [UMAP](https://umap-learn.readthedocs.io/en/latest/), and
counterfactual generator plug-ins.

Most such components implement the
[`Interpreter`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/api/components.py) API.
Conceptually, this is any function that takes a set of datapoints and a model,
and produces some output.[^identity-component] For example,
[local gradient-based salience (GradientNorm)](https://github.com/PAIR-code/lit/blob/main/lit_nlp/components/gradient_maps.py)
processes the `TokenGradients` and `Tokens` returned by a model and produces a
list of scores for each token. The Integrated Gradients saliency method
additionally requires a `TokenEmbeddings` input and corresponding output, as
well as a label field `Target` to pin the gradient target to the same class as
an input and corresponding output. See the
[GLUE models class](https://github.com/PAIR-code/lit/blob/main/lit_nlp/examples/glue/models.py)
for an example of these spec requirements.

The core API involves implementing the `run()` method:

```python
  def run(self,
          inputs: list[JsonDict],
          model: lit_model.Model,
          dataset: lit_dataset.Dataset,
          model_outputs: Optional[list[JsonDict]] = None,
          config: Optional[JsonDict] = None):
    # config is any runtime options to this component, such as a threshold for
    # (binary) classification metrics.
```

Output from an interpreter component is unconstrained; it's up to the frontend
component requesting it to process the output correctly. In particular, some
components (such as salience maps) may operate on each example independently,
similar to model predictions, while others (such as metrics) may produce
aggregate summaries of the input set.

Interpreters are also responsible for verifying compatibility by reading the
model and dataset specs; these are also used to determine what fields to operate
on. A typical implementation just loops over the relevant specs. For example,
for
[simple gradient-based salience](https://github.com/PAIR-code/lit/blob/main/lit_nlp/components/gradient_maps.py)
we might have:

```python
  def find_fields(self, output_spec: Spec) -> list[str]:
    # Find TokenGradients fields
    grad_fields = utils.find_spec_keys(output_spec, types.TokenGradients)

    # Check that these are aligned to Tokens fields
    for f in grad_fields:
      tokens_field = output_spec[f].align  # pytype: disable=attribute-error
      assert tokens_field in output_spec
      assert isinstance(output_spec[tokens_field], types.Tokens)
    return grad_fields

  def run(self,
          inputs: list[JsonDict],
          model: lit_model.Model,
          dataset: lit_dataset.Dataset,
          model_outputs: Optional[list[JsonDict]] = None,
          config: Optional[JsonDict] = None) -> Optional[list[JsonDict]]:
    """Run this component, given a model and input(s)."""
    # Find gradient fields to interpret
    output_spec = model.output_spec()
    grad_fields = self.find_fields(output_spec)
    logging.info('Found fields for gradient attribution: %s', str(grad_fields))
    if len(grad_fields) == 0:  # pylint: disable=g-explicit-length-test
      return None

    # do rest of the work to create the salience maps for each available field

    # return a dtypes.TokenSalience for each input, which has a list of
    # tokens (from the model) and their associated scores.
```

This design adds some code overhead to interpretation components, but the
benefit is flexibility - Python can be used to specify complex dependencies
between fields, and multiple outputs can be easily supported in a loop.

[^identity-component]: A trivial one might just run the model and return
    predictions, though in practice we have a separate
    endpoint for that.

### Metrics

For metrics, the
[`SimpleMetrics`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/components/metrics.py)
class implements the spec-matching and input-unpacking logic to satisfy the
general `Interpreter` API. A subclass of `SimpleMetrics` should implement an
`is_compatible()` method and a `compute()` method, which is called on compatible
(prediction, label) pairs and returns a dict of named score fields. For example:

```python
class RegressionMetrics(SimpleMetrics):
  """Standard regression metrics."""

  def is_compatible(self, field_spec: types.LitType) -> bool:
    """Return true if compatible with this field."""
    return isinstance(field_spec, types.RegressionScore)

  def compute(self,
              labels: Sequence[float],
              preds: Sequence[float],
              label_spec: types.Scalar,
              pred_spec: types.RegressionScore,
              config: Optional[JsonDict] = None) -> dict[str, float]:
    """Compute metric(s) between labels and predictions."""
    del config
    mse = sklearn_metrics.mean_squared_error(labels, preds)
    pearsonr = scipy_stats.pearsonr(labels, preds)[0]
    spearmanr = scipy_stats.spearmanr(labels, preds)[0]
    return {'mse': mse, 'pearsonr': pearsonr, 'spearmanr': spearmanr}
```

The implementation of `SimpleMetrics.run()` uses the `parent` key (see
[type system](#type-system)) in fields of the model's output spec to find the
appropriate input fields to compare against, and calls `compute()` accordingly
on the unpacked values.

### Generators

Conceptually, a generator is just an interpreter that returns new input
examples. These may depend on the input only, as for techniques such as back-
translation, or can involve feedback from the model, such as for adversarial
attacks.

The core generator API is:

```python
class Generator(Interpreter):
  """Base class for LIT generators."""

  def generate_all(self,
                   inputs: list[JsonDict],
                   model: lit_model.Model,
                   dataset: lit_dataset.Dataset,
                   config: Optional[JsonDict] = None) -> list[list[JsonDict]]:
    """Run generation on a set of inputs.

    Args:
      inputs: sequence of inputs, following model.input_spec()
      model: optional model to use to generate new examples.
      dataset: optional dataset which the current examples belong to.
      config: optional runtime config.

    Returns:
      list of list of new generated inputs, following model.input_spec()
    """
```

Where the output is a list of lists: a set of generated examples for each input.
For convenience, there is also a `generate()` method which takes a single
example and returns a single list; we provide the more general `generate_all()`
API to support model-based generators (such as back-translation) which benefit
from batched requests.

As with other interpreter components, a generator can take custom arguments
through `config`, such as the list of substitutions for the
[word replacer](https://github.com/PAIR-code/lit/blob/main/lit_nlp/components/word_replacer.py).

#### Backtranslator Generator

The [backtranslator](https://github.com/PAIR-code/lit/blob/main/lit_nlp/components/backtranslator.py)
generator translates text segment inputs into foreign languages and back to the
source language in order to create paraphrases.
It relies on the Google Cloud Translate API to perform those translations.
To use it, you must have a Google Cloud project and set up Cloud Translation
as described at https://cloud.google.com/translate/docs/setup.
Then, download  your application credentials file locally and set the
GOOGLE_APPLICATION_CREDENTIALS environment variable to point to that file.
With that environment variable set to the correct path, LIT can make use of the
backtranlator generator if you pass it as a generator in the Server constructor.

### Configuration UI

Interpreter components support an optional `config` option to specify run-time
options, such as the number of samples for LIME or the pivot languages for
back-translation. LIT provides a simple DSL to define these options, which will
auto-generate a form on the frontend. The DSL uses the same
[type system](#type-system) as used to define data and model outputs, and the
`config` argument will be passed a dict with the form values.

For example, the following spec:

```python
  def config_spec(self) -> types.Spec:
    return {
        "Pivot languages": types.SparseMultilabel(
            vocab=['ar', 'bg', 'de', 'el', 'en', 'es', 'fr', 'hi', 'ru', 'sw',
                   'th', 'tr', 'ur', 'vi', 'zh'],
            default=['de', 'fr']),
        "Source language": types.TextSegment(default='en'),
    }
```

will give this form to configure back-translation:

![Back-translation Config Form](./images/api/backtranslation-form-example.png){w=400px align=center}

Currently `config_spec()` is supported only for generators and salience methods,
though any component can support the `config` argument to its `run()` method,
which can be useful if
[running outside of the LIT UI](#using-lit-components-outside-of-lit).

The following [types](#available-types) are supported (see
[interpreter_controls.ts](https://github.com/PAIR-code/lit/blob/main/lit_nlp/client/elements/interpreter_controls.ts)):

*   `Scalar`, which creates a slider for setting a numeric option. You can
    specify the `min_val`, `max_val`, `default`, and `step`, values for the
    slider through arguments to the `Scalar` constructor.
*   `Boolean` (`BooleanLitType` in TypeScript), which creates a checkbox, with
    a `default` value to be set in the constructor.
*   `CategoryLabel`, which creates a dropdown with options specified in the
    `vocab` argument.
*   `SparseMultilabel`, which creates a series of checkboxes for each option
    specified in the `vocab` argument.
*   `TextSegment`, which creates an input text box for string entry, with an
    optional default value from the `default` argument.
*   `Tokens`, which creates an input text box for entry of multiple,
    comma-separated strings which are parsed into a list of strings to be
    supplied to the interpreter.
*   `SingleFieldMatcher`, which acts like a `CategoryLabel` but where the vocab
    is automatically populated by the names of fields from the data or model
    spec. For example, `SingleFieldMatcher(spec='dataset',
    types=['TextSegment'])` will give a dropdown with the names of all
    `TextSegment` fields in the dataset.
*   `MultiFieldMatcher` is similar to `SingleFieldMatcher` except it gives a set
    of checkboxes to select one or more matching field names. The returned value
    in `config` will be a list of string values.

The field matching controls can be useful for selecting one or more fields to
operate on. For example,to choose which input fields to perturb, or which output
field of a multi-head model to run an adversarial attack (such as HotFlip)
against.

## Type System

LIT passes data around (e.g., between the server and the web app) as flat
records with `string` keys. In Python types these are `Mapping[str, ...]` and in
TypeScript types these are `{[key: string]: unknown}`. LIT serializes these
records to JSON when communicating between the server and the web app client. It
is because of this serialization that we introduced LIT's type system; LIT needs
a way to communicate how to process and understand the semantics of the _shape_
and (allowable) _values_ for the records being passed around in
[JSON's more limited type system][json].

<!--
  TODO(b/290782213): Update the serialization discussion above to reflect any
  changes to LIT's wrire format for HTTP APIs.
-->

<!--
  TODO(b/258531316): Update Spec type once converted to a readonly type
-->
The _shape_ of a record &ndash; its specific keys and the types of their values
&ndash; is defined by a `Spec`; a `dict[str, LitType]`. Each `LitType` class has
a `default` property whose type annotation describes the type of the _value_ for
that field in a JSON record. `LitType`s are implemented using hierarchical
inheritance; the canonical types can be found in [types.py][types_py], with
parallel implementations in [lit_types.ts][types_ts].

### Conventions

LIT supports several different "kinds" of `Spec`s (input vs output vs meta,
etc.), and their use in context has specific implications, described
per base class below.

* [`lit_nlp.api.dataset.Dataset`][dataset-py]
    * **`.spec() -> Spec`** describes the shape of every record in the
      `Sequence` returned by `Dataset.examples()`.
    * **`.init_spec() -> Optional[Spec]`** describes the user-configurable
      arguments for loading a new instance of this `Dataset` class via the web
      app's UI. Returning `None` or an empty `Spec` means that there is nothing
      configurable about how this `Dataset` is loaded, and it will not show up
      in the dataset loading section of the web app's Global Settings.
* [`lit_nlp.api.model.Model`][model-py]
    * **`.input_spec() -> Spec`** describes the shape required of all records
      passed into the `Model.predict()` function via the `inputs` argument. LIT
      checks for compatibility between a `Dataset` and a `Model` by ensuring
      that `Model.input_spec()` is a subset of `Dataset.spec()`.
    * **`.output_spec() -> Spec`** describes the shape of all records returned
      by the `Model.predict()` function.
    * **`.init_spec() -> Optional[Spec]`** describes the user-configurable
      arguments for loading a new instance of this `Model` class via the web
      app's UI. Returning `None` or an empty `Spec` means that there is nothing
      configurable about how this `model` is loaded, and it will not show up in
      the model loading section of the web app's Global Settings.
* [`lit_nlp.api.components.[Interpreter | Generator]`][components-py]
    * **`.config_spec() -> Spec`** describes the user-configurable parameters
      for running this component. Returning an empty `Spec` means that this
      component always processes inputs in the same way.
    * **`.meta_spec() -> Spec`** is essentially unconstrained, but ideally
      describes the shape of the records returned by this component's `.run()`
      method. Note that this `Spec` has different semantics depending on the
      component type. `Interpreter.run()` typically returns an
      `Iterable[Mapping[str, ...]]` of records (i.e., the `Mapping`) with this
      shape, because each input corresponds to one interpretation. Whereas
      `Generator.run()` typically returns an
      `Iterable[Iterable[Mapping[str, ...]]]` of records with this shape,
      because each input may enable the generation of one or more new examples.
* [`lit_nlp.api.components.Metrics`][components-py]
    * **`.config_spec() -> Spec`** describes the user-configurable parameters
      for running this component. Returning an empty `Spec` means that this
      component always processes inputs in the same way.
    * **`.meta_spec() -> Spec`** is a slight variation on the tradition `Spec`;
      it will always be a `Mapping[str, MetricResult]` describing the single
      record returned by the `Metrics.run()` method for each pair of compatible
      keys in the `Model.output_spec()` and `Dataset.spec()`. The `MetricResult`
      type also describes how to interpret the values in each record, e.g., if
      higher, lower, or numbers closer to zero are better.

Each `LitType` subclass encapsulates its own semantics (see
[types.py][types_py]), but there are a few conventions all subclasses follow:

*   The **`align=` attribute** references another field _in the same spec_ and
    implies that both fields have index-aligned elements. For
    example, `Model.output_spec()` may contain `'tokens': lit_types.Tokens(...)`
    and `'pos': lit_types.SequenceTags(align='tokens')`, which references the
    "tokens" field. This implies that the "pos" field contains a corresponding
    value for every item in "tokens" and that you can access them with numeric
    indices. Transitively, this means that using `zip(..., strict=True)` (in
    Python 3.10 and above) will act as a pseudo-validator of this expectation.

*   The **`parent=` attribute** is _typically_ used by `LitType`s in a
    `Model.output_spec()`, and must be a field in the _input spec_ (i.e. the
    `Dataset.spec()`) against which this field's value will be compared. For
    example, the `Model.output_spec()` may contain
    `'probas': lit_types.MulticlassPreds(parent='label', ...)` and the
    `Dataset.spec()` may contain `'label': lit_types.CategoryLabel()`, which
    means that the `Dataset`'s "label" field contains the ground truth values
    for that example, and the class prediction in the "probas" field can be
    compared to this label, e.g., by multi-class metrics.

*   The **`vocab=` attribute** is used to represent the allowable values for
    that field, such as a set of classes for a `MulticlassPreds` field, or the
    set of labels for a `CategoryLabel` field.

*   A field that appears in _both_ the model's input and output specs is assumed
    to represent the same value. This pattern is used for model-based input
    manipulation. For example, a
    [language model](https://github.com/PAIR-code/lit/blob/main/lit_nlp/examples/glue/models.py)
    might output `'tokens': lit_types.Tokens(...)`, and accept as (optional)
    input `'tokens': lit_types.Tokens(required=False, ...)`. An interpretability
    component could take output from the former, swap one or more tokens (e.g.
    with `[MASK]`), and feed them in the corresponding input field to compute
    masked fills.

### Compatibility Checks

LIT's type system plays a critical role in ensuring reliability of and
interoperability between the `Model`, `Dataset`, `Interpreter`, `Generator`, and
`Metrics` classes:

*   The **Model-Dataset compatibility check** ensures that the
    `Model.input_spec()` is a subset of the `Dataset.spec()`. The base
    [`Model` class][model-py] provides a robust and universal implementation of
    this check in the `is_compatible_with_dataset()` API, but you can override
    this method in your `Model` subclass if you so choose.
*   All [`lit_nlp.api.components` classes][components-py] provide an
    `is_compatible` API to check their compatibility against `Model`s and
    `Dataset`s, as appropriate. For example, the
    [`WordReplacer` generator][word-replacer] only checks against the `Dataset`
    spec because it does not depend on model outputs, whereas the
    [`Curves` interpreter][curves-interp] checks the `Model` and `Dataset`
    because it needs labeled predictions, and the
    [`GradientDotInput` interpreter][grad-maps] only checks against the
    `Model.output_spec()` because it needs data that only the model can provide.

The LIT web app also uses `Spec` based compatibility checks. Each TypeScript
module defines a [`shouldDisplayModule` function][should_display_module] that
returns `true` if any active model-dataset pair provides sufficient information
to support the visualization methods encapsulated by that module. If this
function returns `false`, the module is not displayed in the layout. Note that
this can cause jitter (UI modules appearing, disappearing, reordering, resizing,
etc.) when switching between models or datasets with heterogeneous `Spec`s.

When implementing your own LIT components and modules, you can use
[`utils.find_spec_keys()`][utils-lib-py] (Python) and
[`findSpecKeys()`][utils-lib] (TypeScript) to identify fields of interest in a
`Spec`. These methods recognize and respect subclasses. For example,
`utils.find_spec_keys(spec, Scalar)` will also match any `RegressionScore`
fields, but `utils.find_spec_keys(spec, RegressionScore)` will not return all
`Scalar` fields in the `Spec`.

Important: Compatibility checks are performed automatically when
[building the `LitMetadata`][build-metadata] for an instance of `LitApp`,
typically by calling `dev_server.Serve()`. **These checks are not performed when
using components in a raw Python context** (e.g., Colab, Jupyter, a REPL), as
[described below](#using-lit-components-outside-of-lit), and it is encouraged
that you call these explicitly to ensure compatibility and avoid chasing red
herrings.

### An In-Depth Example

Consider the following example from the [MNLI demo][mnli-demo]. The
[MultiNLI][mnli-dataset] dataset might define the following `Spec`.

```python
# Dataset.spec()
{
  "premise": lit_types.TextSegment(),
  "hypothesis": lit_types.TextSegment(),
  "label": lit_types.CategoryLabel(
      vocab=["entailment", "neutral", "contradiction"]
  ),
  "genre": lit_types.CategoryLabel(),
}
```

An example record in this `Dataset` might be:

```python
# dataset.examples[0]
{
  "premise": "Buffet and a la carte available.",
  "hypothesis": "It has a buffet."
  "label": "entailment",
  "genre": "travel",
}
```

A classification model for this task might have the following `input_spec()` and
`output_spec()`. Notice that the input spec is a subset of the `Dataset.spec()`,
thus LIT considers these to be compatible.

```python
# model.input_spec()
{
  "premise": lit_types.TextSegment(),
  "hypothesis": lit_types.TextSegment(),
}

# model.output_spec()
{
  "probas": lit_types.MulticlassPreds(
      parent="label",
      vocab=["entailment", "neutral", "contradiction"]
  ),
}
```

Running this model over the input might yield the following prediction.

```python
# model.predict([dataset.examples[0]])[0]
{
  "probas": [0.967, 0.024, 0.009],
}
```

Passing this input and the prediction to the `ClassificationResults` interpreter
would yield additional human-readable information as follows.

```python
# classification_results.run(
#     dataset.examples[:1], model, dataset, [prediction]
# )[0]
{
  "probas": {
      "scores": [0.967, 0.024, 0.009],
      "predicted_class": "entailment",
      "correct": True,
  },
}
```

_See the [examples](https://github.com/PAIR-code/lit/blob/main/lit_nlp/examples) for more._

### Available types

The full set of `LitType`s is defined in
[types.py](https://github.com/PAIR-code/lit/blob/main/lit_nlp/api/types.py). Numeric types
such as `Integer` and `Scalar` have predefined ranges that can be overridden
using corresponding `min_val` and `max_val` attributes as seen in
[penguin data](https://github.com/PAIR-code/lit/blob/main/lit_nlp/examples/penguin/data.py)
`INPUT_SPEC`. The different types available in LIT are summarized in the table
below.

Note: Bracket syntax, such as `<float>[num_tokens]`, refers to the shapes of
NumPy arrays where each element inside the brackets is an integer.

Name                      | Description                                                                                                                                                           | Value Type
------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------
`TextSegment`             | Natural language text, untokenized.                                                                                                                                   | `str`
`GeneratedText`           | Untokenized text, generated from a model (such as seq2seq).                                                                                                           | `str`
`URL`                     | TextSegment, but interpreted as a URL.                                                                                                                                | `str`
`GeneratedURL`            | Generated TextSegment, but interpreted as a URL (i.e., it maye not be real/is inappropriate as a label).                                                              | `str`
`SearchQuery`             | TextSegment, but interpreted as a search query.                                                                                                                       | `str`
`String`                  | Opaque string data; ignored by components such as perturbation methods that operate on natural language.                                                              | `str`
`ReferenceTexts`          | Multiple texts, such as a set of references for summarization or MT.                                                                                                  | `list[tuple[str, float]]`
`GeneratedTextCandidates` | Multiple generation candidates, such as beam search output from a seq2seq model.                                                                                      | `list[tuple[str, float]]`
`Tokens`                  | Tokenized text.                                                                                                                                                       | `list[str]`
`TokenTopKPreds`          | Predicted tokens and their scores, as from a language model or seq2seq model.                                                                                         | `list[list[tuple[str, float]]]`
`Boolean`                 | Boolean value.                                                                                                                                                        | `bool`
`Scalar`                  | Scalar numeric value.                                                                                                                                                 | `float`
`Integer`                 | Integer, with a default range from -32768 to +32767. value.                                                                                                                                                        | `int`
`ImageBytes`              | Image, represented by a base64 encoded string. LIT also provides `JPEGBytes` and `PNGBytes` types for those specific encodings.                                       | `str`
`RegressionScore`         | Scalar value, treated as a regression target or prediction.                                                                                                           | `float`
`ReferenceScores`         | Scores for one or more reference texts.                                                                                                                               | `list[float]`
`CategoryLabel`           | Categorical label, from open or fixed vocabulary.                                                                                                                     | `str`
`MulticlassPreds`         | Multiclass predicted probabilities.                                                                                                                                   | `<float>[num_labels]`
`SparseMultilabel`        | Multiple non-exclusive labels, such as a set of attributes.                                                                                                           | `list[str]`
`SparseMultilabelPreds`   | Sparse multi-label predictions, represented as scored candidates.                                                                                                     | `list[tuple[str, float]]`
`SequenceTags`            | Sequence tags, aligned to tokens.                                                                                                                                     | `list[str]`
`SpanLabels`              | Span labels, aligned to tokens. Each label is (i,j,label).                                                                                                            | `list[SpanLabel]`
`EdgeLabels`              | Edge labels, aligned to tokens. This is a general way to represent many structured prediction tasks, such as coreference or SRL. See https://arxiv.org/abs/1905.06316 | `list[EdgeLabel]`
`MultiSegmentAnnotations` | In-line byte-span annotations, which can span multiple text segments.                                                                                                 | `list[AnnotationCluster]`
`Embeddings`              | Fixed-length embeddings or model activations.                                                                                                                         | `<float>[emb_dim]`
`Gradients`               | Gradients with respect to embeddings or model activations.                                                                                                            | `<float>[emb_dim]`
`TokenEmbeddings`         | Per-token embeddings or model activations.                                                                                                                            | `<float>[num_tokens, emb_dim]`
`TokenGradients`          | Gradients with respect to per-token embeddings or model activations.                                                                                                  | `<float>[num_tokens, emb_dim]`
`ImageGradients`          | Gradients with respect to image pixels.                                                                                                                               | `<float>[image_height, image_width, color_channels]`
`AttentionHeads`          | Attention heads, grouped by layer.                                                                                                                                    | `<float>[num_heads, num_tokens, num_tokens]`

Values can be plain data, NumPy arrays, or custom dataclasses - see
[dtypes.py](https://github.com/PAIR-code/lit/blob/main/lit_nlp/api/dtypes.py) for further
detail.

*Note: Note that `String`, `Boolean` and `URL` types in Python are represented
as `StringLitType`, `BooleanLitType` and `URLLitType` in TypeScript to avoid
naming collisions with protected TypeScript keywords.*

## Server Configuration

Some properties of the LIT frontend can be configured from Python as
**arguments to `dev_server.Server()`**. These include:

*   `page_title`: set a custom page title.
*   `canonical_url`: set a "canonical" URL (such as a shortlink) that will be
    used as the base when copying links from the LIT UI.
*   `default_layout`: set the default UI layout, by name. See `layout.ts` and
    the section below for available layouts.
*   `demo_mode`: demo / kiosk mode, which disables some functionality (such as
    save/load datapoints) which you may not want to expose to untrusted users.
*   `inline_doc`: a markdown string that will be rendered in a documentation
    module in the main LIT panel.
*   `onboard_start_doc`: a markdown string that will be rendered as the first
    panel of the LIT onboarding splash-screen.
*   `onboard_end_doc`: a markdown string that will be rendered as the last
    panel of the LIT onboarding splash-screen.

For detailed documentation, see
[server_flags.py](https://github.com/PAIR-code/lit/blob/main/lit_nlp/server_flags.py).

Most Python components (such as `Model`, `Dataset`, and `Interpreter`) also have
a `description()` method which can be used to specify a human-readable
description or help text that will appear in the UI.

### Customizing the Layout

You can specify custom web app layouts from Python via the `layouts=` attribute.
The value should be a `Mapping[str, LitCanonicalLayout]`, such as:

```python
PENGUIN_LAYOUT = layout.LitCanonicalLayout(
    upper={
        'Main': [
            modules.DiveModule,
            modules.DataTableModule,
            modules.DatapointEditorModule,
        ]
    },
    lower=layout.STANDARD_LAYOUT.lower,
    description='Custom layout for the Palmer Penguins demo.',
)
```

You can pass this to the server as:

```python
lit_demo = dev_server.Server(
    models,
    datasets,
    # other args...
    layouts=layout.DEFAULT_LAYOUTS | {'penguins': PENGUIN_LAYOUT},
    default_layout='penguins',
    **server_flags.get_flags())
return lit_demo.serve()
```

You can see the pre-configured layouts provided by LIT, as well as the list of
modules that can be included in your custom layout in
[`layout.py`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/api/layout.py). A
`LitCanonicalLayout` can be defined to achieve four different configurations of
the major content areas:

* Single-panel: Define only the `upper=` parameter.
* Two-panel, upper/lower: Define the `upper=` and `lower=` parameters.
* Two-panel, left/right: Define the `left=` and `upper=` parameters; the
  `upper=` section will be shown on the right.
* Three-panel: Define the `left=`, `upper=`, and `lower=` parameters; the
  `upper=` and `lower=` sections will be shown on the right.

To use a specific layout by default for a given LIT instance, pass the key
(e.g., "simple", "default", or the name of a custom layout) as a server flag
when initializing LIT (`--default_layout=<layout>`) or by setting the default
value for that flag in you `server.py` file, e.g.,
`flags.FLAGS.set_default('default_layout', 'my_layout_name')`. The layout can
also be set on-the-fly with the `layout=` URL param, which will take precedence.

Note: The pre-configured layouts are added to every `LitApp` instance using
[dictionary comprehension](https://docs.python.org/3/library/stdtypes.html#dict)
where the Mapping passed to the `LitApp` constructor overrides the
pre-configured layouts `Mapping`. Thus, you can remove or change these
pre-configured layouts as you like by passing a `Mapping` where the values of
`simple`, `default`, and/or `experimental` is `None` (to remove) or a
`LitCanonicalLayout` instance (to override) as you desire.

## Accessing the LIT UI in Notebooks

As an alternative to running a LIT server and connecting to it through a web
browser, LIT can be used directly inside of python notebook environments, such
as [Colab](https://colab.research.google.com/) and
[Jupyter](https://jupyter.org/).

After installing LIT through pip, create a `lit_nlp.notebook.LitWidget` object,
passing in a dict of models and a dict of datasets, similar to the
`lit_nlp.dev_server.Server` constructor. You can optionally provide a height
parameter that specifies the height in pixels to render the LIT UI.

Then, in its own output cell, call the `render` method on the widget object to
render the LIT UI. The LIT UI can be rendered in multiple cells if desired. The
LIT UI can also be rendered in its own browser tab, outside of the notebook, by
passing the parameter `open_in_new_tab=True` to the `render` method. The
`render` method can optionally take in a configuration object to specify
certain options to render the LIT UI using, such as the selected layout,
current display tab, dataset, and models. See
[notebook.py](https://github.com/PAIR-code/lit/blob/main/lit_nlp/notebook.py) for details.

The widget has a `stop` method which shuts down the widget's server. This can be
important for freeing up resources if you plan to create multiple LIT widget
instances in a single notebook. Stopping the server doesn't disable the model
and dataset instances used by the server; they can still be used in the notebook
and take up the resources they require.

Check out an
[example notebook](https://colab.research.google.com/github/pair-code/lit/blob/main/lit_nlp/examples/notebooks/LIT_sentiment_classifier.ipynb).

## Using LIT components outside of LIT

All LIT Python components (models, datasets, interpreters, metrics, generators,
etc.) are standalone classes that do not depend on the serving framework. You
can easily use them from Colab, in scripts, or in your libraries. This can
also be handy for development, as you can test new models or components without
needing to reload the server or click the UI.

For example, to view examples in a dataset:

```python
from lit_nlp.examples.glue import data as glue_data
dataset = glue_data.SST2Data('validation')
print(dataset.examples)  # list of records {"sentence": ..., "label": ...}
```

And to run inference on a few of them:

```python
from lit_nlp.examples.glue import models as glue_models

model = glue_models.SST2Model("/path/to/model/files")
preds = list(model.predict(dataset.examples[:5]))
# will return records {"probas": ..., "cls_emb": ..., ...} for each input
```

Or to compute input salience using
[LIME](https://homes.cs.washington.edu/~marcotcr/blog/lime/):

```python
from lit_nlp.components import lime_explainer

lime = lime_explainer.LIME()
lime.run([dataset.examples[0]], model, dataset)
# will return {"tokens": ..., "salience": ...} for each example given
```

For a full working example in Colab, see [LIT_components_example.ipynb](https://colab.research.google.com/github/pair-code/lit/blob/dev/lit_nlp/examples/notebooks/LIT_components_example.ipynb).


<!-- Links -->

[build-metadata]: https://github.com/PAIR-code/lit/blob/main/lit_nlp/app.py
[components-py]: https://github.com/PAIR-code/lit/blob/main/lit_nlp/api/components.py
[curves-interp]: https://github.com/PAIR-code/lit/blob/main/lit_nlp/components/curves.py
[dataset-py]: https://github.com/PAIR-code/lit/blob/main/lit_nlp/api/dataset.py
[grad-maps]: https://github.com/PAIR-code/lit/blob/main/lit_nlp/components/gradient_maps.py
[json]: https://www.json.org
[mnli-dataset]: https://cims.nyu.edu/~sbowman/multinli/

[mnli-demo]: https://pair-code.github.io/lit/demos/glue.html

[model-py]: https://github.com/PAIR-code/lit/blob/main/lit_nlp/api/model.py
[should_display_module]: https://github.com/PAIR-code/lit/blob/main/lit_nlp/client/core/lit_module.ts
[types_py]: https://github.com/PAIR-code/lit/blob/main/lit_nlp/api/types.py
[types_ts]: https://github.com/PAIR-code/lit/blob/main/lit_nlp/client/lib/lit_types.ts
[utils-lib]: https://github.com/PAIR-code/lit/blob/main/lit_nlp/client/lib/utils.ts
[utils-lib-py]: https://github.com/PAIR-code/lit/blob/main/lit_nlp/lib/utils.py
[word-replacer]: https://github.com/PAIR-code/lit/blob/main/lit_nlp/components/word_replacer.py

================
File: website/sphinx_src/components.md
================
# Components and Features

<!--* freshness: { owner: 'lit-dev' reviewed: '2024-06-24' } *-->

<!-- [TOC] placeholder - DO NOT REMOVE -->

## Framework and Model Support

LIT is framework-agnostic and is compatible with any model that can be wrapped
in a Python class for inference. In particular, we've tested with TF1.x, TF2,
JAX, and PyTorch, as well as models that use custom C++ inference code (wrapped
via CLIF) and with remote models over RPC. In general, there aren't any
constraints beyond those imposed by the modeling platform (for example, TF1.x
and TF2 models can't coexist in the same process) or underlying hardware (such
as GPU memory). For working with very large models, also see the
[Scale section of the FAQ](./faq.md#scale).

Many LIT users implement their own
[model and dataset classes](./api.md#adding-models-and-data), but we also have
out-of-the-box support for a few modeling frameworks, described below.

### HuggingFace Transformers

Many of the
[open-source LIT examples](https://github.com/PAIR-code/lit/blob/main/lit_nlp/examples/) use
HuggingFace Transformers via their TF2/Keras model classes. These give easy
access to model internals such as embeddings, attention, and gradients, and the
LIT wrappers for these support many interpretability methods - such as
[integrated gradients](https://arxiv.org/abs/1703.01365) out-of-the-box.

These models are a great place to start for small-scale experiments or for
working on academic projects.

### TF1.x Estimator

LIT supports Estimator and other TF1.x models, but the model wrappers can be
more involved due to the need to explicitly manage the graph and sessions. (In
particular: `Estimator.predict()` cannot be used because it reloads the model on
every invocation.) Generally, you'll need to:

*   In your model's `__init__()`, build the graph, create a persistent TF
    session, and load the model weights.
*   In your `predict()` function, build a feed dict and call `session.run`
    directly.

Alternatively, you can export to a `SavedModel` and load this in an eager mode
runtime. This leads to much simpler code, but may require changes to your
`SavedModel` exporter in order to access model internals like embeddings,
gradients, or attention.

### Remote or hosted models

LIT can easily interact with models hosted via an RPC or HTTP endpoint,
including Servomatic. In this usage, the model weights and computation remain on
the server, while the LIT `Model` implementation simply manages the RPC stub and
handles format conversion and any additional pre- or post-processing.

*   For a general-purpose interface to connect to another LIT server over HTTP,
    see [components/remote_model.py](https://github.com/PAIR-code/lit/blob/main/lit_nlp/components/remote_model.py).

### Static predictions

LIT works best when the model can be queried interactively, but this isn't
always possible for all situations. The
[`StaticPredictions`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/components/static_preds.py)
class allows LIT to serve a set of pre-computed predictions by creating a
"model" backed by a lookup table. This can be handy for quickly browsing data,
while still retaining access to LIT's rich visualizations.

Note: `StaticPredictions` does not support counterfactuals or any methods such
as LIME which involve querying the model on new examples.

### Data loading

LIT can load data from almost any format, including TFRecord, Capacitor,
SSTable, or even SQL queries, via a custom Python class that implements the
[Dataset API](./api.md#datasets). Many of our demos use TFDS, and the LIT loader
code is only a small wrapper class which performs minimal post-processing. See
the [demos page](./demos.md) for specific examples.

Datasets can also be loaded or saved from the UI, if the appropriate methods are
implemented on the dataset class. See the
[Workflow and Integrations FAQ](./faq.md#workflow-and-integrations) for more
details.

## Input and Output Types

LIT uses an extensible system of semantic types to describe data and models.
This allows for flexible support of a number of different input and output
modalities common to NLP and other domains. For a full reference, see the
documentation of the [Python API](api.md), in particular:

*   [Adding models and data](api.md#adding-models-and-data)
*   [Type system](api.md#type-system)

Below, we describe a few common model types and usage patterns. Note that while
some features (such as metrics and output visualizations) are specific to an
output type, many LIT features - such as datapoint exploration, side-by-side
functionality, and counterfactual generation - are available for any model.

### Classification

LIT supports many features for classification tasks, including common metrics,
confusion matrices, and custom thresholding via the UI. Classification is
implemented with the `MulticlassPreds` and `CategoryLabel` types.

*   Models should define a `MulticlassPreds` field in their output spec with the
    `vocab=` attribute as the set of class labels, and for each example should
    return a vector of probabilities for each class.
*   To provide labels for evaluation, the data should define a `CategoryLabel`
    field which contains string-valued labels. The model's `MulticlassPreds`
    field should set the `parent=` attribute to the name of this field.
*   A negative class can be designated using the `null_idx` attribute of
    `MulticlassPreds` (most commonly, `null_idx=0`), and metrics such as
    precision, recall, F1 will be computed for the remaining classes. AUC and
    AUCPR will be computed for binary classification tasks.
*   If `null_idx` is set and there is only one other class, the other class
    (often, class `1`) is treated as a positive class, and the LIT UI can be
    used to change the classification threshold. If `null_idx` is set and there
    are more than two classes, a "margin" can be set which acts as a bias (in
    log space) for the negative class.

![Classification Results Module](images/components/classification-results.png){w=600px align=center}

### Regression / Scoring

Regression or scoring models also are well-supported with metrics, bucketed
faceting, and scatterplots of scalar output. Regression is implemented with the
`Scalar` (input) and `RegressionScore` (output) types.

*   Models should define a `RegressionScore` field in their output spec, and for
    each example should return a numerical score.
*   To provide labels for evaluation, the data should define a `Scalar` field
    which contains numerical targets, and the model's `RegressionScore` field
    should set `parent=` to the name of this field.
*   For an example, see the STS-B textual similarity task in
    [examples/glue/demo.py](https://github.com/PAIR-code/lit/blob/main/lit_nlp/examples/glue/demo.py).

### Multi-label classification

LIT supports multi-label tasks, when a model can label a single example with
more than one label. Multi-label classification is implemented with the
`SparseMultilabelPreds` and `SparseMultilabel` types.

*   Models should define a `SparseMultilabelPreds` field in their output spec
    with the`vocab=` attribute as the set of class labels, and for each example
    should return a list of class score tuples. Each tuple contains two
    elements: a string class label and a non-negative numeric score for that
    class.
*   To provide labels for evaluation, the data should define a
    `SparseMultilabel` field which contains a list of string-valued labels. The
    model's `SparseMultilabelPreds` field should set the `parent=` attribute to
    the name of this field.

### Seq2Seq / Generation

LIT has a number of features for sequence generation models, though support is
not quite as mature as for classification or regression. In particular, LIT can
display single generations as well as scored candidates from beam search, and
can highlight diffs against one or more reference texts. If supported by the
model, LIT can also render per-token output probabilities from a language model
or decoder.

*   Models should define a `GeneratedText` field (for single generation) and
    emit a single string per example, or a `GeneratedTextCandidates` field and
    emit multiple candidates and their scores.
*   To provide target sequences for evaluation, the data should include a
    `TextSegment` field (for a single reference) or a `ReferenceTexts` field
    (for multiple references), and the model's output field should set `parent=`
    accordingly.
*   To use a model in scoring mode over one or more predefined target sequences,
    the model can also output a `ReferenceScores` field (with values as
    `list[float]`) with `parent=` set to reference a `TextSegment` or
    `ReferenceTexts` field from the input.

![Generated Text Module](images/components/generation-results.png){w=600px align=center}

### Span Labeling and Structured Prediction

LIT can support a variety of structured prediction types, and provides rich,
interactive visualizations.

*   For token-aligned output, models should define a `Tokens` field in their
    output, and return a list of tokens for each example.
*   For part-of-speech and other per-token tags, models should define a
    `SequenceTags` type with the `align=` attribute set to the name of the
    appropriate `Tokens` field. For each example, they should return a list of
    tags, one per token.
*   For span labels such as named entities (NER), models can use the
    `SpanLabels` type and return tuples (as `dtypes.SpanLabel`) of
    `(i,j,label)`. Similarly, an `EdgeLabel` type is available for tasks such as
    SRL and dependency parsing that consist of relations between two spans.
*   Experimentally, byte-based annotations are supported via the
    `MultiSegmentAnnotations` type.

![Structured Predictions Module](images/components/structured-preds.png){w=400px align=center}

### Multiple input segments

LIT can easily handle multiple text fields, or a mix of text, categorical,
scalar, and other input features. LIT does not explicitly "privilege" one input
field, and metadata in the model spec can be used to align gradients, attention,
and otherwise to different parts of the input.

*   For an example with two-sentence input, see the
    [Dataset class documentation](./api.md#datasets) and the corresponding
    [Model](./api.md#models).
*   For a more involved code example including per-token gradients, see
    [examples/glue/demo.py](https://github.com/PAIR-code/lit/blob/main/lit_nlp/examples/glue/demo.py).

### Tabular data

LIT can be used as a replacement for the [What-If Tool](https://whatif-tool.dev)
but with more extensibility, when working with predictions over tabular data.

Some interpreters, such as Kernel SHAP, require models that use tabular data. In
these cases, LIT validates model compatibility by checking that:

*   The model inputs (`input_spec()`) are exclusively categorical
    (`CategoryLabel`) or numeric (`Scalar`), and none of these are marked as
    optional (`required=False`).
*   The model outputs include at least one classification (`MulticlassPreds`),
    regression (`RegressionScore` or `Scalar`), or multilabel
    (`SparseMultilabel`) field.

For a demo using a penguin stats dataset/binary classification task, see
[examples/penguin/demo.py](https://github.com/PAIR-code/lit/blob/main/lit_nlp/examples/penguin/demo.py).

### Images

LIT also contains support for models with images as input features or generated
images as model output. The LIT type `ImageBytes` can be used as a feature in
datasets and as part of an input spec or output spec for a model. That feature's
value must be a base64 encoded string for an image.

NOTE: We may transition images away from encoded strings, moving to individual
pixel color values. We will ensure we don't break existing checked-in code with
such a change.

## Token-based Salience

LIT supports several methods for token-based input salience, including
gradient-based methods as well as black-box techniques like LIME that don't
require any access to model internals. Output is rendered in the Salience Maps
module in the LIT UI, which allows for comparison of multiple methods at once:

![Salience Map Module](./images/components/salience-map.png){w=600px align=center}

For a demo with a BERT-based classifier, see https://pair-code.github.io/lit/demos/glue.html and navigate to the
"Explanations" tab.

Currently, salience is supported for classification ( `MulticlassPreds`),
regression (`RegressionScore`) and generation (`GeneratedText` or
`GeneratedTextCandidates`) outputs.

### Gradient Norm

This is a simple method, in which salience scores are proportional to the L2
norm of the gradient, i.e. the score for token $i$ is:

$$S(i) \propto ||\nabla_{x_i} \hat{y}||_2$$

To enable this method, your model should, as part of the
[output spec and `predict()` implementation](./api.md#models):

*   Return a `Tokens` field with values (as `list[str]`) containing the
    tokenized input.
*   Return a `TokenGradients` field with the `align` attribute pointing to the
    name of the `Tokens` field (i.e. `align="tokens"`). Values should be arrays
    of shape `<float>[num_tokens, emb_dim]` representing the gradient
    $\nabla_{x} \hat{y}$ of the embeddings with respect to the prediction
    $\hat{y}$.

Because LIT is framework-agnostic, the model code is responsible for performing
the gradient computation and returning the result as a NumPy array. The choice
of $\hat{y}$ is up to the developer; typically for regression/scoring this is
the raw score and for classification this is the score of the predicted (argmax)
class.

### Gradient-dot-Input

In this method, salience scores are proportional to the dot product of the input
embeddings and their gradients, i.e. for token $i$ we compute:

$$S(i) \propto x_i \cdot \nabla_{x_i} \hat{y}$$

Compared to grad-norm, this gives directional scores: a positive score is can be
interpreted as that token having a positive influence on the prediction
$\hat{y}$, while a negative score suggests that the prediction would be
stronger if that token was removed.

To enable this method, your model should, as part of the
[output spec and `predict()` implementation](./api.md#models):

*   Return a `Tokens` field with values (as `list[str]`) containing the
    tokenized input.
*   Return a `TokenEmbeddings` field with values as arrays of shape
    `<float>[num_tokens, emb_dim]` containing the input embeddings $x$.
*   Return a `TokenGradients` field with the `align` attribute pointing to the
    name of the `Tokens` field (i.e. `align="tokens"`), and the `grad_for`
    attribute pointing to the name of the `TokenEmbeddings` field. Values should
    be arrays of shape `<float>[num_tokens, emb_dim]` representing the gradient
    $\nabla_{x} \hat{y}$ of the embeddings with respect to the prediction
    $\hat{y}$.

As with grad-norm, the model should return embeddings and gradients as NumPy
arrays. The LIT `GradientDotInput` component will compute the dot products and
appropriate normalization.

### Integrated Gradients

Integrated gradients is a more robust method for estimating feature
contribution, based on integrating a gradients along a path in embedding space.
See [Sundararajan et al. 2017](https://arxiv.org/abs/1703.01365) for additional
detail on the algorithm. This method may give better results than grad-norm and
grad-dot-input, but also requires more involved instrumentation of the model.

To support this method, your model needs to return the gradients and embeddings
needed for grad-dot-input, and also to *accept* modified embeddings as input.

*   The model output should be as for grad-dot-input, plus
    `grad_target_field_key` must be set to the name of a label field from the
    input.
*   The model should have an [optional input](./api.md#optional-inputs) of type
    `TokenEmbeddings` with the same name as the output `TokenEmbeddings` field
    (see [type system conventions](./api.md#conventions)), which will be used to
    feed in the interpolated inputs as arrays of shape `<float>[num_tokens,
    emb_dim]`.

An example spec would look like:

```python
   def input_spec(self) -> types.Spec:
     return {
         # ...
         "token_embs": lit_types.TokenEmbeddings(align='tokens', required=False),
         # ...
     }

   def output_spec(self) -> types.Spec:
     return {
         # ...
         "tokens": lit_types.Tokens(parent="input_text"),
         "token_embs": lit_types.TokenEmbeddings(align='tokens'),
         "token_grads": lit_types.TokenGradients(align='tokens',
                                                 grad_for="token_embs",
                                                 grad_target_field_key="label"),
         # ...
     }
```

For a more concrete example that also supports multiple segments with separate
gradients, see our
[BERT classifier demo model](https://github.com/PAIR-code/lit/blob/main/lit_nlp/examples/glue/models.py),
or contact the LIT team for assistance.

### LIME

[LIME](https://arxiv.org/abs/1602.04938) is a black-box salience method that
does not require access to any model internals. It works by generating a set of
perturbed inputs - generally, by dropping out or masking tokens - and training a
local linear model to reconstruct the original model's predictions. The weights
of this linear model are treated as the salience values.

The trade-off, compared to gradient-based methods, is that computing LIME can be
slow as it requires many evaluations of the model. Additionally, LIME can be
noisy on longer inputs, as there are more tokens to ablate. To compensate, you
can increase the number of samples:

![LIME configuration options](./images/components/lime-options.png){w=600px align=center}

LIME works out-of-the-box with any classification (`MulticlassPreds`) or
regression/scoring (`RegressionScore`) model.

### Target Selection on Classification Output

For all salience methods, we require that the class to explain is given as a
label field in the input. For example, if the input example is:

```
{"text": "this movie was terrible!", "label": "0"}
```

Our model should return gradients with respect to the class 0. Conversely, we
might want to ask what features would encourage the model to predict a different
class. If we select class 1 from the UI:

![Target Selection](./images/components/salience-target-select.png){w=400px align=center}

Then the model will receive a modified input with this target:

```
{"text": "this movie was terrible!", "label": "1"}
```

To support this, the model should have the label field in the `input_spec`:

```
def input_spec(self) -> types.Spec:
  return {
    'text': lit_types.TextSegment(),
    'label': lit_types.CategoryLabel(..., required=False),
    ...
  }
```

and have an output field which references this using `parent=`:

```
def output_spec(self) -> types.Spec:
  return {
    'probas': lit_types.MulticlassPreds(..., parent="label"),
    ...
  }
```

You don't have to call the field "label", and it's okay if this field isn't
present in the *dataset* - as long as it's something that the model will
recognize and use as the target to derive gradients.

## Sequence Salience

Sequence salience generalizes token-based salience to text-to-text models,
allowing you to explain the impact of the prompt tokens on parts of the model
output.

LIT has a general-purpose sequence salience visualization designed for
left-to-right ("causal") language models:

![Sequence salience - sequence selection](./images/components/sequence-salience-1.png){w=650px align=center}

![Sequence salience - visualization](./images/components/sequence-salience-2.png){w=650px align=center}

The UI supports multiple options for analysis, including:

*   Select from predefined target sequences, or explain generations from the
    model.
*   Different salience methods, including [Gradient Norm](#gradient-norm) and
    [Gradient-dot-Input](#gradient-dot-input).
*   Multiple granularity levels for analysis, from individual sub-word tokens up
    to words, sentences, lines, or paragraphs. Quickly switch between different
    views to refine your analysis to different parts of a prompt.
*   Display density options to enable working with longer sequences, such as
    document text, few-shot examples, or chain-of-thought prompts.

For a walkthrough of how to use sequence salience to debug LLMs, check out the
Responsible Generative AI Toolkit at
https://ai.google.dev/responsible/model_behavior and for more on design of the
system see our paper at https://arxiv.org/abs/2404.07498.

If you find this useful in your work, please cite Sequence Salience as:

```
@article{tenney2024interactive,
  title={Interactive Prompt Debugging with Sequence Salience},
  author={Tenney, Ian and Mullins, Ryan and Du, Bin and Pandya, Shree and Kahng, Minsuk and Dixon, Lucas},
  journal={arXiv preprint arXiv:2404.07498},
  year={2024}
}
```

**Code:**

Currently, this works out-of-the-box with Gemma, Llama 2, Mistral, and GPT-2,
using either KerasNLP or Transformers.

*   LIT-for-Gemma Colab:
    [`lit_gemma.ipynb`](https://colab.research.google.com/github/google/generative-ai-docs/blob/main/site/en/gemma/docs/lit_gemma.ipynb)
*   Demo binary:
    https://github.com/PAIR-code/lit/blob/main/lit_nlp/examples/prompt_debugging/server.py
*   KerasNLP model wrappers:
    https://github.com/PAIR-code/lit/blob/main/lit_nlp/examples/prompt_debugging/keras_lms.py
*   Transformers model wrappers:
    https://github.com/PAIR-code/lit/blob/main/lit_nlp/examples/prompt_debugging/transformers_lms.py

## Salience Clustering

LIT includes a basic implementation of the salience clustering method from
[Ebert et al. 2022](https://arxiv.org/abs/2211.05485), which uses k-means on a
salience-weighted bag-of-words representation to find patterns in model
behavior. This method is available using any of the token-based salience methods
above, and if enabled will appear in the "Salience Clustering" tab:

![Salience clustering UI](./images/components/salience-clustering.png)

To run clustering, select a group of examples or the entire dataset, choose a
salience method, and run using the "Apply" button. The result will be a set of
top tokens for each cluster, as in Table 6 of
[the paper](https://arxiv.org/pdf/2211.05485.pdf).

## Tabular Feature Attribution

Tabular feature attribution seeks to understand the importance of a column of
data on a model's predictions. LIT's tabular feature attribution module supports
this analysis using the [SHAP interpreter](https://github.com/slundberg/shap).
Please check out
[our tutorial](https://pair-code.github.io/lit/tutorials/tab-feat-attr/) to
learn more about how to use this module to analyze feature importance in the
[Penguins demo](https://pair-code.github.io/lit/demos/penguins.html).

![Tabular feature attribution module module](./images/components/tabular-feature-attribution.png){w=500px align=center}

## Pixel-based Salience

LIT also supports pixel-based salience methods, for models that take images as
inputs. Output is rendered in the Salience Maps module in the LIT UI, which
allows for comparison of multiple methods at once.

To enable pixel-based salience methods for models that take images as inputs,
your model should, as part of the
[output spec and `predict()` implementation](./api.md#models):

*   Return a `ImageGradients` field with the `align` attribute pointing to the
    name of the `ImageBytes` field and, optionally, the `grad_target_field_key`
    attribute pointing to the `CategoryLabel` field in input spec that specifies
    the target class for which to take gradients, if the model can process that
    as an input. Without this gradient target field key, the model should return
    gradients with respect to the argmax class for classification models. The
    model should also return the actual class for which the gradients have been
    computed in the `grad_target` output field. The values returned in this
    field (as `<float>[image_height, image_width, color_channels]`) should be
    the gradients with respect to each pixel in each color channel in the 2D
    input image.

    The model should be able to accept input images as numpy arrays in addition
    to accepting base64 URL encoded format.

A variety of image saliency techniques are implemented for models that return
image gradients, through use of the
[PAIR-code saliency library](https://github.com/PAIR-code/saliency), including
integrated gradients, guided integrated gradients, blurred integrated gradients,
and XRAI.

Each of these techniques returns a saliency map image as a base64-encoded string
through the `ImageSalience` type.

## Embedding Projector

LIT includes a version of the
[embedding projector](https://projector.tensorflow.org/) which can be used to
visualize the latent space of your model, in order to find clusters or patterns
in the data. [UMAP](https://umap-learn.readthedocs.io/en/latest/) and PCA are
both supported as projection techniques.

![Embedding Projector](./images/components/embeddings.png){w=500px align=center}

The plot can be panned, zoomed, and rotated, and you can click a point to select
an example, or shift-click to select a group. You can also use LIT's global
colormap (the "Color By" menu) to color points by features such as the original
label, the model's predictions, or another categorical feature from the input.

To enable the embedding projector, your model should return one or more
`Embeddings` fields, with corresponding values as fixed-length vectors
`<float>[emb_dim]` for each example.

## Aggregate Analysis

### Metrics

LIT includes common metrics for classification, regression, and seq2seq (BLEU)
by default, which will appear in the table when the appropriate types are
present in the model output and input data. Metrics can be computed on the whole
dataset, a selected subset, or on facets defined by particular features. For
example, we could facet by class label:

![Metrics Table](./images/components/metrics-table.png)

To try this out, see https://pair-code.github.io/lit/demos/glue.html and navigate to the "Metrics" tab.

To enable metrics, your model should set the `parent` attribute on one or more
output fields, pointing to the name of the input field that it should be
evaluated against. For example, for classification, the data spec might have:

```python
    def spec(self) -> types.Spec:
      return {
          # ...
          "label": lit_types.CategoryLabel(vocab=self.LABELS),
          # ...
      }
```

and the model would include:

```python
    def output_spec(self) -> types.Spec:
      return {
          # ...
          "probas": lit_types.MulticlassPreds(vocab=self.LABELS, parent='label'),
          # ...
      }
```

Custom metrics can be easily defined in Python; see the
[API documentation](./api.md#metrics) for more.

### Confusion Matrix

LIT includes a powerful and flexible confusion matrix, which can be used to
compare predictions to gold labels as well as to compare between two models or
between different categorical features. You can click cells or row/column
headers to select a subset of examples, which is useful for intersectional
analysis.

![Confusion Matrix](./images/components/confusion-matrix.png){w=600px align=center}

To try this out, see https://pair-code.github.io/lit/demos/glue.html and navigate to the "Metrics" tab.

The confusion matrix is supported for classification models, or if the input
data includes any categorical features (`CategoryLabel`).

### Scalar Plots

LIT includes scatterplots for scalar features, including plain scalars (`Scalar`
or `RegressionScore`) as well as per-class probabilities from classification
output (`MulticlassPreds`).

![Scalar Plot Module](./images/components/scalars-sst2.png)

You can click individual points to select them, or drag to select all examples
in a range of scores - for example, to find examples near the decision boundary.
The plots also respond to LIT's global colormap (the "Color By" menu), which can
color points by categorical features or the model's predicted class.

To try this out, see https://pair-code.github.io/lit/demos/glue.html and navigate to the "Predictions" tab.

### Binary Classification Thresholds

For binary classification models, LIT contains a module for setting
classification thresholds, which determine at what score for the positive class
the model determines that an example should be classified as belonging to the
positive class.

This threshold can be set either on the entire dataset, or can be set separately
on faceted subsets of the dataset. Checkboxes in this module are used to select
which features in the dataset will be used to create the faceted subsets.
Multiple features can be selected, which leads to intersectional subsets.

Additionally, if the dataset has ground truth labels for the value being
predicted, then the module can calculate the optimal value to set these
thresholds. The cost ratio input box allows setting of the relative penalty of
the model producing a false positive, compared to a false negative. By default,
this is set to 1, meaning that false positives and false negatives are equally
costly, in terms of how we should calculate the optimal threshold(s). Setting it
to 2 would mean that false positives are twice as costly as false negatives, and
setting it to .5 would mean that false negatives are twice as costly as false
positives.

The "Get optimal threshold" button will calculate optimal thresholds for each
subset specified by the checkboxes, or the entire dataset if no subsets are
created. These are displayed in the thresholds table along with the slider to
manually change the thresholds. The buttons in the table header allow easy
setting of those optimal thresholds.

When the dataset is faceted into subsets, along with calculating optimal
individual thresholds per subset, and an optimal overall threshold for the
entire dataset, a number of other threshold sets are calculated. These are based
on different fairness constraints that may be of interest to the user.

One such constraint is demographic parity, which attempts to have an equal
percentage of positive classifications for each subset. Another is equal
accuracy, which attempts to have an equal accuracy score for each subset. There
is also equal opportunity, which attempts to equalize for each subset the
percentage of positive predictions among those datapoints with a positive ground
truth label.

![Binary Classifier Thresholds Module](./images/components/lit-thresholder.png)

### Partial Dependence Plots

For classification or regression models with `CategoryLabel` or `Scalar` input
features, the Partial Dependence Plots module shows plots indicating the effect
that changing those individual features has on model output.

If a single datapoint is selected, then a feature's plot shows the effect of
changing that one value has on model output. For numeric features, the model
output is calculated for 10 different values, ranging from the minimum value of
that feature in the dataset to its maximum value in the dataset, and the results
are shown in the line chart. For categorical features, the model output is
calculated for all values of that feature from the `vocab` specified in the
`CategoryLabel` for that feature, and the results are shown in a column chart.

If multiple datapoints are selected, then the model outputs are calculated using
the same logic for each datapoint, and the outputs are averaged to create the
points on the line or column charts. In this way, the charts show the average
effect of that feature on model output, given the datapoints chosen.

If no datapoints are selected, then the calculations are done across all
datapoints, giving a global view of feature effects.

![Partial Dependence Plots Module](./images/components/lit-pdps.png){w=400px align=center}

To try this out, see https://pair-code.github.io/lit/demos/penguins.html and navigate to the "Predictions" tab.

### Dive

Dive is a visualization module, inspired by our prior work on
[Facets Dive](https://pair-code.github.io/facets/) and its use in the
[What-If Tool](https://pair-code.github.io/what-if-tool/), that enables
exploration of data subsets grouped by feature values.

![Dive module](./images/components/dive.png){w=500px align=center}

Data are displayed in a matrix of groups based on feature values, with each
group containing the datapoints at the intersection of the feature values for
that column and row. Use the drop-downs at the top to select the feature to use
for the rows and columns in the matrix. You can use the "Color By" drop-down in
the main toolbar to change the feature by which datapoints are colored in the
matrix.

This visualization is powered by
[Megaplot](https://github.com/PAIR-code/megaplot), which allows it to support up
to 100k datapoints. Dive support mouse-based zoom (scroll) and pan (drag)
interactions to help you navigate these very large datasets. You can also use
the "zoom in", "zoom out", and "reset view" buttons in the module toolbar to
help navigate with more precision.

Dive is currently integrated in the
[Penguins demo](https://pair-code.github.io/lit/demos/penguins.html), and will
be supported in other demos in future releases.

## TCAV

Many interpretability methods provide importance values per input feature (e.g,
token). By contrast, [TCAV](https://arxiv.org/abs/1711.11279) shows the
importance of high-level concepts (e.g., color, gender, race) for a prediction
class, which is more akin to how humans communicate.

From those examples, TCAV learns a vector representing those concepts in a model
layer, which we call a concept activation vector (CAV). A CAV is learned using a
linear classifier. Intuitively, CAV measures how sensitive prediction is to the
concept (i.e., directional derivative of the prediction with respect to the
CAV). Unlike many local attribution methods mentioned above, TCAV is a global
method. This means that TCAV explains "a class" rather than "a data point". TCAV
does this by aggregating concepts' influence on data points in a class (i.e.,
ratio of data points with positive directional derivatives).

The TCAV method can be applied to models with any input modality. To enable
TCAV, your model should return one or more example-level Embeddings fields for a
layer, the predicted class, and the corresponding gradient values for that layer
and class.

### Example

1.) To use TCAV, begin by creating one or more 'concept' slices.

Every dataset/model is different, but for images, as low as 15 data points are
shown to be sufficient. Start by adding at least 3 data points and add more as
needed.

For this example, we select all examples related to acting in the data table
using the selector `acting|actor|actress`.

![Data table - select examples](./images/components/tcav-search-examples.png){w=400px align=center}

2.) Next, name the slice `acting` and click 'Create slice'.

![Slice](./images/components/tcav-create-slice.png){w=280px align=center}

3.) Finally, navigate to the TCAV tab, select the newly created slice, and click
'Run TCAV'.

This initiates standard TCAV, which compares the selected examples against
random splits of examples in the rest of the dataset. Alternatively, selecting a
second 'negative' slice would initiate relative TCAV, which compares the
selected slice's examples against those in the negative slice.

![TCAV1](./images/components/tcav-select-slice.png){w=800px align=center}

When the run is complete (usually after a few seconds), the results are
displayed in the table. In this example, the TCAV score is ~0.9 (shown by the
blue bar in the score bar), which is higher than the baseline (shown as the
black bar in the score bar ), indicating that the acting concept positively
influences the prediction class 1, or positive sentiment. (Technically, the
baseline represents 'null hypothesis', calculated with random concepts.)

![TCAV2](./images/components/tcav-results-table.png){w=800px align=center}

### Statistical Significance

One of the pitfalls with the TCAV method is the potential generating meaningless
CAVs, since any randomly chosen set of images will still produce a CAV (even if
it is not meaningful).

To guard against this, we use statistical testing to verify whether CAVs are
statistically significant. For standard TCAV, we generate 15 possibly meaningful
CAVs using the selected concept slice and random splits of the same size from
the remainder of the dataset. We also generate 15 random CAVs using random
splits against random splits. We then do a t-test to check if these two sets of
scores are from the same distribution and reject CAVs as insignificant if the
p-value is greater than 0.05. (If this happens, a warning is displayed in place
of the TCAV score in the UI.)

For relative TCAV, users would ideally test concepts with at least ~100 examples
each so we can perform ~15 runs on unique subsets. In practice, users may not
pass in this many examples.

To accommodate this, we use a cross-validation approach, where we will try
different subset split sizes, and return one with a statistically significant
result (when compared against random CAVs). We set the minimum number of
examples to run TCAV at 3 examples, and need at least 2 runs for statistical
testing. If there are too few examples for this, we will perform 1 run of size
min(concept set length, negative set length), and return the result without
statistical testing (which is indicated in the UI).

### Sorting by Cosine Similarity

The option to sort examples by cosine similarity to a CAV will be available in
an upcoming release.

## Counterfactual Analysis

While aggregate metrics can give a picture of overall behavior, and salience
maps can give quick insight into a model's local behavior, many questions about
model behavior are best answered in a counterfactual setting: "How does my model
behave under a controlled change in inputs?"

For example, you might want to see what happens if a single token is deleted, a
word is substituted, or some systematic transformation - like paraphrasing or an
adversarial attack - is applied to the whole dataset. LIT includes features to
explore this, both through manual edits and through automatic "generator"
components.

### Manual Editing

Examples can be edited manually in the Datapoint Editor module:

![Manual Edit in the Datapoint Editor](./images/components/manual-edit.png){w=400px align=center}

The "Add and Compare" button can be used to enter comparison mode, which will
automatically "pin" the original example as a reference selection. Many LIT
modules will automatically duplicate to show the predictions on this example
side-by-side with the original. For example:

![Side-by-Side](./images/components/side-by-side-salience.png)

You can also use the toolbar controls to enter comparison mode. LIT also keeps
track of the relationship between examples, and you can use the pair selection
controls to cycle through the available (original, edited) examples:

![Pair Selection Controls](./images/components/pair-selection.png){w=700px align=center}

### Generators

The **Generator Module** supports automatic generation of counterfactuals
through a variety of plug-in components:

![Generator Module](./images/components/generator-module.png)

Semantically, generators are Python classes which take one or more input
examples and return a new set of transformed examples. This can include simple
transformations such as scrambling word order or making regex substitutions, or
more complex methods such as back-translation or adversarial methods such as
[HotFlip](https://arxiv.org/abs/1712.06751).

Generators can be easily defined using the [Python API](./api.md#generators) and
customized for particular applications or domains.

We also include a handful of off-the-shelf methods:

*   The
    [**scrambler**](https://github.com/PAIR-code/lit/blob/main/lit_nlp/components/scrambler.py)
    simply randomizes word order of the input.
*   The
    [**word replacer**](https://github.com/PAIR-code/lit/blob/main/lit_nlp/components/word_replacer.py)
    makes simple substitutions, such as `great -> terrible`.
*   [**HotFlip**](https://github.com/PAIR-code/lit/blob/main/lit_nlp/components/hotflip.py)
    ([Ebrahimi et al. 2017](https://arxiv.org/abs/1712.06751)) tries to find
    minimal token substitutions to change the model's prediction. Compatible
    with classification models (`MulticlassPreds`) or regression models
    (`RegressionScore`) via thresholding, and requires access to
    `TokenGradients` as well as a special `get_embedding_table()` method on the
    model class.
*   [**Ablation Flip**](https://github.com/PAIR-code/lit/blob/main/lit_nlp/components/ablation_flip.py)
    is similar to HotFlip, but tries to change the prediction by selectively
    dropping tokens from the input. Unlike HotFlip, this does not require
    gradients or access to the embedding table and can work with any
    classification or regression model.

================
File: website/sphinx_src/conf.py
================
"""Configuration file for the Sphinx documentation builder."""
# For the full list of built-in configuration values, see the documentation:
# https://www.sphinx-doc.org/en/master/usage/configuration.html
# -- Project information -----------------------------------------------------
# https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information
project = 'LIT'
copyright = '2023, Google LLC'  # pylint: disable=redefined-builtin
author = 'People + AI Research'
release = '1.0'
# -- General configuration ---------------------------------------------------
# https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration
extensions = ['myst_parser']
myst_heading_anchors = 3
myst_enable_extensions = [
    'dollarmath',
    'linkify',
    'attrs_inline',
]
templates_path = ['_templates']
exclude_patterns = []
source_suffix = {
    '.rst': 'restructuredtext',
    '.txt': 'markdown',
    '.md': 'markdown',
}
# -- Options for HTML output -------------------------------------------------
# https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-html-output
html_theme = 'furo'
html_context = {'default_mode': 'light'}
html_static_path = ['sphinx_static']
html_css_files = [
    'furo_custom.css',
]

================
File: website/sphinx_src/demos.md
================
# Demos

<!-- freshness: { owner: 'lit-dev' reviewed: '2024-06-24' } -->

<!-- [TOC] placeholder - DO NOT REMOVE -->

The LIT team maintains a number of hosted demos, as well as pre-built launchers
for some common tasks and model types.

For publicly-visible demos hosted on Google Cloud, see
https://pair-code.github.io/lit/demos/.

--------------------------------------------------------------------------------

## Classification <!-- DO NOT REMOVE {#classification .demo-section-header} -->

### Sentiment and NLI <!-- DO NOT REMOVE {#glue .demo-header} -->

**Hosted instance:** https://pair-code.github.io/lit/demos/glue.html \
**Code:** [examples/glue/demo.py](https://github.com/PAIR-code/lit/blob/main/lit_nlp/examples/glue/demo.py)

*   Multi-task demo:
    *   Sentiment analysis as a binary classification task
        ([SST-2](https://nlp.stanford.edu/sentiment/treebank.html)) on single
        sentences.
    *   Natural Language Inference (NLI) using
        [MultiNLI](https://cims.nyu.edu/~sbowman/multinli/), as a three-way
        classification task with two-segment input (premise, hypothesis).
    *   STS-B textual similarity task (see
        [Regression / Scoring](#regression-scoring) below).
    *   Switch tasks using the Settings () menu.
*   BERT models of different sizes, built on HuggingFace TF2 (Keras).
*   Supports the widest range of LIT interpretability features:
    *   Model output probabilities, custom thresholds, and multiclass metrics.
    *   Jitter plot of output scores, to find confident examples or ones near
        the margin.
    *   Embedding projector to find clusters in representation space.
    *   Integrated Gradients, LIME, and other salience methods.
    *   Counterfactual generators, including HotFlip for targeted adversarial
        perturbations.

Tip: check out a case study for this demo on the public LIT website:
https://pair-code.github.io/lit/tutorials/sentiment

--------------------------------------------------------------------------------

## Regression / Scoring <!-- DO NOT REMOVE {#regression-scoring .demo-section-header} -->

### Textual Similarity (STS-B) <!-- DO NOT REMOVE {#stsb .demo-header} -->

**Hosted instance:** https://pair-code.github.io/lit/demos/glue.html?models=stsb&dataset=stsb_dev \
**Code:** [examples/glue/demo.py](https://github.com/PAIR-code/lit/blob/main/lit_nlp/examples/glue/demo.py)

*   STS-B textual similarity task, predicting scores on a range from 0
    (unrelated) to 5 (very similar).
*   BERT models built on HuggingFace TF2 (Keras).
*   Supports a wide range of LIT interpretability features:
    *   Model output scores and metrics.
    *   Scatter plot of scores and error, and jitter plot of true labels for
        quick filtering.
    *   Embedding projector to find clusters in representation space.
    *   Integrated Gradients, LIME, and other salience methods.

--------------------------------------------------------------------------------

## Sequence-to-Sequence <!-- DO NOT REMOVE {#seq2seq .demo-section-header} -->

### Gemma <!-- DO NOT REMOVE {#gemma .demo-header} -->

**Code:**
[examples/prompt_debugging/server.py](https://github.com/PAIR-code/lit/blob/main/lit_nlp/examples/prompt_debugging/server.py)

*   Supports Gemma 2B and 7B models using KerasNLP (with TensorFlow or PyTorch)
    and Transformers (with PyTorch).
*   Interactively debug LLM prompts using
    [sequence salience](./components.md#sequence-salience).
*   Multiple salience methods (grad-l2 and grad-dot-input), at multiple
    granularities: token-, word-, line-, sentence-, and paragraph-level.

Tip: check out the in-depth walkthrough at
https://ai.google.dev/responsible/model_behavior, part of the Responsible
Generative AI Toolkit.

--------------------------------------------------------------------------------

## Multimodal <!-- DO NOT REMOVE {#multimodal .demo-section-header} -->

### Tabular Data: Penguin Classification <!-- DO NOT REMOVE {#penguin .demo-header} -->

**Hosted instance:** https://pair-code.github.io/lit/demos/penguins.html \
**Code:** [examples/penguin/demo.py](https://github.com/PAIR-code/lit/blob/main/lit_nlp/examples/penguin/demo.py)

*   Binary classification on
    [penguin dataset](https://www.tensorflow.org/datasets/catalog/penguins).
*   Showing using of LIT on non-text data (numeric and categorical features).
*   Use partial-dependence plots to understand feature importance on individual
    examples, selections, or the entire evaluation dataset.
*   Use binary classifier threshold setters to find best thresholds for slices
    of examples to achieve specific fairness constraints, such as demographic
    parity.

================
File: website/sphinx_src/docker.md
================
# Running LIT in a Docker container

<!--* freshness: { owner: 'lit-dev' reviewed: '2024-06-04' } *-->

Users might want to deploy LIT onto servers for public-facing, long-running
instances. This is how we host the LIT demos found on
https://pair-code.github.io/lit/demos/. This doc describes the basic usage of
LIT's built-in demos, how to integrate your custom demo into this

## Basic Usage

LIT can be run as a containerized app using [Docker](https://www.docker.com/) or
your preferred engine. This is how we run our
[hosted demos](https://pair-code.github.io/lit/demos/).

We provide a basic Dockerfile https://github.com/PAIR-code/lit/blob/main/Dockerfile that you can use to build and run any of the demos in the `lit_nlp/examples` directory.
The `Dockerfile` installs all necessary dependencies for LIT and builds the
front-end code from source. Then it runs [gunicorn](https://gunicorn.org/) as
the HTTP server, invoking the `get_wsgi_app()` method from our demo file to get
the WSGI app to serve. The options provided to gunicorn for our use-case can be
found in
[`gunicorn_config.py`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/examples/gunicorn_config.py).
You can find a reference implementation in
[`glue/demo.py`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/examples/glue/demo.py).

Use the following shell
https://github.com/PAIR-code/lit/blob/main/.github/workflows/ci.yml commands to build the
default Docker image for LIT from the provided `Dockerfile`, and then run a
container from that image. Comments are provided in-line to help explain what
each step does.

```shell
# Build the docker image using the -t argument to name the image. Remember to
# include the trailing . so Docker knows where to look for the Dockerfile.
docker build --file Dockerfile --tag lit-nlp .

# Now you can run LIT as a containerized app using the following command. Note
# that the last parameter to the run command is the value you passed to the -t
# argument in the build command above.
docker run --rm -p 5432:5432 lit-nlp
```

The image above defaults to launching the GLUE demo on port 5432, but you can
override this using the DEMO_NAME and DEMO_PORT environment variables, as shown
below.

```shell
# DEMO_NAME is used to complete the Python module path
#
#     "lit_nlp.examples.$DEMO_NAME.demo:get_wsgi_app()"
#
# Therefore, valid values for DEMO_NAME are Python module paths in the
# lit_nlp/examples directory, such as glue, penguin, tydi, etc.
docker run --rm -p 5432:5432 -e DEMO_NAME=penguin lit-nlp

# Use the DEMO_PORT environment variable as to change the port that LIT uses in
# the container. Be sure to also change the -p option to map the container's
# DEMO_PORT to a port on the host system.
docker run --rm -p 2345:2345 -e DEMO_PORT=2345 lit-nlp

# Bringing this all together, you can run multiple LIT apps in separate
# containers on your machine using the combination of the DEMO_NAME and
# DEMO_PORT arguments, and docker run with the -d flag to run the container in
# the background.
docker run -d -p 5432:5432 -e DEMO_NAME=penguin lit-nlp
docker run -d -p 2345:2345 -e DEMO_NAME=tydi -e DEMO_PORT=2345 lit-nlp
```

## Integrating Custom LIT Instances with the Default Docker Image

Many LIT users create their own custom LIT server script to demo or serve, which
involves creating an executable Python module with a `main()` method, as
described in the [Python API docs](api.md#adding-models-and-data).

These custom server scripts can be easily integrated with LIT's default image as
long as your server meets two requirements:

1.  Ensure your server script is located in the `lit_nlp/examples` directory (or
    in a nested directory under `lit_nlp/examples`).
2.  Ensure that your server script defines a `get_wsgi_app()` function similar
    to the minimal example shown below.

```python
def get_wsgi_app() -> Optional[dev_server.LitServerType]:
  """Return WSGI app for container-hosted demos."""
  # Set any flag defaults for this LIT instance
  FLAGS.set_default("server_type", "external")
  FLAGS.set_default("demo_mode", True)
  # Parse flags before calling main()
  unused = flags.FLAGS(sys.argv, known_only=True)
  if unused:
    logging.info("get_wsgi_app() called with unused args: %s", unused)
  return main([])
```

Assuming your custom script meets the two requirements above, you can simply
rebuild the default Docker image and run a container using the steps above,
ensuring that you pass the `-e DEMO_NAME=your_server_script_path_here` to the
run command.

A more detailed description of the `get_wsgi_app()` code can be found below.

```python
def get_wsgi_app() -> Optional[dev_server.LitServerType]:
  """Returns a WSGI app for gunicorn to consume in container-hosted demos."""
  # Start by setting any default values for flags your LIT instance requires.
  # Here we set:
  #
  #     server_type to "external" (required), and
  #     demo_mode to "True" (optional)
  #
  # You can add additional defaults as required for your use case.
  FLAGS.set_default("server_type", "external")
  FLAGS.set_default("demo_mode", True)

  # Parse any parameters from flags before calling main(). All flags should
  # defined using one of absl's flags.DEFINE methods.
  #
  # Note the use of the known_only=True parameter here. This ensures that only
  # those flags that have been define using one of absl's flags.DEFINE methods
  # will be parsed from the command line arguments in sys.argv. All unused
  # arguments will be returned as a Sequence[str].
  unused = flags.FLAGS(sys.argv, known_only=True)

  # Running a LIT instance in a container based on the default Dockerfile and
  # image will always produce unused arguments, because sys.argv contains the
  # command and parameters used to run the gunicorn sever. While not stricly
  # required, we recommend logging these to the console, e.g., in case you need
  # to verify the value of an environment variable.
  if unused:
    logging.info("get_wsgi_app() called with unused args: %s", unused)

  # Always pass an empty list to main() inside of get_wsgi_app() functions, as
  # absl apps are supposed to use absl.flags to define any and all flags
  # required to run the app.
  return main([])
```

## Building Your Own Image

Coming soon.

================
File: website/sphinx_src/faq.md
================
# Frequently Asked Questions

<!--* freshness: { owner: 'lit-dev' reviewed: '2024-06-03' } *-->

<!-- [TOC] placeholder - DO NOT REMOVE -->

Looking for help? Submit bugs, ask questions, suggest content, and request
features on our
[Github issues list](https://github.com/pair-code/lit/issues/).

## Model and Data Types

LIT can handle a variety of models with different input and output types, and
works with any modern ML framework. For more information, see
[Framework & Model Support](components.md#framework-and-model-support).

In addition to text, LIT has good support for different modalities, including
images and tabular data. For examples, see:

*   [Tabular demo](https://github.com/PAIR-code/lit/blob/main/lit_nlp/examples/penguin/demo.py) -
    multi-class classification on tabular (numeric and categorical string) data,
    using the
    [Palmer Penguins](https://www.tensorflow.org/datasets/catalog/penguins)
    dataset.

For more details, see
[the features guide to input and output types](api.md#type-system).

## Languages

All strings in LIT are unicode and most components use model-provided
tokenization if available, so in most cases non-English languages and non-Latin
scripts should work without any modifications.

## Scale

### Dataset Size

LIT can comfortably handle 10k-100k datapoints, depending on the speed of the
server (for hosting the model) and your local machine (for viewing the UI). When
working with large datasets, there are a couple caveats:

*   LIT expects predictions to be available on the whole dataset when the UI
    loads. This can take a while if you have a lot of examples or a larger model
    like BERT. In this case, we recommend adding the flag `--warm_start=1` (or
    pass `warm_start=1` to the `Server` constructor in Python) to pre-compute
    predictions before starting the server.

*   Datasets containing images may take a while to load. If full "native"
    resolution is not needed (such as if the model operates on a smaller size
    anyway, such as 256x256), then you can speed things up by resizing images in
    your `Dataset` loading code.

*   LIT uses WebGL for the embedding projector (via
    [ScatterGL](https://github.com/PAIR-code/scatter-gl)) and for the Scalars
    and Dive modules (via [Megaplot](https://github.com/PAIR-code/megaplot)),
    which may be slow on older machines if you have more than a few thousand
    points.

If you have more data, you can use `Dataset.sample` or `Dataset.slice` to select
a smaller number of examples to load. You can also pass individual examples to
LIT [through URL params](#sending-examples-from-another-tool), or load custom
data files at runtime using the settings () menu.

### Large Models

LIT can work with large or slow models, as long as you can wrap them into the
model API. If you have more than a few preloaded datapoints, however, you'll
probably want to use `warm_start=1` (or pass `--warm_start=1` as a flag) to
pre-compute predictions when the server loads, so you don't have to wait when
you first visit the UI.

Also, beware of memory usage: since LIT keeps the models in memory to support
new queries, only so many models can fit on a single node or GPU. If you want to
load more or larger models than can fit in local memory, you can host your model
with your favorite serving framework and connect to it using a custom
[`Model`](api.md#models) class.

We also have experimental support for using LIT as a lightweight model server;
this can be useful, e.g., for comparing an experimental model running locally
against a production model already running in an existing LIT demo. See
[`remote_model.py`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/components/remote_model.py)
for more details.

## Privacy and Security

LIT allows users to query the model, as well as to view the loaded evaluation
data. The LIT UI state is ephemeral and exists only in the browser window;
however, model predictions and any newly-generated examples (including as
manually entered in the web UI) are stored in server memory, and if `--data_dir`
is specified, may be cached to disk.

LIT has the ability to create or edit datapoints in the UI and then save them to
disk. If you do not want the tool to be able to write edited datapoints to disk,
then pass the `--demo_mode` runtime flag to the LIT server.

### I have proprietary data. Is LIT secure for my team to use?

We don't store, collect or share datasets, models or any other information
loaded into LIT. When you run a LIT server, anyone with access to the web
address of the server will be able to see data from the loaded datasets and
interact with the loaded models. If you need to restrict access to a LIT
server, then make sure to configure the hosting of your LIT server to do so.

The default LIT development server does not implement any explicit access
controls. However, this server is just a thin convenience wrapper, and the
underlying WSGI App can be easily exported and used with additional middleware
layers or external serving frameworks. See
[Running LIT in a Docker container](./docker.md) for an example.

## Workflow and Integrations

### Sending examples from another tool

LIT can read input fields directly from the URL; they should be encoded as
`data_<fieldname>=<value>`, and field names should match those in the (default)
dataset.

There is also (experimental) support for sending more than one example, as long
as the total URL length is within the browser's size limit. Specify as above,
but using `data0`, `data1`, `data2`, e.g. `data0_<fieldname>=<value>`.

### Downloading or exporting data

Currently, there are three ways to export data from the LIT UI:

-   In the Data Table, you can copy or download the current view in CSV format -
    see [the UI guide](./ui_guide.md#data-table) for more details.
-   In the "Dataset" tab of the settings () menu, you can enter a path to save
    data to. Data is pushed to the server and written by the server backend, so
    be sure the path is writable.

-   If using LIT in a Colab or other notebook environment, you can access the
    current selection from another cell using `widget.ui_state.primary`,
    `widget.ui_state.selection`, and `widget.ui_state.pinned`.

### Loading data from the UI

There is limited support for this via the settings () menu. Select a dataset,
and enter a path to load from:

![Load data from the UI](./images/lit-load-data.png)

The supported format(s) depend on the dataset class; in most cases, the user
should implement the `load()` method on their dataset class to handle the
appropriate format.

### Using components outside the LIT UI

Python components such as models, datasets, and generators are designed to
support standalone use. These don't depend on the LIT serving framework, and you
can treat them as any other Python class and use from Colab, regular scripts,
bulk inference pipelines, etc. For an example, see
[the API documentation](./api.md#using-lit-components-outside-of-lit).

For the frontend, it's a little more difficult. In order to respond to and
interact with the shared UI state, there's a lot more "framework" code involved
(see the [frontend development guide](./frontend_development.md) for more).
We're working on refactoring the LIT
[`modules`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/client/modules) to separate
framework and API code from the visualizations (e.g.
[`elements`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/client/elements)), which can
then be re-used in other environments.

### Training models with LIT

LIT is primarily an evaluation/inference-time tool, so we don't provide any
official training APIs. However, to facilitate code reuse you can easily add
training methods to your model class. In fact, several of our demos do exactly
this, using LIT's `Dataset` objects to manage training data along with standard
training APIs (such as Keras' `model.fit()`). See
[`glue/models.py`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/examples/glue/models.py)
for examples.

### Debug LIT UI in Colab

The LIT instance launched from CLI typically has helpful error messages in the
UI. However, this is not the case for the LIT UI in Colab and the error message
does not report any stacktrace, which makes debugging very difficult.

![LIT UI error in colab](./images/lit-ui-error-in-colab.png "LIT UI error in colab")

While in
[Chrome developer tools](https://support.google.com/campaignmanager/answer/2828688?hl=en),
you will be able to debug issues solely related to the frontend, but not so for
issues related to the backend or on the HTTP request path.

Thus, to show the full stacktrace, you would need to find the HTTP request sent
from the frontend to the backend, compose the same request in colab and send it
to the server.

1.  When rendering the UI, display it in a separate tab to make things a bit
    easier to work with, e.g. `lit_widget.render(open_in_new_tab=True)`.
2.  Open
    [Chrome developer tools](https://support.google.com/campaignmanager/answer/2828688?hl=en),
    go to "Sources" tab and find the file
    [client/services/api_service.ts](https://github.com/PAIR-code/lit/blob/main/lit_nlp/client/services/api_service.ts) and set a
    breakpoint right after where the HTTP request is set up in the `queryServer`
    method, e.g. after this line `const res = await fetch(url, {method: 'POST',
    body});`.
    *   Note it is possible that the whole frontend source code is compiled into
        a `main.js` file, and the code is not exactly the same as that in LIT
        frontend source code. You might have to do a bit digging to find the
        right line.
3.  Go to the UI and trigger the behavior that causes the error. Now in Chrome
    developer tools you will be able to see the variables and their values in
    the `queryServer` method. Copy the values of the `url` and `body` variables
    in the method.
4.  Go back to Colab, compose your HTTP request method. Look for the main server
    address printed out from `lit_widget.render(open_in_new_tab=True)`.

![LIT colab server address](./images/lit-colab-server-address.png "LIT colab server address")

Let's say the server address is "https://localhost:32943/?" as shown above, the
`body` variable obtained earlier has value "request_body_text" and the `url`
variable has value "./get_preds?param1=value1". Then your HTTP request will be
like this:

```sh
! curl -H "Content-Type: application/json" \
       -d "request_body_text" \
       -X POST "http://localhost:32943/get_preds?param1=value1"
```

Run this in Colab and you should be able to retrieve the full stacktrace of the
error.

================
File: website/sphinx_src/frontend_development.md
================
# Frontend Developer Guide

<!--* freshness: { owner: 'lit-dev' reviewed: '2024-06-24' } *-->

<!-- [TOC] placeholder - DO NOT REMOVE -->

This document aims to describe the current LIT frontend system, including
conventions, best practices, and gotchas.

## High Level Overview

LIT is powered by two central pieces of tech -
[lit-element](https://lit-element.polymer-project.org/) for components and HTML
rendering, and [mobx](https://mobx.js.org/README.html) for observable-oriented
state management.

Lit-element is a simple, web-component based library for building small,
self-contained pieces of web functionality. It uses a template-string based
output to declaratively render small, isolated pieces of UI.

Mobx is a tool centered around observable data, and it makes managing state
simple and scalable.

We highly recommend reading the docs for both projects - they both have fairly
simple APIs and are easy to digest in comparison to some heavier-weight toolkits
like Angular.

## Application Architecture

The LIT client frontend is roughly divided into three conceptual groups -
**Modules** (which render visualizations), **Services** (which manage data), and
the **App** itself (which coordinates initialization of services and determines
which modules to render).

### Bootstrapping

The LIT app bootstrapping takes place in two steps: First, the served
[`index.html`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/client/static/index.html)
page contains a single web component for the
[`<lit-app>`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/client/core/lit_app.ts).
This component is responsible for the overall layout of the app, including the
toolbar, footer, and the
[`<lit-modules>`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/client/core/modules.ts)
component. The `<lit-modules>` component is responsible for actually laying out
and rendering the various `LitModule` components, a process about which we'll go
into greater detail later.

The JS bundle entry point is
[`main.ts`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/client/main.ts), which first
imports the loaded, the `<lit-app>` web component is declared, and attaches
itself to the DOM, waiting for the app to be initialized.

The second step is kicking off app initialization. The
[`LitApp`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/client/core/app.ts) singleton
class is provided with a layout declaring which `LitModule` components to use,
then builds the app services and kicks off app initialization and loading data.

### Layout

A layout defines the arraignment of `LitModule` classes in the UI. Layouts are
specified in Python as `LitCanonicalLayout` instances, and LIT includes three
pre-configured layouts in
[`layout.py`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/api/layout.py):

*   `simple`: A minimalist layout with the examples on top (either individually
    (selected by default) or in a table) and predictions on the bottom;
*   `default`: The original LIT layout with a single group of modules on top for
    exploring and selecting data, and a collection of tabs supporting different
    analytical tasks on the bottom; and
*   `three_panel`: A three-panel layout that puts exploratory data
    visualizations at full-page height on the left, tools for inspecting and
    manipulating examples and their associated predictions in the upper right,
    and a collection of tabs supporting different analytical tasks in the lower
    left. Note that this was introduced in v1.0 as an experimental feature, your
    feedback is appreciated.

You can also add [custom layouts](./api.md#customizing-the-layout) to your LIT
instance by defining one or more `LitCanonicalLayout` instances and passing them
to the server. For an example, see
[`prompt_debugging/layouts.py`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/examples/prompt_debugging/layouts.py).

Note: The pre-configured layouts are added to every `LitApp` instance using
[dictionary updates](https://docs.python.org/3/library/stdtypes.html#dict) where
the Mapping passed to the `LitApp` constructor overrides the pre-configured
layouts `Mapping`. Thus, you can remove or change these pre-configured layouts
as you like by passing a `Mapping` where the values of `simple`, `default`,
and/or `three_panel` is `None` (to remove) or a `LitCanonicalLayout` instance
(to override) as you desire.

The actual layout of components in the LIT UI, see
[`<lit-modules>`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/client/core/modules.ts),
can be different than the declared layout, since the visibility of modules
depends on a number of factors, including the user-chosen visibility, the
compatibility of the configured modules with the selected model and dataset, and
whether or not specific modules show multiple copies per selected model. The
actual layout is computed in
[`modules_service`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/client/services/modules_service.ts).

### Initialization

Finally, the LIT App initializes by building the various service classes and
starting the initial load of data from the server. This process consists of:

1.  Parsing the URL query params to get the url configuration
1.  Fetching the app metadata, which includes what models/datasets are available
    to use.
1.  Determining which models/datasets to load and then loading them.

## Modules (LitModule)

The
[`LitModule`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/client/core/lit_module.ts)
is the base class from which all module components derive. It provides a number
of convenience methods for handling common update / data loading patterns. Each
LIT Module also requires a few static methods by convention, responsible for
specifying Module display and behavior. These helpers and conventions are
outlined below:

```typescript
/**
 * A dummy module that responds to changes in selected data by making a request
 * to an API service to get the pig latin translation.
 */
@customElement('demo-module')                                                   // (0)
export class DemoTextModule extends LitModule {
  static override title = 'Demo Module';                                        // (1)
  static override template =
      (model: string, selectionServiceIndex: number, shouldReact: number) =>    // (2)
          html`<demo-module model=${model} .shouldReact=${shouldReact}
                selectionServiceIndex=${selectionServiceIndex}></demo-module>`;
  static override duplicateForModelComparison = true;                           // (3)

  static override get styles() {
    return [styles];                                                            // (4)
  }

  private readonly colorService = app.getService(ColorService);                 // (5)

  @observable private pigLatin: string = '';                                    // (6)

  override firstUpdated() {
    this.reactImmediately(() => this.selectionService.primarySelectedInputData, // (7)
      primarySelectedInputData => {
        this.getTranslation(primarySelectedInputData);
      });
  }

  private async getTranslation(primarySelectedInputData: IndexedInput) {
    if (primarySelectedInputData === null) return;

    const promise = this.apiService.getPigLatin(primarySelectedInputData);      // (8)
    const results = await this.loadLatest('pigLatin', promise);                 // (9)
    if (results === null) return;

    this.pigLatin = results;
  }

  override renderImpl() {                                                       // (10)
    const color = this.colorService.getDatapointColor(
        this.selectionService.primarySelectedInputData);
    return html`
      <div class="results" style=${styleMap({'color': color})}>
        ${this.pigLatin}
      </div>
    `;
  }

  static checkModule(modelSpecs: ModelsMap, datasetSpec: Spec): boolean {       // (11)
    return true;
  }
}

declare global {                                                                // (12)
  interface HTMLElementTagNameMap {
    'demo-module': DemoTextModule;
  }
}
```

The above LitModule, while just a dummy example, illustrates all of the
necessary static properties and many of the most common patterns found in the
LIT app.

### Setup

First, a `LitModule` must declare a static `title` string (1) and `template`
function (2). The `template` function determines how the modules layout renders
the component template and passes in module properties, such as the name of the
`model` this should respond to. (3) specified behavior in model comparison mode;
if duplicate is set to true, the layout engine will create two (or more)
instances of this module, each responsible for a different model.

*Note: there are additional static attributes which control module behavior; see
the
[`LitModule`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/client/core/lit_module.ts)
base class for full definitions.*

Styles are also declared with a static get method (4), following the lit-element
convention. These styles can be built using the lit-element `css` template
function, or by importing a separate .css file. Styles can be shared between
components by importing a shared styles .css file (for instance,
[`shared_styles.css`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/client/lib/shared_styles.css))

Services are used by requesting them from the LitApp `app` singleton class (5).
This can be thought of as a super-simple dependency injection system, and allows
for much easier stubbing / mocking of services in testing. We request the
[`colorService`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/client/services/color_service.ts)
here, but the base `LitModule` class initializes the most common services
([`apiService`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/client/services/api_service.ts),
[`appState`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/client/services/state_service.ts),
and
[`selectionService`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/client/services/selection_service.ts))
for us automatically.

The `LitModule` must also provide a static `checkModule` (11) method, which
determines if this module should display for the given model(s) and dataset.

Finally, the `@customElement('demo-module')` decorator (0) defines this class as
a custom HTML element `<demo-module>`, and (12) ensures this is accessible to
other TypeScript files in different build units.

### Functionality

The above module has a very simple task - When the user selects input data, it
makes a request to an API service to fetch and display a pig latin translation
of the data. Since we're using mobx observables to store and compute our state,
we do this all in a reactive way.

First, since the `LitModule` base class derives from `MobxLitElement`, any
observable data that we use in the `renderImpl` method automatically triggers a
re-render when updated. This is excellent for simple use cases, but what about
when we want to trigger more complex behavior, such as the asynchronous request
outlined above?

The pattern that we leverage across the app is as follows: The `renderImpl`
method (10) accesses a private observable `pigLatin` property (6) that, when
updated, will re-render the template and show the results of the translation
automatically. In order to update the `pigLatin` observable, we need to set up a
bit of machinery. In the lit-element lifecycle method `firstUpdated`, we use a
helper method `reactImmediately` (7) to set up an explicit reaction to the user
selecting data. Whatever is returned by the first function (in this case
`this.selectionService.primarySelectedInputData`) is observed and passed to the
second function immediately **and** whenever it changes, allowing us to do
something whenever the selection changes. Note, another helper method `react` is
used in the same way as `reactImmediately`, in instances where you don't want to
immediately invoke the reaction. Also note that modules should override
`renderImpl` and not the base `render` method as our `LitModule` base class
overrides `render` with custom logic which calls our `renderImpl` method for
modules to perform their rendering in.

We pass the selection to the `getTranslation` method to fetch the data from our
API service. However rather than awaiting our API request directly, we pass the
request promise (8) to another helper method `loadLatest` (9). This ensures that
we won't have any race conditions if, for instance, the user selects different
data rapidly - the function returns `null` when the request being fetched has
been superseded by a more recent call to the same endpoint. Finally, we set the
private `pigLatin` observable with the results of our API request and the
template is automatically rerendered, displaying our data.

This may seem like a bit of work for a simple module, but the pattern of using
purely observable data to declaratively specify what gets rendered is very
powerful for simplifying the logic around building larger, more complex
components.

### Escape Hatches

Finally, it's worth noting that the declarative template-based rendering setup,
while effective for handling most component render logic, is sometimes
inadequate for more advanced visualizations. In particular, the template
approach is not well suited for animations, rapidly changing data, or things
that MUST be done imperatively (such as drawing to a canvas). Fortunately, it's
very easy to "bridge" from declarative to imperative code by leveraging the
lit-element lifecycle methods.

In particular, the `updated` and `firstUpdated` methods are useful for
explicitly doing work after the component has rendered. You can use normal
`querySelector` methods to select elements and update their properties
imperatively (note that you must make selections using the shadow root, not the
document, since we're using isolated web components).

One important caveat is that messing with the actual structure of the rendered
DOM output (such as removing/reordering DOM nodes) **will** cause issues with
lit-element, since it relies on a consistent template output to do its
reconciliation of what needs to be updated per render.

```typescript
// An example of a LITModule imperative "escape hatch"
  updated() {
    const canvas = this.shadowRoot!.querySelector('canvas');
    this.drawCanvas(canvas);
  }

  override renderImpl() {
    return html`<canvas></canvas>`;
  }
```

### Stateful Child Elements

Some modules may contain stateful child elements, where the element has some
internal state that can have an effect on the module that contains it. Examples
of this include any modules that contain the
[core/faceting_control.ts](https://github.com/PAIR-code/lit/blob/main/lit_nlp/client/core/faceting_control.ts)
element.

With these types of child elements, it's important for the containing module to
construct them programmatically and store them in a class member variable, as
opposed to only constructing them in the module's html template string returned
by the `renderImpl` method. Otherwise they will be destroyed and recreated when
a module is hidden off-screen and then brought back on-screen, leading them to
lose whatever state they previously held. Below is a snippet of example code to
handle these types of elements.

```typescript
// An example of a LITModule using a stateful child element.
@customElement('example-module')
export class ExampleModule extends LitModule {
  private readonly facetingControl = document.createElement('faceting-control');

  constructor() {
    super();

    const facetsChange = (event: CustomEvent<FacetsChange>) => {
      // Do something with the information from the event.
    };
    // Set the necessary properties on the faceting-control element.
    this.facetingControl.contextName = ExampleModule.title;
    this.facetingControl.addEventListener(
        'facets-change', facetsChange as EventListener)
  }

  override renderImpl() {
    // Render the faceting-control element.
    return html`${this.facetingControl}`;
  }
```

## Style Guide

*   Please disable clang-format on `lit-html` templates and format these
    manually instead:

    ```ts
    // clang-format off
    return html`
      <div class=...>
        <button id=... @click=${doSomething}>Foo</button>
      </div>
    `;
    // clang format on
    ```

*   For new modules, in most cases you should implement two classes: one module
    (subclassing
    [`LitModule`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/client/core/lit_module.ts))
    that interfaces with the LIT framework, and another element which subclasses
    `LitElement`, `MobxLitElement`, or preferably,
    [`ReactiveElement`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/client/lib/elements.ts?),
    and implements self-contained visualization code. For an example, see
    [modules/annotated_text_module.ts](https://github.com/PAIR-code/lit/blob/main/lit_nlp/client/modules/annotated_text_module.ts)
    and
    [elements/annotated_text_vis.ts](https://github.com/PAIR-code/lit/blob/main/lit_nlp/client/elements/annotated_text_vis.ts).

*   On supported components (`ReactiveElement` and `LitModule`), use
    `this.react()` or `this.reactImmediately()` instead of registering reactions
    directly. This ensures that reactions will be properly cleaned up if the
    element is later removed (such as a layout change or leaving comparison
    mode).

*   Use
    [shared styles](https://github.com/PAIR-code/lit/blob/main/lit_nlp/client/lib/shared_styles.css)
    when possible.

## Development Tips (open-source)

If you're modifying any TypeScript code, you'll need to re-build the frontend.
You can have yarn do this automatically. In one terminal, run:

```sh
cd ~/lit/lit_nlp
yarn
yarn build --watch
```

And in the second, run the LIT server:

```sh
cd ~/lit
python -m lit_nlp.examples.<example_name> --port=5432 [optional --args]
```

You can then access the LIT UI at http://localhost:5432.

If you only change frontend files, you can use <kbd>Ctrl/Cmd+Shift+R</kbd> to do
a hard refresh in your browser, and it should automatically pick up the updated
source from the build output.

If you're modifying the Python backend, there is experimental support for
hot-reloading the LIT application logic (`app.py`) and some dependencies without
needing to reload models or datasets. See
[`dev_server.py`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/dev_server.py) for
details.

You can use the `--data_dir` flag (see
[`server_flags.py`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/server_flags.py) to
save the predictions cache to disk, and automatically reload it on a subsequent
run. In conjunction with `--warm_start`, you can use this to avoid re-running
inference during development - though if you modify the model at all, you should
be sure to remove any stale cache files.

## Custom Client / Modules

The LIT frontend can be extended with custom visualizations or interactive
modules, though this is currently provided as "best effort" support and the API
is not as mature as for Python extensions.

An example of a custom LIT client application, including a custom
(potato-themed) module can be found in
[`lit_nlp/examples/custom_module`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/examples/custom_module).
You need only define any custom modules (subclass of `LitModule`) and include
them in the build.

When you build the app, specify the directory to build with the `env.build`
flag. For example, to build the `custom_module` demo app:

```sh
yarn build --env.build=examples/custom_module
```

This builds the client app and moves all static assets to a `build` directory in
the specified directory containing the `main.ts` file
(`examples/custom_module/build`).

Finally, to serve the bundle, set the `client_root` flag in your python code to
point to this build directory. For this example we specify the build directory
in `examples/custom_module/potato_demo.py`.

```python
# Here, client build/ is in the same directory as this file
parent_dir = os.path.join(pathlib.Path(__file__).parent.absolute()
FLAGS.set_default("client_root", parent_dir, "build"))
```

You must also define a
[custom layout definition](./api.md#customizing-the-layout) in Python which
references your new module. Note that because Python enums are not extensible,
you need to reference the custom module using its HTML tag name:

```python
modules = layout.LitModuleName
POTATO_LAYOUT = layout.LitCanonicalLayout(
    upper={
        "Main": [modules.DatapointEditorModule, modules.ClassificationModule],
    },
    lower={
        "Data": [modules.DataTableModule, "potato-module"],
    },
    description="Custom layout with our spud-tastic potato module.",
)
```

See
[`potato_demo.py`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/examples/custom_module/potato_demo.py)
for the full example.

================
File: website/sphinx_src/getting_started.md
================
# Getting Started with LIT

<!--* freshness: { owner: 'lit-dev' reviewed: '2024-06-25' } *-->

<!-- [TOC] placeholder - DO NOT REMOVE -->

## Installation

Using pip:

```sh
pip install lit-nlp
```

For more details or to install from source, see
[GitHub](https://github.com/pair-code/lit#download-and-installation).

## Hosted demos

If you want to jump in and start playing with the LIT UI, check out
https://pair-code.github.io/lit/demos/ for links to our hosted demos.

For a guide to the many features available, check out the
[UI guide](./ui_guide.md) or this
[short video](https://www.youtube.com/watch?v=j0OfBWFUqIE).

## LIT with your model <!-- DO NOT REMOVE {#custom-demos} -->

LIT provides a simple [Python API](./api.md) for use with custom models and
data, as well as components such as metrics and counterfactual generators. Most
LIT users will take this route, which involves writing a short `demo.py` binary
to link in `Model` and `Dataset` implementations and configure the server. In
most cases this can be just a few lines:

```python
  datasets = {
      'foo_data': FooDataset('/path/to/foo.tsv'),
      'bar_data': BarDataset('/path/to/bar.tfrecord'),
  }
  models = {'my_model': MyModel('/path/to/model/files')}
  lit_demo = lit_nlp.dev_server.Server(models, datasets, port=4321)
  lit_demo.serve()
```

Check out the [API documentation](./api.md#adding-models-and-data) for more, and
the [demos directory](./demos.md) for a wealth of examples. The
[components guide](./components.md) also gives an overview of interpretability
methods and other features available in LIT, and describes how to enable each
for your task.

## Using LIT in notebooks <!-- DO NOT REMOVE {#colab} -->

LIT can also be used directly from Colab and Jupyter notebooks, with the LIT UI
rendered in an output cell. See [LIT_sentiment_classifier.ipynb](https://colab.research.google.com/github/pair-code/lit/blob/dev/lit_nlp/examples/notebooks/LIT_sentiment_classifier.ipynb) for an example.

Note: if you see a 403 error in the output cell where LIT should render, you may
need to enable cookies on the Colab site, or pass a custom `port=` to the
`LitWidget` constructor.

## Stand-alone components <!-- DO NOT REMOVE {#standalone} -->

Many LIT components - such as models, datasets, metrics, and salience methods -
are stand-alone Python classes and can be easily used outside of the LIT UI. For
additional details, see the
[API documentation](./api.md#using-lit-components-outside-of-lit) and an example Colab
at [LIT_components_example.ipynb](https://colab.research.google.com/github/pair-code/lit/blob/dev/lit_nlp/examples/notebooks/LIT_components_example.ipynb).

## Run an existing example <!-- DO NOT REMOVE {#running-lit} -->

The [demos page](./demos.md) lists some of the pre-built demos available for a
variety of model types. The code for these is under [examples](https://github.com/PAIR-code/lit/blob/main/lit_nlp/examples)
;
each is a small script that loads one or more models and starts a LIT server.

Most demos can be run with a single command. To run the default one, you can do:

```sh
python -m lit_nlp.examples.glue.demo \
  --quickstart --port=4321 --alsologtostderr
```

Then navigate to https://localhost:4321 to access the UI.

For most models we recommend using a GPU, though the `--quickstart` flag above
loads a set of smaller models that run well on CPU. You can also pass
`--warm_start=1.0`, and LIT will run inference and cache the results before
server start.

For an overview of supported model types and frameworks, see the
[components guide](./components.md).

================
File: website/sphinx_src/glossary.md
================
# Glossary

There are a few commonly-overloaded terms which refer to specific things in the
LIT APIs and codebase:

*   **Component**, a backend component in Python. Includes things like
    counterfactual generators, metrics classes, UMAP and PCA implementations,
    and salience methods.
*   **Element**, a Web Component or another HTML element. The `client/elements/`
    folder contains many custom elements used for visualizations and parts of
    the UI, but which are not full-fledged LIT Modules.
*   **Example** or **Datapoint**, an element of a dataset - the things that we
    feed to models and get predictions back.
*   **Instance**, a specific implementation of LIT (e.g. a demo.py binary) or
    server job running the former.
*   **LIT**, the Learning Interpretability Tool. Always fully capitalized,
    sometimes accompanied by a  emoji. Pronounced "lit", not "ell-eye-tee".
    Formerly known as the Language Interpretability Tool.
*   **Lit**, the web framework consisting of
    [lit-element](https://lit-element.polymer-project.org/guide) and
    [lit-html](https://lit-html.polymer-project.org/guide) and maintained by the
    Polymer project. LIT is built on this framework, but the naming is
    coincidental (we like it, of course).
*   **Model**, a machine-learning model that we're exploring or debugging with
    LIT. Doesn't need to be a single neural network; could be a pipelined or
    composite system, and may be hosted remotely.
*   **Module**, a frontend visualization module (see
    [Frontend Dev Guide](./frontend_development.md)). Strictly speaking, this is
    something that inherits from LitModule, renders a part of the UI, and
    interacts with the frontend framework. Usually found in `client/modules/`.
    All modules are elements, but not all elements are modules.
*   **Potato** (noun or verb), a frontend error. See
    [potato.io](https://potato.io/).
*   **Server**, the Python backend. A WSGI application that provides a handful
    of HTTP endpoints to serve models, datasets, and other components.
*   **Service**, a part of the frontend framework that handles state and
    provides helper methods. Most of these are global singletons, with the
    notable exception of SelectionService which is duplicated when in
    example-comparison model.
*   **Slice**, a set of examples from a dataset. Often created by **Faceting**
    along a specific feature.
*   **Widget** and **Widget Group**, elements of the frontend layout. A Widget
    is a thin wrapper over a Module, and a Widget Group contains one or more
    widgets along with header bars, resize, and minimize/maximize controls -
    roughly, like a regular GUI window. Sometimes we refer to a widget or a
    widget group as a **Panel**.

================
File: website/sphinx_src/index.md
================
# Learning Interpretability Tool (LIT)

<!--* freshness: { owner: 'lit-dev' reviewed: '2024-10-25' } *-->

<!-- [TOC] placeholder - DO NOT REMOVE -->

Welcome to LIT, the Learning Interpretability Tool!

If you want to jump in and start playing with the LIT UI, check out the hosted demos at https://pair-code.github.io/lit/demos/.

## Research

Found LIT useful in your research? Please cite our
[system demonstration paper](https://aclanthology.org/2020.emnlp-demos.15/)!

```
@misc{tenney2020language,
    title={The Language Interpretability Tool: Extensible, Interactive Visualizations and Analysis for {NLP} Models},
    author={Ian Tenney and James Wexler and Jasmijn Bastings and Tolga Bolukbasi and Andy Coenen and Sebastian Gehrmann and Ellen Jiang and Mahima Pushkarna and Carey Radebaugh and Emily Reif and Ann Yuan},
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    year = "2020",
    publisher = "Association for Computational Linguistics",
    pages = "107--118",
    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.15",
}
```


```{toctree}
   :maxdepth: 2

   Main Site <https://pair-code.github.io/lit/>
   Getting Started <getting_started.md>
   Examples <demos.md>
   UI Guide <ui_guide.md>
   Components & Features <components.md>
   Python API <api.md>
   Frontend Development <frontend_development.md>
   Running in Docker <docker.md>
   Glossary <glossary.md>
   FAQ <faq.md>
   GitHub <https://github.com/pair-code/lit>
```

================
File: website/sphinx_src/ui_guide.md
================
# UI Guide

<!--* freshness: { owner: 'lit-dev' reviewed: '2024-06-24' } *-->

This is a user guide for the Learning Interpretability Tool (LIT) UI.

For a quick video tour of LIT, check out this
[video](https://www.youtube.com/watch?v=CuRI_VK83dU).

<!-- [TOC] placeholder - DO NOT REMOVE -->

## General Layout

LIT lives inside a single page web application, comprised of multiple toolbars
and a main section consisting of individual modules. Modules will automatically
display if they are applicable to the current model and dataset; for example,
the module that shows classification results will only show if the model returns
`MulticlassPreds`.

![LIT overall UI](./images/lit-ui.png "LIT overall UI")

LIT's layout consist of as many as three sections, described in the API docs for
[custom layouts](./api.md#customizing-the-layout). When the layout provides more than one
major content section, they are separated by draggable dividers that are built
into LIT's toolbars (for allocating vertical space) or in the space between
sections and modules (for allocating horizontal space). Any section may include
multiple tabs, where each tab contains a collection of modules. LIT's
[pre-configured layouts](./frontend_development.md#layout) group modules into
tabs based on analytical task (e.g., metrics analysis vs. input salience
visualization vs. counterfactual example generation), but you can adopt whatever
organizational scheme you desire in your custom layouts.

### Layout Options
<!--
  TODO: Add an image of the 3 LIT layouts side by side.
-->

LIT provides three pre-configured layouts:

*   `simple`: A minimalist layout with the examples on top (either individually
    (selected by default) or in a table) and predictions on the bottom;
*   `default`: The original LIT layout with a single group of modules on top for
    exploring and selecting data, and a collection of tabs supporting different
    analytical tasks on the bottom; and
*   `three_panel`: A three-panel layout that puts exploratory data
    visualizations at full-page height on the left, tools for inspecting and
    manipulating examples and their associated predictions in the upper right,
    and a collection of tabs supporting different analytical tasks in the lower
    right. Note that this was introduced in v1.0 as an experimental feature,
    your feedback is appreciated.

## Datapoint Selections

LIT displays a loaded dataset and its model results across the set of selected
models. Users can dive into detailed results by selecting datapoints from the
dataset.

![LIT datapoint selection](./images/lit-datapoint-selection.png "LIT datapoint selection")

LIT provides two levels of precision for selections. The first is the current
selection, which consists of one or more datapoints that are selected through
one of the interactive modules (such as the *Data Table*, *Embeddings*,
*Scalars*, or *Confusion Matrix* module). When a set of datapoints is selected
in a module, this selection is reflected across all other modules, along with
the selection toolbar. For example, the *Metrics* module shows model metrics not
just across the entire dataset, but also for the current selection of
datapoints.

The second is the primary selection. This is a single datapoint within the
current selection that is being explored in more detail in modules that focus on
a single datapoint (such as the *Datapoint Editor* and *Salience Maps* modules).
If the current selection only consists of a single datapoint, then that
datapoint is also the primary selection. If the current selection consists of
multiple datapoints, the primary selection defaults to the first datapoint in
that selection but can be changed through the arrow controls in the selection
toolbar or by clicking another datapoint in the selection. The primary selection
is highlighted in a darker blue in the *Data Table* module and its ID is
displayed in the selection toolbar.

A selection of datapoints can be saved as a "slice" through the
*[Slice Editor](#slices)*. Saving a selection as a slice allows for easy
navigation back to that selection in the future. It also allows for comparison
of metrics across subsets of datapoints, as described in the
*[Metrics Module](#metrics-table)* section.

## Toolbars

There are three toolbars provided in LIT. The top bar includes the selected
model(s) and dataset, a settings button, and URL sharing functionality. Below
that is the main toolbar with the menus and controls for navigation and
selection. At the bottom of the page is a status bar.

![LIT toolbars](./images/lit-toolbars.png "LIT toolbars")

### Top Bar

#### Global Settings

The global settings dialog is accessible through the **"Configure"** button in
the top bar.

LIT can be launched with a set of models and datasets. The settings screen
allows users to select which models to analyze. Any number of models can be
analyzed together, assuming they are compatible in the input data format they
use (i.e. two different toxicity classifiers can be analyzed together for
comparison). Once a model or models is selected, you can then select from any
dataset compatible with those models.

The settings dialog also contains controls switching the layout of the tool.
This can help declutter the UI when analysis doesn't require all of the
compatible modules that LIT contains.

![LIT global settings](./images/lit-settings.png "LIT global settings")

#### URL Sharing

Much of the LIT app's state &mdash; the loaded models and datasets, selected
datapoints, minimized and/or full-screen modules &mdash; is stored in URL
parameters. The **"Copy Link"** button in the top bar allows a user to share
their specific LIT view and setup with someone else. The URL can also be copied
manually from the address bar.

The base url that will be copied with the **"Copy Link"** button can be
configured by passing the `--canonical_url=<url base>` flag to the server.

### Main Toolbar

The main toolbar is right below the top bar and contains a number of different
controls and information. The left side of the toolbar contains a set of menus
for quickly controlling datapoint selection and coloring. This includes the
following controls:

*   The **"Select datapoint"** menu provides a drop-down of several options:
    *   the **"Random"** option selects a random datapoint,
    *   the **"All related"** option adds any datapoints "related" to the
        current selection. In LIT, "related" is defined as datapoints created
        from some source datapoint (through manual editing or a datapoint
        generator), or a source datapoint that a selected datapoint was created
        from,
    *   the **"Parents"** option adds the source datapoints that the selected
        datapoints were created from,
    *   the **"Children"** option adds the datapoints created from the selected
        datapoints (through manual editing or a datapoint generator),
    *   the **Slices** option allows quick selection of an already-created slice
        of datapoints,
    *   the **"Clear selection"** button deselects all selected datapoints.
*   The **"Color by"** menu enables setting of the color of each datapoint in
    the modules that visualize all datapoints (such as the *Embeddings* and
    *Scalars* modules) by any number of datapoint features or model outputs on
    those datapoints (such as coloring by some categorical input feature, or by
    prediction error for a regression task).
*   The **Slices** menu allows adding/selecting/removing slices of datapoints.

Next to the menus is a button for pinning/unpinning a datapoint. Pinning a
datapoint puts LIT into datapoint comparison mode, where two datapoints can be
compared against each other, across all applicable modules. This mode is
described in more detail [below](#comparing-datapoints).

The right side of the toolbar displays how many datapoints are in the loaded
dataset and how many of those are currently selected. If only a single datapoint
is selected, the left and right arrow buttons in this toolbar allow cycling of
the selected datapoint through the loaded dataset. If the current selection is a
set of datapoints, then the left and right arrow buttons control which of those
datapoints is the primary selected datapoint, cycling through the datapoints in
the current selection. A **"Select random"** button allows selection of a random
datapoint, as opposed to the ordered cycling done through the left and right
arrows.The **"Select all"** and **"Clear selection"** buttons are also provided
to easily select all or none of the datapoints, respectively.

### Status Bar

The status bar at the bottom of the tool contains a text area on the left side.
If the tool is currently waiting on the results of a call to the backend (such
as for running predictions or getting embeddings), this information will be
displayed in the status bar along with an indeterminant progress bar showing
that a result is pending. If a call to the backend fails, information about the
failure will be displayed in this area in red to call out the error, and that
information will persist in the status bar until the user clicks the **"x"**
button by the error to clear the status display. The full error log can also be
displayed by clicking the error icon in the message.

## Comparing Models

By loading more than one model in the global settings controls, LIT can compare
multiple models. A subset of modules that show per-model information are then
duplicated to allow easy comparison across two models. Other modules, such the
*Embeddings* and *Metrics* modules are updated to show information from all
models.

![LIT model comparison](./images/lit-model-compare.png "LIT model comparison")

## Comparing Datapoints

Pinning a datapoint, through either the toolbar button or controls in modules
(e.g., the pin icons in Data Table rows), puts LIT into **datapoint comparison
mode**. In this mode, the pinned datapoint is used as a reference to compare the
primary selection. The pinned datapoint is indicated by a pin icon in modules
that support datapoint comparison, such as the Data Table. Any changes to the
primary selection will update datapoint comparison visualizations in all
supporting modules.

As with model comparison, some modules may be duplicated, one showing the pinned
datapoint and one showing the primary selected datapoint.

This allows for easy comparison of model results on a datapoint to any generated
counterfactual datapoints, or any other datapoint from the loaded dataset.

![LIT datapoint comparison](./images/lit-datapoint-compare.png "LIT datapoint comparison")

## Slices

The *Slice Editor* allow users to create, edit, select, and delete slices. The
current selection can be saved as a slice by giving it a name and clicking
"Create slice". The slice list allows you to select any of the previously-saved
slices. This includes the "Starred" slice that is described above in the
[Main Toolbar](#main-toolbar) section.

The feature checkboxes enable the user to facet the data by input feature when
creating a slice. In the screenshot below, we are creating a new slice named
"interesting", and have selected the checkbox to facet by the "label" feature.
In this example, the "label" feature is a feature in the dataset that for each
datapoint describes which ground truth class it belongs to for some
classification task (either "0" or "1" for this binary classification example).
So, by creating a slice with this checkbox enabled, the tool will actually
create two slices: one named "interesting label:0" for datapoints with their
label set to 0, and one named "interesting label:1" for those with their label
set to "1".

![LIT slice controls](./images/lit-slices.png "LIT slice controls")

## Module Details

This section contains details on using and interacting with individual modules
that are built into LIT. Note that this list may not be complete and additional
modules can be created and used in LIT by clients.

All modules can be toggled to be shown full-screen through use of the
full-screen button in the top-right of each module.

### Embedding Projector

When using LIT with a model that returns embeddings (or activations) in addition
to predictions, the embedding projector will show all datapoints by their
embeddings projected down to 3 dimensions. This is useful for exploring and
understanding clusters of datapoints.

![LIT embeddings](./images/lit-embeddings.png "LIT embeddings"){w=500px align=center}

The specific embedding used to generate the projection can be selected in a
dropdown, along with the method of projection (either UMAP or PCA). An
additional drop-down allows changing of the datapoint feature used for the label
of each datapoint. The labels are shown on datapoint hover or click.

The visualization can be rotated through click-and-drag interaction, and panned
through control+click-and-drag. A datapoint can be selected with a click, or a
set of datapoints can be selected using a lasso through a shift+click-and-drag
interaction.

The color of the datapoints is controlled by the color settings in the selection
toolbar.

### Data Table

The data table shows all datapoints in a simple table. Datapoints can be
selected or unselected through a click. Shift+click allows selecting a set of
consecutive datapoints, and control+click allows selecting a set of individual
datapoints one at a time. Currently selected datapoints are highlighted with a
light blue background. The primary selected datapoint is highlighted with a
darker blue background. If a set of datapoints is currently selected, clicking
on a single datapoint in that set will change it to be the primary selected
datapoint without changing the overall set of selected datapoints.

The default sort order shows datapoints in the order they were loaded from the
dataset, but with newly-generated datapoints being placed directly below their
"source" datapoint, instead of at the end of the table.

The sort order can be changed to sort by columns through use of the up and down
arrows in the table header row. Additionally, the data table can be filtered
through text, regex, numerical ranges, and column-name prefixes using a global
search box. The table can also be filtered by column through a text search using
the search buttons for each column in the header row. All columns that have
filters set on them have their search button outlined. Clicking the **"x"**
button in the search box for a column will clear that column's filter.

The **"show selected"** checkbox toggles the data table to only show the
datapoints that are currently selected.

The **"show generated"** checkbox toggles the data table to only show generated
datapoints, that is, the datapoints that have been added through modules such as
the *Datapoint Editor* or the *Counterfactual Generators*.

The **"reset view"** button returns the data table to its standard, default
view.

A **"columns"** drop-down allows showing/hiding of specific columns to customize
what the data table shows. Model predictions can be added as columns through
this dropdown, but they are not shown in the data table by default, in order to
keep the table decluttered.

Column names that exceed the maximum length are truncated with an ellipsis to
the left, and can be viewed in their entirety when hovered over. Similarly,
table cells that exceed 3 lines of text are truncated with a Show More icon,
which can be clicked to view the full content. Text cells can be collapsed to
their default state using the **"reset view"** button.

The below data table shows one sorted by the "label" field, with the "sentence"
field being filtered to show only those datapoints that contain the word "film"
in them.

![LIT data table](./images/lit-datatable.png "LIT data table"){w=500px align=center}

A datapoint can be pinned to enable comparison by clicking the pin icon on the
left side of the datapoint's table entry when the datapoint is hovered over or
selected. A pinned datapoint can be unpinned by clicking on its pin icon again.
Similarly, a datapoint can be starred and unstarred by clicking the neighboring
star icon. Starred datapoints are tracked in an automatically generated Starred
slice for convenience.

You can also export data to CSV using the copy or download buttons in the bottom
right:

![LIT data table](./images/lit-datatable-export.png "LIT data table export controls"){w=400px align=center}

This will export all data in the current table view. To export only the
selection, use the "Show only selected" toggle. To include additional columns
such as model predictions, enable them from the "Columns" dropdown.

### Datapoint Editor

The datapoint editor shows the details of the primary selected datapoint, if one
is selected. Any field can be edited, and a new datapoint created with those
edits through the **"Add"** button. Any edit to an existing datapoint must be
saved as a new datapoint to be explored, to keep datapoints immutable for
simplicity of use.

When no datapoint is selected, the editor shows a blank datapoint that can be
filled out by hand to create a completely new datapoint.

Features shown with a "(\*)" next to their name are required as model input and
must be filled out to create a new datapoint. Other fields are optional.

![LIT datapoint editor](./images/lit-datapoint-editor.png "LIT datapoint editor"){w=500px align=center}

### Datapoint Generator

The datapoint generator module allows creation of new datapoints from all
currently-selected datapoints (or the entire dataset if no datapoints are
selected) through a set of counterfactual datapoint generators. These generators
are provided by the backend and all available generators will show up as buttons
in the module. Clicking one of these buttons causes the creation of new
datapoints that are displayed in a table inside the module and can be added to
the dataset either individually, or altogether, through the add buttons.

Generators built into LIT include:

*   **Scrambler**: Scrambles the words in a text feature randomly.
*   **Back-translation**: Translates a text feature into other languages and
    then back to the source language to create paraphrases of the initial text
    feature.
*   **Hotflip**: When analyzing a classification task and the model provides
    token-based gradients, this generator will change the token with the highest
    influence on the prediction to the token with the most opposite influence.
*   **Word replacer**: Provides a text box to define a comma-separated set of
    replacements to perform (such as "great -> terrible, hi -> hello").
    Counterfactual datapoints are created for any datapoint found that contains
    the source word, with it replaced with the provided result word. Word
    replacer also supports multiple targets per word with "|" separator. For
    example, "great -> terrible | bad" will produce two outputs where "great" is
    replaced with "terrible" and "bad".

The non-text fields in the generated datapoints can be edited before adding them
to the dataset. This is important in case some datapoint feature is no longer
correct after the counterfactual generation. For example, in a sentiment
classifier, if you used the word replacer generator to replace the word "good"
with "terrible" in the input "this movie is good", then you probably want to
change the ground truth sentiment of that datapoint from 1 to 0 before you add
it to your dataset for analysis.

![LIT datapoint generator](./images/lit-datapoint-generator.png "LIT datapoint generator")

### Metrics Table

The metrics table shows model metrics for each model in a table format. The
exact metric types are determined by the python metrics component that
calculates metrics given the model types being evaluated. These can include
measures such as accuracy (for classifiers), error (for regression tasks), and
BLEU score (for translation tasks). By default, the measures are calculated and
shown for the entire dataset, and also for the current selection. Additionally,
through the **"show slices"** checkbox, the metrics table can calculate and
display metrics for each saved slice as well.

There is also a **"Facet by"** set of dataset feature checkboxes; one checkbox
for each feature in the dataset that results can be faceted by. When one or more
of these are checked, the dataset (or current selection, if there is one) is
faceted into sub groups for each of the calculated buckets, and metrics are
displayed for those subsets of the datapoints of interest. This could be used,
for example, to compare metrics for a toxicity classifier, broken down by
gender, assuming the dataset has a categorical gender feature in it.

The below screenshot shows the metrics table with metrics for the entire
dataset, for the dataset faceted into datapoints with label 0 and with label 1,
and also for two named slices that have been created by a user.

![LIT metrics](./images/lit-metrics.png "LIT metrics")

### Confusion Matrix

The confusion matrix buckets all datapoints from the dataset (or the current
selection, if one is made) into buckets in a 2D matrix. This is normally used to
compare classification predictions on a model versus the ground truth classes of
the datapoints. In this case, the axes of the matrix are configurable to be set
to any categorical field in the dataset or return from a model. For example,
when comparing two models, the confusion matrix can be set up to show
agreements/disagreements between classifications in the two models, as opposed
to agreements/disagreements between one model's classifications and the ground
truth.

The individual cells and the row and column headers are all clickable to toggle
on/off selection of the datapoints in that cell or row or column. In this way,
the confusion matrix module can be used to select points of interest, such as
all false positives in a binary classification task, or all datapoints where two
models being compared disagree on classification.

![LIT confusion matrix](./images/lit-conf-matrix.png "LIT confusion matrix")

### Scalars

The scalars module shows a set of scatter or jitter plots, one for each scalar
output of a loaded model (such as a regression score, or a classification score
for a specific class). Each of them contains all datapoints in the dataset, laid
out horizontally by the score. For classification scores, the Y axis is a random
jitter of the data to better view all datapoints. For regression scores, where
ground truth is known, the Y axis is the error in the prediction (points below
the x-axis are under-predicted).

Datapoints can be selected either though clicking, or through lasso selection by
clicking and dragging.

The color of the datapoints is controlled by the color settings in the selection
toolbar.

For binary classification tasks, this module also contains a threshold slider in
order to change the positive classification threshold at which datapoints are
classified as being in the positive class. This slider value defaults to 0.5.

For multi-class classification tasks where a null index (default class) is set
in the model specifications, this module also contains a margin slider for the
non-default classes, to control how high a classification score must be in that
class before a datapoint is classified as that class as opposed to the default
class. The margin value defaults to 0, meaning the class with the highest score
is the class the datapoint is inferred to be.

![LIT prediction scores](./images/lit-pred-score.png "LIT prediction scores")

### Model Output

Model output modules show the result of a model on the primary selected
datapoint. The visuals of these modules depend on the model task being
performed. For a simple classification task, it will show the class scores from
the model, the predicted class, and, if ground truth is available in the
dataset, it will also show the ground truth classification.

![LIT classification results](./images/lit-classification-results.png "LIT classification results")

For structured prediction tasks like span labeling, a span graph module can
display all tagged spans returned by the model, along with a visualization of
the ground truth spans if one is available in the dataset.

![LIT structured prediction](./images/lit-structured-prediction.png "LIT structured prediction"){w=800px align=center}

### Salience Maps

Salience maps show the influence of different parts of inputs features on a
model's prediction on the primary selection. This module can contain multiple
methodologies for calculating this salience, depending on the capabilities of
the model being analyzed (e.x. if the model provides gradients, then
gradient-based token-wise salience can be calculated and displayed -- see
[adding models and data](api.md#adding-models-and-data) for more). The
background of each text piece is colored by the salience of that piece on the
prediction, and hovering on any piece will display the exact value calculated
for that piece.

There is an **"autorun"** button by each methodology on the right side of the
bar (the methodoloy name is on the left side). If it is checked, then that
calculation is made when a new primary datapoint is selected. If it is
unchecked, the calculation isn't made until it is checked. This can be valuable
so that expensive, long-running saliency calculations (such as LIME) aren't
performed on every datapoint selection, but only when explicitly asked for.

![LIT saliency maps](./images/lit-salience.png "LIT saliency maps")

## User Journeys

In this section, we explore some example user journeys and how LIT enables them.

### Sentiment Analysis

How well does a sentiment classifier handle negation? We load the development
set of the Stanford Sentiment Treebank, and use the search function in LITs
data table to find the 56 datapoints containing the word not. Looking at the
*Metrics* Table, we find that surprisingly, our BERT model gets 100% of these
correct! But we might want to know if this is truly robust. With LIT, we can
select individual datapoints and look for explanations. For example, take the
negative review, Its not the ultimate depression-era gangster movie.. As
shown in the screenshot below, salience maps suggest that not and ultimate
are important to the prediction.

We can verify this by creating modified inputs, using LITs *Datapoint Editor*.
Removing not gets a strongly positive prediction from Its the ultimate
depression-era gangster movie., while replacing ultimate to get Its not the
worst depression-era gangster movie. elicits a mildly positive score from our
model.

![Sentiment analysis](./images/lit-sentiment-analysis.png "Sentiment analysis")

### Sequence salience

Sequence salience generalizes token-based salience to text-to-text models,
allowing you to explain the impact of the prompt tokens on parts of the model
output.

Check out [here](components.md#sequence-salience) for more details on how to
navigate the Sequence Salience UI module.

================
File: website/src/demos.md
================
---
title: LIT - Demos
layout: layouts/sub.liquid

hero-image: /assets/images/LIT_Demos_Banner.png
hero-title: "Take LIT for a spin!"
hero-copy: "Get a feel for LIT in a variety of hosted demos."

color: "#49596c"
---

<div class="mdl-cell--8-col mdl-cell--8-col-tablet mdl-cell--4-col-phone">
  <div class="mdl-grid no-padding">
  {%  include partials/demo-card,
      c-title: "Tabular data",
      link: "/demos/penguins.html",
      c-data-source: "Palmer Penguins",
      c-copy: "Analyze a tabular data model with LIT, including exploring partial dependence plots and automatically finding counterfactuals.",
      tags: "tabular, binary classification",
      external:"true" %}

  {%  include partials/demo-card,
      c-title: "Classification and regression models",
      link: "/demos/glue.html",
      c-data-source: "Stanford Sentiment Treebank,  Multi-Genre NLI Corpus, Semantic Textual Similarity Benchmark"
      c-copy: "Use LIT with any of three tasks from the General Language Understanding Evaluation (GLUE) benchmark suite. This demo contains binary classification (for sentiment analysis, using SST2), multi-class classification (for textual entailment, using MultiNLI), and regression (for measuring text similarity, using STS-B).",
      tags: "BERT, binary classification, multi-class classification, regression",
      external:"true" %}

  {%  include partials/external-demo-card,
      c-title: "Notebook usage",
      link: "https://colab.research.google.com/github/PAIR-code/lit/blob/main/lit_nlp/examples/notebooks/LIT_sentiment_classifier.ipynb",
      c-data-source: "Stanford Sentiment Treebank"
      c-copy: "Use LIT directly inside a Colab notebook. Explore binary classification for sentiment analysis using SST2 from the General Language Understanding Evaluation (GLUE) benchmark suite.",
      tags: "BERT, binary classification, notebooks",
      external:"true" %}
  </div>
</div>

================
File: website/src/demos/glue.md
================
---
layout: "layouts/redirect.liquid"
permalink: "/demos/glue.html"
url: "http://34.160.227.66/"
---

================
File: website/src/demos/penguins.md
================
---
layout: "layouts/redirect.liquid"
permalink: "/demos/penguins.html"
url: "http://34.149.94.130/"
---

================
File: website/src/index.md
================
---
title: Learning Interpretability Tool
layout: layouts/main.liquid
---

<div class="mdl-cell--8-col mdl-cell--12-col-tablet mdl-cell--8-col-phone">

{% include partials/display1 text:"The Learning Interpretability Tool (LIT) is a <strong>visual, interactive ML model-understanding</strong> tool that supports text, image, and tabular data." %}

{% include partials/home-cta-button text:"Take a tour", link:"/tutorials/tour" %}
{% include partials/home-cta-button text:"Setup LIT", link:"/documentation/getting_started.html" %}

{% include partials/spacer height:60 %}

</div>

![overview of LIT]({% root %}/assets/images/lit-tweet.gif)

{% include partials/spacer height:60 %}

<div class="mdl-cell--8-col mdl-cell--12-col-tablet mdl-cell--8-col-phone">

The Learning Interpretability Tool (LIT) is for researchers and practitioners looking to understand NLP model behavior through a visual, interactive, and extensible tool.

Use LIT to ask and answer questions like:
- What kind of examples does my model perform poorly on?
- Why did my model make this prediction? Can it attribute it to adversarial behavior, or undesirable priors from the training set?
- Does my model behave consistently if I change things like textual style, verb tense, or pronoun gender?

LIT contains many built-in capabilities but is also customizable, with the ability to add custom interpretability techniques, metrics calculations, counterfactual generators, visualizations, and more.

In addition to language, LIT also includes preliminary support for models operating on tabular and image data. For a similar tool built to explore general-purpose machine learning models, check out the [What-If Tool](https://whatif-tool.dev).

LIT can be run as a standalone server, or inside of python notebook environments such as Colab, Jupyter, and Google Cloud Vertex AI Notebooks.
</div>

{% include partials/spacer height:50 %}

{% include partials/display2 text:"Flexible and powerful model probing" %}

<div class="mdl-grid no-padding">

{% include partials/one-of-three-column title:"Built-in capabilities", text: "

Salience maps

Metrics calculations

Counterfactual generation

Model and datapoint comparison

Embedding visualization

TCAV

And more...

" %}
{% include partials/one-of-three-column title:"Supported task types", text: "

Classification

Regression

Text generation / seq2seq

Masked language models

Span labeling

Multi-headed models

Image and tabular data

And more...

" %}
{% include partials/one-of-three-column title:"Framework agnostic", text: "

TensorFlow 1.x

TensorFlow 2.x

PyTorch

Notebook compatibility

Custom inference code

Remote Procedure Calls

And more...

" %}

</div>

{% include partials/spacer height:50 %}

## What's the latest

<div class="mdl-grid no-padding">
  {% include partials/home-card image: '/assets/images/LIT_Updates.png',
      action: 'UPDATES',
      title: 'Version 1.2',
      desc: 'Input salience for text-to-text LLMs, with wrappers for HuggingFace Transformers and KerasNLP models.',
      cta-text:"See release notes",
      link: 'https://github.com/PAIR-code/lit/blob/main/RELEASE.md'
      external:"true" %}

  {% include partials/home-card image: '/assets/images/LIT_Contribute.png',
      action: 'DOCS',
      title: 'Documentation',
      desc: 'LIT is open-source and easily extensible to new models, tasks, and more.',
      cta-text:"View documentation",
      link: '/documentation/',
      external:"true" %}

  {% include partials/home-card image: '/assets/images/LIT_Paper.png',
      action: 'RESEARCH',
      title: 'Demo Paper at EMNLP 20',
      desc: 'Read about what went into LIT in our demo paper, presented at EMNLP 20.',
      cta-text:"Read the paper",
      link: 'https://www.aclweb.org/anthology/2020.emnlp-demos.15.pdf'
      external:"true" %}

</div>

================
File: website/src/tutorials.md
================
---
title: LIT - Tutorials
layout: layouts/sub.liquid

hero-height: 245
hero-image: /assets/images/LIT_Tutorials_Banner.png
hero-title: "Model probing for understandable, reliable, and fair NLP"
hero-copy: "Learn how to navigate LIT and use it to analyze different types of models. "

color: "#fef0f7"
---

<div class="mdl-cell--8-col mdl-cell--8-col-tablet mdl-cell--4-col-phone">

<a name="basics"></a>

## Discover the basics

{% include partials/tutorial-link-element c-title: "A Tour of LIT", link: "/tutorials/tour",
c-copy: "Get familiar with the interface of the Learning Interpretability Tool." %}

{% include partials/spacer height:50 %}

<a name="analysis"></a>

## Conducting analysis in LIT

{% include partials/tutorial-link-element c-title: "Prompt Engineering with Sequence Salience", link: "/tutorials/sequence-salience",
c-copy: "Learn how to debug and iterate on prompt designs in LIT with the Sequence Salience module." %}

{% include partials/tutorial-link-element c-title: "Salience Maps for Text", link: "/tutorials/text-salience",
c-copy: "Learn how to use salience maps for text data in LIT." %}

{% include partials/tutorial-link-element c-title: "Tabular Feature Attribution", link: "/tutorials/tab-feat-attr",
c-copy: "Learn how to use the Kernel SHAP based Tabular Feature Attribution module in LIT." %}

{% include partials/tutorial-link-element c-title: "Global Model Analysis with TCAV", link: "/tutorials/tcav",
c-copy: "Learn about examining model behavior through user-curated concepts." %}

{% include partials/tutorial-link-element c-title: "Exploring a Sentiment Classifier", link: "/tutorials/sentiment",
c-copy: "Learn about how we used LIT to analyze a sentiment classifier." %}

{% include partials/spacer height:50 %}

</div>

================
File: website/src/tutorials/sentiment.md
================
---
title: Exploring a Sentiment Classifier
layout: layouts/tutorial.liquid

hero-image: /assets/images/sample-banner.png
hero-title: "Sentiment Analysis"
hero-copy: "Learn about how we used LIT to analyze a sentiment classifier."

bc-anchor-category: "analysis"
bc-category-title: "Analysis"
bc-title: "Sentiment"

time: "3 minutes"
takeaways: "Learn about how the metrics table and saliency maps assisted an analysis of a sentiment classifier's performance when dealing with negation."
---

## Exploring a Sentiment Classifier

{% include partials/link-out link: "../../demos/glue.html", text: "Explore this demo yourself." %}

Or, run your own with [`examples/glue/demo.py`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/examples/glue/demo.py)

How well does a sentiment classifier handle negation? We can use LIT to interactively ask this question and get answers. We loaded up LIT the development set of the Stanford Sentiment Treebank (SST), which contains sentences from movie reviews that have been human-labeled as having a negative sentiment (0), or a positive sentiment (1). For a model, we are using a BERT-based binary classifier that has been trained to classify sentiment.

Using the search function in LITs data table, we find the 67 datapoints containing the word not. By selecting these datapoints and looking at the Metrics Table, we find that our BERT model gets 91% of these correct, which is slightly higher than the accuracy across the entire dataset.


{% include partials/inset-image image: '/assets/images/lit-metrics-not.png',
  caption: 'Above: A comparison of metrics on datapoints containing "not" versus the entire dataset.'%}

But we might want to know if this is truly robust. We can select individual datapoints and look for explanations. For example, take the negative review, Its not the ultimate depression-era gangster movie.. As shown below, salience maps suggest that not and ultimate are important to the prediction. We can verify this by creating modified inputs, using LITs datapoint editor. Removing not gets a strongly positive prediction from Its the ultimate depression-era gangster movie..

{% include partials/inset-image image: '/assets/images/lit-not-saliency.png',
  caption: 'Above: Prediction saliency of the original sentence, including "not".'%}

{% include partials/inset-image image: '/assets/images/lit-saliency.png',
  caption: 'Above: Prediction saliency of the altered sentence, with "not" removed.'%}

Using the LIT features of data table searching, the metrics table, salience maps, and manual editing, were able to show both in aggregate and in a specific instance, that our model handles negation correctly.

================
File: website/src/tutorials/sequence-salience.md
================
---
title: Prompt Debugging with Sequence Salience
layout: layouts/tutorial.liquid

hero-image: /assets/images/sample-banner.png
hero-title: "Prompt Debugging with Sequence Salience"
hero-copy: "Learn to use LIT's Sequence Salience module for prompt debugging."

bc-anchor-category: "analysis"
bc-category-title: "Analysis"
bc-title: "Prompt Debugging with Sequence Salience"

time: "20 minutes"
takeaways: "Learn to use LIT's Sequence Salience module for prompt debugging."
---

## Prompt Debugging with Sequence Salience

{%  include partials/link-out,
    link: "https://colab.research.google.com/github/google/generative-ai-docs/blob/main/site/en/gemma/docs/lit_gemma.ipynb",
    text: "Follow along in Google Colab." %}

Or, run this locally with [`examples/prompt_debugging/server.py`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/examples/prompt_debugging/server.py)

Large language models (LLMs), such as [Gemini][gemini] and [GPT-4][gpt4], have
become ubiquitous. Recent releases of "open weights" models, including
[Llama 2][llama], [Mistral][mistral], and [Gemma][gemma], have made it easier
for hobbyists, professionals, and researchers alike to access, use, and study
the complex and diverse capabilities of LLMs.

Many LLM interactions use [prompt engineering][prompteng] methods to control the
model's generation behavior. [Generative AI Studio][ai_studio] and other tools
have made it easier to construct prompts, and model interpretability can help
engineer prompt designs more effectively by showing us which parts on the prompt
the model is using during generation.

In this tutorial, you will learn to use the
[Sequence Salience module][seqsal_docs], introduced in
[LIT v1.1][lit_1_1_release_notes], to explore the impact of your prompt designs
on model generation behavior in three case studies. In short, this module allows
you to select a segment of the model's output and see a heatmap depicting how
much influence each preceding segment had on the selection.

{%  include partials/inset-image,
    image: '/assets/images/seqsal-hero.png',
    caption: "LIT's Language Model Salience demo. Use the Data Table (left) and
        Datapoint Editor (not shown) to select or create prompt designs, and
        visualize the salient information therein using the Sequence Salience
        module (right)." %}

All examples in this tutorial use the [Gemma][gemma] LLM as the analysis target.
Most of the time, this is Gemma Instruct 2B, but we also use Gemma Instruct 7B
in Case Study 3; [more info about variants][gemma_variants] is available online.
LIT supports additional LLMs, including [Llama 2][llama] and [Mistral][mistral],
via the HuggingFace Transformers and KerasNLP libraries.

This tutorial was adapted from and expands upon LIT's contributions to the
[Responsible Generative AI Tookit][rai_toolkit] and the related
[paper][seqsal_paper] and [video][seqsal_video] submitted to the ACL 2024
System Demonstrations track. This is an active and ongoing research area for
the LIT team, so expect changes and further expansions to this tutorial over
time.

## Case Study 1: Debugging Few-Shot Prompts

Few-shot prompting was introduced with [GPT-2][gpt2]: an ML developer provides
examples of how to perform a task in a prompt, affixes user-provided content at
the end, and sends the prompt to the LLM so it will generate the desired output.
This technique has been useful for a number of use cases, including
[solving math problems][cot], [code synthesis][synapis], and more.

Imagine yourself as a developer working on an AI-powered recommendation system.
The goal is to recommend dishes from a restaurant's menu based on a user's
preferences&mdash;what they like and do not like. You are designing and few-shot
prompt to enable an LLM to complete this task. Your prompt design, shown below,
includes five clauses: `Taste-likes` and `Taste-dislikes` are provided by the
user, `Suggestion` is the item from the restaurant's menu, and `Analysis` and
`Recommendation` are generated by the LLM. The dynamic content for the final
example is injected before the prompt is sent to the model.

```text
Analyze a menu item in a restaurant.

## For example:

Taste-likes: I've a sweet-tooth
Taste-dislikes: Don't like onions or garlic
Suggestion: Onion soup
Analysis: it has cooked onions in it, which you don't like.
Recommendation: You have to try it.

Taste-likes: I've a sweet-tooth
Taste-dislikes: Don't like onions or garlic
Suggestion: Baguette maison au levain
Analysis: Home-made leaven bread in France is usually great
Recommendation: Likely good.

Taste-likes: I've a sweet-tooth
Taste-dislikes: Don't like onions or garlic
Suggestion: Macaron in France
Analysis: Sweet with many kinds of flavours
Recommendation: You have to try it.

## Now analyse one more example:

Taste-likes: users-food-like-preferences
Taste-dislikes: users-food-dislike-preferences
Suggestion: menu-item-to-analyse
Analysis:
```

There's a problem with this prompt. Can you spot it? If you find it, how long
do you think it took before you noticed it? Let's see how Sequence Salience can
speed up bug identification and triage with a simple example.

Consider the following values for the variables in the prompt template above.

```text
users-food-like-preferences = Cheese
users-food-dislike-preferences = Can't eat eggs
menu-item-to-analyse = Quiche Lorraine
```

When you run this through the model it generates the following (we show the
entire example, but the model only generated the text after `Analysis`):

```text
Taste-likes: Cheese
Taste-dislikes: Can't eat eggs
Suggestion: Quiche Lorraine
Analysis: A savoury tart with cheese and eggs
Recommendation: You might not like it, but it's worth trying.
```

Why is the model suggesting something that contains an ingredient that the user
cannot eat (eggs)? Is this a problem with the model or a problem with the
prompt? The Sequence Salience module can help us find out.

If you are following along [in Colab][lit_colab], you can select this example
from the [Data Table][data_table] by selecting the example with the `source`
value `fewshot-mistake`. Alternatively, you can add the example directly using
the [Datapoint Editor][datapoint_editor].

Once selected, the Sequence Salience module will allow you to choose the
`response` field from the model (bottom) and see a running-text view of the
prompt. The module defaults to word-level granularity, but this prompt design is
more suitable for sentence-level analysis since the data it contained in each
example is separated into distinct, sentence-like clauses. After enabling
sentence-level aggregation with [Granularity controls][seqsal_docs], select the
`Recommendation` line from the model's generated response to see a heatmap that
shows the impact preceding lines have on that line. You can also use
paragraph-level aggregation to help quickly identify the most influential
examples and then switch to a finer-grained aggregation to see how different
statements in the prompt influence generation. These two perspectives are shown
in the figure below.

{%  include partials/inset-image,
    image: '/assets/images/seqsal-fewshot-wrong.png',
    caption: 'Sequence Salience maps depicting the influence from few-shot
        examples at two levels. Paragraph-level aggregation (left) allows us to
        quickly identify the most influential complete example, and
        sentence-level (right) aids in differentiating the influence of
        constituent clauses. Notice that the most influential example is the
        first one, and that the most salient clause in that example is the
        Analysis line. However, the Recommendation that follows contradicts the
        stated taste preferences and Analysis.'%}

{%  include partials/expandable-info-box,
    title: 'Adjusting Segment Granularity',
    text: "Input salience methods for text-to-text generation tasks operate over
        the subword tokens used by the model. However, human tend not to reason
        effectively over these tokenized representations, so we provide a
        granularity control that (roughly) aggregates tokens into words,
        sentences, and paragraphs, or into custom segments using a regular
        expression parser. The salience score for each aggregate segment is the
        sum of the scores for its constituent tokens. Selecting an aggregate
        segment is equivalent to selecting all constituent tokens." %}

{%  include partials/expandable-info-box,
    title: 'Adjusting Color Map Intensity',
    text: "The Sequence Salience module allows you to control the intensity of
        the color map, which can balance the visual presence of segments at
        different granularities. We've tried to set a suitable default
        intensity, but encourage you to play around with these controls to see
        what works well for your needs." %}

As you scan up through the sentence-level heatmap, you will notice two
things:  1) the strongest influence on the recommendation in the instruction at
the top to analyze the menu item; and 2) the next most influential segments are
the `Analysis` lines in each of the few-shot examples. These suggest that the
model is correctly attending to the task and leaning on the analyses to guide
generation, so what could be going wrong? The most influential `Analysis` clause
is from the `Onion soup` example. Looking at this example more closely we see
that the `Recommendation` clause for this example does not align with the user's
tastes; they dislike onions but the model recommends the onion soup anyway.

[Research suggests][howitworks_icl] that the relatively tight distribution over
the taste and recommendation spaces in the limited examples in the prompt can
affect the model's ability to learn the recommendation task. The other examples
in the prompt appear to have the correct recommendation given the user's tastes.
If we fix the `Onion soup` example recommendation, maybe the model will perform
better.

{%  include partials/inset-image,
    image: '/assets/images/seqsal-fewshot-fixed.png',
    caption: 'The Datapoint Editor (left) allows you to edit the prompt text
        directly in LIT, with any edited fields highlighted in yellow until they
        are added to the dataset. After adding the edited prompt, the Sequence
        Salience module (right) will update and allow you to view the influence
        of prior clauses in the corrected prompt. Fixing the few-shot example
        appears to have correctly adjusted model behavior.' %}

After making adjustments in the Datapoint Editor (or selecting the
`fewshot-fixed` example in the Data Table if you're following along in Colab),
we can again load the example into the Sequence Salience module and, with
sentence-level granularity selected, select the new `Recommendation` line in
the model's generated response. We can immediately see that the response is now
correct. The heatmap looks largely the same as before, but the corrected
examples have improved the models performance.

## Case Study 2: Assessing Constitutional Principles in Prompts

[Constitutional principles][constitutions] are a more recent development in the
pantheon of prompt engineering. The core concept is that clear, concise
instructions describing the qualities of a good output can improve model
performance and increase developers' ability to control generations. Initial
research has shown that self-critique from the model is best for this, and
[tools][constitution_maker] have been developed to help humans have control over
the principles that are added into prompts. The Sequence Salience module can
take this one step further by providing a feedback loop to assess the influence
of constitutional principles on generations.

Building on the task from Case Study 1, let's consider how the following
constitutional principles might impact a prompt designed for food
recommendations from a restaurant menu.

```text
* The analysis should be brief and to the point.
* The analysis and recommendation should both be clear about the suitability for someone with a specified dietary restriction.
```

The location of principles in a prompt can directly affect model performance.
To start, let's look at how they impact generations when placed between the
instruction (`Analyze a menu...`) and the start of the few-shot examples. The
heatmap shown in the figure below shows a desirable pattern; the model is being
strongly influenced by the task instruction and the principle related to the
`Recommendation` component of the generation, with support from the `Analysis`
clauses in the few-shot examples.

{%  include partials/inset-image,
    image: '/assets/images/seqsal-constitutions.png',
    caption: 'A Sequence Salience map depicting the influence of constitutional
        principles on a model generation with few-shot examples. Notice that
        placing the principles near the task instruction seems to give them
        significant influence compared to the heatmaps in Case Study 1.' %}

What happens if we change the location of these principles? You can use LIT's
[Datapoint Editor][datapoint_editor] to move the principles to their own section
in the prompt, between the few-shot examples and the completion, a shown in the
figure below.

{%  include partials/inset-image,
    image: '/assets/images/seqsal-constitutions-moved.png',
    caption: 'This Sequence Salience maps suggests that moving principles around
        in the prompt does not seem to affect model generation on this example,
        but it does change the influence pattern changes dramatically.'%}

After moving the principles, the influence seems to be more diffuse across all
of the `Analysis` sections and the relevant principle. The sentiment conveyed in
the `Recommendation` is similar to the original, and even more terse after
they were moved, which better aligns with the principle. If similar patterns
were found across multiple test examples, this might suggest the model does a
better job of following the principles when they come later on in the prompt.

Constitutional principles are still very new, and the interactions between them
and model size, for example, are not well understood at this time. We hope that
LIT's Sequence Salience module will help develop and validate methods for using
them in prompt engineering use cases.

## Case Study 3: Side-by-Side Behavior Comparisons

LIT support a [side-by-side (SxS) mode][lit_sxs] that can be used to compare two
models, or here, compare model behavior on two related examples.
Let's see how we can use this to understand differences in prompt designs with
Sequence Salience.

[GSM8K][gsm8k] is a benchmark dataset of grade school math problems commonly
used to evaluate LLMs' mathematical reasoning abilities. Most evaluations employ
a [chain-of-thought prompt design][cot] where a set of few-shot examples
demonstrate how to decompose a word problem into subproblems and then combine
the results from the various subproblems to arrive at the desired answer. GSM8K
and other work has shown that LLMs often need assistance to perform
calculations, introducing the idea of [tool use][toolformer] by LLMs.

Less explored is the Socratic form of the dataset, where subproblems are framed
as questions instead of declarative statements. One might assume that a model
will perform similarly or even better on the Socratic form than the conventional
form, especially when you consider modifying the prompt design to include the
preceding Socratic questions in the prompt, isolating the work the model must
perform to the final question, as shown in the following example.

```text
A carnival snack booth made $50 selling popcorn each day. It made three times as much selling cotton candy. For a 5-day activity, the booth has to pay $30 rent and $75 for the cost of the ingredients. How much did the booth earn for 5 days after paying the rent and the cost of ingredients?
How much did the booth make selling cotton candy each day? ** The booth made $50 x 3 = $<<50*3=150>>150 selling cotton candy each day.
How much did the booth make in a day? ** In a day, the booth made a total of $150 + $50 = $<<150+50=200>>200.
How much did the booth make in 5 days? ** In 5 days, they made a total of $200 x 5 = $<<200*5=1000>>1000.
How much did the booth have to pay? ** The booth has to pay a total of $30 + $75 = $<<30+75=105>>105.
How much did the booth earn after paying the rent and the cost of ingredients? **
```

When we inspect the model's response to a zero-shot prompt in the Sequence
Salience module, we notice two things. First, the model failed to compute the
correct answer. It was able to correctly set up the problem as the difference
between two values, but the calculated value is incorrect ($995 when it should
be $895). Second, we see a fairly diffuse heatmap attending near equally to the
operands for the final problem and all of the preceding answers to the Socratic
questions.

{%  include partials/inset-image,
    image: '/assets/images/seqsal-gsm8k-sxs-target-select.png',
    caption: 'On load, the Sequence Salience module lets you choose which target
        sequence to analyze. Sequences from the dataset are shown on top, there
        is typically only one of these as it acts as the ground truth against
        which predictions are compared. Sequences from the model are shown on
        the bottom; there may be more than one of these depending on the
        sampling strategy used by the model.' %}

This dataset does provide ground truth, so let's use SxS mode to compare the
generated response with the ground truth. The fastest way to enter SxS mode for
the selected datapoint is by using the pin button in the
[main toolbar][main_toolbar]. When you enable SxS mode, the Sequence Salience
module will ask you to choose which target sequence to view on each side. The
order doesn't matter, but ground truth is on the left and the models' response
is on the right in the figure below.

{%  include partials/inset-image,
    image: '/assets/images/seqsal-gsm8k-sxs-gt-resp.png',
    caption: 'Side-by-side Sequence Salience maps for the ground truth (left)
        and model generated response (right) for a zero-shot prompt of a GSM8K
        example. Note the similarities between these heatmaps, with diffuse
        influence over the preceding answers and the incorrect calculation to
        the final question.' %}

Next, ensure that the same granularity (word-level) is being used on both
Sequence Salience visualizations, and then select the segment for the last
calculation on both sides. The heatmap is quite similar on both sides; the same
diffuse pattern suggesting the model isn't quite sure what to pay attention to.

{%  include partials/inset-image,
    image: '/assets/images/seqsal-gsm8k-sxs-gt-resp.png',
    caption: 'Side-by-side Sequence Salience maps for the ground truth (left)
        and model generated response (right) for a zero-shot prompt of a GSM8K
        example. Note the similarities between these heatmaps, with diffuse
        influence over the preceding answers and the incorrect calculation to
        the final question.' %}

One possibility that might improve performance is to adjust the prompt so that
the segments used in the calculations are more salient. GSM8K uses a
[special calculation annotation][gsm8k_paper] to tell the model when it should
employ an external calculator tool during generation. The naive zero-shot prompt
above left these annotations intact and they might be confusing the model. Let's
see what happens when we remove these annotations. Using the
[Datapoint Editor][datapoint_editor] we can remove all of the `<< ... >>`
content from the prompt, then use the "Add" button to add it to our dataset, run
generation, and load the example in the Sequence Salience module as the
"selected" datapoint on the right. Choose to view the model's response field in
the Sequence Salience module, ensure the same granularity is being used, and
then select the segment containing the calculated value on both sides, as shown
in the figure below.

We can immediately see that the modified prompt has a much more intense salience
map focusing on the operands to the calculation and the preceding answers from
which they originate. That said, the model still gets the calculation wrong.

{%  include partials/inset-image,
    image: '/assets/images/seqsal-gsm8k-sxs-no-annos.png',
    caption: "Side-by-side Sequence Salience maps of the model's response for
        the original zero-shot prompt (left) and a revised prompt (right) that
        removes the special calculation annotations. Despite the more focused
        influence of the segments relevant to the final question, the model
        still fails to calculate the correct answer." %}

In addition to these between-examples comparisons, LIT's SxS mode also supports
comparison between two models. [Prior][gsm8k] [research][toolformer]
investigating the necessity of tool use by models has noted that model size does
seem to correlate with performance on mathematical reasoning benchmarks. Let's
test that hypothesis here.

{%  include partials/info-box,
    title: 'Resource Needs for Between-Model Comparisons',
    text: "Side-by-side comparison requires loading both models at once, which
        requires additional memory. To load both Gemma 2B and 7B, we recommend a
        GPU or TPU with 40GB of memory, such as the Nvidia A100 available
        through Colab Pro."%}

To enable between-model comparison, first unpin the original example using the
button in the [main toolbar][main_toolbar], then enable the 7B and 2B model
instances using the checkboxes (also in the main toolbar). This will duplicate
the Sequence Salience module, with the 7B model on the left and the 2B model on
the right. Select model response for both, and then select the final calculation
result segment to see their respective heatmaps.

{%  include partials/inset-image,
    image: '/assets/images/seqsal-gsm8k-sxs-between-model.png',
    caption: 'Side-by-side Sequence Salience maps for the responses from two
        models&mdash;Gemma 7B IT (left) and Gemma 2B IT (right)&mdash;to the
        revised zero-shot prompt from above.' %}

Notice that the heatmaps are quite similar, suggesting the models have similar
behavioral characteristics, but that both still get the answer wrong. At this
point, it may be possible to improve performance by revisiting different
[prompting strategies][prompteng_strats] or by training the model to
[use tools][toolformer].

## Conclusion

The case studies above demonstrate how to use LIT's Sequence Salience module to
evaluate prompt designs rapidly and iteratively, in combination with LIT's tools
for side-by-side comparison and datapoint editing.

Salience methods for LLMs is an [active][salience_research_1]
[research][salience_research_1] area. The LIT team has provided reference
implementations for computing gradient-based salience&mdash;
[Grad L2 Norm][grad_norm] and [Grad  Input][grad_dot]&mdash;for LLMs in two
popular frameworks: [KerasNLP][lit_keras] and
[HuggingFace Transformers][lit_hf].

There is considerable opportunity to research how the model analysis foundations
described in this tutorial can support richer workflows, particularly as they
relate to aggregate analysis of salience results over many examples, and the
semi-automated generation of new prompt designs. Consider contributing
your ideas, prototypes, and implementations with us [via GitHub][lit_issues].

### Further Reading

In addition to the links above, the Google Cloud, Responsible AI and
Human-Centered Technologies, and the People + AI Research teams have several
helpful guides that can help you develop better prompts, including:

* Cloud's overview of [prompt design strategies][prompteng_strats];
* Cloud's [best practices][prompteng_bestpracs] for prompt engineering;
* The [Responsible Generative AI Tookit][rai_toolkit];
* The [PAIR Guidebook][pair_guidebook] discusses the importance of iterative
  testing and revision; and
* The interactive [saliency explorable][explorable_salience] dives deep into the
  inner working of salience methods, and how they can be used.

<!-- Links -->

[ai_studio]: https://cloud.google.com/generative-ai-studio?hl=en
[constitution_maker]: https://arxiv.org/abs/2310.15428
[constitutions]: https://arxiv.org/abs/2212.08073
[cot]: https://proceedings.neurips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html
[data_table]: ../../documentation/ui_guide.html#data-table
[datapoint_editor]: ../../documentation/ui_guide.html#datapoint-editor
[datapoint_editor_add_comp]: ../../documentation/components.html#manual-editing
[explorable_salience]: https://pair.withgoogle.com/explorables/saliency/
[fewshot]: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompt-design-strategies#zero-shot-vs-few-shot-prompts
[gemini]:https://gemini.google.com/
[gemma]: https://ai.google.dev/gemma
[gemma_variants]: https://ai.google.dev/gemma/docs#models
[generators]: ../../documentation/components.html#generators
[global_settings]: ../../documentation/ui_guide.html#global-settings
[gpt2]: https://cdn.openai.com/better-language-models/language-models.pdf
[gpt4]: https://arxiv.org/abs/2303.08774
[grad_dot]: https://arxiv.org/abs/1412.6815
[grad_norm]: https://aclanthology.org/P18-1032/
[gsm8k]: https://github.com/openai/grade-school-math
[gsm8k_paper]: https://arxiv.org/abs/2110.14168
[howitworks_icl]: https://par.nsf.gov/servlets/purl/10462310
[lit_1_1_release_notes]:https://github.com/PAIR-code/lit/blob/main/RELEASE.md#release-11
[lit_colab]: https://colab.research.google.com/github/google/generative-ai-docs/blob/main/site/en/gemma/docs/lit_gemma.ipynb
[lit_hf]: https://github.com/PAIR-code/lit/blob/main/lit_nlp/examples/prompt_debugging/transformers_lms.py
[lit_issues]: https://github.com/PAIR-code/lit/issues
[lit_keras]: https://github.com/PAIR-code/lit/blob/main/lit_nlp/examples/prompt_debugging/keras_lms.py
[lit_sxs]: ../../documentation/ui_guide.html#comparing-datapoints
[llama]: https://llama.meta.com/
[main_toolbar]: ../../documentation/ui_guide.html#main-toolbar
[mistral]: https://mistral.ai/news/announcing-mistral-7b/
[pair_guidebook]: https://pair.withgoogle.com/guidebook/
[prompteng]: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/introduction-prompt-design
[prompteng_bestpracs]: https://cloud.google.com/blog/products/application-development/five-best-practices-for-prompt-engineering
[prompteng_strats]: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompt-design-strategies
[rai_toolkit]: https://ai.google.dev/responsible
[salience_research_1]: https://dl.acm.org/doi/full/10.1145/3639372
[salience_research_2]: https://arxiv.org/abs/2402.01761
[seqsal_docs]: ../../documentation/components.html#sequence-salience
[seqsal_paper]: https://arxiv.org/abs/2404.07498
[seqsal_video]: https://youtu.be/EZgUlnWdh0w
[synapis]: https://scholarspace.manoa.hawaii.edu/items/65312e48-5954-4a5f-a1e8-e5119e6abc0a
[toolformer]: https://arxiv.org/abs/2302.04761

================
File: website/src/tutorials/tab-feat-attr.md
================
---
title: Tabular Feature Attribution
layout: layouts/tutorial.liquid

hero-image: /assets/images/sample-banner.png
hero-title: "Tabular Feature Attribution"
hero-copy: "Learn how to use the Tabular Feature Attribution module in LIT."

bc-anchor-category: "analysis"
bc-category-title: "Analysis"
bc-title: "Tabular Feature Attribution"

time: "15 minutes"
takeaways: "Learn how to use the Kernel SHAP based Tabular Feature Attribution module in LIT."
---

## Tabular Feature Attribution

{%  include partials/link-out,
    link: "../../demos/penguins.html",
    text: "Explore this demo yourself." %}

Or, run your own with
[`examples/penguin/demo.py`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/examples/penguin/demo.py)

LIT supports many techniques like salience maps and counterfactual generators
for text data. But what if you have a tabular dataset? You might want to find
out which features (columns) are most relevant to the models predictions. LIT's
Feature Attribution module for
[tabular datasets](../../documentation/components.html#tabular-data)
support identification of these important features. This tutorial provides a
walkthrough for this module within LIT, on the
[Palmer Penguins dataset](https://allisonhorst.github.io/palmerpenguins/).

{%  include partials/info-box,
    title: 'Kernel SHAP based Feature Attribution',
    text: "The Feature Attribution functionality is
        [achieved using SHAP](https://proceedings.neurips.cc/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf).
        In particular LIT uses
        [Kernel SHAP](https://shap-lrjball.readthedocs.io/en/latest/generated/shap.KernelExplainer.html)
        over tabular data, which is basically a specially weighted local linear
        regression for estimating SHAP values and works for any model. For now,
        the feature attribution module is only shown in the UI when working with
        [tabular data](../../documentation/components.html#tabular-data)."%}

### **Overview**

The [penguins demo](../../demos/penguins.html) is a
simple classifier for predicting penguin species from the Palmer Penguins
dataset. It classifies the penguins as either Adelie, Chinstrap, or Gentoo based
on 6 features&mdash;body mass (g), [culmen](https://en.wikipedia.org/wiki/Beak#Culmen)
depth (mm), culmen length (mm), flipper length (mm), island, and sex.

{% include partials/info-box title: 'Filtering out incomplete data points',
  text: "Palmer Penguins is a tabular dataset with 344 penguin specimens. LITs
  penguin demo filters out 11 of these penguins due to missing information (sex
  is missing for all penguins, though some are missing additional information),
  resulting in 333 data points being loaded for analysis."%}

The Feature Attribution module shows up in the bottom right of the demo within
the Explanations tab. It computes
[Shapley Additive exPlanation (SHAP)](https://proceedings.neurips.cc/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf)
values for each feature in a set of inputs and displays these values in a table.
The controls for this module are:

1.  The **sample size slider,** which defaults to a value of 30. SHAP
    computations are very expensive and it is infeasible to compute them for the
    entire dataset. Through testing, we found that 30 is about the maximum
    number of samples we can run SHAP on before performance takes a significant
    hit, and it becomes difficult to use above 50 examples. Clicking the Apply
    button will automatically check the Show attributions from the Tabular SHAP
    checkbox, and LIT will start computing the SHAP values.
2.  The **prediction key** selects the model output value for which influence is
    computed. Since the penguin mode only predicts one feature, species, this is
    set to species and cannot be changed. If a model can predict multiple values
    in different fields, for example predicting species and island or species
    and sex, then you could change which output field to explain before clicking
    Apply.
3.  The **heatmap toggle** can be enabled to color code the SHAP values.
4.  The **facets button** and **show attributions for selection checkbox**
    enable conditionally running the Kernel SHAP interpreter over subsets of the
    data. We will get into the specifics of this with an example later on in
    this tutorial.

{%  include partials/inset-image,
    image: '/assets/images/tab-feat-attr-image-1.png',
    caption: 'An overview of the Penguins demo, notice the tabular feature
        attribution (1) and salience maps (2) modules in the bottom right and
        center, respectively.'%}

{%  include partials/inset-image,
    image: '/assets/images/tab-feat-attr-image-2.png',
    caption: 'The tabular feature attribution module has three main elements of
        interactivity: an expansion panel where you can configure the SHAP
        parameters (1), a heatmap toggle to activate color the cells in the
        results table based on the scores (2), and a facets control for
        exploring subsets of the data (3).'%}

#### **A Simple Use Case : Feature Attribution for 10 samples**

To get started with the module, we set sample size to a small value, 10, and
start the SHAP computation with heatmap enabled.

{% include partials/info-box title: 'Edge cases for the sample size button',
  text: "Kernel SHAP computes feature importance relative to a pseudo-random
  sample of the dataset. The sample size is set with the slider, and the samples
  are drawn from either the current selection (i.e., a subset of the data that
  were manually selected or are included as part of a slice) or the entire
  dataset. When sampling from the current selection, the sample size can have
  interesting edge cases:

* If the selection is empty, LIT samples the sample size number of data points
  from the entire dataset.
* If the sample size is zero or larger than the selection, then LIT computes
  SHAP for the entire selection and does not sample additional data from the
  dataset.
* If sample size is smaller than the selection, then LIT samples the sample
  size number of data points from the selected inputs."%}

Enabling the heatmap provides a visual indicator of the polarity and strength of
a feature's influence. A reddish hue indicates negative attribution for that
particular feature and a bluish hue indicates positive attribution. The deeper
the color the stronger its influence on the predictions.

{%  include partials/info-box,
    title: 'Interpreting salience polarity',
    text: "Salience is always relative to the model's prediction of one class.
        Intuitively, a positive attribution score for a feature of an example
        means that if this feature was removed we expect a drop in model
        confidence in the prediction of this class. Similarly, removing a
        feature with a negative score would correspond to an increase in the
        model's confidence in the prediction of this class."%}

SHAP values are computed per feature per example, from which LIT computes the
mean, min, median, and max feature values across the examples. The min and max
values can be used to spot any outliers during analysis. The difference between
the mean and the median can be used to gain more insights about the
distribution. All of this enables statistical comparisons and will be enhanced
in future releases of LIT.

Each of the columns in the table can be sorted using the up (ascending) or down
(descending) arrow symbols in the column headers. The table is sorted in
ascending alphabetical order of input feature names (field) by default. If there
are many features in a dataset this space will get crowded, so LIT offers a
filter button for each of the columns to look up a particular feature or value
directly.

{%  include partials/inset-image,
    image: '/assets/images/tab-feat-attr-image-4.png',
    caption: 'Start by reducing the sample size from 30 to 10, this will speed
        up the SHAP computations.'%}

{%  include partials/inset-image,
    image: '/assets/images/tab-feat-attr-image-5.png',
    caption: 'The results of the SHAP run over a sample of 10 inputs from the
        entire dataset. Notice how subtle the salience values are in the "mean"
        column.'%}

#### **Faceting & Binning of Features**

Simply speaking, facets are subsets of the dataset based on specific feature
values. We can use facets to explore differences in SHAP values between subsets.
For example, instead of looking at SHAP values from 10 samples containing both
male and female penguins, we can look at male penguins and female penguins
separately by faceting based on sex. LIT also allows you to select multiple
features for faceting, and it will generate the facets by feature crosses. For
example, if you select both sex (either male or female) and island (one of
Biscoe, Dream and Torgersen), then LIT will create 6 facets for (Male, Biscoe),
(Male, Dream), (Male, Torgersen), (Female, Biscoe), (Female, Dream), (Female,
Torgersen) and show the SHAP values for whichever facets have a non-zero number
of samples.

{%  include partials/inset-image,
    image: '/assets/images/tab-feat-attr-image-6.png',
    caption: 'Each facet of the dataset is given its own expansion panel. Click
        on the down arrow on the right to expand the section and see the results
        for that facet.'%}

Numerical features support more complex faceting options. Faceting based on
numerical features allows for defining bins using 4 methods: discrete, equal
intervals, quantile, and threshold. Equal intervals will evenly divide the
features [domain](https://en.wikipedia.org/wiki/Domain_of_a_function) into N
equal-sized bins. Quantile will create N bins that each contain (approximately)
the same number of examples. Threshold creates two bins, one for the examples
with values up to and including the threshold value, and one for examples with
values above the threshold value. The discrete method requires specific dataset
or model spec configuration, and we do not recommend using that method with this
demo.

Categorical and boolean features do not have controllable binning behavior. A
bin is created for each label in their vocabulary.

{%  include partials/inset-image,
    image: '/assets/images/tab-feat-attr-image-7.png',
    caption: 'Clicking the facets button will open the configuration controls.
        Use these to configure how divide the dataset into subsets.'%}

LIT supports as many as 100 facets (aka bins). An indicator in the faceting
config dialog lets you know how many would be created given the current
settings.

Faceting is not supported for selections, meaning that if you already have a
selection of elements (lets say 10 penguins), then facets wont split it
further.

{%  include partials/inset-image,
    image: '/assets/images/tab-feat-attr-image-9.png',
    caption: 'LIT limits the number of facets to 100 bins for performance
        reasons. Attempting to exceed this limit will cause the active features
        to highlight red so you can adjust their configurations.'%}

### **Side-by-side comparison : Salience Maps Vs Tabular Feature Attribution**

The Feature Attribution module works well in conjunction with other modules. In
particular, we are going to look at the Salience Maps module which allows us to
enhance our analysis. Salience Maps work on one data point at a time, whereas
the Tabular Feature Attribution usually looks at a set of data points.

{% include partials/info-box,
  title: 'Slightly different color scales',
  text: "The color scales are slightly different between the salience maps
  module and the tabular feature attribution module. Salience maps use a
  gamma-adjusted color scale to make values more prominent."%}

#### **One random data point**

In this example, a random data point is chosen using the select random button in
the top right corner and the unselected data points are hidden in the Data
Table. After running both the salience maps module and the feature attribution
module for the selected point, we can see that the values in the mean column of
Tabular SHAP output match the saliency scores exactly. Note also that the mean,
min, median and max values are all the same when a single datapoint is selected.

{%  include partials/inset-image,
    image: '/assets/images/tab-feat-attr-image-11.png',
    caption: 'The results in the tabular feature attribution and salience maps
        modules will be the same for single datapoint selections.'%}

#### **A slice of 5 random data points**

LIT uses a [complex selection model](../../documentation/ui_guide.md#datapoint-selections)
and different modules react to it differently. Salience Maps only care about the
primary selection (the data point highlighted in a deep cyan hue in the data
table) in a slice of elements, whereas Feature Attribution uses the entire list
of selected elements.

{%  include partials/info-box,
    title: 'Using Salience Maps to support Tabular Feature Attribution',
    text: "Changing primary selection reruns SHAP in the Salience Maps module
        but not in Tabular Feature Attribution. So, we can effectively toggle
        through the items in our selection one-by-one and see how they compare
        to the mean values in the Feature Attribution module. Another thing to
        note is that the Salience Maps module supports comparison between a
        pinned datapoint and the primary selection, so we can do the above
        comparisons in a pair-wise manner as well."%}

As we can see in this example, where we run both modules on a slice of 5
elements, the Salience Maps module is only providing its output for the primary
selection (data point 0), whereas the Tabular Feature Attribution module is
providing values for the entire selection by enabling the Show attributions for
selection checkbox. This allows us to use the salience map module as a kind of
magnifying glass to focus on any individual example even when we are considering
a slice of examples in our exploration of the dataset.

{%  include partials/inset-image,
    image: '/assets/images/tab-feat-attr-image-12.png',
    caption: 'The salience maps module is a great way to compare the scores for
        each datapoint in a selection against the scores for that entire
        selection from. the tabular feature attribution module.'%}

### **Conclusion**

Tabular Feature Attribution based on Kernel SHAP allows LIT users to explore
their tabular data and find the most influential features affecting model
predictions. It also integrates nicely with the Salience Maps module to allow
for fine-grained inspections. This is the first of many features in LIT for
exploring tabular data, and more exciting updates would be coming in future
releases!

================
File: website/src/tutorials/tcav.md
================
---
title: Performing Global Model Analysis with TCAV
layout: layouts/tutorial.liquid

hero-image: /assets/images/sample-banner.png
hero-title: "TCAV"
hero-copy: "Performing Global Model Analysis with TCAV"

bc-anchor-category: "analysis"
bc-category-title: "Analysis"
bc-title: "TCAV"

time: "15 minutes"
takeaways: "Learn what TCAV is and how to perform TCAV analysis in the Learning Interpretability Tool."
---

## Performing Global Model Analysis with TCAV
{% include partials/link-out link: "../../demos/glue.html", text: "Follow along in the hosted demo." %}
{% include partials/spacer height:10 %}

LIT contains many techniques to analyze individual predictions, such as salience maps and counterfactual analysis. But, what if you want to understand more global characteristics about a model, and not just its behavior on a single example/prediction? Or what if you have too many input features or your input features are too complicated that saliency maps arent as insightful? One technique that can help here is called [Testing with Concept Activation Vectors](https://arxiv.org/abs/1711.11279), or TCAV. For more technical details on how TCAV works, see our [LIT documentation on the feature](../../documentation/components.html#tcav). This tutorial provides a walkthrough of using TCAV inside of LIT.

TCAV shows the importance of high-level concepts on a models prediction, as opposed to showing the importance of individual feature values (such as tokens for a language model). Another difference is that TCAV gives aggregated explanations for many data points (a global explanation) instead of one data point at a time, like saliency maps (a local explanation).The concepts are defined by the user creating a subset of datapoints (a slice in the LIT tool) containing the concept they wish to test. LIT runs TCAV over the slice to determine if that concept has a positive or negative effect on the prediction of a given class by a classification model, or if that concept has no statistically significant effect on the prediction of that class.

{% include partials/info-box title: 'TCAV example use-case', 
  text: "In the original research describing the TCAV method, they show an example of testing the importance of the stripes concept on an image classifier for predictions of the zebra class. This is done by creating a concept from a set of images consisting of stripes (e.g., fabrics and designs). TCAV finds that the stripes concept has a strong, positive impact on classification of zebras. This means that, for a given image, adding more stripes to it is correlated with high prediction scores of the zebra class."%}


### Running TCAV in LIT

#### Use Case

[One of our LIT demos](https://pair-code.github.io/lit/demos/glue.html) is a BERT-based binary classifier of movie review sentences (from the [Stanford Sentiment Treebank dataset](https://pair-code.github.io/lit/demos/glue.html)). It classifies the sentiment of sentences as positive (class 1) or negative (class 0).

Let's try to determine if there is any correlation between other language features, such as the use of the terms acting, actor, and actress, or mentions of music, with the classification of sentiment. This can help us understand what concepts a model is sensitive to, and when and how errors may occur.

#### Create a Concept

{% include partials/inset-image image: '/assets/images/lit_tcav_screen_annotated.png', 
  caption: 'LIT modules used in our demo. 1: The data table to search for and select datapoints. 2: The Slice Editor to save the selected datapoints to a named slice. 3. The TCAV Explorer in the TCAV tab to run TCAV on our slice.'%}

First, lets test if the mention of the phrases acting, actor, or actress has any influence on the models predictions. To test a concept in LIT, we need to create a slice containing exemplary datapoints of the concept we are curious about. We can go to the data table, and use the search feature of the sentence field and search for acting|actor|actress to search for all mentions of those three terms. We then select all those examples using the select all button in the Data Table, expand the minimized Slice Editor, and use it to create a new slice named acting, containing those 28 datapoints matching our query.


{% include partials/inset-image image: '/assets/images/lit_data_table_annotated.png', 
  caption: '1: Searching for a datapoints containing certain words and 2: selecting all matching datapoints'%}


{% include partials/inset-image image: '/assets/images/lit_slice_editor_annotated.png', 
  caption: '1: Naming a slice and 2: Creating the named slice from the selected datapoints.'%}

Now we can go to the TCAV module and select our options to run the method. There are three sets of options to select. 

1. **Slice selection**: Youll notice that each slice can be selected through a left-side checkbox. You can test multiple concepts at a time by selecting more than one slice, if desired. Slices can also be selected as a negative slice through a right-side checkbox. For basic TCAV use, we dont select any negative slice. See the below section on Relative TCAV for use of that option.

2. **Model class selection**: The default selection of class 1 means that TCAV will test to see if the concept selected has influence over the model predicting that class (which in this case means positive sentiment). If we were to change this to class 0, we would be testing to see if a concept has influence of the model predicting negative sentiment. For a binary classifier, you would only need to test one of the two classes because if a concept has a positive influence on class 1, then it would automatically have a negative influence on class 0. For multi-class classification with many possible classes, changing the explainable class is more important, in order to test out the concepts influence across any specific class.

3. **Embedding layer selection**: TCAV requires that a model be able to return a per-datapoint embedding during prediction for use in its calculations. For models that return multiple per-datapoint embeddings at different layers of the model, you may find that certain concepts have influence at different layers. For example, TCAV research on image models found that embeddings early in a models architecture had significant concepts around color and basic patterns and embeddings later in the architecture had significant concepts based around more complex patterns. Typically, we recommend choosing the layer closest to the prediction layer. 

For our use-case, we click on our acting slice as the concept to test, and can use the default selections of explaining class 1 and using the cls_grad embedding, which is the only per-datapoint embedding returned by this model. In our BERT-based model, the cls_grad embedding is the activation of the \[CLS\] token, which captures information about the entire sentence to be used for the final classification layer of the model.


{% include partials/inset-image-small image: '/assets/images/lit_tcav_settings_annotated.png', 
  caption: '1: Selecting a slice as a concept and 2: running TCAV with the specified options'%}

#### Interpreting TCAV scores

Once we run TCAV, we see an entry in the table in the TCAV module for each concept tested. Each concept gets a CAV (Concept Activation Vector) score between 0 and 1 describing the concepts effect on the prediction of the class in question. What matters is where the blue bar (CAV score) is relative to the black line (reference point). The reference point indicates the effect that slices made of randomly-chosen datapoints outside of the concept being tested has on prediction of the class. For a well-calibrated classifier, the reference point will usually be near 0.5 (i.e. no effect).

A blue bar extending right or left of the black line means the concept is influencing the prediction. If the blue bar extends to the right of the black line, the concept is positively influencing the prediction. Conversely, if the bar extended to the left, it is negatively influencing. In either case, the larger the bar, the greater the influence.

In our example, the CAV score of ~0.91 indicates that our acting concept has a strong positive effect on the prediction of this class. So we have found that this concept has a positive effect on predicting positive sentiment for our classifier.

{% include partials/inset-image image: '/assets/images/tcav_result.png', 
  caption: 'TCAV results for our acting concept'%}

Now, lets try a different concept.  Well go through the same process, except with datapoints containing the word music, so we can see if mentions of music in a review have a significant effect on prediction. We can repeat the steps above to select matching datapoints, create a slice of these datapoints, and run TCAV on that slice. After doing so, we get a new line in the TCAV table, but it indicates this run was not statistically significant. That means that TCAV did not find a statistically significant difference between the effect of this concept versus concepts made of randomly sampled datapoints. So, we have found that mentions of music dont seem to have any specific effect on predicting positive sentiment in this classifier. For details on this check, read the [LIT TCAV documentation](../../documentation/components.html#tcav).


{% include partials/inset-image image: '/assets/images/tcav_results_2.png', 
  caption: 'Updated TCAV results including our music concept'%}

### Creating Useful Concepts

As you can imagine, it is important to select a set of datapoints that capture the concept you wish to test. If your datapoints capture more concepts than what you wish to test then the results wont be what you expect. In practice, this can be difficult to do with precision as different language features overlap in ways we don't always intuit.

One way to overcome this challenge is providing more data for TCAV to analyze. If you do not select enough datapoints, your set of examples might capture more, different, and/or overlapping concepts than you intend. A good rule of thumb, as seen in [ML interpretability research](https://arxiv.org/abs/1902.02960), is to use at least 15 examples for a concept, but it can differ based on model type, data format, and the number of total examples in your dataset in LIT.

Imagine if, when we created a concept by searching for the word music, that every time music is used in a review, that review happens to be glowingly-positive. Then our concept doesnt only capture the concept of music-related reviews, it also captures a set of very positive reviews. This introduces a bias into our analysis process. The music concept we created would obviously be shown to have a positive influence on prediction, but it might not be due to the music itself. So, take care in defining concept slices, and look at the datapoints you have chosen to ensure they represent the independent concept you wish to test.

Concretely you can sort your dataset with respect to concepts created through TCAV (e.g., using cosine similarity) to do a qualitative check: does the sorting make sense to you? Does it reveal something that CAV learned that you didnt know about? Sorting datapoints with respect to concepts in a feature that will be added to LIT in the future. 

If the sorting reveals unwanted correlated concepts, you can help the CAV to forget this concept by collecting negative examples with only the correlated concepts. For example, if your stripes images all happen to be t-shirts, use non-striped t-shirts images as negative examples and create a relative concept as described below. This will help the concept to not contain information about t-shirts. In some cases, you can debias accidentally added concepts [using causal analysis](https://arxiv.org/abs/2007.11500).

### Relative TCAV

In the Select Slices menu, youll notice that there are checkboxes to select a slice as a Negative slice in addition to the checkboxes to select the concepts to test. You can test a relative concept by selecting one slice as the positive slice and another slice as the negative slice and running TCAV that way. Testing a relative concept allows you to see if adjusting datapoints from containing one concept to containing another will generally have an impact on classification.

For example, what if we wanted to test if the use of the gendered terms for acting professionals  actor for males and actress for females  has an impact on sentiment prediction? Instead of testing concepts of actor and actress separately for significance, you could generate a set of gender-flipped examples and use Relative TCAV to explore the differences. To start, create an actor slice with all datapoints containing the word actor. Then use the Word Replacer counterfactual generator to change all instances of actor to actress in those datapoints, add those newly generated datapoints to the dataset, and save them as a slice named actress. Next, in the TCAV module, set the positive slice to actor and the negative slice to actress and run TCAV. The resulting concept is shown to be not-significant. This means that our classifier doesnt seem to be sensitive to using the gendered terms actor over actress in the predictions of review sentiment.

{% include partials/inset-image image: '/assets/images/actor_to_actress.png', 
  caption: 'Using the Word Replacer datapoint generator to replace instances of actor with actress, in support of testing a relative concept.'%}

{% include partials/inset-image image: '/assets/images/actress_relative_cav.png', 
  caption: 'TCAV settings and results for our relative concept of actor vs.actress, which is deemed not statistically significant.'%}

### Conclusion

TCAV offers a way to look for global characteristics about classification models by analyzing the influence of concepts, represented as user-curated slices of exemplary datapoints, on class predictions. It can be very useful for hypothesis generation and for testing theories generated by other techniques. As with any interpretability technique, it is only one piece of a larger puzzle. 

Care must be taken in defining concepts to ensure that they represent what you expect them to and do not contain surprising or spurious concepts (such as defining a concept based around inclusion of the word music in sentences where all those sentences also happen to contain glowing, positive reviews, whereas the wider dataset contains both positive and negative reviews). We provide some ways to do this above.

By including TCAV in the LIT tool, we encourage people analyzing ML models to investigate global model behavior through concepts. No single interpretability technique can fully answer the questions we have about model behavior, but the more techniques that are accessible, the more power we have to improve model transparency and interpretability.

================
File: website/src/tutorials/text-salience.md
================
---
title: Salience Maps for Text
layout: layouts/tutorial.liquid

hero-image: /assets/images/sample-banner.png
hero-title: "Salience Maps for Text"
hero-copy: "Learn how to use salience maps for text data in LIT."

bc-anchor-category: "analysis"
bc-category-title: "Analysis"
bc-title: "Salience Maps for Text"

time: "15 minutes"
takeaways: "Learn how to use salience maps for text data in LIT."
---

## Salience Maps for Text

{%  include partials/link-out,
    link: "../../demos/glue.html",
    text: "Explore this demo yourself." %}

Or, run your own with [`examples/glue/demo.py`](https://github.com/PAIR-code/lit/blob/main/lit_nlp/examples/glue/demo.py)

LIT enables users to analyze individual predictions for text input using
salience maps, for which gradient-based and/or blackbox methods are available.
In this tutorial, we will explore how to use salience maps to analyze a text
classifier in the [Classification and Regression models demo](https://pair-code.github.io/lit/demos/glue.html)
from the LIT website, and
how these findings can support counterfactual analysis using LIT's generators,
such as Hotflip, to test hypotheses. The Salience Maps module can be found under
the Explanations tab in the bottom half of this demo and it supports four
different methods for the GLUE model under test (with other models it might
support a different number of these methods) -
[Grad L2 Norm](https://aclanthology.org/P18-1032/),
[Grad  Input](https://arxiv.org/abs/1412.6815),
[Integrated Gradients](https://arxiv.org/pdf/1703.01365.pdf) (IG)
and [LIME](https://arxiv.org/pdf/1602.04938v3.pdf).

### Heuristics: Which salience method for which task?

Salience methods are imperfect. Research has shown that salience methods are
often
[sensitive to factors that do not contribute to a model's prediction](https://arxiv.org/abs/1711.00867);
that people tend to
[overly trust salience values or use methods they believe they know incorrectly](https://dl.acm.org/doi/10.1145/3313831.3376219);
and that
[model architecture may directly impact the utility](https://arxiv.org/pdf/2111.07367.pdf)
of different salience methods.

With those limitations in mind, the question remains as to which methods should
be used and when. To offer some guidance, we have come up with the following
decision aid that provides some ideas about which salience method(s) might be
appropriate.

{%  include partials/inset-image,
    image: '/assets/images/text-salience-image-1.png',
    caption: 'This flow chart can help you decide which salience interpreter to
        apply given the information provided by your model.'%}

If your model does not output gradients with its predictions (i.e., is a
blackbox), [LIME](https://arxiv.org/pdf/1602.04938v3.pdf) is your only choice as
it is currently the only black-box method LIT supports for text data.

If your model does output gradients, then you can choose among three methods:
[Grad L2 Norm](https://aclanthology.org/P18-1032/),
[Grad  Input](https://arxiv.org/abs/1412.6815), and
[Integrated Gradients](https://arxiv.org/pdf/1703.01365.pdf) (IG).
Grad L2 Norm and Grad  Input are easy to use and fast to compute, but can
suffer from gradient saturation. IG addresses the gradient saturation issue in
the Grad methods (described in detail below), but requires that the model output
both gradients and embeddings, is much more expensive to compute, and requires
parameterization to optimize results.

Remember that a good investigative process will check for commonalities and
patterns across salience values from multiple salience methods. Further,
salience methods should be an entry point for developing hypotheses about your
model's behavior, and for identifying subsets of examples and/or creating
counterfactual examples that test those hypotheses.

### Salience Maps for Text: Theoretical background and LIT overview

All methods calculate salience, but there are subtle differences in their
approaches towards calculating a salience score for each token. Grad L2 Norm
only produces absolute salience scores while other methods like Grad  Input
(and also Integrated Gradients and LIME) produce signed values, leading to an
improved interpretation of whether a token has positive or negative influence on
the prediction.

**_LIT uses different color scales to represent signed and unsigned salience scores_**.
Methods that produce unsigned salience values, such as Grad L2 Norm, use a
purple scale where darker colors indicate greater salience, whereas the other
methods use a red-to-green scale, with red denoting negative scores and green
denoting positive.

{%  include partials/info-box,
    title: 'Interpreting salience polarity',
    text: "Salience is always relative to the model's prediction of one class.
        Intuitively, a positive influence score (attribution) for a token (or
        word, depending on your method) in an example means that if this token
        was removed we expect a drop in model confidence in the prediction of
        the class. Similarly, removing a negative token would correspond to an
        increase in the model's confidence in the prediction of this class."%}

{%  include partials/inset-image,
    image: '/assets/images/text-salience-image-2.png',
    caption: 'The tokens from same example in the SST-2 dataset can have
        dramatically different scores depending on the interpreter, as seen in
        this screenshot. Different salience interpreters output scores in
        different ranges, for example, Grad L2 Norm outputs unsigned values in
        the range from 0 to 1, denoted by the purple colors (more purple means
        closer to one), whereas others output signed scores in the range -1 to
        1, denoted by the pink to green color scale.'%}

#### Token-Based Methods

[Gradient saturation](https://towardsdatascience.com/the-vanishing-gradient-problem-69bf08b15484)
is a potential problem for all of the Gradient based methods, such as
[Grad L2 Norm](https://aclanthology.org/P18-1032/) and
[Grad  Input](https://arxiv.org/abs/1412.6815), that we need to look out
for. Essentially if the model learning saturates for a particular token, then
its gradient goes to zero and appears to have zero salience. At the same time,
some tokens actually have a zero salience score, because they do not affect the
predictions. And there is no simple way to tell if a token that we are
interested in is legitimately irrelevant or if we are just observing the effects
of gradient saturation.

The [integrated gradients](https://arxiv.org/pdf/1703.01365.pdf) method
addresses the gradient saturation problem by enriching gradients with
embeddings.
[Tokens](https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/)
are the discrete building blocks of text sequences, but they can also be
represented as vectors in a
[continuous embedding space](https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/).
IG computes per-token salience as the average salience over a set of local
gradients computed by interpolating between the token's embedding vectors and a
baseline (typically the zero vector). The tradeoff is that IG requires more
effort to identify the right number of interpolation steps to be
effective (configurable in LIT's interface), with the number of steps
correlating directly with runtime. It also requires more information,
which the model may or may not be able to provide.

{%  include partials/inset-image,
    image: '/assets/images/text-salience-image-3.png',
    caption: 'Integrated gradients can be configured to explain a specific
        class, to normalize the data during analysis, and to interpolate a
        given number of steps (between 5 and 100).'%}

#### Blackbox Methods

Some models do not provide tokens or token-level gradients, effectively making
them blackboxes. [LIME](https://arxiv.org/pdf/1602.04938v3.pdf) can be used with
these models. LIME works by generating a set of perturbed inputs, generally, by
dropping out or masking tokens, and training a local linear model to reconstruct
the original model's predictions. The weights of this linear model are treated
as the salience values.

LIME has two limitations, compared to gradient-based methods:

1.  it can be slow as it requires many evaluations of the model, and
2.  it can be noisy on longer inputs where there are more tokens to ablate.

We can increase **_the number of samples to be used for LIME_** within LIT to
counter the potential noisiness, however this is at the cost of computation
time.

{%  include partials/inset-image,
    image: '/assets/images/text-salience-image-4.png',
    caption: 'LIME can be configured to explain a specific output field and/or
        class, to use a specific masking token, and to use a specific seed for
        its random number generator. The most often used configuration
        parameters are the number of samples and kerne size, which can reduce
        noise in the results, but also affect the time required for each run.'%}

Another interesting difference between the gradient based methods and LIME lies
in how they analyze the input. The gradient based methods use the model's
tokenizer, which splits up words into smaller constituents, whereas LIME splits
the text into words at whitespaces. Thus, LIME's word-level results are often
incomparable with the token-level results from other methods, as you can see in
the salience maps below.

{%  include partials/inset-image,
    image: '/assets/images/text-salience-image-2.png',
    caption: "LIME splits the input sentence based on whitespace and punctuation
        characters, whereas the other methods use the model's tokenizer to
        separate the input into its constituent parts."%}

### Single example use-case : Interpreting the salience maps module

Let's take a concrete example and walkthrough how we might use the salience maps
module and counterfactual generators to analyze the behavior of the `sst2-tiny`
model on the classification task.

First, let's refer back to our heuristic for choosing appropriate methods.
Because `sst2-tiny` does not have a LSTM architecture, we shouldn't rely too
much on Grad  Input. So, we are left with Grad L2 Norm, Integrated Gradients
and LIME to base our decisions on.

To gain some confidence in our heuristic, we look for examples where Grad 
Input performs poorly compared to the other methods. There are quite a few in
the dataset, for example the sentence below where Grad  Input predicts
completely opposite salience scores to its counterparts.

{%  include partials/inset-image,
    image: '/assets/images/text-salience-image-10.png',
    caption: 'An example of how Grad  Input can perform poorly&mdash;all pink
        values, the opposite of what. other methods found&mdash;on certain input
        and model architecture combinations.'%}

#### Use Case 1: Sexism Analysis with Counterfactuals

Coming back to our use-case, we want to investigate if the model displays sexist
behavior for a particular input sentence. We take a datapoint with a negative
sentiment label, which talks about the performance of an actress in the movie.

The key words/tokens (based on salience scores across the three chosen methods)
in this sentence are hampered, lifetime-channel, lead, actress, her
and depth. The only words out of this which are related to gender are
actress and her. The words actress and her get a significant weight
for both Grad L2 Norm and IG, and is assigned a positive score (IG scores are
slightly stronger than Grad L2 Norm scores), indicating that the gender of the
person is helping the model be sure of its predictions of this sentence being a
negative review sentiment. However for LIME, the salience scores for these two
words is a small negative number, indicating that the gender of the model is
actually causing a small decrease in model confidence for the prediction of this
being a negative review. Even with this small disparity between the token-based
and blackbox methods in the gender related words in the sentence, it turns out
that these are not the most important words. Hampered, lifetime-channel and
plot are the dominating words/tokens for this particular example in helping
the model make its decision. We still want to explore if reversing the gender
might change this. Would it make the model give more or less importance to other
tokens or the tokens we replaced? Would it change the model prediction
confidence scores?

To do this, we generate a counterfactual example using the Datapoint
Editor which is located right beside the Data Table in the UI, changing
"actress" with "actor" and "her" with "his" after selecting our datapoint of
interest. An alternative to this approach is to use the Word replacer under the
Counterfactuals tab in the bottom half of the LIT app to achieve the same task.
If our model is predicting a negative sentiment due to sexist influences towards
actress or her, then the hypothesis is that it should show opposite
sentiments if we flip those key tokens.

{%  include partials/inset-image,
    image: '/assets/images/text-salience-image-11.png',
    caption: 'Manually generating a counterfactual example in the Datapoint
        Editor, in this case changing "actress" to "actor" and "her" to "his",
        does not induce much change in the token salience scores.'%}

However, it turns out that there is very minimal (and hence negligible) change
in the salience score values of any of the tokens. The model doesn't change its
prediction either. It still predicts this to be a negative review sentiment with
approximately the same prediction confidence. This indicates that at least for
this particular example, our model isn't displaying sexist behavior and is
actually making its prediction based on key tokens in the sentence which are not
related to the gender of the actress/actor.

#### Use Case 2: Pairwise Comparisons

Let's take another example. This time we consider the sentence a sometimes
tedious film and generate three counterfactuals, first by replacing the two
words sometimes and tedious with their respective antonyms one-by-one and
then together to observe the changes in predictions and  salience.

To create the counterfactuals, we can simply use the Datapoint Editor which is
located right beside the Data Table in the UI. We can just select our data point
of interest (data point 6), and then replace the words we are interested in with
the respective substitutes. Then we assign a `label` to the newly created
sentence and add it to our data. For this particular example, we are assigning 0
when "tedious" appears and 1 when "exciting" appears in the sentence. An
alternative to this approach is to use the Word replacer under the
Counterfactuals tab in the bottom half of the LIT app to achieve the same task.

{%  include partials/inset-image,
    image: '/assets/images/text-salience-image-12.png',
    caption: 'The Data Table and Datapoint Editor modules showing the three
        manually generated counterfactuals that will be used to explore pairwise
        comparisons of salience results.'%}

We can pin the original sentence in the data table and then cycle through the
three available pairs by selecting each of the new sentences as our primary
selection. This will give us a comparison-type output in the Salience Maps
module between the pinned and the selected examples.

{%  include partials/inset-image,
    image: '/assets/images/text-salience-image-13.png',
    caption: 'A table of the salience scores for each token in the inputs.'%}

When we replace sometimes with often, it gets a negative score of almost
equal magnitude (reversing polarity) from LIME which makes sense, because
often makes the next word in the sentence more impactful, linguistically. The
model prediction doesn't change either, and this new review is still classified
as having a negative sentiment.

{%  include partials/inset-image,
    image: '/assets/images/text-salience-image-14.png',
    caption: 'Replacing "sometimes" with "often" had a minimal impact on the
        gradient-based salience interpreters, but it did flip the polarity of
        that token in the LIME results.'%}

On replacing tedious with exciting, the salience for sometimes changes
from positive score to negative in the LIME output. In the IG output,
sometimes changes from a strong positive score to a weak positive score. These
changes are also justified because in this new sentence sometimes counters the
positive effect of the word exciting. The main negative word in our original
datapoint was tedious and by replacing this with a positive word exciting,
the model's classification of this new sentence also changes and the new
sentence is classified as positive with a very high confidence score.

{%  include partials/inset-image,
    image: '/assets/images/text-salience-image-15.png',
    caption: 'Replacing "tedious" with "exciting" had a substantial impact on
        the salience interpreters that output signed results, but only a minimal
        impact on the Grad L2 Norm interpreter.'%}

And finally, when we replace both sometimes tedious with often exciting, we
get strong positive scores from both LIME and IG, which is in line with the
overall strong positive sentiment of the sentence. The model predicts this new
sentence as positive sentiment, and the confidence score for this prediction is
slightly higher than the previous sentence where instead of often we had used
sometimes. This makes sense as well because often enhances the positive
sentiment slightly more than using sometimes in a positive review.

{%  include partials/inset-image,
    image: '/assets/images/text-salience-image-16.png',
    caption: 'Replacing both "sometimes" and "tedious" has a substantial impact
        on all salience interpreters, attenuating some results, accentuating
        others, and in the case of Grad  Input, demonstrating how this
        counterfactual captures an opposing sentiment to the original.'%}

In this second example, we mostly based our observation on LIME and IG, because
we could observe visual changes directly from the outputs of these methods. Grad
L2 Norm outputs were comparatively inconclusive, highlighting the need to
select appropriate methods and compare results between them. The model
predictions were in
line with our expected class labels and the confidence scores for predictions on
the counterfactuals could be justified using salience scores assigned to the new
tokens.

#### Use Case 3: Quality Assurance

A real life use case for the salience maps module can be in Quality Assurance.
For example, if there is a failure in production (e.g., wrong results for a search
query), we know the text input and the label the model predicted. We can use LIT
Salience Maps to debug this failure and figure out which tokens were most
influential in the prediction of the wrong label, and which alternative labels
could have been predicted (i.e., is there one clear winner, or are there a few
that are roughly the same?). Once we are done with debugging using LIT, we can
make the necessary changes to the model or training data (eg. adding fail-safes
or checks) to solve the production failure.

### Conclusion

Three gradient-based salience methods and one black box method are provided out
of the box to LIT users who need to use these post-hoc interpretations to make
sense of their language model's predictions. This diverse array of built-in
techniques can be used in combination with other LIT modules like
counterfactuals to support robust exploration of a model's behavior, as
illustrated in this tutorial. And as always, LIT strives to enable users to
[add their own salience interpreters](../../documentation/api.md#interpretation-components)
to allow for a wider variety of use cases beyond these default capabilities!

================
File: website/src/tutorials/tour.md
================
---
title: A Quick Tour of the Learning Interpretability Tool
layout: layouts/tutorial.liquid

hero-image: /assets/images/sample-banner.png
hero-title: "Tour"
hero-copy: "A Tour of LIT"

bc-anchor-category: "basics"
bc-category-title: "Basics"
bc-title: "Tour"

time: "7 minutes"
takeaways: "Get familiar with the interface of the Learning Interpretability Tool."
---

## A Quick Tour of the Learning Interpretability Tool
{% include partials/link-out link: "../../demos/glue.html", text: "Follow along in the hosted demo." %}
{% include partials/spacer height:10 %}

<div class="video-container">
  <iframe src="https://www.youtube.com/embed/CuRI_VK83dU" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div>
{% include partials/spacer height:10 %}

The Learning Interpretability Tool (LIT)  is a modular and extensible tool to interactively analyze and debug a variety of NLP models. LIT brings together common machine learning performance checks with interpretability methods specifically designed for NLP.

### Building blocks - modules, groups, and workspaces

**Modules, groups, and workspaces** form the building blocks of LIT. Modules are discrete windows in which you can perform a specific set of tasks or analyses. Workspaces display combinations of modules known as groups, so you can view different visualizations and interpretability methods side-by-side.

{% include partials/inset-image image: '/assets/images/lit-workspaces.png', 
  caption: 'Above: Building blocks of the Learning Interpretability Tool: (1) Modules, (2) Groups, (3) Static workspace, (4) Group-based workspace.'%}

LIT is divided into two workspaces - a Main workspace in the upper half of the interface, and a Group-based workspace in the lower half.

The Main workspace contains core modules that play a role in many analyses. By default, these include:
- **Embeddings** - explore UMAP and TSNE embeddings from your model.
- **Data Table** -  explore, navigate, and make selections from your dataset.
- **Datapoint Editor** - deep-dive into individual examples from your dataset.
- **Slice Editor** - create and manage slices of interest from your dataset through your LIT session. 

In the Group-based workspace, modules that offer related insights are organized together under tabs. By default, LIT offers a few default groups based on common analysis workflows: performance, predictions, explanations, and counterfactuals.
- Use the **Performance** group to compare the performance of models across the entire dataset, or on individual slices. 
- Explore model results on individual data points in the **Predictions** group.
- Investigate salience maps and attention for different data points in the **Explanations** group.
- Generate data points using automated generators in the **Counterfactuals** group, and evaluate your model on them instantly.

You can organize modules into groups and define your own layout when you [set up your LIT server](../../documentation/getting_started.html#lit-with-your-model).

### Toolbars

At the very top, youll see the LIT toolbar. Here, you can quickly check which models have been loaded, configure LIT, or share a URL to your session. Below that is a toolbar which makes it easier to perform actions applied across all of LIT. Here you can:
- Select data points by relationship, or by slice.
- Choose a feature to color data points, across all modules.
- Track the datapoint you are looking at, navigate to the next, mark a datapoint as a favorite, or clear your selection.
- Select the active models and dataset, including multiple models to compare.

{% include partials/inset-image image: '/assets/images/lit-toolbars.gif', 
  caption: 'Above: Use toolbars to quickly select or re-color data points, and switch to comparison mode.'%}

The footer at the very bottom of the interface will display any error messages. If anything goes wrong, you can also use the feedback button to let us know on GitHub.

### Using Modules

LIT makes it easy to interact within and across modules. Many modules in LIT have their own toolbars which contain controls and selections for that module. The results of actions performed in one module can impact other modules. For example, if you select five data points in the **Data Table** module, other modules (such as the **Scalars** module) will respond to your selection. Similarly, the active selection, no matter where it was triggered from, can be saved as a slice in the **Slice Editor**. Model performance for this new slice will now show up in the **Metrics** module. 

Finally, you can easily expand and collapse individual modules using the icons on the top-right of each module. 

{% include partials/info-box title: 'Visualizations that scale', 
  text: "Visualizations and results within modules can scale depending on if you're looking at one data point, multiple data points, one or multiple models. For instance, turning on the compare datapoints toggle allows you to compare a selected datapoint (known as the reference datapoint) to others by presenting individual results side-by-side within relevant modules." %}

Now that you are familiar with LITs interface, take LIT for a spin in our [demos](../../demos) or explore different [case studies](../).



================================================================
End of Codebase
================================================================
