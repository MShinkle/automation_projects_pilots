This file is a merged representation of a subset of the codebase, containing specifically included files and files not matching ignore patterns, combined into a single document by Repomix.
The content has been processed where empty lines have been removed.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
- Pay special attention to the Repository Description. These contain important context and guidelines specific to this project.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: **/*.py, **/*.md, **/*.txt
- Files matching these patterns are excluded: **/.git/**, **/.github/**, CHANGELOG.md
- Empty lines have been removed from all files
- Files are sorted by Git change count (files with more changes are at the bottom)

Additional Info:
----------------
User Provided Header:
-----------------------
This file is a consolidated single-file compilation of all code in the repository generated by Repomix. Note that .ipynb files have been converted to .py files.

================================================================
Directory Structure
================================================================
neuronpedia/butanium_dictionary_learning/dictionary_learning/__init__.py
neuronpedia/butanium_dictionary_learning/dictionary_learning/buffer.py
neuronpedia/butanium_dictionary_learning/dictionary_learning/cache.py
neuronpedia/butanium_dictionary_learning/dictionary_learning/config.py
neuronpedia/butanium_dictionary_learning/dictionary_learning/dictionary.py
neuronpedia/butanium_dictionary_learning/dictionary_learning/evaluation.py
neuronpedia/butanium_dictionary_learning/dictionary_learning/grad_pursuit.py
neuronpedia/butanium_dictionary_learning/dictionary_learning/interp.py
neuronpedia/butanium_dictionary_learning/dictionary_learning/scripts/train_crosscoder.py
neuronpedia/butanium_dictionary_learning/dictionary_learning/trainers/__init__.py
neuronpedia/butanium_dictionary_learning/dictionary_learning/trainers/batch_top_k.py
neuronpedia/butanium_dictionary_learning/dictionary_learning/trainers/crosscoder.py
neuronpedia/butanium_dictionary_learning/dictionary_learning/trainers/gated_anneal.py
neuronpedia/butanium_dictionary_learning/dictionary_learning/trainers/gdm.py
neuronpedia/butanium_dictionary_learning/dictionary_learning/trainers/jumprelu.py
neuronpedia/butanium_dictionary_learning/dictionary_learning/trainers/p_anneal.py
neuronpedia/butanium_dictionary_learning/dictionary_learning/trainers/standard.py
neuronpedia/butanium_dictionary_learning/dictionary_learning/trainers/top_k.py
neuronpedia/butanium_dictionary_learning/dictionary_learning/trainers/trainer.py
neuronpedia/butanium_dictionary_learning/dictionary_learning/training.py
neuronpedia/butanium_dictionary_learning/dictionary_learning/utils.py
neuronpedia/butanium_dictionary_learning/README.md
neuronpedia/butanium_dictionary_learning/requirements.txt
neuronpedia/np_list.py
neuronpedia/np_sae_feature.py
neuronpedia/np_vector.py
neuronpedia/requests/activation_request.py
neuronpedia/requests/base_request.py
neuronpedia/requests/list_request.py
neuronpedia/requests/sae_feature_request.py
neuronpedia/requests/steer_request.py
neuronpedia/requests/vector_request.py
neuronpedia/sample_data.py
README.md
tests/test_vector.py

================================================================
Files
================================================================

================
File: neuronpedia/butanium_dictionary_learning/dictionary_learning/__init__.py
================
from .dictionary import AutoEncoder, GatedAutoEncoder, JumpReluAutoEncoder, CrossCoder
from .buffer import ActivationBuffer

================
File: neuronpedia/butanium_dictionary_learning/dictionary_learning/buffer.py
================
import torch as t
from nnsight import LanguageModel
import gc
from tqdm import tqdm
from .config import DEBUG
if DEBUG:
    tracer_kwargs = {"scan": True, "validate": True}
else:
    tracer_kwargs = {"scan": False, "validate": False}
class ActivationBuffer:
    """
    Implements a buffer of activations. The buffer stores activations from a model,
    yields them in batches, and refreshes them when the buffer is less than half full.
    """
    def __init__(
        self,
        data,  # generator which yields text data
        model: LanguageModel,  # LanguageModel from which to extract activations
        submodule,  # submodule of the model from which to extract activations
        d_submodule=None,  # submodule dimension; if None, try to detect automatically
        io="out",  # can be 'in' or 'out'; whether to extract input or output activations
        n_ctxs=3e4,  # approximate number of contexts to store in the buffer
        ctx_len=128,  # length of each context
        refresh_batch_size=512,  # size of batches in which to process the data when adding to buffer
        out_batch_size=8192,  # size of batches in which to yield activations
        device="cpu",  # device on which to store the activations
    ):
        if io not in ["in", "out"]:
            raise ValueError("io must be either 'in' or 'out'")
        if d_submodule is None:
            try:
                if io == "in":
                    d_submodule = submodule.in_features
                else:
                    d_submodule = submodule.out_features
            except:
                raise ValueError(
                    "d_submodule cannot be inferred and must be specified directly"
                )
        self.activations = t.empty(0, d_submodule, device=device)
        self.read = t.zeros(0).bool()
        self.data = data
        self.model = model
        self.submodule = submodule
        self.d_submodule = d_submodule
        self.io = io
        self.n_ctxs = n_ctxs
        self.ctx_len = ctx_len
        self.activation_buffer_size = n_ctxs * ctx_len
        self.refresh_batch_size = refresh_batch_size
        self.out_batch_size = out_batch_size
        self.device = device
    def __iter__(self):
        return self
    def __next__(self):
        """
        Return a batch of activations
        """
        with t.no_grad():
            # if buffer is less than half full, refresh
            if (~self.read).sum() < self.activation_buffer_size // 2:
                self.refresh()
            # return a batch
            unreads = (~self.read).nonzero().squeeze()
            idxs = unreads[
                t.randperm(len(unreads), device=unreads.device)[: self.out_batch_size]
            ]
            self.read[idxs] = True
            return self.activations[idxs]
    def text_batch(self, batch_size=None):
        """
        Return a list of text
        """
        if batch_size is None:
            batch_size = self.refresh_batch_size
        try:
            return [next(self.data) for _ in range(batch_size)]
        except StopIteration:
            raise StopIteration("End of data stream reached")
    def tokenized_batch(self, batch_size=None):
        """
        Return a batch of tokenized inputs.
        """
        texts = self.text_batch(batch_size=batch_size)
        return self.model.tokenizer(
            texts,
            return_tensors="pt",
            max_length=self.ctx_len,
            padding=True,
            truncation=True,
        )
    def refresh(self):
        gc.collect()
        t.cuda.empty_cache()
        self.activations = self.activations[~self.read]
        current_idx = len(self.activations)
        new_activations = t.empty(
            self.activation_buffer_size, self.d_submodule, device=self.device
        )
        new_activations[: len(self.activations)] = self.activations
        self.activations = new_activations
        # Optional progress bar when filling buffer. At larger models / buffer sizes (e.g. gemma-2-2b, 1M tokens on a 4090) this can take a couple minutes.
        # pbar = tqdm(total=self.activation_buffer_size, initial=current_idx, desc="Refreshing activations")
        while current_idx < self.activation_buffer_size:
            with t.no_grad():
                with self.model.trace(
                    self.text_batch(),
                    **tracer_kwargs,
                    invoker_args={"truncation": True, "max_length": self.ctx_len},
                ):
                    if self.io == "in":
                        hidden_states = self.submodule.input[0].save()
                    else:
                        hidden_states = self.submodule.output.save()
                    input = self.model.input.save()
            attn_mask = input.value[1]["attention_mask"]
            hidden_states = hidden_states.value
            if isinstance(hidden_states, tuple):
                hidden_states = hidden_states[0]
            hidden_states = hidden_states[attn_mask != 0]
            remaining_space = self.activation_buffer_size - current_idx
            assert remaining_space > 0
            hidden_states = hidden_states[:remaining_space]
            self.activations[current_idx : current_idx + len(hidden_states)] = (
                hidden_states.to(self.device)
            )
            current_idx += len(hidden_states)
            # pbar.update(len(hidden_states))
        # pbar.close()
        self.read = t.zeros(len(self.activations), dtype=t.bool, device=self.device)
    @property
    def config(self):
        return {
            "d_submodule": self.d_submodule,
            "io": self.io,
            "n_ctxs": self.n_ctxs,
            "ctx_len": self.ctx_len,
            "refresh_batch_size": self.refresh_batch_size,
            "out_batch_size": self.out_batch_size,
            "device": self.device,
        }
    def close(self):
        """
        Close the text stream and the underlying compressed file.
        """
        self.text_stream.close()
class HeadActivationBuffer:
    """
    This is specifically designed for training SAEs for individual attn heads in Llama3.
    Much redundant code; can eventually be merged to ActivationBuffer.
    Implements a buffer of activations. The buffer stores activations from a model,
    yields them in batches, and refreshes them when the buffer is less than half full.
    """
    def __init__(
        self,
        data,  # generator which yields text data
        model: LanguageModel,  # LanguageModel from which to extract activations
        layer,  # submodule of the model from which to extract activations
        n_ctxs=3e4,  # approximate number of contexts to store in the buffer
        ctx_len=128,  # length of each context
        refresh_batch_size=512,  # size of batches in which to process the data when adding to buffer
        out_batch_size=8192,  # size of batches in which to yield activations
        device="cpu",  # device on which to store the activations
        apply_W_O=False,
        remote=False,
    ):
        self.layer = layer
        self.n_heads = model.config.num_attention_heads
        self.resid_dim = model.config.hidden_size
        self.head_dim = self.resid_dim // self.n_heads
        self.data = data
        self.model = model
        self.n_ctxs = n_ctxs
        self.ctx_len = ctx_len
        self.refresh_batch_size = refresh_batch_size
        self.out_batch_size = out_batch_size
        self.device = device
        self.apply_W_O = apply_W_O
        self.remote = remote
        self.activations = t.empty(
            0, self.n_heads, self.head_dim, device=device
        )  # [seq-pos, n_layers, n_head, head_dim]
        self.read = t.zeros(0).bool()
    def __iter__(self):
        return self
    def __next__(self):
        """
        Return a batch of activations
        """
        with t.no_grad():
            # if buffer is less than half full, refresh
            if (~self.read).sum() < self.n_ctxs * self.ctx_len // 2:
                self.refresh()
            # return a batch
            unreads = (~self.read).nonzero().squeeze()
            idxs = unreads[
                t.randperm(len(unreads), device=unreads.device)[: self.out_batch_size]
            ]
            self.read[idxs] = True
            return self.activations[idxs]
    def text_batch(self, batch_size=None):
        """
        Return a list of text
        """
        if batch_size is None:
            batch_size = self.refresh_batch_size
        try:
            return [next(self.data) for _ in range(batch_size)]
        except StopIteration:
            raise StopIteration("End of data stream reached")
    def tokenized_batch(self, batch_size=None):
        """
        Return a batch of tokenized inputs.
        """
        texts = self.text_batch(batch_size=batch_size)
        return self.model.tokenizer(
            texts,
            return_tensors="pt",
            max_length=self.ctx_len,
            padding=True,
            truncation=True,
        )
    def refresh(self):
        self.activations = self.activations[~self.read]
        while len(self.activations) < self.n_ctxs * self.ctx_len:
            with t.no_grad():
                with self.model.trace(
                    self.text_batch(),
                    **tracer_kwargs,
                    invoker_args={"truncation": True, "max_length": self.ctx_len},
                    remote=self.remote,
                ):
                    input = self.model.input.save()
                    hidden_states = self.model.model.layers[
                        self.layer
                    ].self_attn.o_proj.input[0][
                        0
                    ]  # .save()
                    if isinstance(hidden_states, tuple):
                        hidden_states = hidden_states[0]
                    # Reshape by head
                    new_shape = hidden_states.size()[:-1] + (
                        self.n_heads,
                        self.head_dim,
                    )  # (batch_size, seq_len, n_heads, head_dim)
                    hidden_states = hidden_states.view(*new_shape)
                    # Optionally map from head dim to resid dim
                    if self.apply_W_O:
                        hidden_states_W_O_shape = hidden_states.size()[:-1] + (
                            self.model.config.hidden_size,
                        )  # (batch_size, seq_len, n_heads, resid_dim)
                        hidden_states_W_O = t.zeros(
                            hidden_states_W_O_shape, device=hidden_states.device
                        )
                        for h in range(self.n_heads):
                            start = h * self.head_dim
                            end = (h + 1) * self.head_dim
                            hidden_states_W_O[..., h, start:end] = hidden_states[
                                ..., h, :
                            ]
                        hidden_states = (
                            self.model.model.layers[self.layer]
                            .self_attn.o_proj(hidden_states_W_O)
                            .save()
                        )
            # Apply attention mask
            attn_mask = input.value[1]["attention_mask"]
            hidden_states = hidden_states[attn_mask != 0]
            # Save results
            self.activations = t.cat(
                [self.activations, hidden_states.to(self.device)], dim=0
            )
            self.read = t.zeros(len(self.activations), dtype=t.bool, device=self.device)
    @property
    def config(self):
        return {
            "layer": self.layer,
            "n_ctxs": self.n_ctxs,
            "ctx_len": self.ctx_len,
            "refresh_batch_size": self.refresh_batch_size,
            "out_batch_size": self.out_batch_size,
            "device": self.device,
        }
    def close(self):
        """
        Close the text stream and the underlying compressed file.
        """
        self.text_stream.close()
class NNsightActivationBuffer:
    """
    Implements a buffer of activations. The buffer stores activations from a model,
    yields them in batches, and refreshes them when the buffer is less than half full.
    """
    def __init__(
        self,
        data,  # generator which yields text data
        model: LanguageModel,  # LanguageModel from which to extract activations
        submodule,  # submodule of the model from which to extract activations
        d_submodule=None,  # submodule dimension; if None, try to detect automatically
        io="out",  # can be 'in' or 'out'; whether to extract input or output activations, "in_and_out" for transcoders
        n_ctxs=3e4,  # approximate number of contexts to store in the buffer
        ctx_len=128,  # length of each context
        refresh_batch_size=512,  # size of batches in which to process the data when adding to buffer
        out_batch_size=8192,  # size of batches in which to yield activations
        device="cpu",  # device on which to store the activations
    ):
        if io not in ["in", "out", "in_and_out"]:
            raise ValueError("io must be either 'in' or 'out' or 'in_and_out'")
        if d_submodule is None:
            try:
                if io == "in":
                    d_submodule = submodule.in_features
                else:
                    d_submodule = submodule.out_features
            except:
                raise ValueError(
                    "d_submodule cannot be inferred and must be specified directly"
                )
        if io in ["in", "out"]:
            self.activations = t.empty(0, d_submodule, device=device)
        elif io == "in_and_out":
            self.activations = t.empty(0, 2, d_submodule, device=device)
        self.read = t.zeros(0).bool()
        self.data = data
        self.model = model
        self.submodule = submodule
        self.d_submodule = d_submodule
        self.io = io
        self.n_ctxs = n_ctxs
        self.ctx_len = ctx_len
        self.refresh_batch_size = refresh_batch_size
        self.out_batch_size = out_batch_size
        self.device = device
    def __iter__(self):
        return self
    def __next__(self):
        """
        Return a batch of activations
        """
        with t.no_grad():
            # if buffer is less than half full, refresh
            if (~self.read).sum() < self.n_ctxs * self.ctx_len // 2:
                self.refresh()
            # return a batch
            unreads = (~self.read).nonzero().squeeze()
            idxs = unreads[
                t.randperm(len(unreads), device=unreads.device)[: self.out_batch_size]
            ]
            self.read[idxs] = True
            return self.activations[idxs]
    def tokenized_batch(self, batch_size=None):
        """
        Return a batch of tokenized inputs.
        """
        texts = self.text_batch(batch_size=batch_size)
        return self.model.tokenizer(
            texts,
            return_tensors="pt",
            max_length=self.ctx_len,
            padding=True,
            truncation=True,
        )
    def token_batch(self, batch_size=None):
        """
        Return a list of text
        """
        if batch_size is None:
            batch_size = self.refresh_batch_size
        try:
            return t.tensor(
                [next(self.data) for _ in range(batch_size)], device=self.device
            )
        except StopIteration:
            raise StopIteration("End of data stream reached")
    def text_batch(self, batch_size=None):
        """
        Return a list of text
        """
        # if batch_size is None:
        #     batch_size = self.refresh_batch_size
        # try:
        #     return [next(self.data) for _ in range(batch_size)]
        # except StopIteration:
        #     raise StopIteration("End of data stream reached")
        return self.token_batch(batch_size)
    def _reshaped_activations(self, hidden_states):
        hidden_states = hidden_states.value
        if isinstance(hidden_states, tuple):
            hidden_states = hidden_states[0]
        batch_size, seq_len, d_model = hidden_states.shape
        hidden_states = hidden_states.view(batch_size * seq_len, d_model)
        return hidden_states
    def refresh(self):
        self.activations = self.activations[~self.read]
        while len(self.activations) < self.n_ctxs * self.ctx_len:
            with t.no_grad(), self.model.trace(
                self.token_batch(),
                **tracer_kwargs,
                invoker_args={"truncation": True, "max_length": self.ctx_len},
            ):
                if self.io in ["in", "in_and_out"]:
                    hidden_states_in = self.submodule.input[0].save()
                if self.io in ["out", "in_and_out"]:
                    hidden_states_out = self.submodule.output.save()
            if self.io == "in":
                hidden_states = self._reshaped_activations(hidden_states_in)
            elif self.io == "out":
                hidden_states = self._reshaped_activations(hidden_states_out)
            elif self.io == "in_and_out":
                hidden_states_in = self._reshaped_activations(
                    hidden_states_in
                ).unsqueeze(1)
                hidden_states_out = self._reshaped_activations(
                    hidden_states_out
                ).unsqueeze(1)
                hidden_states = t.cat([hidden_states_in, hidden_states_out], dim=1)
            self.activations = t.cat(
                [self.activations, hidden_states.to(self.device)], dim=0
            )
            self.read = t.zeros(len(self.activations), dtype=t.bool, device=self.device)
    @property
    def config(self):
        return {
            "d_submodule": self.d_submodule,
            "io": self.io,
            "n_ctxs": self.n_ctxs,
            "ctx_len": self.ctx_len,
            "refresh_batch_size": self.refresh_batch_size,
            "out_batch_size": self.out_batch_size,
            "device": self.device,
        }
    def close(self):
        """
        Close the text stream and the underlying compressed file.
        """
        self.text_stream.close()

================
File: neuronpedia/butanium_dictionary_learning/dictionary_learning/cache.py
================
import torch as th
import torch.nn as nn
from torch.utils.data import DataLoader
from datasets import Dataset
from nnsight import LanguageModel
from typing import Tuple, List
import numpy as np
import os
from tqdm.auto import tqdm
import json
from .config import DEBUG
if DEBUG:
    tracer_kwargs = {"scan": True, "validate": True}
else:
    tracer_kwargs = {"scan": False, "validate": False}
class ActivationShard:
    def __init__(self, store_dir: str, shard_idx: int):
        self.shard_file = os.path.join(store_dir, f"shard_{shard_idx}.memmap")
        with open(self.shard_file.replace(".memmap", ".meta"), "r") as f:
            self.shape = tuple(json.load(f)["shape"])
        self.activations = np.memmap(
            self.shard_file, dtype=np.float32, mode="r", shape=self.shape
        )
    def __len__(self):
        return self.activations.shape[0]
    def __getitem__(self, *indices):
        return th.tensor(self.activations[*indices], dtype=th.float32)
class ActivationCache:
    def __init__(self, store_dir: str):
        self.store_dir = store_dir
        self.config = json.load(open(os.path.join(store_dir, "config.json"), "r"))
        self.shards = [
            ActivationShard(store_dir, i) for i in range(self.config["shard_count"])
        ]
        self._range_to_shard_idx = np.cumsum([0] + [s.shape[0] for s in self.shards])
    def __len__(self):
        return self.config["total_size"]
    def __getitem__(self, index: int):
        shard_idx = np.searchsorted(self._range_to_shard_idx, index, side="right") - 1
        offset = index - self._range_to_shard_idx[shard_idx]
        shard = self.shards[shard_idx]
        return shard[offset]
    @staticmethod
    def get_activations(submodule: nn.Module, io: str):
        if io == "in":
            return submodule.input[0]
        else:
            return submodule.output[0]
    @staticmethod
    def collate_store_shards(
        store_dirs: Tuple[str],
        shard_count: int,
        activation_cache: List[th.Tensor],
        submodule_names: Tuple[str],
        shuffle_shards: bool = True,
        io: str = "out",
    ):
        for i, name in enumerate(submodule_names):
            activations = th.cat(
                activation_cache[i], dim=0
            )  # (N x B x T) x D (N = number of batches per shard)
            print(f"Storing activation shard ({activations.shape}) for {name} {io}")
            if shuffle_shards:
                idx = np.random.permutation(activations.shape[0])
                activations = activations[idx]
            # use memmap to store activations
            memmap_file = os.path.join(store_dirs[i], f"shard_{shard_count}.memmap")
            memmap_file_meta = memmap_file.replace(".memmap", ".meta")
            memmap = np.memmap(
                memmap_file,
                dtype=np.float32,
                mode="w+",
                shape=(activations.shape[0], activations.shape[1]),
            )
            memmap[:] = activations.numpy()
            memmap.flush()
            with open(memmap_file_meta, "w") as f:
                json.dump({"shape": list(activations.shape)}, f)
            del memmap
    @th.no_grad()
    @staticmethod
    def collect(
        data: Dataset,
        submodules: Tuple[nn.Module],
        submodule_names: Tuple[str],
        model: LanguageModel,
        store_dir: str,
        batch_size: int = 64,
        context_len: int = 128,
        shard_size: int = 10**6,
        d_model: int = 1024,
        shuffle_shards: bool = False,
        io: str = "out",
        num_workers: int = 8,
        max_total_tokens: int = 10**8,
        last_submodule: nn.Module = None,
    ):
        dataloader = DataLoader(data, batch_size=batch_size, num_workers=num_workers)
        activation_cache = [[] for _ in submodules]
        store_dirs = [
            os.path.join(store_dir, f"{submodule_names[i]}_{io}")
            for i in range(len(submodules))
        ]
        for store_dir in store_dirs:
            os.makedirs(store_dir, exist_ok=True)
        total_size = 0
        current_size = 0
        shard_count = 0
        for batch in tqdm(dataloader, desc="Collecting activations"):
            tokens = model.tokenizer(
                batch,
                max_length=context_len,
                truncation=True,
                return_tensors="pt",
                padding=True,
            ).to(model.device)
            attention_mask = tokens["attention_mask"]
            with model.trace(
                tokens,
                **tracer_kwargs,
            ):
                for i, submodule in enumerate(submodules):
                    local_activations = (
                        ActivationCache.get_activations(submodule, io)
                        .reshape(-1, d_model)
                        .save()
                    )  # (B x T) x D
                    activation_cache[i].append(local_activations)
                if last_submodule is not None:
                    last_submodule.output.stop()
            for i in range(len(submodules)):
                activation_cache[i][-1] = (
                    activation_cache[i][-1]
                    .value[attention_mask.reshape(-1).bool()]
                    .cpu()
                    .to(th.float32)
                )  # remove padding tokens
            current_size += activation_cache[0][-1].shape[0]
            if current_size > shard_size:
                ActivationCache.collate_store_shards(
                    store_dirs,
                    shard_count,
                    activation_cache,
                    submodule_names,
                    shuffle_shards,
                    io,
                )
                shard_count += 1
                total_size += current_size
                current_size = 0
                activation_cache = [[] for _ in submodules]
            if total_size > max_total_tokens:
                print(f"Max total tokens reached. Stopping collection.")
                break
        if current_size > 0:
            ActivationCache.collate_store_shards(
                store_dirs,
                shard_count,
                activation_cache,
                submodule_names,
                shuffle_shards,
                io,
            )
        # store configs
        for i, store_dir in enumerate(store_dirs):
            with open(os.path.join(store_dir, "config.json"), "w") as f:
                json.dump(
                    {
                        "batch_size": batch_size,
                        "context_len": context_len,
                        "shard_size": shard_size,
                        "d_model": d_model,
                        "shuffle_shards": shuffle_shards,
                        "io": io,
                        "total_size": total_size,
                        "shard_count": shard_count,
                    },
                    f,
                )
        print(f"Finished collecting activations. Total size: {total_size}")
class PairedActivationCache:
    def __init__(self, store_dir_1: str, store_dir_2: str):
        self.activation_cache_1 = ActivationCache(store_dir_1)
        self.activation_cache_2 = ActivationCache(store_dir_2)
        assert len(self.activation_cache_1) == len(self.activation_cache_2)
    def __len__(self):
        return len(self.activation_cache_1)
    def __getitem__(self, index: int):
        return th.stack(
            (self.activation_cache_1[index], self.activation_cache_2[index]), dim=0
        )
class ActivationCacheTuple:
    def __init__(self, *store_dirs: str):
        self.activation_caches = [
            ActivationCache(store_dir) for store_dir in store_dirs
        ]
        assert len(self.activation_caches) > 0
        for i in range(1, len(self.activation_caches)):
            assert len(self.activation_caches[i]) == len(self.activation_caches[0])
    def __len__(self):
        return len(self.activation_caches[0])
    def __getitem__(self, index: int):
        return th.stack([cache[index] for cache in self.activation_caches], dim=0)

================
File: neuronpedia/butanium_dictionary_learning/dictionary_learning/config.py
================
# debugging flag for use in other scripts
DEBUG = False

================
File: neuronpedia/butanium_dictionary_learning/dictionary_learning/dictionary.py
================
"""
Defines the dictionary classes
"""
from abc import ABC, abstractclassmethod, abstractmethod
from huggingface_hub import PyTorchModelHubMixin
import torch as th
import torch.nn as nn
import torch.nn.init as init
from torch.nn.functional import relu
import einops
from warnings import warn
class Dictionary(ABC, nn.Module, PyTorchModelHubMixin):
    """
    A dictionary consists of a collection of vectors, an encoder, and a decoder.
    """
    dict_size: int  # number of features in the dictionary
    activation_dim: int  # dimension of the activation vectors
    @abstractmethod
    def encode(self, x):
        """
        Encode a vector x in the activation space.
        """
        pass
    @abstractmethod
    def decode(self, f):
        """
        Decode a dictionary vector f (i.e. a linear combination of dictionary elements)
        """
        pass
    @classmethod
    @abstractmethod
    def from_pretrained(
        cls, path, from_hub=False, device=None, dtype=None, **kwargs
    ) -> "Dictionary":
        """
        Load a pretrained dictionary from a file or hub.
        Args:
            path: Path to local file or hub model id
            from_hub: If True, load from HuggingFace hub using PyTorchModelHubMixin
            device: Device to load the model to
            **kwargs: Additional arguments passed to loading function
        """
        model = super(Dictionary, cls).from_pretrained(path, **kwargs)
        if device is not None:
            model.to(device)
        if dtype is not None:
            model.to(dtype=dtype)
        return model
class AutoEncoder(Dictionary, nn.Module):
    """
    A one-layer autoencoder.
    """
    def __init__(self, activation_dim, dict_size):
        super().__init__()
        self.activation_dim = activation_dim
        self.dict_size = dict_size
        self.bias = nn.Parameter(th.zeros(activation_dim))
        self.encoder = nn.Linear(activation_dim, dict_size, bias=True)
        # rows of decoder weight matrix are unit vectors
        self.decoder = nn.Linear(dict_size, activation_dim, bias=False)
        dec_weight = th.randn_like(self.decoder.weight)
        dec_weight = dec_weight / dec_weight.norm(dim=0, keepdim=True)
        self.decoder.weight = nn.Parameter(dec_weight)
    def encode(self, x):
        return nn.ReLU()(self.encoder(x - self.bias))
    def decode(self, f):
        return self.decoder(f) + self.bias
    def forward(self, x, output_features=False, ghost_mask=None):
        """
        Forward pass of an autoencoder.
        x : activations to be autoencoded
        output_features : if True, return the encoded features as well as the decoded x
        ghost_mask : if not None, run this autoencoder in "ghost mode" where features are masked
        """
        if ghost_mask is None:  # normal mode
            f = self.encode(x)
            x_hat = self.decode(f)
            if output_features:
                return x_hat, f
            else:
                return x_hat
        else:  # ghost mode
            f_pre = self.encoder(x - self.bias)
            f_ghost = th.exp(f_pre) * ghost_mask.to(f_pre)
            f = nn.ReLU()(f_pre)
            x_ghost = self.decoder(
                f_ghost
            )  # note that this only applies the decoder weight matrix, no bias
            x_hat = self.decode(f)
            if output_features:
                return x_hat, x_ghost, f
            else:
                return x_hat, x_ghost
    @classmethod
    def from_pretrained(
        cls, path, dtype=th.float, from_hub=False, device=None, **kwargs
    ):
        if from_hub:
            return super().from_pretrained(path, dtype=dtype, device=device, **kwargs)
        # Existing custom loading logic
        state_dict = th.load(path)
        dict_size, activation_dim = state_dict["encoder.weight"].shape
        autoencoder = cls(activation_dim, dict_size)
        autoencoder.load_state_dict(state_dict)
        if device is not None:
            autoencoder.to(dtype=dtype, device=device)
        return autoencoder
class IdentityDict(Dictionary, nn.Module):
    """
    An identity dictionary, i.e. the identity function.
    """
    def __init__(self, activation_dim=None):
        super().__init__()
        self.activation_dim = activation_dim
        self.dict_size = activation_dim
    def encode(self, x):
        return x
    def decode(self, f):
        return f
    def forward(self, x, output_features=False, ghost_mask=None):
        if output_features:
            return x, x
        else:
            return x
    @classmethod
    def from_pretrained(cls, path, dtype=th.float, device=None):
        """
        Load a pretrained dictionary from a file.
        """
        return cls(None)
class GatedAutoEncoder(Dictionary, nn.Module):
    """
    An autoencoder with separate gating and magnitude networks.
    """
    def __init__(
        self, activation_dim, dict_size, initialization="default", device=None
    ):
        super().__init__()
        self.activation_dim = activation_dim
        self.dict_size = dict_size
        self.decoder_bias = nn.Parameter(th.empty(activation_dim, device=device))
        self.encoder = nn.Linear(activation_dim, dict_size, bias=False, device=device)
        self.r_mag = nn.Parameter(th.empty(dict_size, device=device))
        self.gate_bias = nn.Parameter(th.empty(dict_size, device=device))
        self.mag_bias = nn.Parameter(th.empty(dict_size, device=device))
        self.decoder = nn.Linear(dict_size, activation_dim, bias=False, device=device)
        if initialization == "default":
            self._reset_parameters()
        else:
            initialization(self)
    def _reset_parameters(self):
        """
        Default method for initializing GatedSAE weights.
        """
        # biases are initialized to zero
        init.zeros_(self.decoder_bias)
        init.zeros_(self.r_mag)
        init.zeros_(self.gate_bias)
        init.zeros_(self.mag_bias)
        # decoder weights are initialized to random unit vectors
        dec_weight = th.randn_like(self.decoder.weight)
        dec_weight = dec_weight / dec_weight.norm(dim=0, keepdim=True)
        self.decoder.weight = nn.Parameter(dec_weight)
    def encode(self, x, return_gate=False):
        """
        Returns features, gate value (pre-Heavyside)
        """
        x_enc = self.encoder(x - self.decoder_bias)
        # gating network
        pi_gate = x_enc + self.gate_bias
        f_gate = (pi_gate > 0).to(self.encoder.weight.dtype)
        # magnitude network
        pi_mag = self.r_mag.exp() * x_enc + self.mag_bias
        f_mag = nn.ReLU()(pi_mag)
        f = f_gate * f_mag
        # W_dec norm is not kept constant, as per Anthropic's April 2024 Update
        # Normalizing after encode, and renormalizing before decode to enable comparability
        f = f * self.decoder.weight.norm(dim=0, keepdim=True)
        if return_gate:
            return f, nn.ReLU()(pi_gate)
        return f
    def decode(self, f):
        # W_dec norm is not kept constant, as per Anthropic's April 2024 Update
        # Normalizing after encode, and renormalizing before decode to enable comparability
        f = f / self.decoder.weight.norm(dim=0, keepdim=True)
        return self.decoder(f) + self.decoder_bias
    def forward(self, x, output_features=False):
        f = self.encode(x)
        x_hat = self.decode(f)
        f = f * self.decoder.weight.norm(dim=0, keepdim=True)
        if output_features:
            return x_hat, f
        else:
            return x_hat
    @classmethod
    def from_pretrained(cls, path, from_hub=False, device=None, dtype=None, **kwargs):
        if from_hub:
            return super().from_pretrained(path, device=device, dtype=dtype, **kwargs)
        # Existing custom loading logic
        state_dict = th.load(path)
        dict_size, activation_dim = state_dict["encoder.weight"].shape
        autoencoder = cls(activation_dim, dict_size)
        autoencoder.load_state_dict(state_dict)
        if device is not None:
            autoencoder.to(device)
        return autoencoder
class JumpReluAutoEncoder(Dictionary, nn.Module):
    """
    An autoencoder with jump ReLUs.
    """
    def __init__(self, activation_dim, dict_size, device="cpu"):
        super().__init__()
        self.activation_dim = activation_dim
        self.dict_size = dict_size
        self.W_enc = nn.Parameter(th.empty(activation_dim, dict_size, device=device))
        self.b_enc = nn.Parameter(th.zeros(dict_size, device=device))
        self.W_dec = nn.Parameter(th.empty(dict_size, activation_dim, device=device))
        self.b_dec = nn.Parameter(th.zeros(activation_dim, device=device))
        self.threshold = nn.Parameter(th.zeros(dict_size, device=device))
        self.apply_b_dec_to_input = False
        # rows of decoder weight matrix are initialized to unit vectors
        self.W_enc.data = th.randn_like(self.W_enc)
        self.W_enc.data = self.W_enc / self.W_enc.norm(dim=0, keepdim=True)
        self.W_dec.data = self.W_enc.data.clone().T
    def encode(self, x, output_pre_jump=False):
        if self.apply_b_dec_to_input:
            x = x - self.b_dec
        pre_jump = x @ self.W_enc + self.b_enc
        f = nn.ReLU()(pre_jump * (pre_jump > self.threshold))
        f = f * self.W_dec.norm(dim=1)
        if output_pre_jump:
            return f, pre_jump
        else:
            return f
    def decode(self, f):
        f = f / self.W_dec.norm(dim=1)
        return f @ self.W_dec + self.b_dec
    def forward(self, x, output_features=False):
        """
        Forward pass of an autoencoder.
        x : activations to be autoencoded
        output_features : if True, return the encoded features (and their pre-jump version) as well as the decoded x
        """
        f = self.encode(x)
        x_hat = self.decode(f)
        if output_features:
            return x_hat, f
        else:
            return x_hat
    @classmethod
    def from_pretrained(
        cls,
        path: str | None = None,
        load_from_sae_lens: bool = False,
        from_hub: bool = False,
        dtype: th.dtype = th.float32,
        device: th.device | None = None,
        **kwargs,
    ):
        """
        Load a pretrained autoencoder from a file.
        If sae_lens=True, then pass **kwargs to sae_lens's
        loading function.
        """
        if not load_from_sae_lens:
            if from_hub:
                return super().from_pretrained(
                    path, device=device, dtype=dtype, **kwargs
                )
            state_dict = th.load(path)
            dict_size, activation_dim = state_dict["W_enc"].shape
            autoencoder = cls(activation_dim, dict_size)
            autoencoder.load_state_dict(state_dict)
        else:
            from sae_lens import SAE
            sae, cfg_dict, _ = SAE.from_pretrained(**kwargs)
            assert (
                cfg_dict["finetuning_scaling_factor"] == False
            ), "Finetuning scaling factor not supported"
            dict_size, activation_dim = cfg_dict["d_sae"], cfg_dict["d_in"]
            autoencoder = JumpReluAutoEncoder(activation_dim, dict_size, device=device)
            autoencoder.load_state_dict(sae.state_dict())
            autoencoder.apply_b_dec_to_input = cfg_dict["apply_b_dec_to_input"]
        if device is not None:
            device = autoencoder.W_enc.device
        return autoencoder.to(dtype=dtype, device=device)
# TODO merge this with AutoEncoder
class AutoEncoderNew(Dictionary, nn.Module):
    """
    The autoencoder architecture and initialization used in https://transformer-circuits.pub/2024/april-update/index.html#training-saes
    """
    def __init__(self, activation_dim, dict_size):
        super().__init__()
        self.activation_dim = activation_dim
        self.dict_size = dict_size
        self.encoder = nn.Linear(activation_dim, dict_size, bias=True)
        self.decoder = nn.Linear(dict_size, activation_dim, bias=True)
        # initialize encoder and decoder weights
        w = th.randn(activation_dim, dict_size)
        ## normalize columns of w
        w = w / w.norm(dim=0, keepdim=True) * 0.1
        ## set encoder and decoder weights
        self.encoder.weight = nn.Parameter(w.clone().T)
        self.decoder.weight = nn.Parameter(w.clone())
        # initialize biases to zeros
        init.zeros_(self.encoder.bias)
        init.zeros_(self.decoder.bias)
    def encode(self, x):
        return nn.ReLU()(self.encoder(x))
    def decode(self, f):
        return self.decoder(f)
    def forward(self, x, output_features=False):
        """
        Forward pass of an autoencoder.
        x : activations to be autoencoded
        """
        if not output_features:
            return self.decode(self.encode(x))
        else:  # TODO rewrite so that x_hat depends on f
            f = self.encode(x)
            x_hat = self.decode(f)
            # multiply f by decoder column norms
            f = f * self.decoder.weight.norm(dim=0, keepdim=True)
            return x_hat, f
    @classmethod
    def from_pretrained(cls, path, device=None, from_hub=False, dtype=None, **kwargs):
        if from_hub:
            return super().from_pretrained(path, device=device, dtype=dtype, **kwargs)
        state_dict = th.load(path)
        dict_size, activation_dim = state_dict["encoder.weight"].shape
        autoencoder = cls(activation_dim, dict_size)
        autoencoder.load_state_dict(state_dict)
        if device is not None:
            autoencoder.to(device)
        return autoencoder
class CrossCoderEncoder(nn.Module):
    """
    A cross-coder encoder
    """
    def __init__(
        self,
        activation_dim,
        dict_size,
        num_layers=None,
        same_init_for_all_layers: bool = False,
        norm_init_scale: float | None = None,
        encoder_layers: list[int] | None = None,
    ):
        super().__init__()
        if encoder_layers is None:
            if num_layers is None:
                raise ValueError(
                    "Either encoder_layers or num_layers must be specified"
                )
            encoder_layers = list(range(num_layers))
        else:
            num_layers = len(encoder_layers)
        self.encoder_layers = encoder_layers
        self.activation_dim = activation_dim
        self.dict_size = dict_size
        self.num_layers = num_layers
        if same_init_for_all_layers:
            weight = init.kaiming_uniform_(th.empty(activation_dim, dict_size))
            weight = weight.repeat(num_layers, 1, 1)
        else:
            weight = init.kaiming_uniform_(
                th.empty(num_layers, activation_dim, dict_size)
            )
        if norm_init_scale is not None:
            weight = weight / weight.norm(dim=1, keepdim=True) * norm_init_scale
        self.weight = nn.Parameter(weight)
        self.bias = nn.Parameter(th.zeros(dict_size))
    def forward(
        self,
        x: th.Tensor,
        return_no_sum: bool = False,
        select_features: list[int] | None = None,
    ) -> th.Tensor:  # (batch_size, activation_dim)
        """
        Convert activations to features for each layer
        Args:
            x: (batch_size, n_layers, activation_dim)
        Returns:
            f: (batch_size, dict_size)
        """
        x = x[:, self.encoder_layers]
        if select_features is not None:
            w = self.weight[:, :, select_features]
            bias = self.bias[select_features]
        else:
            w = self.weight
            bias = self.bias
        f = th.einsum("bld, ldf -> blf", x, w)
        if not return_no_sum:
            return relu(f.sum(dim=1) + bias)
        else:
            return relu(f.sum(dim=1) + bias), relu(f + bias)
class CrossCoderDecoder(nn.Module):
    """
    A cross-coder decoder
    """
    def __init__(
        self,
        activation_dim,
        dict_size,
        num_layers,
        same_init_for_all_layers: bool = False,
        norm_init_scale: float | None = None,
        init_with_weight: th.Tensor | None = None,
    ):
        super().__init__()
        self.activation_dim = activation_dim
        self.dict_size = dict_size
        self.num_layers = num_layers
        self.bias = nn.Parameter(th.zeros(num_layers, activation_dim))
        if init_with_weight is not None:
            self.weight = nn.Parameter(init_with_weight)
        else:
            if same_init_for_all_layers:
                weight = init.kaiming_uniform_(th.empty(dict_size, activation_dim))
                weight = weight.repeat(num_layers, 1, 1)
            else:
                weight = init.kaiming_uniform_(
                    th.empty(num_layers, dict_size, activation_dim)
                )
            if norm_init_scale is not None:
                weight = weight / weight.norm(dim=2, keepdim=True) * norm_init_scale
            self.weight = nn.Parameter(weight)
    def forward(
        self, f: th.Tensor, select_features: list[int] | None = None
    ) -> th.Tensor:  # (batch_size, n_layers, activation_dim)
        # f: (batch_size, n_layers, dict_size)
        """
        Convert features to activations for each layer
        Args:
            f: (batch_size, dict_size)
        Returns:
            x: (batch_size, n_layers, activation_dim)
        """
        if select_features is not None:
            w = self.weight[:, select_features]
        else:
            w = self.weight
        return th.einsum("bf, lfd -> bld", f, w) + self.bias
class CrossCoder(Dictionary, nn.Module):
    """
    A cross-coder using the AutoEncoderNew architecture for two models.
    encoder: shape (num_layers, activation_dim, dict_size)
    decoder: shape (num_layers, dict_size, activation_dim)
    """
    def __init__(
        self,
        activation_dim,
        dict_size,
        num_layers,
        same_init_for_all_layers=False,
        norm_init_scale: float | None = None,  # neel's default: 0.005
        init_with_transpose=True,
        encoder_layers: list[int] | None = None,
    ):
        """
        Args:
            same_init_for_all_layers: if True, initialize all layers with the same vector
            norm_init_scale: if not None, initialize the weights with a norm of this value
            init_with_transpose: if True, initialize the decoder weights with the transpose of the encoder weights
        """
        super().__init__()
        self.activation_dim = activation_dim
        self.dict_size = dict_size
        self.num_layers = num_layers
        self.encoder = CrossCoderEncoder(
            activation_dim,
            dict_size,
            num_layers,
            same_init_for_all_layers=same_init_for_all_layers,
            norm_init_scale=norm_init_scale,
            encoder_layers=encoder_layers,
        )
        if init_with_transpose:
            decoder_weight = einops.rearrange(
                self.encoder.weight.data.clone(),
                "num_layers activation_dim dict_size -> num_layers dict_size activation_dim",
            )
        else:
            decoder_weight = None
        self.decoder = CrossCoderDecoder(
            activation_dim,
            dict_size,
            num_layers,
            same_init_for_all_layers=same_init_for_all_layers,
            init_with_weight=decoder_weight,
            norm_init_scale=norm_init_scale,
        )
    def encode(
        self, x: th.Tensor, **kwargs
    ) -> th.Tensor:  # (batch_size, n_layers, dict_size)
        # x: (batch_size, n_layers, activation_dim)
        return self.encoder(x, **kwargs)
    def get_activations(
        self, x: th.Tensor, select_features: list[int] | None = None, **kwargs
    ) -> th.Tensor:
        f = self.encode(x, select_features=select_features, **kwargs)
        if select_features is not None:
            dw = self.decoder.weight[:, select_features]
        else:
            dw = self.decoder.weight
        return f * dw.norm(dim=2).sum(dim=0, keepdim=True)
    def decode(
        self, f: th.Tensor, **kwargs
    ) -> th.Tensor:  # (batch_size, n_layers, activation_dim)
        # f: (batch_size, n_layers, dict_size)
        return self.decoder(f, **kwargs)
    def forward(self, x: th.Tensor, output_features=False):
        """
        Forward pass of the cross-coder.
        x : activations to be encoded and decoded
        output_features : if True, return the encoded features as well as the decoded x
        """
        f = self.encode(x)
        x_hat = self.decode(f)
        if output_features:
            # Scale features by decoder column norms
            f_scaled = f * self.decoder.weight.norm(dim=2).sum(
                dim=0, keepdim=True
            )  # Also sum across layers for the loss
            return x_hat, f_scaled
        else:
            return x_hat
    @classmethod
    def from_pretrained(
        cls,
        path: str,
        dtype: th.dtype = th.float32,
        device: th.device | None = None,
        from_hub: bool = False,
        **kwargs,
    ):
        """
        Load a pretrained cross-coder from a file.
        """
        if from_hub:
            return super().from_pretrained(path, device=device, dtype=dtype, **kwargs)
        state_dict = th.load(path, map_location="cpu", weights_only=True)
        if "encoder.weight" not in state_dict:
            warn(
                "Cross-coder state dict was saved while torch.compiled was enabled. Fixing..."
            )
            state_dict = {k.split("_orig_mod.")[1]: v for k, v in state_dict.items()}
        num_layers, activation_dim, dict_size = state_dict["encoder.weight"].shape
        cross_coder = cls(activation_dim, dict_size, num_layers)
        cross_coder.load_state_dict(state_dict)
        if device is not None:
            cross_coder = cross_coder.to(device)
        return cross_coder.to(dtype=dtype)
    def resample_neurons(self, deads, activations):
        # https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-resampling
        # compute loss for each activation
        losses = (
            (activations - self.forward(activations)).norm(dim=-1).mean(dim=-1).square()
        )
        # sample input to create encoder/decoder weights from
        n_resample = min([deads.sum(), losses.shape[0]])
        print("Resampling", n_resample, "neurons")
        indices = th.multinomial(losses, num_samples=n_resample, replacement=False)
        sampled_vecs = activations[indices]  # (n_resample, num_layers, activation_dim)
        # get norm of the living neurons
        # encoder.weight: (num_layers, activation_dim, dict_size)
        # decoder.weight: (num_layers, dict_size, activation_dim)
        alive_norm = self.encoder.weight[:, :, ~deads].norm(dim=-2)
        alive_norm = alive_norm.mean(dim=-1)  # (num_layers)
        # convert to (num_layers, 1, 1)
        alive_norm = einops.repeat(alive_norm, "num_layers -> num_layers 1 1")
        # resample first n_resample dead neurons
        deads[deads.nonzero()[n_resample:]] = False
        self.encoder.weight[:, :, deads] = (
            sampled_vecs.permute(1, 2, 0) * alive_norm * 0.05
        )
        sampled_vecs = sampled_vecs.permute(1, 0, 2)
        self.decoder.weight[:, deads, :] = th.nn.functional.normalize(
            sampled_vecs, dim=-1
        )
        self.encoder.bias[deads] = 0.0

================
File: neuronpedia/butanium_dictionary_learning/dictionary_learning/evaluation.py
================
"""
Utilities for evaluating dictionaries on a model and dataset.
"""
import torch as t
from .buffer import ActivationBuffer, NNsightActivationBuffer
from nnsight import LanguageModel
from .config import DEBUG
def loss_recovered(
    text,  # a batch of text
    model: LanguageModel,  # an nnsight LanguageModel
    submodule,  # submodules of model
    dictionary,  # dictionaries for submodules
    max_len=None,  # max context length for loss recovered
    normalize_batch=False,  # normalize batch before passing through dictionary
    io="out",  # can be 'in', 'out', or 'in_and_out'
    tracer_args={
        "use_cache": False,
        "output_attentions": False,
    },  # minimize cache during model trace.
):
    """
    How much of the model's loss is recovered by replacing the component output
    with the reconstruction by the autoencoder?
    """
    if max_len is None:
        invoker_args = {}
    else:
        invoker_args = {"truncation": True, "max_length": max_len}
    # unmodified logits
    with model.trace(text, invoker_args=invoker_args):
        logits_original = model.output.save()
    logits_original = logits_original.value
    # logits when replacing component activations with reconstruction by autoencoder
    with model.trace(text, **tracer_args, invoker_args=invoker_args):
        if io == "in":
            x = submodule.input[0]
            if type(submodule.input.shape) == tuple:
                x = x[0]
            if normalize_batch:
                scale = (dictionary.activation_dim**0.5) / x.norm(dim=-1).mean()
                x = x * scale
        elif io == "out":
            x = submodule.output
            if type(submodule.output.shape) == tuple:
                x = x[0]
            if normalize_batch:
                scale = (dictionary.activation_dim**0.5) / x.norm(dim=-1).mean()
                x = x * scale
        elif io == "in_and_out":
            x = submodule.input[0]
            if type(submodule.input.shape) == tuple:
                x = x[0]
            print(f"x.shape: {x.shape}")
            if normalize_batch:
                scale = (dictionary.activation_dim**0.5) / x.norm(dim=-1).mean()
                x = x * scale
        else:
            raise ValueError(f"Invalid value for io: {io}")
        x = x.save()
    # pull this out so dictionary can be written without FakeTensor (top_k needs this)
    x_hat = dictionary(x.view(-1, x.shape[-1])).view(x.shape).to(model.dtype)
    # intervene with `x_hat`
    with model.trace(text, **tracer_args, invoker_args=invoker_args):
        if io == "in":
            x = submodule.input[0]
            if normalize_batch:
                scale = (dictionary.activation_dim**0.5) / x.norm(dim=-1).mean()
                x_hat = x_hat / scale
            if type(submodule.input.shape) == tuple:
                submodule.input[0][:] = x_hat
            else:
                submodule.input = x_hat
        elif io == "out":
            x = submodule.output
            if normalize_batch:
                scale = (dictionary.activation_dim**0.5) / x.norm(dim=-1).mean()
                x_hat = x_hat / scale
            if type(submodule.output.shape) == tuple:
                submodule.output = (x_hat,)
            else:
                submodule.output = x_hat
        elif io == "in_and_out":
            x = submodule.input[0]
            if normalize_batch:
                scale = (dictionary.activation_dim**0.5) / x.norm(dim=-1).mean()
                x_hat = x_hat / scale
            submodule.output = x_hat
        else:
            raise ValueError(f"Invalid value for io: {io}")
        logits_reconstructed = model.output.save()
    logits_reconstructed = logits_reconstructed.value
    # logits when replacing component activations with zeros
    with model.trace(text, **tracer_args, invoker_args=invoker_args):
        if io == "in":
            x = submodule.input[0]
            if type(submodule.input.shape) == tuple:
                submodule.input[0][:] = t.zeros_like(x[0])
            else:
                submodule.input = t.zeros_like(x)
        elif io in ["out", "in_and_out"]:
            x = submodule.output
            if type(submodule.output.shape) == tuple:
                submodule.output[0][:] = t.zeros_like(x[0])
            else:
                submodule.output = t.zeros_like(x)
        else:
            raise ValueError(f"Invalid value for io: {io}")
        input = model.input.save()
        logits_zero = model.output.save()
    logits_zero = logits_zero.value
    # get everything into the right format
    try:
        logits_original = logits_original.logits
        logits_reconstructed = logits_reconstructed.logits
        logits_zero = logits_zero.logits
    except:
        pass
    if isinstance(text, t.Tensor):
        tokens = text
    else:
        try:
            tokens = input[1]["input_ids"]
        except:
            tokens = input[1]["input"]
    # compute losses
    losses = []
    if hasattr(model, "tokenizer") and model.tokenizer is not None:
        loss_kwargs = {"ignore_index": model.tokenizer.pad_token_id}
    else:
        loss_kwargs = {}
    for logits in [logits_original, logits_reconstructed, logits_zero]:
        loss = t.nn.CrossEntropyLoss(**loss_kwargs)(
            logits[:, :-1, :].reshape(-1, logits.shape[-1]), tokens[:, 1:].reshape(-1)
        )
        losses.append(loss)
    return tuple(losses)
def evaluate(
    dictionary,  # a dictionary
    activations,  # a generator of activations; if an ActivationBuffer, also compute loss recovered
    max_len=128,  # max context length for loss recovered
    batch_size=128,  # batch size for loss recovered
    io="out",  # can be 'in', 'out', or 'in_and_out'
    normalize_batch=False,  # normalize batch before passing through dictionary
    tracer_args={
        "use_cache": False,
        "output_attentions": False,
    },  # minimize cache during model trace.
    device="cpu",
):
    with t.no_grad():
        out = {}  # dict of results
        try:
            x = next(activations).to(device)
            if normalize_batch:
                x = x / x.norm(dim=-1).mean() * (dictionary.activation_dim**0.5)
        except StopIteration:
            raise StopIteration(
                "Not enough activations in buffer. Pass a buffer with a smaller batch size or more data."
            )
        x_hat, f = dictionary(x, output_features=True)
        l2_loss = t.linalg.norm(x - x_hat, dim=-1).mean()
        l1_loss = f.norm(p=1, dim=-1).mean()
        l0 = (f != 0).float().sum(dim=-1).mean()
        frac_alive = (
            t.flatten(f, start_dim=0, end_dim=1).any(dim=0).sum() / dictionary.dict_size
        )
        # cosine similarity between x and x_hat
        x_normed = x / t.linalg.norm(x, dim=-1, keepdim=True)
        x_hat_normed = x_hat / t.linalg.norm(x_hat, dim=-1, keepdim=True)
        cossim = (x_normed * x_hat_normed).sum(dim=-1).mean()
        # l2 ratio
        l2_ratio = (t.linalg.norm(x_hat, dim=-1) / t.linalg.norm(x, dim=-1)).mean()
        # compute variance explained
        total_variance = t.var(x, dim=0).sum()
        residual_variance = t.var(x - x_hat, dim=0).sum()
        frac_variance_explained = 1 - residual_variance / total_variance
        # Equation 10 from https://arxiv.org/abs/2404.16014
        x_hat_norm_squared = t.linalg.norm(x_hat, dim=-1, ord=2) ** 2
        x_dot_x_hat = (x * x_hat).sum(dim=-1)
        relative_reconstruction_bias = x_hat_norm_squared.mean() / x_dot_x_hat.mean()
        out["l2_loss"] = l2_loss.item()
        out["l1_loss"] = l1_loss.item()
        out["l0"] = l0.item()
        out["frac_alive"] = frac_alive.item()
        out["frac_variance_explained"] = frac_variance_explained.item()
        out["cossim"] = cossim.item()
        out["l2_ratio"] = l2_ratio.item()
        out["relative_reconstruction_bias"] = relative_reconstruction_bias.item()
        if not isinstance(activations, (ActivationBuffer, NNsightActivationBuffer)):
            return out
        # compute loss recovered
        loss_original, loss_reconstructed, loss_zero = loss_recovered(
            activations.text_batch(batch_size=batch_size),
            activations.model,
            activations.submodule,
            dictionary,
            max_len=max_len,
            normalize_batch=normalize_batch,
            io=io,
            tracer_args=tracer_args,
        )
        frac_recovered = (loss_reconstructed - loss_zero) / (loss_original - loss_zero)
        out["loss_original"] = loss_original.item()
        out["loss_reconstructed"] = loss_reconstructed.item()
        out["loss_zero"] = loss_zero.item()
        out["frac_recovered"] = frac_recovered.item()
        return out

================
File: neuronpedia/butanium_dictionary_learning/dictionary_learning/grad_pursuit.py
================
"""
Implements batched gradient pursuit algorithm here:
https://www.lesswrong.com/posts/C5KAZQib3bzzpeyrg/full-post-progress-update-1-from-the-gdm-mech-interp-team#Inference_Time_Optimisation:~:text=two%20seem%20promising.-,Details%20of%20Sparse%20Approximation%20Algorithms%20(for%20accelerators),-This%20section%20gets
"""
import torch as t
def _grad_pursuit_update_step(
    signal, weights, dictionary, batch_arange, selected_features
):
    """
    signal: b x d, weights: b x n, dictionary: d x n, batch_arange: b, selected_features: b x n
    """
    residual = signal - t.einsum("bn,dn -> bd", weights, dictionary)
    # choose the element with largest inner product with residual, as in matched pursuit.
    inner_products = t.einsum("dn,bd -> bn", dictionary, residual)
    idxs = t.argmax(inner_products, dim=1)
    # add the new feature to the active set.
    selected_features[batch_arange, idxs] = 1
    # the gradient for the weights is the inner product, restricted to the chosen features
    grad = selected_features * inner_products
    # the next two steps compute the optimal step size
    c = t.einsum("bn,dn -> bd", grad, dictionary)
    step_size = t.einsum("bd,bd -> b", c, residual) / t.einsum("bd,bd -> b ", c, c)
    weights = weights + t.einsum("b,bn -> bn", step_size, grad)
    weights = t.clip(weights, min=0)  # clip the weights to be positive
    return weights, selected_features
def grad_pursuit(signal, dictionary, target_l0: int = 20, device: str = "cpu"):
    """
    Inputs: signal: b x d, dictionary: d x n, target_l0: int, device: str
    Outputs: weights: b x n
    """
    assert len(signal.shape) == 2  # makes sure this a batch of signals
    with t.no_grad():
        batch_arange = t.arange(signal.shape[0]).to(device)
        weights = t.zeros((signal.shape[0], dictionary.shape[1])).to(device)
        selected_features = t.zeros((signal.shape[0], dictionary.shape[1])).to(device)
        for _ in range(target_l0):
            weights, selected_features = _grad_pursuit_update_step(
                signal, weights, dictionary, batch_arange, selected_features
            )
    return weights

================
File: neuronpedia/butanium_dictionary_learning/dictionary_learning/interp.py
================
import random
from circuitsvis.activations import text_neuron_activations
from einops import rearrange
import torch as t
from collections import namedtuple
import umap
import pandas as pd
import plotly.express as px
def feature_effect(
    model,
    submodule,
    dictionary,
    feature,
    inputs,
    max_length=128,
    add_residual=True,  # whether to compensate for dictionary reconstruction error by adding residual
    k=10,
    largest=True,
):
    """
    Effect of ablating the feature on top k predictions for next token.
    """
    tracer_kwargs = {
        "scan": False,
        "validate": False,
        "invoker_args": dict(max_length=max_length),
    }
    # clean run
    with t.no_grad(), model.trace(inputs, **tracer_kwargs):
        if dictionary is None:
            pass
        elif not add_residual:  # run hidden state through autoencoder
            if type(submodule.output.shape) == tuple:
                submodule.output[0][:] = dictionary(submodule.output[0])
            else:
                submodule.output = dictionary(submodule.output)
        clean_output = model.output.save()
    try:
        clean_logits = clean_output.value.logits[:, -1, :]
    except:
        clean_logits = clean_output.value[:, -1, :]
    clean_logprobs = t.nn.functional.log_softmax(clean_logits, dim=-1)
    # ablated run
    with t.no_grad(), model.trace(inputs, **tracer_kwargs):
        if dictionary is None:
            if type(submodule.output.shape) == tuple:
                submodule.output[0][:, -1, feature] = 0
            else:
                submodule.output[:, -1, feature] = 0
        else:
            x = submodule.output
            if type(x.shape) == tuple:
                x = x[0]
            x_hat, f = dictionary(x, output_features=True)
            residual = x - x_hat
            f[:, -1, feature] = 0
            if add_residual:
                x_hat = dictionary.decode(f) + residual
            else:
                x_hat = dictionary.decode(f)
            if type(submodule.output.shape) == tuple:
                submodule.output[0][:] = x_hat
            else:
                submodule.output = x_hat
        ablated_output = model.output.save()
    try:
        ablated_logits = ablated_output.value.logits[:, -1, :]
    except:
        ablated_logits = ablated_output.value[:, -1, :]
    ablated_logprobs = t.nn.functional.log_softmax(ablated_logits, dim=-1)
    diff = clean_logprobs - ablated_logprobs
    top_probs, top_tokens = t.topk(diff.mean(dim=0), k=k, largest=largest)
    return top_tokens, top_probs
def examine_dimension(
    model,
    submodule,
    buffer,
    dictionary=None,
    max_length=128,
    n_inputs=512,
    dim_idx=None,
    k=30,
):
    tracer_kwargs = {
        "scan": False,
        "validate": False,
        "invoker_args": dict(max_length=max_length),
    }
    def _list_decode(x):
        if isinstance(x, int):
            return model.tokenizer.decode(x)
        else:
            return [_list_decode(y) for y in x]
    if dim_idx is None:
        dim_idx = random.randint(0, activations.shape[-1] - 1)
    inputs = buffer.tokenized_batch(batch_size=n_inputs)
    with t.no_grad(), model.trace(inputs, **tracer_kwargs):
        tokens = model.input[1][
            "input_ids"
        ].save()  # if you're getting errors, check here; might only work for pythia models
        activations = submodule.output
        if type(activations.shape) == tuple:
            activations = activations[0]
        if dictionary is not None:
            activations = dictionary.encode(activations)
        activations = activations[:, :, dim_idx].save()
    activations = activations.value
    # get top k tokens by mean activation
    tokens = tokens.value
    token_mean_acts = {}
    for ctx in tokens:
        for tok in ctx:
            if tok.item() in token_mean_acts:
                continue
            idxs = (tokens == tok).nonzero(as_tuple=True)
            token_mean_acts[tok.item()] = activations[idxs].mean().item()
    top_tokens = sorted(token_mean_acts.items(), key=lambda x: x[1], reverse=True)[:k]
    top_tokens = [(model.tokenizer.decode(tok), act) for tok, act in top_tokens]
    flattened_acts = rearrange(activations, "b n -> (b n)")
    topk_indices = t.argsort(flattened_acts, dim=0, descending=True)[:k]
    batch_indices = topk_indices // activations.shape[1]
    token_indices = topk_indices % activations.shape[1]
    tokens = [
        tokens[batch_idx, : token_idx + 1].tolist()
        for batch_idx, token_idx in zip(batch_indices, token_indices)
    ]
    activations = [
        activations[batch_idx, : token_id + 1, None, None]
        for batch_idx, token_id in zip(batch_indices, token_indices)
    ]
    decoded_tokens = _list_decode(tokens)
    top_contexts = text_neuron_activations(decoded_tokens, activations)
    top_affected = feature_effect(
        model, submodule, dictionary, dim_idx, tokens, max_length=max_length, k=k
    )
    top_affected = [
        (model.tokenizer.decode(tok), prob.item()) for tok, prob in zip(*top_affected)
    ]
    return namedtuple("featureProfile", ["top_contexts", "top_tokens", "top_affected"])(
        top_contexts, top_tokens, top_affected
    )
def feature_umap(
    dictionary,
    weight="decoder",  # 'encoder' or 'decoder'
    # UMAP parameters
    n_neighbors=15,
    metric="cosine",
    min_dist=0.05,
    n_components=2,  # dimension of the UMAP embedding
    feat_idxs=None,  # if not none, indicate the feature with a red dot
):
    """
    Fit a UMAP embedding of the dictionary features and return a plotly plot of the result.
    """
    if weight == "encoder":
        df = pd.DataFrame(dictionary.encoder.weight.cpu().detach().numpy())
    else:
        df = pd.DataFrame(dictionary.decoder.weight.T.cpu().detach().numpy())
    reducer = umap.UMAP(
        n_neighbors=n_neighbors,
        metric=metric,
        min_dist=min_dist,
        n_components=n_components,
    )
    embedding = reducer.fit_transform(df)
    if feat_idxs is None:
        colors = None
    if isinstance(feat_idxs, int):
        feat_idxs = [feat_idxs]
    else:
        colors = [
            "blue" if i not in feat_idxs else "red" for i in range(embedding.shape[0])
        ]
    if n_components == 2:
        return px.scatter(
            x=embedding[:, 0], y=embedding[:, 1], hover_name=df.index, color=colors
        )
    if n_components == 3:
        return px.scatter_3d(
            x=embedding[:, 0],
            y=embedding[:, 1],
            z=embedding[:, 2],
            hover_name=df.index,
            color=colors,
        )
    raise ValueError("n_components must be 2 or 3")

================
File: neuronpedia/butanium_dictionary_learning/dictionary_learning/scripts/train_crosscoder.py
================
"""
Train a Crosscoder using pre-computed activations.
Activations are assumed to be stored in the directory specified by `--activation-store-dir`, organized by model and dataset:
    activations/<base-model>/<dataset>/<submodule-name>/
"""
import torch as th
import argparse
from pathlib import Path
from dictionary_learning.cache import PairedActivationCache
from dictionary_learning import CrossCoder
from dictionary_learning.trainers import CrossCoderTrainer
from dictionary_learning.training import trainSAE
import os
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--activation-store-dir", type=str, default="activations")
    parser.add_argument("--base-model", type=str, default="gemma-2-2b")
    parser.add_argument("--instruct-model", type=str, default="gemma-2-2b-it")
    parser.add_argument("--layer", type=int, default=13)
    parser.add_argument("--wandb-entity", type=str, default="")
    parser.add_argument("--disable-wandb", action="store_true")
    parser.add_argument("--expansion-factor", type=int, default=32)
    parser.add_argument("--batch-size", type=int, default=2048)
    parser.add_argument("--workers", type=int, default=32)
    parser.add_argument("--mu", type=float, default=1e-1)
    parser.add_argument("--seed", type=int, default=42)
    parser.add_argument("--max-steps", type=int, default=None)
    parser.add_argument("--validate-every-n-steps", type=int, default=10000)
    parser.add_argument("--same-init-for-all-layers", action="store_true")
    parser.add_argument("--norm-init-scale", type=float, default=0.005)
    parser.add_argument("--init-with-transpose", action="store_true")
    parser.add_argument("--run-name", type=str, default=None)
    parser.add_argument("--resample-steps", type=int, default=None)
    parser.add_argument("--lr", type=float, default=1e-3)
    parser.add_argument("--pretrained", type=str, default=None)
    parser.add_argument("--encoder-layers", type=int, default=None, nargs="+")
    parser.add_argument(
        "--dataset", type=str, nargs="+", default=["fineweb", "lmsys_chat"]
    )
    args = parser.parse_args()
    print(f"Training args: {args}")
    th.manual_seed(args.seed)
    th.cuda.manual_seed_all(args.seed)
    activation_store_dir = Path(args.activation_store_dir)
    base_model_dir = activation_store_dir / args.base_model
    instruct_model_dir = activation_store_dir / args.instruct_model
    caches = []
    submodule_name = f"layer_{args.layer}_out"
    for dataset in args.dataset:
        base_model_dataset = base_model_dir / dataset
        instruct_model_dataset = instruct_model_dir / dataset
        caches.append(
            PairedActivationCache(
                base_model_dataset / submodule_name,
                instruct_model_dataset / submodule_name,
            )
        )
    dataset = th.utils.data.ConcatDataset(caches)
    activation_dim = dataset[0].shape[1]
    dictionary_size = args.expansion_factor * activation_dim
    device = "cuda" if th.cuda.is_available() else "cpu"
    print(f"Training on device={device}.")
    trainer_cfg = {
        "trainer": CrossCoderTrainer,
        "dict_class": CrossCoder,
        "activation_dim": activation_dim,
        "dict_size": dictionary_size,
        "lr": args.lr,
        "resample_steps": args.resample_steps,
        "device": device,
        "warmup_steps": 1000,
        "layer": args.layer,
        "lm_name": f"{args.instruct_model}-{args.base_model}",
        "compile": True,
        "wandb_name": f"L{args.layer}-mu{args.mu:.1e}-lr{args.lr:.0e}"
        + (f"-{args.run_name}" if args.run_name is not None else ""),
        "l1_penalty": args.mu,
        "dict_class_kwargs": {
            "same_init_for_all_layers": args.same_init_for_all_layers,
            "norm_init_scale": args.norm_init_scale,
            "init_with_transpose": args.init_with_transpose,
            "encoder_layers": args.encoder_layers,
        },
        "pretrained_ae": (
            CrossCoder.from_pretrained(args.pretrained)
            if args.pretrained is not None
            else None
        ),
    }
    validation_size = 10**6
    train_dataset, validation_dataset = th.utils.data.random_split(
        dataset, [len(dataset) - validation_size, validation_size]
    )
    print(f"Training on {len(train_dataset)} token activations.")
    dataloader = th.utils.data.DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=args.workers,
        pin_memory=True,
    )
    validation_dataloader = th.utils.data.DataLoader(
        validation_dataset,
        batch_size=8192,
        shuffle=False,
        num_workers=args.workers,
        pin_memory=True,
    )
    # train the sparse autoencoder (SAE)
    ae = trainSAE(
        data=dataloader,
        trainer_config=trainer_cfg,
        validate_every_n_steps=args.validate_every_n_steps,
        validation_data=validation_dataloader,
        use_wandb=not args.disable_wandb,
        wandb_entity=args.wandb_entity,
        wandb_project="crosscoder",
        log_steps=50,
        save_dir="checkpoints",
        steps=args.max_steps,
        save_steps=args.validate_every_n_steps,
    )

================
File: neuronpedia/butanium_dictionary_learning/dictionary_learning/trainers/__init__.py
================
from .standard import StandardTrainer
from .gdm import GatedSAETrainer
from .p_anneal import PAnnealTrainer
from .gated_anneal import GatedAnnealTrainer
from .top_k import TrainerTopK
from .jumprelu import TrainerJumpRelu
from .batch_top_k import TrainerBatchTopK, BatchTopKSAE
from .crosscoder import CrossCoderTrainer

================
File: neuronpedia/butanium_dictionary_learning/dictionary_learning/trainers/batch_top_k.py
================
import torch as t
import torch.nn as nn
import einops
from collections import namedtuple
from ..config import DEBUG
from ..dictionary import Dictionary
from ..trainers.trainer import SAETrainer
class BatchTopKSAE(Dictionary, nn.Module):
    def __init__(self, activation_dim: int, dict_size: int, k: int):
        super().__init__()
        self.activation_dim = activation_dim
        self.dict_size = dict_size
        self.k = k
        self.encoder = nn.Linear(activation_dim, dict_size)
        self.encoder.bias.data.zero_()
        self.decoder = nn.Linear(dict_size, activation_dim, bias=False)
        self.decoder.weight.data = self.encoder.weight.data.clone().T
        self.set_decoder_norm_to_unit_norm()
        self.b_dec = nn.Parameter(t.zeros(activation_dim))
    def encode(self, x: t.Tensor, return_active: bool = False):
        post_relu_feat_acts_BF = nn.functional.relu(self.encoder(x - self.b_dec))
        # Flatten and perform batch top-k
        flattened_acts = post_relu_feat_acts_BF.flatten()
        post_topk = flattened_acts.topk(self.k * x.size(0), sorted=False, dim=-1)
        buffer_BF = t.zeros_like(post_relu_feat_acts_BF)
        encoded_acts_BF = (
            buffer_BF.flatten()
            .scatter(-1, post_topk.indices, post_topk.values)
            .reshape(buffer_BF.shape)
        )
        if return_active:
            return encoded_acts_BF, encoded_acts_BF.sum(0) > 0
        else:
            return encoded_acts_BF
    def decode(self, x: t.Tensor) -> t.Tensor:
        return self.decoder(x) + self.b_dec
    def forward(self, x: t.Tensor, output_features: bool = False):
        encoded_acts_BF = self.encode(x)
        x_hat_BD = self.decode(encoded_acts_BF)
        if not output_features:
            return x_hat_BD
        else:
            return x_hat_BD, encoded_acts_BF
    @t.no_grad()
    def set_decoder_norm_to_unit_norm(self):
        eps = t.finfo(self.decoder.weight.dtype).eps
        norm = t.norm(self.decoder.weight.data, dim=0, keepdim=True)
        self.decoder.weight.data /= norm + eps
    @t.no_grad()
    def remove_gradient_parallel_to_decoder_directions(self):
        assert self.decoder.weight.grad is not None
        parallel_component = einops.einsum(
            self.decoder.weight.grad,
            self.decoder.weight.data,
            "d_in d_sae, d_in d_sae -> d_sae",
        )
        self.decoder.weight.grad -= einops.einsum(
            parallel_component,
            self.decoder.weight.data,
            "d_sae, d_in d_sae -> d_in d_sae",
        )
class TrainerBatchTopK(SAETrainer):
    def __init__(
        self,
        dict_class=BatchTopKSAE,
        activation_dim=512,
        dict_size=64 * 512,
        k=8,
        auxk_alpha=1 / 32,
        decay_start=24000,
        steps=30000,
        top_k_aux=512,
        seed=None,
        device=None,
        layer=None,
        lm_name=None,
        wandb_name="BatchTopKSAE",
        submodule_name=None,
    ):
        super().__init__(seed)
        assert layer is not None and lm_name is not None
        self.layer = layer
        self.lm_name = lm_name
        self.submodule_name = submodule_name
        self.wandb_name = wandb_name
        self.steps = steps
        self.k = k
        if seed is not None:
            t.manual_seed(seed)
            t.cuda.manual_seed_all(seed)
        self.ae = dict_class(activation_dim, dict_size, k)
        if device is None:
            self.device = "cuda" if t.cuda.is_available() else "cpu"
        else:
            self.device = device
        self.ae.to(self.device)
        scale = dict_size / (2**14)
        self.lr = 2e-4 / scale**0.5
        self.auxk_alpha = auxk_alpha
        self.dead_feature_threshold = 10_000_000
        self.top_k_aux = top_k_aux
        self.optimizer = t.optim.Adam(
            self.ae.parameters(), lr=self.lr, betas=(0.9, 0.999)
        )
        def lr_fn(step):
            if step < decay_start:
                return 1.0
            else:
                return (steps - step) / (steps - decay_start)
        self.scheduler = t.optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda=lr_fn)
        self.num_tokens_since_fired = t.zeros(dict_size, dtype=t.long, device=device)
        self.logging_parameters = ["effective_l0", "dead_features"]
        self.effective_l0 = -1
        self.dead_features = -1
    def get_auxiliary_loss(self, x, x_reconstruct, acts):
        dead_features = self.num_tokens_since_fired >= self.dead_feature_threshold
        if dead_features.sum() > 0:
            residual = x.float() - x_reconstruct.float()
            acts_topk_aux = t.topk(
                acts[:, dead_features],
                min(self.top_k_aux, dead_features.sum()),
                dim=-1,
            )
            acts_aux = t.zeros_like(acts[:, dead_features]).scatter(
                -1, acts_topk_aux.indices, acts_topk_aux.values
            )
            x_reconstruct_aux = acts_aux @ self.W_dec[dead_features]
            l2_loss_aux = (
                self.auxk_alpha
                * (x_reconstruct_aux.float() - residual.float()).pow(2).mean()
            )
            return l2_loss_aux
        else:
            return t.tensor(0, dtype=x.dtype, device=x.device)
    def loss(self, x, step=None, logging=False):
        f, active_indices = self.ae.encode(x, return_active=True)
        l0 = (f != 0).float().sum(dim=-1).mean().item()
        x_hat = self.ae.decode(f)
        e = x_hat - x
        self.effective_l0 = self.k
        num_tokens_in_step = x.size(0)
        did_fire = t.zeros_like(self.num_tokens_since_fired, dtype=t.bool)
        did_fire[active_indices] = True
        self.num_tokens_since_fired += num_tokens_in_step
        self.num_tokens_since_fired[did_fire] = 0
        auxk_loss = self.get_auxiliary_loss(x, x_hat, f)
        l2_loss = e.pow(2).sum(dim=-1).mean()
        auxk_loss = auxk_loss.sum(dim=-1).mean()
        loss = l2_loss + self.auxk_alpha * auxk_loss
        if not logging:
            return loss
        else:
            return namedtuple("LossLog", ["x", "x_hat", "f", "losses"])(
                x,
                x_hat,
                f,
                {
                    "l2_loss": l2_loss.item(),
                    "auxk_loss": auxk_loss.item(),
                    "loss": loss.item(),
                },
            )
    def update(self, step, x):
        if step == 0:
            median = self.geometric_median(x)
            self.ae.b_dec.data = median
        self.ae.set_decoder_norm_to_unit_norm()
        x = x.to(self.device)
        loss = self.loss(x, step=step)
        loss.backward()
        t.nn.utils.clip_grad_norm_(self.ae.parameters(), 1.0)
        self.ae.remove_gradient_parallel_to_decoder_directions()
        self.optimizer.step()
        self.optimizer.zero_grad()
        self.scheduler.step()
        return loss.item()
    @property
    def config(self):
        return {
            "trainer_class": "TrainerBatchTopK",
            "dict_class": "BatchTopKSAE",
            "lr": self.lr,
            "steps": self.steps,
            "seed": self.seed,
            "activation_dim": self.ae.activation_dim,
            "dict_size": self.ae.dict_size,
            "k": self.ae.k,
            "device": self.device,
            "layer": self.layer,
            "lm_name": self.lm_name,
            "wandb_name": self.wandb_name,
            "submodule_name": self.submodule_name,
        }
    @staticmethod
    def geometric_median(points: t.Tensor, max_iter: int = 100, tol: float = 1e-5):
        guess = points.mean(dim=0)
        prev = t.zeros_like(guess)
        weights = t.ones(len(points), device=points.device)
        for _ in range(max_iter):
            prev = guess
            weights = 1 / t.norm(points - guess, dim=1)
            weights /= weights.sum()
            guess = (weights.unsqueeze(1) * points).sum(dim=0)
            if t.norm(guess - prev) < tol:
                break
        return guess

================
File: neuronpedia/butanium_dictionary_learning/dictionary_learning/trainers/crosscoder.py
================
"""
Implements the standard SAE training scheme.
"""
import torch as th
from ..trainers.trainer import SAETrainer
from ..config import DEBUG
from ..dictionary import CrossCoder
from collections import namedtuple
class CrossCoderTrainer(SAETrainer):
    """
    Standard SAE training scheme for cross-coding.
    """
    def __init__(
        self,
        dict_class=CrossCoder,
        num_layers=2,
        activation_dim=512,
        dict_size=64 * 512,
        lr=1e-3,
        l1_penalty=1e-1,
        warmup_steps=1000,  # lr warmup period at start of training and after each resample
        resample_steps=None,  # how often to resample neurons
        seed=None,
        device=None,
        layer=None,
        lm_name=None,
        wandb_name="CrossCoderTrainer",
        submodule_name=None,
        compile=False,
        dict_class_kwargs={},
        pretrained_ae=None,
    ):
        super().__init__(seed)
        assert layer is not None and lm_name is not None
        self.layer = layer
        self.lm_name = lm_name
        self.submodule_name = submodule_name
        self.compile = compile
        if seed is not None:
            th.manual_seed(seed)
            th.cuda.manual_seed_all(seed)
        # initialize dictionary
        if pretrained_ae is None:
            self.ae = dict_class(
                activation_dim, dict_size, num_layers=num_layers, **dict_class_kwargs
            )
        else:
            self.ae = pretrained_ae
        if compile:
            self.ae = th.compile(self.ae)
        self.lr = lr
        self.l1_penalty = l1_penalty
        self.warmup_steps = warmup_steps
        self.wandb_name = wandb_name
        if device is None:
            self.device = "cuda" if th.cuda.is_available() else "cpu"
        else:
            self.device = device
        self.ae.to(self.device)
        self.resample_steps = resample_steps
        if self.resample_steps is not None:
            # how many steps since each neuron was last activated?
            self.steps_since_active = th.zeros(self.ae.dict_size, dtype=int).to(
                self.device
            )
        else:
            self.steps_since_active = None
        self.optimizer = th.optim.Adam(self.ae.parameters(), lr=lr)
        if resample_steps is None:
            def warmup_fn(step):
                return min(step / warmup_steps, 1.0)
        else:
            def warmup_fn(step):
                return min((step % resample_steps) / warmup_steps, 1.0)
        self.scheduler = th.optim.lr_scheduler.LambdaLR(
            self.optimizer, lr_lambda=warmup_fn
        )
    def resample_neurons(self, deads, activations):
        with th.no_grad():
            if deads.sum() == 0:
                return
            self.ae.resample_neurons(deads, activations)
            # reset Adam parameters for dead neurons
            state_dict = self.optimizer.state_dict()["state"]
            ## encoder weight
            state_dict[0]["exp_avg"][:, :, deads] = 0.0
            state_dict[0]["exp_avg_sq"][:, :, deads] = 0.0
            ## encoder bias
            state_dict[1]["exp_avg"][deads] = 0.0
            state_dict[1]["exp_avg_sq"][deads] = 0.0
            ## decoder weight
            state_dict[3]["exp_avg"][:, deads, :] = 0.0
            state_dict[3]["exp_avg_sq"][:, deads, :] = 0.0
    def loss(self, x, logging=False, return_deads=False, **kwargs):
        x_hat, f = self.ae(x, output_features=True)
        l2_loss = th.linalg.norm(x - x_hat, dim=-1).mean()
        l1_loss = f.norm(p=1, dim=-1).mean()
        deads = (f <= 1e-8).all(dim=0)
        if self.steps_since_active is not None:
            # update steps_since_active
            self.steps_since_active[deads] += 1
            self.steps_since_active[~deads] = 0
        loss = l2_loss + self.l1_penalty * l1_loss
        if not logging:
            return loss
        else:
            return namedtuple("LossLog", ["x", "x_hat", "f", "losses"])(
                x,
                x_hat,
                f,
                {
                    "l2_loss": l2_loss.item(),
                    "mse_loss": (x - x_hat).pow(2).sum(dim=-1).mean().item(),
                    "sparsity_loss": l1_loss.item(),
                    "loss": loss.item(),
                    "deads": deads if return_deads else None,
                },
            )
    def update(self, step, activations):
        activations = activations.to(self.device)
        self.optimizer.zero_grad()
        loss = self.loss(activations)
        loss.backward()
        self.optimizer.step()
        self.scheduler.step()
        if self.resample_steps is not None and step % self.resample_steps == 0:
            self.resample_neurons(
                self.steps_since_active > self.resample_steps / 2, activations
            )
    @property
    def config(self):
        return {
            "dict_class": (
                self.ae.__class__.__name__
                if not self.compile
                else self.ae._orig_mod.__class__.__name__
            ),
            "trainer_class": self.__class__.__name__,
            "activation_dim": self.ae.activation_dim,
            "dict_size": self.ae.dict_size,
            "lr": self.lr,
            "l1_penalty": self.l1_penalty,
            "warmup_steps": self.warmup_steps,
            "resample_steps": self.resample_steps,
            "device": self.device,
            "layer": self.layer,
            "lm_name": self.lm_name,
            "wandb_name": self.wandb_name,
            "submodule_name": self.submodule_name,
        }

================
File: neuronpedia/butanium_dictionary_learning/dictionary_learning/trainers/gated_anneal.py
================
"""
Implements the training scheme for a gated SAE described in https://arxiv.org/abs/2404.16014
"""
import torch as t
from ..trainers.trainer import SAETrainer
from ..config import DEBUG
from ..dictionary import GatedAutoEncoder
from collections import namedtuple
class ConstrainedAdam(t.optim.Adam):
    """
    A variant of Adam where some of the parameters are constrained to have unit norm.
    """
    def __init__(self, params, constrained_params, lr):
        super().__init__(params, lr=lr, betas=(0, 0.999))
        self.constrained_params = list(constrained_params)
    def step(self, closure=None):
        with t.no_grad():
            for p in self.constrained_params:
                normed_p = p / p.norm(dim=0, keepdim=True)
                # project away the parallel component of the gradient
                p.grad -= (p.grad * normed_p).sum(dim=0, keepdim=True) * normed_p
        super().step(closure=closure)
        with t.no_grad():
            for p in self.constrained_params:
                # renormalize the constrained parameters
                p /= p.norm(dim=0, keepdim=True)
class GatedAnnealTrainer(SAETrainer):
    """
    Gated SAE training scheme with p-annealing.
    """
    def __init__(
        self,
        dict_class=GatedAutoEncoder,
        activation_dim=512,
        dict_size=64 * 512,
        lr=3e-4,
        warmup_steps=1000,  # lr warmup period at start of training and after each resample
        sparsity_function="Lp^p",  # Lp or Lp^p
        initial_sparsity_penalty=1e-1,  # equal to l1 penalty in standard trainer
        anneal_start=15000,  # step at which to start annealing p
        anneal_end=None,  # step at which to stop annealing, defaults to steps-1
        p_start=1,  # starting value of p (constant throughout warmup)
        p_end=0,  # annealing p_start to p_end linearly after warmup_steps, exact endpoint excluded
        n_sparsity_updates=10,  # number of times to update the sparsity penalty, at most steps-anneal_start times
        sparsity_queue_length=10,  # number of recent sparsity loss terms, onle needed for adaptive_sparsity_penalty
        resample_steps=None,  # number of steps after which to resample dead neurons
        steps=None,  # total number of steps to train for
        device=None,
        seed=42,
        layer=None,
        lm_name=None,
        wandb_name="GatedAnnealTrainer",
    ):
        super().__init__(seed)
        assert layer is not None and lm_name is not None
        self.layer = layer
        self.lm_name = lm_name
        if seed is not None:
            t.manual_seed(seed)
            t.cuda.manual_seed_all(seed)
        # initialize dictionary
        # initialize dictionary
        self.activation_dim = activation_dim
        self.dict_size = dict_size
        self.ae = dict_class(activation_dim, dict_size)
        if device is None:
            self.device = "cuda" if t.cuda.is_available() else "cpu"
        else:
            self.device = device
        self.ae.to(self.device)
        self.lr = lr
        self.sparsity_function = sparsity_function
        self.anneal_start = anneal_start
        self.anneal_end = anneal_end if anneal_end is not None else steps
        self.p_start = p_start
        self.p_end = p_end
        self.p = p_start  # p is set in self.loss()
        self.next_p = None  # set in self.loss()
        self.lp_loss = None  # set in self.loss()
        self.scaled_lp_loss = None  # set in self.loss()
        if n_sparsity_updates == "continuous":
            self.n_sparsity_updates = self.anneal_end - anneal_start + 1
        else:
            self.n_sparsity_updates = n_sparsity_updates
        self.sparsity_update_steps = t.linspace(
            anneal_start, self.anneal_end, self.n_sparsity_updates, dtype=int
        )
        self.p_values = t.linspace(p_start, p_end, self.n_sparsity_updates)
        self.p_step_count = 0
        self.sparsity_coeff = initial_sparsity_penalty  # alpha
        self.sparsity_queue_length = sparsity_queue_length
        self.sparsity_queue = []
        self.warmup_steps = warmup_steps
        self.steps = steps
        self.logging_parameters = [
            "p",
            "next_p",
            "lp_loss",
            "scaled_lp_loss",
            "sparsity_coeff",
        ]
        self.seed = seed
        self.wandb_name = wandb_name
        self.resample_steps = resample_steps
        if self.resample_steps is not None:
            # how many steps since each neuron was last activated?
            self.steps_since_active = t.zeros(self.dict_size, dtype=int).to(self.device)
        else:
            self.steps_since_active = None
        self.optimizer = ConstrainedAdam(
            self.ae.parameters(), self.ae.decoder.parameters(), lr=lr
        )
        if resample_steps is None:
            def warmup_fn(step):
                return min(step / warmup_steps, 1.0)
        else:
            def warmup_fn(step):
                return min((step % resample_steps) / warmup_steps, 1.0)
        self.scheduler = t.optim.lr_scheduler.LambdaLR(
            self.optimizer, lr_lambda=warmup_fn
        )
    def resample_neurons(self, deads, activations):
        with t.no_grad():
            if deads.sum() == 0:
                return
            print(f"resampling {deads.sum().item()} neurons")
            # compute loss for each activation
            losses = (activations - self.ae(activations)).norm(dim=-1)
            # sample input to create encoder/decoder weights from
            n_resample = min([deads.sum(), losses.shape[0]])
            indices = t.multinomial(losses, num_samples=n_resample, replacement=False)
            sampled_vecs = activations[indices]
            # reset encoder/decoder weights for dead neurons
            alive_norm = self.ae.encoder.weight[~deads].norm(dim=-1).mean()
            self.ae.encoder.weight[deads][:n_resample] = sampled_vecs * alive_norm * 0.2
            self.ae.decoder.weight[:, deads][:, :n_resample] = (
                sampled_vecs / sampled_vecs.norm(dim=-1, keepdim=True)
            ).T
            self.ae.encoder.bias[deads][:n_resample] = 0.0
            # reset Adam parameters for dead neurons
            state_dict = self.optimizer.state_dict()["state"]
            ## encoder weight
            state_dict[1]["exp_avg"][deads] = 0.0
            state_dict[1]["exp_avg_sq"][deads] = 0.0
            ## encoder bias
            state_dict[2]["exp_avg"][deads] = 0.0
            state_dict[2]["exp_avg_sq"][deads] = 0.0
            ## decoder weight
            state_dict[3]["exp_avg"][:, deads] = 0.0
            state_dict[3]["exp_avg_sq"][:, deads] = 0.0
    def lp_norm(self, f, p):
        norm_sq = f.pow(p).sum(dim=-1)
        if self.sparsity_function == "Lp^p":
            return norm_sq.mean()
        elif self.sparsity_function == "Lp":
            return norm_sq.pow(1 / p).mean()
        else:
            raise ValueError("Sparsity function must be 'Lp' or 'Lp^p'")
    def loss(self, x, step, logging=False, **kwargs):
        f, f_gate = self.ae.encode(x, return_gate=True)
        x_hat = self.ae.decode(f)
        x_hat_gate = (
            f_gate @ self.ae.decoder.weight.detach().T + self.ae.decoder_bias.detach()
        )
        L_recon = (x - x_hat).pow(2).sum(dim=-1).mean()
        L_aux = (x - x_hat_gate).pow(2).sum(dim=-1).mean()
        fs = f_gate  # feature activation that we use for sparsity term
        lp_loss = self.lp_norm(fs, self.p)
        scaled_lp_loss = lp_loss * self.sparsity_coeff
        self.lp_loss = lp_loss
        self.scaled_lp_loss = scaled_lp_loss
        if self.next_p is not None:
            lp_loss_next = self.lp_norm(fs, self.next_p)
            self.sparsity_queue.append([self.lp_loss.item(), lp_loss_next.item()])
            self.sparsity_queue = self.sparsity_queue[-self.sparsity_queue_length :]
        if step in self.sparsity_update_steps:
            # check to make sure we don't update on repeat step:
            if step >= self.sparsity_update_steps[self.p_step_count]:
                # Adapt sparsity penalty alpha
                if self.next_p is not None:
                    local_sparsity_new = t.tensor(
                        [i[0] for i in self.sparsity_queue]
                    ).mean()
                    local_sparsity_old = t.tensor(
                        [i[1] for i in self.sparsity_queue]
                    ).mean()
                    self.sparsity_coeff = (
                        self.sparsity_coeff
                        * (local_sparsity_new / local_sparsity_old).item()
                    )
                # Update p
                self.p = self.p_values[self.p_step_count].item()
                if self.p_step_count < self.n_sparsity_updates - 1:
                    self.next_p = self.p_values[self.p_step_count + 1].item()
                else:
                    self.next_p = self.p_end
                self.p_step_count += 1
        # Update dead feature count
        if self.steps_since_active is not None:
            # update steps_since_active
            deads = (f == 0).all(dim=0)
            self.steps_since_active[deads] += 1
            self.steps_since_active[~deads] = 0
        loss = L_recon + scaled_lp_loss + L_aux
        if not logging:
            return loss
        else:
            return namedtuple("LossLog", ["x", "x_hat", "f", "losses"])(
                x,
                x_hat,
                f,
                {
                    "mse_loss": L_recon.item(),
                    "aux_loss": L_aux.item(),
                    "loss": loss.item(),
                    "p": self.p,
                    "next_p": self.next_p,
                    "lp_loss": lp_loss.item(),
                    "sparsity_loss": scaled_lp_loss.item(),
                    "sparsity_coeff": self.sparsity_coeff,
                },
            )
    def update(self, step, activations):
        activations = activations.to(self.device)
        self.optimizer.zero_grad()
        loss = self.loss(activations, step, logging=False)
        loss.backward()
        self.optimizer.step()
        self.scheduler.step()
        if (
            self.resample_steps is not None
            and step % self.resample_steps == self.resample_steps - 1
        ):
            self.resample_neurons(
                self.steps_since_active > self.resample_steps / 2, activations
            )
    # @property
    # def config(self):
    #     return {
    #         'trainer_class' : 'GatedSAETrainer',
    #         'activation_dim' : self.ae.activation_dim,
    #         'dict_size' : self.ae.dict_size,
    #         'lr' : self.lr,
    #         'l1_penalty' : self.l1_penalty,
    #         'warmup_steps' : self.warmup_steps,
    #         'device' : self.device,
    #         'wandb_name': self.wandb_name,
    #     }
    @property
    def config(self):
        return {
            "trainer_class": "GatedAnnealTrainer",
            "dict_class": "GatedAutoEncoder",
            "activation_dim": self.activation_dim,
            "dict_size": self.dict_size,
            "lr": self.lr,
            "sparsity_function": self.sparsity_function,
            "sparsity_penalty": self.sparsity_coeff,
            "p_start": self.p_start,
            "p_end": self.p_end,
            "anneal_start": self.anneal_start,
            "sparsity_queue_length": self.sparsity_queue_length,
            "n_sparsity_updates": self.n_sparsity_updates,
            "warmup_steps": self.warmup_steps,
            "resample_steps": self.resample_steps,
            "steps": self.steps,
            "seed": self.seed,
            "layer": self.layer,
            "lm_name": self.lm_name,
            "wandb_name": self.wandb_name,
        }

================
File: neuronpedia/butanium_dictionary_learning/dictionary_learning/trainers/gdm.py
================
"""
Implements the training scheme for a gated SAE described in https://arxiv.org/abs/2404.16014
"""
import torch as t
from ..trainers.trainer import SAETrainer
from ..config import DEBUG
from ..dictionary import GatedAutoEncoder
from collections import namedtuple
class ConstrainedAdam(t.optim.Adam):
    """
    A variant of Adam where some of the parameters are constrained to have unit norm.
    """
    def __init__(self, params, constrained_params, lr):
        super().__init__(params, lr=lr, betas=(0, 0.999))
        self.constrained_params = list(constrained_params)
    def step(self, closure=None):
        with t.no_grad():
            for p in self.constrained_params:
                normed_p = p / p.norm(dim=0, keepdim=True)
                # project away the parallel component of the gradient
                p.grad -= (p.grad * normed_p).sum(dim=0, keepdim=True) * normed_p
        super().step(closure=closure)
        with t.no_grad():
            for p in self.constrained_params:
                # renormalize the constrained parameters
                p /= p.norm(dim=0, keepdim=True)
class GatedSAETrainer(SAETrainer):
    """
    Gated SAE training scheme.
    """
    def __init__(
        self,
        dict_class=GatedAutoEncoder,
        activation_dim=512,
        dict_size=64 * 512,
        lr=5e-5,
        l1_penalty=1e-1,
        warmup_steps=1000,  # lr warmup period at start of training and after each resample
        resample_steps=None,  # how often to resample neurons
        seed=None,
        device=None,
        layer=None,
        lm_name=None,
        wandb_name="GatedSAETrainer",
        submodule_name=None,
    ):
        super().__init__(seed)
        assert layer is not None and lm_name is not None
        self.layer = layer
        self.lm_name = lm_name
        self.submodule_name = submodule_name
        if seed is not None:
            t.manual_seed(seed)
            t.cuda.manual_seed_all(seed)
        # initialize dictionary
        self.ae = dict_class(activation_dim, dict_size)
        self.lr = lr
        self.l1_penalty = l1_penalty
        self.warmup_steps = warmup_steps
        self.wandb_name = wandb_name
        if device is None:
            self.device = "cuda" if t.cuda.is_available() else "cpu"
        else:
            self.device = device
        self.ae.to(self.device)
        self.optimizer = ConstrainedAdam(
            self.ae.parameters(), self.ae.decoder.parameters(), lr=lr
        )
        def warmup_fn(step):
            return min(1, step / warmup_steps)
        self.scheduler = t.optim.lr_scheduler.LambdaLR(self.optimizer, warmup_fn)
    def loss(self, x, logging=False, **kwargs):
        f, f_gate = self.ae.encode(x, return_gate=True)
        x_hat = self.ae.decode(f)
        x_hat_gate = (
            f_gate @ self.ae.decoder.weight.detach().T + self.ae.decoder_bias.detach()
        )
        L_recon = (x - x_hat).pow(2).sum(dim=-1).mean()
        L_sparse = t.linalg.norm(f_gate, ord=1, dim=-1).mean()
        L_aux = (x - x_hat_gate).pow(2).sum(dim=-1).mean()
        loss = L_recon + self.l1_penalty * L_sparse + L_aux
        if not logging:
            return loss
        else:
            return namedtuple("LossLog", ["x", "x_hat", "f", "losses"])(
                x,
                x_hat,
                f,
                {
                    "mse_loss": L_recon.item(),
                    "sparsity_loss": L_sparse.item(),
                    "aux_loss": L_aux.item(),
                    "loss": loss.item(),
                },
            )
    def update(self, step, x):
        x = x.to(self.device)
        self.optimizer.zero_grad()
        loss = self.loss(x)
        loss.backward()
        self.optimizer.step()
        self.scheduler.step()
    @property
    def config(self):
        return {
            "dict_class": "GatedAutoEncoder",
            "trainer_class": "GatedSAETrainer",
            "activation_dim": self.ae.activation_dim,
            "dict_size": self.ae.dict_size,
            "lr": self.lr,
            "l1_penalty": self.l1_penalty,
            "warmup_steps": self.warmup_steps,
            "device": self.device,
            "layer": self.layer,
            "lm_name": self.lm_name,
            "wandb_name": self.wandb_name,
            "submodule_name": self.submodule_name,
        }

================
File: neuronpedia/butanium_dictionary_learning/dictionary_learning/trainers/jumprelu.py
================
from collections import namedtuple
import torch
import torch.autograd as autograd
from torch import nn
from ..dictionary import Dictionary, JumpReluAutoEncoder
from .trainer import SAETrainer
class RectangleFunction(autograd.Function):
    @staticmethod
    def forward(ctx, x):
        ctx.save_for_backward(x)
        return ((x > -0.5) & (x < 0.5)).float()
    @staticmethod
    def backward(ctx, grad_output):
        (x,) = ctx.saved_tensors
        grad_input = grad_output.clone()
        grad_input[(x <= -0.5) | (x >= 0.5)] = 0
        return grad_input
class JumpReLUFunction(autograd.Function):
    @staticmethod
    def forward(ctx, x, threshold, bandwidth):
        ctx.save_for_backward(x, threshold, torch.tensor(bandwidth))
        return x * (x > threshold).float()
    @staticmethod
    def backward(ctx, grad_output):
        x, threshold, bandwidth_tensor = ctx.saved_tensors
        bandwidth = bandwidth_tensor.item()
        x_grad = (x > threshold).float() * grad_output
        threshold_grad = (
            -(threshold / bandwidth)
            * RectangleFunction.apply((x - threshold) / bandwidth)
            * grad_output
        )
        return x_grad, threshold_grad, None  # None for bandwidth
class StepFunction(autograd.Function):
    @staticmethod
    def forward(ctx, x, threshold, bandwidth):
        ctx.save_for_backward(x, threshold, torch.tensor(bandwidth))
        return (x > threshold).float()
    @staticmethod
    def backward(ctx, grad_output):
        x, threshold, bandwidth_tensor = ctx.saved_tensors
        bandwidth = bandwidth_tensor.item()
        x_grad = torch.zeros_like(x)
        threshold_grad = (
            -(1.0 / bandwidth)
            * RectangleFunction.apply((x - threshold) / bandwidth)
            * grad_output
        )
        return x_grad, threshold_grad, None  # None for bandwidth
class TrainerJumpRelu(nn.Module, SAETrainer):
    """
    Trains a JumpReLU autoencoder.
    Note does not use learning rate or sparsity scheduling as in the paper.
    """
    def __init__(
        self,
        dict_class=JumpReluAutoEncoder,
        activation_dim=512,
        dict_size=8192,
        steps=30000,
        # XXX: Training decay is not implemented
        seed=None,
        # TODO: What's the default lr use in the paper?
        lr=7e-5,
        bandwidth=0.001,
        sparsity_penalty=0.1,
        device="cpu",
        layer=None,
        lm_name=None,
        wandb_name="JumpRelu",
        submodule_name=None,
    ):
        super().__init__()
        # TODO: Should just be args, and this should be commonised
        assert layer is not None, "Layer must be specified"
        assert lm_name is not None, "Language model name must be specified"
        self.lm_name = lm_name
        self.layer = layer
        self.submodule_name = submodule_name
        self.device = device
        self.steps = steps
        self.lr = lr
        self.seed = seed
        self.bandwidth = bandwidth
        self.sparsity_coefficient = sparsity_penalty
        # TODO: Better auto-naming (e.g. in BatchTopK package)
        self.wandb_name = wandb_name
        # TODO: Why not just pass in the initialised autoencoder instead?
        self.ae = dict_class(
            activation_dim=activation_dim,
            dict_size=dict_size,
            device=device,
        ).to(self.device)
        # Parameters from the paper
        self.optimizer = torch.optim.Adam(
            self.ae.parameters(), lr=lr, betas=(0.0, 0.999), eps=1e-8
        )
        self.logging_parameters = []
    def loss(self, x, logging=False, **_):
        f = self.ae.encode(x)
        recon = self.ae.decode(f)
        recon_loss = (x - recon).pow(2).sum(dim=-1).mean()
        l0 = StepFunction.apply(f, self.ae.threshold, self.bandwidth).sum(dim=-1).mean()
        sparsity_loss = self.sparsity_coefficient * l0
        loss = recon_loss + sparsity_loss
        if not logging:
            return loss
        else:
            return namedtuple("LossLog", ["x", "recon", "f", "losses"])(
                x,
                recon,
                f,
                {
                    "l2_loss": recon_loss.item(),
                    "loss": loss.item(),
                },
            )
    def update(self, step, x):
        x = x.to(self.device)
        loss = self.loss(x, step=step)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.ae.parameters(), 1.0)
        self.optimizer.step()
        self.optimizer.zero_grad()
        return loss.item()
    @property
    def config(self):
        return {
            "trainer_class": "TrainerJumpRelu",
            "dict_class": "JumpReluAutoEncoder",
            "lr": self.lr,
            "steps": self.steps,
            "seed": self.seed,
            "activation_dim": self.ae.activation_dim,
            "dict_size": self.ae.dict_size,
            "device": self.device,
            "layer": self.layer,
            "lm_name": self.lm_name,
            "wandb_name": self.wandb_name,
            "submodule_name": self.submodule_name,
        }

================
File: neuronpedia/butanium_dictionary_learning/dictionary_learning/trainers/p_anneal.py
================
import torch as t
"""
Implements the standard SAE training scheme.
"""
from ..dictionary import AutoEncoder
from ..trainers.trainer import SAETrainer
from ..config import DEBUG
class ConstrainedAdam(t.optim.Adam):
    """
    A variant of Adam where some of the parameters are constrained to have unit norm.
    """
    def __init__(self, params, constrained_params, lr):
        super().__init__(params, lr=lr)
        self.constrained_params = list(constrained_params)
    def step(self, closure=None):
        with t.no_grad():
            for p in self.constrained_params:
                normed_p = p / p.norm(dim=0, keepdim=True)
                # project away the parallel component of the gradient
                p.grad -= (p.grad * normed_p).sum(dim=0, keepdim=True) * normed_p
        super().step(closure=closure)
        with t.no_grad():
            for p in self.constrained_params:
                # renormalize the constrained parameters
                p /= p.norm(dim=0, keepdim=True)
class PAnnealTrainer(SAETrainer):
    """
    SAE training scheme with the option to anneal the sparsity parameter p.
    You can further choose to use Lp or Lp^p sparsity.
    """
    def __init__(
        self,
        dict_class=AutoEncoder,
        activation_dim=512,
        dict_size=64 * 512,
        lr=1e-3,
        warmup_steps=1000,  # lr warmup period at start of training and after each resample
        sparsity_function="Lp",  # Lp or Lp^p
        initial_sparsity_penalty=1e-1,  # equal to l1 penalty in standard trainer
        anneal_start=15000,  # step at which to start annealing p
        anneal_end=None,  # step at which to stop annealing, defaults to steps-1
        p_start=1,  # starting value of p (constant throughout warmup)
        p_end=0,  # annealing p_start to p_end linearly after warmup_steps, exact endpoint excluded
        n_sparsity_updates=10,  # number of times to update the sparsity penalty, at most steps-anneal_start times
        sparsity_queue_length=10,  # number of recent sparsity loss terms, onle needed for adaptive_sparsity_penalty
        resample_steps=None,  # number of steps after which to resample dead neurons
        steps=None,  # total number of steps to train for
        device=None,
        seed=42,
        layer=None,
        lm_name=None,
        wandb_name="PAnnealTrainer",
        submodule_name: str = None,
    ):
        super().__init__(seed)
        assert layer is not None and lm_name is not None
        self.layer = layer
        self.lm_name = lm_name
        self.submodule_name = submodule_name
        if seed is not None:
            t.manual_seed(seed)
            t.cuda.manual_seed_all(seed)
        if device is None:
            self.device = t.device("cuda" if t.cuda.is_available() else "cpu")
        else:
            self.device = device
        # initialize dictionary
        self.activation_dim = activation_dim
        self.dict_size = dict_size
        self.ae = dict_class(activation_dim, dict_size)
        self.ae.to(self.device)
        self.lr = lr
        self.sparsity_function = sparsity_function
        self.anneal_start = anneal_start
        self.anneal_end = anneal_end if anneal_end is not None else steps
        self.p_start = p_start
        self.p_end = p_end
        self.p = p_start
        self.next_p = None
        if n_sparsity_updates == "continuous":
            self.n_sparsity_updates = self.anneal_end - anneal_start + 1
        else:
            self.n_sparsity_updates = n_sparsity_updates
        self.sparsity_update_steps = t.linspace(
            anneal_start, self.anneal_end, self.n_sparsity_updates, dtype=int
        )
        self.p_values = t.linspace(p_start, p_end, self.n_sparsity_updates)
        self.p_step_count = 0
        self.sparsity_coeff = initial_sparsity_penalty  # alpha
        self.sparsity_queue_length = sparsity_queue_length
        self.sparsity_queue = []
        self.warmup_steps = warmup_steps
        self.steps = steps
        self.logging_parameters = [
            "p",
            "next_p",
            "lp_loss",
            "scaled_lp_loss",
            "sparsity_coeff",
        ]
        self.seed = seed
        self.wandb_name = wandb_name
        self.resample_steps = resample_steps
        if self.resample_steps is not None:
            # how many steps since each neuron was last activated?
            self.steps_since_active = t.zeros(self.dict_size, dtype=int).to(self.device)
        else:
            self.steps_since_active = None
        self.optimizer = ConstrainedAdam(
            self.ae.parameters(), self.ae.decoder.parameters(), lr=lr
        )
        if resample_steps is None:
            def warmup_fn(step):
                return min(step / warmup_steps, 1.0)
        else:
            def warmup_fn(step):
                return min((step % resample_steps) / warmup_steps, 1.0)
        self.scheduler = t.optim.lr_scheduler.LambdaLR(
            self.optimizer, lr_lambda=warmup_fn
        )
        if (self.sparsity_update_steps.unique(return_counts=True)[1] > 1).any():
            print("Warning! Duplicates om self.sparsity_update_steps detected!")
    def resample_neurons(self, deads, activations):
        with t.no_grad():
            if deads.sum() == 0:
                return
            print(f"resampling {deads.sum().item()} neurons")
            # compute loss for each activation
            losses = (activations - self.ae(activations)).norm(dim=-1)
            # sample input to create encoder/decoder weights from
            n_resample = min([deads.sum(), losses.shape[0]])
            indices = t.multinomial(losses, num_samples=n_resample, replacement=False)
            sampled_vecs = activations[indices]
            # reset encoder/decoder weights for dead neurons
            alive_norm = self.ae.encoder.weight[~deads].norm(dim=-1).mean()
            self.ae.encoder.weight[deads][:n_resample] = sampled_vecs * alive_norm * 0.2
            self.ae.decoder.weight[:, deads][:, :n_resample] = (
                sampled_vecs / sampled_vecs.norm(dim=-1, keepdim=True)
            ).T
            self.ae.encoder.bias[deads][:n_resample] = 0.0
            # reset Adam parameters for dead neurons
            state_dict = self.optimizer.state_dict()["state"]
            ## encoder weight
            state_dict[1]["exp_avg"][deads] = 0.0
            state_dict[1]["exp_avg_sq"][deads] = 0.0
            ## encoder bias
            state_dict[2]["exp_avg"][deads] = 0.0
            state_dict[2]["exp_avg_sq"][deads] = 0.0
            ## decoder weight
            state_dict[3]["exp_avg"][:, deads] = 0.0
            state_dict[3]["exp_avg_sq"][:, deads] = 0.0
    def lp_norm(self, f, p):
        norm_sq = f.pow(p).sum(dim=-1)
        if self.sparsity_function == "Lp^p":
            return norm_sq.mean()
        elif self.sparsity_function == "Lp":
            return norm_sq.pow(1 / p).mean()
        else:
            raise ValueError("Sparsity function must be 'Lp' or 'Lp^p'")
    def loss(self, x, step, logging=False):
        # Compute loss terms
        x_hat, f = self.ae(x, output_features=True)
        l2_loss = t.linalg.norm(x - x_hat, dim=-1).mean()
        lp_loss = self.lp_norm(f, self.p)
        scaled_lp_loss = lp_loss * self.sparsity_coeff
        self.lp_loss = lp_loss
        self.scaled_lp_loss = scaled_lp_loss
        if self.next_p is not None:
            lp_loss_next = self.lp_norm(f, self.next_p)
            self.sparsity_queue.append([self.lp_loss.item(), lp_loss_next.item()])
            self.sparsity_queue = self.sparsity_queue[-self.sparsity_queue_length :]
        if step in self.sparsity_update_steps:
            # check to make sure we don't update on repeat step:
            if step >= self.sparsity_update_steps[self.p_step_count]:
                # Adapt sparsity penalty alpha
                if self.next_p is not None:
                    local_sparsity_new = t.tensor(
                        [i[0] for i in self.sparsity_queue]
                    ).mean()
                    local_sparsity_old = t.tensor(
                        [i[1] for i in self.sparsity_queue]
                    ).mean()
                    self.sparsity_coeff = (
                        self.sparsity_coeff
                        * (local_sparsity_new / local_sparsity_old).item()
                    )
                # Update p
                self.p = self.p_values[self.p_step_count].item()
                if self.p_step_count < self.n_sparsity_updates - 1:
                    self.next_p = self.p_values[self.p_step_count + 1].item()
                else:
                    self.next_p = self.p_end
                self.p_step_count += 1
        # Update dead feature count
        if self.steps_since_active is not None:
            # update steps_since_active
            deads = (f == 0).all(dim=0)
            self.steps_since_active[deads] += 1
            self.steps_since_active[~deads] = 0
        if logging is False:
            return l2_loss + scaled_lp_loss
        else:
            loss_log = {
                "p": self.p,
                "next_p": self.next_p,
                "lp_loss": lp_loss.item(),
                "scaled_lp_loss": scaled_lp_loss.item(),
                "sparsity_coeff": self.sparsity_coeff,
            }
            return x, x_hat, f, loss_log
    def update(self, step, activations):
        activations = activations.to(self.device)
        self.optimizer.zero_grad()
        loss = self.loss(activations, step, logging=False)
        loss.backward()
        self.optimizer.step()
        self.scheduler.step()
        if (
            self.resample_steps is not None
            and step % self.resample_steps == self.resample_steps - 1
        ):
            self.resample_neurons(
                self.steps_since_active > self.resample_steps / 2, activations
            )
    @property
    def config(self):
        return {
            "trainer_class": "PAnnealTrainer",
            "dict_class": "AutoEncoder",
            "activation_dim": self.activation_dim,
            "dict_size": self.dict_size,
            "lr": self.lr,
            "sparsity_function": self.sparsity_function,
            "sparsity_penalty": self.sparsity_coeff,
            "p_start": self.p_start,
            "p_end": self.p_end,
            "anneal_start": self.anneal_start,
            "sparsity_queue_length": self.sparsity_queue_length,
            "n_sparsity_updates": self.n_sparsity_updates,
            "warmup_steps": self.warmup_steps,
            "resample_steps": self.resample_steps,
            "steps": self.steps,
            "seed": self.seed,
            "layer": self.layer,
            "lm_name": self.lm_name,
            "wandb_name": self.wandb_name,
            "submodule_name": self.submodule_name,
        }

================
File: neuronpedia/butanium_dictionary_learning/dictionary_learning/trainers/standard.py
================
"""
Implements the standard SAE training scheme.
"""
import torch as t
from ..trainers.trainer import SAETrainer
from ..config import DEBUG
from ..dictionary import AutoEncoder
from collections import namedtuple
class ConstrainedAdam(t.optim.Adam):
    """
    A variant of Adam where some of the parameters are constrained to have unit norm.
    """
    def __init__(self, params, constrained_params, lr):
        super().__init__(params, lr=lr)
        self.constrained_params = list(constrained_params)
    def step(self, closure=None):
        with t.no_grad():
            for p in self.constrained_params:
                normed_p = p / p.norm(dim=0, keepdim=True)
                # project away the parallel component of the gradient
                p.grad -= (p.grad * normed_p).sum(dim=0, keepdim=True) * normed_p
        super().step(closure=closure)
        with t.no_grad():
            for p in self.constrained_params:
                # renormalize the constrained parameters
                p /= p.norm(dim=0, keepdim=True)
class StandardTrainer(SAETrainer):
    """
    Standard SAE training scheme.
    """
    def __init__(
        self,
        dict_class=AutoEncoder,
        activation_dim=512,
        dict_size=64 * 512,
        lr=1e-3,
        l1_penalty=1e-1,
        warmup_steps=1000,  # lr warmup period at start of training and after each resample
        resample_steps=None,  # how often to resample neurons
        seed=None,
        device=None,
        layer=None,
        lm_name=None,
        wandb_name="StandardTrainer",
        submodule_name=None,
        compile=False,
    ):
        super().__init__(seed)
        assert layer is not None and lm_name is not None
        self.layer = layer
        self.lm_name = lm_name
        self.submodule_name = submodule_name
        if seed is not None:
            t.manual_seed(seed)
            t.cuda.manual_seed_all(seed)
        # initialize dictionary
        self.ae = dict_class(activation_dim, dict_size)
        if compile:
            self.ae = t.compile(self.ae)
        self.lr = lr
        self.l1_penalty = l1_penalty
        self.warmup_steps = warmup_steps
        self.wandb_name = wandb_name
        if device is None:
            self.device = "cuda" if t.cuda.is_available() else "cpu"
        else:
            self.device = device
        self.ae.to(self.device)
        self.resample_steps = resample_steps
        if self.resample_steps is not None:
            # how many steps since each neuron was last activated?
            self.steps_since_active = t.zeros(self.ae.dict_size, dtype=int).to(
                self.device
            )
        else:
            self.steps_since_active = None
        self.optimizer = ConstrainedAdam(
            self.ae.parameters(), self.ae.decoder.parameters(), lr=lr
        )
        if resample_steps is None:
            def warmup_fn(step):
                return min(step / warmup_steps, 1.0)
        else:
            def warmup_fn(step):
                return min((step % resample_steps) / warmup_steps, 1.0)
        self.scheduler = t.optim.lr_scheduler.LambdaLR(
            self.optimizer, lr_lambda=warmup_fn
        )
    def resample_neurons(self, deads, activations):
        with t.no_grad():
            if deads.sum() == 0:
                return
            print(f"resampling {deads.sum().item()} neurons")
            # compute loss for each activation
            losses = (activations - self.ae(activations)).norm(dim=-1)
            # sample input to create encoder/decoder weights from
            n_resample = min([deads.sum(), losses.shape[0]])
            indices = t.multinomial(losses, num_samples=n_resample, replacement=False)
            sampled_vecs = activations[indices]
            # get norm of the living neurons
            alive_norm = self.ae.encoder.weight[~deads].norm(dim=-1).mean()
            # resample first n_resample dead neurons
            deads[deads.nonzero()[n_resample:]] = False
            self.ae.encoder.weight[deads] = sampled_vecs * alive_norm * 0.2
            self.ae.decoder.weight[:, deads] = (
                sampled_vecs / sampled_vecs.norm(dim=-1, keepdim=True)
            ).T
            self.ae.encoder.bias[deads] = 0.0
            # reset Adam parameters for dead neurons
            state_dict = self.optimizer.state_dict()["state"]
            ## encoder weight
            state_dict[1]["exp_avg"][deads] = 0.0
            state_dict[1]["exp_avg_sq"][deads] = 0.0
            ## encoder bias
            state_dict[2]["exp_avg"][deads] = 0.0
            state_dict[2]["exp_avg_sq"][deads] = 0.0
            ## decoder weight
            state_dict[3]["exp_avg"][:, deads] = 0.0
            state_dict[3]["exp_avg_sq"][:, deads] = 0.0
    def loss(self, x, logging=False, **kwargs):
        x_hat, f = self.ae(x, output_features=True)
        l2_loss = t.linalg.norm(x - x_hat, dim=-1).mean()
        l1_loss = f.norm(p=1, dim=-1).mean()
        if self.steps_since_active is not None:
            # update steps_since_active
            deads = (f == 0).all(dim=0)
            self.steps_since_active[deads] += 1
            self.steps_since_active[~deads] = 0
        loss = l2_loss + self.l1_penalty * l1_loss
        if not logging:
            return loss
        else:
            return namedtuple("LossLog", ["x", "x_hat", "f", "losses"])(
                x,
                x_hat,
                f,
                {
                    "l2_loss": l2_loss.item(),
                    "mse_loss": (x - x_hat).pow(2).sum(dim=-1).mean().item(),
                    "sparsity_loss": l1_loss.item(),
                    "loss": loss.item(),
                },
            )
    def update(self, step, activations):
        activations = activations.to(self.device)
        self.optimizer.zero_grad()
        loss = self.loss(activations)
        loss.backward()
        self.optimizer.step()
        self.scheduler.step()
        if self.resample_steps is not None and step % self.resample_steps == 0:
            self.resample_neurons(
                self.steps_since_active > self.resample_steps / 2, activations
            )
    @property
    def config(self):
        return {
            "dict_class": "AutoEncoder",
            "trainer_class": "StandardTrainer",
            "activation_dim": self.ae.activation_dim,
            "dict_size": self.ae.dict_size,
            "lr": self.lr,
            "l1_penalty": self.l1_penalty,
            "warmup_steps": self.warmup_steps,
            "resample_steps": self.resample_steps,
            "device": self.device,
            "layer": self.layer,
            "lm_name": self.lm_name,
            "wandb_name": self.wandb_name,
            "submodule_name": self.submodule_name,
        }

================
File: neuronpedia/butanium_dictionary_learning/dictionary_learning/trainers/top_k.py
================
"""
Implements the SAE training scheme from https://arxiv.org/abs/2406.04093.
Significant portions of this code have been copied from https://github.com/EleutherAI/sae/blob/main/sae
"""
import einops
import torch as t
import torch.nn as nn
from collections import namedtuple
from ..config import DEBUG
from ..dictionary import Dictionary
from ..trainers.trainer import SAETrainer
@t.no_grad()
def geometric_median(points: t.Tensor, max_iter: int = 100, tol: float = 1e-5):
    """Compute the geometric median `points`. Used for initializing decoder bias."""
    # Initialize our guess as the mean of the points
    guess = points.mean(dim=0)
    prev = t.zeros_like(guess)
    # Weights for iteratively reweighted least squares
    weights = t.ones(len(points), device=points.device)
    for _ in range(max_iter):
        prev = guess
        # Compute the weights
        weights = 1 / t.norm(points - guess, dim=1)
        # Normalize the weights
        weights /= weights.sum()
        # Compute the new geometric median
        guess = (weights.unsqueeze(1) * points).sum(dim=0)
        # Early stopping condition
        if t.norm(guess - prev) < tol:
            break
    return guess
class AutoEncoderTopK(Dictionary, nn.Module):
    """
    The top-k autoencoder architecture and initialization used in https://arxiv.org/abs/2406.04093
    NOTE: (From Adam Karvonen) There is an unmaintained implementation using Triton kernels in the topk-triton-implementation branch.
    We abandoned it as we didn't notice a significant speedup and it added complications, which are noted
    in the AutoEncoderTopK class docstring in that branch.
    With some additional effort, you can train a Top-K SAE with the Triton kernels and modify the state dict for compatibility with this class.
    Notably, the Triton kernels currently have the decoder to be stored in nn.Parameter, not nn.Linear, and the decoder weights must also
    be stored in the same shape as the encoder.
    """
    def __init__(self, activation_dim: int, dict_size: int, k: int):
        super().__init__()
        self.activation_dim = activation_dim
        self.dict_size = dict_size
        self.k = k
        self.encoder = nn.Linear(activation_dim, dict_size)
        self.encoder.bias.data.zero_()
        self.decoder = nn.Linear(dict_size, activation_dim, bias=False)
        self.decoder.weight.data = self.encoder.weight.data.clone().T
        self.set_decoder_norm_to_unit_norm()
        self.b_dec = nn.Parameter(t.zeros(activation_dim))
    def encode(self, x: t.Tensor, return_topk: bool = False):
        post_relu_feat_acts_BF = nn.functional.relu(self.encoder(x - self.b_dec))
        post_topk = post_relu_feat_acts_BF.topk(self.k, sorted=False, dim=-1)
        # We can't split immediately due to nnsight
        tops_acts_BK = post_topk.values
        top_indices_BK = post_topk.indices
        buffer_BF = t.zeros_like(post_relu_feat_acts_BF)
        encoded_acts_BF = buffer_BF.scatter_(
            dim=-1, index=top_indices_BK, src=tops_acts_BK
        )
        if return_topk:
            return encoded_acts_BF, tops_acts_BK, top_indices_BK
        else:
            return encoded_acts_BF
    def decode(self, x: t.Tensor) -> t.Tensor:
        return self.decoder(x) + self.b_dec
    def forward(self, x: t.Tensor, output_features: bool = False):
        encoded_acts_BF = self.encode(x)
        x_hat_BD = self.decode(encoded_acts_BF)
        if not output_features:
            return x_hat_BD
        else:
            return x_hat_BD, encoded_acts_BF
    @t.no_grad()
    def set_decoder_norm_to_unit_norm(self):
        eps = t.finfo(self.decoder.weight.dtype).eps
        norm = t.norm(self.decoder.weight.data, dim=0, keepdim=True)
        self.decoder.weight.data /= norm + eps
    @t.no_grad()
    def remove_gradient_parallel_to_decoder_directions(self):
        assert self.decoder.weight.grad is not None  # keep pyright happy
        parallel_component = einops.einsum(
            self.decoder.weight.grad,
            self.decoder.weight.data,
            "d_in d_sae, d_in d_sae -> d_sae",
        )
        self.decoder.weight.grad -= einops.einsum(
            parallel_component,
            self.decoder.weight.data,
            "d_sae, d_in d_sae -> d_in d_sae",
        )
    def from_pretrained(path, k: int, device=None):
        """
        Load a pretrained autoencoder from a file.
        """
        state_dict = t.load(path)
        dict_size, activation_dim = state_dict["encoder.weight"].shape
        autoencoder = AutoEncoderTopK(activation_dim, dict_size, k)
        autoencoder.load_state_dict(state_dict)
        if device is not None:
            autoencoder.to(device)
        return autoencoder
class TrainerTopK(SAETrainer):
    """
    Top-K SAE training scheme.
    """
    def __init__(
        self,
        dict_class=AutoEncoderTopK,
        activation_dim=512,
        dict_size=64 * 512,
        k=100,
        auxk_alpha=1 / 32,  # see Appendix A.2
        decay_start=24000,  # when does the lr decay start
        steps=30000,  # when when does training end
        seed=None,
        device=None,
        layer=None,
        lm_name=None,
        wandb_name="AutoEncoderTopK",
        submodule_name=None,
    ):
        super().__init__(seed)
        assert layer is not None and lm_name is not None
        self.layer = layer
        self.lm_name = lm_name
        self.submodule_name = submodule_name
        self.wandb_name = wandb_name
        self.steps = steps
        self.k = k
        if seed is not None:
            t.manual_seed(seed)
            t.cuda.manual_seed_all(seed)
        # Initialise autoencoder
        self.ae = dict_class(activation_dim, dict_size, k)
        if device is None:
            self.device = "cuda" if t.cuda.is_available() else "cpu"
        else:
            self.device = device
        self.ae.to(self.device)
        # Auto-select LR using 1 / sqrt(d) scaling law from Figure 3 of the paper
        scale = dict_size / (2**14)
        self.lr = 2e-4 / scale**0.5
        self.auxk_alpha = auxk_alpha
        self.dead_feature_threshold = 10_000_000
        # Optimizer and scheduler
        self.optimizer = t.optim.Adam(
            self.ae.parameters(), lr=self.lr, betas=(0.9, 0.999)
        )
        def lr_fn(step):
            if step < decay_start:
                return 1.0
            else:
                return (steps - step) / (steps - decay_start)
        self.scheduler = t.optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda=lr_fn)
        # Training parameters
        self.num_tokens_since_fired = t.zeros(dict_size, dtype=t.long, device=device)
        # Log the effective L0, i.e. number of features actually used, which should a constant value (K)
        # Note: The standard L0 is essentially a measure of dead features for Top-K SAEs)
        self.logging_parameters = ["effective_l0", "dead_features"]
        self.effective_l0 = -1
        self.dead_features = -1
    def loss(self, x, step=None, logging=False):
        # Run the SAE
        f, top_acts, top_indices = self.ae.encode(x, return_topk=True)
        x_hat = self.ae.decode(f)
        # Measure goodness of reconstruction
        e = x_hat - x
        total_variance = (x - x.mean(0)).pow(2).sum(0)
        # Update the effective L0 (again, should just be K)
        self.effective_l0 = top_acts.size(1)
        # Update "number of tokens since fired" for each features
        num_tokens_in_step = x.size(0)
        did_fire = t.zeros_like(self.num_tokens_since_fired, dtype=t.bool)
        did_fire[top_indices.flatten()] = True
        self.num_tokens_since_fired += num_tokens_in_step
        self.num_tokens_since_fired[did_fire] = 0
        # Compute dead feature mask based on "number of tokens since fired"
        dead_mask = (
            self.num_tokens_since_fired > self.dead_feature_threshold
            if self.auxk_alpha > 0
            else None
        ).to(f.device)
        self.dead_features = int(dead_mask.sum())
        # If dead features: Second decoder pass for AuxK loss
        if dead_mask is not None and (num_dead := int(dead_mask.sum())) > 0:
            # Heuristic from Appendix B.1 in the paper
            k_aux = x.shape[-1] // 2
            # Reduce the scale of the loss if there are a small number of dead latents
            scale = min(num_dead / k_aux, 1.0)
            k_aux = min(k_aux, num_dead)
            # Don't include living latents in this loss
            auxk_latents = t.where(dead_mask[None], f, -t.inf)
            # Top-k dead latents
            auxk_acts, auxk_indices = auxk_latents.topk(k_aux, sorted=False)
            auxk_buffer_BF = t.zeros_like(f)
            auxk_acts_BF = auxk_buffer_BF.scatter_(
                dim=-1, index=auxk_indices, src=auxk_acts
            )
            # Encourage the top ~50% of dead latents to predict the residual of the
            # top k living latents
            e_hat = self.ae.decode(auxk_acts_BF)
            auxk_loss = (e_hat - e).pow(2)  # .sum(0)
            auxk_loss = scale * t.mean(auxk_loss / total_variance)
        else:
            auxk_loss = x_hat.new_tensor(0.0)
        l2_loss = e.pow(2).sum(dim=-1).mean()
        auxk_loss = auxk_loss.sum(dim=-1).mean()
        loss = l2_loss + self.auxk_alpha * auxk_loss
        if not logging:
            return loss
        else:
            return namedtuple("LossLog", ["x", "x_hat", "f", "losses"])(
                x,
                x_hat,
                f,
                {
                    "l2_loss": l2_loss.item(),
                    "auxk_loss": auxk_loss.item(),
                    "loss": loss.item(),
                },
            )
    def update(self, step, x):
        # Initialise the decoder bias
        if step == 0:
            median = geometric_median(x)
            self.ae.b_dec.data = median
        # Make sure the decoder is still unit-norm
        self.ae.set_decoder_norm_to_unit_norm()
        # compute the loss
        x = x.to(self.device)
        loss = self.loss(x, step=step)
        loss.backward()
        # clip grad norm and remove grads parallel to decoder directions
        t.nn.utils.clip_grad_norm_(self.ae.parameters(), 1.0)
        self.ae.remove_gradient_parallel_to_decoder_directions()
        # do a training step
        self.optimizer.step()
        self.optimizer.zero_grad()
        self.scheduler.step()
        return loss.item()
    @property
    def config(self):
        return {
            "trainer_class": "TrainerTopK",
            "dict_class": "AutoEncoderTopK",
            "lr": self.lr,
            "steps": self.steps,
            "seed": self.seed,
            "activation_dim": self.ae.activation_dim,
            "dict_size": self.ae.dict_size,
            "k": self.ae.k,
            "device": self.device,
            "layer": self.layer,
            "lm_name": self.lm_name,
            "wandb_name": self.wandb_name,
            "submodule_name": self.submodule_name,
        }

================
File: neuronpedia/butanium_dictionary_learning/dictionary_learning/trainers/trainer.py
================
class SAETrainer:
    """
    Generic class for implementing SAE training algorithms
    """
    def __init__(self, seed=None):
        self.seed = seed
        self.logging_parameters = []
    def update(
        self,
        step,  # index of step in training
        activations,  # of shape [batch_size, d_submodule]
    ):
        pass  # implemented by subclasses
    def get_logging_parameters(self):
        stats = {}
        for param in self.logging_parameters:
            if hasattr(self, param):
                stats[param] = getattr(self, param)
            else:
                print(f"Warning: {param} not found in {self}")
        return stats
    @property
    def config(self):
        return {
            "wandb_name": "trainer",
        }

================
File: neuronpedia/butanium_dictionary_learning/dictionary_learning/training.py
================
"""
Training dictionaries
"""
import json
import os
from collections import defaultdict
import torch as t
from tqdm import tqdm
import wandb
from .dictionary import AutoEncoder
from .evaluation import evaluate
from .trainers.standard import StandardTrainer
from .trainers.crosscoder import CrossCoderTrainer
def get_stats(
    trainer,
    act: t.Tensor,
    deads_sum: bool = True,
):
    with t.no_grad():
        act, act_hat, f, losslog = trainer.loss(
            act, step=0, logging=True, return_deads=True
        )
    # L0
    l0 = (f != 0).float().detach().cpu().sum(dim=-1).mean().item()
    out = {
        "l0": l0,
        **{f"{k}": v for k, v in losslog.items() if k != "deads"},
    }
    if losslog["deads"] is not None:
        total_feats = losslog["deads"].shape[0]
        out["frac_deads"] = (
            losslog["deads"].sum().item() / total_feats
            if deads_sum
            else losslog["deads"]
        )
    # fraction of variance explained
    if act.dim() == 2:
        # act.shape: [batch, d_model]
        # fraction of variance explained
        total_variance = t.var(act, dim=0).sum()
        residual_variance = t.var(act - act_hat, dim=0).sum()
        frac_variance_explained = 1 - residual_variance / total_variance
    else:
        # act.shape: [batch, layer, d_model]
        total_variance_per_layer = []
        residual_variance_per_layer = []
        for l in range(act.shape[1]):
            total_variance_per_layer.append(t.var(act[:, l, :], dim=0).cpu().sum())
            residual_variance_per_layer.append(
                t.var(act[:, l, :] - act_hat[:, l, :], dim=0).cpu().sum()
            )
            out[f"cl{l}_frac_variance_explained"] = (
                1 - residual_variance_per_layer[l] / total_variance_per_layer[l]
            )
        total_variance = sum(total_variance_per_layer)
        residual_variance = sum(residual_variance_per_layer)
        frac_variance_explained = 1 - residual_variance / total_variance
        out["frac_variance_explained"] = frac_variance_explained.item()
    return out
def log_stats(
    trainer,
    step: int,
    act: t.Tensor,
    activations_split_by_head: bool,
    transcoder: bool,
    stage: str = "train",
):
    with t.no_grad():
        log = {}
        if activations_split_by_head:  # x.shape: [batch, pos, n_heads, d_head]
            act = act[..., 0, :]
        if not transcoder:
            stats = get_stats(trainer, act)
            log.update({f"{stage}/{k}": v for k, v in stats.items()})
        else:  # transcoder
            x, x_hat, f, losslog = trainer.loss(act, step=step, logging=True)
            # L0
            l0 = (f != 0).float().sum(dim=-1).mean().item()
            log[f"{stage}/l0"] = l0
        # log parameters from training
        log["step"] = step
        trainer_log = trainer.get_logging_parameters()
        for name, value in trainer_log.items():
            log[f"{stage}/{name}"] = value
        wandb.log(log, step=step)
@t.no_grad()
def run_validation(
    trainer,
    validation_data,
    step: int = None,
):
    l0 = []
    frac_variance_explained = []
    deads = []
    if isinstance(trainer, CrossCoderTrainer):
        frac_variance_explained_per_layer = defaultdict(list)
    for val_step, act in enumerate(tqdm(validation_data, total=len(validation_data))):
        act = act.to(trainer.device)
        stats = get_stats(trainer, act, deads_sum=False)
        l0.append(stats["l0"])
        deads.append(stats["frac_deads"])
        frac_variance_explained.append(stats["frac_variance_explained"])
        if isinstance(trainer, CrossCoderTrainer):
            for l in range(act.shape[1]):
                frac_variance_explained_per_layer[l].append(
                    stats[f"cl{l}_frac_variance_explained"]
                )
    log = {}
    log["val/frac_deads"] = t.stack(deads).all(dim=0).float().mean().item()
    log["val/l0"] = t.tensor(l0).mean().item()
    log["val/frac_variance_explained"] = t.tensor(frac_variance_explained).mean()
    if isinstance(trainer, CrossCoderTrainer):
        for l in range(act.shape[1]):
            log[f"val/cl{l}_frac_variance_explained"] = t.tensor(
                frac_variance_explained_per_layer[l]
            ).mean()
    if step is not None:
        log["step"] = step
    wandb.log(log, step=step)
def trainSAE(
    data,
    trainer_config,
    use_wandb=False,
    wandb_entity="",
    wandb_project="",
    steps=None,
    save_steps=None,
    save_dir=None,
    log_steps=None,
    activations_split_by_head=False,
    validate_every_n_steps=None,
    validation_data=None,
    transcoder=False,
    run_cfg={},
):
    """
    Train SAE using the given trainer
    """
    assert not (
        validation_data is None and validate_every_n_steps is not None
    ), "Must provide validation data if validate_every_n_steps is not None"
    trainer_class = trainer_config["trainer"]
    del trainer_config["trainer"]
    trainer = trainer_class(**trainer_config)
    wandb_config = trainer.config | run_cfg
    wandb.init(
        entity=wandb_entity,
        project=wandb_project,
        config=wandb_config,
        name=wandb_config["wandb_name"],
        mode="disabled" if not use_wandb else "online",
    )
    # make save dir, export config
    if save_dir is not None:
        os.makedirs(save_dir, exist_ok=True)
        # save config
        config = {"trainer": trainer.config}
        try:
            config["buffer"] = data.config
        except:
            pass
        with open(os.path.join(save_dir, "config.json"), "w") as f:
            json.dump(config, f, indent=4)
    for step, act in enumerate(tqdm(data, total=steps)):
        if steps is not None and step >= steps:
            break
        act = act.to(trainer.device)
        # logging
        if log_steps is not None and step % log_steps == 0 and step != 0:
            log_stats(trainer, step, act, activations_split_by_head, transcoder)
        # saving
        if save_steps is not None and step % save_steps == 0:
            if save_dir is not None:
                os.makedirs(
                    os.path.join(save_dir, trainer.config["wandb_name"].lower()),
                    exist_ok=True,
                )
                t.save(
                    (
                        trainer.ae.state_dict()
                        if not trainer_config["compile"]
                        else trainer.ae._orig_mod.state_dict()
                    ),
                    os.path.join(
                        save_dir, trainer.config["wandb_name"].lower(), f"ae_{step}.pt"
                    ),
                )
        # training
        trainer.update(step, act)
        if (
            validate_every_n_steps is not None
            and step % validate_every_n_steps == 0
            and step != 0
        ):
            print(f"Validating at step {step}")
            run_validation(trainer, validation_data, step=step)
    try:
        run_validation(trainer, validation_data, step=step)
    except Exception as e:
        print(f"Error during final validation: {str(e)}")
    # save final SAE
    if save_dir is not None:
        os.makedirs(
            os.path.join(save_dir, trainer.config["wandb_name"].lower()), exist_ok=True
        )
        t.save(
            (
                trainer.ae.state_dict()
                if not trainer_config["compile"]
                else trainer.ae._orig_mod.state_dict()
            ),
            os.path.join(
                save_dir, trainer.config["wandb_name"].lower(), f"ae_final.pt"
            ),
        )
    if use_wandb:
        wandb.finish()

================
File: neuronpedia/butanium_dictionary_learning/dictionary_learning/utils.py
================
from datasets import load_dataset
import zstandard as zstd
import io
import json
def hf_dataset_to_generator(dataset_name, split="train", streaming=True):
    dataset = load_dataset(dataset_name, split=split, streaming=streaming)
    def gen():
        for x in iter(dataset):
            yield x["text"]
    return gen()
def zst_to_generator(data_path):
    """
    Load a dataset from a .jsonl.zst file.
    The jsonl entries is assumed to have a 'text' field
    """
    compressed_file = open(data_path, "rb")
    dctx = zstd.ZstdDecompressor()
    reader = dctx.stream_reader(compressed_file)
    text_stream = io.TextIOWrapper(reader, encoding="utf-8")
    def generator():
        for line in text_stream:
            yield json.loads(line)["text"]
    return generator()

================
File: neuronpedia/butanium_dictionary_learning/README.md
================
# Dictionary Learning and Crosscoders
This repo contains a few new features compared to the original repo:
- It is `pip` installable.
- A new `Crosscoder` class for training CrossCoders as described in [the anthropic paper](https://transformer-circuits.pub/drafts/crosscoders/index.html#model-diffing).
```py
!pip install git+https://github.com/jkminder/dictionary_learning
from dictionary_learning import CrossCoder
from nnsight import LanguageModel
import torch as th

crosscoder = CrossCoder.from_pretrained("Butanium/gemma-2-2b-crosscoder-l13-mu4.1e-02-lr1e-04", from_hub=True)
gemma_2 = LanguageModel("google/gemma-2-2b", device_map="cuda:0")
gemma_2_it = LanguageModel("google/gemma-2-2b-it", device_map="cuda:1")
prompt = "quick fox brown"

with gemma_2.trace(prompt):
    l13_act_base = gemma_2.model.layers[13].output[0][:, -1].save() # (1, 2304)
    gemma_2.model.layers[13].output.stop()

with gemma_2_it.trace(prompt):
    l13_act_it = gemma_2_it.model.layers[13].output[0][:, -1].save() # (1, 2304)
    gemma_2_it.model.layers[13].output.stop()


crosscoder_input = th.cat([l13_act_base, l13_act_it], dim=0).unsqueeze(0).cpu() # (batch, 2, 2304)
print(crosscoder_input.shape)
reconstruction, features = crosscoder(crosscoder_input, output_features=True)

# print metrics
print(f"MSE loss: {th.nn.functional.mse_loss(reconstruction, crosscoder_input).item():.2f}")
print(f"L1 sparsity: {features.abs().sum():.1f}")
print(f"L0 sparsity: {(features > 1e-4).sum()}")
```
- A way to cache activations in order to load them later to train a SAE or Crosscoder in `cache.py`.
- A script for training a Crosscoder using pre-computed activations in `scripts/train_crosscoder.py`.
- You can now load and push dictionaries to the Huggingface model hub.
```py
my_super_cool_dictionary.push_to_hub("username/my-super-cool-dictionary")
loaded_dictionary = MyDictionary.from_pretrained("username/my-super-cool-dictionary", from_hub=True)
```

# Original README

This is a repository for doing dictionary learning via sparse autoencoders on neural network activations. It was developed by Samuel Marks and Aaron Mueller. 

For accessing, saving, and intervening on NN activations, we use the [`nnsight`](http://nnsight.net/) package; as of March 2024, `nnsight` is under active development and may undergo breaking changes. That said, `nnsight` is easy to use and quick to learn; if you plan to modify this repo, then we recommend going through the main `nnsight` demo [here](https://nnsight.net/notebooks/tutorials/walkthrough/).

Some dictionaries trained using this repository (and asociated training checkpoints) can be accessed at [https://baulab.us/u/smarks/autoencoders/](https://baulab.us/u/smarks/autoencoders/). See below for more information about these dictionaries.

# Set-up

Navigate to the to the location where you would like to clone this repo, clone and enter the repo, and install the requirements.
```bash
git clone https://github.com/saprmarks/dictionary_learning
cd dictionary_learning
pip install -r requirements.txt
```

To use `dictionary_learning`, include it as a subdirectory in some project's directory and import it; see the examples below.

# Using trained dictionaries

You can load and used a pretrained dictionary as follows
```python
from dictionary_learning import AutoEncoder

# load autoencoder
ae = AutoEncoder.from_pretrained("path/to/dictionary/weights")

# get NN activations using your preferred method: hooks, transformer_lens, nnsight, etc. ...
# for now we'll just use random activations
activations = torch.randn(64, activation_dim)
features = ae.encode(activations) # get features from activations
reconstructed_activations = ae.decode(features)

# you can also just get the reconstruction ...
reconstructed_activations = ae(activations)
# ... or get the features and reconstruction at the same time
reconstructed_activations, features = ae(activations, output_features=True)
```
Dictionaries have `encode`, `decode`, and `forward` methods -- see `dictionary.py`.

## Loading JumpReLU SAEs from `sae_lens`
We have limited support for automatically converting SAEs from `sae_lens`; currently this is only supported for JumpReLU SAEs, but we may expand support if users are interested.
```python
from dictionary_learning import JumpReluAutoEncoder

ae = JumpReluAutoEncoder.from_pretrained(
    load_from_sae_lens=True,
    release="your_release_name",
    sae_id="your_sae_id"
)
```
The arguments should should match those used in the `SAE.from_pretrained` call you would use to load an SAE in `sae_lens`. For this to work, `sae_lens` should be installed in your environment.


# Training your own dictionaries

To train your own dictionaries, you'll need to understand a bit about our infrastructure. (See below for downloading our dictionaries.)

This repository supports different sparse autoencoder architectures, including standard `AutoEncoder` ([Bricken et al., 2023](https://transformer-circuits.pub/2023/monosemantic-features/index.html)), `GatedAutoEncoder` ([Rajamanoharan et al., 2024](https://arxiv.org/abs/2404.16014)), and `AutoEncoderTopK` ([Gao et al., 2024](https://arxiv.org/abs/2406.04093)).
Each sparse autoencoder architecture is implemented with a corresponding trainer that implements the training protocol described by the authors.
This allows us to implement different training protocols (e.g. p-annealing) for different architectures without a lot of overhead.
Specifically, this repository supports the following trainers:
- [`StandardTrainer`](trainers/standard.py): Implements a training scheme similar to that of [Bricken et al., 2023](https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder).
- [`GatedSAETrainer`](trainers/gdm.py): Implements the training scheme for Gated SAEs described in [Rajamanoharan et al., 2024](https://arxiv.org/abs/2404.16014).
- [`AutoEncoderTopK`](trainers/top_k.py): Implemented the training scheme for Top-K SAEs described in [Gao et al., 2024](https://arxiv.org/abs/2406.04093).
- [`PAnnealTrainer`](trainers/p_anneal.py): Extends the `StandardTrainer` by providing the option to anneal the sparsity parameter p.
- [`GatedAnnealTrainer`](trainers/gated_anneal.py): Extends the `GatedSAETrainer` by providing the option for p-annealing, similar to `PAnnealTrainer`.

Another key object is the `ActivationBuffer`, defined in `buffer.py`. Following [Neel Nanda's appraoch](https://www.lesswrong.com/posts/fKuugaxt2XLTkASkk/open-source-replication-and-commentary-on-anthropic-s), `ActivationBuffer`s maintain a buffer of NN activations, which it outputs in batches.

An `ActivationBuffer` is initialized from an `nnsight` `LanguageModel` object, a submodule (e.g. an MLP), and a generator which yields strings (the text data). It processes a large number of strings, up to some capacity, and saves the submodule's activations. You sample batches from it, and when it is half-depleted, it refreshes itself with new text data.

Here's an example for training a dictionary; in it we load a language model as an `nnsight` `LanguageModel` (this will work for any Huggingface model), specify a submodule, create an `ActivationBuffer`, and then train an autoencoder with `trainSAE`.
```python
from nnsight import LanguageModel
from dictionary_learning import ActivationBuffer, AutoEncoder
from dictionary_learning.trainers import StandardTrainer
from dictionary_learning.training import trainSAE

device = "cuda:0"
model_name = "EleutherAI/pythia-70m-deduped" # can be any Huggingface model

model = LanguageModel(
    model_name,
    device_map=device,
)
submodule = model.gpt_neox.layers[1].mlp # layer 1 MLP
activation_dim = 512 # output dimension of the MLP
dictionary_size = 16 * activation_dim

# data must be an iterator that outputs strings
data = iter(
    [
        "This is some example data",
        "In real life, for training a dictionary",
        "you would need much more data than this",
    ]
)
buffer = ActivationBuffer(
    data=data,
    model=model,
    submodule=submodule,
    d_submodule=activation_dim, # output dimension of the model component
    n_ctxs=3e4,  # you can set this higher or lower dependong on your available memory
    device=device,
)  # buffer will yield batches of tensors of dimension = submodule's output dimension

trainer_cfg = {
    "trainer": StandardTrainer,
    "dict_class": AutoEncoder,
    "activation_dim": activation_dim,
    "dict_size": dictionary_size,
    "lr": 1e-3,
    "device": device,
}

# train the sparse autoencoder (SAE)
ae = trainSAE(
    data=buffer,  # you could also use another (i.e. pytorch dataloader) here instead of buffer
    trainer_configs=[trainer_cfg],
)
```
Some technical notes our training infrastructure and supported features:
* Training uses the `ConstrainedAdam` optimizer defined in `training.py`. This is a variant of Adam which supports constraining the `AutoEncoder`'s decoder weights to be norm 1.
* Neuron resampling: if a `resample_steps` argument is passed to `trainSAE`, then dead neurons will periodically be resampled according to the procedure specified [here](https://transformer-circuits.pub/2023/monosemantic-features/index.html#appendix-autoencoder-resampling).
* Learning rate warmup: if a `warmup_steps` argument is passed to `trainSAE`, then a linear LR warmup is used at the start of training and, if doing neuron resampling, also after every time neurons are resampled.

If `submodule` is a model component where the activations are tuples (e.g. this is common when working with residual stream activations), then the buffer yields the first coordinate of the tuple.

# Downloading our open-source dictionaries

To download our pretrained dictionaries automatically, run:

```bash
./pretrained_dictionary_downloader.sh
```
This will download dictionaries of all submodules (~2.5 GB) hosted on huggingface. Currently, we provide dictionaries from the `10_32768` training run. This set has dictionaries for MLP outputs, attention outputs, and residual streams (including embeddings) in all layers of EleutherAI's Pythia-70m-deduped model. These dictionaries were trained on 2B tokens from The Pile.

Let's explain the directory structure by example. After using the script above, you'll have a `dictionaries/pythia-70m-deduped/mlp_out_layer1/10_32768` directory corresponding to the layer 1 MLP dictionary from the `10_32768` set. This directory contains:
* `ae.pt`: the `state_dict` of the fully trained dictionary
* `config.json`: a json file which specifies the hyperparameters used to train the dictionary
* `checkpoints/`: a directory containing training checkpoints of the form `ae_step.pt` (only if you used the `--checkpoints` flag)

We've also previously released other dictionaries which can be found and downloaded [here](https://baulab.us/u/smarks/autoencoders/). 

## Statistics for our dictionaries

We'll report the following statistics for our `10_32768` dictionaries. These were measured using the code in `evaluation.py`.
* **MSE loss**: average squared L2 distance between an activation and the autoencoder's reconstruction of it
* **L1 loss**: a measure of the autoencoder's sparsity
* **L0**: average number of features active above a random token
* **Percentage of neurons alive**: fraction of the dictionary features which are active on at least one token out of 8192 random tokens
* **CE diff**: difference between the usual cross-entropy loss of the model for next token prediction and the cross entropy when replacing activations with our dictionary's reconstruction
* **Percentage of CE loss recovered**: when replacing the activation with the dictionary's reconstruction, the percentage of the model's cross-entropy loss on next token prediction that is recovered (relative to the baseline of zero ablating the activation)

### Attention output dictionaries

| Layer | Variance Explained (%) | L1 | L0  | % Alive | CE Diff | % CE Recovered |
|-------|------------------------|----|-----|---------|---------|----------------|
| 0     | 92                     | 8  | 128 | 17      | 0.02    | 99             |
| 1     | 87                     | 9  | 127 | 17      | 0.03    | 94             |
| 2     | 90                     | 19 | 215 | 12      | 0.05    | 93             |
| 3     | 89                     | 12 | 169 | 13      | 0.03    | 93             |
| 4     | 83                     | 8  | 132 | 14      | 0.01    | 95             |
| 5     | 89                     | 11 | 144 | 20      | 0.02    | 93             |


### MLP output dictionaries

| Layer  | Variance Explained (%) | L1 | L0  | % Alive | CE Diff | % CE Recovered |
|--------|------------------------|----|-----|---------|---------|----------------|
|     0  | 97                     | 5  | 5   | 40      | 0.10    | 99             |
|     1  | 85                     | 8  | 69  | 44      | 0.06    | 95             |
|     2  | 99                     | 12 | 88  | 31      | 0.11    | 88             |
|     3  | 88                     | 20 | 160 | 25      | 0.12    | 94             |
|     4  | 92                     | 20 | 100 | 29      | 0.14    | 90             |
|     5  | 96                     | 31 | 102 | 35      | 0.15    | 97             |


### Residual stream dictionaries
NOTE: these are indexed so that the resid_i dictionary is the *output* of the ith layer. Thus embeddings go first, then layer 0, etc.

| Layer   | Variance Explained (%) | L1 | L0  | % Alive | CE Diff | % CE Recovered |
|---------|------------------------|----|-----|---------|---------|----------------|
|    embed| 96                     |  1 |  3  | 36      | 0.17    | 98             |
|       0 | 92                     | 11 | 59  | 41      | 0.24    | 97             |
|       1 | 85                     | 13 | 54  | 38      | 0.45    | 95             |
|       2 | 96                     | 24 | 108 | 27      | 0.55    | 94             |
|       3 | 96                     | 23 | 68  | 22      | 0.58    | 95             |
|       4 | 88                     | 23 | 61  | 27      | 0.48    | 95             |
|       5 | 90                     | 35 | 72  | 45      | 0.55    | 92             |




# Extra functionality supported by this repo

**Note:** these features are likely to be depricated in future releases.

We've included support for some experimental features. We briefly investigated them as an alternative approaches to training dictionaries.

* **MLP stretchers.** Based on the perspective that one may be able to identify features with "[neurons in a sufficiently large model](https://transformer-circuits.pub/2022/toy_model/index.html)," we experimented with training "autoencoders" to, given as input an MLP *input* activation $x$, output not $x$ but $MLP(x)$ (the same output as the MLP). For instance, given an MLP which maps a 512-dimensional input $x$ to a 1024-dimensional hidden state $h$ and then a 512-dimensional output $y$, we train a dictionary $A$ with hidden dimension 16384 = 16 x 1024 so that $A(x)$ is close to $y$ (and, as usual, so that the hidden state of the dictionary is sparse).
    * The resulting dictionaries seemed decent, but we decided not to pursue the idea further.
    * To use this functionality, set the `io` parameter of an activaiton buffer to `'in_to_out'` (default is `'out'`).
    * h/t to Max Li for this suggestion.
* **Replacing L1 loss with entropy**. Based on the ideas in this [post](https://transformer-circuits.pub/2023/may-update/index.html#simple-factorization), we experimented with using entropy to regularize a dictionary's hidden state instead of L1 loss. This seemed to cause the features to split into dead features (which never fired) and very high-frequency features which fired on nearly every input, which was not the desired behavior. But plausibly there is a way to make this work better.
* **Ghost grads**, as described [here](https://transformer-circuits.pub/2024/jan-update/index.html).

================
File: neuronpedia/butanium_dictionary_learning/requirements.txt
================
circuitsvis>=1.43.2
datasets>=2.18.0
einops>=0.7.0
matplotlib>=3.8.3
nnsight>=0.2.11
pandas>=2.2.1
plotly>=5.18.0
torch>=2.1.2
tqdm>=4.66.1
umap-learn>=0.5.6
zstandard>=0.22.0
wandb

================
File: neuronpedia/np_list.py
================
from dataclasses import dataclass
from typing import List
@dataclass
class NPListItem:
    model_id: str
    source: str
    index: str
    description: str = ""
@dataclass
class NPList:
    id: str
    name: str
    description: str
    items: List[NPListItem]
    @classmethod
    def new(cls, name: str, description: str = "") -> "NPList":
        # import here to avoid circular import
        from neuronpedia.requests.list_request import ListRequest
        return ListRequest().new(name, description)
    @classmethod
    def get_owned(cls) -> list["NPList"]:
        # import here to avoid circular import
        from neuronpedia.requests.list_request import ListRequest
        return ListRequest().get_owned()
    def add_items(self, items: list[NPListItem]) -> "NPList":
        # import here to avoid circular import
        from neuronpedia.requests.list_request import ListRequest
        return ListRequest().add_items(self, items)
    @classmethod
    def get(cls, list_id: str) -> "NPList":
        # import here to avoid circular import
        from neuronpedia.requests.list_request import ListRequest
        return ListRequest().get(list_id)

================
File: neuronpedia/np_sae_feature.py
================
from dataclasses import dataclass
@dataclass
class SAEFeature:
    modelId: str
    source: str
    index: str
    jsonData: str
    @classmethod
    def get(cls, model_id: str, source: str, index: str) -> "SAEFeature":
        from neuronpedia.requests.sae_feature_request import SAEFeatureRequest
        request = SAEFeatureRequest()
        return request.get(model_id, source, index)

================
File: neuronpedia/np_vector.py
================
from dataclasses import dataclass
import os
from typing import List
from neuronpedia.requests.activation_request import Activation
@dataclass
class NPVector:
    """A Vector returned by the Neuronpedia API."""
    label: str
    model_id: str
    source: str
    index: str
    values: List[float]
    hook_name: str
    default_steer_strength: float | None
    url: str | None = None
    def __post_init__(self):
        if not self.url:
            USE_LOCALHOST = os.getenv("USE_LOCALHOST", False)
            BASE_URL = "https://neuronpedia.org/api" if not USE_LOCALHOST else "http://localhost:3000/api"
            self.url = f"{BASE_URL}/{self.model_id}/{self.source}/{self.index}"
    def __eq__(self, other: "NPVector") -> bool:
        return (
            self.model_id == other.model_id
            and self.source == other.source
            and self.index == other.index
            and self.label == other.label
            and self.hook_name == other.hook_name
            and self.values == other.values
            and self.default_steer_strength == other.default_steer_strength
        )
    def delete(self):
        # import here to avoid circular import
        from neuronpedia.requests.vector_request import VectorRequest
        return VectorRequest().delete(self)
    def steer_chat(self, steered_chat_messages: list[dict[str, str]]):
        # import here to avoid circular import
        from neuronpedia.requests.steer_request import SteerChatRequest
        return SteerChatRequest().steer(
            model_id=self.model_id, vectors=[self], steered_chat_messages=steered_chat_messages
        )
    def steer_completion(self, prompt: str):
        # import here to avoid circular import
        from neuronpedia.requests.steer_request import SteerCompletionRequest
        return SteerCompletionRequest().steer(model_id=self.model_id, vectors=[self], prompt=prompt)
    def compute_activation_for_text(self, text: str) -> Activation:
        # import here to avoid circular import
        from neuronpedia.requests.activation_request import ActivationRequest
        return ActivationRequest().compute_activation_for_text(self.model_id, self.source, self.index, text)
    def upload_activations(self, activations: List[Activation]):
        from neuronpedia.requests.activation_request import ActivationRequest
        return ActivationRequest().upload_batch(self.model_id, self.source, self.index, activations)
    @classmethod
    def get(cls, model_id: str, source: str, index: str) -> "NPVector":
        # import here to avoid circular import
        from neuronpedia.requests.vector_request import VectorRequest
        return VectorRequest().get(model_id, source, index)
    @classmethod
    def get_owned(cls) -> List["NPVector"]:
        # import here to avoid circular import
        from neuronpedia.requests.vector_request import VectorRequest
        return VectorRequest().get_owned()
    @classmethod
    def new(
        cls,
        label: str,
        model_id: str,
        layer_num: int,
        hook_type: str,
        vector: list[float],
        default_steer_strength: float | None = 10,
    ) -> "NPVector":
        # import here to avoid circular import
        from neuronpedia.requests.vector_request import VectorRequest
        np_vector = VectorRequest().new(
            label=label,
            model_id=model_id,
            layer_num=layer_num,
            hook_type=hook_type,
            vector=vector,
            default_steer_strength=default_steer_strength,
        )
        return np_vector

================
File: neuronpedia/requests/activation_request.py
================
from requests import Response
from neuronpedia.requests.base_request import (
    NPRequest,
)
from typing import TypedDict, List
class Activation(TypedDict):
    tokens: List[str]
    values: List[float]
    def __init__(self, tokens: List[str], values: List[float]):
        if len(tokens) != len(values):
            raise ValueError("tokens and activation values must have the same length")
        self["tokens"] = tokens
        self["values"] = values
class ActivationRequest(NPRequest):
    def __init__(
        self,
    ):
        super().__init__("activation")
    def compute_activation_for_text(self, model_id: str, source: str, index: str, text: str) -> Activation:
        payload = {
            "feature": {
                "modelId": model_id,
                "layer": source,
                "index": index,
            },
            "customText": text,
        }
        result = self.send_request(method="POST", json=payload, uri="new")
        return Activation(tokens=result["tokens"], values=result["values"])
    def upload_batch(
        self,
        model_id: str,
        source: str,
        index: str,
        activations: List[Activation],
    ) -> Response:
        payload = {
            "modelId": model_id,
            "source": source,
            "index": index,
            "activations": activations,
        }
        return self.send_request(method="POST", json=payload, uri="upload-batch")

================
File: neuronpedia/requests/base_request.py
================
import os
import requests
from dotenv import load_dotenv
load_dotenv()
class NPKeyMissingError(Exception):
    """The environment variable 'NEURONPEDIA_API_KEY' is not set."""
    pass
class NPUnauthorizedError(Exception):
    """Unauthorized. Please check your NEURONPEDIA_API_KEY environment variable."""
    pass
class NPRateLimitError(Exception):
    """Rate limit exceeded. Please try again later or email support@neuronpedia.org to raise your limit."""
    pass
class NPInvalidResponseError(Exception):
    """The API returned an invalid JSON response. Please contact support@neuronpedia.org."""
    pass
class NPRequest:
    USE_LOCALHOST = os.getenv("USE_LOCALHOST", False)
    BASE_URL = "https://neuronpedia.org/api" if not USE_LOCALHOST else "http://localhost:3000/api"
    def __init__(
        self,
        endpoint: str,
    ):
        if not self.BASE_URL:
            raise ValueError("BASE_URL must be set in a subclass or globally.")
        self.endpoint = endpoint
        self.api_key = self._get_api_key()
    @staticmethod
    def _get_api_key():
        """Retrieve and validate the NEURONPEDIA_API_KEY environment variable."""
        api_key = os.getenv("NEURONPEDIA_API_KEY")
        if not api_key:
            raise NPKeyMissingError("The environment variable 'NEURONPEDIA_API_KEY' is not set.")
        return api_key
    def get_url(
        self,
    ):
        """Construct the full URL for the request."""
        return f"{self.BASE_URL}/{self.endpoint}"
    def send_request(
        self,
        uri: str = "",
        method: str = "POST",
        **kwargs,
    ):
        """Send an HTTP request."""
        headers = kwargs.get(
            "headers",
            {},
        )
        headers["X-Api-Key"] = self.api_key
        url = f"{self.BASE_URL}/{self.endpoint}/{uri}"
        print(f"Sending {method} request to {url}")
        # print(f"Body: {json.dumps(kwargs.get('json', {}), indent=2)}")
        response = requests.request(
            method,
            url,
            headers=headers,
            **kwargs,
        )
        if response.status_code == 401:
            raise NPUnauthorizedError("Unauthorized. Please check your NEURONPEDIA_API_KEY environment variable.")
        elif response.status_code == 404:
            raise requests.exceptions.HTTPError(f"Resource not found: {response.status_code}")
        elif response.status_code == 429:
            raise NPRateLimitError(
                "You've exceeded the API rate limit. Try later or email support@neuronpedia.org to raise your limit."
            )
        elif 400 <= response.status_code < 500:
            raise requests.exceptions.HTTPError(f"Request failed with status {response.status_code}: {response.json()}")
        elif 500 <= response.status_code < 600:
            raise requests.exceptions.HTTPError(
                f"Server error occurred: {response.status_code}. Try again later or contact support@neuronpedia.org."
            )
        response.raise_for_status()  # Raise an exception for HTTP errors
        try:
            response_json = response.json()
            print("Got a successful response.")
            # print(json.dumps(response_json, indent=2))
            return response_json
        except requests.exceptions.JSONDecodeError:
            print(f"Error: Response text: {response.text[:1024]}")
            raise NPInvalidResponseError(
                "The API returned an invalid JSON response. Please contact support@neuronpedia.org."
            )

================
File: neuronpedia/requests/list_request.py
================
from neuronpedia.np_list import NPListItem, NPList
from neuronpedia.requests.base_request import (
    NPRequest,
)
class ListRequest(NPRequest):
    def __init__(
        self,
    ):
        super().__init__("list")
    def get_owned(self) -> list[NPList]:
        response = self.send_request(method="POST", uri="list")
        return [
            NPList(id=list["id"], name=list["name"], description=list["description"], items=[]) for list in response
        ]
    def new(self, name: str, description: str = "") -> NPList:
        payload = {"name": name, "description": description}
        response = self.send_request(method="POST", json=payload, uri="new")
        return NPList(
            id=response["id"],
            name=response["name"],
            description=response["description"],
            items=[],
        )
    def add_items(self, nplist: NPList, items: list[NPListItem]) -> NPList:
        formatted_items = [
            {"modelId": item.model_id, "layer": item.source, "index": item.index, "description": item.description}
            for item in items
        ]
        payload = {"listId": nplist.id, "featuresToAdd": formatted_items}
        response = self.send_request(method="POST", json=payload, uri="add-features")
        for item in response:
            nplist.items.append(
                NPListItem(
                    model_id=item["modelId"],
                    source=item["layer"],
                    index=item["index"],
                )
            )
        return nplist
    def get(self, list_id: str) -> NPList:
        payload = {"listId": list_id}
        response = self.send_request(method="POST", json=payload, uri="get")
        return NPList(
            id=response["id"],
            name=response["name"],
            description=response["description"],
            items=[
                NPListItem(
                    model_id=item["modelId"],
                    source=item["layer"],
                    index=item["index"],
                )
                for item in response["neurons"]
            ],
        )

================
File: neuronpedia/requests/sae_feature_request.py
================
import json
from neuronpedia.requests.base_request import (
    NPRequest,
)
from neuronpedia.np_sae_feature import SAEFeature
class SAEFeatureRequest(NPRequest):
    def __init__(
        self,
    ):
        super().__init__("feature")
    def get(self, model_id: str, source: str, index: str) -> SAEFeature:
        result = self.send_request(method="GET", uri=f"{model_id}/{source}/{index}")
        return SAEFeature(
            modelId=result["modelId"], source=result["layer"], index=result["index"], jsonData=json.dumps(result)
        )

================
File: neuronpedia/requests/steer_request.py
================
from requests import Response
from neuronpedia.requests.base_request import (
    NPRequest,
)
from neuronpedia.np_vector import NPVector
ChatMessage = dict[str, str]
class SteerChatRequest(NPRequest):
    def __init__(
        self,
    ):
        super().__init__("steer-chat")
    def steer(
        self,
        model_id: str,
        vectors: list[NPVector],
        default_chat_messages: list[ChatMessage] = [{"role": "user", "content": "Write a one sentence story."}],
        steered_chat_messages: list[ChatMessage] = [{"role": "user", "content": "Write a one sentence story."}],
        temperature: float = 0.5,
        n_tokens: int = 32,
        freq_penalty: float = 2,
        seed: int = 16,
        strength_multiplier: float = 4,
        steer_special_tokens: bool = True,
    ) -> Response:
        # convert the vectors to the feature format
        features = [
            {
                "modelId": vector.model_id,
                "layer": vector.source,
                "index": vector.index,
                "strength": vector.default_steer_strength,
            }
            for vector in vectors
        ]
        payload = {
            "modelId": model_id,
            "features": features,
            "defaultChatMessages": default_chat_messages,
            "steeredChatMessages": steered_chat_messages,
            "temperature": temperature,
            "n_tokens": n_tokens,
            "freq_penalty": freq_penalty,
            "seed": seed,
            "strength_multiplier": strength_multiplier,
            "steer_special_tokens": steer_special_tokens,
        }
        return self.send_request(method="POST", json=payload)
class SteerCompletionRequest(NPRequest):
    def __init__(self):
        super().__init__("steer")
    def steer(
        self,
        model_id: str,
        vectors: list[NPVector],
        prompt: str,
        temperature: float = 0.5,
        n_tokens: int = 32,
        freq_penalty: float = 2,
        seed: int = 42,
        strength_multiplier: float = 4,
    ) -> Response:
        # convert the vectors to the feature format
        features = [
            {
                "modelId": vector.model_id,
                "layer": vector.source,
                "index": vector.index,
                "strength": vector.default_steer_strength,
            }
            for vector in vectors
        ]
        payload = {
            "modelId": model_id,
            "features": features,
            "prompt": prompt,
            "temperature": temperature,
            "n_tokens": n_tokens,
            "freq_penalty": freq_penalty,
            "seed": seed,
            "strength_multiplier": strength_multiplier,
        }
        return self.send_request(method="POST", json=payload)

================
File: neuronpedia/requests/vector_request.py
================
from requests import Response
from neuronpedia.requests.base_request import (
    NPRequest,
)
from neuronpedia.np_vector import NPVector
VALID_HOOK_TYPES = ["hook_resid_pre"]
class VectorRequest(NPRequest):
    def __init__(
        self,
    ):
        super().__init__("vector")
    def new(
        self,
        label: str,
        model_id: str,
        layer_num: int,
        hook_type: str,
        vector: list[float],
        default_steer_strength: float | None = 10,
    ) -> NPVector:
        if hook_type not in VALID_HOOK_TYPES:
            raise ValueError(f"Invalid hook name: {hook_type}. Valid hook names are {VALID_HOOK_TYPES}.")
        payload = {
            "modelId": model_id,
            "layerNumber": layer_num,
            "hookType": hook_type,
            "vector": vector,
            "vectorDefaultSteerStrength": default_steer_strength,
            "vectorLabel": label,
        }
        response = self.send_request(
            method="POST",
            json=payload,
            uri="new",
        )
        return NPVector(
            model_id=response["vector"]["modelId"],
            source=response["vector"]["source"],
            index=response["vector"]["index"],
            label=response["vector"]["label"],
            hook_name=response["vector"]["hookName"],
            values=response["vector"]["values"],
            default_steer_strength=response["vector"]["defaultSteerStrength"],
            url=response["url"],
        )
    def delete(self, vector: NPVector) -> Response:
        return self._delete(vector.model_id, vector.source, vector.index)
    def _delete(self, modelId: str, source: str, index: str) -> Response:
        payload = {
            "modelId": modelId,
            "source": source,
            "index": index,
        }
        return self.send_request(
            method="POST",
            uri="delete",
            json=payload,
        )
    def get(self, model_id: str, source: str, index: str) -> NPVector:
        response = self.send_request(
            method="POST",
            uri="get",
            json={"modelId": model_id, "source": source, "index": index},
        )
        return NPVector(
            model_id=response["vector"]["modelId"],
            source=response["vector"]["layer"],
            index=response["vector"]["index"],
            label=response["vector"]["vectorLabel"],
            hook_name=response["vector"]["hookName"],
            values=response["vector"]["vector"],
            default_steer_strength=response["vector"]["vectorDefaultSteerStrength"],
        )
    def get_owned(self) -> list[NPVector]:
        response = self._get_owned()
        return [
            NPVector(
                model_id=vector["modelId"],
                source=vector["layer"],
                index=vector["index"],
                label=vector["vectorLabel"],
                hook_name=vector["hookName"],
                values=vector["vector"],
                default_steer_strength=vector["vectorDefaultSteerStrength"],
            )
            for vector in response["vectors"]
        ]
    def _get_owned(self) -> Response:
        return self.send_request(
            method="POST",
            uri="list-owned",
        )

================
File: neuronpedia/sample_data.py
================
GEMMA2_2B_IT_DINOSAURS_VECTOR = [
    -0.0052,
    -0.0064,
    -0.0112,
    -0.021,
    0.0401,
    -0.0016,
    0.0026,
    -0.0071,
    0.0384,
    0.0433,
    0.0056,
    0.002,
    -0.0145,
    -0.0107,
    0.0094,
    -0.001,
    -0.0145,
    -0.019,
    -0.0247,
    -0.0107,
    -0.0078,
    -0.0185,
    0.033,
    0.0102,
    0.0198,
    -0.025,
    0.024,
    0.0173,
    0.0159,
    -0.008,
    0.0056,
    0.0264,
    0.0045,
    -0.0143,
    -0.0212,
    0.0102,
    -0.0041,
    0.0258,
    -0.0291,
    0.0106,
    0.0046,
    0.0086,
    0.0263,
    -0.0157,
    0.0033,
    0.0077,
    0.0031,
    -0.0169,
    -0.0181,
    0.0075,
    -0.0227,
    0.0319,
    -0.0526,
    0.0081,
    0.0129,
    -0.0067,
    -0.0137,
    0.003,
    -0.0085,
    -0.0226,
    -0.0242,
    0.0399,
    0.036,
    0.0087,
    -0.0258,
    -0.0349,
    -0.0001,
    -0.034,
    -0.0498,
    0.0247,
    -0.021,
    -0.0098,
    0.0296,
    0.019,
    0.017,
    0.0161,
    0.0291,
    0.0232,
    -0.0065,
    -0.0202,
    -0.0259,
    -0.002,
    -0.0027,
    0.0167,
    0.001,
    0.0214,
    -0.038,
    -0.0368,
    -0.0049,
    0.0129,
    0.0024,
    -0.0028,
    -0.012,
    -0.0201,
    -0.0098,
    0.0046,
    -0.0133,
    0.035,
    -0.0144,
    0.0012,
    -0.0124,
    -0.0172,
    -0.0238,
    0.0314,
    -0.0025,
    -0.0022,
    0.0001,
    -0.0005,
    0.0156,
    0.001,
    0.0154,
    0.011,
    0.008,
    0.0214,
    0.0398,
    0.0069,
    -0.0154,
    -0.0009,
    0.0628,
    0.0315,
    0.0332,
    -0.0436,
    -0.0142,
    0.0111,
    0.0272,
    0.0082,
    0.0102,
    0.0005,
    0.0074,
    0.0032,
    -0.044,
    0.0137,
    -0.0264,
    -0.0057,
    0.0306,
    0.013,
    0.0261,
    0.0093,
    -0.0401,
    0.0023,
    0.0157,
    -0.0106,
    -0.0007,
    -0.04,
    0.0116,
    -0.002,
    0.0025,
    0.0053,
    -0.0303,
    -0.0159,
    0.0188,
    0.0149,
    0.0228,
    -0.0015,
    -0.0087,
    -0.0152,
    0.0078,
    -0.0196,
    -0.0279,
    0.0106,
    -0.01,
    -0.0242,
    -0.0063,
    -0.0182,
    -0.0244,
    0.0189,
    0.0119,
    0.0037,
    0.0198,
    0.0078,
    0.0022,
    0.0255,
    -0.008,
    -0.0007,
    0.0108,
    -0.0363,
    0.0114,
    -0.0044,
    -0.0012,
    0.0169,
    -0.0044,
    -0.0087,
    0.0522,
    0.0226,
    0.0034,
    0.0116,
    0.0075,
    -0.0072,
    0.0053,
    0.0424,
    0.0294,
    0.0173,
    -0.0064,
    -0.0008,
    -0.021,
    -0.008,
    0.0364,
    -0.0357,
    0.0216,
    -0.0183,
    0.0312,
    -0.0068,
    -0.0052,
    0.0198,
    0.0548,
    -0.0137,
    -0.0067,
    0.0318,
    0.0155,
    -0.022,
    0.0009,
    0.011,
    0.0055,
    0.0129,
    -0.0072,
    -0.0155,
    0.0106,
    -0.0083,
    -0.0001,
    0.0384,
    -0.034,
    -0.0117,
    0.0131,
    0.0252,
    0.0459,
    0.0173,
    -0.052,
    0.0147,
    -0.0163,
    -0.0301,
    -0.0084,
    0.0165,
    0.015,
    0.0258,
    0.0279,
    -0.0127,
    -0.0185,
    -0.0186,
    -0.0048,
    0.0158,
    -0.0062,
    0.0028,
    -0.0024,
    0.0037,
    0.0194,
    0.0176,
    -0.0029,
    0.0181,
    0.026,
    -0.0246,
    0.0155,
    0.0164,
    -0.0032,
    0.0082,
    -0.0069,
    0.0056,
    -0.0085,
    -0.0072,
    -0.0218,
    0.0238,
    0.003,
    0.0339,
    0.0117,
    0.0087,
    0.0149,
    0.0113,
    0.0092,
    0.0017,
    0.0042,
    -0.0011,
    -0.0028,
    -0.0222,
    0.0244,
    0.0099,
    -0.0023,
    0.0288,
    0.042,
    -0.0273,
    0.0183,
    0.021,
    -0.0361,
    0.003,
    -0.0213,
    0.0173,
    -0.0051,
    -0.0197,
    0.0015,
    -0.02,
    0.0166,
    0.0371,
    0.031,
    -0.0035,
    0.0078,
    0.0062,
    -0.0229,
    0.0024,
    0.0138,
    -0.0118,
    -0.0085,
    -0.0246,
    0.0139,
    -0.012,
    -0.0043,
    -0.0118,
    0.0017,
    0.0187,
    0.0074,
    0.0213,
    -0.0194,
    -0.0218,
    0.0015,
    0.0163,
    0.009,
    0.0029,
    0.009,
    0.0143,
    -0.0188,
    0.0037,
    -0.0206,
    -0.0402,
    0.0355,
    -0.022,
    -0.0088,
    0.0459,
    -0.0281,
    -0.0207,
    -0.02,
    -0.018,
    -0.0176,
    0.0251,
    0.0045,
    -0.0107,
    -0.0629,
    0.0394,
    -0.0013,
    -0.0036,
    -0.0176,
    0.0508,
    0.0194,
    0.0181,
    0.0261,
    0.0147,
    -0.0073,
    0.0006,
    0.0272,
    -0.0119,
    -0.0074,
    0.0066,
    -0.0296,
    -0.0092,
    0.0034,
    0.0118,
    -0.0278,
    0.0163,
    0.0091,
    0.01,
    0.0014,
    0.0121,
    0.0399,
    -0.0107,
    0.0021,
    0.0199,
    0.0006,
    -0.0265,
    -0.0088,
    -0.0201,
    -0.003,
    0.0001,
    -0.0266,
    0.0199,
    0.0019,
    0.0353,
    -0.016,
    -0.0539,
    0.0039,
    0.0245,
    0.0371,
    -0.0019,
    -0.0175,
    0.0007,
    0.0287,
    -0.026,
    -0.0118,
    0.0031,
    -0.0415,
    -0.0353,
    -0.0071,
    -0.034,
    -0.0305,
    -0.0113,
    0.0149,
    -0.0147,
    -0.001,
    0.0013,
    0.0218,
    -0.0179,
    0.0222,
    0.0071,
    0.0157,
    0.0405,
    -0.0123,
    0.0183,
    -0.0634,
    -0.0489,
    -0.0232,
    0.0058,
    0.0208,
    -0.017,
    0.0181,
    -0.013,
    0.0047,
    0.0057,
    -0.0142,
    0.0008,
    0.0121,
    -0.0039,
    0.0289,
    -0.028,
    0.0117,
    -0.0081,
    0.0377,
    0.0009,
    0.0029,
    -0.008,
    0.0043,
    -0.0238,
    -0.0172,
    -0.0225,
    0.0203,
    0.0365,
    0.0089,
    -0.0217,
    -0.0017,
    0.0019,
    0.0075,
    -0.0131,
    0.0101,
    0.0128,
    0.0404,
    -0.0045,
    -0.0087,
    0.0081,
    0.0389,
    0.0066,
    0.0151,
    -0.0104,
    -0.0186,
    0.0008,
    0.0151,
    0.0176,
    0.0084,
    0.0432,
    -0.0502,
    0.0166,
    0.0155,
    0.0038,
    0.012,
    -0.029,
    0.008,
    0.0133,
    0.0168,
    0.0394,
    0.0245,
    0.0229,
    0.0082,
    -0.0128,
    0.0076,
    -0.0412,
    -0.0112,
    0.0064,
    0.0062,
    -0.0068,
    0.0038,
    0.0217,
    0.0141,
    -0.0345,
    -0.0212,
    -0.0028,
    -0.01,
    0.0025,
    -0.023,
    -0.0025,
    -0.0413,
    0.0182,
    0.0565,
    0.0266,
    -0.0035,
    0.0096,
    -0.0047,
    0.0303,
    0.0214,
    0.03,
    0.042,
    0.0016,
    0.0147,
    -0.0075,
    0.0066,
    -0.0068,
    -0.0217,
    -0.0104,
    0.0356,
    -0.0089,
    -0.0076,
    -0.0066,
    -0.0023,
    0.0125,
    -0.0274,
    0.0217,
    -0.0044,
    -0.0257,
    -0.0256,
    -0.0123,
    -0.036,
    0.0299,
    0.0114,
    -0.0013,
    0.0264,
    -0.0172,
    0.0096,
    0.0057,
    0.0238,
    0.0197,
    -0.0132,
    0.0193,
    0.0029,
    0.012,
    0.0071,
    -0.0013,
    -0.0077,
    -0.0066,
    0.0067,
    0.0113,
    0.0461,
    -0.0086,
    -0.0083,
    0.0097,
    -0.0115,
    -0.0059,
    0.0296,
    -0.0058,
    -0.0166,
    -0.0171,
    0.0041,
    0.0012,
    -0.0079,
    -0.0105,
    0.0091,
    -0.0193,
    0.0125,
    -0.0064,
    -0.004,
    0.0322,
    -0.0004,
    -0.0256,
    -0.0242,
    0.0507,
    -0.0211,
    0.018,
    0.0073,
    -0.0059,
    0.0182,
    0.0097,
    0.0146,
    -0.0181,
    -0.0134,
    -0.0253,
    0.0025,
    -0.0021,
    -0.0201,
    0.0373,
    -0.0094,
    0.0146,
    -0.0149,
    0.0245,
    -0.0076,
    -0.0088,
    0.0022,
    -0.0043,
    0.0024,
    -0.0123,
    0.0411,
    0.0442,
    -0.0404,
    0.009,
    0.0115,
    -0.0105,
    -0.0176,
    0.0182,
    0.0177,
    -0.0039,
    -0.0082,
    0.008,
    0.0115,
    0.0092,
    0.0811,
    0.0441,
    -0.0623,
    -0.0531,
    0.0197,
    -0.0035,
    0.0099,
    0.011,
    -0.0075,
    0.0177,
    0.0292,
    0.0309,
    0.0399,
    -0.0166,
    -0.0012,
    0.0092,
    -0.0162,
    0.0072,
    0.0169,
    -0.0084,
    -0.0087,
    -0.0145,
    0.0062,
    0.0421,
    -0.0024,
    -0.0232,
    -0.0243,
    -0.0277,
    -0.003,
    0.0119,
    -0.0495,
    0.0038,
    0.0084,
    -0.0045,
    -0.0123,
    0.0099,
    0.0114,
    0.0163,
    -0.0234,
    -0.0117,
    0.0039,
    -0.0302,
    -0.0295,
    -0.0297,
    0.0282,
    0.0003,
    0.0533,
    0.0327,
    -0.0188,
    0.0073,
    0.0525,
    -0.0559,
    -0.0268,
    -0.0066,
    -0.0314,
    -0.0178,
    0.0019,
    -0.0146,
    0.0197,
    -0.0155,
    -0.0069,
    0.0075,
    0.0142,
    -0.0117,
    -0.0137,
    -0.0331,
    0.0223,
    0.0096,
    -0.0127,
    0.0013,
    0.0365,
    0.0024,
    -0.0292,
    0.0255,
    0.0111,
    0.0141,
    -0.0166,
    0.0019,
    -0.0057,
    -0.0058,
    0.0239,
    0.0027,
    -0.0225,
    -0.0033,
    0.0377,
    0.0085,
    0.0226,
    -0.0105,
    -0.013,
    0.0071,
    -0.0037,
    -0.0366,
    0.0031,
    -0.0017,
    0.0503,
    0.0118,
    0.0143,
    0.0172,
    -0.0092,
    -0.0015,
    -0.0069,
    0.0067,
    -0.0163,
    -0.031,
    0.0305,
    -0.0053,
    -0.0129,
    -0.0254,
    -0.0076,
    -0.047,
    -0.0018,
    -0.0034,
    0.0136,
    -0.0297,
    -0.0186,
    -0.0405,
    -0.0152,
    0.0158,
    -0.0418,
    -0.0209,
    -0.0043,
    0.0177,
    0.0292,
    0.0162,
    -0.0505,
    0.039,
    0.0042,
    0.0263,
    -0.0036,
    -0.0217,
    -0.0051,
    0.0013,
    -0.0069,
    0.0048,
    0.024,
    0.0093,
    0.0193,
    0.0214,
    0.0096,
    -0.0257,
    0.0046,
    -0.0265,
    0.0182,
    -0.0057,
    -0.0092,
    0.0207,
    -0.0073,
    0.0211,
    0.0012,
    -0.0046,
    -0.0015,
    -0.0045,
    0.0181,
    0.0146,
    0.0189,
    0.0214,
    -0.0099,
    0.0124,
    0.0052,
    -0.0637,
    0.0142,
    -0.0116,
    0.0264,
    0.0053,
    -0.0042,
    -0.0575,
    0.0265,
    -0.0334,
    0.025,
    0.0208,
    0.0168,
    0.0152,
    -0.0266,
    -0.0196,
    -0.0264,
    0.0197,
    0.0247,
    0.0063,
    -0.0217,
    0.0095,
    0.0152,
    -0.0294,
    0,
    0.0058,
    0.0436,
    0.0057,
    -0.0166,
    0.0087,
    -0.0264,
    0.0038,
    0.0106,
    0.0094,
    0.006,
    -0.0282,
    0.0004,
    -0.0336,
    0.0242,
    0.0534,
    -0.0001,
    0.0135,
    -0.0053,
    -0.0195,
    -0.0016,
    -0.0015,
    -0.0315,
    0.0178,
    0.0065,
    -0.0126,
    -0.0038,
    0.0123,
    0.0359,
    0.0077,
    -0.0253,
    0.0066,
    -0.014,
    0.0046,
    -0.0062,
    0.0177,
    -0.0169,
    0.0035,
    0.0101,
    0.0046,
    -0.0097,
    0.03,
    -0.008,
    -0.0207,
    -0.0157,
    -0.0299,
    0.0049,
    0.0142,
    0.003,
    0.0079,
    0.0239,
    -0.0013,
    -0.0004,
    -0.0219,
    0.0197,
    -0.0248,
    0.0269,
    0.0025,
    0.026,
    0.0069,
    -0.0195,
    -0.0209,
    0.0025,
    0.0015,
    0.0089,
    0.0273,
    -0.0115,
    0.0235,
    -0.0132,
    -0.0318,
    -0.0076,
    -0.0157,
    -0.044,
    -0.0198,
    0.0136,
    -0.0435,
    -0.0388,
    -0.0118,
    -0.009,
    -0.0167,
    0.0064,
    0.0173,
    0.0298,
    -0.0104,
    0.0045,
    0.0191,
    -0.0314,
    0.0368,
    -0.0038,
    -0.0229,
    0.018,
    0.0019,
    0.0308,
    -0.0024,
    -0.0183,
    -0.0186,
    -0.0225,
    -0.0101,
    0.0088,
    -0.0037,
    0.0485,
    0.0269,
    0.0156,
    -0.014,
    -0.0335,
    0.0326,
    -0.0023,
    -0.0159,
    0.0046,
    0.0142,
    -0.0077,
    0.0494,
    0.0231,
    0.0127,
    -0.0111,
    0.0111,
    -0.0266,
    0.0275,
    -0.0046,
    -0.024,
    -0.0042,
    0.0116,
    0.0234,
    0.0365,
    -0.031,
    0.0025,
    0.0257,
    0.0141,
    -0.0073,
    -0.0043,
    -0.0307,
    -0.0073,
    0.0042,
    -0.0064,
    -0.0064,
    0.0054,
    0.0048,
    0.0161,
    0.0173,
    0.0009,
    0.0116,
    -0.0051,
    0.0259,
    -0.0086,
    0.0228,
    -0.0661,
    -0.0297,
    -0.0151,
    0.0239,
    -0.0236,
    0.0245,
    0.0404,
    0.0263,
    -0.0123,
    0.0053,
    0.016,
    0.0254,
    -0.0086,
    0.0077,
    -0.0117,
    0.0015,
    -0.0146,
    0.0044,
    -0.0312,
    0.0418,
    -0.0268,
    0.0266,
    -0.0042,
    -0.0134,
    0.0121,
    0.0243,
    -0.0119,
    -0.0082,
    -0.0215,
    -0.0172,
    0.0035,
    -0.0066,
    0.0053,
    0.0061,
    -0.0102,
    0.0062,
    -0.0062,
    0,
    -0.0262,
    0.0067,
    0.0081,
    0.0068,
    -0.0008,
    0.0142,
    -0.0008,
    0.0244,
    0.0019,
    0.0278,
    0.0006,
    0.0003,
    0.0057,
    0.0115,
    0.0076,
    -0.0028,
    0.0291,
    0.0077,
    0.0071,
    0,
    -0.0134,
    0.0084,
    0.0279,
    0.008,
    -0.0154,
    0.022,
    0.0257,
    0.0107,
    -0.0023,
    0.0104,
    0.02,
    0.0024,
    -0.0011,
    -0.0229,
    0.0015,
    0.0041,
    -0.0169,
    -0.0213,
    0.0007,
    -0.0089,
    -0.0152,
    0.0267,
    -0.0198,
    -0.0125,
    0.0208,
    0.0288,
    -0.009,
    -0.0039,
    0.0234,
    -0.0259,
    0.0096,
    0.0038,
    0.0097,
    0.014,
    -0.0328,
    0.0312,
    -0.0104,
    -0.0058,
    0.0047,
    0.0005,
    0.0213,
    -0.0023,
    -0.0241,
    -0.0223,
    -0.0372,
    -0.0369,
    -0.0076,
    -0.0121,
    0.0088,
    -0.0057,
    0.0164,
    0.0007,
    0.0159,
    0.0001,
    0.0072,
    0.0318,
    -0.0135,
    0.0095,
    -0.013,
    0.0277,
    0.0162,
    -0.0277,
    0.0133,
    0.0089,
    -0.0055,
    0.0176,
    0.0351,
    0.0097,
    0.0082,
    -0.0065,
    0.0333,
    -0.0076,
    -0.0001,
    0.0184,
    -0.0237,
    0.0352,
    0.0298,
    0.018,
    -0.0205,
    0.0361,
    -0.0011,
    0.03,
    -0.0046,
    0.0218,
    0.0019,
    0.0032,
    0.0258,
    0.0348,
    -0.0132,
    -0.0107,
    -0.0097,
    -0.0005,
    -0.0097,
    -0.0001,
    -0.0143,
    -0.0045,
    0.0008,
    -0.0046,
    -0.0144,
    0.0032,
    0.0005,
    -0.0234,
    0.0219,
    0.0028,
    0.0197,
    -0.0186,
    0.0457,
    -0.0004,
    -0.0276,
    0.0093,
    -0.0152,
    -0.0183,
    0.0306,
    -0.0102,
    -0.002,
    0,
    -0.0026,
    -0.0458,
    -0.0202,
    -0.0215,
    -0.0163,
    0.0058,
    -0.0486,
    0.0082,
    -0.0261,
    -0.0038,
    0.0126,
    -0.0092,
    -0.0219,
    0.0079,
    -0.0421,
    0.021,
    -0.0126,
    -0.0048,
    0.0206,
    -0.0216,
    0.0099,
    -0.0014,
    -0.01,
    0.0189,
    -0.0154,
    0.0185,
    -0.0213,
    -0.0076,
    0.0526,
    0.0346,
    -0.0013,
    -0.0014,
    0.0046,
    0.0128,
    -0.0075,
    -0.0142,
    0.0089,
    -0.0154,
    -0.0374,
    0.0058,
    0.0213,
    -0.0246,
    0.0389,
    -0.0266,
    -0.0055,
    0.0182,
    -0.0221,
    0.0116,
    -0.0108,
    0.0065,
    0.0262,
    -0.0245,
    -0.0105,
    0.0076,
    0.0138,
    0.0016,
    -0.0191,
    0.0289,
    -0.0118,
    -0.0249,
    0.0009,
    -0.0212,
    0.0004,
    0.0179,
    0.0001,
    0.0139,
    0.004,
    -0.0045,
    0.0093,
    -0.0269,
    0.0115,
    -0.0076,
    -0.0093,
    -0.0143,
    -0.0124,
    0.0045,
    0.0062,
    0.0268,
    0.0177,
    -0.0217,
    -0.0308,
    0.0253,
    -0.0605,
    0.0355,
    -0.0105,
    -0.0169,
    0.0143,
    -0.0004,
    -0.0015,
    -0.0149,
    -0.0156,
    0.0097,
    0.0195,
    0.0087,
    -0.0043,
    -0.0001,
    0.0124,
    0.0045,
    0.0022,
    0.0126,
    -0.0285,
    -0.0203,
    -0.0202,
    -0.0238,
    -0.0241,
    -0.0071,
    0.023,
    -0.0152,
    -0.0297,
    0.0071,
    0.0068,
    -0.0083,
    0.0093,
    0.0248,
    -0.0273,
    0.0173,
    0.0053,
    -0.0256,
    -0.017,
    0.0441,
    -0.0069,
    0.011,
    0.0001,
    0.0214,
    -0.0082,
    0.0115,
    0.0118,
    -0.0041,
    0.0066,
    0.0155,
    0.0014,
    -0.0386,
    0.0038,
    -0.0165,
    0.0378,
    0.0008,
    0.0179,
    0.0484,
    0.033,
    -0.0027,
    -0.0065,
    -0.0203,
    0.0024,
    0.0387,
    0.0272,
    -0.0096,
    -0.0002,
    -0.0082,
    -0.0084,
    0.0171,
    -0.0082,
    0.0052,
    -0.0394,
    -0.0195,
    -0.0005,
    0.0091,
    0.0165,
    0.0239,
    0.002,
    0.0466,
    0.0086,
    0.0016,
    0.0219,
    0.0117,
    0.012,
    0.0499,
    -0.0013,
    0.0051,
    0.0113,
    -0.0112,
    0.0081,
    -0.0087,
    -0.0075,
    -0.0326,
    0.0092,
    -0.0006,
    0.0167,
    0.0158,
    0.0102,
    0.028,
    -0.0135,
    -0.0102,
    -0.0302,
    -0.0299,
    -0.0103,
    -0.0398,
    0.0116,
    0.0147,
    -0.0067,
    -0.0226,
    0.0243,
    0.013,
    0.0041,
    0.0024,
    0.0113,
    0.053,
    -0.0084,
    -0.0299,
    0.0149,
    0.0471,
    0.0012,
    0.0142,
    0.0089,
    0.0096,
    0.0257,
    -0.0002,
    -0.0046,
    -0.0067,
    0.0142,
    0.0054,
    -0.0234,
    0.0263,
    -0.0033,
    -0.0402,
    -0.02,
    -0.008,
    0.0327,
    -0.0061,
    0.0094,
    0.0207,
    0.039,
    -0.0036,
    -0.0096,
    0.013,
    0.0027,
    0.0159,
    -0.0179,
    -0.0346,
    -0.0082,
    0.0017,
    -0.0198,
    -0.0149,
    -0.0084,
    -0.0041,
    0.0318,
    0.0365,
    -0.004,
    0.009,
    0.0227,
    -0.0325,
    -0.0052,
    0.0394,
    -0.0021,
    -0.0184,
    0.0086,
    0.0225,
    0.0241,
    -0.0115,
    -0.0201,
    -0.0128,
    0.0309,
    -0.006,
    0.0225,
    0.0063,
    -0.0294,
    -0.0493,
    0.0092,
    0.0082,
    -0.0057,
    0.042,
    0.01,
    -0.0108,
    -0.014,
    0.0507,
    0.0135,
    -0.0224,
    -0.0406,
    0.0086,
    -0.0029,
    0.0221,
    0.0064,
    -0.0015,
    -0.0017,
    -0.0096,
    0.0142,
    -0.0069,
    -0.0158,
    -0.006,
    -0.0283,
    -0.0036,
    0.0085,
    -0.0209,
    -0.0364,
    0.0559,
    0.0191,
    0.028,
    0.02,
    -0.0048,
    0.0428,
    -0.0185,
    0.0236,
    -0.0189,
    0.021,
    0.0172,
    0.0034,
    0.0013,
    0.0104,
    -0.0056,
    -0.004,
    -0.0072,
    -0.0201,
    -0.0026,
    0.0402,
    0.0194,
    -0.001,
    -0.0082,
    0.0108,
    0.0062,
    0.0158,
    0.0055,
    0.0026,
    0.0476,
    0.0222,
    -0.0291,
    0.0035,
    0.0534,
    -0.0021,
    -0.0031,
    0.032,
    0.0387,
    0.0048,
    -0.0204,
    -0.0002,
    0.0117,
    -0.0007,
    -0.012,
    -0.0472,
    0.004,
    0.0283,
    -0.0023,
    0.0156,
    0.0239,
    -0.0094,
    -0.0273,
    0.0008,
    -0.0146,
    0.0035,
    0.0127,
    0.0005,
    -0.0111,
    0.0365,
    0.0028,
    -0.034,
    0.0217,
    -0.0157,
    -0.008,
    0.0177,
    0.0336,
    -0.0267,
    0.0025,
    0.0048,
    0.0236,
    0.0316,
    -0.0018,
    0.0021,
    0.003,
    -0.0138,
    0.0131,
    -0.0065,
    0.0133,
    0.0128,
    -0.0197,
    0.0031,
    -0.0213,
    0.0078,
    0.0059,
    0.0076,
    -0.0088,
    0.0298,
    0.0089,
    0.0051,
    -0.0275,
    -0.0349,
    0.0606,
    0.008,
    -0.0092,
    -0.0004,
    -0.011,
    0.0151,
    0.0129,
    0.0052,
    0.016,
    0.0215,
    0.007,
    0.0086,
    0.0385,
    0.0004,
    0.0256,
    0.0505,
    -0.0057,
    0.0107,
    -0.0103,
    -0.0381,
    -0.0229,
    0.0015,
    0.0209,
    -0.0051,
    0.0171,
    -0.016,
    0.0024,
    0.0042,
    -0.0157,
    0.0292,
    0.0223,
    0.0077,
    0.0129,
    -0.0186,
    0.0149,
    0.0174,
    -0.0212,
    0.0234,
    0.0183,
    -0.0296,
    0.0215,
    -0.0179,
    -0.0407,
    0.0005,
    -0.0021,
    -0.0134,
    -0.0248,
    -0.0004,
    0.0299,
    0.0048,
    -0.0247,
    0.0346,
    0.0045,
    0.0365,
    -0.0358,
    -0.0427,
    0.0293,
    0.0175,
    -0.0253,
    0.0219,
    -0.0223,
    -0.0015,
    -0.0194,
    -0.0062,
    -0.0325,
    0.0078,
    -0.0217,
    0.0174,
    0.0018,
    0.0071,
    0.0156,
    0.0143,
    -0.0043,
    0.0275,
    0.0252,
    -0.0083,
    0.0191,
    -0.0049,
    0.0021,
    0.0003,
    -0.0017,
    -0.0288,
    0,
    0.0087,
    0.0037,
    0.0019,
    0.0152,
    0.0351,
    -0.0349,
    -0.0045,
    0.0063,
    -0.0279,
    -0.0057,
    -0.0287,
    -0.0083,
    0.0299,
    0.0099,
    -0.0043,
    0.0012,
    -0.0033,
    -0.0132,
    -0.0063,
    0.0173,
    0.0358,
    -0.0052,
    -0.0057,
    0.0342,
    -0.0155,
    0.0067,
    0.0071,
    0.0566,
    0.0155,
    -0.0027,
    -0.0239,
    0.0225,
    -0.0179,
    -0.0186,
    0.0187,
    0.0154,
    -0.0024,
    -0.019,
    -0.0335,
    -0.0153,
    -0.017,
    0.0111,
    -0.0125,
    -0.0193,
    0.0198,
    0.0037,
    0.0019,
    0.002,
    -0.0271,
    -0.0178,
    -0.0085,
    0.0367,
    -0.0131,
    -0.0155,
    0.0109,
    -0.0195,
    0.0243,
    -0.0233,
    0.0226,
    -0.0115,
    0.0136,
    -0.0078,
    0.002,
    0.0513,
    -0.0035,
    -0.0131,
    -0.0412,
    0.0086,
    -0.009,
    0.0158,
    0.0187,
    0.0097,
    -0.0103,
    0.0272,
    -0.0182,
    0.0162,
    0.0135,
    0.0236,
    0.0251,
    -0.0013,
    -0.0476,
    0.0205,
    -0.0258,
    0.0263,
    0.0224,
    0.0188,
    0.0019,
    0.0001,
    -0.0069,
    0.0473,
    0.0339,
    0.0006,
    0.0556,
    0.0307,
    -0.0027,
    0.0015,
    0.028,
    0.0001,
    0.018,
    -0.0213,
    -0.0022,
    -0.0032,
    -0.0226,
    0.0224,
    -0.0139,
    0.0177,
    0.0113,
    0.0095,
    0.0184,
    -0.0114,
    0.0235,
    0.0038,
    0.0092,
    0.0455,
    0.0327,
    -0.0357,
    -0.0058,
    -0.0222,
    -0.0119,
    -0.016,
    0.019,
    0.0046,
    -0.0253,
    0.0157,
    -0.0103,
    0.0191,
    -0.0038,
    0.0013,
    -0.0277,
    -0.0012,
    0.0297,
    -0.0164,
    -0.047,
    0.0115,
    -0.0058,
    0.0059,
    0.0139,
    -0.009,
    -0.0385,
    -0.0257,
    -0.0054,
    0,
    -0.0383,
    0,
    -0.0045,
    -0.0452,
    0.0029,
    0.0092,
    -0.0066,
    0.0128,
    -0.0026,
    0.0074,
    -0.0045,
    0.004,
    0.011,
    -0.0184,
    -0.0057,
    -0.0022,
    -0.0047,
    -0.008,
    0.0241,
    -0.0119,
    -0.0288,
    -0.021,
    0.0011,
    0.0272,
    -0.0016,
    -0.0379,
    -0.0247,
    -0.0194,
    -0.0284,
    0.0046,
    0.0198,
    0.0055,
    0.0151,
    0.01,
    0.0007,
    0.0014,
    0.0058,
    0.0083,
    0.0176,
    -0.0149,
    -0.0118,
    0.0355,
    -0.0122,
    0.0248,
    -0.0077,
    -0.0278,
    0.0182,
    -0.0092,
    -0.0098,
    -0.0559,
    0.0106,
    0.0122,
    -0.0256,
    -0.0171,
    0.0051,
    0.0424,
    -0.0175,
    -0.0376,
    -0.0023,
    0.0233,
    0.027,
    -0.0592,
    -0.0023,
    -0.0318,
    0.0355,
    -0.0219,
    0.0364,
    -0.0388,
    -0.0055,
    -0.0349,
    0.0118,
    -0.0305,
    -0.0102,
    -0.0059,
    0.0509,
    -0.0029,
    0.0107,
    0.028,
    0.0379,
    0.0154,
    0.0153,
    0.0278,
    -0.0347,
    -0.0024,
    0.022,
    -0.0087,
    0.0379,
    -0.0041,
    0.0264,
    0.0171,
    -0.0427,
    0.034,
    -0.0151,
    -0.0135,
    0.0329,
    0.0007,
    -0.013,
    0.0019,
    -0.0069,
    0.027,
    -0.0001,
    0.0102,
    0.0317,
    0.0271,
    0.025,
    0.0065,
    -0.0307,
    -0.0009,
    -0.0205,
    0.0037,
    0.0429,
    0.0007,
    0.0037,
    -0.0342,
    -0.0284,
    -0.0062,
    0.0073,
    0.0238,
    -0.0055,
    -0.0262,
    -0.0083,
    0.0035,
    0.0088,
    0.0035,
    -0.0198,
    -0.018,
    -0.0028,
    -0.0192,
    -0.0239,
    -0.0074,
    0.0039,
    -0.0222,
    0.0165,
    -0.0192,
    0.0185,
    0.0217,
    -0.0048,
    0.0026,
    -0.0152,
    0.013,
    -0.0294,
    0.0199,
    0.0107,
    -0.025,
    0.0331,
    0.0196,
    0.0222,
    -0.02,
    -0.0078,
    -0.0194,
    -0.0041,
    0.001,
    -0.0066,
    -0.0086,
    -0.0006,
    0.0318,
    -0.0191,
    0.0098,
    0.0177,
    0.0235,
    -0.0096,
    0.0163,
    -0.0198,
    0.0083,
    -0.0408,
    0.0188,
    -0.003,
    0.0471,
    -0.0003,
    -0.0307,
    0.0547,
    -0.0244,
    -0.0238,
    -0.0202,
    0.0253,
    0.0322,
    -0.0353,
    0.0396,
    0.0375,
    -0.006,
    -0.0022,
    0.0047,
    -0.011,
    -0.0453,
    -0.0059,
    0.0448,
    0.0093,
    -0.003,
    0.0187,
    0.0006,
    -0.0021,
    -0.0046,
    0.0007,
    0.01,
    0.0518,
    -0.0112,
    -0.0007,
    -0.0201,
    -0.0022,
    -0.037,
    -0.0277,
    -0.0268,
    0.0161,
    -0.0009,
    0.0033,
    -0.0165,
    0.0114,
    0.0545,
    -0.006,
    -0.0024,
    0.0017,
    0.0085,
    0.0084,
    -0.0056,
    -0.012,
    -0.0125,
    -0.0029,
    -0.0248,
    -0.0152,
    0.0385,
    -0.0055,
    -0.0136,
    0.0241,
    0.0075,
    0.008,
    0.0091,
    0.0052,
    -0.0095,
    0.0164,
    0.0137,
    0.0202,
    0.0141,
    -0.0131,
    -0.0076,
    0.0325,
    -0.0001,
    -0.0079,
    0.0292,
    -0.0075,
    0.0101,
    0.0034,
    -0.0138,
    -0.0067,
    0.014,
    -0.006,
    0.0004,
    0.0215,
    0.0063,
    0.0229,
    -0.0308,
    -0.0218,
    -0.0289,
    0.0132,
    0.0144,
    0.0143,
    -0.0136,
    -0.0217,
    -0.0003,
    -0.0042,
    -0.0449,
    -0.0155,
    0.0101,
    -0.0301,
    0.0157,
    0.0197,
    0.0275,
    -0.0063,
    0.014,
    -0.0194,
    -0.0019,
    -0.0344,
    0.0112,
    0.0217,
    0.0059,
    0.034,
    -0.0036,
    0.0153,
    -0.0267,
    -0.0355,
    0.0174,
    -0.0091,
    0.0085,
    0.0389,
    -0.0296,
    0.0113,
    -0.0001,
    0.0203,
    0.0295,
    -0.0103,
    0.0012,
    -0.0193,
    -0.0042,
    0.0305,
    0.0262,
    0.0028,
    0.0092,
    -0.0277,
    -0.011,
    0.0156,
    0.0314,
    -0.009,
    -0.0306,
    -0.0127,
    0.0133,
    -0.029,
    -0.0209,
    0,
    0.012,
    0.0006,
    -0.0052,
    -0.0335,
    -0.0109,
    -0.0039,
    0.0403,
    -0.0118,
    0.0428,
    0.0102,
    0.01,
    0.0188,
    0.0192,
    -0.0088,
    0.0357,
    0.0219,
    -0.0349,
    0.0042,
    -0.006,
    -0.0142,
    -0.0369,
    -0.0218,
    0.023,
    0.0156,
    -0.0233,
    0.0471,
    0.0222,
    0.0063,
    -0.0136,
    0.0019,
    -0.026,
    -0.0159,
    -0.0144,
    -0.0089,
    0.0112,
    0.0243,
    -0.0302,
    0.018,
    0.0384,
    0.01,
    -0.0379,
    -0.0517,
    -0.0055,
    -0.0191,
    0.022,
    -0.0279,
    -0.0235,
    0,
    -0.04,
    0.0266,
    -0.0005,
    -0.0436,
    0.0084,
    -0.0183,
    -0.0177,
    0.0038,
    0.0245,
    -0.0319,
    0.0004,
    0.0214,
    0.0087,
    0.0026,
    -0.0442,
    0.0352,
    -0.0145,
    -0.0374,
    -0.0288,
    -0.0171,
    0.0078,
    0.0049,
    0.0246,
    -0.0266,
    -0.0054,
    -0.0166,
    -0.0317,
    -0.0167,
    -0.0068,
    0.0325,
    0.03,
    0.0053,
    -0.0177,
    -0.0056,
    -0.0241,
    -0.0089,
    -0.0043,
    -0.0177,
    -0.0112,
    -0.0317,
    0.0396,
    -0.0127,
    -0.0394,
    0.0047,
    -0.0229,
    0.0125,
    -0.0019,
    0.0187,
    0.0257,
    0.0212,
    0.0598,
    -0.0087,
    0.0236,
    -0.0071,
    -0.003,
    0.0113,
    0.0135,
    0.0078,
    -0.0211,
    -0.0013,
    -0.0023,
    0.0226,
    0.0046,
    -0.0359,
    0.0286,
    0.0162,
    -0.0098,
    -0.0282,
    0.0207,
    0.0235,
    -0.0203,
    0.0205,
    -0.0022,
    0.0164,
    -0.0132,
    -0.0156,
    0.0021,
    -0.008,
    -0.0222,
    -0.0095,
    0.0173,
    -0.0034,
    0.0015,
    -0.0178,
    -0.0238,
    -0.0064,
    -0.0086,
    -0.0103,
    -0.0128,
    0.0161,
    -0.0049,
    0.0323,
    -0.0174,
    0.0036,
    -0.0201,
    -0.0354,
    0.0355,
    0.0295,
    -0.0211,
    0.0053,
    -0.0179,
    -0.0392,
    -0.023,
    -0.0541,
    0.0225,
    0.0071,
    0.0145,
    -0.0279,
    0.0145,
    0.0016,
    -0.019,
    -0.0187,
    0.016,
    0.0278,
    -0.0223,
    0.0069,
    0.0279,
    0.0164,
    0.0397,
    -0.0459,
    0.0132,
    -0.0187,
    -0.0026,
    -0.0166,
    0.0349,
    0.0115,
    0.0289,
    -0.0093,
    -0.0044,
    -0.0303,
    0.0018,
    0.0188,
    0.0131,
    -0.028,
    0.0102,
    -0.0246,
    0.0433,
    0.0046,
    -0.014,
    0.0136,
    0.0126,
    -0.0131,
    -0.0039,
    -0.0023,
    0.0515,
    -0.0183,
    -0.0079,
    0.0026,
    -0.0016,
    -0.0321,
    0.0143,
    0.0136,
    -0.0109,
    -0.002,
    -0.0106,
    0.0317,
    -0.0048,
    0.0071,
    0.0174,
    0.0248,
    0.01,
    0.0141,
    -0.0027,
    -0.0118,
    0.0251,
    -0.0059,
    -0.0329,
    0.014,
    0.0139,
    0.0065,
    -0.0094,
    0.0219,
    -0.0248,
    0.0159,
    0.0081,
    -0.0421,
    0.0574,
    0.024,
    0.0199,
    -0.0129,
    -0.0067,
    0.0034,
    -0.022,
    0.0106,
    -0.0085,
    -0.0119,
    0.019,
    0.0144,
    0.0232,
    0.0153,
    0.0225,
    0.0276,
    0.0288,
    -0.0151,
    0.0123,
    0.0167,
    -0.0095,
    -0.0331,
    0.0219,
    0.0036,
    0.0063,
    0.0395,
    0.0566,
    -0.0267,
    0.0085,
    0.0039,
    -0.0328,
    0.0041,
    -0.0182,
    0.0084,
    -0.0096,
    0.0351,
    0.0201,
    0,
    -0.0203,
    -0.0078,
    0.0038,
    -0.0096,
    0.0242,
    0.0305,
    -0.0272,
    -0.004,
    -0.0136,
    0.0321,
    0.0144,
    -0.0507,
    -0.0309,
    0.0265,
    -0.0116,
    0.0011,
    0.014,
    -0.0037,
    -0.0055,
    0.0102,
    -0.0205,
    0.0072,
    -0.0088,
    0.0079,
    -0.01,
    0.0061,
    0.0261,
    -0.0013,
    -0.0021,
    -0.0244,
    -0.0531,
    -0.0086,
    -0.0159,
    0.0088,
    -0.0083,
    0.0229,
    0.0049,
    0.0003,
    0.0177,
    0.0086,
    0.0172,
    -0.0015,
]
GPT2_SMALL_RES_JB_JEDI_VECTOR = [
    -0.012915895320475101,
    0.011403760872781277,
    -0.10492245852947235,
    -0.01497142668813467,
    0.021994445472955704,
    -0.013033101335167885,
    0.05632844939827919,
    -0.055134356021881104,
    0.10170524567365646,
    0.04698412865400314,
    -0.03286226466298103,
    -0.013610946014523506,
    0.03238048404455185,
    -0.06944364309310913,
    -0.008210182189941406,
    0.021862251684069633,
    0.02946983464062214,
    -0.003314903937280178,
    -0.02653784491121769,
    0.05724040046334267,
    -0.008645097725093365,
    -0.025678543373942375,
    -2.0576722818077542e-05,
    -0.01728714630007744,
    0.02973332814872265,
    0.15288083255290985,
    -0.08643782138824463,
    -0.026074953377246857,
    0.004251175560057163,
    0.066792331635952,
    -0.11305764317512512,
    0.018245156854391098,
    0.06480985134840012,
    -0.006107497960329056,
    -0.053002845495939255,
    0.17444708943367004,
    -0.0028229665476828814,
    0.13234417140483856,
    -0.060213133692741394,
    0.01713014766573906,
    -0.0303414985537529,
    -0.02963494509458542,
    0.06938713788986206,
    0.01596681773662567,
    0.029082205146551132,
    0.01636333577334881,
    -0.12606453895568848,
    0.01775827445089817,
    0.07665145397186279,
    0.041978683322668076,
    -0.03941856697201729,
    0.08792361617088318,
    -0.08837465941905975,
    0.01834201067686081,
    0.004767143167555332,
    0.0031328368932008743,
    -0.13664859533309937,
    0.008158545941114426,
    0.023783711716532707,
    0.10397462546825409,
    -0.13533388078212738,
    0.11246072500944138,
    0.042430706322193146,
    -0.012424282729625702,
    0.003455136204138398,
    -0.046338532119989395,
    -0.12830039858818054,
    -0.06268614530563354,
    -0.022118747234344482,
    0.002730321604758501,
    0.059012558311223984,
    0.10762395709753036,
    0.041718848049640656,
    0.015046272426843643,
    -0.06097480282187462,
    0.06123334914445877,
    0.006972817238420248,
    0.023517688736319542,
    0.005996201653033495,
    -0.09988772869110107,
    0.028318284079432487,
    -0.018744533881545067,
    -0.0019959742203354836,
    0.007952840998768806,
    0.13779781758785248,
    0.02088257111608982,
    -0.018166838213801384,
    -0.0019680149853229523,
    0.017015261575579643,
    -0.0682772770524025,
    -0.014635824598371983,
    -0.007190420292317867,
    0.05003580078482628,
    -0.05721655115485191,
    -0.030748549848794937,
    -0.02955184504389763,
    0.0299200639128685,
    -0.09466999769210815,
    0.04487614333629608,
    0.023677775636315346,
    0.028903625905513763,
    0.11189864575862885,
    0.009109999053180218,
    -0.018640266731381416,
    0.09166257083415985,
    0.09656581282615662,
    0.07967039197683334,
    -0.011224751360714436,
    0.04277060553431511,
    0.08121945708990097,
    -0.029350213706493378,
    0.0400337353348732,
    -0.04110985994338989,
    -0.09828604757785797,
    0.01999860443174839,
    -0.062056656926870346,
    -0.021556684747338295,
    -0.045327503234148026,
    -0.07866837084293365,
    0.11490224301815033,
    0.033822886645793915,
    -0.13797114789485931,
    0.18933406472206116,
    0.029407508671283722,
    -0.10821052640676498,
    -0.033333197236061096,
    -0.028948500752449036,
    0.00021225678210612386,
    0.06902395188808441,
    -0.1131182461977005,
    -0.0408337339758873,
    -0.057132069021463394,
    -0.004620124585926533,
    0.029339587315917015,
    -0.03725336864590645,
    0.014107885770499706,
    -0.0229662973433733,
    -0.04698660224676132,
    -0.026431741192936897,
    -0.02059979923069477,
    0.04640926420688629,
    -0.08413060754537582,
    -0.048470888286828995,
    0.008765841834247112,
    0.04822830483317375,
    -0.11379723250865936,
    -0.10269832611083984,
    -0.08145927637815475,
    0.009213523007929325,
    -0.07796420156955719,
    -0.02617083117365837,
    0.06672445684671402,
    -0.038297295570373535,
    0.061743274331092834,
    0.0538036972284317,
    0.024515287950634956,
    -0.07467503100633621,
    0.080886110663414,
    0.018331289291381836,
    0.058489903807640076,
    0.06694665551185608,
    0.04444298893213272,
    -0.180792897939682,
    0.04985407367348671,
    -0.019230296835303307,
    -0.00046459928853437304,
    0.0452861525118351,
    -0.04209026321768761,
    0.03008255735039711,
    -0.08476433902978897,
    -0.03479486331343651,
    0.04360863193869591,
    0.10983268171548843,
    -0.04460304230451584,
    0.008991528302431107,
    -0.02787928469479084,
    0.02126609906554222,
    0.05419079586863518,
    0.04490216076374054,
    -0.03128235787153244,
    0.08277194947004318,
    0.0019537126645445824,
    0.05840820446610451,
    0.12526388466358185,
    -0.08629254996776581,
    0.15594492852687836,
    0.032410066574811935,
    0.03474947437644005,
    -0.09211856871843338,
    0.05958942323923111,
    -0.03706245496869087,
    0.10237804055213928,
    -0.0537971630692482,
    -0.09319375455379486,
    -0.04130402207374573,
    -0.04191187024116516,
    -0.11439400166273117,
    -0.03585941344499588,
    0.019719572737812996,
    -0.02645862102508545,
    -0.032218243926763535,
    0.030278652906417847,
    -0.0878659263253212,
    -0.08377736806869507,
    -0.17210274934768677,
    -0.004637696780264378,
    -0.07912583649158478,
    -0.0619407519698143,
    0.08414115011692047,
    -0.01584763266146183,
    -0.05355781689286232,
    -0.05558032914996147,
    0.07410790026187897,
    0.042558301240205765,
    0.05197093263268471,
    0.024203604087233543,
    0.09704188257455826,
    0.038505855947732925,
    0.0016703923465684056,
    0.08225593715906143,
    0.08294890820980072,
    0.11625244468450546,
    0.09185932576656342,
    0.04155569523572922,
    0.11526930332183838,
    -0.04771162196993828,
    0.037896573543548584,
    -0.006484464276582003,
    0.041421279311180115,
    0.019296931102871895,
    0.06060358136892319,
    -0.10505496710538864,
    0.06713960319757462,
    -0.06950634717941284,
    -0.04868226498365402,
    -0.010317033156752586,
    -0.11017078906297684,
    -0.16977965831756592,
    0.14338451623916626,
    -0.02887980081140995,
    0.01654207333922386,
    0.026612279936671257,
    0.0007666578749194741,
    0.012705324217677116,
    -0.018977085128426552,
    0.06750525534152985,
    -0.09268132597208023,
    0.1049579530954361,
    0.021402433514595032,
    -0.053506165742874146,
    0.01265198178589344,
    0.03710414469242096,
    -0.02096107043325901,
    -0.050518665462732315,
    -0.03826410323381424,
    0.16758468747138977,
    0.08740667998790741,
    -0.0656118243932724,
    0.033170800656080246,
    0.10285067558288574,
    0.00253111170604825,
    -0.0855899453163147,
    0.03960476070642471,
    0.014373700134456158,
    -0.05039287731051445,
    0.05977731943130493,
    0.024947794154286385,
    0.026188908144831657,
    0.012246834114193916,
    0.04405686631798744,
    -7.04218473401852e-05,
    0.04743938148021698,
    0.08562589436769485,
    0.017628392204642296,
    0.12599004805088043,
    -0.007112068589776754,
    -0.016390224918723106,
    0.003837141441181302,
    0.03159286454319954,
    0.08167105168104172,
    0.018603036180138588,
    0.08347637206315994,
    0.021934084594249725,
    -0.016073687002062798,
    0.05766867473721504,
    0.05385206267237663,
    0.11090557277202606,
    -0.025798723101615906,
    -0.10512727499008179,
    -0.01830488257110119,
    -0.07347409427165985,
    -0.009644607082009315,
    0.055658821016550064,
    -0.03663695976138115,
    -0.11469416320323944,
    0.13957297801971436,
    -0.06525330990552902,
    0.024175260215997696,
    0.0008309278637170792,
    -0.08329255133867264,
    -0.019235627725720406,
    -0.0019472424173727632,
    0.038994189351797104,
    0.031071841716766357,
    -0.09958741813898087,
    0.01937221549451351,
    0.05421693995594978,
    -0.02204241044819355,
    -0.015216423198580742,
    0.030567381531000137,
    -0.03586345911026001,
    -0.05463491007685661,
    0.09426725655794144,
    0.09934817254543304,
    -0.0012900258880108595,
    0.003835205687209964,
    0.13588279485702515,
    -0.07937373220920563,
    0.05139879137277603,
    -0.06787700206041336,
    0.09148730337619781,
    0.017933238297700882,
    -0.06077488884329796,
    -0.02054606005549431,
    0.10513787716627121,
    0.20792515575885773,
    0.07958941906690598,
    -0.03171663358807564,
    -0.08117400109767914,
    0.0978526696562767,
    -0.018314199522137642,
    -0.007140830624848604,
    0.10476487129926682,
    -0.17057697474956512,
    -0.018953299149870872,
    0.023174069821834564,
    0.0761512741446495,
    -0.0012000901624560356,
    -0.06017241254448891,
    -0.0386979803442955,
    0.03292059525847435,
    -0.05344139412045479,
    -0.14029547572135925,
    0.03616335242986679,
    0.07858037203550339,
    0.039658352732658386,
    0.04739327356219292,
    -0.11943082511425018,
    -0.0653473511338234,
    0.00047537818318232894,
    0.08980785310268402,
    0.0351916067302227,
    -0.0993705689907074,
    -0.01559705100953579,
    0.09442345798015594,
    -0.06470410525798798,
    -0.005032383371144533,
    0.059374939650297165,
    0.028637776151299477,
    -0.09518188238143921,
    -0.06239926069974899,
    0.007935418747365475,
    -0.0023214167449623346,
    -0.0005639090668410063,
    -0.008193889632821083,
    -0.05419563502073288,
    -0.0535520575940609,
    -0.047376781702041626,
    0.005991299171000719,
    0.024832764640450478,
    0.09911215305328369,
    -0.05463860183954239,
    -0.00981749314814806,
    0.034510958939790726,
    -0.006604708731174469,
    -0.03679358959197998,
    -0.020913368090987206,
    -0.010190796107053757,
    0.051909103989601135,
    0.08531633019447327,
    0.09330901503562927,
    0.11693167686462402,
    -0.013616599142551422,
    0.01614421047270298,
    -0.2117757648229599,
    0.08032979816198349,
    -0.04764597862958908,
    -0.058591362088918686,
    -0.04586685448884964,
    -0.08203145861625671,
    0.08814719319343567,
    -0.05351297929883003,
    0.06740746647119522,
    -0.08339938521385193,
    -0.07161816209554672,
    -0.05778832361102104,
    0.07111985981464386,
    0.04129209369421005,
    0.07306382060050964,
    -0.045119158923625946,
    -0.042933735996484756,
    0.011449233628809452,
    0.02718614973127842,
    0.11457806080579758,
    0.029052363708615303,
    0.061701055616140366,
    -0.013055087067186832,
    0.0430641770362854,
    0.01581958495080471,
    -0.10588468611240387,
    -0.017223497852683067,
    0.10946489870548248,
    -0.04570314288139343,
    -0.03022736683487892,
    -0.0785449892282486,
    0.16636262834072113,
    -0.05149739980697632,
    0.04995081573724747,
    0.0744788870215416,
    0.1474052220582962,
    -0.043364863842725754,
    -0.05463887378573418,
    -0.015619550831615925,
    0.0424845926463604,
    0.0051076761446893215,
    0.02159157209098339,
    0.10743315517902374,
    0.015227955766022205,
    -0.10811782628297806,
    -0.045432984828948975,
    -0.0020885199774056673,
    -0.07062357664108276,
    0.03550385683774948,
    -0.017209578305482864,
    -0.04379260540008545,
    0.1519845873117447,
    0.04019906744360924,
    0.09364286065101624,
    0.032804571092128754,
    0.04701026529073715,
    -0.10455767810344696,
    0.019189365208148956,
    -0.015166391618549824,
    -0.05415432155132294,
    0.04854016751050949,
    0.04484215006232262,
    0.02422163262963295,
    -0.0021813292987644672,
    -0.00974415335804224,
    0.0004857210151385516,
    0.09234249591827393,
    -0.010290532372891903,
    -0.026185287162661552,
    -0.011114101856946945,
    0.007749231066554785,
    0.006774785928428173,
    -0.01985788717865944,
    0.0440710186958313,
    0.03587350249290466,
    -0.03333676606416702,
    0.14144070446491241,
    -0.1159464567899704,
    -0.04920080304145813,
    -0.040886927396059036,
    0.03233357146382332,
    -0.057025328278541565,
    -0.0199050921946764,
    0.09386203438043594,
    -0.04846831411123276,
    0.057698991149663925,
    -0.06344913691282272,
    -0.009605219587683678,
    0.04951193183660507,
    -0.07471289485692978,
    -0.06989097595214844,
    0.021991930902004242,
    -0.01865888014435768,
    -0.09756002575159073,
    0.06285800039768219,
    -0.018023744225502014,
    -0.015919186174869537,
    -0.02209571935236454,
    -0.0018601114861667156,
    -0.06272278726100922,
    -0.03947022929787636,
    0.09377246350049973,
    0.028235221281647682,
    -0.02817118726670742,
    0.16430027782917023,
    -0.0950562059879303,
    -0.04122506082057953,
    0.02307266741991043,
    0.056444939225912094,
    0.021422067657113075,
    0.015881162136793137,
    0.006773617118597031,
    0.03411511704325676,
    -0.007232538890093565,
    0.05694196745753288,
    -0.03276193514466286,
    -0.11355940252542496,
    -0.06227286905050278,
    -0.04643517732620239,
    0.03145328536629677,
    0.02059006877243519,
    -0.06609151512384415,
    -0.03378134220838547,
    0.033783260732889175,
    0.03881155326962471,
    -0.01795986108481884,
    -0.02715139277279377,
    0.03277236968278885,
    0.03904581815004349,
    -0.011519132182002068,
    -0.08233840018510818,
    0.1020076796412468,
    0.04873879253864288,
    0.06976783275604248,
    -0.04960104823112488,
    -0.035586338490247726,
    -0.061769999563694,
    -0.008929675444960594,
    -0.016464777290821075,
    0.0020251034293323755,
    -0.041429080069065094,
    0.012396185658872128,
    -0.0362420529127121,
    -0.014272365719079971,
    -0.03543086349964142,
    0.04897690191864967,
    0.057526953518390656,
    0.09969635307788849,
    0.06085296720266342,
    -0.09254342317581177,
    -0.1359260529279709,
    0.09093103557825089,
    -0.03884044662117958,
    -0.14111797511577606,
    -0.02554687298834324,
    -0.04253726825118065,
    0.06093543767929077,
    0.02407062239944935,
    -0.08569047600030899,
    -0.012669802643358707,
    -0.03564133867621422,
    0.05767832323908806,
    -0.08780904859304428,
    0.05581677705049515,
    0.06986478716135025,
    -0.028327690437436104,
    0.015259791165590286,
    -0.026979666203260422,
    -0.04347855970263481,
    0.03445959836244583,
    -0.037901051342487335,
    0.05562586709856987,
    0.08917310833930969,
    -0.039732515811920166,
    -0.04702257737517357,
    0.054087333381175995,
    0.017046814784407616,
    0.001795333344489336,
    -0.1769256293773651,
    0.021037321537733078,
    -0.09591447561979294,
    -0.002656516619026661,
    -0.013782848604023457,
    0.09826216846704483,
    0.018154514953494072,
    -0.13754647970199585,
    0.07294518500566483,
    0.07809054851531982,
    0.004515578504651785,
    0.038788385689258575,
    -0.048245612531900406,
    0.02828608639538288,
    0.09508462250232697,
    -0.10742699354887009,
    -0.10847938060760498,
    0.06640679389238358,
    0.016175828874111176,
    0.09387189149856567,
    0.04674124717712402,
    -0.017566753551363945,
    -0.06828232109546661,
    -0.06682940572500229,
    -0.03332791104912758,
    0.06603306531906128,
    0.006858245003968477,
    0.04939008504152298,
    -0.029853317886590958,
    -0.05700688436627388,
    0.012617697939276695,
    -0.05836028233170509,
    -0.10615357756614685,
    -0.03408313915133476,
    -0.05350998044013977,
    -0.056045643985271454,
    -0.017821481451392174,
    -0.018234627321362495,
    0.07949994504451752,
    -0.03899874538183212,
    -0.09654565155506134,
    -0.030603481456637383,
    0.0033716887701302767,
    -0.04164522886276245,
    -0.026008618995547295,
    0.0062101432122290134,
    -0.09671106934547424,
    -0.030781570822000504,
    0.02970091812312603,
    -0.07273396104574203,
    -0.03518738970160484,
    -0.08541319519281387,
    -0.10269024968147278,
    -0.01414656825363636,
    -0.030702602118253708,
    -0.1410716027021408,
    0.07040269672870636,
    -0.0736587718129158,
    -0.000576585647650063,
    -0.019645128399133682,
    -0.053715601563453674,
    -0.05837931111454964,
    -0.02104507014155388,
    0.042171455919742584,
    0.010858789086341858,
    0.003979169763624668,
    0.036036018282175064,
    0.0432451106607914,
    0.028496405109763145,
    -0.008270565420389175,
    -0.01890539564192295,
    -0.0034758313558995724,
    0.11299823224544525,
    -0.08186318725347519,
    0.02986963279545307,
    -0.050074394792318344,
    -0.041918620467185974,
    0.11608032882213593,
    -0.029645776376128197,
    -0.047628603875637054,
    -0.06011022999882698,
    -0.02444797195494175,
    0.04423116147518158,
    -0.03849606215953827,
    -0.06562990695238113,
    0.0503050796687603,
    0.09820854663848877,
    -0.0018153429264202714,
    -0.000788444303907454,
    -0.022008081898093224,
    0.12007950246334076,
    -0.024370405822992325,
    -0.10495422035455704,
    0.0481158085167408,
    -0.0007472062716260552,
    -0.03807282820343971,
    0.04805908724665642,
    -0.02069174498319626,
    0.042877357453107834,
    -0.053928978741168976,
    0.04897189140319824,
    0.09396560490131378,
    -0.006738744676113129,
    -0.07442837953567505,
    -0.15147119760513306,
    0.033168062567710876,
    0.14789557456970215,
    -0.09295832365751266,
    -0.03466947376728058,
    -0.06618322432041168,
    -0.09072338044643402,
    0.023536546155810356,
    -0.10934437811374664,
    -0.039209164679050446,
    -0.004131300840526819,
    0.041443392634391785,
    -0.02931474708020687,
    0.021189020946621895,
    -0.03470478951931,
    -0.012230003252625465,
    -0.08253762125968933,
    -0.03804022818803787,
    -0.05844716727733612,
    0.04310380294919014,
    -0.0492694154381752,
    0.05849909782409668,
    -0.048054855316877365,
    0.03926233947277069,
    0.0739254578948021,
    -0.04473955184221268,
    0.05571679398417473,
    -0.0315820649266243,
    0.025712862610816956,
    0.005111750215291977,
    -0.04666430130600929,
    -0.03057560697197914,
    0.036373719573020935,
    -0.03170318901538849,
    -0.05117582157254219,
    0.04110492393374443,
    -0.007791638839989901,
    -0.13768227398395538,
    -0.0012097889557480812,
    -0.029396142810583115,
    -0.05264038220047951,
    0.05910157784819603,
    -0.04178577661514282,
    -0.04573341831564903,
    -0.024277953431010246,
    0.04621105268597603,
    0.0775759294629097,
    -0.054747410118579865,
    0.032529063522815704,
    -0.0398530475795269,
    0.09366759657859802,
    0.1127728596329689,
    0.09418827295303345,
    -0.01973789371550083,
    -0.058952126652002335,
    0.06105401739478111,
    -0.03515557199716568,
    0.034166157245635986,
    0.026321498677134514,
    -0.031795743852853775,
    -0.043128665536642075,
    -0.11441454291343689,
    0.04556124284863472,
    0.03175213932991028,
    -0.018392447382211685,
    0.033935822546482086,
    -0.03440392017364502,
    0.024327361956238747,
    -0.01387596596032381,
    -0.10976888984441757,
    0.01336835790425539,
    -0.013394126668572426,
    0.037418436259031296,
    0.09367295354604721,
    -0.06742767244577408,
    -0.08721280097961426,
    -0.06118406727910042,
    0.14865317940711975,
    0.004924853798002005,
    -0.059459492564201355,
    -0.004273603670299053,
    0.1271214336156845,
    0.17421744763851166,
    -0.16524428129196167,
    0.010389430448412895,
    -0.1313668191432953,
    0.10740058869123459,
    -0.07124214619398117,
    -0.17858833074569702,
    0.10484950244426727,
    0.06638481467962265,
    0.018767045810818672,
    0.013342738151550293,
    0.016659775748848915,
    0.07219117879867554,
    -0.027567058801651,
    0.016614321619272232,
    -0.0863317921757698,
    -0.012284600175917149,
    0.04063008353114128,
    0.12215588986873627,
    -0.12284398078918457,
]

================
File: README.md
================
# Neuronpedia Python Library

## Authentication

Some APIs on Neuronpedia require an API key. For example, if you want to bookmark something in your account, or upload a new vector, you'll need to identify yourself with a Neuronpedia API key.

### Setting the API Key

1. Sign up for free at `neuronpedia.org`.
2. Get your Neuronpedia API key from `neuronpedia.org/account`.
3. Set the environment variable `NEURONPEDIA_API_KEY` to your API key. You can do this through a `.env` file or other similar methods.

### Example: Upload a Vector, then Steer With It

```
from neuronpedia.sample_data import GEMMA2_2B_IT_DINOSAURS_VECTOR
from neuronpedia.np_vector import NPVector
import os, json

# from neuronpedia.org/account
os.environ["NEURONPEDIA_API_KEY"] = "YOUR_NP_API_KEY"

# upload the custom vector
np_vector = NPVector.new(
    label="dinosaurs",
    model_id="gemma-2-2b-it",
    layer_num=20,
    hook_type="hook_resid_pre",
    vector=GEMMA2_2B_IT_DINOSAURS_VECTOR,
    default_steer_strength=44,
)

# steer with it
responseJson = np_vector.steer_chat(
    steered_chat_messages=[{"role": "user", "content": "Write a one sentence story."}]
)

print(json.dumps(responseJson, indent=2))
print("UI Steering at: " + responseJson["shareUrl"])
```

The output of the above will be similar to:

```
{
  "STEERED": {
    "chat_template": [
      {
        "content": "Write a one sentence story.",
        "role": "user"
      },
      {
        "content": "The last dinosaur roared, its breath a smoke-filled mirror of the dying sun.",
        "role": "model"
      }
    ],
    "raw": "<bos><start_of_turn>user\nWrite a one sentence story.<end_of_turn>\n<start_of_turn>model\nThe last dinosaur roared, its breath a smoke-filled mirror of the dying sun. \n<end_of_turn><eos>"
  },
  [...]
}
```

See the `examples` folder for detailed notebooks and usage.

### Important: Uniquely Identifying a Vector/Feature

On Neuronpedia, a vector or feature has a unique identifier comprised of three parts:

- **Model ID:** The ID of the model that this vector belongs to. For example, `gpt2-small`.
- **Source:** A "group name" that starts with the layer number, and usually contains some other identifying information depending on what it is, like number of features in the group. Some examples of source:
  - `6-res-jb`: `6` = Layer 6, `res` = residual stream, `jb` = Joseph Bloom
  - `3-gemmascope-att-16k`: `3` = Layer 3, `gemmascope` = Gemma Scope, `att` = Attention, `16k` = 16k-width SAE
- **Index:** The index in the model + group. This is a string, and currently the string is always an integer. However, this may change in the future.

Example:

- `gemma-2-2b/3-gemmascope-att-16k/4232`
  - Model: gemma-2-2b
  - Source: 3-gemmascope-att-16k
  - Index: 4232

To make API calls that involve an existing vector or feature on Neuronpedia, you pass the model, source, and index as required.

Example API Call: Get SAE Feature (including activations, explanations, etc)

```
from neuronpedia.np_sae_feature import SAEFeature

sae_feature = SAEFeature.get("gemma-2-2b", "3-gemmascope-att-16k", "4232")
print(sae_feature)
```

================
File: tests/test_vector.py
================
import random
import pytest
from neuronpedia.np_vector import NPVector
from neuronpedia.requests.vector_request import VectorRequest
from neuronpedia.sample_data import GPT2_SMALL_RES_JB_JEDI_VECTOR
class TestNewVectorRequest:
    @pytest.fixture
    def vector_request(self):
        return VectorRequest()
    def test_new_and_deletevector(self, vector_request):
        label = "Jedi " + str(random.randint(0, 1000000))
        model_id = "gpt2-small"
        layer_num = 0
        hook_type = "hook_resid_pre"
        vector = GPT2_SMALL_RES_JB_JEDI_VECTOR
        steer_strength = 15.0
        # create it
        np_vector = NPVector.new(
            label=label,
            model_id=model_id,
            layer_num=layer_num,
            hook_type=hook_type,
            vector=vector,
            default_steer_strength=steer_strength,
        )
        assert np_vector.url is not None
        assert model_id in np_vector.url
        assert np_vector.model_id == model_id
        assert np_vector.source.startswith(f"{layer_num}")
        assert np_vector.label == label
        assert np_vector.hook_name.endswith(hook_type)
        assert all(abs(a - b) <= 0.0001 for a, b in zip(np_vector.values, vector))
        assert np_vector.default_steer_strength == steer_strength
        # delete it
        np_vector.delete()
    def test_get_owned(self, vector_request):
        # make a test vector
        label = "Jedi " + str(random.randint(0, 1000000))
        model_id = "gpt2-small"
        layer_num = 0
        hook_type = "hook_resid_pre"
        vector = GPT2_SMALL_RES_JB_JEDI_VECTOR
        steer_strength = 15.0
        # create it
        new_np_vector = vector_request.new(
            label=label,
            model_id=model_id,
            layer_num=layer_num,
            hook_type=hook_type,
            vector=vector,
            default_steer_strength=steer_strength,
        )
        # list all vectors
        np_vectors = vector_request.get_owned()
        assert len(np_vectors) > 0
        # check for new_np_vector in the list
        matching_vector = None
        for vector in np_vectors:
            print(vector)
            if vector == new_np_vector:
                matching_vector = vector
                break
        assert matching_vector is not None
        # delete the vector
        new_np_vector.delete()



================================================================
End of Codebase
================================================================
