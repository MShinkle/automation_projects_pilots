This file is a merged representation of a subset of the codebase, containing specifically included files and files not matching ignore patterns, combined into a single document by Repomix.
The content has been processed where empty lines have been removed.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
- Pay special attention to the Repository Description. These contain important context and guidelines specific to this project.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: **/*.py, **/*.md, **/*.txt
- Files matching these patterns are excluded: **/.git/**, **/.github/**, CHANGELOG.md
- Empty lines have been removed from all files
- Files are sorted by Git change count (files with more changes are at the bottom)

Additional Info:
----------------
User Provided Header:
-----------------------
This file is a consolidated single-file compilation of all code in the repository generated by Repomix. Note that .ipynb files have been converted to .py files.

================================================================
Directory Structure
================================================================
CODE_OF_CONDUCT.md
CONTRIBUTING.md
docker/README.md
docs/source/conf.py
inseq/__init__.py
inseq/attr/__init__.py
inseq/attr/attribution_decorators.py
inseq/attr/feat/__init__.py
inseq/attr/feat/attribution_utils.py
inseq/attr/feat/feature_attribution.py
inseq/attr/feat/gradient_attribution.py
inseq/attr/feat/internals_attribution.py
inseq/attr/feat/ops/__init__.py
inseq/attr/feat/ops/discretized_integrated_gradients.py
inseq/attr/feat/ops/lime.py
inseq/attr/feat/ops/monotonic_path_builder.py
inseq/attr/feat/ops/reagent_core/__init__.py
inseq/attr/feat/ops/reagent_core/importance_score_evaluator.py
inseq/attr/feat/ops/reagent_core/rationalizer.py
inseq/attr/feat/ops/reagent_core/stopping_condition_evaluator.py
inseq/attr/feat/ops/reagent_core/token_replacer.py
inseq/attr/feat/ops/reagent_core/token_sampler.py
inseq/attr/feat/ops/reagent.py
inseq/attr/feat/ops/sequential_integrated_gradients.py
inseq/attr/feat/ops/value_zeroing.py
inseq/attr/feat/perturbation_attribution.py
inseq/attr/step_functions.py
inseq/commands/attribute_context/__init__.py
inseq/commands/attribute_context/attribute_context_args.py
inseq/commands/attribute_context/attribute_context_helpers.py
inseq/commands/attribute_context/attribute_context_viz_helpers.py
inseq/commands/attribute_context/attribute_context.py
inseq/commands/attribute_dataset/__init__.py
inseq/commands/attribute_dataset/attribute_dataset_args.py
inseq/commands/attribute_dataset/attribute_dataset.py
inseq/commands/attribute/__init__.py
inseq/commands/attribute/attribute_args.py
inseq/commands/attribute/attribute.py
inseq/commands/base.py
inseq/commands/cli.py
inseq/commands/commands_utils.py
inseq/data/__init__.py
inseq/data/aggregation_functions.py
inseq/data/aggregator.py
inseq/data/attribution.py
inseq/data/batch.py
inseq/data/data_utils.py
inseq/data/viz.py
inseq/models/__init__.py
inseq/models/attribution_model.py
inseq/models/decoder_only.py
inseq/models/encoder_decoder.py
inseq/models/huggingface_model.py
inseq/models/model_config.py
inseq/models/model_decorators.py
inseq/utils/__init__.py
inseq/utils/alignment_utils.py
inseq/utils/argparse.py
inseq/utils/cache.py
inseq/utils/contrast_utils.py
inseq/utils/errors.py
inseq/utils/hooks.py
inseq/utils/id_utils.py
inseq/utils/import_utils.py
inseq/utils/misc.py
inseq/utils/registry.py
inseq/utils/serialization.py
inseq/utils/torch_utils.py
inseq/utils/typing.py
inseq/utils/viz_utils.py
README.md
requirements-dev.txt
requirements.txt
SECURITY.md
tests/__init__.py
tests/attr/feat/ops/test_monotonic_path_builder.py
tests/attr/feat/test_attribution_utils.py
tests/attr/feat/test_feature_attribution.py
tests/attr/feat/test_step_functions.py
tests/commands/test_attribute_context.py
tests/data/test_aggregator.py
tests/data/test_attribution.py
tests/inference_commons.py
tests/models/test_huggingface_model.py
tests/models/test_model_config.py
tests/utils/test_torch_utils.py

================================================================
Files
================================================================

================
File: CODE_OF_CONDUCT.md
================
# Contributor Covenant Code of Conduct

## Our Pledge

In the interest of fostering an open and welcoming environment, we as
contributors and maintainers pledge to making participation in our project and
our community a harassment-free experience for everyone, regardless of age, body
size, disability, ethnicity, sex characteristics, gender identity and expression,
level of experience, education, socio-economic status, nationality, personal
appearance, race, religion, or sexual identity and orientation.

## Our Standards

Examples of behavior that contributes to creating a positive environment
include:

* Using welcoming and inclusive language
* Being respectful of differing viewpoints and experiences
* Gracefully accepting constructive criticism
* Focusing on what is best for the community
* Showing empathy towards other community members

Examples of unacceptable behavior by participants include:

* The use of sexualized language or imagery and unwelcome sexual attention or
 advances
* Trolling, insulting/derogatory comments, and personal or political attacks
* Public or private harassment
* Publishing others' private information, such as a physical or electronic
 address, without explicit permission
* Other conduct which could reasonably be considered inappropriate in a
 professional setting

## Our Responsibilities

Project maintainers are responsible for clarifying the standards of acceptable
behavior and are expected to take appropriate and fair corrective action in
response to any instances of unacceptable behavior.

Project maintainers have the right and responsibility to remove, edit, or
reject comments, commits, code, wiki edits, issues, and other contributions
that are not aligned to this Code of Conduct, or to ban temporarily or
permanently any contributor for other behaviors that they deem inappropriate,
threatening, offensive, or harmful.

## Scope

This Code of Conduct applies both within project spaces and in public spaces
when an individual is representing the project or its community. Examples of
representing a project or community include using an official project e-mail
address, posting via an official social media account, or acting as an appointed
representative at an online or offline event. Representation of a project may be
further defined and clarified by project maintainers.

## Enforcement

Instances of abusive, harassing, or otherwise unacceptable behavior may be
reported by contacting the project team at [info@inseq.org](mailto:info@inseq.org). All
complaints will be reviewed and investigated and will result in a response that
is deemed necessary and appropriate to the circumstances. The project team is
obligated to maintain confidentiality with regard to the reporter of an incident.
Further details of specific enforcement policies may be posted separately.

Project maintainers who do not follow or enforce the Code of Conduct in good
faith may face temporary or permanent repercussions as determined by other
members of the project's leadership.

## Attribution

This Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4,
available at <https://www.contributor-covenant.org/version/1/4/code-of-conduct.html>

[homepage]: https://www.contributor-covenant.org

For answers to common questions about this code of conduct, see
<https://www.contributor-covenant.org/faq>

================
File: CONTRIBUTING.md
================
# How to contribute

## Dependencies

We use [`uv`](https://github.com/astral-sh/uv) to manage Inseq dependencies.
If you dont have `uv`, you should install with `make uv-download`.

To install dependencies and prepare [`pre-commit`](https://pre-commit.com/) hooks you would need to run `install` command:

```bash
make install

or

make install-dev
```

To activate your `virtualenv` run `make uv-activate`.

## Codestyle

After installation you may execute code formatting.

```bash
make lint
```

### Checks

Many checks are configured for this project. Command `make check-style` will check style with `ruff`.
The `make check-safety` command will look at the security of your code.

Comand `make lint` applies all checks.

### Before submitting

Before submitting your code please do the following steps:

1. Add any changes you want
2. Add tests for the new changes
3. Edit documentation if you have changed something significant
4. Run `make fix-style` to format your changes.
5. Run `make lint` to ensure that types and security are okay.

## Other help

You can contribute by spreading a word about this library. It would also be a huge contribution to write a short article on how you are using this project. You can also share your best practices with us.

================
File: docker/README.md
================
# Docker for inseq

## Installation

To create Docker you need to run:

```bash
make docker-build
```

which is equivalent to:

```bash
make docker-build VERSION=latest
```

You may provide name and version for the image.
Default name is `IMAGE := inseq`.
Default version is `VERSION := latest`.

```bash
make docker-build IMAGE=some_name VERSION=0.1.0
```

## Usage

```bash
docker run -it --rm \
   -v $(pwd):/workspace \
   inseq bash
```

## How to clean up

To uninstall docker image run `make docker-remove` with `VERSION`:

```bash
make docker-remove VERSION=0.1.0
```

you may also choose the image name

```bash
make docker-remove IMAGE=some_name VERSION=latest
```

If you want to clean all, including `build` and `pycache` run `make clean-all`

================
File: docs/source/conf.py
================
#
# Configuration file for the Sphinx documentation builder.
#
# This file does only contain a selection of the most common options. For a
# full list see the documentation:
# http://www.sphinx-doc.org/en/master/config
# -- Path setup --------------------------------------------------------------
# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
#
import sys
from pathlib import Path
from typing import Dict
_PATH_ROOT = Path(__file__).parent.parent.parent.absolute()
# _PATH_SRC = Path(_PATH_ROOT, "inseq")
sys.path.insert(0, str(_PATH_ROOT.absolute()))
# -- Project information -----------------------------------------------------
project = "inseq"
copyright = "2024 , The Inseq Team, Licensed under the Apache License, Version 2.0"
author = "The Inseq Team"
# The short X.Y version
version = "0.7"
# The full version, including alpha/beta/rc tags
release = "0.7.0.dev0"
# Prefix link to point to master, comment this during version release and uncomment below line
extlinks = {"prefix_link": ("https://github.com/inseq-team/inseq/blob/master/%s", "version %s")}
# -- General configuration ---------------------------------------------------
# If your documentation needs a minimal Sphinx version, state it here.
#
# needs_sphinx = '1.0'
# Add any Sphinx extension module names here, as strings. They can be
# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
# ones.
extensions = [
    "sphinx.ext.autodoc",
    "sphinx.ext.doctest",
    "sphinx.ext.autosummary",
    "sphinx.ext.extlinks",
    "sphinx.ext.todo",
    "sphinx.ext.coverage",
    "sphinx.ext.napoleon",
    "sphinx.ext.intersphinx",
    "sphinx_gitstamp",
    "sphinx.ext.viewcode",
    "sphinxext.opengraph",
    "sphinx_copybutton",
    "sphinxemoji.sphinxemoji",
    "sphinx_design",
]
# Add any paths that contain templates here, relative to this directory.
templates_path = ["_templates"]
# The suffix(es) of source filenames.
# You can specify multiple suffix as a list of string:
source_parsers = {
    ".md": "recommonmark.parser.CommonMarkParser",
}
source_suffix = [".rst", ".md"]
# source_suffix = '.rst'
# The master toctree document.
master_doc = "index"
# The language for content autogenerated by Sphinx. Refer to documentation
# for a list of supported languages.
#
# This is also used if you do content translation via gettext catalogs.
# Usually you set "language" from the command line for these cases.
language = "en"
# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
# This pattern also affects html_static_path and html_extra_path.
exclude_patterns = ["_build", "Thumbs.db", ".DS_Store"]
# The name of the Pygments (syntax highlighting) style to use.
pygments_style = None
# Permalinks
html_permalinks_icon = "<i class='fas fa-link'></i>"
# If true, `todo` and `todoList` produce output, else they produce nothing.
todo_include_todos = True
# Remove the prompt when copying examples
copybutton_prompt_text = r">>> |\.\.\. "
copybutton_prompt_is_regexp = True
# -- Options for intersphinx extension ---------------------------------------
# Example configuration for intersphinx: refer to the Python standard library.
intersphinx_mapping = {
    #    "python": ("https://docs.python.org/3", None),
    #    "torch": ("https://pytorch.org/docs/stable/", None),
    #    "numpy": ("https://numpy.org/doc/stable/", None),
}
# -- Options for HTML output -------------------------------------------------
# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
#
html_theme = "furo"
html_logo = "images/inseq_logo.png"
html_title = f"inseq {release}"
# Theme options are theme-specific and customize the look and feel of a theme
# further.  For a list of options available for each theme, see the
# documentation.
#
html_theme_options = {
    "navigation_with_keys": True,
    "footer_icons": [
        {
            "name": "GitHub",
            "url": "https://github.com/inseq-team/inseq",
            "html": """
                <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16">
                    <path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z"></path>
                </svg>
            """,
            "class": "",
        },
        {
            "name": "Twitter",
            "url": "https://twitter.com/InseqLib",
            "html": """
                <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024" height="1em" width="1em">
                    <path d="M928 254.3c-30.6 13.2-63.9 22.7-98.2 26.4a170.1 170.1 0 0 0 75-94 336.64 336.64 0 0 1-108.2 41.2A170.1 170.1 0 0 0 672 174c-94.5 0-170.5 76.6-170.5 170.6 0 13.2 1.6 26.4 4.2 39.1-141.5-7.4-267.7-75-351.6-178.5a169.32 169.32 0 0 0-23.2 86.1c0 59.2 30.1 111.4 76 142.1a172 172 0 0 1-77.1-21.7v2.1c0 82.9 58.6 151.6 136.7 167.4a180.6 180.6 0 0 1-44.9 5.8c-11.1 0-21.6-1.1-32.2-2.6C211 652 273.9 701.1 348.8 702.7c-58.6 45.9-132 72.9-211.7 72.9-14.3 0-27.5-.5-41.2-2.1C171.5 822 261.2 850 357.8 850 671.4 850 843 590.2 843 364.7c0-7.4 0-14.8-.5-22.2 33.2-24.3 62.3-54.4 85.5-88.2z"></path>
                </svg>
            """,
            "class": "",
        },
        {
            "name": "InDeep Consortium",
            "url": "https://interpretingdl.github.io",
            "html": """
                <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg">
                    <path d="M333.78 20.188c-39.97 0-79.96 15.212-110.405 45.656-58.667 58.667-60.796 152.72-6.406 213.97l-15.782 15.748 13.25 13.25 15.75-15.78c61.248 54.39 155.3 52.26 213.968-6.407 60.887-60.886 60.888-159.894 0-220.78C413.713 35.4 373.753 20.187 333.78 20.187zm0 18.562c35.15 0 70.285 13.44 97.158 40.313 53.745 53.745 53.744 140.6 0 194.343-51.526 51.526-133.46 53.643-187.5 6.375l.218-.217c-2.35-2.05-4.668-4.17-6.906-6.407-2.207-2.206-4.288-4.496-6.313-6.812l-.218.22c-47.27-54.04-45.152-135.976 6.374-187.502C263.467 52.19 298.63 38.75 333.78 38.75zm0 18.813c-30.31 0-60.63 11.6-83.81 34.78-46.362 46.362-46.362 121.234 0 167.594 10.14 10.142 21.632 18.077 33.905 23.782-24.91-19.087-40.97-49.133-40.97-82.94 0-15.323 3.292-29.888 9.22-43-4.165 20.485.44 40.88 14.47 54.907 24.583 24.585 68.744 20.318 98.624-9.562 29.88-29.88 34.146-74.04 9.56-98.625-2.375-2.376-4.943-4.473-7.655-6.313 45.13 8.648 79.954 46.345 84.25 92.876 4.44-35.07-6.82-71.726-33.813-98.72-23.18-23.18-53.47-34.78-83.78-34.78zM176.907 297.688L42.094 432.5l34.562 34.563L211.47 332.25l-34.564-34.563zM40 456.813L24 472.78 37.22 486l15.968-16L40 456.812z"></path>
                </svg>
            """,
            "class": "",
        },
    ],
}
ogp_image = ""
ogp_description = "Inseq: an Interpretability Toolkit for Sequence Generation Models"
ogp_description_length = 55
ogp_custom_meta_tags = [
    f'<meta name="twitter:image" content="{ogp_image}">',
    f'<meta name="twitter:description" content="{ogp_description}">',
]
# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = [
    "images",
    "html_outputs",
    "_static",
]
html_css_files = ["inseq.css"]
# Custom sidebar templates, must be a dictionary that maps document names
# to template names.
#
# The default sidebars (for documents that don't match any pattern) are
# defined by theme itself.  Builtin themes are using these templates by
# default: ``['localtoc.html', 'relations.html', 'sourcelink.html',
# 'searchbox.html']``.
#
# html_sidebars = {}
# This must be the name of an image file (path relative to the configuration
# directory) that is the favicon of the docs. Modern browsers use this as
# the icon for tabs, windows and bookmarks. It should be a Windows-style
# icon file (.ico).
html_favicon = "images/favicon.ico"
# -- Options for Napoleon Extension --------------------------------------------
# Parse Google style docstrings.
# See http://google.github.io/styleguide/pyguide.html
napoleon_google_docstring = True
# Parse NumPy style docstrings.
# See https://github.com/numpy/numpy/blob/master/doc/HOWTO_DOCUMENT.rst.txt
napoleon_numpy_docstring = True
# Should special members (like __membername__) and private members
# (like _membername) members be included in the documentation if they
# have docstrings.
napoleon_include_private_with_doc = False
napoleon_include_special_with_doc = True
# If True, docstring sections will use the ".. admonition::" directive.
# If False, docstring sections will use the ".. rubric::" directive.
# One may look better than the other depending on what HTML theme is used.
napoleon_use_admonition_for_examples = False
napoleon_use_admonition_for_notes = False
napoleon_use_admonition_for_references = False
# If True, use Sphinx :ivar: directive for instance variables:
#     :ivar attr1: Description of attr1.
#     :type attr1: type
# If False, use Sphinx .. attribute:: directive for instance variables:
#     .. attribute:: attr1
#
#        *type*
#
#        Description of attr1.
napoleon_use_ivar = False
# If True, use Sphinx :param: directive for function parameters:
#     :param arg1: Description of arg1.
#     :type arg1: type
# If False, output function parameters using the :parameters: field:
#     :parameters: **arg1** (*type*) -- Description of arg1.
napoleon_use_param = True
# If True, use Sphinx :rtype: directive for the return type:
#     :returns: Description of return value.
#     :rtype: type
# If False, output the return type inline with the return description:
#     :returns: *type* -- Description of return value.
napoleon_use_rtype = True
# -- Options for HTMLHelp output ---------------------------------------------
# Output file base name for HTML help builder.
htmlhelp_basename = "inseq_doc"
# -- Options for LaTeX output ------------------------------------------------
latex_elements: Dict[str, str] = {
    # The paper size ('letterpaper' or 'a4paper').
    #
    # 'papersize': 'letterpaper',
    # The font size ('10pt', '11pt' or '12pt').
    #
    # 'pointsize': '10pt',
    # Additional stuff for the LaTeX preamble.
    #
    # 'preamble': '',
    # Latex figure (float) alignment
    #
    # 'figure_align': 'htbp',
}
# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title,
#  author, documentclass [howto, manual, or own class]).
latex_documents = [
    (master_doc, "inseq.tex", "inseq Documentation", "The Inseq Team", "manual"),
]
# -- Options for manual page output ------------------------------------------
# One entry per manual page. List of tuples
# (source start file, name, description, authors, manual section).
man_pages = [(master_doc, "inseq", "inseq Documentation", [author], 1)]
# -- Options for Texinfo output ----------------------------------------------
# Grouping the document tree into Texinfo files. List of tuples
# (source start file, target name, title, author,
#  dir menu entry, description, category)
texinfo_documents = [
    (
        master_doc,
        "inseq",
        "inseq Documentation",
        author,
        "inseq",
        "Interpretability for Sequence Generation Models.",
        "Miscellaneous",
    ),
]
sphinxemoji_style = "twemoji"
def setup(app):
    app.add_js_file("inseq.js")
    app.add_css_file("inseq.css")

================
File: inseq/__init__.py
================
"""Interpretability for Sequence Generation Models ðŸ”."""
from .attr import list_feature_attribution_methods, list_step_functions, register_step_function
from .data import (
    FeatureAttributionOutput,
    list_aggregation_functions,
    list_aggregators,
    merge_attributions,
    show_attributions,
    show_granular_attributions,
    show_token_attributions,
)
from .models import AttributionModel, list_supported_frameworks, load_model, register_model_config
from .utils.id_utils import explain
def get_version() -> str:
    """Returns the current version of the Inseq library."""
    try:
        import pkg_resources
        return pkg_resources.get_distribution("inseq").version
    except pkg_resources.DistributionNotFound:
        return "unknown"
__all__ = [
    "AttributionModel",
    "FeatureAttributionOutput",
    "load_model",
    "explain",
    "show_attributions",
    "show_granular_attributions",
    "show_token_attributions",
    "list_feature_attribution_methods",
    "list_aggregators",
    "list_aggregation_functions",
    "list_step_functions",
    "list_supported_frameworks",
    "register_step_function",
    "register_model_config",
    "merge_attributions",
]

================
File: inseq/attr/__init__.py
================
from .feat import FeatureAttribution, extract_args, list_feature_attribution_methods
from .step_functions import (
    STEP_SCORES_MAP,
    StepFunctionArgs,
    list_step_functions,
    register_step_function,
)
__all__ = [
    "FeatureAttribution",
    "list_feature_attribution_methods",
    "list_step_functions",
    "register_step_function",
    "STEP_SCORES_MAP",
    "extract_args",
    "StepFunctionArgs",
]

================
File: inseq/attr/attribution_decorators.py
================
# Copyright 2021 The Inseq Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""Decorators for attribution methods."""
import logging
from collections.abc import Callable, Sequence
from functools import wraps
from typing import Any
from ..data.data_utils import TensorWrapper
logger = logging.getLogger(__name__)
def set_hook(f: Callable[[Any], Any]) -> Callable[[Any], Any]:
    """Sets the status of the attribution model associated to the function
    to is_hooked = True.
    Required to decorate the hook functions in subclasses.
    """
    @wraps(f)
    def set_hook_wrapper(self, **kwargs):
        f(self, **kwargs)
        self.attribution_model.is_hooked = True
    return set_hook_wrapper
def unset_hook(f: Callable[[Any], Any]) -> Callable[[Any], Any]:
    """Sets the status of the attribution model associated to the function
    to is_hooked = False.
    Required to decorate the unhook functions in subclasses.
    """
    @wraps(f)
    def unset_hook_wrapper(self, **kwargs):
        f(self, **kwargs)
        self.attribution_model.is_hooked = False
    return unset_hook_wrapper
def batched(f: Callable[..., Any]) -> Callable[..., Any]:
    """Decorator that enables batching of the args."""
    @wraps(f)
    def batched_wrapper(self, *args, batch_size: int | None = None, **kwargs):
        def get_batched(bs: int | None, seq: Sequence[Any]) -> list[list[Any]]:
            if isinstance(seq, str):
                seq = [seq]
            if isinstance(seq, list):
                return [seq[i : i + bs] for i in range(0, len(seq), bs)]  # noqa
            if isinstance(seq, tuple):
                return list(zip(*[get_batched(bs, s) for s in seq], strict=False))
            elif isinstance(seq, TensorWrapper):
                return [seq.slice_batch(slice(i, i + bs)) for i in range(0, len(seq), bs)]  # noqa
            else:
                raise TypeError(f"Unsupported type {type(seq)} for batched attribution computation.")
        if batch_size is None:
            out = f(self, *args, **kwargs)
            return out if isinstance(out, list) else [out]
        batched_args = [get_batched(batch_size, arg) for arg in args]
        len_batches = len(batched_args[0])
        assert all(len(batch) == len_batches for batch in batched_args)
        output = []
        zipped_batched_args = (
            zip(*batched_args, strict=False) if len(batched_args) > 1 else [(x,) for x in batched_args[0]]
        )
        for i, batch in enumerate(zipped_batched_args):
            logger.debug(f"Batching enabled: processing batch {i + 1} of {len_batches}...")
            out = f(self, *batch, **kwargs)
            output += out if isinstance(out, list) else [out]
        return output
    return batched_wrapper

================
File: inseq/attr/feat/__init__.py
================
from .attribution_utils import extract_args, join_token_ids
from .feature_attribution import FeatureAttribution, list_feature_attribution_methods
from .gradient_attribution import (
    DeepLiftAttribution,
    DiscretizedIntegratedGradientsAttribution,
    GradientAttributionRegistry,
    GradientShapAttribution,
    InputXGradientAttribution,
    IntegratedGradientsAttribution,
    LayerDeepLiftAttribution,
    LayerGradientXActivationAttribution,
    LayerIntegratedGradientsAttribution,
    SaliencyAttribution,
    SequentialIntegratedGradientsAttribution,
)
from .internals_attribution import AttentionWeightsAttribution, InternalsAttributionRegistry
from .perturbation_attribution import (
    LimeAttribution,
    OcclusionAttribution,
    PerturbationAttributionRegistry,
    ReagentAttribution,
    ValueZeroingAttribution,
)
__all__ = [
    "FeatureAttribution",
    "extract_args",
    "list_feature_attribution_methods",
    "join_token_ids",
    "GradientAttributionRegistry",
    "GradientShapAttribution",
    "DeepLiftAttribution",
    "InputXGradientAttribution",
    "IntegratedGradientsAttribution",
    "DiscretizedIntegratedGradientsAttribution",
    "SaliencyAttribution",
    "LayerIntegratedGradientsAttribution",
    "LayerGradientXActivationAttribution",
    "LayerDeepLiftAttribution",
    "InternalsAttributionRegistry",
    "AttentionWeightsAttribution",
    "OcclusionAttribution",
    "LimeAttribution",
    "SequentialIntegratedGradientsAttribution",
    "ValueZeroingAttribution",
    "PerturbationAttributionRegistry",
    "ReagentAttribution",
]

================
File: inseq/attr/feat/attribution_utils.py
================
import logging
import math
from collections.abc import Callable
from typing import TYPE_CHECKING, Any
from ...utils import extract_signature_args, get_aligned_idx
from ...utils.typing import (
    OneOrMoreAttributionSequences,
    OneOrMoreIdSequences,
    OneOrMoreTokenSequences,
    SingleScorePerStepTensor,
    StepAttributionTensor,
    TextInput,
    TokenWithId,
)
from ..step_functions import get_step_scores_args
if TYPE_CHECKING:
    from ...models import AttributionModel
    from .feature_attribution import FeatureAttribution
logger = logging.getLogger(__name__)
def tok2string(
    attribution_model: "AttributionModel",
    token_lists: OneOrMoreTokenSequences,
    start: int | None = None,
    end: int | None = None,
    as_targets: bool = True,
) -> TextInput:
    """Enables bounded tokenization of a list of lists of tokens with start and end positions."""
    start = [0 if start is None else start for _ in token_lists]
    end = [len(tokens) if end is None else end for tokens in token_lists]
    return attribution_model.convert_tokens_to_string(
        [tokens[start[i] : end[i]] for i, tokens in enumerate(token_lists)],  # noqa: E203
        as_targets=as_targets,
    )
def rescale_attributions_to_tokens(
    attributions: OneOrMoreAttributionSequences, tokens: OneOrMoreTokenSequences
) -> OneOrMoreAttributionSequences:
    return [
        attr[: len(tokens)] if not all(math.isnan(x) for x in attr) else []
        for attr, tokens in zip(attributions, tokens, strict=False)
    ]
def check_attribute_positions(
    max_length: int,
    attr_pos_start: int | None = None,
    attr_pos_end: int | None = None,
) -> tuple[int, int]:
    r"""Checks whether the combination of start/end positions for attribution is valid.
    Args:
        max_length (:obj:`int`): The maximum length of sequences in the batch.
        attr_pos_start (:obj:`int`, `optional`): The initial position for performing
            sequence attribution. Defaults to 1 (0 is the default BOS token).
        attr_pos_end (:obj:`int`, `optional`): The final position for performing sequence
            attribution. Defaults to None (full string).
    Raises:
        ValueError: If the start position is greater or equal than the end position or < 0.
    Returns:
        `tuple[int, int]`: The start and end positions for attribution.
    """
    if attr_pos_start is None:
        attr_pos_start = 1
    if attr_pos_end is None or attr_pos_end > max_length:
        attr_pos_end = max_length
    if attr_pos_start < -max_length:
        raise ValueError(f"Invalid starting position for attribution: {attr_pos_start}")
    if attr_pos_start < 0:
        attr_pos_start = max_length + attr_pos_start
    if attr_pos_end < -max_length:
        raise ValueError(f"Invalid ending position for attribution: {attr_pos_end}")
    if attr_pos_end < 0:
        attr_pos_end = max_length + attr_pos_end
    if attr_pos_start > attr_pos_end:
        raise ValueError(f"Invalid starting position for attribution: {attr_pos_start} > {attr_pos_end}")
    if attr_pos_start == attr_pos_end:
        raise ValueError("Start and end attribution positions cannot be the same.")
    return attr_pos_start, attr_pos_end
def join_token_ids(
    tokens: OneOrMoreTokenSequences,
    ids: OneOrMoreIdSequences,
    contrast_tokens: OneOrMoreTokenSequences | None = None,
    contrast_targets_alignments: list[list[tuple[int, int]]] | None = None,
) -> list[TokenWithId]:
    """Joins tokens and ids into a list of TokenWithId objects."""
    if contrast_tokens is None:
        contrast_tokens = tokens
    # 1:1 alignment between target and contrast tokens
    if contrast_targets_alignments is None:
        contrast_targets_alignments = [[(idx, idx) for idx, _ in enumerate(seq)] for seq in tokens]
    sequences = []
    for target_tokens_seq, contrast_target_tokens_seq, input_ids_seq, alignments_seq in zip(
        tokens, contrast_tokens, ids, contrast_targets_alignments, strict=False
    ):
        curr_seq = []
        for pos_idx, (token, token_idx) in enumerate(zip(target_tokens_seq, input_ids_seq, strict=False)):
            contrast_pos_idx = get_aligned_idx(pos_idx, alignments_seq)
            if contrast_pos_idx != -1 and token != contrast_target_tokens_seq[contrast_pos_idx]:
                curr_seq.append(TokenWithId(f"{contrast_target_tokens_seq[contrast_pos_idx]} â†’ {token}", -1))
            else:
                curr_seq.append(TokenWithId(token, token_idx))
        sequences.append(curr_seq)
    return sequences
def extract_args(
    attribution_method: "FeatureAttribution",
    attributed_fn: Callable[..., SingleScorePerStepTensor],
    step_scores: list[str],
    default_args: list[str],
    **kwargs,
) -> tuple[dict[str, Any], dict[str, Any], dict[str, Any]]:
    attribution_args = kwargs.pop("attribution_args", {})
    attributed_fn_args = kwargs.pop("attributed_fn_args", {})
    step_scores_args = kwargs.pop("step_scores_args", {})
    extra_attribution_args, attribution_unused_args = attribution_method.get_attribution_args(**kwargs)
    extra_attributed_fn_args, attributed_fn_unused_args = extract_signature_args(
        kwargs, attributed_fn, exclude_args=default_args, return_remaining=True
    )
    extra_step_scores_args = get_step_scores_args(step_scores, kwargs, default_args)
    step_scores_unused_args = {k: v for k, v in kwargs.items() if k not in extra_step_scores_args}
    unused_args = {
        k: v
        for k, v in kwargs.items()
        if k in attribution_unused_args.keys() & attributed_fn_unused_args.keys() & step_scores_unused_args.keys()
    }
    if unused_args:
        logger.warning(f"Unused arguments during attribution: {unused_args}")
    attribution_args.update(extra_attribution_args)
    attributed_fn_args.update(extra_attributed_fn_args)
    step_scores_args.update(extra_step_scores_args)
    return attribution_args, attributed_fn_args, step_scores_args
def get_source_target_attributions(
    attr: StepAttributionTensor | tuple[StepAttributionTensor, StepAttributionTensor],
    is_encoder_decoder: bool,
    has_sequence_scores: bool = False,
) -> tuple[StepAttributionTensor | None, StepAttributionTensor | None]:
    if isinstance(attr, tuple):
        if is_encoder_decoder:
            if has_sequence_scores:
                return (attr[0], attr[1], attr[2])
            else:
                return (attr[0], attr[1]) if len(attr) > 1 else (attr[0], None)
        else:
            return (None, None, attr[0]) if has_sequence_scores else (None, attr[0])
    else:
        return (attr, None) if is_encoder_decoder else (None, attr)

================
File: inseq/attr/feat/feature_attribution.py
================
# Copyright 2021 The Inseq Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""Feature attribution methods registry.
Todo:
    * ðŸŸ¡: Allow custom arguments for model loading in the :class:`FeatureAttribution` :meth:`load` method.
"""
import logging
from collections.abc import Callable
from datetime import datetime
from typing import TYPE_CHECKING, Any, Optional
import torch
from jaxtyping import Int
from ...data import (
    DecoderOnlyBatch,
    EncoderDecoderBatch,
    FeatureAttributionInput,
    FeatureAttributionOutput,
    FeatureAttributionSequenceOutput,
    FeatureAttributionStepOutput,
    get_batch_from_inputs,
)
from ...data.viz import close_progress_bar, get_progress_bar, update_progress_bar
from ...utils import (
    Registry,
    UnknownAttributionMethodError,
    available_classes,
    extract_signature_args,
    find_char_indexes,
    get_front_padding,
    pretty_tensor,
)
from ...utils.typing import ModelIdentifier, OneOrMoreTokenSequences, SingleScorePerStepTensor, TextSequences
from ..attribution_decorators import batched, set_hook, unset_hook
from ..step_functions import get_step_function, get_step_scores, get_step_scores_args
from .attribution_utils import (
    check_attribute_positions,
    get_source_target_attributions,
    tok2string,
)
if TYPE_CHECKING:
    from ...models import AttributionModel
logger = logging.getLogger(__name__)
class FeatureAttribution(Registry):
    r"""Abstract registry for feature attribution methods.
    Attributes:
        attr (:obj:`str`): Attribute of child classes that will act as lookup name
            for the registry.
        ignore_extra_args (:obj:`list` of :obj:`str`): Arguments used by default in the
            attribute step and thus ignored as extra arguments during attribution.
            The selection of defaults follows the `Captum <https://captum.ai/api/integrated_gradients.html>`__
            naming convention.
    """
    registry_attr = "method_name"
    ignore_extra_args = ["inputs", "baselines", "target", "additional_forward_args"]
    def __init__(self, attribution_model: "AttributionModel", hook_to_model: bool = True, **kwargs):
        r"""Common instantiation steps for FeatureAttribution methods. Hooks the attribution method
        to the model calling the :meth:`~inseq.attr.feat.FeatureAttribution.hook` method of the child class.
        Args:
            attribution_model (:class:`~inseq.models.AttributionModel`): The attribution model
                that is used to obtain predictions and on which attribution is performed.
            hook_to_model (:obj:`bool`, default `True`): Whether the attribution method should be
                hooked to the attribution model during initialization.
            **kwargs: Additional keyword arguments to pass to the hook method.
        Attributes:
            attribute_batch_ids (:obj:`bool`, default `False`): If True, the attribution method will receive batch ids
                instead of batch embeddings for attribution. Used by layer gradient-based attribution methods mapping
                saliency scores to the output of a layer instead of model inputs.
            forward_batch_embeds (:obj:`bool`, default `True`): If True, the model will use embeddings in the
                forward pass instead of token ids. Using this in combination with `attribute_batch_ids` will allow for
                custom conversion of ids into embeddings inside the attribution method.
            target_layer (:obj:`torch.nn.Module`, default `None`): The layer on which attribution should be
                performed for layer attribution methods.
            use_baselines (:obj:`bool`, default `False`): Whether a baseline should be used for the attribution method.
            use_attention_weights (:obj:`bool`, default `False`): Whether attention weights are used in the attribution
                method.
            use_hidden_states (:obj:`bool`, default `False`): Whether hidden states are used in the attribution method.
            use_predicted_target (:obj:`bool`, default `True`): Whether the attribution method uses the predicted
                target for attribution. In case it doesn't, a warning message will be shown if the target is not
                the default one.
            use_model_config (:obj:`bool`, default `False`): Whether the attribution method uses the model config. If
                True, the method will try to load the config matching the model when hooking to the model. Missing
                configurations can be registered using :meth:`~inseq.models.register_model_config`.
        """
        super().__init__()
        self.attribution_model = attribution_model
        self.attribute_batch_ids: bool = False
        self.forward_batch_embeds: bool = True
        self.target_layer = None
        self.use_baselines: bool = False
        self.use_attention_weights: bool = False
        self.use_hidden_states: bool = False
        self.use_predicted_target: bool = True
        self.use_model_config: bool = False
        self.is_final_step_method: bool = False
        if hook_to_model:
            self.hook(**kwargs)
    @classmethod
    def load(
        cls,
        method_name: str,
        attribution_model: Optional["AttributionModel"] = None,
        model_name_or_path: ModelIdentifier | None = None,
        **kwargs,
    ) -> "FeatureAttribution":
        r"""Load the selected method and hook it to an existing or available
        attribution model.
        Args:
            method_name (:obj:`str`): The name of the attribution method to load.
            attribution_model (:class:`~inseq.models.AttributionModel`, `optional`): An instance of an
                :class:`~inseq.models.AttributionModel` child class. If not provided, the method
                will try to load the model from the model_name_or_path argument. Defaults to None.
            model_name_or_path (:obj:`ModelIdentifier`, `optional`): The name of the model to load or its
                path on disk. If not provided, an instantiated model must be provided. If the model is loaded
                in this way, the model will be created with default arguments. Defaults to None.
            **kwargs: Additional arguments to pass to the attribution method :obj:`__init__` function.
        Raises:
            :obj:`RuntimeError`: Raised if both or neither model_name_or_path and attribution_model are
                provided.
            :obj:`UnknownAttributionMethodError`: Raised if the method_name is not found in the registry.
        Returns:
            :class:`~inseq.attr.feat.FeatureAttribution`: The loaded attribution method.
        """
        from ...models import load_model
        methods = cls.available_classes()
        if method_name not in methods:
            raise UnknownAttributionMethodError(method_name)
        if model_name_or_path is not None:
            model = load_model(model_name_or_path)
        elif attribution_model is not None:
            model = attribution_model
        else:
            raise RuntimeError(
                "Only one among an initialized model and a model identifier "
                "must be defined when loading the attribution method."
            )
        return methods[method_name](model, **kwargs)
    @batched
    def prepare_and_attribute(
        self,
        sources: FeatureAttributionInput,
        targets: FeatureAttributionInput,
        attr_pos_start: int | None = None,
        attr_pos_end: int | None = None,
        show_progress: bool = True,
        pretty_progress: bool = True,
        output_step_attributions: bool = False,
        attribute_target: bool = False,
        step_scores: list[str] = [],
        include_eos_baseline: bool = False,
        skip_special_tokens: bool = False,
        clean_special_chars: bool = False,
        attributed_fn: str | Callable[..., SingleScorePerStepTensor] | None = None,
        attribution_args: dict[str, Any] = {},
        attributed_fn_args: dict[str, Any] = {},
        step_scores_args: dict[str, Any] = {},
    ) -> FeatureAttributionOutput:
        r"""Prepares inputs and performs attribution.
        Wraps the attribution method :meth:`~inseq.attr.feat.FeatureAttribution.attribute` method
        and the :meth:`~inseq.models.InputFormatter.prepare_inputs_for_attribution` method.
        Args:
            sources (:obj:`FeatureAttributionInput`): The sources provided to the
                :meth:`~inseq.attr.feat.FeatureAttribution.prepare` method.
            targets (:obj:`FeatureAttributionInput`): The targets provided to the
                :meth:`~inseq.attr.feat.FeatureAttribution.prepare` method.
            attr_pos_start (:obj:`int`, `optional`): The initial position for performing
                sequence attribution. Defaults to 0.
            attr_pos_end (:obj:`int`, `optional`): The final position for performing sequence
                attribution. Defaults to None (full string).
            show_progress (:obj:`bool`, `optional`): Whether to show a progress bar. Defaults to True.
            pretty_progress (:obj:`bool`, `optional`): Whether to use a pretty progress bar. Defaults to True.
            output_step_attributions (:obj:`bool`, `optional`): Whether to output a list of
                FeatureAttributionStepOutput objects for each step. Defaults to False.
            attribute_target (:obj:`bool`, `optional`): Whether to include target prefix for feature attribution.
                Defaults to False.
            step_scores (:obj:`list` of `str`): List of identifiers for step scores that need to be computed during
                attribution. The available step scores are defined in :obj:`inseq.attr.feat.STEP_SCORES_MAP` and new
                step scores can be added by using the :meth:`~inseq.register_step_function` function.
            include_eos_baseline (:obj:`bool`, `optional`): Whether to include the EOS token in the baseline for
                attribution. By default the EOS token is not used for attribution. Defaults to False.
            skip_special_tokens (:obj:`bool`, `optional`): Whether to skip special tokens when encoding the input.
                Defaults to False.
            clean_special_chars (:obj:`bool`, `optional`): Whether to clean special characters from the input and the
                generated tokens. Defaults to False.
            attributed_fn (:obj:`str` or :obj:`Callable[..., SingleScorePerStepTensor]`, `optional`): The identifier or
                function of model outputs representing what should be attributed (e.g. output probits of model best
                prediction after softmax). If it is a string, it must be a valid function.
                Otherwise, it must be a function that taking multiple keyword arguments and returns a :obj:`tensor`
                of size (batch_size,). If not provided, the default attributed function for the model will be used
                (change attribution_model.default_attributed_fn_id).
            attribution_args (:obj:`dict`, `optional`): Additional arguments to pass to the attribution method.
            attributed_fn_args (:obj:`dict`, `optional`): Additional arguments to pass to the attributed function.
            step_scores_args (:obj:`dict`, `optional`): Additional arguments to pass to the step scores functions.
        Returns:
            :class:`~inseq.data.FeatureAttributionOutput`: An object containing a list of sequence attributions, with
                an optional added list of single :class:`~inseq.data.FeatureAttributionStepOutput` for each step and
                extra information regarding the attribution parameters.
        """
        inputs = (sources, targets)
        if not self.attribution_model.is_encoder_decoder:
            inputs = targets
            encoded_sources = self.attribution_model.encode(
                sources, return_baseline=True, add_special_tokens=not skip_special_tokens
            )
            # We do this here to support separate attr_pos_start for different sentences when batching
            if attr_pos_start is None or attr_pos_start < encoded_sources.input_ids.shape[1]:
                attr_pos_start = encoded_sources.input_ids.shape[1]
        batch = self.attribution_model.formatter.prepare_inputs_for_attribution(
            self.attribution_model, inputs, include_eos_baseline, skip_special_tokens
        )
        # If prepare_and_attribute was called from AttributionModel.attribute,
        # attributed_fn is already a Callable. Keep here to allow for usage independently
        # of AttributionModel.attribute.
        attributed_fn = self.attribution_model.get_attributed_fn(attributed_fn)
        attribution_output = self.attribute(
            batch,
            attributed_fn=attributed_fn,
            attr_pos_start=attr_pos_start,
            attr_pos_end=attr_pos_end,
            show_progress=show_progress,
            pretty_progress=pretty_progress,
            output_step_attributions=output_step_attributions,
            attribute_target=attribute_target,
            step_scores=step_scores,
            skip_special_tokens=skip_special_tokens,
            clean_special_chars=clean_special_chars,
            attribution_args=attribution_args,
            attributed_fn_args=attributed_fn_args,
            step_scores_args=step_scores_args,
        )
        # Same here, repeated from AttributionModel.attribute
        # to allow independent usage
        attribution_output.info["include_eos_baseline"] = include_eos_baseline
        attribution_output.info["attributed_fn"] = attributed_fn.__name__
        attribution_output.info["attribution_args"] = attribution_args
        attribution_output.info["attributed_fn_args"] = attributed_fn_args
        attribution_output.info["step_scores_args"] = step_scores_args
        return attribution_output
    def _run_compatibility_checks(self, attributed_fn) -> None:
        default_attributed_fn = get_step_function(self.attribution_model.default_attributed_fn_id)
        if not self.use_predicted_target and attributed_fn != default_attributed_fn:
            logger.warning(
                "Internals attribution methods are output agnostic, since they do not rely on specific output"
                " targets to compute importance scores. Using a custom attributed function in this context does not"
                " influence in any way the method's results."
            )
        if self.use_model_config and self.attribution_model.is_distributed:
            raise RuntimeError(
                "Distributed models are incompatible with attribution methods requiring access to models' internals "
                "for storing or intervention purposes. Please use a non-distributed model with the current attribution"
                " method."
            )
    @staticmethod
    def _build_multistep_output_from_single_step(
        single_step_output: FeatureAttributionStepOutput,
        attr_pos_start: int,
        attr_pos_end: int,
    ) -> list[FeatureAttributionStepOutput]:
        if single_step_output.step_scores:
            raise ValueError("step_scores are not supported for final step attribution methods.")
        num_seq = len(single_step_output.prefix)
        steps = []
        for pos_idx in range(attr_pos_start, attr_pos_end):
            step_output = single_step_output.clone_empty()
            step_output.source = single_step_output.source
            step_output.prefix = [single_step_output.prefix[seq_idx][:pos_idx] for seq_idx in range(num_seq)]
            step_output.target = (
                single_step_output.target
                if pos_idx == attr_pos_end - 1
                else [[single_step_output.prefix[seq_idx][pos_idx]] for seq_idx in range(num_seq)]
            )
            if single_step_output.source_attributions is not None:
                step_output.source_attributions = single_step_output.source_attributions[:, :, pos_idx - 1]
            if single_step_output.target_attributions is not None:
                step_output.target_attributions = single_step_output.target_attributions[:, :pos_idx, pos_idx - 1]
            single_step_output.step_scores = {}
            if single_step_output.sequence_scores is not None:
                step_output.sequence_scores = single_step_output.sequence_scores
            steps.append(step_output)
        return steps
    def format_contrastive_targets(
        self,
        target_sequences: TextSequences,
        target_tokens: OneOrMoreTokenSequences,
        attributed_fn_args: dict[str, Any],
        step_scores_args: dict[str, Any],
        attr_pos_start: int,
        attr_pos_end: int,
        skip_special_tokens: bool = False,
    ) -> tuple[DecoderOnlyBatch | None, list[list[tuple[int, int]]] | None, dict[str, Any], dict[str, Any]]:
        contrast_batch, contrast_targets_alignments = None, None
        contrast_targets = attributed_fn_args.get("contrast_targets", None)
        if contrast_targets is None:
            contrast_targets = step_scores_args.get("contrast_targets", None)
        contrast_targets_alignments = attributed_fn_args.get("contrast_targets_alignments", None)
        if contrast_targets_alignments is None:
            contrast_targets_alignments = step_scores_args.get("contrast_targets_alignments", None)
        if contrast_targets_alignments is not None and contrast_targets is None:
            raise ValueError("contrast_targets_alignments requires contrast_targets to be specified.")
        contrast_targets = [contrast_targets] if isinstance(contrast_targets, str) else contrast_targets
        if contrast_targets is not None:
            as_targets = self.attribution_model.is_encoder_decoder
            contrast_batch = get_batch_from_inputs(
                attribution_model=self.attribution_model,
                inputs=contrast_targets,
                as_targets=as_targets,
                skip_special_tokens=skip_special_tokens,
            )
            contrast_batch = DecoderOnlyBatch.from_batch(contrast_batch)
            clean_tgt_tokens = self.attribution_model.clean_tokens(target_tokens, as_targets=as_targets)
            clean_c_tokens = self.attribution_model.clean_tokens(contrast_batch.target_tokens, as_targets=as_targets)
            contrast_targets_alignments = self.attribution_model.formatter.format_contrast_targets_alignments(
                contrast_targets_alignments=contrast_targets_alignments,
                target_sequences=target_sequences,
                target_tokens=clean_tgt_tokens,
                contrast_sequences=contrast_targets,
                contrast_tokens=clean_c_tokens,
                special_tokens=self.attribution_model.special_tokens,
                start_pos=attr_pos_start,
                end_pos=attr_pos_end,
            )
            if "contrast_targets" in step_scores_args:
                step_scores_args["contrast_targets_alignments"] = contrast_targets_alignments
            if "contrast_targets" in attributed_fn_args:
                attributed_fn_args["contrast_targets_alignments"] = contrast_targets_alignments
        return contrast_batch, contrast_targets_alignments, attributed_fn_args, step_scores_args
    def attribute(
        self,
        batch: DecoderOnlyBatch | EncoderDecoderBatch,
        attributed_fn: Callable[..., SingleScorePerStepTensor],
        attr_pos_start: int | None = None,
        attr_pos_end: int | None = None,
        show_progress: bool = True,
        pretty_progress: bool = True,
        output_step_attributions: bool = False,
        attribute_target: bool = False,
        step_scores: list[str] = [],
        skip_special_tokens: bool = False,
        clean_special_chars: bool = False,
        attribution_args: dict[str, Any] = {},
        attributed_fn_args: dict[str, Any] = {},
        step_scores_args: dict[str, Any] = {},
    ) -> FeatureAttributionOutput:
        r"""Performs the feature attribution procedure using the specified attribution method.
        Args:
            batch (:class:`~inseq.data.EncoderDecoderBatch` or :class:`~inseq.data.DecoderOnlyBatch`): The batch of
                sequences to attribute.
            attributed_fn (:obj:`Callable[..., SingleScorePerStepTensor]`): The function of model
                outputs representing what should be attributed (e.g. output probits of model best
                prediction after softmax). It must be a function that taking multiple keyword
                arguments and returns a :obj:`tensor` of size (batch_size,). If not provided,
                the default attributed function for the model will be used.
            attr_pos_start (:obj:`int`, `optional`): The initial position for performing
                sequence attribution. Defaults to 1 (0 is the default BOS token).
            attr_pos_end (:obj:`int`, `optional`): The final position for performing sequence
                attribution. Defaults to None (full string).
            show_progress (:obj:`bool`, `optional`): Whether to show a progress bar. Defaults to True.
            pretty_progress (:obj:`bool`, `optional`): Whether to use a pretty progress bar. Defaults to True.
            output_step_attributions (:obj:`bool`, `optional`): Whether to output a list of
                FeatureAttributionStepOutput objects for each step. Defaults to False.
            attribute_target (:obj:`bool`, `optional`): Whether to include target prefix for feature attribution.
                Defaults to False.
            step_scores (:obj:`list` of `str`): List of identifiers for step scores that need to be computed during
                attribution. The available step scores are defined in :obj:`inseq.attr.feat.STEP_SCORES_MAP` and new
                step scores can be added by using the :meth:`~inseq.register_step_function` function.
            skip_special_tokens (:obj:`bool`, `optional`): Whether to skip special tokens when encoding the input.
                Defaults to False.
            clean_special_chars (:obj:`bool`, `optional`): Whether to clean special characters from the input and the
                generated tokens. Defaults to False.
            attribution_args (:obj:`dict`, `optional`): Additional arguments to pass to the attribution method.
            attributed_fn_args (:obj:`dict`, `optional`): Additional arguments to pass to the attributed function.
            step_scores_args (:obj:`dict`, `optional`): Additional arguments to pass to the step scores function.
        Returns:
            :class:`~inseq.data.FeatureAttributionOutput`: An object containing a list of sequence attributions, with
                an optional added list of single :class:`~inseq.data.FeatureAttributionStepOutput` for each step and
                extra information regarding the attribution parameters.
        """
        if self.attribute_batch_ids and not self.forward_batch_embeds and attribute_target:
            raise ValueError(
                "Layer attribution methods do not support attribute_target=True. Use regular attributions instead."
            )
        self._run_compatibility_checks(attributed_fn)
        attr_pos_start, attr_pos_end = check_attribute_positions(
            batch.max_generation_length,
            attr_pos_start,
            attr_pos_end,
        )
        logger.debug("=" * 30 + f"\nfull batch: {batch}\n" + "=" * 30)
        # Sources are empty for decoder-only models
        sequences = self.attribution_model.formatter.get_text_sequences(self.attribution_model, batch)
        (
            contrast_batch,
            contrast_targets_alignments,
            attributed_fn_args,
            step_scores_args,
        ) = self.format_contrastive_targets(
            sequences.targets,
            batch.target_tokens,
            attributed_fn_args,
            step_scores_args,
            attr_pos_start,
            attr_pos_end,
            skip_special_tokens,
        )
        target_tokens_with_ids = self.attribution_model.get_token_with_ids(
            batch,
            contrast_target_tokens=contrast_batch.target_tokens if contrast_batch is not None else None,
            contrast_targets_alignments=contrast_targets_alignments,
        )
        # Manages front padding for decoder-only models, using 0 as lower bound
        # when attr_pos_start exceeds target length.
        targets_lengths = [
            max(
                0,
                min(attr_pos_end, len(target_tokens_with_ids[idx]))
                - (attr_pos_start + 1)
                + get_front_padding(batch.target_mask)[idx],
            )
            for idx in range(len(target_tokens_with_ids))
        ]
        if self.attribution_model.is_encoder_decoder:
            iter_pos_end = min(attr_pos_end + 1, batch.max_generation_length)
        else:
            iter_pos_end = attr_pos_end
        pbar = get_progress_bar(
            sequences=sequences,
            target_lengths=targets_lengths,
            method_name=self.method_name,
            show=show_progress,
            pretty=False if self.is_final_step_method else pretty_progress,
            attr_pos_start=attr_pos_start,
            attr_pos_end=1 if self.is_final_step_method else attr_pos_end,
        )
        whitespace_indexes = find_char_indexes(sequences.targets, " ")
        attribution_outputs = []
        start = datetime.now()
        # Attribution loop for generation
        for step in range(attr_pos_start, iter_pos_end):
            if self.is_final_step_method and step != iter_pos_end - 1:
                continue
            tgt_ids, tgt_mask = batch.get_step_target(step, with_attention=True)
            step_output = self.filtered_attribute_step(
                batch[:step],
                target_ids=tgt_ids.unsqueeze(1),
                attributed_fn=attributed_fn,
                target_attention_mask=tgt_mask.unsqueeze(1),
                attribute_target=attribute_target,
                step_scores=step_scores,
                attribution_args=attribution_args,
                attributed_fn_args=attributed_fn_args,
                step_scores_args=step_scores_args,
            )
            # Add batch information to output
            step_output = self.attribution_model.formatter.enrich_step_output(
                self.attribution_model,
                step_output,
                batch[:step],
                self.attribution_model.convert_ids_to_tokens(tgt_ids.unsqueeze(1), skip_special_tokens=False),
                tgt_ids.detach().to("cpu"),
                contrast_batch=contrast_batch,
                contrast_targets_alignments=contrast_targets_alignments,
            )
            attribution_outputs.append(step_output)
            if pretty_progress and not self.is_final_step_method:
                tgt_tokens = batch.target_tokens
                skipped_prefixes = tok2string(self.attribution_model, tgt_tokens, end=attr_pos_start)
                attributed_sentences = tok2string(self.attribution_model, tgt_tokens, attr_pos_start, step + 1)
                unattributed_suffixes = tok2string(self.attribution_model, tgt_tokens, step + 1, attr_pos_end)
                skipped_suffixes = tok2string(self.attribution_model, tgt_tokens, start=attr_pos_end)
                update_progress_bar(
                    pbar,
                    skipped_prefixes,
                    attributed_sentences,
                    unattributed_suffixes,
                    skipped_suffixes,
                    whitespace_indexes,
                    show=show_progress,
                    pretty=True,
                )
            else:
                update_progress_bar(pbar, show=show_progress, pretty=False)
        end = datetime.now()
        close_progress_bar(pbar, show=show_progress, pretty=False if self.is_final_step_method else pretty_progress)
        batch.detach().to("cpu")
        if self.is_final_step_method:
            attribution_outputs = self._build_multistep_output_from_single_step(
                attribution_outputs[0],
                attr_pos_start=attr_pos_start,
                attr_pos_end=iter_pos_end,
            )
        if clean_special_chars:
            for out in attribution_outputs:
                out.source = self.attribution_model.clean_tokens(out.source) if out.source is not None else None
                out.prefix = (
                    self.attribution_model.clean_tokens(out.prefix, as_targets=True)
                    if out.prefix is not None
                    else None
                )
                out.target = (
                    self.attribution_model.clean_tokens(out.target, as_targets=True)
                    if out.target is not None
                    else None
                )
            target_tokens_with_ids = self.attribution_model.clean_tokens(target_tokens_with_ids, as_targets=True)
        out = FeatureAttributionOutput(
            sequence_attributions=FeatureAttributionSequenceOutput.from_step_attributions(
                attributions=attribution_outputs,
                tokenized_target_sentences=target_tokens_with_ids,
                pad_token=self.attribution_model.pad_token,
                attr_pos_end=attr_pos_end,
            ),
            step_attributions=attribution_outputs if output_step_attributions else None,
            info={
                "attribution_method": self.method_name,
                "attr_pos_start": attr_pos_start,
                "attr_pos_end": attr_pos_end,
                "output_step_attributions": output_step_attributions,
                "attribute_target": attribute_target,
                "step_scores": step_scores,
                # Convert to datetime.timedelta as timedelta(seconds=exec_time)
                "exec_time": (end - start).total_seconds(),
            },
        )
        out.info.update(self.attribution_model.info)
        return out
    def filtered_attribute_step(
        self,
        batch: DecoderOnlyBatch | EncoderDecoderBatch,
        target_ids: Int[torch.Tensor, "batch_size 1"],
        attributed_fn: Callable[..., SingleScorePerStepTensor],
        target_attention_mask: Int[torch.Tensor, "batch_size 1"] | None = None,
        attribute_target: bool = False,
        step_scores: list[str] = [],
        attribution_args: dict[str, Any] = {},
        attributed_fn_args: dict[str, Any] = {},
        step_scores_args: dict[str, Any] = {},
    ) -> FeatureAttributionStepOutput:
        r"""Performs a single attribution step for all the sequences in the batch that
        still have valid target_ids, as identified by the target_attention_mask.
        Finished sentences are temporarily filtered out to make the attribution step
        faster and then reinserted before returning.
        Args:
            batch (:class:`~inseq.data.EncoderDecoderBatch` or :class:`~inseq.data.DecoderOnlyBatch`): The batch of
                sequences to attribute.
            target_ids (:obj:`torch.Tensor`): Target token ids of size `(batch_size, 1)` corresponding to tokens
                for which the attribution step must be performed.
            attributed_fn (:obj:`Callable[..., SingleScorePerStepTensor]`): The function of model outputs
                representing what should be attributed (e.g. output probits of model best prediction after softmax).
                The parameter must be a function that taking multiple keyword arguments and returns a :obj:`tensor`
                of size (batch_size,). If not provided, the default attributed function for the model will be used
                (change attribution_model.default_attributed_fn_id).
            target_attention_mask (:obj:`torch.Tensor`, `optional`): Boolean attention mask of size `(batch_size, 1)`
                specifying which target_ids are valid for attribution and which are padding.
            attribute_target (:obj:`bool`, `optional`): Whether to include target prefix for feature attribution.
                Defaults to False.
            step_scores (:obj:`list` of `str`): List of identifiers for step scores that need to be computed during
                attribution. The available step scores are defined in :obj:`inseq.attr.feat.STEP_SCORES_MAP` and new
                step scores can be added by using the :meth:`~inseq.register_step_function` function.
            attribution_args (:obj:`dict`, `optional`): Additional arguments to pass to the attribution method.
            attributed_fn_args (:obj:`dict`, `optional`): Additional arguments to pass to the attributed function.
            step_scores_args (:obj:`dict`, `optional`): Additional arguments to pass to the step scores functions.
        Returns:
            :class:`~inseq.data.FeatureAttributionStepOutput`: A dataclass containing attribution tensors for source
                and target attributions of size `(batch_size, source_length)` and `(batch_size, prefix length)`.
                (target optional if attribute_target=True), plus batch information and any step score present.
        """
        orig_batch = batch.clone().detach().to("cpu")
        is_filtered = False
        # Filter out finished sentences
        if target_attention_mask is not None and int(target_attention_mask.sum()) < target_ids.shape[0]:
            batch = batch.select_active(target_attention_mask)
            target_ids = target_ids.masked_select(target_attention_mask.bool())
            target_ids = target_ids.view(-1, 1)
            is_filtered = True
        target_ids = target_ids.squeeze()
        logger.debug(
            f"\ntarget_ids: {pretty_tensor(target_ids)},\n"
            f"target_attention_mask: {pretty_tensor(target_attention_mask)}"
        )
        logger.debug(f"batch: {batch},\ntarget_ids: {pretty_tensor(target_ids, lpad=4)}")
        attribute_main_args = self.attribution_model.formatter.format_attribution_args(
            batch=batch,
            target_ids=target_ids,
            attributed_fn=attributed_fn,
            attribute_target=attribute_target,
            attributed_fn_args=attributed_fn_args,
            attribute_batch_ids=self.attribute_batch_ids,
            forward_batch_embeds=self.forward_batch_embeds,
            use_baselines=self.use_baselines,
        )
        if len(step_scores) > 0 or self.use_attention_weights or self.use_hidden_states:
            with torch.no_grad():
                output = self.attribution_model.get_forward_output(
                    batch,
                    use_embeddings=self.forward_batch_embeds,
                    output_attentions=self.use_attention_weights,
                    output_hidden_states=self.use_hidden_states,
                )
            if self.use_attention_weights:
                attentions_dict = self.attribution_model.get_attentions_dict(output)
                attribution_args = {**attribution_args, **attentions_dict}
            if self.use_hidden_states:
                hidden_states_dict = self.attribution_model.get_hidden_states_dict(output)
                attribution_args = {**attribution_args, **hidden_states_dict}
        # Perform attribution step
        step_output = self.attribute_step(
            attribute_main_args,
            attribution_args,
        )
        # Format step scores arguments and calculate step scores
        for score in step_scores:
            step_fn_args = self.attribution_model.formatter.format_step_function_args(
                attribution_model=self.attribution_model,
                forward_output=output,
                target_ids=target_ids,
                is_attributed_fn=False,
                batch=batch,
            )
            step_fn_extra_args = get_step_scores_args([score], step_scores_args)
            step_output.step_scores[score] = get_step_scores(score, step_fn_args, step_fn_extra_args).to("cpu")
        # Reinsert finished sentences
        if target_attention_mask is not None and is_filtered:
            step_output.remap_from_filtered(target_attention_mask, orig_batch, self.is_final_step_method)
        step_output = step_output.detach().to("cpu")
        return step_output
    def get_attribution_args(self, **kwargs) -> tuple[dict[str, Any], dict[str, Any]]:
        if hasattr(self, "method") and hasattr(self.method, "attribute"):
            return extract_signature_args(kwargs, self.method.attribute, self.ignore_extra_args, return_remaining=True)
        return {}, {}
    def attribute_step(
        self,
        attribute_fn_main_args: dict[str, Any],
        attribution_args: dict[str, Any] = {},
    ) -> FeatureAttributionStepOutput:
        r"""Performs a single attribution step for the specified attribution arguments.
        Args:
            attribute_fn_main_args (:obj:`dict`): Main arguments used for the attribution method. These are built from
                model inputs at the current step of the feature attribution process.
            attribution_args (:obj:`dict`, `optional`): Additional arguments to pass to the attribution method.
                These can be specified by the user while calling the top level `attribute` methods. Defaults to {}.
        Returns:
            :class:`~inseq.data.FeatureAttributionStepOutput`: A dataclass containing a tensor of source
                attributions of size `(batch_size, source_length)`. At this point the batch
                information is empty, and will later be filled by the enrich_step_output function.
        """
        attr = self.method.attribute(**attribute_fn_main_args, **attribution_args)
        source_attributions, target_attributions = get_source_target_attributions(
            attr, self.attribution_model.is_encoder_decoder
        )
        return FeatureAttributionStepOutput(
            source_attributions=source_attributions,
            target_attributions=target_attributions,
            step_scores={},
        )
    @set_hook
    def hook(self, **kwargs) -> None:
        r"""Hooks the attribution method to the model. Useful to implement pre-attribution logic
        (e.g. freezing layers, replacing embeddings, raise warnings, etc.).
        """
        from ...models.model_config import get_model_config
        if self.use_model_config and self.attribution_model is not None:
            self.attribution_model.config = get_model_config(self.attribution_model.info["model_class"])
    @unset_hook
    def unhook(self, **kwargs) -> None:
        r"""Unhooks the attribution method from the model. If the model was modified in any way, this
        should restore its initial state.
        """
        if self.use_model_config and self.attribution_model is not None:
            self.attribution_model.config = None
def list_feature_attribution_methods():
    """Lists identifiers for all available feature attribution methods. A feature attribution method identifier (e.g.
    `integrated_gradients`) can be passed to :class:`~inseq.models.AttributionModel` or :meth:`~inseq.load_model`
    to define a model for attribution.
    """
    return available_classes(FeatureAttribution)
class DummyAttribution(FeatureAttribution):
    """Dummy attribution method that returns empty attributions."""
    method_name = "dummy"
    def attribute_step(
        self, attribute_fn_main_args: dict[str, Any], attribution_args: dict[str, Any] = {}
    ) -> FeatureAttributionStepOutput:
        return FeatureAttributionStepOutput(
            source_attributions=None,
            target_attributions=None,
            step_scores={},
        )

================
File: inseq/attr/feat/gradient_attribution.py
================
# Copyright 2021 The Inseq Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""Gradient-based feature attribution methods."""
import logging
from typing import Any
from captum.attr import (
    DeepLift,
    GradientShap,
    InputXGradient,
    IntegratedGradients,
    LayerDeepLift,
    LayerGradientXActivation,
    LayerIntegratedGradients,
    Saliency,
)
from ...data import GranularFeatureAttributionStepOutput
from ...utils import Registry, extract_signature_args, rgetattr
from ..attribution_decorators import set_hook, unset_hook
from .attribution_utils import get_source_target_attributions
from .feature_attribution import FeatureAttribution
from .ops import DiscretetizedIntegratedGradients, SequentialIntegratedGradients
logger = logging.getLogger(__name__)
class GradientAttributionRegistry(FeatureAttribution, Registry):
    r"""Gradient-based attribution method registry."""
    @set_hook
    def hook(self, **kwargs):
        r"""Hooks the attribution method to the model by replacing normal :obj:`nn.Embedding` with Captum's
        `InterpretableEmbeddingBase <https://captum.ai/api/utilities.html#captum.attr.InterpretableEmbeddingBase>`__.
        """
        super().hook(**kwargs)
        if self.attribute_batch_ids and not self.forward_batch_embeds:
            self.target_layer = kwargs.pop("target_layer", self.attribution_model.get_embedding_layer())
            logger.debug(f"target_layer={self.target_layer}")
            if isinstance(self.target_layer, str):
                self.target_layer = rgetattr(self.attribution_model.model, self.target_layer)
        if not self.attribute_batch_ids:
            self.attribution_model.configure_interpretable_embeddings()
    @unset_hook
    def unhook(self, **kwargs):
        r"""Unhook the attribution method by restoring the model's original embeddings."""
        super().hook(**kwargs)
        if self.attribute_batch_ids and not self.forward_batch_embeds:
            self.target_layer = None
        else:
            self.attribution_model.remove_interpretable_embeddings()
    def attribute_step(
        self,
        attribute_fn_main_args: dict[str, Any],
        attribution_args: dict[str, Any] = {},
    ) -> GranularFeatureAttributionStepOutput:
        r"""Performs a single attribution step for the specified attribution arguments.
        Args:
            attribute_fn_main_args (:obj:`dict`): Main arguments used for the attribution method. These are built from
                model inputs at the current step of the feature attribution process.
            attribution_args (:obj:`dict`, `optional`): Additional arguments to pass to the attribution method.
                These can be specified by the user while calling the top level `attribute` methods. Defaults to {}.
        Returns:
            :class:`~inseq.data.GranularFeatureAttributionStepOutput`: A dataclass containing a tensor of source
                attributions of size `(batch_size, source_length)`, possibly a tensor of target attributions of size
                `(batch_size, prefix length) if attribute_target=True and possibly a tensor of deltas of size
                `(batch_size)` if the attribution step supports deltas and they are requested. At this point the batch
                information is empty, and will later be filled by the enrich_step_output function.
        """
        attr = self.method.attribute(**attribute_fn_main_args, **attribution_args)
        deltas = None
        if (
            attribution_args.get("return_convergence_delta", False)
            and hasattr(self.method, "has_convergence_delta")
            and self.method.has_convergence_delta()
        ):
            attr, deltas = attr
        source_attributions, target_attributions = get_source_target_attributions(
            attr, self.attribution_model.is_encoder_decoder
        )
        return GranularFeatureAttributionStepOutput(
            source_attributions=source_attributions if source_attributions is not None else None,
            target_attributions=target_attributions if target_attributions is not None else None,
            step_scores={"deltas": deltas} if deltas is not None else None,
        )
class DeepLiftAttribution(GradientAttributionRegistry):
    """DeepLIFT attribution method.
    Reference implementation:
    `https://captum.ai/api/deep_lift.html <https://captum.ai/api/deep_lift.html>`__.
    """
    method_name = "deeplift"
    def __init__(self, attribution_model, multiply_by_inputs: bool = True, **kwargs):
        super().__init__(attribution_model)
        self.method = DeepLift(self.attribution_model, multiply_by_inputs)
        self.use_baselines = True
class GradientShapAttribution(GradientAttributionRegistry):
    """GradientSHAP attribution method.
    Reference implementation:
    `https://captum.ai/api/gradient_shap.html <https://captum.ai/api/gradient_shap.html>`__.
    """
    method_name = "gradient_shap"
    def __init__(self, attribution_model, multiply_by_inputs: bool = True, **kwargs):
        super().__init__(attribution_model)
        self.method = GradientShap(self.attribution_model, multiply_by_inputs)
        self.use_baselines = True
class DiscretizedIntegratedGradientsAttribution(GradientAttributionRegistry):
    """Discretized Integrated Gradients attribution method.
    Reference: https://arxiv.org/abs/2108.13654
    Original implementation: https://github.com/INK-USC/DIG
    """
    method_name = "discretized_integrated_gradients"
    def __init__(self, attribution_model, multiply_by_inputs: bool = False, **kwargs):
        super().__init__(attribution_model, hook_to_model=False)
        self.attribution_model = attribution_model
        self.attribute_batch_ids = True
        self.use_baselines = True
        self.method = DiscretetizedIntegratedGradients(
            self.attribution_model,
            multiply_by_inputs,
        )
        self.hook(**kwargs)
    @set_hook
    def hook(self, **kwargs):
        load_kwargs, other_kwargs = extract_signature_args(
            kwargs,
            self.method.load_monotonic_path_builder,
            return_remaining=True,
        )
        self.method.load_monotonic_path_builder(
            self.attribution_model.model_name,
            vocabulary_embeddings=self.attribution_model.vocabulary_embeddings.detach(),
            special_tokens=self.attribution_model.special_tokens_ids,
            embedding_scaling=self.attribution_model.embed_scale,
            **load_kwargs,
        )
        super().hook(**other_kwargs)
class IntegratedGradientsAttribution(GradientAttributionRegistry):
    """Integrated Gradients attribution method.
    Reference implementation:
    `https://captum.ai/api/integrated_gradients.html <https://captum.ai/api/integrated_gradients.html>`__.
    """
    method_name = "integrated_gradients"
    def __init__(self, attribution_model, multiply_by_inputs: bool = True, **kwargs):
        super().__init__(attribution_model)
        self.method = IntegratedGradients(self.attribution_model, multiply_by_inputs)
        self.use_baselines = True
class InputXGradientAttribution(GradientAttributionRegistry):
    """Input x Gradient attribution method.
    Reference implementation:
    `https://captum.ai/api/input_x_gradient.html <https://captum.ai/api/input_x_gradient.html>`__.
    """
    method_name = "input_x_gradient"
    def __init__(self, attribution_model):
        super().__init__(attribution_model)
        self.method = InputXGradient(self.attribution_model)
class SaliencyAttribution(GradientAttributionRegistry):
    """Saliency attribution method.
    Reference implementation:
    `https://captum.ai/api/saliency.html <https://captum.ai/api/saliency.html>`__.
    """
    method_name = "saliency"
    def __init__(self, attribution_model):
        super().__init__(attribution_model)
        self.method = Saliency(self.attribution_model)
class SequentialIntegratedGradientsAttribution(GradientAttributionRegistry):
    """Sequential Integrated Gradients attribution method.
    Reference: https://aclanthology.org/2023.findings-acl.477/
    Original implementation: https://github.com/josephenguehard/time_interpret/blob/main/tint/attr/seq_ig.py
    """
    method_name = "sequential_integrated_gradients"
    def __init__(self, attribution_model, multiply_by_inputs: bool = True, **kwargs):
        super().__init__(attribution_model)
        self.method = SequentialIntegratedGradients(self.attribution_model, multiply_by_inputs)
        self.use_baselines = True
# Layer methods
class LayerIntegratedGradientsAttribution(GradientAttributionRegistry):
    """Layer Integrated Gradients attribution method.
    Reference implementation:
    `https://captum.ai/api/layer.html#layer-integrated-gradients <https://captum.ai/api/layer.html#layer-integrated-gradients>`__.
    """  # noqa E501
    method_name = "layer_integrated_gradients"
    def __init__(self, attribution_model, multiply_by_inputs: bool = True, **kwargs):
        super().__init__(attribution_model, hook_to_model=False)
        self.attribute_batch_ids = True
        self.forward_batch_embeds = False
        self.use_baselines = True
        self.hook(**kwargs)
        self.method = LayerIntegratedGradients(
            self.attribution_model,
            self.target_layer,
            multiply_by_inputs=multiply_by_inputs,
        )
class LayerGradientXActivationAttribution(GradientAttributionRegistry):
    """Layer Integrated Gradients attribution method.
    Reference implementation:
    `https://captum.ai/api/layer.html#layer-gradient-x-activation <https://captum.ai/api/layer.html#layer-gradient-x-activation>`__.
    """  # noqa E501
    method_name = "layer_gradient_x_activation"
    def __init__(self, attribution_model, multiply_by_inputs: bool = True, **kwargs):
        super().__init__(attribution_model, hook_to_model=False)
        self.attribute_batch_ids = True
        self.forward_batch_embeds = False
        self.use_baselines = False
        self.hook(**kwargs)
        self.method = LayerGradientXActivation(
            self.attribution_model,
            self.target_layer,
            multiply_by_inputs=multiply_by_inputs,
        )
class LayerDeepLiftAttribution(GradientAttributionRegistry):
    """Layer DeepLIFT attribution method.
    Reference implementation:
    `https://captum.ai/api/layer.html#layer-deeplift <https://captum.ai/api/layer.html#layer-deeplift>`__.
    """
    method_name = "layer_deeplift"
    def __init__(self, attribution_model, multiply_by_inputs: bool = True, **kwargs):
        super().__init__(attribution_model, hook_to_model=False)
        self.attribute_batch_ids = True
        self.forward_batch_embeds = False
        self.use_baselines = True
        self.hook(**kwargs)
        self.method = LayerDeepLift(
            self.attribution_model,
            self.target_layer,
            multiply_by_inputs=multiply_by_inputs,
        )

================
File: inseq/attr/feat/internals_attribution.py
================
# Copyright 2021 The Inseq Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""Attention-based feature attribution methods."""
import logging
from typing import Any
from captum._utils.typing import TensorOrTupleOfTensorsGeneric
from ...data import MultiDimensionalFeatureAttributionStepOutput
from ...utils import Registry
from ...utils.typing import InseqAttribution, MultiLayerMultiUnitScoreTensor
from .feature_attribution import FeatureAttribution
logger = logging.getLogger(__name__)
class InternalsAttributionRegistry(FeatureAttribution, Registry):
    r"""Model Internals-based attribution method registry."""
    pass
class AttentionWeightsAttribution(InternalsAttributionRegistry):
    """The basic attention attribution method, which retrieves the attention weights from the model."""
    method_name = "attention"
    class AttentionWeights(InseqAttribution):
        @staticmethod
        def has_convergence_delta() -> bool:
            return False
        def attribute(
            self,
            inputs: TensorOrTupleOfTensorsGeneric,
            additional_forward_args: TensorOrTupleOfTensorsGeneric,
            encoder_self_attentions: MultiLayerMultiUnitScoreTensor | None = None,
            decoder_self_attentions: MultiLayerMultiUnitScoreTensor | None = None,
            cross_attentions: MultiLayerMultiUnitScoreTensor | None = None,
        ) -> MultiDimensionalFeatureAttributionStepOutput:
            """Extracts the attention weights from the model.
            Args:
                inputs (`TensorOrTupleOfTensorsGeneric`):
                    Tensor or tuple of tensors that are inputs to the model. Used to match standard Captum API, and to
                    determine whether both source and target are being attributed.
                additional_forward_args (`TensorOrTupleOfTensorsGeneric`):
                    Tensor or tuple of tensors that are additional arguments to the model. Unused, but included to
                    match standard Captum API.
                encoder_self_attentions (:obj:`tuple(torch.Tensor)`, *optional*, defaults to None): Tensor of encoder
                    self-attention weights of the forward pass with shape
                    :obj:`(batch_size, n_layers, n_heads, source_seq_len, source_seq_len)`.
                decoder_self_attentions (:obj:`tuple(torch.Tensor)`, *optional*, defaults to None): Tensor of decoder
                    self-attention weights of the forward pass with shape
                    :obj:`(batch_size, n_layers, n_heads, target_seq_len, target_seq_len)`.
                cross_attentions (:obj:`tuple(torch.Tensor)`, *optional*, defaults to None):
                    Tensor of cross-attention weights computed during the forward pass with shape
                    :obj:`(batch_size, n_layers, n_heads, source_seq_len, target_seq_len)`.
            Returns:
                :class:`~inseq.data.MultiDimensionalFeatureAttributionStepOutput`: A step output containing attention
                weights for each layer and head, with shape :obj:`(batch_size, seq_len, n_layers, n_heads)`.
            """
            # We adopt the format [batch_size, sequence_length, sequence_length, num_layers, num_heads]
            # for consistency with other multi-unit methods (e.g. gradient attribution)
            decoder_self_attentions = decoder_self_attentions.to("cpu").clone().permute(0, 4, 3, 1, 2)
            if self.forward_func.is_encoder_decoder:
                sequence_scores = {}
                if len(inputs) > 1:
                    target_attributions = decoder_self_attentions
                else:
                    target_attributions = None
                    sequence_scores["decoder_self_attentions"] = decoder_self_attentions
                sequence_scores["encoder_self_attentions"] = (
                    encoder_self_attentions.to("cpu").clone().permute(0, 4, 3, 1, 2)
                )
                cross_attentions = cross_attentions.to("cpu").clone().permute(0, 4, 3, 1, 2)
                return MultiDimensionalFeatureAttributionStepOutput(
                    source_attributions=cross_attentions,
                    target_attributions=target_attributions,
                    sequence_scores=sequence_scores,
                    _num_dimensions=2,  # num_layers, num_heads
                )
            else:
                return MultiDimensionalFeatureAttributionStepOutput(
                    source_attributions=None,
                    target_attributions=decoder_self_attentions,
                    _num_dimensions=2,  # num_layers, num_heads
                )
    def __init__(self, attribution_model, **kwargs):
        super().__init__(attribution_model)
        # Attention weights will be passed to the attribute_step method
        self.use_attention_weights = True
        # Does not rely on predicted output (i.e. decoding strategy agnostic)
        self.use_predicted_target = False
        # Needs only the final generation step to extract scores
        self.is_final_step_method = True
        self.method = self.AttentionWeights(attribution_model)
    def attribute_step(
        self,
        attribute_fn_main_args: dict[str, Any],
        attribution_args: dict[str, Any],
    ) -> MultiDimensionalFeatureAttributionStepOutput:
        return self.method.attribute(**attribute_fn_main_args, **attribution_args)

================
File: inseq/attr/feat/ops/__init__.py
================
from .discretized_integrated_gradients import DiscretetizedIntegratedGradients
from .lime import Lime
from .monotonic_path_builder import MonotonicPathBuilder
from .reagent import Reagent
from .sequential_integrated_gradients import SequentialIntegratedGradients
from .value_zeroing import ValueZeroing
__all__ = [
    "DiscretetizedIntegratedGradients",
    "MonotonicPathBuilder",
    "ValueZeroing",
    "Lime",
    "Reagent",
    "SequentialIntegratedGradients",
]

================
File: inseq/attr/feat/ops/discretized_integrated_gradients.py
================
# Adapted from https://github.com/INK-USC/DIG/blob/main/dig.py, licensed MIT:
# Copyright Â© 2021 Intelligence and Knowledge Discovery (INK) Research Lab at University of Southern California
# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and
# associated documentation files (the â€œSoftwareâ€), to deal in the Software without restriction,
# including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense,
#  and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so,
# subject to the following conditions:
# The above copyright notice and this permission notice shall be included in all copies
# or substantial portions of the Software.
# THE SOFTWARE IS PROVIDED â€œAS ISâ€, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT
# LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
# IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,
# WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE
# OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
from collections.abc import Callable
from pathlib import Path
from typing import Any
import torch
from captum._utils.common import (
    _expand_additional_forward_args,
    _expand_target,
    _format_additional_forward_args,
    _format_output,
    _is_tuple,
)
from captum._utils.typing import BaselineType, TargetType, TensorOrTupleOfTensorsGeneric
from captum.attr._core.integrated_gradients import IntegratedGradients
from captum.attr._utils.batching import _batch_attribution
from captum.attr._utils.common import _format_input_baseline, _reshape_and_sum, _validate_input
from torch import Tensor
from ....utils import INSEQ_ARTIFACTS_CACHE
from ....utils.typing import MultiStepEmbeddingsTensor, VocabularyEmbeddingsTensor
from .monotonic_path_builder import MonotonicPathBuilder
class DiscretetizedIntegratedGradients(IntegratedGradients):
    def __init__(
        self,
        forward_func: Callable,
        multiply_by_inputs: bool = False,
    ) -> None:
        super().__init__(forward_func, multiply_by_inputs)
        self.path_builder = None
    def load_monotonic_path_builder(
        self,
        model_name: str,
        vocabulary_embeddings: VocabularyEmbeddingsTensor,
        special_tokens: list[int],
        cache_dir: Path = INSEQ_ARTIFACTS_CACHE / "dig_knn",
        embedding_scaling: int = 1,
        **kwargs,
    ) -> None:
        """Loads the Discretized Integrated Gradients (DIG) path builder."""
        self.path_builder = MonotonicPathBuilder.load(
            model_name,
            vocabulary_embeddings=vocabulary_embeddings.to("cpu"),
            special_tokens=special_tokens,
            cache_dir=cache_dir,
            embedding_scaling=embedding_scaling,
            **kwargs,
        )
    @staticmethod
    def get_inputs_baselines(scaled_features_tpl: tuple[Tensor, ...], n_steps: int) -> tuple[Tensor, ...]:
        # Baseline and inputs are reversed in the path builder
        # For every element in the batch, the first embedding of the sub-tensor
        # of shape (n_steps x embedding_dim) is the baseline, the last is the input.
        n_examples = scaled_features_tpl[0].shape[0] // n_steps
        baselines = tuple(
            torch.cat(
                [features[i, :, :].unsqueeze(0) for i in range(0, n_steps * n_examples, n_steps)],
            )
            for features in scaled_features_tpl
        )
        inputs = tuple(
            torch.cat(
                [features[i, :, :].unsqueeze(0) for i in range(n_steps - 1, n_steps * n_examples, n_steps)],
            )
            for features in scaled_features_tpl
        )
        return inputs, baselines
    def attribute(  # type: ignore
        self,
        inputs: MultiStepEmbeddingsTensor,
        baselines: BaselineType = None,
        target: TargetType = None,
        additional_forward_args: Any = None,
        n_steps: int = 50,
        method: str = "greedy",
        internal_batch_size: None | int = None,
        return_convergence_delta: bool = False,
    ) -> TensorOrTupleOfTensorsGeneric | tuple[TensorOrTupleOfTensorsGeneric, Tensor]:
        n_examples = inputs[0].shape[0]
        # Keeps track whether original input is a tuple or not before
        # converting it into a tuple.
        is_inputs_tuple = _is_tuple(inputs)
        inputs, baselines = _format_input_baseline(inputs, baselines)
        _validate_input(inputs, baselines, n_steps)
        scaled_features_tpl = tuple(
            self.path_builder.scale_inputs(
                input_tensor,
                baseline_tensor,
                n_steps=n_steps,
                scale_strategy=method,
            )
            for input_tensor, baseline_tensor in zip(inputs, baselines, strict=False)
        )
        if internal_batch_size is not None:
            attributions = _batch_attribution(
                self,
                n_examples,
                internal_batch_size,
                n_steps,
                scaled_features_tpl=scaled_features_tpl,
                target=target,
                additional_forward_args=additional_forward_args,
            )
        else:
            attributions = self._attribute(
                scaled_features_tpl=scaled_features_tpl,
                target=target,
                additional_forward_args=additional_forward_args,
                n_steps=n_steps,
            )
        if return_convergence_delta:
            start_point, end_point = self.get_inputs_baselines(scaled_features_tpl, n_steps)
            # computes approximation error based on the completeness axiom
            delta = self.compute_convergence_delta(
                attributions,
                start_point,
                end_point,
                additional_forward_args=additional_forward_args,
                target=target,
            )
            return _format_output(is_inputs_tuple, attributions), delta
        return _format_output(is_inputs_tuple, attributions)
    def _attribute(
        self,
        scaled_features_tpl: tuple[Tensor, ...],
        target: TargetType = None,
        additional_forward_args: Any = None,
        n_steps: int = 50,
    ) -> tuple[Tensor, ...]:
        additional_forward_args = _format_additional_forward_args(additional_forward_args)
        input_additional_args = (
            _expand_additional_forward_args(additional_forward_args, n_steps)
            if additional_forward_args is not None
            else None
        )
        expanded_target = _expand_target(target, n_steps)
        # grads: dim -> (bsz * #steps x inputs[0].shape[1:], ...)
        grads = self.gradient_func(
            forward_fn=self.forward_func,
            inputs=scaled_features_tpl,
            target_ind=expanded_target,
            additional_forward_args=input_additional_args,
        )
        # calculate (x - x') for each interpolated point
        shifted_inputs_tpl = tuple(
            torch.cat(
                [
                    torch.cat([features[idx + 1 : idx + n_steps], features[idx + n_steps - 1].unsqueeze(0)])
                    for idx in range(0, scaled_features_tpl[0].shape[0], n_steps)
                ]
            )
            for features in scaled_features_tpl
        )
        steps = tuple(shifted_inputs_tpl[i] - scaled_features_tpl[i] for i in range(len(shifted_inputs_tpl)))
        scaled_grads = tuple(grads[i] * steps[i] for i in range(len(grads)))
        # aggregates across all steps for each tensor in the input tuple
        # total_grads has the same dimensionality as the original inputs
        total_grads = tuple(
            _reshape_and_sum(scaled_grad, n_steps, grad.shape[0] // n_steps, grad.shape[1:])
            for (scaled_grad, grad) in zip(scaled_grads, grads, strict=False)
        )
        # computes attribution for each tensor in input_tuple
        # attributions has the same dimensionality as the original inputs
        if not self.multiplies_by_inputs:
            return total_grads
        else:
            inputs, baselines = self.get_inputs_baselines(scaled_features_tpl, n_steps)
            return tuple(
                total_grad * (input - baseline)
                for (total_grad, input, baseline) in zip(total_grads, inputs, baselines, strict=False)
            )

================
File: inseq/attr/feat/ops/lime.py
================
import inspect
import logging
import math
from collections.abc import Callable
from functools import partial
from typing import Any, cast
import torch
from captum._utils.common import _expand_additional_forward_args, _expand_target
from captum._utils.models.linear_model import SkLearnLinearModel
from captum._utils.models.model import Model
from captum._utils.progress import progress
from captum._utils.typing import TargetType, TensorOrTupleOfTensorsGeneric
from captum.attr import LimeBase
from torch import Tensor
from torch.utils.data import DataLoader, TensorDataset
logger = logging.getLogger(__name__)
class Lime(LimeBase):
    def __init__(
        self,
        attribution_model: Callable,
        interpretable_model: Model = None,
        similarity_func: Callable = None,
        perturb_func: Callable = None,
        perturb_interpretable_space: bool = False,
        from_interp_rep_transform: Callable | None = None,
        to_interp_rep_transform: Callable | None = None,
        mask_prob: float = 0.3,
    ) -> None:
        if interpretable_model is None:
            interpretable_model = SkLearnLinearModel("linear_model.Ridge")
        if similarity_func is None:
            similarity_func = self.token_similarity_kernel
        if perturb_func is None:
            perturb_func = partial(
                self.perturb_func,
                mask_prob=mask_prob,
            )
        if to_interp_rep_transform is None:
            to_interp_rep_transform_func = self.to_interp_rep_transform
        else:
            # Use custom function
            to_interp_rep_transform_func = to_interp_rep_transform
        super().__init__(
            forward_func=attribution_model,
            interpretable_model=interpretable_model,
            similarity_func=similarity_func,
            perturb_func=perturb_func,
            perturb_interpretable_space=perturb_interpretable_space,
            from_interp_rep_transform=from_interp_rep_transform,
            to_interp_rep_transform=to_interp_rep_transform_func,
        )
        self.attribution_model = attribution_model
    def attribute(
        self,
        inputs: TensorOrTupleOfTensorsGeneric,
        target: TargetType = None,
        additional_forward_args: Any = None,
        n_samples: int = 50,
        perturbations_per_eval: int = 1,
        show_progress: bool = False,
        **kwargs,
    ) -> Tensor:
        r"""Adapted from Captum: Two modifications at the end ensure that 3D
        tensors (needed for transformers inference) are reshaped as 2D tensors
        before being passed to the linear surrogate model, and reshaped again
        back to their 3D equivalents.
        See the LimeBase (super class) docstring for a proper description of
        LIME's functionality. What follows is an abbreviated docstring.
        Args:
            inputs (tensor or tuple of tensors):  Input for which LIME
                        is computed.
            target (int, tuple, tensor or list, optional):  Output indices for
                        which surrogate model is trained
                        (for classification cases,
                        this is usually the target class).
            additional_forward_args (any, optional): If the forward function
                        requires additional arguments other than the inputs for
                        which attributions should not be computed, this argument
                        can be provided.
            n_samples (int, optional):  The number of samples of the original
                        model used to train the surrogate interpretable model.
                        Default: `50` if `n_samples` is not provided.
            perturbations_per_eval (int, optional): Allows multiple samples
                        to be processed simultaneously in one call to forward_fn.
            show_progress (bool, optional): Displays the progress of computation.
            **kwargs (Any, optional): Any additional arguments necessary for
                        sampling and transformation functions (provided to
                        constructor).
        Returns:
            **interpretable model representation**:
            - **interpretable model representation* (*Any*):
                    A representation of the interpretable model trained.
                    In this adaptation, the return is a 3D tensor.
        """
        with torch.no_grad():
            inp_tensor = cast(Tensor, inputs) if isinstance(inputs, Tensor) else inputs[0]
            device = inp_tensor.device
            interpretable_inps = []
            similarities = []
            outputs = []
            curr_model_inputs = []
            expanded_additional_args = None
            expanded_target = None
            perturb_generator = None
            if inspect.isgeneratorfunction(self.perturb_func):
                perturb_generator = self.perturb_func(inputs, **kwargs)
            if show_progress:
                attr_progress = progress(
                    total=math.ceil(n_samples / perturbations_per_eval),
                    desc=f"{self.get_name()} attribution",
                )
                attr_progress.update(0)
            batch_count = 0
            for _ in range(n_samples):
                if perturb_generator:
                    try:
                        curr_sample = next(perturb_generator)
                    except StopIteration:
                        logger.warning("Generator completed prior to given n_samples iterations!")
                        break
                else:
                    curr_sample = self.perturb_func(inputs, **kwargs)
                batch_count += 1
                if self.perturb_interpretable_space:
                    interpretable_inps.append(curr_sample)
                    curr_model_inputs.append(
                        self.from_interp_rep_transform(curr_sample, inputs, **kwargs)  # type: ignore
                    )
                else:
                    curr_model_inputs.append(curr_sample)
                    interpretable_inps.append(
                        self.to_interp_rep_transform(curr_sample, inputs, **kwargs)  # type: ignore
                    )
                curr_sim = self.similarity_func(inputs, curr_model_inputs[-1], interpretable_inps[-1], **kwargs)
                similarities.append(
                    curr_sim.flatten() if isinstance(curr_sim, Tensor) else torch.tensor([curr_sim], device=device)
                )
                if len(curr_model_inputs) == perturbations_per_eval:
                    if expanded_additional_args is None:
                        expanded_additional_args = _expand_additional_forward_args(
                            additional_forward_args, len(curr_model_inputs)
                        )
                    if expanded_target is None:
                        expanded_target = _expand_target(target, len(curr_model_inputs))
                    model_out = self._evaluate_batch(
                        curr_model_inputs,
                        expanded_target,
                        expanded_additional_args,
                        device,
                    )
                    if show_progress:
                        attr_progress.update()
                    outputs.append(model_out)
                    curr_model_inputs = []
            if len(curr_model_inputs) > 0:
                expanded_additional_args = _expand_additional_forward_args(
                    additional_forward_args, len(curr_model_inputs)
                )
                expanded_target = _expand_target(target, len(curr_model_inputs))
                model_out = self._evaluate_batch(
                    curr_model_inputs,
                    expanded_target,
                    expanded_additional_args,
                    device,
                )
                if show_progress:
                    attr_progress.update()
                outputs.append(model_out)
            if show_progress:
                attr_progress.close()
            """ Modification of original attribute function:
            Squeeze the batch dimension out of interpretable_inps
            -> 2D tensor (n_samples âœ• (input_dim * embedding_dim))
            Zero-indexed interpretable_inps elements for unpacking the tuples.
            """
            combined_interp_inps = torch.cat([i[0].view(-1).unsqueeze(dim=0) for i in interpretable_inps]).double()
            combined_outputs = (torch.cat(outputs) if outputs[0].ndim > 0 else torch.stack(outputs)).double()
            combined_sim = (
                torch.cat(similarities) if similarities[0].ndim > 0 else torch.stack(similarities)
            ).double()
            dataset = TensorDataset(combined_interp_inps, combined_outputs, combined_sim)
            self.interpretable_model.fit(DataLoader(dataset, batch_size=batch_count))
            """ Second modification:
            Reshape of the learned representation
            -> 3D tensor (b=1 âœ• input_dim âœ• embedding_dim)
            """
            return self.interpretable_model.representation().reshape(inp_tensor.shape)
    @staticmethod
    def token_similarity_kernel(
        original_input: tuple,
        perturbed_input: tuple,
        perturbed_interpretable_input: tuple,
        **kwargs,
    ) -> torch.Tensor:
        r"""Calculates the similarity between original and perturbed input."""
        if len(original_input) == 1:
            original_input_tensor = original_input[0][0]
            perturbed_input_tensor = perturbed_input[0][0]
        elif len(original_input) == 2:
            original_input_tensor = torch.cat(original_input, dim=1)
            perturbed_input_tensor = torch.cat(perturbed_input, dim=1)
        else:
            raise ValueError("Original input tuple has to be of either length 1 or 2.")
        assert original_input_tensor.shape == perturbed_input_tensor.shape
        similarity = torch.sum(original_input_tensor == perturbed_input_tensor)
        return similarity
    def perturb_func(
        self,
        original_input_tuple: tuple = (),
        mask_prob: float = 0.3,
        mask_token: str = "unk",
        **kwargs: Any,
    ) -> tuple:
        r"""Sampling function:
        Args:
            original_input_tuple (tuple): Tensor tuple where its first element
                is a 3D tensor (b=1, seq_len, emb_dim)
            mask_prob (float): probability of the MASK token (no information)
                in the mask that the original input tensor is being multiplied
                with.
            mask_token (str): What kind of special token to use for masking the
                input. Options: "unk" and "pad"
        """
        perturbed_inputs = []
        for original_input_tensor in original_input_tuple:
            # Build mask for replacing random tokens with [PAD] token
            mask_value_probs = torch.tensor([mask_prob, 1 - mask_prob])
            mask_multinomial_binary = torch.multinomial(
                mask_value_probs, len(original_input_tensor[0]), replacement=True
            )
            def detach_to_list(t):
                return t.detach().cpu().numpy().tolist() if type(t) == torch.Tensor else t
            # Additionally remove special_token_ids
            mask_special_token_ids = torch.Tensor(
                [
                    1 if id_ in self.attribution_model.special_tokens_ids else 0
                    for id_ in detach_to_list(original_input_tensor[0])
                ]
            ).int()
            # Merge the binary mask with the special_token_ids mask
            mask = (
                torch.tensor(
                    [
                        m + s if s == 0 else s
                        for m, s in zip(mask_multinomial_binary, mask_special_token_ids, strict=False)
                    ]
                )
                .to(self.attribution_model.device)
                .unsqueeze(-1)  # 1D -> 2D
            )
            # Set special token for masking
            if mask_token == "unk":
                tokenizer_mask_token = self.attribution_model.tokenizer.unk_token_id
            elif mask_token == "pad":
                tokenizer_mask_token = self.attribution_model.tokenizer.pad_token_id
            else:
                raise ValueError(f"Invalid mask token {mask_token} for tokenizer: {self.attribution_model.tokenizer}")
            # Apply mask to original input
            perturbed_inputs.append(original_input_tensor * mask + (1 - mask) * tokenizer_mask_token)
        return tuple(perturbed_inputs)
    @staticmethod
    def to_interp_rep_transform(sample, original_input, **kwargs: Any):
        return sample

================
File: inseq/attr/feat/ops/monotonic_path_builder.py
================
# Adapted from https://github.com/INK-USC/DIG/blob/main/monotonic_paths.py, licensed MIT:
# Copyright Â© 2021 The Inseq Team and the Intelligence and Knowledge Discovery (INK) Research Lab
# at the University of Southern California
# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and
# associated documentation files (the â€œSoftwareâ€), to deal in the Software without restriction,
# including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense,
#  and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so,
# subject to the following conditions:
# The above copyright notice and this permission notice shall be included in all copies
# or substantial portions of the Software.
# THE SOFTWARE IS PROVIDED â€œAS ISâ€, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT
# LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
# IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,
# WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE
# OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
"""Monotonic path builder for Discretized Integrated Gradients (DIG)."""
import logging
import os
from enum import Enum
from itertools import islice
from pathlib import Path
from typing import Any
import torch
from jaxtyping import Float, Int
from ....utils import is_joblib_available, is_scikitlearn_available
if is_joblib_available():
    from joblib import Parallel, delayed
if is_scikitlearn_available():
    from scipy.sparse import csr_matrix
    from sklearn.neighbors import kneighbors_graph
from ....utils import INSEQ_ARTIFACTS_CACHE, cache_results, euclidean_distance
from ....utils.typing import MultiStepEmbeddingsTensor, VocabularyEmbeddingsTensor
logger = logging.getLogger(__name__)
class PathBuildingStrategies(Enum):
    """Strategies for building monotonic paths."""
    GREEDY = "greedy"  # Based on the Euclidean distance between embeddings.
    MAXCOUNT = "maxcount"  # Based on the number of monotonic dimensions
class UnknownPathBuildingStrategy(Exception):
    """Raised when a strategy for pathbuilding is not valid."""
    def __init__(
        self,
        strategy: str,
        *args: tuple[Any],
    ) -> None:
        """Initialize the exception."""
        super().__init__(
            (
                f"Unknown strategy: {strategy}.\nAvailable strategies: "
                f"{','.join([s.value for s in PathBuildingStrategies])}"
            ),
            *args,
        )
class MonotonicPathBuilder:
    """Build monotonic paths between two token embeddings."""
    def __init__(
        self,
        vocabulary_embeddings: VocabularyEmbeddingsTensor,
        knn_graph: "csr_matrix",
        special_tokens: list[int] = [],
    ) -> None:
        """Initialize the monotonic path builder."""
        self.vocabulary_embeddings = vocabulary_embeddings
        self.knn_graph = knn_graph
        self.special_tokens = special_tokens
    @staticmethod
    @cache_results
    def compute_embeddings_knn(
        vocabulary_embeddings: VocabularyEmbeddingsTensor | None,
        n_neighbors: int = 50,
        mode: str = "distance",
        n_jobs: int = -1,
    ) -> "csr_matrix":
        """Either loads or computes the knn graph for token embeddings."""
        if not is_scikitlearn_available():
            raise ImportError("scikit-learn is not available. Please install it to use MonotonicPathBuilder.")
        knn_graph = kneighbors_graph(
            vocabulary_embeddings,
            n_neighbors=n_neighbors,
            mode=mode,
            n_jobs=n_jobs,
        )
        return knn_graph
    @classmethod
    def load(
        cls,
        model_name: str,
        n_neighbors: int = 50,
        mode: str = "distance",
        n_jobs: int = -1,
        save_cache: bool = True,
        overwrite_cache: bool = False,
        cache_dir: Path = INSEQ_ARTIFACTS_CACHE / "path_knn",
        vocabulary_embeddings: VocabularyEmbeddingsTensor | None = None,
        special_tokens: list[int] = [],
        embedding_scaling: int = 1,
    ) -> "MonotonicPathBuilder":
        """Load a cached monotonic path builder from a model name, or compute it if it does not exist."""
        cache_filename = os.path.join(cache_dir, f"{model_name.replace('/', '__')}_{n_neighbors}.pkl")
        if vocabulary_embeddings is None:
            logger.warning(
                "Since no token embeddings are passed, a cached file is expected. "
                "If the file is not found, an exception will be raised."
            )
        vocabulary_embeddings = vocabulary_embeddings * embedding_scaling
        # Cache parameters are passed to the cache_results decorator
        knn_graph = cls.compute_embeddings_knn(
            cache_dir,
            cache_filename,
            save_cache,
            overwrite_cache,
            vocabulary_embeddings=vocabulary_embeddings,
            n_neighbors=n_neighbors,
            mode=mode,
            n_jobs=n_jobs,
        )
        return cls(vocabulary_embeddings, knn_graph, special_tokens)
    def scale_inputs(
        self,
        input_ids: Int[torch.Tensor, "batch_size seq_len"],
        baseline_ids: Int[torch.Tensor, "batch_size seq_len"],
        n_steps: int | None = None,
        scale_strategy: str | None = None,
    ) -> MultiStepEmbeddingsTensor:
        """Generate paths required by DIG."""
        if n_steps is None:
            n_steps = 30
        if scale_strategy is None:
            scale_strategy = "greedy"
        if not is_joblib_available():
            raise ImportError("joblib is not available. Please install it to use MonotonicPathBuilder.")
        word_paths_flat = Parallel(n_jobs=3, prefer="threads")(
            delayed(self.find_path)(
                int(input_ids[seq_idx, tok_idx]),
                int(baseline_ids[seq_idx, tok_idx]),
                n_steps=n_steps,
                strategy=scale_strategy,
            )
            for seq_idx in range(input_ids.shape[0])
            for tok_idx in range(input_ids.shape[1])
        )
        # Unflatten word paths
        word_paths_iter = iter(word_paths_flat)
        word_paths = [list(islice(word_paths_iter, input_ids.shape[1])) for _ in range(input_ids.shape[0])]
        # Fill embeddings list
        lst_all_seq_embeds = []
        for seq_idx in range(input_ids.shape[0]):
            lst_curr_seq_embeds = []
            for tok_idx in range(input_ids.shape[1]):
                lst_curr_seq_embeds.append(
                    self.build_monotonic_path_embedding(
                        word_path=word_paths[seq_idx][tok_idx],
                        baseline_idx=int(baseline_ids[seq_idx, tok_idx]),
                        n_steps=n_steps,
                    )
                )
            # out shape: n_steps x seq_len x hidden_size
            t_curr_seq_embeds = torch.stack(lst_curr_seq_embeds, axis=1).float()
            lst_all_seq_embeds.append(t_curr_seq_embeds)
        # concat sequences on batch dimension
        t_all_seq_embeds = torch.cat(lst_all_seq_embeds).to(input_ids.device).requires_grad_()
        return t_all_seq_embeds
    def find_path(
        self,
        word_idx: int,
        baseline_idx: int,
        n_steps: int | None = 30,
        strategy: str | None = "greedy",
    ) -> list[int]:
        """Find a monotonic path from a word to a baseline."""
        # if word_idx is a special token copy it and return
        if word_idx in self.special_tokens:
            return [word_idx] * (n_steps - 1)
        word_path = [word_idx]
        for _ in range(n_steps - 2):
            word_path.append(
                word_idx := self.get_closest_word(
                    word_idx=word_idx,
                    baseline_idx=baseline_idx,
                    word_path=word_path,
                    strategy=strategy,
                    n_steps=n_steps,
                )
            )
        return word_path
    def build_monotonic_path_embedding(
        self, word_path: list[int], baseline_idx: int, n_steps: int = 30
    ) -> Float[torch.Tensor, "n_steps embed_size"]:
        """Build a monotonic path embedding from a word path."""
        baseline_vec = self.vocabulary_embeddings[baseline_idx]
        monotonic_embs = [self.vocabulary_embeddings[word_path[0]]]
        for idx in range(len(word_path) - 1):
            monotonic_embs.append(
                self.make_monotonic_vec(
                    anchor=self.vocabulary_embeddings[word_path[idx + 1]],
                    baseline=baseline_vec,
                    input=monotonic_embs[-1],
                    n_steps=n_steps,
                )
            )
        monotonic_embs += [baseline_vec]
        # reverse the list so that baseline is the first and input word is the last
        monotonic_embs.reverse()
        assert self.check_monotonic(monotonic_embs), "The embeddings are not monotonic"
        return torch.stack(monotonic_embs)
    def get_closest_word(
        self,
        word_idx: int,
        baseline_idx: int,
        word_path: list[int],
        strategy: str = "greedy",
        n_steps: int = 30,
    ) -> int:
        """Get the closest word to the current word in the path."""
        # If (for some reason) we do select the ref_idx as the previous anchor word,
        # then all further anchor words should be ref_idx
        if word_idx == baseline_idx:
            return baseline_idx
        cx = self.knn_graph[word_idx].tocoo()
        # ignore anchor word if equals the baseline (padding, special tokens)
        # remove words that are already selected in the path
        anchor_map = {
            anchor_idx: self.get_word_distance(strategy, anchor_idx, baseline_idx, word_idx, n_steps)
            for anchor_idx in cx.col
            if anchor_idx not in word_path + [baseline_idx]
        }
        if len(anchor_map) == 0:
            return baseline_idx
        # return the top key
        return [k for k, _ in sorted(anchor_map.items(), key=lambda pair: pair[1])].pop(0)
    def get_word_distance(
        self,
        strategy: str,
        anchor_idx: int,
        baseline_idx: int,
        original_idx: int,
        n_steps: int,
    ) -> float | int:
        """Get the distance between the anchor word and the baseline word."""
        if strategy == PathBuildingStrategies.GREEDY.value:
            # calculate the distance of the monotonized vec from the interpolated point
            monotonic_vec = self.make_monotonic_vec(
                self.vocabulary_embeddings[anchor_idx],
                self.vocabulary_embeddings[baseline_idx],
                self.vocabulary_embeddings[original_idx],
                n_steps,
            )
            return euclidean_distance(self.vocabulary_embeddings[anchor_idx], monotonic_vec)
        elif strategy == PathBuildingStrategies.MAXCOUNT.value:
            # count the number of non-monotonic dimensions
            monotonic_dims = self.get_monotonic_dims(
                self.vocabulary_embeddings[anchor_idx],
                self.vocabulary_embeddings[baseline_idx],
                self.vocabulary_embeddings[original_idx],
            )
            # 10000 is an arbitrarily high to be agnostic of embeddings dimensionality
            return 10000 - monotonic_dims.sum()
        else:
            raise UnknownPathBuildingStrategy(strategy)
    @classmethod
    def check_monotonic(cls, input: torch.Tensor) -> bool:
        """Return true if input dimensions are monotonic, false otherwise."""
        check = True
        for i in range(len(input) - 1):
            monotonic_dims = cls.get_monotonic_dims(input[i + 1], input[-1], input[i])
            is_fully_monotonic = monotonic_dims.sum() == input[-1].shape[0]
            check *= is_fully_monotonic
        return check
    @classmethod
    def make_monotonic_vec(
        cls,
        anchor: torch.Tensor,
        baseline: torch.Tensor,
        input: torch.Tensor,
        n_steps: int | None = 30,
    ) -> torch.Tensor:
        """Create a new monotonic vector w.r.t. input and baseline from an existing anchor."""
        non_monotonic_dims = ~cls.get_monotonic_dims(anchor, baseline, input)
        if non_monotonic_dims.sum() == 0:
            return anchor
        # make the anchor monotonic
        monotonic_vec = anchor.clone()
        monotonic_vec[non_monotonic_dims] = input[non_monotonic_dims] - (1.0 / n_steps) * (
            input[non_monotonic_dims] - baseline[non_monotonic_dims]
        )
        return monotonic_vec
    @staticmethod
    def get_monotonic_dims(
        anchor: torch.Tensor,
        baseline: torch.Tensor,
        input: torch.Tensor,
    ) -> torch.Tensor:
        """Check if the anchor vector is monotonic w.r.t. the baseline and the input."""
        # fmt: off
        return torch.where(
            (baseline > input)  * (baseline >= anchor) * (anchor >= input) + # noqa E211 W504
            (baseline < input)  * (baseline <= anchor) * (anchor <= input) + # noqa E211 W504
            (baseline == input) * (baseline == anchor) * (anchor == input),
            1, 0
        ).bool()
        # fmt: on

================
File: inseq/attr/feat/ops/reagent_core/__init__.py
================
from .importance_score_evaluator import DeltaProbImportanceScoreEvaluator
from .rationalizer import AggregateRationalizer
from .stopping_condition_evaluator import TopKStoppingConditionEvaluator
from .token_replacer import UniformTokenReplacer
from .token_sampler import POSTagTokenSampler
__all__ = [
    "DeltaProbImportanceScoreEvaluator",
    "AggregateRationalizer",
    "TopKStoppingConditionEvaluator",
    "UniformTokenReplacer",
    "POSTagTokenSampler",
]

================
File: inseq/attr/feat/ops/reagent_core/importance_score_evaluator.py
================
from __future__ import annotations
import logging
from abc import ABC, abstractmethod
import torch
from jaxtyping import Float
from transformers import AutoModelForCausalLM, AutoModelForSeq2SeqLM, AutoTokenizer
from typing_extensions import override
from .....utils.typing import IdsTensor, MultipleScoresPerStepTensor, TargetIdsTensor
from .stopping_condition_evaluator import StoppingConditionEvaluator
from .token_replacer import TokenReplacer
class BaseImportanceScoreEvaluator(ABC):
    """Importance Score Evaluator"""
    def __init__(self, model: AutoModelForCausalLM | AutoModelForSeq2SeqLM, tokenizer: AutoTokenizer) -> None:
        """Base Constructor
        Args:
            model: A Huggingface AutoModelForCausalLM or AutoModelForSeq2SeqLM model
            tokenizer: A Huggingface AutoTokenizer
        """
        self.model = model
        self.tokenizer = tokenizer
        self.importance_score = None
    @abstractmethod
    def __call__(
        self,
        input_ids: IdsTensor,
        target_id: TargetIdsTensor,
        decoder_input_ids: IdsTensor | None = None,
        attribute_target: bool = False,
    ) -> MultipleScoresPerStepTensor:
        """Evaluate importance score of input sequence
        Args:
            input_ids: input sequence [batch, sequence]
            target_id: target token [batch]
            decoder_input_ids (optional): decoder input sequence for AutoModelForSeq2SeqLM [batch, sequence]
            attribute_target: whether attribute target for encoder-decoder models
        Return:
            importance_score: evaluated importance score for each token in the input [batch, sequence]
        """
        raise NotImplementedError()
class DeltaProbImportanceScoreEvaluator(BaseImportanceScoreEvaluator):
    """Importance Score Evaluator"""
    @override
    def __init__(
        self,
        model: AutoModelForCausalLM | AutoModelForSeq2SeqLM,
        tokenizer: AutoTokenizer,
        token_replacer: TokenReplacer,
        stopping_condition_evaluator: StoppingConditionEvaluator,
        max_steps: float,
    ) -> None:
        """Constructor
        Args:
            model: A Huggingface AutoModelForCausalLM or AutoModelForSeq2SeqLM model
            tokenizer: A Huggingface AutoTokenizer
            token_replacer: A TokenReplacer
            stopping_condition_evaluator: A StoppingConditionEvaluator
        """
        super().__init__(model, tokenizer)
        self.token_replacer = token_replacer
        self.stopping_condition_evaluator = stopping_condition_evaluator
        self.max_steps = max_steps
        self.importance_score = None
        self.num_steps = 0
    def update_importance_score(
        self,
        logit_importance_score: MultipleScoresPerStepTensor,
        input_ids: IdsTensor,
        target_id: TargetIdsTensor,
        prob_original_target: Float[torch.Tensor, "batch_size 1"],
        decoder_input_ids: IdsTensor | None = None,
        attribute_target: bool = False,
    ) -> MultipleScoresPerStepTensor:
        """Update importance score by one step
        Args:
            logit_importance_score: Current importance score in logistic scale [batch, sequence]
            input_ids: input tensor [batch, sequence]
            target_id: target tensor [batch]
            prob_original_target: predictive probability of the target on the original sequence [batch, 1]
            decoder_input_ids (optional): decoder input sequence for AutoModelForSeq2SeqLM [batch, sequence]
            attribute_target: whether attribute target for encoder-decoder models
        Return:
            logit_importance_score: updated importance score in logistic scale [batch, sequence]
        """
        # Randomly replace a set of tokens R to form a new sequence \hat{y_{1...t}}
        if not attribute_target:
            input_ids_replaced, mask_replacing = self.token_replacer(input_ids)
        else:
            ids_replaced, mask_replacing = self.token_replacer(torch.cat((input_ids, decoder_input_ids), 1))
            input_ids_replaced = ids_replaced[:, : input_ids.shape[1]]
            decoder_input_ids_replaced = ids_replaced[:, input_ids.shape[1] :]
        logging.debug(f"Replacing mask:     { mask_replacing }")
        logging.debug(
            f"Replaced sequence:  { [[ self.tokenizer.decode(seq[i]) for i in range(input_ids_replaced.shape[1]) ] for seq in input_ids_replaced ] }"
        )
        # Inference \hat{p^{(y)}} = p(y_{t+1}|\hat{y_{1...t}})
        kwargs = {"input_ids": input_ids_replaced}
        if decoder_input_ids is not None:
            kwargs["decoder_input_ids"] = decoder_input_ids_replaced if attribute_target else decoder_input_ids
        logits_replaced = self.model(**kwargs)["logits"]
        prob_replaced_target = torch.softmax(logits_replaced[:, -1, :], -1)[:, target_id]
        # Compute changes delta = p^{(y)} - \hat{p^{(y)}}
        delta_prob_target = prob_original_target - prob_replaced_target
        logging.debug(f"likelihood delta: { delta_prob_target }")
        # Update importance scores based on delta (magnitude) and replacement (direction)
        delta_score = mask_replacing * delta_prob_target + ~mask_replacing * -delta_prob_target
        # TODO: better solution?
        # Rescaling from [-1, 1] to [0, 1] before logit function
        logit_delta_score = torch.logit(delta_score * 0.5 + 0.5)
        logit_importance_score = logit_importance_score + logit_delta_score
        logging.debug(f"Updated importance score: { torch.softmax(logit_importance_score, -1) }")
        return logit_importance_score
    @override
    def __call__(
        self,
        input_ids: IdsTensor,
        target_id: TargetIdsTensor,
        decoder_input_ids: IdsTensor | None = None,
        attribute_target: bool = False,
    ) -> MultipleScoresPerStepTensor:
        """Evaluate importance score of input sequence
        Args:
            input_ids: input sequence [batch, sequence]
            target_id: target token [batch]
            decoder_input_ids (optional): decoder input sequence for AutoModelForSeq2SeqLM [batch, sequence]
            attribute_target: whether attribute target for encoder-decoder models
        Return:
            importance_score: evaluated importance score for each token in the input [batch, sequence]
        """
        self.stop_mask = torch.zeros([input_ids.shape[0]], dtype=torch.bool, device=input_ids.device)
        # Inference p^{(y)} = p(y_{t+1}|y_{1...t})
        if decoder_input_ids is None:
            logits_original = self.model(input_ids)["logits"]
        else:
            logits_original = self.model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)["logits"]
        prob_original_target = torch.softmax(logits_original[:, -1, :], -1)[:, target_id]
        # Initialize importance score s for each token in the sequence y_{1...t}
        if not attribute_target:
            logit_importance_score = torch.rand(input_ids.shape, device=input_ids.device)
        else:
            logit_importance_score = torch.rand(
                (input_ids.shape[0], input_ids.shape[1] + decoder_input_ids.shape[1]), device=input_ids.device
            )
        logging.debug(f"Initialize importance score -> { torch.softmax(logit_importance_score, -1) }")
        # TODO: limit max steps
        self.num_steps = 0
        while self.num_steps < self.max_steps:
            self.num_steps += 1
            # Update importance score
            logit_importance_score_update = self.update_importance_score(
                logit_importance_score, input_ids, target_id, prob_original_target, decoder_input_ids, attribute_target
            )
            logit_importance_score = (
                ~torch.unsqueeze(self.stop_mask, 1) * logit_importance_score_update
                + torch.unsqueeze(self.stop_mask, 1) * logit_importance_score
            )
            self.importance_score = torch.softmax(logit_importance_score, -1)
            # Evaluate stop condition
            self.stop_mask = self.stop_mask | self.stopping_condition_evaluator(
                input_ids, target_id, self.importance_score, decoder_input_ids, attribute_target
            )
            if torch.prod(self.stop_mask) > 0:
                break
        logging.info(f"Importance score evaluated in {self.num_steps} steps.")
        return torch.softmax(logit_importance_score, -1)

================
File: inseq/attr/feat/ops/reagent_core/rationalizer.py
================
import math
from abc import ABC, abstractmethod
import torch
from jaxtyping import Int64
from typing_extensions import override
from .....utils.typing import IdsTensor, TargetIdsTensor
from .importance_score_evaluator import BaseImportanceScoreEvaluator
class BaseRationalizer(ABC):
    def __init__(self, importance_score_evaluator: BaseImportanceScoreEvaluator) -> None:
        super().__init__()
        self.importance_score_evaluator = importance_score_evaluator
        self.mean_importance_score = None
    @abstractmethod
    def __call__(
        self,
        input_ids: IdsTensor,
        target_id: TargetIdsTensor,
        decoder_input_ids: IdsTensor | None = None,
        attribute_target: bool = False,
    ) -> Int64[torch.Tensor, "batch_size other_dims"]:
        """Compute rational of a sequence on a target
        Args:
            input_ids: The sequence [batch, sequence] (first dimension need to be 1)
            target_id: The target [batch]
            decoder_input_ids (optional): decoder input sequence for AutoModelForSeq2SeqLM [batch, sequence]
            attribute_target: whether attribute target for encoder-decoder models
        Return:
            pos_top_n: rational position in the sequence [batch, rational_size]
        """
        raise NotImplementedError()
class AggregateRationalizer(BaseRationalizer):
    """AggregateRationalizer"""
    @override
    def __init__(
        self,
        importance_score_evaluator: BaseImportanceScoreEvaluator,
        batch_size: int,
        overlap_threshold: int,
        overlap_strict_pos: bool = True,
        keep_top_n: int = 0,
        keep_ratio: float = 0,
    ) -> None:
        """Constructor
        Args:
            importance_score_evaluator: A ImportanceScoreEvaluator
            batch_size: Batch size for aggregate
            overlap_threshold: Overlap threshold of rational tokens within a batch
            overlap_strict_pos: Whether overlap strict to position ot not
            keep_top_n: If set to a value greater than 0, the top n tokens based on their importance score will be
                kept, and the rest will be flagged for replacement. If set to 0, the top n will be determined by
                ``keep_ratio``.
            keep_ratio: If ``keep_top_n`` is set to 0, this specifies the proportion of tokens to keep.
        """
        super().__init__(importance_score_evaluator)
        self.batch_size = batch_size
        self.overlap_threshold = overlap_threshold
        self.overlap_strict_pos = overlap_strict_pos
        self.keep_top_n = keep_top_n
        self.keep_ratio = keep_ratio
        assert overlap_strict_pos, "overlap_strict_pos = False is not supported yet"
    @override
    @torch.no_grad()
    def __call__(
        self,
        input_ids: IdsTensor,
        target_id: TargetIdsTensor,
        decoder_input_ids: IdsTensor | None = None,
        attribute_target: bool = False,
    ) -> Int64[torch.Tensor, "batch_size other_dims"]:
        """Compute rational of a sequence on a target
        Args:
            input_ids: A tensor of ids of shape [batch, sequence_len]
            target_id: A tensor of predicted targets of size [batch]
            decoder_input_ids (optional): A tensor of ids representing the decoder input sequence for
                ``AutoModelForSeq2SeqLM``, with shape [batch, sequence_len]
            attribute_target: whether attribute target for encoder-decoder models
        Return:
            pos_top_n: rational position in the sequence [batch, rational_size]
        """
        assert input_ids.shape[0] == 1, "the first dimension of input (batch_size) need to be 1"
        batch_input_ids = input_ids.repeat(self.batch_size, 1)
        batch_decoder_input_ids = (
            decoder_input_ids.repeat(self.batch_size, 1) if decoder_input_ids is not None else None
        )
        batch_importance_score = self.importance_score_evaluator(
            batch_input_ids, target_id, batch_decoder_input_ids, attribute_target
        )
        importance_score_masked = batch_importance_score * torch.unsqueeze(
            self.importance_score_evaluator.stop_mask, -1
        )
        self.mean_importance_score = torch.sum(importance_score_masked, dim=0) / torch.sum(
            self.importance_score_evaluator.stop_mask
        )
        pos_sorted = torch.argsort(batch_importance_score, dim=-1, descending=True)
        top_n = int(math.ceil(self.keep_ratio * input_ids.shape[-1])) if not self.keep_top_n else self.keep_top_n
        pos_top_n = pos_sorted[:, :top_n]
        self.pos_top_n = pos_top_n
        if self.overlap_strict_pos:
            count_overlap = torch.bincount(pos_top_n.flatten(), minlength=input_ids.shape[1])
            pos_top_n_overlap = torch.unsqueeze(
                torch.nonzero(count_overlap >= self.overlap_threshold, as_tuple=True)[0], 0
            )
            return pos_top_n_overlap
        else:
            raise NotImplementedError("overlap_strict_pos = False not been supported yet")
            # TODO: Convert back to pos
            # token_id_top_n = input_ids[0, pos_top_n]
            # count_overlap = torch.bincount(token_id_top_n.flatten(), minlength=input_ids.shape[1])
            # _token_id_top_n_overlap = torch.unsqueeze(
            #     torch.nonzero(count_overlap >= self.overlap_threshold, as_tuple=True)[0], 0
            # )

================
File: inseq/attr/feat/ops/reagent_core/stopping_condition_evaluator.py
================
import logging
from abc import ABC, abstractmethod
import torch
from transformers import AutoModelForCausalLM
from .....utils.typing import IdsTensor, MultipleScoresPerStepTensor, TargetIdsTensor
from .token_replacer import RankingTokenReplacer
from .token_sampler import TokenSampler
class StoppingConditionEvaluator(ABC):
    """Base class for Stopping Condition Evaluators"""
    @abstractmethod
    def __call__(
        self,
        input_ids: IdsTensor,
        target_id: TargetIdsTensor,
        importance_score: MultipleScoresPerStepTensor,
        decoder_input_ids: IdsTensor | None = None,
        attribute_target: bool = False,
    ) -> TargetIdsTensor:
        """Evaluate stop condition according to the specified strategy.
        Args:
            input_ids: Input sequence [batch, sequence]
            target_id: Target token [batch]
            importance_score: Importance score of the input [batch, sequence]
            decoder_input_ids (optional): decoder input sequence for AutoModelForSeq2SeqLM [batch, sequence]
            attribute_target: whether attribute target for encoder-decoder models
        Return:
            Boolean flag per sequence signaling whether the stop condition was reached [batch]
        """
        raise NotImplementedError()
class TopKStoppingConditionEvaluator(StoppingConditionEvaluator):
    """
    Evaluator stopping when target exist among the top k predictions,
    while top n tokens based on importance_score are not been replaced.
    """
    def __init__(
        self,
        model: AutoModelForCausalLM,
        sampler: TokenSampler,
        top_k: int,
        keep_top_n: int = 0,
        keep_ratio: float = 0,
        invert_keep: bool = False,
    ) -> None:
        """Constructor for the TopKStoppingConditionEvaluator class.
        Args:
            model: A Huggingface ``AutoModelForCausalLM``.
            sampler: A :class:`~inseq.attr.feat.ops.reagent_core.TokenSampler` object to sample replacement tokens.
            top_k: Top K predictions in which the target must be included in order to achieve the stopping condition.
            keep_top_n: If set to a value greater than 0, the top n tokens based on their importance score will be
                kept, and the rest will be flagged for replacement. If set to 0, the top n will be determined by
                ``keep_ratio``.
            keep_ratio: If ``keep_top_n`` is set to 0, this specifies the proportion of tokens to keep.
            invert_keep: If specified, the top tokens selected either via ``keep_top_n`` or ``keep_ratio`` will be
                replaced instead of being kept.
        """
        self.model = model
        self.top_k = top_k
        self.replacer = RankingTokenReplacer(sampler, keep_top_n, keep_ratio, invert_keep)
    def __call__(
        self,
        input_ids: IdsTensor,
        target_id: TargetIdsTensor,
        importance_score: MultipleScoresPerStepTensor,
        decoder_input_ids: IdsTensor | None = None,
        attribute_target: bool = False,
    ) -> TargetIdsTensor:
        """Evaluate stop condition
        Args:
            input_ids: Input sequence [batch, sequence]
            target_id: Target token [batch]
            importance_score: Importance score of the input [batch, sequence]
            decoder_input_ids (optional): decoder input sequence for AutoModelForSeq2SeqLM [batch, sequence]
            attribute_target: whether attribute target for encoder-decoder models
        Return:
            Boolean flag per sequence signaling whether the stop condition was reached [batch]
        """
        # Replace tokens with low importance score and then inference \hat{y^{(e)}_{t+1}}
        self.replacer.set_score(importance_score)
        if not attribute_target:
            input_ids_replaced, mask_replacing = self.replacer(input_ids)
        else:
            ids_replaced, mask_replacing = self.replacer(torch.cat((input_ids, decoder_input_ids), 1))
            input_ids_replaced = ids_replaced[:, : input_ids.shape[1]]
            decoder_input_ids_replaced = ids_replaced[:, input_ids.shape[1] :]
        logging.debug(f"Replacing mask based on importance score -> { mask_replacing }")
        # Whether the result \hat{y^{(e)}_{t+1}} consistent with y_{t+1}
        assert not input_ids_replaced.requires_grad, "Error: auto-diff engine not disabled"
        with torch.no_grad():
            kwargs = {"input_ids": input_ids_replaced}
            if decoder_input_ids is not None:
                kwargs["decoder_input_ids"] = decoder_input_ids_replaced if attribute_target else decoder_input_ids
            logits_replaced = self.model(**kwargs)["logits"]
        ids_prediction_sorted = torch.argsort(logits_replaced[:, -1, :], descending=True)
        ids_prediction_top_k = ids_prediction_sorted[:, : self.top_k]
        match_mask = ids_prediction_top_k == target_id
        match_hit = torch.sum(match_mask, dim=-1, dtype=torch.bool)
        return match_hit
class DummyStoppingConditionEvaluator(StoppingConditionEvaluator):
    """
    Stopping Condition Evaluator which stop when target exist in top k predictions,
    while top n tokens based on importance_score are not been replaced.
    """
    def __call__(self, input_ids: IdsTensor, **kwargs) -> TargetIdsTensor:
        """Evaluate stop condition
        Args:
            input_ids: Input sequence [batch, sequence]
            target_id: Target token [batch]
            importance_score: Importance score of the input [batch, sequence]
            attribute_target: whether attribute target for encoder-decoder models
        Return:
            Boolean flag per sequence signaling whether the stop condition was reached [batch]
        """
        return torch.ones([input_ids.shape[0]], dtype=torch.bool, device=input_ids.device)

================
File: inseq/attr/feat/ops/reagent_core/token_replacer.py
================
import math
from abc import ABC, abstractmethod
import torch
from typing_extensions import override
from .....utils.typing import IdsTensor
from .token_sampler import TokenSampler
class TokenReplacer(ABC):
    """
    Base class for token replacers
    """
    def __init__(self, sampler: TokenSampler) -> None:
        self.sampler = sampler
    @abstractmethod
    def __call__(self, input: IdsTensor) -> tuple[IdsTensor, IdsTensor]:
        """Replace tokens according to the specified strategy.
        Args:
            input: input sequence [batch, sequence]
        Returns:
            input_replaced: A replaced sequence [batch, sequence]
            replacement_mask: Boolean mask identifying which token has been replaced [batch, sequence]
        """
        raise NotImplementedError()
class RankingTokenReplacer(TokenReplacer):
    """Replace tokens in a sequence based on top-N ranking"""
    @override
    def __init__(
        self, sampler: TokenSampler, keep_top_n: int = 0, keep_ratio: float = 0, invert_keep: bool = False
    ) -> None:
        """Constructor for the RankingTokenReplacer class.
        Args:
            sampler: A :class:`~inseq.attr.feat.ops.reagent_core.TokenSampler` object for sampling replacement tokens.
            keep_top_n: If set to a value greater than 0, the top n tokens based on their importance score will be
                kept, and the rest will be flagged for replacement. If set to 0, the top n will be determined by
                ``keep_ratio``.
            keep_ratio: If ``keep_top_n`` is set to 0, this specifies the proportion of tokens to keep.
            invert_keep: If specified, the top tokens selected either via ``keep_top_n`` or ``keep_ratio`` will be
                replaced instead of being kept.
        """
        super().__init__(sampler)
        self.keep_top_n = keep_top_n
        self.keep_ratio = keep_ratio
        self.invert_keep = invert_keep
    def set_score(self, value: torch.Tensor) -> None:
        pos_sorted = torch.argsort(value, descending=True)
        top_n = int(math.ceil(self.keep_ratio * value.shape[-1])) if not self.keep_top_n else self.keep_top_n
        pos_top_n = pos_sorted[..., :top_n]
        self.replacement_mask = torch.ones_like(value, device=value.device, dtype=torch.bool).scatter(
            -1, pos_top_n, self.invert_keep
        )
    @override
    def __call__(self, input: IdsTensor) -> tuple[IdsTensor, IdsTensor]:
        """Sample a sequence
        Args:
            input: Input sequence of ids of shape [batch, sequence]
        Returns:
            input_replaced: A replaced sequence [batch, sequence]
            replacement_mask: Boolean mask identifying which token has been replaced [batch, sequence]
        """
        token_sampled = self.sampler(input)
        input_replaced = input * ~self.replacement_mask + token_sampled * self.replacement_mask
        return input_replaced, self.replacement_mask
class UniformTokenReplacer(TokenReplacer):
    """Replace tokens in a sequence where selecting is base on uniform distribution"""
    @override
    def __init__(self, sampler: TokenSampler, ratio: float) -> None:
        """Constructor
        Args:
            sampler: A :class:`~inseq.attr.feat.ops.reagent_core.TokenSampler` object for sampling replacement tokens.
            ratio: Ratio of tokens to replace in the sequence.
        """
        super().__init__(sampler)
        self.ratio = ratio
    @override
    def __call__(self, input: IdsTensor) -> tuple[IdsTensor, IdsTensor]:
        """Sample a sequence
        Args:
            input: Input sequence of ids of shape [batch, sequence]
        Returns:
            input_replaced: A replaced sequence [batch, sequence]
            replacement_mask: Boolean mask identifying which token has been replaced [batch, sequence]
        """
        sample_uniform = torch.rand(input.shape, device=input.device)
        replacement_mask = sample_uniform < self.ratio
        token_sampled = self.sampler(input)
        input_replaced = input * ~replacement_mask + token_sampled * replacement_mask
        return input_replaced, replacement_mask

================
File: inseq/attr/feat/ops/reagent_core/token_sampler.py
================
import logging
from abc import ABC, abstractmethod
from collections import defaultdict
from pathlib import Path
from typing import Any
import torch
from transformers import AutoTokenizer, PreTrainedTokenizerBase
from typing_extensions import override
from .....utils import INSEQ_ARTIFACTS_CACHE, cache_results, is_nltk_available
from .....utils.typing import IdsTensor
logger = logging.getLogger(__name__)
class TokenSampler(ABC):
    """Base class for token samplers"""
    @abstractmethod
    def __call__(self, input: IdsTensor, **kwargs) -> IdsTensor:
        """Sample tokens according to the specified strategy.
        Args:
            input: input tensor [batch, sequence]
        Returns:
            token_uniform: A sampled tensor where its shape is the same with the input
        """
        raise NotImplementedError()
class POSTagTokenSampler(TokenSampler):
    """Sample tokens from Uniform distribution on a set of words with the same POS tag."""
    def __init__(
        self,
        tokenizer: str | PreTrainedTokenizerBase,
        identifier: str = "pos_tag_sampler",
        save_cache: bool = True,
        overwrite_cache: bool = False,
        cache_dir: Path = INSEQ_ARTIFACTS_CACHE / "pos_tag_sampler_cache",
        device: str | None = None,
        tokenizer_kwargs: dict[str, Any] | None = {},
    ) -> None:
        if isinstance(tokenizer, PreTrainedTokenizerBase):
            self.tokenizer = tokenizer
        else:
            self.tokenizer = AutoTokenizer.from_pretrained(tokenizer, **tokenizer_kwargs)
        cache_filename = cache_dir / f"{identifier.split('/')[-1]}.pkl"
        self.pos2ids = self.build_pos_mapping_from_vocab(
            cache_dir,
            cache_filename,
            save_cache,
            overwrite_cache,
            tokenizer=self.tokenizer,
        )
        num_postags = len(self.pos2ids)
        self.id2pos = torch.zeros([self.tokenizer.vocab_size], dtype=torch.long, device=device)
        for pos_idx, ids in enumerate(self.pos2ids.values()):
            self.id2pos[ids] = pos_idx
        self.num_ids_per_pos = torch.tensor(
            [len(ids) for ids in self.pos2ids.values()], dtype=torch.long, device=device
        )
        self.offsets = torch.sum(
            torch.tril(torch.ones([num_postags, num_postags], device=device), diagonal=-1) * self.num_ids_per_pos,
            dim=-1,
        )
        self.compact_idx = torch.cat(
            tuple(torch.tensor(v, dtype=torch.long, device=device) for v in self.pos2ids.values())
        )
    @staticmethod
    @cache_results
    def build_pos_mapping_from_vocab(
        tokenizer: PreTrainedTokenizerBase,
        log_every: int = 5000,
    ) -> dict[str, list[int]]:
        """Build mapping from POS tags to list of token ids from tokenizer's vocabulary."""
        if not is_nltk_available():
            raise ImportError("nltk is required to build POS tag mapping. Please install nltk.")
        import nltk
        nltk.download("averaged_perceptron_tagger")
        pos2ids = defaultdict(list)
        for i in range(tokenizer.vocab_size):
            word = tokenizer.decode([i])
            _, tag = nltk.pos_tag([word.strip()])[0]
            pos2ids[tag].append(i)
            if i % log_every == 0:
                logger.info(f"Loading vocab from tokenizer - {i / tokenizer.vocab_size * 100:.2f}%")
        return pos2ids
    @override
    def __call__(self, input_ids: IdsTensor) -> IdsTensor:
        """Sample a tensor
        Args:
            input: input tensor [batch, sequence]
        Returns:
            token_uniform: A sampled tensor where its shape is the same with the input
        """
        input_ids_pos = self.id2pos[input_ids]
        sample_uniform = torch.rand(input_ids.shape, device=input_ids.device)
        compact_group_idx = (sample_uniform * self.num_ids_per_pos[input_ids_pos] + self.offsets[input_ids_pos]).long()
        return self.compact_idx[compact_group_idx]

================
File: inseq/attr/feat/ops/reagent.py
================
from typing import TYPE_CHECKING, Any
import torch
from captum._utils.typing import TargetType, TensorOrTupleOfTensorsGeneric
from torch import Tensor
from typing_extensions import override
from ....utils.typing import InseqAttribution
from .reagent_core import (
    AggregateRationalizer,
    DeltaProbImportanceScoreEvaluator,
    POSTagTokenSampler,
    TopKStoppingConditionEvaluator,
    UniformTokenReplacer,
)
if TYPE_CHECKING:
    from ....models import HuggingfaceModel
class Reagent(InseqAttribution):
    r"""Recursive attribution generator (ReAGent) method.
    Measures importance as the drop in prediction probability produced by replacing a token with a plausible
    alternative predicted by a LM.
    Reference implementation:
    `ReAGent: A Model-agnostic Feature Attribution Method for Generative Language Models
        <https://arxiv.org/abs/2402.00794>`__
    Args:
        forward_func (callable): The forward function of the model or any modification of it
        keep_top_n (int): If set to a value greater than 0, the top n tokens based on their importance score will be
            kept during the prediction inference. If set to 0, the top n will be determined by ``keep_ratio``.
        keep_ratio (float): If ``keep_top_n`` is set to 0, this specifies the proportion of tokens to keep.
        invert_keep: If specified, the top tokens selected either via ``keep_top_n`` or ``keep_ratio`` will be
            replaced instead of being kept.
        stopping_condition_top_k (int): Threshold indicating that the stop condition achieved when the predicted target
            exist in top k predictions
        replacing_ratio (float): replacing ratio of tokens for probing
        max_probe_steps (int): max_probe_steps
        num_probes (int): number of probes in parallel
    Example:
        ```
        import inseq
        model = inseq.load_model("gpt2-medium", "reagent",
            keep_top_n=5,
            stopping_condition_top_k=3,
            replacing_ratio=0.3,
            max_probe_steps=3000,
            num_probes=8
        )
        out = model.attribute("Super Mario Land is a game that developed by")
        out.show()
        ```
    """
    def __init__(
        self,
        attribution_model: "HuggingfaceModel",
        keep_top_n: int = 5,
        keep_ratio: float = None,
        invert_keep: bool = False,
        stopping_condition_top_k: int = 3,
        replacing_ratio: float = 0.3,
        max_probe_steps: int = 3000,
        num_probes: int = 16,
    ) -> None:
        super().__init__(attribution_model)
        model = attribution_model.model
        tokenizer = attribution_model.tokenizer
        model_name = attribution_model.model_name
        sampler = POSTagTokenSampler(tokenizer=tokenizer, identifier=model_name, device=attribution_model.device)
        stopping_condition_evaluator = TopKStoppingConditionEvaluator(
            model=model,
            sampler=sampler,
            top_k=stopping_condition_top_k,
            keep_top_n=keep_top_n,
            keep_ratio=keep_ratio,
            invert_keep=invert_keep,
        )
        importance_score_evaluator = DeltaProbImportanceScoreEvaluator(
            model=model,
            tokenizer=tokenizer,
            token_replacer=UniformTokenReplacer(sampler=sampler, ratio=replacing_ratio),
            stopping_condition_evaluator=stopping_condition_evaluator,
            max_steps=max_probe_steps,
        )
        self.rationalizer = AggregateRationalizer(
            importance_score_evaluator=importance_score_evaluator,
            batch_size=num_probes,
            overlap_threshold=0,
            overlap_strict_pos=True,
            keep_top_n=keep_top_n,
            keep_ratio=keep_ratio,
        )
    @override
    def attribute(  # type: ignore
        self,
        inputs: TensorOrTupleOfTensorsGeneric,
        _target: TargetType = None,
        additional_forward_args: Any = None,
    ) -> TensorOrTupleOfTensorsGeneric | tuple[TensorOrTupleOfTensorsGeneric, Tensor]:
        """Implement attribute"""
        # encoder-decoder
        if self.forward_func.is_encoder_decoder:
            # with target-side attribution
            if len(inputs) > 1:
                self.rationalizer(
                    additional_forward_args[0], additional_forward_args[2], additional_forward_args[1], True
                )
                mean_importance_score = torch.unsqueeze(self.rationalizer.mean_importance_score, 0)
                res = torch.unsqueeze(mean_importance_score, 2).repeat(1, 1, inputs[0].shape[2])
                return (
                    res[:, : additional_forward_args[0].shape[1], :],
                    res[:, additional_forward_args[0].shape[1] :, :],
                )
            # source-side only
            else:
                self.rationalizer(additional_forward_args[1], additional_forward_args[3], additional_forward_args[2])
        # decoder-only
        self.rationalizer(additional_forward_args[0], additional_forward_args[1])
        mean_importance_score = torch.unsqueeze(self.rationalizer.mean_importance_score, 0)
        res = torch.unsqueeze(mean_importance_score, 2).repeat(1, 1, inputs[0].shape[2])
        return (res,)

================
File: inseq/attr/feat/ops/sequential_integrated_gradients.py
================
# Adapted from https://github.com/josephenguehard/time_interpret/blob/main/tint/attr/seq_ig.py, licensed MIT:
# Copyright Â© 2023 Babylon Health
# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and
# associated documentation files (the â€œSoftwareâ€), to deal in the Software without restriction,
# including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense,
#  and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so,
# subject to the following conditions:
# The above copyright notice and this permission notice shall be included in all copies
# or substantial portions of the Software.
# THE SOFTWARE IS PROVIDED â€œAS ISâ€, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT
# LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
# IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,
# WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE
# OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
import typing
from collections.abc import Callable
from typing import Any
import torch
from captum._utils.common import (
    _expand_additional_forward_args,
    _expand_target,
    _format_additional_forward_args,
    _format_output,
    _is_tuple,
)
from captum._utils.typing import (
    BaselineType,
    Literal,
    TargetType,
    TensorOrTupleOfTensorsGeneric,
)
from captum.attr._utils.approximation_methods import approximation_parameters
from captum.attr._utils.attribution import GradientAttribution
from captum.attr._utils.batching import _batch_attribution
from captum.attr._utils.common import (
    _format_input_baseline,
    _reshape_and_sum,
    _validate_input,
)
from torch import Tensor
class SequentialIntegratedGradients(GradientAttribution):
    r"""
    Sequential Integrated Gradients.
    This method is the regular Integrated Gradients (IG) applied on each
    component of a sequence. However, the baseline is specific to each
    component: it keeps fixed the rest of the sequence while only setting the
    component of interest to a reference baseline.
    For instance, on a setence of m words, the attribution of each word is
    computed by running IG with a specific baseline: fixing every other word
    to their current value, and replacing the word of interest with "<pad>",
    an uninformative baseline.
    This method can be computationally expensive on long sequences, as it
    needs to compute IG on each component individually. It is therefore
    suggested to reduce ``n_steps`` when using this method on long sequences.
    Args:
        forward_func (callable):  The forward function of the model or any
            modification of it
        multiply_by_inputs (bool, optional): Indicates whether to factor
            model inputs' multiplier in the final attribution scores.
            In the literature this is also known as local vs global
            attribution. If inputs' multiplier isn't factored in,
            then that type of attribution method is also called local
            attribution. If it is, then that type of attribution
            method is called global.
            More detailed can be found here:
            https://arxiv.org/abs/1711.06104
            In case of integrated gradients, if `multiply_by_inputs`
            is set to True, final sensitivity scores are being multiplied by
            (inputs - baselines).
    References:
        `Sequential Integrated Gradients: a simple but effective method for explaining language models
        <https://arxiv.org/abs/2305.15853>`_
    Examples:
        >>> import torch as th
        >>> from tint.attr import SequentialIntegratedGradients
        >>> from tint.models import MLP
        <BLANKLINE>
        >>> inputs = th.rand(8, 7, 5)
        >>> mlp = MLP([5, 3, 1])
        <BLANKLINE>
        >>> explainer = SequentialIntegratedGradients(mlp)
        >>> attr = explainer.attribute(inputs, target=0)
    """
    def __init__(
        self,
        forward_func: Callable,
        multiply_by_inputs: bool = True,
    ) -> None:
        r"""
        Args:
        """
        GradientAttribution.__init__(self, forward_func)
        self._multiply_by_inputs = multiply_by_inputs
    # The following overloaded method signatures correspond to the case where
    # return_convergence_delta is False, then only attributions are returned,
    # and when return_convergence_delta is True, the return type is
    # a tuple with both attributions and deltas.
    @typing.overload
    def attribute(
        self,
        inputs: TensorOrTupleOfTensorsGeneric,
        baselines: BaselineType = None,
        target: TargetType = None,
        additional_forward_args: Any = None,
        n_steps: int = 50,
        method: str = "gausslegendre",
        internal_batch_size: None | int = None,
        return_convergence_delta: Literal[False] = False,
    ) -> TensorOrTupleOfTensorsGeneric:
        ...
    @typing.overload
    def attribute(
        self,
        inputs: TensorOrTupleOfTensorsGeneric,
        baselines: BaselineType = None,
        target: TargetType = None,
        additional_forward_args: Any = None,
        n_steps: int = 50,
        method: str = "gausslegendre",
        internal_batch_size: None | int = None,
        *,
        return_convergence_delta: Literal[True],
    ) -> tuple[TensorOrTupleOfTensorsGeneric, Tensor]:
        ...
    def attribute(  # type: ignore
        self,
        inputs: TensorOrTupleOfTensorsGeneric,
        baselines: BaselineType = None,
        target: TargetType = None,
        additional_forward_args: Any = None,
        n_steps: int = 50,
        method: str = "gausslegendre",
        internal_batch_size: None | int = None,
        return_convergence_delta: bool = False,
    ) -> TensorOrTupleOfTensorsGeneric | tuple[TensorOrTupleOfTensorsGeneric, Tensor]:
        r"""
        This method attributes the output of the model with given target index
        (in case it is provided, otherwise it assumes that output is a
        scalar) to the inputs of the model using the approach described above.
        In addition to that it also returns, if `return_convergence_delta` is
        set to True, integral approximation delta based on the completeness
        property of integrated gradients.
        Args:
            inputs (tensor or tuple of tensors):  Input for which integrated
                gradients are computed. If forward_func takes a single
                tensor as input, a single input tensor should be provided.
                If forward_func takes multiple tensors as input, a tuple
                of the input tensors should be provided. It is assumed
                that for all given input tensors, dimension 0 corresponds
                to the number of examples, and if multiple input tensors
                are provided, the examples must be aligned appropriately.
            baselines (scalar, tensor, tuple of scalars or tensors, optional):
                Baselines define the starting point from which integral
                is computed and can be provided as:
                - a single tensor, if inputs is a single tensor, with
                  exactly the same dimensions as inputs or the first
                  dimension is one and the remaining dimensions match
                  with inputs.
                - a single scalar, if inputs is a single tensor, which will
                  be broadcasted for each input value in input tensor.
                - a tuple of tensors or scalars, the baseline corresponding
                  to each tensor in the inputs' tuple can be:
                  - either a tensor with matching dimensions to
                    corresponding tensor in the inputs' tuple
                    or the first dimension is one and the remaining
                    dimensions match with the corresponding
                    input tensor.
                  - or a scalar, corresponding to a tensor in the
                    inputs' tuple. This scalar value is broadcasted
                    for corresponding input tensor.
                In the cases when `baselines` is not provided, we internally
                use zero scalar corresponding to each input tensor.
                Default: None
            target (int, tuple, tensor or list, optional):  Output indices for
                which gradients are computed (for classification cases,
                this is usually the target class).
                If the network returns a scalar value per example,
                no target index is necessary.
                For general 2D outputs, targets can be either:
                - a single integer or a tensor containing a single
                  integer, which is applied to all input examples
                - a list of integers or a 1D tensor, with length matching
                  the number of examples in inputs (dim 0). Each integer
                  is applied as the target for the corresponding example.
                For outputs with > 2 dimensions, targets can be either:
                - A single tuple, which contains #output_dims - 1
                  elements. This target index is applied to all examples.
                - A list of tuples with length equal to the number of
                  examples in inputs (dim 0), and each tuple containing
                  #output_dims - 1 elements. Each tuple is applied as the
                  target for the corresponding example.
                Default: None
            additional_forward_args (any, optional): If the forward function
                requires additional arguments other than the inputs for
                which attributions should not be computed, this argument
                can be provided. It must be either a single additional
                argument of a Tensor or arbitrary (non-tuple) type or a
                tuple containing multiple additional arguments including
                tensors or any arbitrary python types. These arguments
                are provided to forward_func in order following the
                arguments in inputs.
                For a tensor, the first dimension of the tensor must
                correspond to the number of examples. It will be
                repeated for each of `n_steps` along the integrated
                path. For all other types, the given argument is used
                for all forward evaluations.
                Note that attributions are not computed with respect
                to these arguments.
                Default: None
            n_steps (int, optional): The number of steps used by the approximation
                method. Default: 50.
            method (string, optional): Method for approximating the integral,
                one of `riemann_right`, `riemann_left`, `riemann_middle`,
                `riemann_trapezoid` or `gausslegendre`.
                Default: `gausslegendre` if no method is provided.
            internal_batch_size (int, optional): Divides total #steps * #examples
                data points into chunks of size at most internal_batch_size,
                which are computed (forward / backward passes)
                sequentially. internal_batch_size must be at least equal to
                #examples.
                For DataParallel models, each batch is split among the
                available devices, so evaluations on each available
                device contain internal_batch_size / num_devices examples.
                If internal_batch_size is None, then all evaluations are
                processed in one batch.
                Default: None
            return_convergence_delta (bool, optional): Indicates whether to return
                convergence delta or not. If `return_convergence_delta`
                is set to True convergence delta will be returned in
                a tuple following attributions.
                Default: False
        Returns:
            **attributions** or 2-element tuple of **attributions**, **delta**:
            - **attributions** (*tensor* or tuple of *tensors*):
                Integrated gradients with respect to each input feature.
                attributions will always be the same size as the provided
                inputs, with each value providing the attribution of the
                corresponding input index.
                If a single tensor is provided as inputs, a single tensor is
                returned. If a tuple is provided for inputs, a tuple of
                corresponding sized tensors is returned.
            - **delta** (*tensor*, returned if return_convergence_delta=True):
                The difference between the total approximated and true
                integrated gradients. This is computed using the property
                that the total sum of forward_func(inputs) -
                forward_func(baselines) must equal the total sum of the
                integrated gradient.
                Delta is calculated per example, meaning that the number of
                elements in returned delta tensor is equal to the number of
                of examples in inputs.
        Examples::
            >>> # ImageClassifier takes a single input tensor of images Nx3x32x32,
            >>> # and returns an Nx10 tensor of class probabilities.
            >>> net = ImageClassifier()
            >>> sig = SequentialIntegratedGradients(net)
            >>> input = torch.randn(2, 3, 32, 32, requires_grad=True)
            >>> # Computes integrated gradients for class 3.
            >>> attribution = sig.attribute(input, target=3)
        """
        # Keeps track whether original input is a tuple or not before
        # converting it into a tuple.
        is_inputs_tuple = _is_tuple(inputs)
        inputs, baselines = _format_input_baseline(inputs, baselines)
        _validate_input(inputs, baselines, n_steps, method)
        assert all(
            x.shape[1] == inputs[0].shape[1] for x in inputs
        ), "All inputs must have the same sequential dimension. (dimension 1)"
        indexes = range(inputs[0].shape[1])
        # Loop over the sequence
        attributions_partial_list = []
        for idx in indexes:
            if internal_batch_size is not None:
                num_examples = inputs[0].shape[0]
                attributions_partial = _batch_attribution(
                    self,
                    num_examples,
                    internal_batch_size,
                    n_steps,
                    inputs=inputs,
                    baselines=baselines,
                    target=target,
                    additional_forward_args=additional_forward_args,
                    method=method,
                    idx=idx,
                )
            else:
                attributions_partial = self._attribute(
                    inputs=inputs,
                    baselines=baselines,
                    target=target,
                    additional_forward_args=additional_forward_args,
                    n_steps=n_steps,
                    method=method,
                    idx=idx,
                )
            attributions_partial_list.append(attributions_partial)
        # Merge collected attributions
        attributions = ()
        for i in range(len(attributions_partial_list[0])):
            attributions += (
                torch.stack(
                    [x[i][:, idx, ...] for idx, x in enumerate(attributions_partial_list)],
                    dim=1,
                ),
            )
        if return_convergence_delta:
            start_point, end_point = baselines, inputs
            # computes approximation error based on the completeness axiom
            delta = self.compute_convergence_delta(
                attributions,
                start_point,
                end_point,
                additional_forward_args=additional_forward_args,
                target=target,
            )
            return _format_output(is_inputs_tuple, attributions), delta
        return _format_output(is_inputs_tuple, attributions)
    def _attribute(
        self,
        inputs: tuple[Tensor, ...],
        baselines: tuple[Tensor | int | float, ...],
        target: TargetType = None,
        additional_forward_args: Any = None,
        n_steps: int = 50,
        method: str = "gausslegendre",
        idx: int = None,
        step_sizes_and_alphas: None | tuple[list[float], list[float]] = None,
    ) -> tuple[Tensor, ...]:
        if step_sizes_and_alphas is None:
            # retrieve step size and scaling factor for specified
            # approximation method
            step_sizes_func, alphas_func = approximation_parameters(method)
            step_sizes, alphas = step_sizes_func(n_steps), alphas_func(n_steps)
        else:
            step_sizes, alphas = step_sizes_and_alphas
        # Keep only idx index if baselines is a tensor
        baselines_ = tuple(
            baseline[:, idx, ...] if isinstance(baseline, Tensor) else baseline for baseline in baselines
        )
        # scale features and compute gradients. (batch size is abbreviated as bsz)
        # scaled_features' dim -> (bsz * #steps x inputs[0].shape[1:], ...)
        # Only scale features on the idx index.
        scaled_features_tpl = tuple(
            torch.cat(
                [
                    torch.cat(
                        [input[:, :idx, ...] for _ in alphas],
                        dim=0,
                    ).requires_grad_(),
                    torch.cat(
                        [baseline + alpha * (input[:, idx, ...] - baseline) for alpha in alphas],
                        dim=0,
                    )
                    .unsqueeze(1)
                    .requires_grad_(),
                    torch.cat(
                        [input[:, idx + 1 :, ...] for _ in alphas],
                        dim=0,
                    ).requires_grad_(),
                ],
                dim=1,
            )
            for input, baseline in zip(inputs, baselines_, strict=False)
        )
        additional_forward_args = _format_additional_forward_args(additional_forward_args)
        # apply number of steps to additional forward args
        # currently, number of steps is applied only to additional forward arguments
        # that are nd-tensors. It is assumed that the first dimension is
        # the number of batches.
        # dim -> (bsz * #steps x additional_forward_args[0].shape[1:], ...)
        input_additional_args = (
            _expand_additional_forward_args(additional_forward_args, n_steps)
            if additional_forward_args is not None
            else None
        )
        expanded_target = _expand_target(target, n_steps)
        # grads: dim -> (bsz * #steps x inputs[0].shape[1:], ...)
        grads = self.gradient_func(
            forward_fn=self.forward_func,
            inputs=scaled_features_tpl,
            target_ind=expanded_target,
            additional_forward_args=input_additional_args,
        )
        # flattening grads so that we can multiply it with step-size
        # calling contiguous to avoid `memory whole` problems
        scaled_grads = [
            grad.contiguous().view(n_steps, -1) * torch.tensor(step_sizes).view(n_steps, 1).to(grad.device)
            for grad in grads
        ]
        # aggregates across all steps for each tensor in the input tuple
        # total_grads has the same dimensionality as inputs
        total_grads = tuple(
            _reshape_and_sum(scaled_grad, n_steps, grad.shape[0] // n_steps, grad.shape[1:])
            for (scaled_grad, grad) in zip(scaled_grads, grads, strict=False)
        )
        # computes attribution for each tensor in input tuple
        # attributions has the same dimensionality as inputs
        if not self.multiplies_by_inputs:
            attributions = total_grads
        else:
            attributions = tuple(
                total_grad * (input - baseline)
                for total_grad, input, baseline in zip(total_grads, inputs, baselines, strict=False)
            )
        return attributions
    def has_convergence_delta(self) -> bool:
        return True
    @property
    def multiplies_by_inputs(self):
        return self._multiply_by_inputs

================
File: inseq/attr/feat/ops/value_zeroing.py
================
# Copyright 2023 The Inseq Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import logging
from collections.abc import Callable
from enum import Enum
from types import FrameType
from typing import TYPE_CHECKING
import torch
from captum._utils.typing import TensorOrTupleOfTensorsGeneric
from torch import nn
from torch.utils.hooks import RemovableHandle
from ....utils import (
    find_block_stack,
    get_post_variable_assignment_hook,
    recursive_get_submodule,
    validate_indices,
)
from ....utils.typing import (
    EmbeddingsTensor,
    InseqAttribution,
    MultiLayerEmbeddingsTensor,
    MultiLayerScoreTensor,
    OneOrMoreIndices,
    OneOrMoreIndicesDict,
)
if TYPE_CHECKING:
    from ....models import HuggingfaceModel
logger = logging.getLogger(__name__)
class ValueZeroingSimilarityMetric(Enum):
    COSINE = "cosine"
    EUCLIDEAN = "euclidean"
class ValueZeroingModule(Enum):
    DECODER = "decoder"
    ENCODER = "encoder"
class ValueZeroing(InseqAttribution):
    """Value Zeroing method for feature attribution.
    Introduced by `Mohebbi et al. (2023) <https://aclanthology.org/2023.eacl-main.245/>`__ to quantify context mixing inside
    Transformer models. The method is based on the observation that context mixing is regulated by the value vectors
    of the attention mechanism. The method consists of two steps:
    1. Zeroing the value vectors of the attention mechanism for a given token index at a given layer of the model.
    2. Computing the similarity between hidden states produced with and without the zeroing operation, and using it
         as a measure of context mixing for the given token at the given layer.
    The method is converted into a feature attribution method by allowing for extraction of value zeroing scores at
    specific layers, or by aggregating them across layers.
    Attributes:
        SIMILARITY_METRICS (:obj:`Dict[str, Callable]`):
            Dictionary of available similarity metrics to be used forvcomputing the distance between hidden states
            produced with and without the zeroing operation. Converted to distances as 1 - produced values.
        forward_func (:obj:`AttributionModel`):
            The attribution model to be used for value zeroing.
        clean_block_output_states (:obj:`Dict[int, torch.Tensor]`):
            Dictionary to store the hidden states produced by the model without the zeroing operation.
        corrupted_block_output_states (:obj:`Dict[int, torch.Tensor]`):
            Dictionary to store the hidden states produced by the model with the zeroing operation.
    """
    SIMILARITY_METRICS = {
        "cosine": nn.CosineSimilarity(dim=-1),
        "euclidean": lambda x, y: torch.cdist(x, y, p=2),
    }
    def __init__(self, forward_func: "HuggingfaceModel") -> None:
        super().__init__(forward_func)
        self.clean_block_output_states: dict[int, EmbeddingsTensor] = {}
        self.corrupted_block_output_states: dict[int, EmbeddingsTensor] = {}
    @staticmethod
    def get_value_zeroing_hook(varname: str = "value") -> Callable[..., None]:
        """Returns a hook to zero the value vectors of the attention mechanism.
        Args:
            varname (:obj:`str`, optional): The name of the variable containing the value vectors. The variable
                is expected to be a 3D tensor of shape (batch_size, num_heads, seq_len) and is retrieved from the
                local variables of the execution frame during the forward pass.
        """
        def value_zeroing_forward_mid_hook(
            frame: FrameType,
            zeroed_token_index: int | None = None,
            zeroed_units_indices: OneOrMoreIndices | None = None,
            batch_size: int = 1,
        ) -> None:
            if varname not in frame.f_locals:
                raise ValueError(
                    f"Variable {varname} not found in the local frame."
                    f"Other variable names: {', '.join(frame.f_locals.keys())}"
                )
            # Zeroing value vectors corresponding to the given token index
            if zeroed_token_index is not None:
                values_size = frame.f_locals[varname].size()
                if len(values_size) == 3:  # Assume merged shape (bsz * num_heads, seq_len, hidden_size) e.g. Whisper
                    values = frame.f_locals[varname].view(batch_size, -1, *values_size[1:])
                elif len(values_size) == 4:  # Assume per-head shape (bsz, num_heads, seq_len, hidden_size) e.g. GPT-2
                    values = frame.f_locals[varname].clone()
                else:
                    raise ValueError(
                        f"Value vector shape {frame.f_locals[varname].size()} not supported. "
                        "Supported shapes: (batch_size, num_heads, seq_len, hidden_size) or "
                        "(batch_size * num_heads, seq_len, hidden_size)"
                    )
                zeroed_units_indices = validate_indices(values, 1, zeroed_units_indices).to(values.device)
                zeroed_token_index = torch.tensor(zeroed_token_index, device=values.device)
                # Mask heads corresponding to zeroed units and tokens corresponding to zeroed tokens
                values[:, zeroed_units_indices, zeroed_token_index] = 0
                if len(values_size) == 3:
                    frame.f_locals[varname] = values.view(-1, *values_size[1:])
                elif len(values_size) == 4:
                    frame.f_locals[varname] = values
        return value_zeroing_forward_mid_hook
    def get_states_extract_and_patch_hook(self, block_idx: int, hidden_state_idx: int = 0) -> Callable[..., None]:
        """Returns a hook to extract the produced hidden states (corrupted by value zeroing)
          and patch them with pre-computed clean states that will be passed onwards in the model forward.
        Args:
            block_idx (:obj:`int`): The idx of the block at which the hook is applied, used to store extracted states.
            hidden_state_idx (:obj:`int`, optional): The index of the hidden state in the model output tuple.
        """
        def states_extract_and_patch_forward_hook(module, args, output) -> None:
            self.corrupted_block_output_states[block_idx] = output[hidden_state_idx].clone().float().detach().cpu()
            # Rebuild the output tuple patching the clean states at the place of the corrupted ones
            output = (
                output[:hidden_state_idx]
                + (self.clean_block_output_states[block_idx].to(output[hidden_state_idx].device),)
                + output[hidden_state_idx + 1 :]
            )
            return output
        return states_extract_and_patch_forward_hook
    @staticmethod
    def has_convergence_delta() -> bool:
        return False
    def compute_modules_post_zeroing_similarity(
        self,
        inputs: TensorOrTupleOfTensorsGeneric,
        additional_forward_args: TensorOrTupleOfTensorsGeneric,
        hidden_states: MultiLayerEmbeddingsTensor,
        attention_module_name: str,
        attributed_seq_len: int | None = None,
        similarity_metric: str = ValueZeroingSimilarityMetric.COSINE.value,
        mode: str = ValueZeroingModule.DECODER.value,
        zeroed_units_indices: OneOrMoreIndicesDict | None = None,
        min_score_threshold: float = 1e-5,
        use_causal_mask: bool = False,
    ) -> MultiLayerScoreTensor:
        """Given a ``nn.ModuleList``, computes the similarity between the clean and corrupted states for each block.
        Args:
            modules (:obj:`nn.ModuleList`): The list of modules to compute the similarity for.
            hidden_states (:obj:`MultiLayerEmbeddingsTensor`): The cached hidden states of the modules to use as clean
                counterparts when computing the similarity.
            attention_module_name (:obj:`str`): The name of the attention module to zero the values for.
            attributed_seq_len (:obj:`int`): The length of the sequence to attribute. If not specified, it is assumed
                to be the same as the length of the hidden states.
            similarity_metric (:obj:`str`): The name of the similarity metric used. Default: "cosine".
            mode (:obj:`str`): The mode of the model to compute the similarity for. Default: "decoder".
            zeroed_units_indices (:obj:`Union[int, tuple[int, int], list[int]]` or :obj:`dict` with :obj:`int` keys and
                `Union[int, tuple[int, int], list[int]]` values, optional): The indices of the attention heads
                that should be zeroed to compute corrupted states.
                    - If None, all attention heads across all layers are zeroed.
                    - If an integer, the same attention head is zeroed across all layers.
                    - If a tuple of two integers, the attention heads in the range are zeroed across all layers.
                    - If a list of integers, the attention heads in the list are zeroed across all layers.
                    - If a dictionary, the keys are the layer indices and the values are the zeroed attention heads for
                      the corresponding layer. Any missing layer will not be zeroed.
                Default: None.
            min_score_threshold (:obj:`float`, optional): The minimum score threshold to consider when computing the
                similarity. Default: 1e-5.
            use_causal_mask (:obj:`bool`, optional): Whether a causal mask is applied to zeroing scores Default: False.
        Returns:
            :obj:`MultiLayerScoreTensor`: A tensor of shape ``[batch_size, seq_len, num_layer]`` containing distances
                (1 - similarity score) between original and corrupted states for each layer.
        """
        if mode == ValueZeroingModule.DECODER.value:
            modules: nn.ModuleList = find_block_stack(self.forward_func.get_decoder())
        elif mode == ValueZeroingModule.ENCODER.value:
            modules: nn.ModuleList = find_block_stack(self.forward_func.get_encoder())
        else:
            raise NotImplementedError(f"Mode {mode} not implemented for value zeroing.")
        if attributed_seq_len is None:
            attributed_seq_len = hidden_states.size(2)
        batch_size = hidden_states.size(0)
        generated_seq_len = hidden_states.size(2)
        num_layers = len(modules)
        # Store clean hidden states for later use. Starts at 1 since the first element of the modules stack is the
        # embedding layer, and we are only interested in the transformer blocks outputs.
        self.clean_block_output_states = {
            block_idx: hidden_states[:, block_idx + 1, ...].clone().detach().cpu() for block_idx in range(len(modules))
        }
        # Scores for every layer of the model
        all_scores = torch.ones(
            batch_size, num_layers, generated_seq_len, attributed_seq_len, device=hidden_states.device
        ) * float("nan")
        # Hooks:
        #   1. states_extract_and_patch_hook on the transformer block stores corrupted states and force clean states
        #      as the output of the block forward pass, i.e. the zeroing is done independently across layers.
        #   2. value_zeroing_hook on the attention module performs the value zeroing by replacing the "value" tensor
        #      during the forward (name is config-dependent) with a zeroed version for the specified token index.
        #
        # State extraction hooks can be registered only once since they are token-independent
        # Skip last block since its states are not used raw, but may have further transformations applied to them
        # (e.g. LayerNorm, Dropout). These are extracted separately from the model outputs.
        states_extraction_hook_handles: list[RemovableHandle] = []
        for block_idx in range(len(modules) - 1):
            states_extract_and_patch_hook = self.get_states_extract_and_patch_hook(block_idx, hidden_state_idx=0)
            states_extraction_hook_handles.append(
                modules[block_idx].register_forward_hook(states_extract_and_patch_hook)
            )
        # Zeroing is done for every token in the sequence separately (O(n) complexity)
        for token_idx in range(attributed_seq_len):
            value_zeroing_hook_handles: list[RemovableHandle] = []
            # Value zeroing hooks are registered for every token separately since they are token-dependent
            for block_idx, block in enumerate(modules):
                attention_module = recursive_get_submodule(block, attention_module_name)
                if attention_module is None:
                    raise ValueError(f"Attention module {attention_module_name} not found in block {block_idx}.")
                if isinstance(zeroed_units_indices, dict):
                    if block_idx not in zeroed_units_indices:
                        continue
                    zeroed_units_indices_block = zeroed_units_indices[block_idx]
                else:
                    zeroed_units_indices_block = zeroed_units_indices
                value_zeroing_hook = get_post_variable_assignment_hook(
                    module=attention_module,
                    varname=self.forward_func.config.value_vector,
                    hook_fn=self.get_value_zeroing_hook(self.forward_func.config.value_vector),
                    zeroed_token_index=token_idx,
                    zeroed_units_indices=zeroed_units_indices_block,
                    batch_size=batch_size,
                )
                value_zeroing_hook_handle = attention_module.register_forward_pre_hook(value_zeroing_hook)
                value_zeroing_hook_handles.append(value_zeroing_hook_handle)
            # Run forward pass with hooks. Fills self.corrupted_hidden_states with corrupted states across layers
            # when zeroing the specified token index.
            with torch.no_grad():
                output = self.forward_func.forward_with_output(
                    *inputs, *additional_forward_args, output_hidden_states=True
                )
                # Extract last layer states directly from the model outputs
                # This allows us to handle the presence of additional transformations (e.g. LayerNorm, Dropout)
                # in the last layer automatically.
                corrupted_states_dict = self.forward_func.get_hidden_states_dict(output)
                corrupted_decoder_last_hidden_state = (
                    corrupted_states_dict[f"{mode}_hidden_states"][:, -1, ...].clone().detach().cpu()
                )
                self.corrupted_block_output_states[len(modules) - 1] = corrupted_decoder_last_hidden_state
            for handle in value_zeroing_hook_handles:
                handle.remove()
            for block_idx in range(len(modules)):
                similarity_scores = self.SIMILARITY_METRICS[similarity_metric](
                    self.clean_block_output_states[block_idx].float(), self.corrupted_block_output_states[block_idx]
                )
                if use_causal_mask:
                    all_scores[:, block_idx, token_idx:, token_idx] = 1 - similarity_scores[:, token_idx:]
                else:
                    all_scores[:, block_idx, :, token_idx] = 1 - similarity_scores
            self.corrupted_block_output_states = {}
        for handle in states_extraction_hook_handles:
            handle.remove()
        self.clean_block_output_states = {}
        all_scores = torch.where(all_scores < min_score_threshold, torch.zeros_like(all_scores), all_scores)
        # Normalize scores to sum to 1
        per_token_sum_score = all_scores.nansum(dim=-1, keepdim=True)
        per_token_sum_score[per_token_sum_score == 0] = 1
        all_scores = all_scores / per_token_sum_score
        # Final shape: [batch_size, attributed_seq_len, generated_seq_len, num_layers]
        return all_scores.permute(0, 3, 2, 1)
    def attribute(
        self,
        inputs: TensorOrTupleOfTensorsGeneric,
        additional_forward_args: TensorOrTupleOfTensorsGeneric,
        similarity_metric: str = ValueZeroingSimilarityMetric.COSINE.value,
        encoder_zeroed_units_indices: OneOrMoreIndicesDict | None = None,
        decoder_zeroed_units_indices: OneOrMoreIndicesDict | None = None,
        cross_zeroed_units_indices: OneOrMoreIndicesDict | None = None,
        encoder_hidden_states: MultiLayerEmbeddingsTensor | None = None,
        decoder_hidden_states: MultiLayerEmbeddingsTensor | None = None,
        output_decoder_self_scores: bool = True,
        output_encoder_self_scores: bool = True,
    ) -> TensorOrTupleOfTensorsGeneric:
        """Perform attribution using the Value Zeroing method.
        Args:
            similarity_metric (:obj:`str`, optional): The similarity metric to use for computing the distance between
                hidden states produced with and without the zeroing operation. Default: cosine similarity.
            zeroed_units_indices (:obj:`Union[int, tuple[int, int], list[int]]` or :obj:`dict` with :obj:`int` keys and
                `Union[int, tuple[int, int], list[int]]` values, optional): The indices of the attention heads
                that should be zeroed to compute corrupted states.
                    - If None, all attention heads across all layers are zeroed.
                    - If an integer, the same attention head is zeroed across all layers.
                    - If a tuple of two integers, the attention heads in the range are zeroed across all layers.
                    - If a list of integers, the attention heads in the list are zeroed across all layers.
                    - If a dictionary, the keys are the layer indices and the values are the zeroed attention heads for
                        the corresponding layer.
                Default: None (all heads are zeroed for every layer).
            encoder_hidden_states (:obj:`torch.Tensor`, optional): A tensor of shape ``[batch_size, num_layers + 1,
                source_seq_len, hidden_size]`` containing hidden states of the encoder. Available only for
                encoder-decoders models. Default: None.
            decoder_hidden_states (:obj:`torch.Tensor`, optional): A tensor of shape ``[batch_size, num_layers + 1,
                target_seq_len, hidden_size]`` containing hidden states of the decoder.
            output_decoder_self_scores (:obj:`bool`, optional): Whether to produce scores derived from zeroing the
                decoder self-attention value vectors in encoder-decoder models. Cannot be false for decoder-only, or
                if target-side attribution is requested using `attribute_target=True`. Default: True.
            output_encoder_self_scores (:obj:`bool`, optional): Whether to produce scores derived from zeroing the
                encoder self-attention value vectors in encoder-decoder models. Default: True.
        Returns:
            `TensorOrTupleOfTensorsGeneric`: Attribution outputs for source-only or source + target feature attribution
        """
        if similarity_metric not in self.SIMILARITY_METRICS:
            raise ValueError(
                f"Similarity metric {similarity_metric} not available."
                f"Available metrics: {','.join(self.SIMILARITY_METRICS.keys())}"
            )
        decoder_scores = None
        if not self.forward_func.is_encoder_decoder or output_decoder_self_scores or len(inputs) > 1:
            decoder_scores = self.compute_modules_post_zeroing_similarity(
                inputs=inputs,
                additional_forward_args=additional_forward_args,
                hidden_states=decoder_hidden_states,
                attention_module_name=self.forward_func.config.self_attention_module,
                similarity_metric=similarity_metric,
                mode=ValueZeroingModule.DECODER.value,
                zeroed_units_indices=decoder_zeroed_units_indices,
                use_causal_mask=True,
            )
        # Encoder-decoder models also perform zeroing on the encoder self-attention and cross-attention values
        # Adapted from https://github.com/hmohebbi/ContextMixingASR/blob/master/scoring/valueZeroing.py
        if self.forward_func.is_encoder_decoder:
            encoder_scores = None
            if output_encoder_self_scores:
                encoder_scores = self.compute_modules_post_zeroing_similarity(
                    inputs=inputs,
                    additional_forward_args=additional_forward_args,
                    hidden_states=encoder_hidden_states,
                    attention_module_name=self.forward_func.config.self_attention_module,
                    similarity_metric=similarity_metric,
                    mode=ValueZeroingModule.ENCODER.value,
                    zeroed_units_indices=encoder_zeroed_units_indices,
                )
            cross_scores = self.compute_modules_post_zeroing_similarity(
                inputs=inputs,
                additional_forward_args=additional_forward_args,
                hidden_states=decoder_hidden_states,
                attributed_seq_len=encoder_hidden_states.size(2),
                attention_module_name=self.forward_func.config.cross_attention_module,
                similarity_metric=similarity_metric,
                mode=ValueZeroingModule.DECODER.value,
                zeroed_units_indices=cross_zeroed_units_indices,
            )
            return encoder_scores, cross_scores, decoder_scores
        elif encoder_zeroed_units_indices is not None or cross_zeroed_units_indices is not None:
            logger.warning(
                "Zeroing indices for encoder and cross-attentions were specified, but the model is not an "
                "encoder-decoder. Use `decoder_zeroed_units_indices` to parametrize zeroing for the decoder module."
            )
        return (decoder_scores,)

================
File: inseq/attr/feat/perturbation_attribution.py
================
import logging
from typing import TYPE_CHECKING, Any
from captum.attr import Occlusion
from ...data import (
    CoarseFeatureAttributionStepOutput,
    GranularFeatureAttributionStepOutput,
    MultiDimensionalFeatureAttributionStepOutput,
)
from ...utils import Registry
from .attribution_utils import get_source_target_attributions
from .gradient_attribution import FeatureAttribution
from .ops import Lime, Reagent, ValueZeroing
if TYPE_CHECKING:
    from ...models import HuggingfaceModel
logger = logging.getLogger(__name__)
class PerturbationAttributionRegistry(FeatureAttribution, Registry):
    """Perturbation-based attribution method registry."""
    pass
class OcclusionAttribution(PerturbationAttributionRegistry):
    """Occlusion-based attribution method.
    Reference implementation:
    `https://captum.ai/api/occlusion.html <https://captum.ai/api/occlusion.html>`__.
    Usage in other implementations:
    `niuzaisheng/AttExplainer <https://github.com/niuzaisheng/AttExplainer/blob/main/baseline_methods/\
    explain_baseline_captum.py>`__
    `andrewPoulton/explainable-asag <https://github.com/andrewPoulton/explainable-asag/blob/main/explanation.py>`__
    `copenlu/xai-benchmark <https://github.com/copenlu/xai-benchmark/blob/master/saliency_gen/\
    interpret_grads_occ.py>`__
    `DFKI-NLP/thermostat <https://github.com/DFKI-NLP/thermostat/blob/main/src/thermostat/explainers/occlusion.py>`__
    """
    method_name = "occlusion"
    def __init__(self, attribution_model):
        super().__init__(attribution_model)
        self.use_baselines = True
        self.method = Occlusion(self.attribution_model)
    def attribute_step(
        self,
        attribute_fn_main_args: dict[str, Any],
        attribution_args: dict[str, Any] = {},
    ) -> CoarseFeatureAttributionStepOutput:
        r"""Sliding window shapes is defined as a tuple.
        First entry is between 1 and length of input.
        Second entry is given by the embedding dimension of the underlying model.
        If not explicitly given via attribution_args, the default is (1, embedding_dim).
        """
        if "sliding_window_shapes" not in attribution_args:
            embedding_layer = self.attribution_model.get_embedding_layer()
            attribution_args["sliding_window_shapes"] = tuple(
                (1, embedding_layer.embedding_dim) for _ in range(len(attribute_fn_main_args["inputs"]))
            )
            if len(attribution_args["sliding_window_shapes"]) == 1:
                attribution_args["sliding_window_shapes"] = attribution_args["sliding_window_shapes"][0]
        attr = self.method.attribute(**attribute_fn_main_args, **attribution_args)
        source_attributions, target_attributions = get_source_target_attributions(
            attr, self.attribution_model.is_encoder_decoder
        )
        # Make sure that the computed attributions are the same for every "embedding slice"
        attr = source_attributions if source_attributions is not None else target_attributions
        embedding_attributions = [attr[:, :, i].tolist()[0] for i in range(attr.shape[2])]
        assert all(x == embedding_attributions[0] for x in embedding_attributions)
        # Access the first embedding slice, provided it's the same result as the other slices
        if source_attributions is not None:
            source_attributions = source_attributions[:, :, 0].abs()
        if target_attributions is not None:
            target_attributions = target_attributions[:, :, 0].abs()
        return CoarseFeatureAttributionStepOutput(
            source_attributions=source_attributions.to("cpu") if source_attributions is not None else None,
            target_attributions=target_attributions.to("cpu") if target_attributions is not None else None,
        )
class LimeAttribution(PerturbationAttributionRegistry):
    """LIME-based attribution method.
    Reference implementations:
    `https://captum.ai/api/lime.html <https://captum.ai/api/lime.html>`__.
    `https://github.com/DFKI-NLP/thermostat/ <https://github.com/DFKI-NLP/thermostat/>`__.
    `https://github.com/copenlu/ALPS_2021 <https://github.com/copenlu/ALPS_2021>`__.
    The main part of the code is in Lime of ops/lime.py.
    """
    method_name = "lime"
    def __init__(self, attribution_model, **kwargs):
        super().__init__(attribution_model)
        self.method = Lime(attribution_model=self.attribution_model, **kwargs)
    def attribute_step(
        self,
        attribute_fn_main_args: dict[str, Any],
        attribution_args: dict[str, Any] = {},
    ) -> GranularFeatureAttributionStepOutput:
        if len(attribute_fn_main_args["inputs"]) > 1:
            # Captum's `_evaluate_batch` function for LIME does not account for multiple inputs when encoder-decoder
            # models and attribute_target=True are used. The model output is of length two and if the inputs are either
            # of length one (list containing a tuple) or of length two (tuple unpacked from the list), an error is
            # raised. A workaround will be added soon.
            raise NotImplementedError(
                "LIME attribution with attribute_target=True currently not supported for encoder-decoder models."
            )
        out = super().attribute_step(attribute_fn_main_args, attribution_args)
        return GranularFeatureAttributionStepOutput(
            source_attributions=out.source_attributions,
            target_attributions=out.target_attributions,
            sequence_scores=out.sequence_scores,
        )
class ReagentAttribution(PerturbationAttributionRegistry):
    """Recursive attribution generator (ReAGent) method.
    Measures importance as the drop in prediction probability produced by replacing a token with a plausible
    alternative predicted by a LM.
    Reference implementation:
    `ReAGent: A Model-agnostic Feature Attribution Method for Generative Language Models <https://arxiv.org/abs/2402.00794>`__
    """
    method_name = "reagent"
    def __init__(
        self,
        attribution_model: "HuggingfaceModel",
        keep_top_n: int = 5,
        keep_ratio: float = None,
        invert_keep: bool = False,
        stopping_condition_top_k: int = 3,
        replacing_ratio: float = 0.3,
        max_probe_steps: int = 3000,
        num_probes: int = 16,
    ):
        """ReAGent method constructor.
        Args:
            keep_top_n (:obj:`int`, `optional`): If set to a value greater than 0, the top n tokens based on their importance score will be
                kept during the prediction inference. If set to 0, the top n will be determined by ``keep_ratio``. Default: ``5``.
            keep_ratio (:obj:`float`, `optional`): If ``keep_top_n`` is set to 0, this specifies the proportion of tokens to keep.
            invert_keep (:obj:`bool`, `optional`): If specified, the top tokens selected either via ``keep_top_n`` or ``keep_ratio`` will be
                replaced instead of being kept. Default: ``False``.
            stopping_condition_top_k (:obj:`int`, `optional`): Threshold indicating that the stop condition achieved when the predicted target
                exist in top k predictions. Default: ``3``.
            replacing_ratio (:obj:`float`, `optional`): replacing ratio of tokens for probing. Default: ``0.3``.
            max_probe_steps (:obj:`int`, `optional`): Max number of steps before stopping the probing. Default: ``3000``.
            num_probes (:obj:`int`, `optional`): Number of probes performed in parallel. Default: ``16``.
        """
        super().__init__(attribution_model)
        # Custom target attribution is currently not supported
        self.use_predicted_target = False
        self.method = Reagent(
            attribution_model=self.attribution_model,
            keep_top_n=keep_top_n,
            keep_ratio=keep_ratio,
            invert_keep=invert_keep,
            stopping_condition_top_k=stopping_condition_top_k,
            replacing_ratio=replacing_ratio,
            max_probe_steps=max_probe_steps,
            num_probes=num_probes,
        )
    def attribute_step(
        self,
        attribute_fn_main_args: dict[str, Any],
        attribution_args: dict[str, Any] = {},
    ) -> GranularFeatureAttributionStepOutput:
        out = super().attribute_step(attribute_fn_main_args, attribution_args)
        return GranularFeatureAttributionStepOutput(
            source_attributions=out.source_attributions,
            target_attributions=out.target_attributions,
            sequence_scores=out.sequence_scores,
        )
class ValueZeroingAttribution(PerturbationAttributionRegistry):
    """Value Zeroing method for feature attribution.
    Introduced by `Mohebbi et al. (2023) <https://aclanthology.org/2023.eacl-main.245/>`__ to quantify context mixing
    in Transformer models. The method is based on the observation that context mixing is regulated by the value vectors
    of the attention mechanism. The method consists of two steps:
    1. Zeroing the value vectors of the attention mechanism for a given token index at a given layer of the model.
    2. Computing the similarity between hidden states produced with and without the zeroing operation, and using it
       as a measure of context mixing for the given token at the given layer.
    The method is converted into a feature attribution method by allowing for extraction of value zeroing scores at
    specific layers, or by aggregating them across layers.
    Reference implementations:
    - Original implementation: `hmohebbi/ValueZeroing <https://github.com/hmohebbi/ValueZeroing>`__
    - Encoder-decoder implementation: `hmohebbi/ContextMixingASR <https://github.com/hmohebbi/ContextMixingASR>`__
    Args:
        similarity_metric (:obj:`str`, optional): The similarity metric to use for computing the distance between
            hidden states produced with and without the zeroing operation. Options: cosine, euclidean. Default: cosine.
        encoder_zeroed_units_indices (:obj:`Union[int, tuple[int, int], list[int], dict]`, optional): The indices of
            the attention heads that should be zeroed to compute corrupted states in the encoder self-attention module.
            Not used for decoder-only models, or if ``output_encoder_self_scores`` is False. Format
            - None: all attention heads across all layers are zeroed.
            - int: the same attention head is zeroed across all layers.
            - tuple of two integers: the attention heads in the range are zeroed across all layers.
            - list of integers: the attention heads in the list are zeroed across all layers.
            - dictionary: the keys are the layer indices and the values are the zeroed attention heads for the corresponding layer.
            Default: None (all heads are zeroed for every encoder layer).
        decoder_zeroed_units_indices (:obj:`Union[int, tuple[int, int], list[int], dict]`, optional): Same as
            ``encoder_zeroed_units_indices`` but for the decoder self-attention module. Not used for encoder-decoder
            models or if ``output_decoder_self_scores`` is False. Default: None (all heads are zeroed for every decoder layer).
        cross_zeroed_units_indices (:obj:`Union[int, tuple[int, int], list[int], dict]`, optional): Same as
            ``encoder_zeroed_units_indices`` but for the cross-attention module in encoder-decoder models. Not used
            if the model is decoder-only. Default: None (all heads are zeroed for every layer).
        output_decoder_self_scores (:obj:`bool`, optional): Whether to produce scores derived from zeroing the
            decoder self-attention value vectors in encoder-decoder models. Cannot be false for decoder-only, or
            if target-side attribution is requested using `attribute_target=True`. Default: True.
        output_encoder_self_scores (:obj:`bool`, optional): Whether to produce scores derived from zeroing the
            encoder self-attention value vectors in encoder-decoder models. Default: True.
    Returns:
        :class:`~inseq.data.MultiDimensionalFeatureAttributionStepOutput`: The final dimension returned by the method
        is ``[attributed_seq_len, generated_seq_len, num_layers]``. If ``output_decoder_self_scores`` and
        ``output_encoder_self_scores`` are True, the respective scores are returned in the ``sequence_scores``
        output dictionary.
    """
    method_name = "value_zeroing"
    def __init__(self, attribution_model, **kwargs):
        super().__init__(attribution_model, hook_to_model=False)
        # Hidden states will be passed to the attribute_step method
        self.use_hidden_states = True
        # Does not rely on predicted output (i.e. decoding strategy agnostic)
        self.use_predicted_target = False
        # Uses model configuration to access attention module and value vector variable
        self.use_model_config = True
        # Needs only the final generation step to extract scores
        self.is_final_step_method = True
        self.method = ValueZeroing(attribution_model)
        self.hook(**kwargs)
    def attribute_step(
        self,
        attribute_fn_main_args: dict[str, Any],
        attribution_args: dict[str, Any] = {},
    ) -> MultiDimensionalFeatureAttributionStepOutput:
        attr = self.method.attribute(**attribute_fn_main_args, **attribution_args)
        encoder_self_scores, decoder_cross_scores, decoder_self_scores = get_source_target_attributions(
            attr, self.attribution_model.is_encoder_decoder, has_sequence_scores=True
        )
        sequence_scores = {}
        if self.attribution_model.is_encoder_decoder:
            if len(attribute_fn_main_args["inputs"]) > 1:
                target_attributions = decoder_self_scores.to("cpu")
            else:
                target_attributions = None
                if decoder_self_scores is not None:
                    sequence_scores["decoder_self_scores"] = decoder_self_scores.to("cpu")
            if encoder_self_scores is not None:
                sequence_scores["encoder_self_scores"] = encoder_self_scores.to("cpu")
            return MultiDimensionalFeatureAttributionStepOutput(
                source_attributions=decoder_cross_scores.to("cpu"),
                target_attributions=target_attributions,
                sequence_scores=sequence_scores,
                _num_dimensions=1,  # num_layers
            )
        return MultiDimensionalFeatureAttributionStepOutput(
            source_attributions=None,
            target_attributions=decoder_self_scores,
            _num_dimensions=1,  # num_layers
        )

================
File: inseq/attr/step_functions.py
================
import logging
from dataclasses import dataclass
from inspect import signature
from typing import TYPE_CHECKING, Any, Protocol
import torch
import torch.nn.functional as F
from transformers.modeling_outputs import ModelOutput
from ..data import FeatureAttributionInput
from ..data.aggregation_functions import DEFAULT_ATTRIBUTION_AGGREGATE_DICT
from ..utils import extract_signature_args, filter_logits, top_p_logits_mask
from ..utils.contrast_utils import _get_contrast_inputs, _setup_contrast_args, contrast_fn_docstring
from ..utils.typing import EmbeddingsTensor, IdsTensor, SingleScorePerStepTensor, TargetIdsTensor
if TYPE_CHECKING:
    from ..models import AttributionModel
logger = logging.getLogger(__name__)
@dataclass
class StepFunctionBaseArgs:
    """Base class for step function base arguments. These arguments are passed to all step functions and are
    complemented by the ones defined in the step function signature.
    Attributes:
        attribution_model (:class:`~inseq.models.AttributionModel`): The attribution model used in the current step.
        forward_output (:class:`~inseq.models.ModelOutput`): The output of the model's forward pass.
        target_ids (:obj:`torch.Tensor`): Tensor of target token ids of size :obj:`(batch_size,)` corresponding to
            the target predicted tokens for the next generation step.
        is_attributed_fn (:obj:`bool`, `optional`, defaults to :obj:`False`): Whether the step function is being used
            as attribution target. Defaults to :obj:`False`. Enables custom behavior that is different whether the fn
            is used as target or not.
        encoder_input_ids (:obj:`torch.Tensor`): Tensor of ids of encoder input tokens of size
            :obj:`(batch_size, source_seq_len)`, representing encoder inputs at the present step. Available only for
            encoder-decoder models.
        decoder_input_ids (:obj:`torch.Tensor`): Tensor of ids of decoder input tokens of size
            :obj:`(batch_size, target_seq_len)`, representing decoder inputs at the present step.
        encoder_input_embeds (:obj:`torch.Tensor`): Tensor of embeddings of encoder input tokens of size
            :obj:`(batch_size, source_seq_len, hidden_size)`, representing encoder inputs at the present step.
            Available only for encoder-decoder models.
        decoder_input_embeds (:obj:`torch.Tensor`): Tensor of embeddings of decoder input tokens of size
            :obj:`(batch_size, target_seq_len, hidden_size)`, representing decoder inputs at the present step.
        encoder_attention_mask (:obj:`torch.Tensor`): Tensor of attention mask of encoder input tokens of size
            :obj:`(batch_size, source_seq_len)`, used for masking padding tokens in the encoder input. Available only
            for encoder-decoder models.
        decoder_attention_mask (:obj:`torch.Tensor`): Tensor of attention mask of decoder input tokens of size
            :obj:`(batch_size, target_seq_len)`, used for masking padding tokens in the decoder input.
    """
    attribution_model: "AttributionModel"
    forward_output: ModelOutput
    target_ids: TargetIdsTensor
    decoder_input_ids: IdsTensor
    decoder_input_embeds: EmbeddingsTensor
    decoder_attention_mask: IdsTensor
    is_attributed_fn: bool
@dataclass
class StepFunctionEncoderDecoderArgs(StepFunctionBaseArgs):
    encoder_input_ids: IdsTensor
    encoder_input_embeds: EmbeddingsTensor
    encoder_attention_mask: IdsTensor
@dataclass
class StepFunctionDecoderOnlyArgs(StepFunctionBaseArgs):
    pass
StepFunctionArgs = StepFunctionEncoderDecoderArgs | StepFunctionDecoderOnlyArgs
class StepFunction(Protocol):
    def __call__(
        self,
        args: StepFunctionArgs,
        **kwargs,
    ) -> SingleScorePerStepTensor:
        ...
def logit_fn(args: StepFunctionArgs) -> SingleScorePerStepTensor:
    """Compute the logit of the target_ids from the model's output logits."""
    logits = args.attribution_model.output2logits(args.forward_output)
    target_ids = args.target_ids.reshape(logits.shape[0], 1).to(logits.device)
    return logits.gather(-1, target_ids).squeeze(-1)
def probability_fn(args: StepFunctionArgs, logprob: bool = False) -> SingleScorePerStepTensor:
    """Compute the probabilty of target_ids from the model's output logits."""
    logits = args.attribution_model.output2logits(args.forward_output)
    target_ids = args.target_ids.reshape(logits.shape[0], 1).to(logits.device)
    logits = logits.softmax(dim=-1) if not logprob else logits.log_softmax(dim=-1)
    # Extracts the ith score from the softmax output over the vocabulary (dim -1 of the logits)
    # where i is the value of the corresponding index in target_ids.
    return logits.gather(-1, target_ids).squeeze(-1)
def entropy_fn(args: StepFunctionArgs) -> SingleScorePerStepTensor:
    """Compute the entropy of the model's output distribution."""
    logits = args.attribution_model.output2logits(args.forward_output)
    entropy = torch.zeros(logits.size(0)).to(logits.device)
    for i in range(logits.size(0)):
        entropy[i] = torch.distributions.Categorical(logits=logits[i]).entropy()
    return entropy
def crossentropy_fn(args: StepFunctionArgs) -> SingleScorePerStepTensor:
    """Compute the cross entropy between the target_ids and the logits.
    See: https://github.com/ZurichNLP/nmtscore/blob/master/src/nmtscore/models/m2m100.py#L99.
    """
    logits = args.attribution_model.output2logits(args.forward_output)
    return F.cross_entropy(logits, args.target_ids.to(logits.device), reduction="none").squeeze(-1)
def perplexity_fn(args: StepFunctionArgs) -> SingleScorePerStepTensor:
    """Compute perplexity of the target_ids from the logits.
    Perplexity is the weighted branching factor. If we have a perplexity of 100, it means that whenever the model is
    trying to guess the next word it is as confused as if it had to pick between 100 words.
    Reference: https://chiaracampagnola.io/2020/05/17/perplexity-in-language-models/.
    """
    return 2 ** crossentropy_fn(args)
@contrast_fn_docstring()
def contrast_logits_fn(
    args: StepFunctionArgs,
    contrast_sources: FeatureAttributionInput | None = None,
    contrast_targets: FeatureAttributionInput | None = None,
    contrast_targets_alignments: list[list[tuple[int, int]]] | None = None,
    contrast_force_inputs: bool = False,
    skip_special_tokens: bool = False,
):
    """Returns the logit of a generation target given contrastive context or target prediction alternative.
    If only ``contrast_targets`` are specified, the logit of the contrastive prediction is computed given same
    context. The logit for the same token given contrastive source/target preceding context can also be computed
    using ``contrast_sources`` without specifying ``contrast_targets``.
    """
    c_args = _setup_contrast_args(
        args,
        contrast_sources=contrast_sources,
        contrast_targets=contrast_targets,
        contrast_targets_alignments=contrast_targets_alignments,
        contrast_force_inputs=contrast_force_inputs,
        skip_special_tokens=skip_special_tokens,
    )
    return logit_fn(c_args)
@contrast_fn_docstring()
def contrast_prob_fn(
    args: StepFunctionArgs,
    contrast_sources: FeatureAttributionInput | None = None,
    contrast_targets: FeatureAttributionInput | None = None,
    contrast_targets_alignments: list[list[tuple[int, int]]] | None = None,
    logprob: bool = False,
    contrast_force_inputs: bool = False,
    skip_special_tokens: bool = False,
):
    """Returns the probability of a generation target given contrastive context or target prediction alternative.
    If only ``contrast_targets`` are specified, the probability of the contrastive prediction is computed given same
    context. The probability for the same token given contrastive source/target preceding context can also be computed
    using ``contrast_sources`` without specifying ``contrast_targets``.
    """
    c_args = _setup_contrast_args(
        args,
        contrast_sources=contrast_sources,
        contrast_targets=contrast_targets,
        contrast_targets_alignments=contrast_targets_alignments,
        contrast_force_inputs=contrast_force_inputs,
        skip_special_tokens=skip_special_tokens,
    )
    return probability_fn(c_args, logprob=logprob)
@contrast_fn_docstring()
def pcxmi_fn(
    args: StepFunctionArgs,
    contrast_sources: FeatureAttributionInput | None = None,
    contrast_targets: FeatureAttributionInput | None = None,
    contrast_targets_alignments: list[list[tuple[int, int]]] | None = None,
    contrast_force_inputs: bool = False,
    skip_special_tokens: bool = False,
) -> SingleScorePerStepTensor:
    """Compute the pointwise conditional cross-mutual information (P-CXMI) of target ids given original and contrastive
    input options. The P-CXMI is defined as the negative log-ratio between the conditional probability of the target
    given the original input and the conditional probability of the target given the contrastive input, as defined
    by `Yin et al. (2021) <https://arxiv.org/abs/2109.07446>`__.
    """
    original_probs = probability_fn(args)
    contrast_probs = contrast_prob_fn(
        args=args,
        contrast_sources=contrast_sources,
        contrast_targets=contrast_targets,
        contrast_targets_alignments=contrast_targets_alignments,
        contrast_force_inputs=contrast_force_inputs,
        skip_special_tokens=skip_special_tokens,
    ).to(original_probs.device)
    return -torch.log2(torch.div(original_probs, contrast_probs))
@contrast_fn_docstring()
def kl_divergence_fn(
    args: StepFunctionArgs,
    contrast_sources: FeatureAttributionInput | None = None,
    contrast_targets: FeatureAttributionInput | None = None,
    contrast_targets_alignments: list[list[tuple[int, int]]] | None = None,
    top_k: int = 0,
    top_p: float = 1.0,
    min_tokens_to_keep: int = 1,
    contrast_force_inputs: bool = False,
    skip_special_tokens: bool = False,
) -> SingleScorePerStepTensor:
    """Compute the pointwise Kullback-Leibler divergence of target ids given original and contrastive input options.
    The KL divergence is the expectation of the log difference between the probabilities of regular (P) and contrastive
    (Q) inputs.
    Args:
        top_k (:obj:`int`): If set to a value > 0, only the top :obj:`top_k` tokens will be considered for
            computing the KL divergence. Defaults to :obj:`0` (no top-k selection).
        top_p (:obj:`float`): If set to a value > 0 and < 1, only the tokens with cumulative probability above
            :obj:`top_p` will be considered for computing the KL divergence. Defaults to :obj:`1.0` (no filtering),
            applied before :obj:`top_k` filtering.
        min_tokens_to_keep (:obj:`int`): Minimum number of tokens to keep with :obj:`top_p` filtering. Defaults to
            :obj:`1`.
    """
    if not contrast_force_inputs and args.is_attributed_fn:
        raise RuntimeError(
            "Using KL divergence as attribution target might lead to unexpected results, depending on the attribution"
            "method used. Use --contrast_force_inputs in the model.attribute call to proceed."
        )
    original_logits: torch.Tensor = args.attribution_model.output2logits(args.forward_output)
    contrast_inputs = _get_contrast_inputs(
        args=args,
        contrast_sources=contrast_sources,
        contrast_targets=contrast_targets,
        contrast_targets_alignments=contrast_targets_alignments,
        return_contrastive_target_ids=False,
        return_contrastive_batch=True,
        skip_special_tokens=skip_special_tokens,
    )
    c_forward_output = args.attribution_model.get_forward_output(
        contrast_inputs.batch, use_embeddings=args.attribution_model.is_encoder_decoder
    )
    contrast_logits: torch.Tensor = args.attribution_model.output2logits(c_forward_output).to(original_logits.device)
    filtered_original_logits, filtered_contrast_logits = filter_logits(
        original_logits=original_logits,
        contrast_logits=contrast_logits,
        top_p=top_p,
        top_k=top_k,
        min_tokens_to_keep=min_tokens_to_keep,
    )
    filtered_original_logprobs = F.log_softmax(filtered_original_logits, dim=-1)
    filtered_contrast_logprobs = F.log_softmax(filtered_contrast_logits, dim=-1)
    kl_divergence = torch.zeros(filtered_original_logprobs.size(0))
    for i in range(filtered_original_logits.size(0)):
        kl_divergence[i] = F.kl_div(
            filtered_contrast_logprobs[i], filtered_original_logprobs[i], reduction="sum", log_target=True
        )
    return kl_divergence
@contrast_fn_docstring()
def contrast_prob_diff_fn(
    args: StepFunctionArgs,
    contrast_sources: FeatureAttributionInput | None = None,
    contrast_targets: FeatureAttributionInput | None = None,
    contrast_targets_alignments: list[list[tuple[int, int]]] | None = None,
    logprob: bool = False,
    contrast_force_inputs: bool = False,
    skip_special_tokens: bool = False,
):
    """Returns the difference between next step probability for a candidate generation target vs. a contrastive
    alternative. Can be used as attribution target to answer the question: "Which features were salient in the
    choice of picking the selected token rather than its contrastive alternative?". Follows the implementation
    of `Yin and Neubig (2022) <https://aclanthology.org/2022.emnlp-main.14>`__. Can also be used to compute the
    difference in probability for the same token given contrastive source/target preceding context using
    ``contrast_sources`` without specifying ``contrast_targets``.
    """
    model_probs = probability_fn(args, logprob=logprob)
    contrast_probs = contrast_prob_fn(
        args=args,
        contrast_sources=contrast_sources,
        contrast_targets=contrast_targets,
        contrast_targets_alignments=contrast_targets_alignments,
        logprob=logprob,
        contrast_force_inputs=contrast_force_inputs,
        skip_special_tokens=skip_special_tokens,
    ).to(model_probs.device)
    return model_probs - contrast_probs
@contrast_fn_docstring()
def contrast_logits_diff_fn(
    args: StepFunctionArgs,
    contrast_sources: FeatureAttributionInput | None = None,
    contrast_targets: FeatureAttributionInput | None = None,
    contrast_targets_alignments: list[list[tuple[int, int]]] | None = None,
    contrast_force_inputs: bool = False,
    skip_special_tokens: bool = False,
):
    """Equivalent to ``contrast_prob_diff_fn`` but for logits. The original target function used in
    `Yin and Neubig (2022) <https://aclanthology.org/2022.emnlp-main.14>`__
    """
    model_logits = logit_fn(args)
    contrast_logits = contrast_logits_fn(
        args=args,
        contrast_sources=contrast_sources,
        contrast_targets=contrast_targets,
        contrast_targets_alignments=contrast_targets_alignments,
        contrast_force_inputs=contrast_force_inputs,
        skip_special_tokens=skip_special_tokens,
    ).to(model_logits.device)
    return model_logits - contrast_logits
@contrast_fn_docstring()
def in_context_pvi_fn(
    args: StepFunctionArgs,
    contrast_sources: FeatureAttributionInput | None = None,
    contrast_targets: FeatureAttributionInput | None = None,
    contrast_targets_alignments: list[list[tuple[int, int]]] | None = None,
    contrast_force_inputs: bool = False,
    skip_special_tokens: bool = False,
):
    """Returns the in-context pointwise V-usable information as defined by `Lu et al. (2023)
    <https://arxiv.org/abs/2310.12300>`__. In-context PVI is a variant of P-CXMI that captures the amount of usable
    information in a given contextual example, i.e. how much context information contributes to model's prediction.
    In-context PVI was used by `Lu et al. (2023) <https://arxiv.org/abs/2310.12300>`__ to estimate example difficulty
    for a given model, and by `Prasad et al. (2023) <https://arxiv.org/abs/2304.10703>`__ to measure the
    informativeness of intermediate reasoning steps in chain-of-thought prompting.
    Reference implementation: https://github.com/boblus/in-context-pvi/blob/main/in_context_pvi.ipynb
    """
    orig_logprob = probability_fn(args, logprob=True)
    contrast_logprob = contrast_prob_fn(
        args=args,
        contrast_sources=contrast_sources,
        contrast_targets=contrast_targets,
        contrast_targets_alignments=contrast_targets_alignments,
        logprob=True,
        contrast_force_inputs=contrast_force_inputs,
        skip_special_tokens=skip_special_tokens,
    ).to(orig_logprob.device)
    return -orig_logprob + contrast_logprob
def mc_dropout_prob_avg_fn(
    args: StepFunctionArgs,
    n_mcd_steps: int = 5,
    logprob: bool = False,
):
    """Returns the average of probability scores using a pool of noisy prediction computed with MC Dropout. Can be
    used as an attribution target to compute more robust attribution scores.
    Note:
        In order to obtain meaningful results, the :obj:`attribution_model` must contain dropout layers or other
        sources of noise in the forward pass.
    Args:
        n_mcd_steps (:obj:`int`): The number of prediction steps that should be used to normalize the original output.
    """
    # Original probability from the model without noise
    orig_prob = probability_fn(args, logprob=logprob)
    # Compute noisy predictions using the noisy model
    # Important: must be in train mode to ensure noise for MCD
    args.attribution_model.train()
    noisy_probs = []
    for _ in range(n_mcd_steps):
        aux_batch = args.attribution_model.formatter.convert_args_to_batch(args)
        aux_output = args.attribution_model.get_forward_output(
            aux_batch, use_embeddings=args.attribution_model.is_encoder_decoder
        )
        args.forward_output = aux_output
        noisy_prob = probability_fn(args, logprob=logprob).to(orig_prob.device)
        noisy_probs.append(noisy_prob)
    # Z-score the original based on the mean and standard deviation of MC dropout predictions
    return (orig_prob - torch.stack(noisy_probs).mean(0)).div(torch.stack(noisy_probs).std(0))
def top_p_size_fn(
    args: StepFunctionArgs,
    top_p: float,
):
    """Returns the number of tokens that have cumulative probability above :obj:`top_p` in the model's output logits.
    Args:
        top_p (:obj:`float`): The cumulative probability threshold to use for filtering the logits.
    """
    logits: torch.Tensor = args.attribution_model.output2logits(args.forward_output)
    indices_to_remove = top_p_logits_mask(logits, top_p, 1).to(logits.device)
    logits = logits.masked_select(~indices_to_remove)[None, ...]
    return torch.tensor(logits.size(-1))[None, ...]
STEP_SCORES_MAP = {
    "logit": logit_fn,
    "probability": probability_fn,
    "entropy": entropy_fn,
    "crossentropy": crossentropy_fn,
    "perplexity": perplexity_fn,
    "contrast_logits": contrast_logits_fn,
    "contrast_prob": contrast_prob_fn,
    "contrast_logits_diff": contrast_logits_diff_fn,
    "contrast_prob_diff": contrast_prob_diff_fn,
    "pcxmi": pcxmi_fn,
    "kl_divergence": kl_divergence_fn,
    "in_context_pvi": in_context_pvi_fn,
    "mc_dropout_prob_avg": mc_dropout_prob_avg_fn,
    "top_p_size": top_p_size_fn,
}
def check_is_step_function(identifier: str) -> None:
    if identifier not in STEP_SCORES_MAP:
        raise AttributeError(
            f"Step score {identifier} not found. Available step scores are: "
            f"{', '.join(list(STEP_SCORES_MAP.keys()))}. Use the inseq.register_step_function"
            "function to register a custom step score."
        )
def get_step_function(score_identifier: str) -> StepFunction:
    """Returns the step function corresponding to the provided identifier."""
    check_is_step_function(score_identifier)
    return STEP_SCORES_MAP[score_identifier]
def get_step_scores(
    score_identifier: str,
    step_fn_args: StepFunctionArgs,
    step_fn_extra_args: dict[str, Any] = {},
) -> SingleScorePerStepTensor:
    """Returns step scores for the target tokens in the batch."""
    return get_step_function(score_identifier)(step_fn_args, **step_fn_extra_args)
def get_step_scores_args(
    score_identifiers: list[str], kwargs: dict[str, Any], default_args: dict[str, Any] | None = None
) -> dict[str, Any]:
    step_scores_args = {}
    for step_fn_id in score_identifiers:
        step_fn = get_step_function(step_fn_id)
        step_scores_args.update(
            **extract_signature_args(
                kwargs,
                step_fn,
                exclude_args=default_args,
                return_remaining=False,
            )
        )
    return step_scores_args
def list_step_functions() -> list[str]:
    """Lists identifiers for all available step scores. One or more step scores identifiers can be passed to the
    :meth:`~inseq.models.AttributionModel.attribute` method either to compute scores while attributing (``step_scores``
    parameter), or as target function for the attribution, if supported by the attribution method (``attributed_fn``
    parameter).
    """
    return list(STEP_SCORES_MAP.keys())
def register_step_function(
    fn: StepFunction,
    identifier: str,
    aggregate_map: dict[str, str] | None = None,
    overwrite: bool = False,
) -> None:
    """Registers a function to be used to compute step scores and store them in the
    :class:`~inseq.data.attribution.FeatureAttributionOutput` object. Registered step functions can also be used as
    attribution targets by gradient-based feature attribution methods.
    Args:
        fn (:obj:`callable`): The function to be used to compute step scores. Default parameters (use kwargs to capture unused ones when defining your function):
            - :obj:`attribution_model`: an :class:`~inseq.models.AttributionModel` instance, corresponding to the model
                used for computing the score.
            - :obj:`forward_output`: the output of the forward pass from the attribution model.
            - :obj:`encoder_input_ids`, :obj:`decoder_input_ids`, :obj:`encoder_input_embeds`,
                :obj:`decoder_input_embeds`, :obj:`encoder_attention_mask`, :obj:`decoder_attention_mask`: all the
                elements composing the :class:`~inseq.data.Batch` used as context of the model.
            - :obj:`target_ids`: :obj:`torch.Tensor` of target token ids of size `(batch_size,)` and type long,
                corresponding to the target predicted tokens for the next generation step.
            The function can also define an arbitrary number of custom parameters that can later be provided directly
            to the `model.attribute` function call, and it must return a :obj:`torch.Tensor` of size `(batch_size,)` of
            float or long. If parameter names conflict with `model.attribute` ones, pass them as key-value pairs in the
            :obj:`step_scores_args` dict parameter.
        identifier (:obj:`str`): The identifier that will be used for the registered step score.
        aggregate_map (:obj:`dict`, `optional`): An optional dictionary mapping from :class:`~inseq.data.Aggregator`
            name identifiers to aggregation function identifiers. A list of available aggregation functions is
            available using :func:`~inseq.list_aggregation_functions`.
        overwrite (:obj:`bool`, `optional`, defaults to :obj:`False`): Whether to overwrite an existing function
            registered with the same identifier.
    """
    if identifier in STEP_SCORES_MAP:
        if not overwrite:
            raise ValueError(
                f"{identifier} is already registered in step functions map. Override with overwrite=True."
            )
        logger.warning(f"Overwriting {identifier} step function.")
    STEP_SCORES_MAP[identifier] = fn
    if isinstance(aggregate_map, dict):
        for agg_name, aggregation_fn_identifier in aggregate_map.items():
            if agg_name not in DEFAULT_ATTRIBUTION_AGGREGATE_DICT["step_scores"]:
                DEFAULT_ATTRIBUTION_AGGREGATE_DICT["step_scores"][agg_name] = {}
            DEFAULT_ATTRIBUTION_AGGREGATE_DICT["step_scores"][agg_name][identifier] = aggregation_fn_identifier
def is_contrastive_step_function(step_fn_id: str) -> bool:
    return "contrast_targets" in signature(get_step_function(step_fn_id)).parameters

================
File: inseq/commands/attribute_context/__init__.py
================
from .attribute_context import AttributeContextCommand
from .attribute_context_args import AttributeContextArgs
from .attribute_context_helpers import AttributeContextOutput, CCIOutput
from .attribute_context_viz_helpers import visualize_attribute_context
__all__ = [
    "AttributeContextCommand",
    "AttributeContextArgs",
    "AttributeContextOutput",
    "CCIOutput",
    "visualize_attribute_context",
]

================
File: inseq/commands/attribute_context/attribute_context_args.py
================
import logging
from dataclasses import dataclass
from enum import Enum
from typing import Any
from ... import list_step_functions
from ...attr.step_functions import is_contrastive_step_function
from ...utils import cli_arg, pretty_dict
from ..attribute import AttributeBaseArgs
from ..commands_utils import command_args_docstring
logger = logging.getLogger(__name__)
class HandleOutputContextSetting(Enum):
    MANUAL = "manual"
    AUTO = "auto"
    PRE = "pre"
@command_args_docstring
@dataclass
class AttributeContextInputArgs:
    input_current_text: str = cli_arg(
        default="",
        help=(
            "The input text used for generation. If the model is a decoder-only model, the input text is a "
            "prompt used for language modeling. If the model is an encoder-decoder model, the input text is the "
            "source text provided as input to the encoder. It will be formatted as {current} in the "
            "``input_template``."
        ),
    )
    input_context_text: str | None = cli_arg(
        default=None,
        help=(
            "Additional input context influencing the generation of ``output_current_text``. If the model is a"
            " decoder-only model, the input text is a prefix to the ``input_current_text`` prompt. If the model is an"
            " encoder-decoder model, the input context is part of the source text provided as input to the encoder. "
            " It will be formatted as {context} in the ``input_template``."
        ),
    )
    input_template: str | None = cli_arg(
        default=None,
        help=(
            "The template used to format model inputs. The template must contain at least the"
            " ``{current}`` placeholder, which will be replaced by ``input_current_text``. If ``{context}`` is"
            " also specified, input-side context will be used. Can be modified for models requiring special tokens or"
            " formatting in the input text (e.g. <brk> tags to separate context and current inputs)."
            " Defaults to '{context} {current}' if ``input_context_text`` is provided, '{current}' otherwise."
        ),
    )
    output_context_text: str | None = cli_arg(
        default=None,
        help=(
            "An output contexts for which context sensitivity should be detected. For encoder-decoder models, this"
            " is a target-side prefix to the output_current_text used as input to the decoder. For decoder-only "
            " models this is a portion of the model generation that should be considered as an additional context "
            " (e.g. a chain-of-thought sequence). It will be formatted as {context} in the ``output_template``."
            " If not provided but specified in the ``output_template``, the output context will be generated"
            " along with the output current text, and user validation might be required to separate the two."
        ),
    )
    output_current_text: str | None = cli_arg(
        default=None,
        help=(
            "The output text generated by the model when all available contexts are provided. Tokens in "
            " ``output_current_text`` will be tested for context-sensitivity, and their generation will be attributed "
            " to input/target contexts (if present) in case they are found to be context-sensitive. If specified, "
            " this output is force-decoded. Otherwise, it is generated by the model using infilled ``input_template`` "
            " and ``output_template``. It will be formatted as {current} in the ``output_template``."
        ),
    )
    output_template: str | None = cli_arg(
        default=None,
        help=(
            "The template used to format model outputs. The template must contain at least the"
            " ``{current}`` placeholder, which will be replaced by ``output_current_text``. If ``{context}`` is"
            " also specified, output-side context will be used. Can be modified for models requiring special tokens or"
            " formatting in the output text (e.g. <brk> tags to separate context and current outputs)."
            " Defaults to '{context} {current}' if ``output_context_text`` is provided, '{current}' otherwise."
        ),
    )
    contextless_input_current_text: str | None = cli_arg(
        default=None,
        help=(
            "The input current text or template to use in the contrastive comparison with contextual input. By default"
            " it is the same as ``input_current_text``, but it can be useful in cases where the context is nested "
            "inside the current text (e.g. for an ``input_template`` like <user>\n{context}\n{current}\n<assistant> we "
            "can use this parameter to format the contextless version as <user>\n{current}\n<assistant>)."
            "If it contains the tag {current}, it will be infilled with the ``input_current_text``. Otherwise, it will"
            " be used as-is for the contrastive comparison, enabling contrastive comparison with different inputs."
        ),
    )
    contextless_output_current_text: str | None = cli_arg(
        default=None,
        help=(
            "The output current text or template to use in the contrastive comparison with contextual output. By default"
            " it is the same as ``output_current_text``, but it can be useful in cases where the context is nested "
            "inside the current text (e.g. for an ``output_template`` like <user>\n{context}\n{current}\n<assistant> we "
            "can use this parameter to format the contextless version as <user>\n{current}\n<assistant>)."
            "If it contains the tag {current}, it will be infilled with the ``output_current_text``. Otherwise, it will"
            " be used as-is for the contrastive comparison, enabling contrastive comparison with different outputs."
        ),
    )
@command_args_docstring
@dataclass
class AttributeContextMethodArgs(AttributeBaseArgs):
    context_sensitivity_metric: str = cli_arg(
        default="kl_divergence",
        help="The contrastive metric used to detect context-sensitive tokens in ``output_current_text``.",
        choices=[fn for fn in list_step_functions() if is_contrastive_step_function(fn)],
    )
    handle_output_context_strategy: str = cli_arg(
        default=HandleOutputContextSetting.MANUAL.value,
        choices=[e.value for e in HandleOutputContextSetting],
        help=(
            "Specifies how output context should be handled when it is produced together with the output current text,"
            " and the two need to be separated for context sensitivity detection.\n"
            "Options:\n"
            "- `manual`: The user is prompted to verify an automatic context detection attempt, and optionally to"
            "  provide a correct context separation manually.\n"
            "- `auto`: Attempts an automatic detection of context using an automatic alignment with source context"
            " (assuming an MT-like task).\n"
            "- `pre`: If context is required but not pre-defined by the user via the ``output_context_text`` argument,"
            "  the execution will fail instead of attempting to prompt the user for the output context."
        ),
    )
    contextless_output_next_tokens: list[str] = cli_arg(
        default_factory=list,
        help=(
            "If specified, it should provide a list of one token per CCI output indicating the next token that should"
            " be force-decoded as contextless output instead of the natural output produced by"
            " ``get_contextless_output``. This is ignored if the ``attributed_fn`` used is not contrastive."
        ),
    )
    prompt_user_for_contextless_output_next_tokens: bool = cli_arg(
        default=False,
        help=(
            "If specified, the user is prompted to provide the next token that should be force-decoded as contextless"
            " output instead of the natural output produced by ``get_contextless_output``. This is ignored if the"
            " ``attributed_fn`` used is not contrastive."
        ),
    )
    special_tokens_to_keep: list[str] = cli_arg(
        default_factory=list,
        help="Special tokens to preserve in the generated string, e.g. ``<brk>`` separator between context and current.",
    )
    decoder_input_output_separator: str = cli_arg(
        default=" ",
        help=(
            "If specified, the separator used to split the input and output of the decoder. If not specified, the"
            " separator is a whitespace character."
        ),
    )
    context_sensitivity_std_threshold: float = cli_arg(
        default=1.0,
        help=(
            "Parameter to control the selection of ``output_current_text`` tokens considered as context-sensitive for "
            "moving onwards with attribution. Corresponds to the number of standard deviations above or below the mean"
            " ``context_sensitivity_metric`` score for tokens to be considered context-sensitive."
        ),
    )
    context_sensitivity_topk: int | None = cli_arg(
        default=None,
        help=(
            "If set, after selecting the salient context-sensitive tokens with ``context_sensitivity_std_threshold`` "
            "only the top-K remaining tokens are used. By default no top-k selection is performed."
        ),
    )
    attribution_std_threshold: float = cli_arg(
        default=1.0,
        help=(
            "Parameter to control the selection of ``input_context_text`` and ``output_context_text`` tokens "
            "considered as salient as a result for the attribution process. Corresponds to the number of standard "
            "deviations above or below the mean ``attribution_method`` score for tokens to be considered salient. "
            "CCI scores for all context tokens are saved in the output, but this parameter controls which tokens are "
            "used in the visualization of context reliance."
        ),
    )
    attribution_topk: int | None = cli_arg(
        default=None,
        help=(
            "If set, after selecting the most salient tokens with ``attribution_std_threshold`` "
            "only the top-K remaining tokens are used. By default no top-k selection is performed."
        ),
    )
@command_args_docstring
@dataclass
class AttributeContextOutputArgs:
    show_intermediate_outputs: bool = cli_arg(
        default=False,
        help=(
            "If specified, the intermediate outputs produced by the Inseq library for context-sensitive target "
            "identification (CTI) and contextual cues imputation (CCI) are shown during the process."
        ),
    )
    save_path: str | None = cli_arg(
        default=None,
        aliases=["-o"],
        help="If present, the output of the two-step process will be saved in JSON format at the specified path.",
    )
    add_output_info: bool = cli_arg(
        default=True,
        help="If specified, additional information about the attribution process is added to the saved output.",
    )
    viz_path: str | None = cli_arg(
        default=None,
        help="If specified, the visualization produced from the output is saved in HTML format at the specified path.",
    )
    show_viz: bool = cli_arg(
        default=True,
        help="If specified, the visualization produced from the output is shown in the terminal.",
    )
@command_args_docstring
@dataclass
class AttributeContextArgs(AttributeContextInputArgs, AttributeContextMethodArgs, AttributeContextOutputArgs):
    def __repr__(self):
        return f"{self.__class__.__name__}({pretty_dict(self.__dict__)})"
    @classmethod
    def _to_dict(cls, val: Any) -> dict[str, Any]:
        if val is None or isinstance(val, str | int | float | bool):
            return val
        elif isinstance(val, dict):
            return {k: cls._to_dict(v) for k, v in val.items()}
        elif isinstance(val, list | tuple):
            return [cls._to_dict(v) for v in val]
        else:
            return str(val)
    def to_dict(self) -> dict[str, Any]:
        return self._to_dict(self.__dict__)
    def __post_init__(self):
        if (
            self.handle_output_context_strategy == HandleOutputContextSetting.PRE.value
            and not self.output_context_text
            and "{context}" in self.output_template
        ):
            raise ValueError(
                "If --handle_output_context_strategy='pre' and {context} is used in --output_template, --output_context_text"
                " must be specified to avoid user prompt for output context."
            )
        if len(self.contextless_output_next_tokens) > 0 and self.prompt_user_for_contextless_output_next_tokens:
            raise ValueError(
                "Only one of contextless_output_next_tokens and prompt_user_for_contextless_output_next_tokens can be"
                " specified."
            )
        if self.input_template is None:
            self.input_template = "{current}" if self.input_context_text is None else "{context} {current}"
        if self.output_template is None:
            self.output_template = "{current}" if self.output_context_text is None else "{context} {current}"
        if self.contextless_input_current_text is None:
            self.contextless_input_current_text = "{current}"
        if "{current}" not in self.contextless_input_current_text:
            raise ValueError(
                "{current} placeholder is missing from contextless_input_current_text template"
                f" {self.contextless_input_current_text}."
            )
        if self.contextless_output_current_text is None:
            self.contextless_output_current_text = "{current}"
        if "{current}" not in self.contextless_output_current_text:
            raise ValueError(
                "{current} placeholder is missing from contextless_output_current_text template"
                f" {self.contextless_output_current_text}."
            )
        self.has_input_context = "{context}" in self.input_template
        self.has_output_context = "{context}" in self.output_template
        if not self.input_current_text:
            raise ValueError("--input_current_text must be a non-empty string.")
        if self.input_context_text and not self.has_input_context:
            logger.warning(
                f"input_template has format {self.input_template} (no {{context}}), but --input_context_text is"
                " specified. Ignoring provided --input_context_text."
            )
            self.input_context_text = None
        if self.output_context_text and not self.has_output_context:
            logger.warning(
                f"output_template has format {self.output_template} (no {{context}}), but --output_context_text is"
                " specified. Ignoring provided --output_context_text."
            )
            self.output_context_text = None
        if not self.input_context_text and self.has_input_context:
            raise ValueError(
                f"{{context}} format placeholder is present in input_template {self.input_template},"
                " but --input_context_text is not specified."
            )
        if "{current}" not in self.input_template:
            raise ValueError(f"{{current}} format placeholder is missing from input_template {self.input_template}.")
        if "{current}" not in self.output_template:
            raise ValueError(f"{{current}} format placeholder is missing from output_template {self.output_template}.")
        if not self.input_current_text:
            raise ValueError("--input_current_text must be a non-empty string.")
        if self.has_output_context and self.output_template.find("{context}") > self.output_template.find("{current}"):
            raise ValueError(
                f"{{context}} placeholder must appear before {{current}} in output_template '{self.output_template}'."
            )
        if not self.output_template.endswith("{current}"):
            *_, suffix = self.output_template.partition("{current}")
            logger.warning(
                f"Suffix '{suffix}' was specified in output_template and will be used to ignore the specified suffix"
                " tokens during context sensitivity detection. Make sure that the suffix corresponds to the end of the"
                " output_current_text by forcing --output_current_text if necessary."
            )

================
File: inseq/commands/attribute_context/attribute_context_helpers.py
================
import logging
import re
from dataclasses import dataclass, field, fields
from typing import Any
from rich import print as rprint
from rich.prompt import Confirm, Prompt
from torch import tensor
from ...data import FeatureAttributionSequenceOutput
from ...models import HuggingfaceModel
from ...utils import pretty_dict
from ...utils.alignment_utils import compute_word_aligns
from .attribute_context_args import AttributeContextArgs, HandleOutputContextSetting
logger = logging.getLogger(__name__)
@dataclass
class CCIOutput:
    """Output of the Contextual Cues Imputation (CCI) step."""
    cti_idx: int
    cti_token: str
    cti_score: float
    contextual_output: str
    contrast_token: str | None = None
    contextless_output: str | None = None
    input_context_scores: list[float] | None = None
    output_context_scores: list[float] | None = None
    def __repr__(self):
        return f"{self.__class__.__name__}({pretty_dict(self.__dict__)})"
    def to_dict(self) -> dict[str, Any]:
        return dict(self.__dict__.items())
    @property
    def minimum(self) -> float:
        scores = [0]
        if self.input_context_scores:
            scores.extend(self.input_context_scores)
        if self.output_context_scores:
            scores.extend(self.output_context_scores)
        return min(scores)
    @property
    def maximum(self) -> float:
        scores = [0]
        if self.input_context_scores:
            scores.extend(self.input_context_scores)
        if self.output_context_scores:
            scores.extend(self.output_context_scores)
        return max(scores)
    @property
    def all_scores(self) -> list[float]:
        scores = []
        if self.input_context_scores:
            scores.extend(self.input_context_scores)
        if self.output_context_scores:
            scores.extend(self.output_context_scores)
        return scores
@dataclass
class AttributeContextOutput:
    """Output of the overall context attribution process."""
    input_context: str | None = None
    input_context_tokens: list[str] | None = None
    output_context: str | None = None
    output_context_tokens: list[str] | None = None
    output_current: str | None = None
    output_current_tokens: list[str] | None = None
    cti_scores: list[float] | None = None
    cci_scores: list[CCIOutput] = field(default_factory=list)
    info: AttributeContextArgs | None = None
    def __repr__(self):
        return f"{self.__class__.__name__}({pretty_dict(self.__dict__)})"
    def __treescope_repr__(self, *args, **kwargs):
        from inseq.commands.attribute_context.attribute_context_viz_helpers import (
            visualize_attribute_context_treescope,
        )
        return visualize_attribute_context_treescope(self)
    def to_dict(self) -> dict[str, Any]:
        out_dict = {k: v for k, v in self.__dict__.items() if k not in ["cci_scores", "info"]}
        out_dict["cci_scores"] = [cci_out.to_dict() for cci_out in self.cci_scores]
        if self.info:
            out_dict["info"] = self.info.to_dict()
        return out_dict
    @classmethod
    def from_dict(cls, out_dict: dict[str, Any]) -> "AttributeContextOutput":
        out = cls()
        for k, v in out_dict.items():
            if k not in ["cci_scores", "info", "has_input_context", "has_output_context"]:
                setattr(out, k, v)
        out.cci_scores = [CCIOutput(**cci_out) for cci_out in out_dict["cci_scores"]]
        if "info" in out_dict:
            field_names = [f.name for f in fields(AttributeContextArgs)]
            out.info = AttributeContextArgs(**{k: v for k, v in out_dict["info"].items() if k in field_names})
        return out
    @property
    def min_cti(self) -> float:
        if self.cti_scores is None:
            return -1
        return min(self.cti_scores)
    @property
    def max_cti(self) -> float:
        if self.cti_scores is None:
            return -1
        return max(self.cti_scores)
    @property
    def mean_cti(self) -> float:
        if self.cti_scores is None:
            return 0
        return sum(self.cti_scores) / len(self.cti_scores)
    @property
    def std_cti(self) -> float:
        if self.cti_scores is None:
            return 0
        return tensor(self.cti_scores).std().item()
    @property
    def min_cci(self) -> float:
        if self.cci_scores is None:
            return -1
        return min(cci.minimum for cci in self.cci_scores)
    @property
    def max_cci(self) -> float:
        if self.cci_scores is None:
            return -1
        return max(cci.maximum for cci in self.cci_scores)
    @property
    def cci_all_scores(self) -> list[float]:
        if self.cci_scores is None:
            return []
        return [score for cci in self.cci_scores for score in cci.all_scores]
    @property
    def mean_cci(self) -> float:
        if self.cci_scores is None:
            return 0
        return sum(self.cci_all_scores) / len(self.cci_all_scores)
    @property
    def std_cci(self) -> float:
        if self.cci_scores is None:
            return 0
        return tensor(self.cci_all_scores).std().item()
    @property
    def input_context_scores(self) -> list[float] | None:
        if self.cci_scores is None or self.cci_scores[0].input_context_scores is None:
            return None
        return [cci.input_context_scores for cci in self.cci_scores]
    @property
    def output_context_scores(self) -> list[float] | None:
        if self.cci_scores is None or self.cci_scores[0].output_context_scores is None:
            return None
        return [cci.output_context_scores for cci in self.cci_scores]
def concat_with_sep(s1: str, s2: str, sep: str) -> bool:
    """Adds separator between two strings if needed."""
    need_sep = not s1.endswith(sep) and not s2.startswith(sep)
    if need_sep:
        return s1 + sep + s2
    return s1 + s2
def format_template(template: str, current: str, context: str | None = None) -> str:
    kwargs = {"current": current}
    if context is not None:
        kwargs["context"] = context
    return template.format(**kwargs)
def get_filtered_tokens(
    text: str,
    model: HuggingfaceModel,
    special_tokens_to_keep: list[str],
    replace_special_characters: bool = False,
    is_target: bool = False,
) -> list[str]:
    """Tokenize text and filter out special tokens, keeping only those in ``special_tokens_to_keep``."""
    as_targets = is_target and model.is_encoder_decoder
    return [
        t.replace("Ä ", " ").replace("ÄŠ", "\n").replace("â–", " ") if replace_special_characters else t
        for t in model.convert_string_to_tokens(text, skip_special_tokens=False, as_targets=as_targets)
        if t not in model.special_tokens or t in special_tokens_to_keep
    ]
def generate_with_special_tokens(
    model: HuggingfaceModel,
    model_input: str,
    special_tokens_to_keep: list[str] = [],
    output_generated_only: bool = True,
    **generation_kwargs,
) -> str:
    """Generate text preserving special tokens in ``special_tokens_to_keep``."""
    # Generate outputs, strip special tokens and remove prefix/suffix
    output_gen = model.generate(
        model_input, skip_special_tokens=False, output_generated_only=output_generated_only, **generation_kwargs
    )[0]
    output_tokens = get_filtered_tokens(output_gen, model, special_tokens_to_keep, is_target=True)
    return model.convert_tokens_to_string(output_tokens, skip_special_tokens=False)
def generate_model_output(
    model: HuggingfaceModel,
    model_input: str,
    generation_kwargs: dict[str, Any],
    special_tokens_to_keep: list[str],
    output_template: str,
    prefix: str,
    suffix: str,
) -> str:
    """Generate the model output, validating the presence of a prefix/suffix and stripping them from the generation."""
    output_gen = generate_with_special_tokens(model, model_input, special_tokens_to_keep, **generation_kwargs)
    if prefix:
        if not output_gen.startswith(prefix):
            raise ValueError(
                f"Output template '{output_template}' contains prefix '{prefix}' but output '{output_gen}' does"
                " not match the prefix. Please check whether the template is correct, or force context/current"
                " outputs."
            )
        output_gen = output_gen[len(prefix) :]
    if suffix:
        if not output_gen.endswith(suffix):
            raise ValueError(
                f"Output template {output_template} contains suffix {suffix} but output '{output_gen}' does"
                " not match the suffix. Please check whether the template is correct, or force context/current"
                " outputs."
            )
        output_gen = output_gen[: -len(suffix)]
    return output_gen
def prompt_user_for_context(output: str, context_candidate: str | None = None) -> str:
    """Prompt the user to provide the correct context for the provided output."""
    while True:
        if context_candidate:
            is_correct_candidate = Confirm.ask(
                f'\n:arrow_right: The model generated the following output: "[bold]{output}[/bold]"'
                f'\n:question: Is [bold]"{context_candidate}"[/bold] the correct context you want to attribute?'
            )
        if is_correct_candidate:
            user_context = context_candidate
        else:
            user_context = Prompt.ask(
                ":writing_hand: Please enter the portion of the generated output representing the correct context"
            )
        if output.startswith(user_context):
            if not user_context.strip():
                use_empty_context = Confirm.ask(
                    ":question: The provided context is empty. Do you want to use an empty context?"
                )
                if use_empty_context:
                    user_context = ""
                else:
                    continue
            break
        rprint(
            "[prompt.invalid]The provided context is invalid. Please provide a non-empty substring of"
            " the model output above to use as context."
        )
    return user_context
def get_output_context_from_aligned_inputs(input_context: str, output_text: str) -> str:
    """Retrieve the output context from alignments between input context and the full output text."""
    aligned_context = compute_word_aligns(input_context, output_text, split_pattern=r"\s+|\b")
    max_context_id = max(pair[1] for pair in aligned_context.alignments)
    output_text_boundary_token = aligned_context.target_tokens[max_context_id]
    # Empty spans correspond to token boundaries
    spans = [m.span() for m in re.finditer(r"\s+|\b", output_text)]
    tok_start_positions = list({start if start == end else end for start, end in spans})
    output_text_context_candidate_boundary = tok_start_positions[max_context_id] + len(output_text_boundary_token)
    return output_text[:output_text_context_candidate_boundary]
def prepare_outputs(
    model: HuggingfaceModel,
    input_context_text: str | None,
    input_full_text: str,
    output_context_text: str | None,
    output_current_text: str | None,
    output_template: str,
    handle_output_context_strategy: str,
    generation_kwargs: dict[str, Any] = {},
    special_tokens_to_keep: list[str] = [],
    decoder_input_output_separator: str = " ",
) -> tuple[str | None, str]:
    """Handle model outputs and prepare them for attribution.
    This procedure is valid both for encoder-decoder and decoder-only models.
    | use_out_ctx | has_out_ctx | has_out_curr | setting
    |-------------|-------------|--------------|--------
    | True        | True        | True         | 1. Use forced context + current as output
    | False       | False       | True         | 2. Use forced current as output
    | True        | True        | False        | 3. Set inputs with forced context, generate output, use as current
    | False       | False       | False        | 4. Generate output, use it as current
    | True        | False       | False        | 5. Generate output, handle context/current splitting
    | True        | False       | True         | 6. Generate output, handle context/current splitting, force current
    NOTE: If ``use_out_ctx`` is True but ``has_out_ctx`` is False, the model generation is assumed to contain both
    a context and a current portion which need to be separated. ``has_out_ctx`` cannot be True if ``use_out_ctx``
    is False (pre-check in ``__post_init__``).
    """
    use_out_ctx = "{context}" in output_template
    has_out_ctx = output_context_text is not None
    has_out_curr = output_current_text is not None
    model_input = input_full_text
    final_current = output_current_text
    final_context = output_context_text
    # E.g. output template "A{context}B{current}C" -> prefix = "A", suffix = "C", separator = "B"
    prefix, _ = output_template.split("{context}" if use_out_ctx else "{current}")
    output_current_prefix_template, suffix = output_template.split("{current}")
    separator = output_template.split("{context}")[1].split("{current}")[0] if use_out_ctx else None
    # Settings 1, 2
    if (has_out_ctx == use_out_ctx) and has_out_curr:
        return final_context, final_current
    # Prepend output prefix and context, if available, if current output needs to be generated
    output_current_prefix = prefix
    if has_out_ctx and not has_out_curr:
        output_current_prefix = output_current_prefix_template.strip().format(context=output_context_text)
        if model.is_encoder_decoder:
            generation_kwargs["decoder_input_ids"] = model.encode(
                output_current_prefix, as_targets=True, add_special_tokens=False
            ).input_ids
            if "forced_bos_token_id" in generation_kwargs:
                generation_kwargs["decoder_input_ids"][0, 0] = generation_kwargs["forced_bos_token_id"]
        else:
            model_input = concat_with_sep(input_full_text, output_current_prefix, decoder_input_output_separator)
            output_current_prefix = model_input
    if not model.is_encoder_decoder:
        model_input = concat_with_sep(input_full_text, "", decoder_input_output_separator)
    output_gen = generate_model_output(
        model, model_input, generation_kwargs, special_tokens_to_keep, output_template, output_current_prefix, suffix
    )
    # Settings 3, 4
    if (has_out_ctx == use_out_ctx) and not has_out_curr:
        return final_context, output_gen.strip()
    # Settings 5, 6
    # Try splitting the output into context and current text using ``separator``. As we have no guarantees of its
    # uniqueness (e.g. it could be whitespace, also found between tokens in context and current) we consider the
    # splitting successful if exactly 2 substrings are produced. If this fails, we try splitting on punctuation.
    output_context_candidate = None
    separator_split_context_current_substring = output_gen.split(separator)
    if len(separator_split_context_current_substring) == 2:
        output_context_candidate = separator_split_context_current_substring[0]
    if not output_context_candidate:
        punct_expr = re.compile(r"[\s{}]+".format(re.escape(".?!,;:)]}")))
        punctuation_split_context_current_substring = [s for s in punct_expr.split(output_gen) if s]
        if len(punctuation_split_context_current_substring) == 2:
            output_context_candidate = punctuation_split_context_current_substring[0]
    # Final resort: if the model is an encoder-decoder model, we align the full input and full output, identifying
    # which tokens correspond to context and which to current. This assumes that input and output texts are alignable
    # (e.g. translations of each other). We prompt the user a yes/no question asking whether the context identified is
    # correct. If not, the user is asked to provide the correct context. If handle_output_context_strategy = "auto", aligned
    # texts are assumed to be correct (no user input required, to automate the procedure)
    if not output_context_candidate and model.is_encoder_decoder and input_context_text is not None:
        output_context_candidate = get_output_context_from_aligned_inputs(input_context_text, output_gen)
    if output_context_candidate and handle_output_context_strategy == HandleOutputContextSetting.AUTO.value:
        final_context = output_context_candidate
    else:
        final_context = prompt_user_for_context(output_gen, output_context_candidate)
    template_output_context = output_template.split("{current}")[0].format(context=final_context)
    if not final_context:
        template_output_context = template_output_context.strip()
    final_current = output_gen[min(len(template_output_context), len(output_gen)) :]
    if not has_out_curr and not final_current:
        raise ValueError(
            f"The model produced an empty current output given the specified context '{final_context}'. If no"
            " context is generated naturally by the model, you can force an output context using the"
            " --output_context_text option."
        )
    if has_out_curr:
        logger.warning(
            f"The model produced current text '{final_current}', but the specified output_current_text"
            f" '{output_current_text}'is used instead. If you want to use the original current output text generated"
            " by the model, remove the --output_current_text option."
        )
    return final_context, final_current.strip()
def get_scores_threshold(scores: list[float], std_weight: float) -> float:
    """Compute the threshold for a given weight."""
    if std_weight is None or len(scores) == 0:
        return 0
    if std_weight == 0 or len(scores) == 1:
        return tensor(scores).mean()
    return tensor(scores).mean() + std_weight * tensor(scores).std()
def filter_rank_tokens(
    tokens: list[str],
    scores: list[float],
    std_threshold: float | None = None,
    topk: int | None = None,
) -> tuple[list[tuple[int, float, str]], float]:
    indices = list(range(0, len(scores)))
    token_score_tuples = sorted(zip(indices, scores, tokens, strict=False), key=lambda x: abs(x[1]), reverse=True)
    threshold = get_scores_threshold(scores, std_threshold)
    token_score_tuples = [(i, s, t) for i, s, t in token_score_tuples if abs(s) >= threshold]
    if topk:
        token_score_tuples = token_score_tuples[:topk]
    return token_score_tuples, threshold
def get_contextless_output(
    model: HuggingfaceModel,
    input_current_text: str,
    output_current_tokens: list[str],
    cti_idx: int,
    cti_ranked_tokens: tuple[int, float, str],
    contextless_output_next_tokens: list[str] | None,
    prompt_user_for_contextless_output_next_tokens: bool,
    cci_step_idx: int,
    decoder_input_output_separator: str = " ",
    special_tokens_to_keep: list[str] = [],
    generation_kwargs: dict[str, Any] = {},
) -> tuple[str, str]:
    n_ctxless_next_tokens = len(contextless_output_next_tokens)
    next_ctxless_token = None
    if n_ctxless_next_tokens > 0:
        if n_ctxless_next_tokens != len(cti_ranked_tokens):
            raise ValueError(
                "The number of manually specified contextless output next tokens must be equal to the number "
                "of context-sensitive tokens identified by CTI."
            )
        next_ctxless_token = contextless_output_next_tokens[cci_step_idx]
    if prompt_user_for_contextless_output_next_tokens:
        next_ctxless_token = prompt_user_for_contextless_output_next_tokens(output_current_tokens, cti_idx, model)
    if isinstance(next_ctxless_token, str):
        next_ctxless_token = model.convert_string_to_tokens(
            next_ctxless_token, skip_special_tokens=False, as_targets=model.is_encoder_decoder
        )[0]
        contextless_output_tokens = output_current_tokens[:cti_idx] + [next_ctxless_token]
        contextless_output = model.convert_tokens_to_string(contextless_output_tokens, skip_special_tokens=False)
    else:
        contextless_output = generate_contextless_output(
            model,
            input_current_text,
            output_current_tokens,
            cti_idx,
            special_tokens_to_keep,
            generation_kwargs,
            decoder_input_output_separator,
        )
    return contextless_output
def generate_contextless_output(
    model: HuggingfaceModel,
    input_current_text: str,
    output_current_tokens: list[str],
    cti_idx: int,
    special_tokens_to_keep: list[str] = [],
    generation_kwargs: dict[str, Any] = {},
    decoder_input_output_separator: str = " ",
) -> tuple[str, str]:
    """Generate the contextless output for the current token identified as context-sensitive."""
    contextual_prefix_tokens = output_current_tokens[:cti_idx]
    contextual_prefix = model.convert_tokens_to_string(contextual_prefix_tokens, skip_special_tokens=False)
    if model.is_encoder_decoder:
        # One extra token for the EOS which is always forced at the end for encoder-decoders
        generation_kwargs["max_new_tokens"] = 2
        decoder_input_ids = model.encode(contextual_prefix, as_targets=True).input_ids
        if int(decoder_input_ids[0, -1]) == model.eos_token_id:
            decoder_input_ids = decoder_input_ids[0, :-1][None, ...]
        generation_kwargs["decoder_input_ids"] = decoder_input_ids
        generation_input = input_current_text
    else:
        generation_kwargs["max_new_tokens"] = 1
        generation_input = concat_with_sep(input_current_text, contextual_prefix, decoder_input_output_separator)
    contextless_output = generate_with_special_tokens(
        model,
        generation_input,
        special_tokens_to_keep,
        output_generated_only=False,
        **generation_kwargs,
    )
    return contextless_output
def get_source_target_cci_scores(
    model: HuggingfaceModel,
    cci_attrib_out: FeatureAttributionSequenceOutput,
    input_template: str,
    input_current_text: str,
    input_context_tokens: list[str],
    input_full_tokens: list[str],
    output_template: str,
    output_context_tokens: list[str],
    has_input_context: bool,
    has_output_context: bool,
    model_has_lang_tag: bool,
    decoder_input_output_separator: str,
    special_tokens_to_keep: list[str] = [],
) -> tuple[list[float] | None, list[float] | None]:
    """Extract attribution scores for the input and output contexts."""
    input_scores, output_scores = None, None
    if has_input_context:
        if model.is_encoder_decoder:
            input_scores = cci_attrib_out.source_attributions[:, 0].tolist()
            if model_has_lang_tag:
                input_scores = input_scores[2:]
        else:
            input_scores = cci_attrib_out.target_attributions[:, 0].tolist()
        input_prefix, *_ = input_template.partition("{context}")
        if "{current}" in input_prefix:
            input_prefix = input_prefix.format(current=input_current_text)
        input_prefix_tokens = get_filtered_tokens(input_prefix, model, special_tokens_to_keep, is_target=False)
        input_prefix_len = len(input_prefix_tokens)
        input_scores = input_scores[input_prefix_len : len(input_context_tokens) + input_prefix_len]
    if has_output_context:
        output_scores = cci_attrib_out.target_attributions[:, 0].tolist()
        if model_has_lang_tag:
            output_scores = output_scores[2:]
        output_prefix, *_ = output_template.partition("{context}")
        if not model.is_encoder_decoder and output_prefix:
            output_prefix = decoder_input_output_separator + output_prefix
        output_prefix_tokens = get_filtered_tokens(output_prefix, model, special_tokens_to_keep, is_target=True)
        prefix_len = len(output_prefix_tokens)
        if not model.is_encoder_decoder:
            prefix_len += len(input_full_tokens)
        output_scores = output_scores[prefix_len : len(output_context_tokens) + prefix_len]
    return input_scores, output_scores
def prompt_user_for_contextless_output_next_tokens(
    output_current_tokens: list[str],
    cti_idx: int,
    model: HuggingfaceModel,
    special_tokens_to_keep: list[str] = [],
) -> str | None:
    """Prompt the user to provide the next tokens of the contextless output.
    Args:
        output_current_tokens (str): list of tokens of the current output
        cti_idx (int): index of the current token identified as context-sensitive
    Returns:
        str: next tokens of the contextless output specified by the user. If None, the user does not want to specify
            the contextless output.
    """
    contextual_prefix_tokens = output_current_tokens[:cti_idx]
    contextual_prefix = model.convert_tokens_to_string(contextual_prefix_tokens, skip_special_tokens=False)
    contextual_output_token = get_filtered_tokens(
        output_current_tokens[cti_idx],
        model,
        special_tokens_to_keep=special_tokens_to_keep,
        is_target=True,
        replace_special_characters=True,
    )[0]
    while True:
        force_contextless_output = Confirm.ask(
            f'\n:arrow_right: Contextual prefix: "[bold]{contextual_prefix}[/bold]"'
            f'\n:question: The token [bold]"{contextual_output_token}"[/bold] is produced in the contextual setting.'
            " Do you want to specify a word for comparison?"
        )
        if not force_contextless_output:
            return None
        provided_contextless_output = Prompt.ask(
            ":writing_hand: Please enter the word to use for comparison with the contextual output:"
        )
        if provided_contextless_output.strip():
            break
        rprint("[prompt.invalid]The provided word is empty. Please provide a non-empty word.")
    return provided_contextless_output

================
File: inseq/commands/attribute_context/attribute_context_viz_helpers.py
================
from copy import deepcopy
from typing import Literal
import treescope as ts
import treescope.figures as fg
import treescope.rendering_parts as rp
from rich.console import Console
from ... import load_model
from ...data.viz import get_single_token_heatmap_treescope, get_tokens_heatmap_treescope
from ...models import HuggingfaceModel
from ...utils.misc import isnotebook
from ...utils.viz_utils import treescope_cmap, treescope_ignore
from .attribute_context_args import AttributeContextArgs
from .attribute_context_helpers import (
    AttributeContextOutput,
    filter_rank_tokens,
    get_filtered_tokens,
    get_scores_threshold,
)
def get_formatted_procedure_details(args: AttributeContextArgs) -> str:
    def format_comment(std: float | None = None, topk: int | None = None) -> str:
        comment = []
        if std:
            comment.append(f"std Î»={std:.2f}")
        if topk:
            comment.append(f"top {topk}")
        if len(comment) > 0:
            return ", ".join(comment)
        return "all"
    cti_comment = format_comment(args.context_sensitivity_std_threshold, args.context_sensitivity_topk)
    cci_comment = format_comment(args.attribution_std_threshold, args.attribution_topk)
    input_context_comment, output_context_comment = "", ""
    if args.has_input_context:
        input_context_comment = f"\n[bold]Input context:[/bold]\t{args.input_context_text}"
    if args.has_output_context:
        output_context_comment = f"\n[bold]Output context:[/bold]\t{args.output_context_text}"
    return (
        f"\nContext with [bold green]contextual cues[/bold green] ({cci_comment}) followed by output"
        f" sentence with [bold dodger_blue1]context-sensitive target spans[/bold dodger_blue1] ({cti_comment})\n"
        f'(CTI = "{args.context_sensitivity_metric}", CCI = "{args.attribution_method}" w/ "{args.attributed_fn}" '
        f"target)\n{input_context_comment}\n[bold]Input current:[/bold] {args.input_current_text}"
        f"{output_context_comment}\n[bold]Output current:[/bold]\t{args.output_current_text}"
    )
def get_formatted_attribute_context_results(
    model: HuggingfaceModel,
    args: AttributeContextArgs,
    output: AttributeContextOutput,
    cti_threshold: float,
) -> str:
    """Format the results of the context attribution process."""
    def format_context_comment(
        model: HuggingfaceModel,
        has_other_context: bool,
        special_tokens_to_keep: list[str],
        context: str,
        context_scores: list[float],
        other_context_scores: list[float] | None = None,
        is_target: bool = False,
        context_type: Literal["Input", "Output"] = "Input",
    ) -> str:
        context_tokens = get_filtered_tokens(
            context, model, special_tokens_to_keep, replace_special_characters=True, is_target=is_target
        )
        scores = context_scores
        if has_other_context:
            scores += other_context_scores
        context_ranked_tokens, threshold = filter_rank_tokens(
            tokens=context_tokens,
            scores=scores,
            std_threshold=args.attribution_std_threshold,
            topk=args.attribution_topk,
        )
        for idx, score, tok in context_ranked_tokens:
            context_tokens[idx] = f"[bold green]{tok}({score:.3f})[/bold green]"
        cci_threshold_comment = f"(CCI > {threshold:.3f})" if threshold is not None else ""
        return f"\n[bold]{context_type} context {cci_threshold_comment}:[/bold]\t{''.join(context_tokens)}"
    out_string = ""
    output_current_tokens = get_filtered_tokens(
        output.output_current, model, args.special_tokens_to_keep, replace_special_characters=True, is_target=True
    )
    cti_theshold_comment = f"(CTI > {cti_threshold:.3f})" if cti_threshold is not None else ""
    for example_idx, cci_out in enumerate(output.cci_scores, start=1):
        curr_output_tokens = output_current_tokens.copy()
        cti_idx = cci_out.cti_idx
        cti_score = cci_out.cti_score
        cti_tok = curr_output_tokens[cti_idx]
        curr_output_tokens[cti_idx] = f"[bold dodger_blue1]{cti_tok}({cti_score:.3f})[/bold dodger_blue1]"
        output_current_comment = "".join(curr_output_tokens)
        input_context_comment, output_context_comment = "", ""
        if args.has_input_context:
            input_context_comment = format_context_comment(
                model,
                args.has_output_context,
                args.special_tokens_to_keep,
                output.input_context,
                cci_out.input_context_scores,
                cci_out.output_context_scores,
            )
        if args.has_output_context:
            output_context_comment = format_context_comment(
                model,
                args.has_input_context,
                args.special_tokens_to_keep,
                output.output_context,
                cci_out.output_context_scores,
                cci_out.input_context_scores,
                is_target=True,
                context_type="Output",
            )
        out_string += (
            f"#{example_idx}."
            f"\n[bold]Generated output {cti_theshold_comment}:[/bold]\t{output_current_comment}"
            f"{input_context_comment}{output_context_comment}\n"
        )
    return out_string
@treescope_ignore
def visualize_attribute_context_rich(
    output: AttributeContextOutput,
    model: HuggingfaceModel | str | None = None,
    cti_threshold: float | None = None,
    return_html: bool = False,
    show_viz: bool | None = None,
    viz_path: str | None = None,
) -> str | None:
    if output.info is None:
        raise ValueError("Cannot visualize attribution results without args. Set add_output_info = True.")
    if show_viz is None:
        show_viz = output.info.show_viz
    if viz_path is None:
        viz_path = output.info.viz_path
    console = Console(record=True)
    if model is None:
        model = output.info.model_name_or_path
    if isinstance(model, str):
        model = load_model(
            output.info.model_name_or_path,
            output.info.attribution_method,
            model_kwargs=deepcopy(output.info.model_kwargs),
            tokenizer_kwargs=deepcopy(output.info.tokenizer_kwargs),
        )
    elif not isinstance(model, HuggingfaceModel):
        raise TypeError(f"Unsupported model type {type(model)} for visualization.")
    if cti_threshold is None and len(output.cti_scores) > 1:
        cti_threshold = get_scores_threshold(output.cti_scores, output.info.context_sensitivity_std_threshold)
    viz = get_formatted_procedure_details(output.info)
    viz += "\n\n" + get_formatted_attribute_context_results(model, output.info, output, cti_threshold)
    with console.capture() as _:
        console.print(viz, soft_wrap=False)
    if show_viz:
        console.print(viz, soft_wrap=False)
    html = console.export_html()
    if viz_path:
        with open(viz_path, "w", encoding="utf-8") as f:
            f.write(html)
    if return_html:
        return html
    return None
def visualize_attribute_context_treescope(
    output: AttributeContextOutput,
    return_html: bool = False,
    show_viz: bool = False,
    viz_path: str | None = None,
) -> str | rp.RenderableTreePart:
    if output.info is None:
        raise ValueError("Cannot visualize attribution results without args. Set add_output_info = True.")
    cmap_cti = treescope_cmap("greens")
    cmap_cci = treescope_cmap("blues")
    parts = [
        fg.treescope_part_from_display_object(
            fg.text_on_color("Context-sensitive tokens", value=1, colormap=cmap_cti)
        ),
        rp.text(" in the generated output can be expanded to visualize the "),
        fg.treescope_part_from_display_object(fg.text_on_color("contextual cues", value=1, colormap=cmap_cci)),
        rp.text(" motivating their prediction.\n\n"),
    ]
    if output.info.context_sensitivity_std_threshold is not None:
        cti_threshold = round(output.mean_cti + (output.std_cti * output.info.context_sensitivity_std_threshold), 4)
    parts += [
        rp.build_full_line_with_annotations(
            rp.build_custom_foldable_tree_node(
                label=rp.custom_style(
                    fg.treescope_part_from_display_object(fg.text_on_color("Parameters", value=0)),
                    css_style="font-weight: bold;",
                ),
                contents=rp.fold_condition(
                    collapsed=rp.empty_part(),
                    expanded=rp.indented_children(
                        [
                            rp.custom_style(rp.text("Model: "), css_style="font-weight: bold;"),
                            rp.indented_children([rp.text(output.info.model_name_or_path + "\n")]),
                            rp.custom_style(rp.text("Context sensitivity metric: "), css_style="font-weight: bold;"),
                            rp.indented_children([rp.text(output.info.context_sensitivity_metric + "\n")]),
                            rp.custom_style(rp.text("Attribution method: "), css_style="font-weight: bold;"),
                            rp.indented_children([rp.text(output.info.attribution_method + "\n")]),
                            rp.custom_style(rp.text("Attributed function: "), css_style="font-weight: bold;"),
                            rp.indented_children([rp.text(output.info.attributed_fn + "\n")]),
                            rp.custom_style(
                                rp.text("Context sensitivity selection: "), css_style="font-weight: bold;"
                            ),
                            rp.indented_children(
                                [
                                    rp.text(
                                        f"|x| â‰¥ {cti_threshold} (Mean Â± {output.info.context_sensitivity_std_threshold} standard deviation)\n"
                                        if output.info.context_sensitivity_std_threshold is not None
                                        else f"Top {output.info.context_sensitivity_topk} scores\n"
                                        if output.info.context_sensitivity_topk is not None
                                        else "All scores\n"
                                    )
                                ]
                            ),
                        ]
                    ),
                ),
                expand_state=rp.ExpandState.COLLAPSED,
            )
        ),
        rp.text("\n\n"),
    ]
    if output.input_context is not None:
        if len(output.input_context) > 1000 or "\n" in output.input_context:
            parts += [
                rp.build_full_line_with_annotations(
                    rp.build_custom_foldable_tree_node(
                        label=rp.custom_style(rp.text("Input context: "), css_style="font-weight: bold;"),
                        contents=rp.fold_condition(
                            collapsed=rp.custom_style(
                                rp.text(
                                    output.input_context[:100].replace("\n", " ")
                                    + ("..." if len(output.input_context) > 100 else "")
                                ),
                                css_style="font-style: italic; color: #888888;",
                            ),
                            expanded=rp.indented_children([rp.text(output.input_context)]),
                        ),
                        expand_state=rp.ExpandState.COLLAPSED,
                    )
                ),
                rp.text("\n"),
            ]
        else:
            parts += [
                rp.custom_style(rp.text(" Input context: "), css_style="font-weight: bold;"),
                rp.text(output.input_context + "\n"),
            ]
    parts += [
        rp.custom_style(rp.text(" Input current: "), css_style="font-weight: bold;"),
        rp.text(output.info.input_current_text + "\n"),
    ]
    if output.output_context is not None:
        if len(output.output_context) > 1000 or "\n" in output.output_context:
            parts += [
                rp.build_full_line_with_annotations(
                    rp.build_custom_foldable_tree_node(
                        label=rp.custom_style(rp.text("Output context: "), css_style="font-weight: bold;"),
                        contents=rp.fold_condition(
                            collapsed=rp.custom_style(
                                rp.text(
                                    output.output_context[:100].replace("\n", " ")
                                    + ("..." if len(output.output_context) > 100 else "")
                                ),
                                css_style="font-style: italic; color: #888888;",
                            ),
                            expanded=rp.indented_children([rp.text(output.output_context)]),
                        ),
                        expand_state=rp.ExpandState.COLLAPSED,
                    )
                ),
                rp.text("\n"),
            ]
        else:
            parts += [
                rp.custom_style(rp.text(" Output context: "), css_style="font-weight: bold;"),
                rp.text(output.output_context + "\n"),
            ]
    parts += [rp.custom_style(rp.text("\n Output current: "), css_style="font-weight: bold;")]
    replace_chars = {"Ä ": " ", "ÄŠ": "\n", "â–": " "}
    cci_idx_map = {cci.cti_idx: cci for cci in output.cci_scores} if output.cci_scores is not None else {}
    for curr_tok_idx, curr_tok in enumerate(output.output_current_tokens):
        curr_tok_parts, highlighted_idx, cleaned_curr_tok = get_single_token_heatmap_treescope(
            curr_tok,
            score=output.cti_scores[curr_tok_idx],
            max_val=output.max_cti,
            colormap=cmap_cti,
            strip_chars=replace_chars,
            show_empty_tokens=True,
            return_highlighted_idx=True,
        )
        if curr_tok_idx in cci_idx_map:
            cci_parts = [rp.text("\n")]
            cci = cci_idx_map[curr_tok_idx]
            if cci.contrast_token is not None:
                contrast_token = cci.contrast_token
                for char in replace_chars.keys():
                    contrast_token = contrast_token.strip(char)
                if contrast_token != cleaned_curr_tok:
                    cci_parts += [
                        rp.custom_style(
                            rp.text("Contrastive alternative: "),
                            css_style="font-weight: bold; font-style: italic; color: #888888;",
                        ),
                        rp.custom_style(
                            rp.text(contrast_token + "\n\n"), css_style="font-style: italic; color: #888888;"
                        ),
                    ]
            if cci.input_context_scores is not None:
                cci_parts.append(
                    get_tokens_heatmap_treescope(
                        tokens=output.input_context_tokens,
                        scores=cci.input_context_scores,
                        title=f'Input contextual cues for "{cleaned_curr_tok}"',
                        title_style="font-style: italic; color: #888888;",
                        min_val=cci.minimum,
                        max_val=cci.maximum,
                        rounding=10,
                        colormap=cmap_cci,
                        strip_chars=replace_chars,
                        show_empty_tokens=False,
                    )
                )
                cci_parts.append(rp.text("\n\n"))
            if cci.output_context_scores is not None:
                cci_parts.append(
                    get_tokens_heatmap_treescope(
                        tokens=output.output_context_tokens,
                        scores=cci.output_context_scores,
                        title=f'Output contextual cue for "{cleaned_curr_tok}"',
                        title_style="font-style: italic; color: #888888;",
                        min_val=cci.minimum,
                        max_val=cci.maximum,
                        rounding=10,
                        colormap=cmap_cci,
                        strip_chars=replace_chars,
                        show_empty_tokens=False,
                    )
                )
                cci_parts.append(rp.text("\n\n"))
            curr_tok_parts[highlighted_idx] = rp.custom_style(
                rp.build_full_line_with_annotations(
                    rp.build_custom_foldable_tree_node(
                        label=curr_tok_parts[highlighted_idx],
                        contents=rp.fold_condition(
                            collapsed=rp.empty_part(),
                            expanded=rp.indented_children([rp.siblings(*cci_parts)]),
                        ),
                    )
                ),
                css_style="margin-left: 0.7em;",
            )
        parts += curr_tok_parts
    out_tree = rp.custom_style(rp.siblings(*parts), css_style="white-space: pre-wrap")
    with ts.active_autovisualizer.set_scoped(ts.ArrayAutovisualizer()):
        fig = fg.figure_from_treescope_rendering_part(out_tree)
        if show_viz:
            import IPython
            IPython.display.display(fig)
        html = ts.lowering.render_to_html_as_root(out_tree)
        if viz_path:
            with open(viz_path, "w", encoding="utf-8") as f:
                f.write(html)
    if return_html:
        return html
    return out_tree
def visualize_attribute_context(
    output: AttributeContextOutput,
    model: HuggingfaceModel | str | None = None,
    cti_threshold: float | None = None,
    show_viz: bool = True,
    viz_path: str | None = None,
    return_html: bool = False,
) -> str | None:
    if isnotebook() or not show_viz:
        return visualize_attribute_context_treescope(output, return_html, show_viz=show_viz, viz_path=viz_path)
    return visualize_attribute_context_rich(output, model, cti_threshold, return_html, show_viz, viz_path)

================
File: inseq/commands/attribute_context/attribute_context.py
================
"""Implementation of the context attribution process described in `Quantifying the Plausibility of Context Reliance in
Neural Machine Translation <https://arxiv.org/abs/2310.01188>`_ for decoder-only and encoder-decoder models.
The process consists of two steps:
    - Context-sensitive Token Identification (CTI): detects which tokens in the generated output of interest are
        influenced by the presence of context.
    - Contextual Cues Imputation (CCI): attributes the generation of context-sensitive tokens identified in the first
        step to the input and output contexts.
Example usage:
```bash
inseq attribute-context \
    --model_name_or_path gpt2 \
    --input_context_text "George was sick yesterday." \
    --input_current_text "His colleagues asked him" \
    --attributed_fn contrast_prob_diff
```
"""
import json
import logging
import warnings
from copy import deepcopy
import transformers
from ... import load_model
from ...attr.step_functions import is_contrastive_step_function
from ...models import HuggingfaceModel
from ..attribute import aggregate_attribution_scores
from ..base import BaseCLICommand
from .attribute_context_args import AttributeContextArgs
from .attribute_context_helpers import (
    AttributeContextOutput,
    CCIOutput,
    concat_with_sep,
    filter_rank_tokens,
    format_template,
    get_contextless_output,
    get_filtered_tokens,
    get_source_target_cci_scores,
    prepare_outputs,
)
from .attribute_context_viz_helpers import visualize_attribute_context
warnings.filterwarnings("ignore")
transformers.logging.set_verbosity_error()
logger = logging.getLogger(__name__)
def attribute_context(args: AttributeContextArgs) -> AttributeContextOutput:
    """Attribute the generation of context-sensitive tokens in ``output_current_text`` to input/output contexts."""
    model: HuggingfaceModel = load_model(
        args.model_name_or_path,
        args.attribution_method,
        model_kwargs=deepcopy(args.model_kwargs),
        tokenizer_kwargs=deepcopy(args.tokenizer_kwargs),
    )
    if not isinstance(args.model_name_or_path, str):
        args.model_name_or_path = model.model_name
    return attribute_context_with_model(args, model)
def attribute_context_with_model(args: AttributeContextArgs, model: HuggingfaceModel) -> AttributeContextOutput:
    # Handle language tag for multilingual models - no need to specify it in generation kwargs
    has_lang_tag = "tgt_lang" in args.tokenizer_kwargs
    if has_lang_tag and "forced_bos_token_id" not in args.generation_kwargs:
        tgt_lang = args.tokenizer_kwargs["tgt_lang"]
        args.generation_kwargs["forced_bos_token_id"] = model.tokenizer.lang_code_to_id[tgt_lang]
    # Prepare input/outputs (generate if necessary)
    input_full_text = format_template(args.input_template, args.input_current_text, args.input_context_text)
    args.output_context_text, args.output_current_text = prepare_outputs(
        model=model,
        input_context_text=args.input_context_text,
        input_full_text=input_full_text,
        output_context_text=args.output_context_text,
        output_current_text=args.output_current_text,
        output_template=args.output_template,
        handle_output_context_strategy=args.handle_output_context_strategy,
        generation_kwargs=deepcopy(args.generation_kwargs),
        special_tokens_to_keep=args.special_tokens_to_keep,
        decoder_input_output_separator=args.decoder_input_output_separator,
    )
    output_full_text = format_template(args.output_template, args.output_current_text, args.output_context_text)
    # Tokenize inputs/outputs and compute offset
    input_context_tokens = None
    if args.input_context_text is not None:
        input_context_tokens = get_filtered_tokens(args.input_context_text, model, args.special_tokens_to_keep)
    if not model.is_encoder_decoder:
        output_full_text = concat_with_sep(input_full_text, output_full_text, args.decoder_input_output_separator)
    output_current_tokens = get_filtered_tokens(
        args.output_current_text, model, args.special_tokens_to_keep, is_target=True
    )
    output_context_tokens = None
    if args.output_context_text is not None:
        output_context_tokens = get_filtered_tokens(
            args.output_context_text, model, args.special_tokens_to_keep, is_target=True
        )
    input_full_tokens = get_filtered_tokens(input_full_text, model, args.special_tokens_to_keep)
    output_full_tokens = get_filtered_tokens(output_full_text, model, args.special_tokens_to_keep, is_target=True)
    output_current_text_offset = len(output_full_tokens) - len(output_current_tokens)
    formatted_input_current_text = args.contextless_input_current_text.format(current=args.input_current_text)
    formatted_output_current_text = args.contextless_output_current_text.format(current=args.output_current_text)
    if not model.is_encoder_decoder:
        formatted_input_current_text = concat_with_sep(
            formatted_input_current_text, "", args.decoder_input_output_separator
        )
        formatted_output_current_text = formatted_input_current_text + formatted_output_current_text
    # Part 1: Context-sensitive Token Identification (CTI)
    cti_out = model.attribute(
        formatted_input_current_text.rstrip(" "),
        formatted_output_current_text,
        attribute_target=model.is_encoder_decoder,
        step_scores=[args.context_sensitivity_metric],
        contrast_sources=input_full_text if model.is_encoder_decoder else None,
        contrast_targets=output_full_text,
        show_progress=False,
        method="dummy",
    )[0]
    if args.show_intermediate_outputs:
        cti_out.show(do_aggregation=False)
    start_pos = 1 if has_lang_tag else 0
    contextless_output_prefix = args.contextless_output_current_text.split("{current}")[0]
    contextless_output_prefix_tokens = get_filtered_tokens(
        contextless_output_prefix, model, args.special_tokens_to_keep, is_target=True
    )
    start_pos += len(contextless_output_prefix_tokens)
    cti_scores = cti_out.step_scores[args.context_sensitivity_metric][start_pos:].tolist()
    cti_tokens = [t.token for t in cti_out.target][start_pos + cti_out.attr_pos_start :]
    if model.is_encoder_decoder:
        cti_scores = cti_scores[:-1]
        cti_tokens = cti_tokens[:-1]
    cti_ranked_tokens, cti_threshold = filter_rank_tokens(
        tokens=cti_tokens,
        scores=cti_scores,
        std_threshold=args.context_sensitivity_std_threshold,
        topk=args.context_sensitivity_topk,
    )
    output = AttributeContextOutput(
        input_context=args.input_context_text,
        input_context_tokens=input_context_tokens,
        output_context=args.output_context_text,
        output_context_tokens=output_context_tokens,
        output_current=args.output_current_text,
        output_current_tokens=cti_tokens,
        cti_scores=cti_scores,
        info=args,
    )
    # Part 2: Contextual Cues Imputation (CCI)
    for cci_step_idx, (cti_idx, cti_score, cti_tok) in enumerate(cti_ranked_tokens):
        contextual_input = model.convert_tokens_to_string(input_full_tokens, skip_special_tokens=False).lstrip(" ")
        contextual_output = model.convert_tokens_to_string(
            output_full_tokens[: output_current_text_offset + cti_idx + 1], skip_special_tokens=False
        ).lstrip(" ")
        if not contextual_output:
            output_ctx_tokens = [output_full_tokens[output_current_text_offset + cti_idx]]
            if model.is_encoder_decoder:
                output_ctx_tokens.append(model.pad_token)
            contextual_output = model.convert_tokens_to_string(output_ctx_tokens, skip_special_tokens=True)
        else:
            output_ctx_tokens = model.convert_string_to_tokens(
                contextual_output, skip_special_tokens=False, as_targets=model.is_encoder_decoder
            )
        cci_kwargs = {}
        contextless_output = None
        contrast_token = None
        if args.attributed_fn is not None and is_contrastive_step_function(args.attributed_fn):
            if not model.is_encoder_decoder:
                formatted_input_current_text = concat_with_sep(
                    formatted_input_current_text, contextless_output_prefix, args.decoder_input_output_separator
                )
            contextless_output = get_contextless_output(
                model,
                formatted_input_current_text,
                output_current_tokens,
                cti_idx,
                cti_ranked_tokens,
                args.contextless_output_next_tokens,
                args.prompt_user_for_contextless_output_next_tokens,
                cci_step_idx,
                args.decoder_input_output_separator,
                args.special_tokens_to_keep,
                deepcopy(args.generation_kwargs),
            )
            cci_kwargs["contrast_sources"] = formatted_input_current_text if model.is_encoder_decoder else None
            cci_kwargs["contrast_targets"] = contextless_output
            output_ctxless_tokens = model.convert_string_to_tokens(
                contextless_output, skip_special_tokens=False, as_targets=model.is_encoder_decoder
            )
            tok_pos = -2 if model.is_encoder_decoder else -1
            contrast_token = output_ctxless_tokens[tok_pos]
            if args.attributed_fn == "kl_divergence" or output_ctx_tokens[tok_pos] == output_ctxless_tokens[tok_pos]:
                cci_kwargs["contrast_force_inputs"] = True
        bos_offset = int(
            model.is_encoder_decoder
            or (output_ctx_tokens[0] == model.bos_token and model.bos_token not in args.special_tokens_to_keep)
        )
        pos_start = output_current_text_offset + cti_idx + bos_offset + int(has_lang_tag)
        cci_attrib_out = model.attribute(
            contextual_input,
            contextual_output,
            attribute_target=model.is_encoder_decoder and args.has_output_context,
            show_progress=False,
            attr_pos_start=pos_start,
            attributed_fn=args.attributed_fn,
            method=args.attribution_method,
            **cci_kwargs,
            **args.attribution_kwargs,
        )
        cci_attrib_out = aggregate_attribution_scores(
            out=cci_attrib_out,
            selectors=args.attribution_selectors,
            aggregators=args.attribution_aggregators,
            normalize_attributions=args.normalize_attributions,
            rescale_attributions=args.rescale_attributions,
        )[0]
        if args.show_intermediate_outputs:
            cci_attrib_out.show(do_aggregation=False)
        source_scores, target_scores = get_source_target_cci_scores(
            model,
            cci_attrib_out,
            args.input_template,
            args.input_current_text,
            input_context_tokens,
            input_full_tokens,
            args.output_template,
            output_context_tokens,
            args.has_input_context,
            args.has_output_context,
            has_lang_tag,
            args.decoder_input_output_separator,
            args.special_tokens_to_keep,
        )
        cci_out = CCIOutput(
            cti_idx=cti_idx,
            cti_token=cti_tok,
            contrast_token=contrast_token,
            cti_score=cti_score,
            contextual_output=contextual_output,
            contextless_output=contextless_output,
            input_context_scores=source_scores,
            output_context_scores=target_scores,
        )
        output.cci_scores.append(cci_out)
    if args.show_viz or args.viz_path:
        visualize_attribute_context(output, model, cti_threshold, args.show_viz, args.viz_path)
    if not args.add_output_info:
        output.info = None
    if args.save_path:
        with open(args.save_path, "w") as f:
            json.dump(output.to_dict(), f, indent=4)
    return output
class AttributeContextCommand(BaseCLICommand):
    _name = "attribute-context"
    _help = "Detect context-sensitive tokens in a generated text and attribute their predictions to available context."
    _dataclasses = AttributeContextArgs
    def run(args: AttributeContextArgs):
        attribute_context(args)

================
File: inseq/commands/attribute_dataset/__init__.py
================
from .attribute_dataset import AttributeDatasetCommand
__all__ = [
    "AttributeDatasetCommand",
]

================
File: inseq/commands/attribute_dataset/attribute_dataset_args.py
================
from dataclasses import dataclass
from ...utils import cli_arg
from ..commands_utils import command_args_docstring
@command_args_docstring
@dataclass
class LoadDatasetArgs:
    dataset_name: str = cli_arg(
        aliases=["-d", "--dataset"],
        help="The type of dataset to be loaded for attribution.",
    )
    input_text_field: str | None = cli_arg(
        aliases=["-in", "--input"], help="Name of the field containing the input texts used for attribution."
    )
    generated_text_field: str | None = cli_arg(
        default=None,
        aliases=["-gen", "--generated"],
        help="Name of the field containing the generated texts used for constrained decoding.",
    )
    dataset_config: str | None = cli_arg(
        default=None, aliases=["--config"], help="The name of the Huggingface dataset configuration."
    )
    dataset_dir: str | None = cli_arg(
        default=None, aliases=["--dir"], help="Path to the directory containing the data files."
    )
    dataset_files: list[str] | None = cli_arg(default=None, aliases=["--files"], help="Path to the dataset files.")
    dataset_split: str | None = cli_arg(default="train", aliases=["--split"], help="Dataset split.")
    dataset_revision: str | None = cli_arg(
        default=None, aliases=["--revision"], help="The Huggingface dataset revision."
    )
    dataset_auth_token: str | None = cli_arg(
        default=None, aliases=["--auth"], help="The auth token for the Huggingface dataset."
    )
    dataset_kwargs: dict | None = cli_arg(
        default_factory=dict,
        help="Additional keyword arguments passed to the dataset constructor in JSON format.",
    )

================
File: inseq/commands/attribute_dataset/attribute_dataset.py
================
from ...utils import is_datasets_available
from ..attribute import AttributeExtendedArgs
from ..attribute.attribute import attribute
from ..base import BaseCLICommand
from .attribute_dataset_args import LoadDatasetArgs
if is_datasets_available():
    from datasets import load_dataset
def load_fields_from_dataset(dataset_args: LoadDatasetArgs) -> tuple[list[str], list[str] | None]:
    if not is_datasets_available():
        raise ImportError("The datasets library needs to be installed to use the attribute-dataset client.")
    dataset = load_dataset(
        dataset_args.dataset_name,
        dataset_args.dataset_config,
        data_dir=dataset_args.dataset_dir,
        data_files=dataset_args.dataset_files,
        split=dataset_args.dataset_split,
        revision=dataset_args.dataset_revision,
        token=dataset_args.dataset_auth_token,
        **dataset_args.dataset_kwargs,
    )
    df = dataset.to_pandas()
    if dataset_args.input_text_field in df.columns:
        input_texts = list(df[dataset_args.input_text_field])
    else:
        raise ValueError(f"The input text field {dataset_args.input_text_field} does not exist in the dataset.")
    generated_texts = None
    if dataset_args.generated_text_field is not None:
        if dataset_args.generated_text_field in df.columns:
            generated_texts = list(df[dataset_args.generated_text_field])
    return input_texts, generated_texts
class AttributeDatasetCommand(BaseCLICommand):
    _name = "attribute-dataset"
    _help = "Perform feature attribution on a full dataset and save the results to a file"
    _dataclasses = AttributeExtendedArgs, LoadDatasetArgs
    def run(args: tuple[AttributeExtendedArgs, LoadDatasetArgs]):
        attribute_args, dataset_args = args
        input_texts, generated_texts = load_fields_from_dataset(dataset_args)
        attribute(input_texts, generated_texts, attribute_args)

================
File: inseq/commands/attribute/__init__.py
================
from .attribute import AttributeCommand, aggregate_attribution_scores
from .attribute_args import AttributeBaseArgs, AttributeExtendedArgs, AttributeWithInputsArgs
__all__ = [
    "AttributeCommand",
    "aggregate_attribution_scores",
    "AttributeBaseArgs",
    "AttributeExtendedArgs",
    "AttributeWithInputsArgs",
]

================
File: inseq/commands/attribute/attribute_args.py
================
from dataclasses import dataclass
from ... import (
    list_aggregation_functions,
    list_aggregators,
    list_feature_attribution_methods,
    list_step_functions,
)
from ...utils import cli_arg, get_default_device
from ..commands_utils import command_args_docstring
@command_args_docstring
@dataclass
class AttributeBaseArgs:
    model_name_or_path: str = cli_arg(
        default=None, aliases=["-m"], help="The name or path of the model on which attribution is performed."
    )
    attribution_method: str | None = cli_arg(
        default="saliency",
        aliases=["-a"],
        help="The attribution method used to perform feature attribution.",
        choices=list_feature_attribution_methods(),
    )
    device: str = cli_arg(
        default=get_default_device(),
        aliases=["--dev"],
        help="The device used for inference with Pytorch. Multi-GPU is not supported.",
    )
    attributed_fn: str | None = cli_arg(
        default=None,
        aliases=["-fn"],
        choices=list_step_functions(),
        help=(
            "The attribution target used for the attribution method. Default: ``probability``. If a"
            " step function requiring additional arguments is used (e.g. ``contrast_prob_diff``), they should be"
            " specified using the ``attribution_kwargs`` argument."
        ),
    )
    attribution_selectors: list[int] | None = cli_arg(
        default=None,
        help=(
            "The indices of the attribution scores to be used for the attribution aggregation. If specified, the"
            " aggregation function is applied only to the selected scores, and the other scores are discarded."
            " If not specified, the aggregation function is applied to all the scores."
        ),
    )
    attribution_aggregators: list[str] = cli_arg(
        default=None,
        help=(
            "The aggregators used to aggregate the attribution scores for each context. The outcome should"
            " produce one score per input token"
        ),
        choices=list_aggregators() + list_aggregation_functions(),
    )
    normalize_attributions: bool = cli_arg(
        default=False,
        help=(
            "Whether to normalize the attribution scores for each context. If ``True``, the attribution scores "
            "for each context are normalized to sum up to 1, providing a relative notion of input salience."
        ),
    )
    rescale_attributions: bool = cli_arg(
        default=False,
        help=(
            "Whether to rescale the attribution scores for each context. If ``True``, the attribution scores "
            "for each context are rescaled to sum up to the number of tokens in the input, providing an absolute"
            " notion of input salience."
        ),
    )
    model_kwargs: dict = cli_arg(
        default_factory=dict,
        help="Additional keyword arguments passed to the model constructor in JSON format.",
    )
    tokenizer_kwargs: dict = cli_arg(
        default_factory=dict,
        help="Additional keyword arguments passed to the tokenizer constructor in JSON format.",
    )
    generation_kwargs: dict = cli_arg(
        default_factory=dict,
        help="Additional keyword arguments passed to the generation method in JSON format.",
    )
    attribution_kwargs: dict = cli_arg(
        default_factory=dict,
        help="Additional keyword arguments passed to the attribution method in JSON format.",
    )
@command_args_docstring
@dataclass
class AttributeExtendedArgs(AttributeBaseArgs):
    attribute_target: bool = cli_arg(
        default=False,
        help="Performs the attribution procedure including the generated target prefix at every step.",
    )
    generate_from_target_prefix: bool = cli_arg(
        default=False,
        help=(
            "Whether the ``generated_texts`` should be used as target prefixes for the generation process. If False,"
            " the ``generated_texts`` are used as full targets. Option only available for encoder-decoder models,"
            " since for decoder-only ones it is sufficient to add prefix to input string. Default: False."
        ),
    )
    step_scores: list[str] = cli_arg(
        default_factory=list,
        help="Adds the specified step scores to the attribution output.",
        choices=list_step_functions(),
    )
    output_step_attributions: bool = cli_arg(default=False, help="Adds step-level feature attributions to the output.")
    include_eos_baseline: bool = cli_arg(
        default=False,
        aliases=["--eos"],
        help="Whether the EOS token should be included in the baseline, used for some attribution methods.",
    )
    batch_size: int = cli_arg(
        default=8, aliases=["-bs"], help="The batch size used for the attribution computation. Default: no batching."
    )
    aggregate_output: bool = cli_arg(
        default=False,
        help="If specified, the attribution output is aggregated using its default aggregator before saving.",
    )
    hide_attributions: bool = cli_arg(
        default=False,
        aliases=["--hide"],
        help="If specified, the attribution visualization are not shown in the output.",
    )
    save_path: str | None = cli_arg(
        default=None,
        aliases=["-o"],
        help="Path where the attribution output should be saved in JSON format.",
    )
    viz_path: str | None = cli_arg(
        default=None,
        help="Path where the attribution visualization should be saved in HTML format.",
    )
    start_pos: int | None = cli_arg(
        default=None, aliases=["-s"], help="Start position for the attribution. Default: first token"
    )
    end_pos: int | None = cli_arg(
        default=None, aliases=["-e"], help="End position for the attribution. Default: last token"
    )
    verbose: bool = cli_arg(
        default=False, aliases=["-v"], help="If specified, use INFO as logging level for the attribution."
    )
    very_verbose: bool = cli_arg(
        default=False, aliases=["-vv"], help="If specified, use DEBUG as logging level for the attribution."
    )
@command_args_docstring
@dataclass
class AttributeWithInputsArgs(AttributeExtendedArgs):
    input_texts: list[str] = cli_arg(default=None, aliases=["-i"], help="One or more input texts used for generation.")
    generated_texts: list[str] | None = cli_arg(
        default=None, aliases=["-g"], help="If specified, constrains the decoding procedure to the specified outputs."
    )
    def __post_init__(self):
        if self.input_texts is None:
            raise RuntimeError("Input texts must be specified.")
        if isinstance(self.input_texts, str):
            self.input_texts = list(self.input_texts)
        if isinstance(self.generated_texts, str):
            self.generated_texts = list(self.generated_texts)

================
File: inseq/commands/attribute/attribute.py
================
import logging
from ... import FeatureAttributionOutput, load_model
from ..base import BaseCLICommand
from .attribute_args import AttributeExtendedArgs, AttributeWithInputsArgs
def aggregate_attribution_scores(
    out: FeatureAttributionOutput,
    selectors: list[int] | None = None,
    aggregators: list[str] | None = None,
    normalize_attributions: bool = False,
    rescale_attributions: bool = False,
) -> FeatureAttributionOutput:
    if selectors is not None and aggregators is not None:
        for select_idx, aggregator_fn in zip(selectors, aggregators, strict=False):
            out = out.aggregate(
                aggregator=aggregator_fn,
                normalize=normalize_attributions,
                rescale=rescale_attributions,
                select_idx=select_idx,
                do_post_aggregation_checks=False,
            )
    else:
        out = out.aggregate(aggregator=aggregators, normalize=normalize_attributions)
    return out
def attribute(input_texts, generated_texts, args: AttributeExtendedArgs):
    if args.very_verbose:
        log_level = logging.DEBUG
    elif args.verbose:
        log_level = logging.INFO
    else:
        log_level = logging.WARNING
    logging.basicConfig(
        level=log_level,
        format="%(asctime)s.%(msecs)03d %(levelname)s %(module)s - %(funcName)s: %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )
    model = load_model(
        args.model_name_or_path,
        attribution_method=args.attribution_method,
        device=args.device,
        model_kwargs=args.model_kwargs,
        tokenizer_kwargs=args.tokenizer_kwargs,
    )
    # Handle language tag for multilingual models - no need to specify it in generation kwargs
    if "tgt_lang" in args.tokenizer_kwargs and "forced_bos_token_id" not in args.generation_kwargs:
        tgt_lang = args.tokenizer_kwargs["tgt_lang"]
        args.generation_kwargs["forced_bos_token_id"] = model.tokenizer.lang_code_to_id[tgt_lang]
    out = model.attribute(
        input_texts,
        generated_texts,
        batch_size=args.batch_size,
        attribute_target=args.attribute_target,
        attributed_fn=args.attributed_fn,
        step_scores=args.step_scores,
        output_step_attributions=args.output_step_attributions,
        include_eos_baseline=args.include_eos_baseline,
        device=args.device,
        generation_args=args.generation_kwargs,
        attr_pos_start=args.start_pos,
        attr_pos_end=args.end_pos,
        generate_from_target_prefix=args.generate_from_target_prefix,
        **args.attribution_kwargs,
    )
    if args.viz_path:
        print(f"Saving visualization to {args.viz_path}")
        html = out.show(return_html=True, display=not args.hide_attributions)
        with open(args.viz_path, "w") as f:
            f.write(html)
    else:
        out.show(display=not args.hide_attributions)
    if args.save_path:
        if args.attribution_aggregators is not None:
            out = aggregate_attribution_scores(
                out=out,
                selectors=args.attribution_selectors,
                aggregators=args.attribution_aggregators,
                normalize_attributions=args.normalize_attributions,
                rescale_attributions=args.rescale_attributions,
            )
        print(f"Saving {'aggregated ' if args.aggregate_output else ''}attributions to {args.save_path}")
        out.save(args.save_path, overwrite=True)
class AttributeCommand(BaseCLICommand):
    _name = "attribute"
    _help = "Perform feature attribution on one or multiple sentences"
    _dataclasses = AttributeWithInputsArgs
    def run(args: AttributeWithInputsArgs):
        attribute(args.input_texts, args.generated_texts, args)

================
File: inseq/commands/base.py
================
import dataclasses
from abc import ABC, abstractmethod
from argparse import Namespace
from collections.abc import Iterable
from typing import Any, NewType
from ..utils import InseqArgumentParser
DataClassType = NewType("DataClassType", Any)
OneOrMoreDataClasses = DataClassType | Iterable[DataClassType]
class BaseCLICommand(ABC):
    """Adapted from https://github.com/huggingface/transformers/blob/main/src/transformers/commands/__init__.py."""
    _name: str = None
    _help: str = None
    _dataclasses: OneOrMoreDataClasses = None
    @classmethod
    def register_subcommand(cls, parser: InseqArgumentParser):
        """Register this command to argparse so it's available for the Inseq cli.
        Args:
            parser: Root parser to register command-specific arguments
        """
        command_parser = parser.add_parser(
            cls._name,
            help=cls._help,
            dataclass_types=cls._dataclasses,
        )
        command_parser.set_defaults(factory_method=cls.build)
    @classmethod
    def build(cls, args: Namespace):
        dataclasses_args = []
        if not isinstance(cls._dataclasses, tuple):
            cls._dataclasses = (cls._dataclasses,)
        for dataclass_type in cls._dataclasses:
            keys = {f.name for f in dataclasses.fields(dataclass_type) if f.init}
            inputs = {k: v for k, v in vars(args).items() if k in keys}
            dataclasses_args.append(dataclass_type(**inputs))
        if len(dataclasses_args) == 1:
            dataclasses_args = dataclasses_args[0]
        return cls, dataclasses_args
    @staticmethod
    @abstractmethod
    def run(args: OneOrMoreDataClasses):
        raise NotImplementedError()

================
File: inseq/commands/cli.py
================
"""Adapted from https://github.com/huggingface/transformers/blob/main/src/transformers/commands/transformers_cli.py."""
import sys
from ..utils import InseqArgumentParser
from .attribute import AttributeCommand
from .attribute_context import AttributeContextCommand
from .attribute_dataset import AttributeDatasetCommand
from .base import BaseCLICommand
COMMANDS: list[BaseCLICommand] = [AttributeCommand, AttributeDatasetCommand, AttributeContextCommand]
def main():
    parser = InseqArgumentParser(prog="Inseq CLI tool", usage="inseq <COMMAND> [<ARGS>]")
    command_parser = parser.add_subparsers(title="Inseq CLI command helpers")
    for command_type in COMMANDS:
        command_type.register_subcommand(command_parser)
    args = parser.parse_args()
    if not hasattr(args, "factory_method"):
        parser.print_help()
        sys.exit(1)
    # Run
    command, command_args = args.factory_method(args)
    command.run(command_args)
if __name__ == "__main__":
    main()

================
File: inseq/commands/commands_utils.py
================
import dataclasses
import textwrap
import typing
def command_args_docstring(cls):
    """
    A decorator that automatically generates a Google-style docstring for a dataclass.
    """
    docstring = f"{cls.__name__}\n\n"
    fields = dataclasses.fields(cls)
    resolved_hints = typing.get_type_hints(cls)
    resolved_field_types = {field.name: resolved_hints[field.name] for field in fields}
    if fields:
        docstring += "**Attributes:**\n"
        for field in fields:
            field_type = resolved_field_types[field.name]
            field_help = field.metadata.get("help", "")
            docstring += textwrap.dedent(f"\n**{field.name}** (``{field_type}``): {field_help}\n")
    cls.__doc__ = docstring
    return cls

================
File: inseq/data/__init__.py
================
from .aggregation_functions import (
    AggregationFunction,
    list_aggregation_functions,
)
from .aggregator import (
    Aggregator,
    AggregatorPipeline,
    ContiguousSpanAggregator,
    PairAggregator,
    SequenceAttributionAggregator,
    SubwordAggregator,
    list_aggregators,
)
from .attribution import (
    CoarseFeatureAttributionSequenceOutput,
    CoarseFeatureAttributionStepOutput,
    FeatureAttributionInput,
    FeatureAttributionOutput,
    FeatureAttributionSequenceOutput,
    FeatureAttributionStepOutput,
    GranularFeatureAttributionStepOutput,
    MultiDimensionalFeatureAttributionStepOutput,
    get_batch_from_inputs,
    merge_attributions,
)
from .batch import (
    Batch,
    BatchEmbedding,
    BatchEncoding,
    DecoderOnlyBatch,
    EncoderDecoderBatch,
    slice_batch_from_position,
)
from .viz import show_attributions, show_granular_attributions, show_token_attributions
__all__ = [
    "Aggregator",
    "AggregatorPipeline",
    "AggregationFunction",
    "SequenceAttributionAggregator",
    "ContiguousSpanAggregator",
    "SubwordAggregator",
    "PairAggregator",
    "Batch",
    "DecoderOnlyBatch",
    "BatchEmbedding",
    "BatchEncoding",
    "EncoderDecoderBatch",
    "FeatureAttributionInput",
    "FeatureAttributionStepOutput",
    "GranularFeatureAttributionStepOutput",
    "CoarseFeatureAttributionSequenceOutput",
    "CoarseFeatureAttributionStepOutput",
    "FeatureAttributionSequenceOutput",
    "FeatureAttributionOutput",
    "ModelIdentifier",
    "OneOrMoreIdSequences",
    "OneOrMoreTokenSequences",
    "TextInput",
    "show_attributions",
    "show_granular_attributions",
    "show_token_attributions",
    "list_aggregation_functions",
    "MultiDimensionalFeatureAttributionStepOutput",
    "get_batch_from_inputs",
    "merge_attributions",
    "list_aggregators",
    "slice_batch_from_position",
]

================
File: inseq/data/aggregation_functions.py
================
# Copyright 2023 The Inseq Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import logging
from abc import abstractmethod
import torch
from torch.linalg import vector_norm
from ..utils import Registry, available_classes
from ..utils.typing import (
    ScoreTensor,
)
logger = logging.getLogger(__name__)
class AggregationFunction(Registry):
    registry_attr = "aggregation_function_name"
    def __init__(self):
        self.takes_single_tensor: bool = True
        self.takes_sequence_scores: bool = False
    @abstractmethod
    def __call__(
        self,
        scores: torch.Tensor | tuple[torch.Tensor, ...],
        dim: int,
        **kwargs,
    ) -> ScoreTensor:
        pass
class MeanAggregationFunction(AggregationFunction):
    aggregation_function_name = "mean"
    def __call__(self, scores: torch.Tensor, dim: int) -> ScoreTensor:
        return scores.mean(dim)
class MaxAggregationFunction(AggregationFunction):
    aggregation_function_name = "max"
    def __call__(self, scores: torch.Tensor, dim: int) -> ScoreTensor:
        return scores.max(dim).values
class MinAggregationFunction(AggregationFunction):
    aggregation_function_name = "min"
    def __call__(self, scores: torch.Tensor, dim: int) -> ScoreTensor:
        return scores.min(dim).values
class SumAggregationFunction(AggregationFunction):
    aggregation_function_name = "sum"
    def __call__(self, scores: torch.Tensor, dim: int) -> ScoreTensor:
        return scores.sum(dim)
class ProdAggregationFunction(AggregationFunction):
    aggregation_function_name = "prod"
    def __call__(self, scores: torch.Tensor, dim: int) -> ScoreTensor:
        return scores.prod(dim)
class AbsMaxAggregationFunction(AggregationFunction):
    aggregation_function_name = "absmax"
    def __call__(self, scores: torch.Tensor, dim: int) -> ScoreTensor:
        return scores.gather(dim, torch.nan_to_num(scores).abs().argmax(dim, keepdim=True)).squeeze(dim)
class VectorNormAggregationFunction(AggregationFunction):
    aggregation_function_name = "vnorm"
    def __call__(self, scores: torch.Tensor, dim: int, vnorm_ord: int = 2) -> ScoreTensor:
        return vector_norm(scores, ord=vnorm_ord, dim=dim)
DEFAULT_ATTRIBUTION_AGGREGATE_DICT = {
    "source_attributions": {"spans": "absmax"},
    "target_attributions": {"spans": "absmax"},
    "step_scores": {
        "spans": {
            "probability": "prod",
            "entropy": "sum",
            "crossentropy": "sum",
            "perplexity": "prod",
            "contrast_prob_diff": "prod",
            "contrast_prob": "prod",
            "pcxmi": "sum",
            "kl_divergence": "sum",
            "mc_dropout_prob_avg": "prod",
        }
    },
}
def list_aggregation_functions() -> list[str]:
    """Lists identifiers for all available aggregation functions."""
    return available_classes(AggregationFunction)

================
File: inseq/data/aggregator.py
================
import logging
import re
from abc import ABC, abstractmethod
from collections.abc import Callable, Sequence
from enum import Enum
from typing import TYPE_CHECKING, Literal, TypeVar
import torch
from ..utils import (
    Registry,
    aggregate_contiguous,
    aggregate_token_pair,
    aggregate_token_sequence,
    available_classes,
    extract_signature_args,
    validate_indices,
)
from ..utils import normalize as normalize_fn
from ..utils import rescale as rescale_fn
from ..utils.typing import IndexSpan, OneOrMoreIndices, TokenWithId
from .aggregation_functions import AggregationFunction
from .data_utils import TensorWrapper
if TYPE_CHECKING:
    from .attribution import FeatureAttributionSequenceOutput
logger = logging.getLogger(__name__)
AggregableMixinClass = TypeVar("AggregableMixinClass", bound="AggregableMixin")
class DictWithDefault(dict):
    """Used to pass specific values to field-specific calls of the aggregate function in Aggregator.
    DictWithDefault dictionary objects won't be passed as a whole to all field-specific functions called by
    Aggregator.aggregate, and instead only the values with the name of the corresponding field will be used.
    When these are missing, the default field of DictWithDefault will be used as fallback.
    """
    @staticmethod
    def _get_fn(name: str) -> Callable:
        if name not in available_classes(AggregationFunction):
            raise ValueError(
                f"Unknown aggregation function {name}. Choose from {','.join(available_classes(AggregationFunction))}."
            )
        return AggregationFunction.available_classes()[name]()
    def __init__(self, default: str | Callable, **kwargs):
        super().__init__(**kwargs)
        self.default = self._get_fn(default) if isinstance(default, str) else default
    def __getitem__(self, key):
        try:
            value = super().__getitem__(key)
            if isinstance(value, str):
                return self._get_fn(value)
            elif isinstance(value, dict):
                return DictWithDefault(self.default, **value)
            return value
        except KeyError:
            return self.default
class Aggregator(Registry):
    registry_attr = "aggregator_name"
    @classmethod
    def start_aggregation_hook(cls, tensors: TensorWrapper, **kwargs):
        """Hook called at the start of the aggregation process.
        Use to ensure a prerequisite that is independent of previous
        aggregation steps and fundamental to the aggregation process
        (e.g. parameters are of the correct type). Will avoid performing aggregation steps before
        returning an error.
        """
        pass
    @classmethod
    def pre_aggregate_hook(cls, tensors: TensorWrapper, **kwargs):
        """Hook called right before the aggregation function is called.
        Use to ensure a prerequisite that is functional of previous
        aggregation steps and fundamental to the aggregation process
        (e.g. the aggregatable object produced by the previous step has correct shapes).
        """
        pass
    @classmethod
    @abstractmethod
    def _aggregate(cls, tensors: TensorWrapper, **kwargs):
        pass
    @classmethod
    def aggregate(
        cls,
        tensors: AggregableMixinClass,
        do_pre_aggregation_checks: bool = True,
        do_post_aggregation_checks: bool = True,
        **kwargs,
    ) -> AggregableMixinClass:
        if do_pre_aggregation_checks:
            cls.start_aggregation_hook(tensors, **kwargs)
        cls.pre_aggregate_hook(tensors, **kwargs)
        aggregated = cls._aggregate(tensors, **kwargs)
        cls.post_aggregate_hook(aggregated, **kwargs)
        if do_post_aggregation_checks:
            cls.end_aggregation_hook(aggregated, **kwargs)
        return aggregated
    @classmethod
    def post_aggregate_hook(cls, tensors: TensorWrapper, **kwargs):
        """Hook called right after the aggregation function is called.
        Verifies that the aggregated object has the correct properties.
        """
        pass
    @classmethod
    def end_aggregation_hook(cls, tensors: TensorWrapper, **kwargs):
        """Hook called at the end of the aggregation process.
        Use to ensure that the final product of aggregation is compliant with
        the requirements of individual aggregators.
        """
        pass
def _get_aggregators_from_id(
    aggregator: str,
    aggregate_fn: str | None = None,
) -> tuple[type[Aggregator], AggregationFunction | None]:
    if aggregator in available_classes(Aggregator):
        aggregator = Aggregator.available_classes()[aggregator]
    elif aggregator in available_classes(AggregationFunction):
        if aggregate_fn is not None:
            raise ValueError(
                "If aggregator is a string identifying an aggregation function, aggregate_fn should not be provided."
            )
        aggregate_fn = aggregator
        aggregator = SequenceAttributionAggregator
    else:
        raise ValueError(
            f"Unknown aggregator {aggregator}. Choose from {', '.join(available_classes(Aggregator))}.\n"
            f"Alternatively, choose from the aggregate_fn options {', '.join(available_classes(AggregationFunction))} "
            "for scores aggregation with the chosen function."
        )
    if aggregate_fn is None:
        return aggregator, aggregate_fn
    if aggregate_fn not in available_classes(AggregationFunction):
        raise ValueError(
            f"Unknown aggregation function {aggregate_fn}. "
            f"Choose from {', '.join(available_classes(AggregationFunction))}"
        )
    aggregate_fn = AggregationFunction.available_classes()[aggregate_fn]()
    return aggregator, aggregate_fn
class AggregatorPipeline:
    def __init__(
        self,
        aggregators: list[str | type[Aggregator]],
        aggregate_fn: list[str | Callable] | None = None,
    ):
        self.aggregators: list[type[Aggregator]] = []
        self.aggregate_fn: list[Callable] = []
        if aggregate_fn is not None:
            if len(aggregate_fn) != len(aggregators):
                raise ValueError(
                    "If custom aggregate_fn are provided, their number should match the number of aggregators."
                )
        for idx in range(len(aggregators)):
            curr_aggregator = aggregators[idx]
            curr_aggregate_fn = aggregate_fn[idx] if aggregate_fn is not None else None
            if isinstance(curr_aggregator, str):
                curr_aggregator, curr_aggregate_fn = _get_aggregators_from_id(curr_aggregator, curr_aggregate_fn)
            self.aggregators.append(curr_aggregator)
            self.aggregate_fn.append(curr_aggregate_fn)
    def aggregate(
        self,
        tensors: AggregableMixinClass,
        do_pre_aggregation_checks: bool = True,
        do_post_aggregation_checks: bool = True,
        **kwargs,
    ) -> AggregableMixinClass:
        if do_pre_aggregation_checks:
            for aggregator in self.aggregators:
                aggregator.start_aggregation_hook(tensors, **kwargs)
        for aggregator, aggregate_fn in zip(self.aggregators, self.aggregate_fn, strict=False):
            curr_aggregation_kwargs = kwargs.copy()
            if aggregate_fn is not None:
                curr_aggregation_kwargs["aggregate_fn"] = aggregate_fn
            tensors = aggregator.aggregate(
                tensors, do_pre_aggregation_checks=False, do_post_aggregation_checks=False, **curr_aggregation_kwargs
            )
        if do_post_aggregation_checks:
            for aggregator in self.aggregators:
                aggregator.end_aggregation_hook(tensors, **kwargs)
        return tensors
AggregatorInput = AggregatorPipeline | type[Aggregator] | str | Sequence[str | type[Aggregator]] | None
def list_aggregators() -> list[str]:
    """Lists identifiers for all available aggregators."""
    return available_classes(Aggregator)
class AggregableMixin(ABC):
    _aggregator: AggregatorPipeline | type[Aggregator]
    def aggregate(
        self: AggregableMixinClass,
        aggregator: AggregatorInput = None,
        aggregate_fn: str | Sequence[str] | None = None,
        do_pre_aggregation_checks: bool = True,
        do_post_aggregation_checks: bool = True,
        **kwargs,
    ) -> AggregableMixinClass:
        """Aggregate outputs using the default or provided aggregator.
        Args:
            aggregator (:obj:`AggregatorPipeline` or :obj:`Type[Aggregator]` or :obj:`str` or , optional): Aggregator
                pipeline to use. If not provided, the default aggregator pipeline is used.
        Returns:
            :obj:`AggregableMixin`: The aggregated output class.
        """
        if aggregator is None:
            aggregator = self._aggregator
        if isinstance(aggregator, str):
            if isinstance(aggregate_fn, list | tuple):
                raise ValueError(
                    "If a single aggregator is used, aggregate_fn should also be a string identifier for the "
                    "corresponding aggregation function if defined."
                )
            aggregator, aggregate_fn = _get_aggregators_from_id(aggregator, aggregate_fn)
            if aggregate_fn is not None:
                kwargs["aggregate_fn"] = aggregate_fn
        elif isinstance(aggregator, list | tuple):
            if all(isinstance(a, str | type) for a in aggregator):
                aggregator = AggregatorPipeline(aggregator, aggregate_fn)
            elif all(isinstance(agg, tuple) for agg in aggregator):
                if all(isinstance(idx, str | type) for agg in aggregator for idx in agg):
                    aggregator = AggregatorPipeline([a[0] for a in aggregator], [a[1] for a in aggregator])
            else:
                raise ValueError(
                    "If aggregator is a sequence, it should contain either strings/classes identifying aggregators"
                    "or tuples of pairs of strings/classes identifying aggregators and aggregate functions."
                )
        return aggregator.aggregate(
            self,
            do_pre_aggregation_checks=do_pre_aggregation_checks,
            do_post_aggregation_checks=do_post_aggregation_checks,
            **kwargs,
        )
    @abstractmethod
    def __post_init__(self):
        pass
class SequenceAttributionAggregator(Aggregator):
    """Aggregates sequence attributions using a custom function. By default, the mean function is used.
    Enables aggregation for the FeatureAttributionSequenceOutput class using an aggregation function of choice.
    Args:
        attr (:class:`~inseq.data.FeatureAttributionSequenceOutput`): The attribution object to aggregate.
        aggregate_fn (:obj:`Callable`, optional): Function used to aggregate sequence attributions.
            Defaults to summing over the last dimension and renormalizing by the norm of the
            source(+target) attributions for granular attributions, no aggregation for token-level
            attributions.
    """
    aggregator_name = "scores"
    aggregator_family = "scores"
    default_fn = "mean"
    @classmethod
    def _aggregate(
        cls, attr: "FeatureAttributionSequenceOutput", aggregate_fn: str | Callable | None = None, **kwargs
    ) -> "FeatureAttributionSequenceOutput":
        if aggregate_fn is None and isinstance(attr._dict_aggregate_fn, dict):
            aggregate_fn = DictWithDefault(default=cls.default_fn, **attr._dict_aggregate_fn)
        elif aggregate_fn is not None:
            aggregate_fn = DictWithDefault(default=aggregate_fn)
        # Dispatch kwargs to the corresponding field-specific functions.
        # E.g. aggregate_source_attributions will take care of the source_attributions field.
        aggregated_sequence_attribution_fields = {}
        for field in attr.to_dict().keys():
            if aggregate_fn is not None:
                kwargs["aggregate_fn"] = aggregate_fn[field]
                # If the subclass is a dict, then we assume its fields represent variants depending on the aggregator
                # family that is being used (see e.g. step_scores in DEFAULT_ATTRIBUTION_AGGREGATE_DICT)
                if isinstance(kwargs["aggregate_fn"], dict):
                    kwargs["aggregate_fn"] = kwargs["aggregate_fn"][cls.aggregator_family]
            field_func = getattr(cls, f"aggregate_{field}")
            aggregated_sequence_attribution_fields[field] = field_func(attr, **kwargs)
        return attr.__class__(**aggregated_sequence_attribution_fields, **attr.config)
    @classmethod
    def _process_attribution_scores(
        cls,
        attr: "FeatureAttributionSequenceOutput",
        aggregate_fn: AggregationFunction,
        select_idx: OneOrMoreIndices | None = None,
        normalize: bool | None = None,
        rescale: bool | None = None,
        **kwargs,
    ):
        if normalize and rescale:
            raise ValueError("Only one of normalize and rescale can be set to True.")
        if normalize is None:
            normalize = rescale is None or not rescale
        fn_kwargs = extract_signature_args(kwargs, aggregate_fn)
        # If select_idx is a single int, no aggregation is performed
        do_aggregate = not isinstance(select_idx, int)
        has_source = attr.source_attributions is not None
        has_target = attr.target_attributions is not None
        src_scores = None
        if has_source:
            src_scores = cls._filter_scores(attr.source_attributions, dim=-1, indices=select_idx)
        tgt_scores = None
        if has_target:
            tgt_scores = cls._filter_scores(attr.target_attributions, dim=-1, indices=select_idx)
        if has_source and has_target:
            scores = (src_scores, tgt_scores)
        else:
            scores = src_scores if src_scores is not None else tgt_scores
        if aggregate_fn.takes_sequence_scores:
            fn_kwargs["sequence_scores"] = attr.sequence_scores
        if do_aggregate:
            scores = cls._aggregate_scores(scores, aggregate_fn, dim=-1, **fn_kwargs)
        if normalize:
            scores = normalize_fn(scores)
        if rescale:
            scores = rescale_fn(scores)
        return scores
    @classmethod
    def post_aggregate_hook(cls, attr: "FeatureAttributionSequenceOutput", **kwargs):
        super().post_aggregate_hook(attr, **kwargs)
        if attr.source_attributions is not None:
            attr._num_dimensions = attr.source_attributions.ndim
        elif attr.target_attributions is not None:
            attr._num_dimensions = attr.target_attributions.ndim
        else:
            attr._num_dimensions = 0
        cls.is_compatible(attr)
    @classmethod
    def end_aggregation_hook(cls, attr: "FeatureAttributionSequenceOutput", **kwargs):
        super().end_aggregation_hook(attr, **kwargs)
        # Needed to ensure the attribution can be visualized
        try:
            if attr.source_attributions is not None:
                assert attr.source_attributions.ndim == 2, attr.source_attributions.shape
            if attr.target_attributions is not None:
                assert attr.target_attributions.ndim == 2, attr.target_attributions.shape
        except AssertionError as e:
            raise RuntimeError(
                f"The aggregated attributions should be 2-dimensional to be visualized.\nFound dimensions: {e.args[0]}"
                "\n\nIf you're performing intermediate aggregation and don't aim to visualize the output right away, "
                "use do_post_aggregation_checks=False in the aggregate method to bypass this check."
            ) from e
    @staticmethod
    def aggregate_source(attr: "FeatureAttributionSequenceOutput", **kwargs):
        return attr.source
    @staticmethod
    def aggregate_target(attr: "FeatureAttributionSequenceOutput", **kwargs):
        return attr.target
    @classmethod
    def aggregate_source_attributions(
        cls,
        attr: "FeatureAttributionSequenceOutput",
        aggregate_fn: AggregationFunction,
        select_idx: OneOrMoreIndices | None = None,
        normalize: bool = True,
        rescale: bool = False,
        **kwargs,
    ):
        if attr.source_attributions is None:
            return attr.source_attributions
        scores = cls._process_attribution_scores(attr, aggregate_fn, select_idx, normalize, rescale, **kwargs)
        return scores[0] if attr.target_attributions is not None else scores
    @classmethod
    def aggregate_target_attributions(
        cls,
        attr: "FeatureAttributionSequenceOutput",
        aggregate_fn: AggregationFunction,
        select_idx: OneOrMoreIndices | None = None,
        normalize: bool = True,
        rescale: bool = False,
        **kwargs,
    ):
        if attr.target_attributions is None:
            return attr.target_attributions
        scores = cls._process_attribution_scores(attr, aggregate_fn, select_idx, normalize, rescale, **kwargs)
        return scores[1] if attr.source_attributions is not None else scores
    @staticmethod
    def aggregate_step_scores(attr: "FeatureAttributionSequenceOutput", **kwargs):
        return attr.step_scores
    @classmethod
    def aggregate_sequence_scores(
        cls,
        attr: "FeatureAttributionSequenceOutput",
        aggregate_fn: AggregationFunction,
        select_idx: OneOrMoreIndices | None = None,
        **kwargs,
    ):
        if aggregate_fn.takes_sequence_scores:
            return attr.sequence_scores
        fn_kwargs = extract_signature_args(kwargs, aggregate_fn)
        new_sequence_scores = {}
        for scores_id, seq_scores in attr.sequence_scores.items():
            filtered_scores = cls._filter_scores(seq_scores, dim=-1, indices=select_idx)
            if not isinstance(select_idx, int):
                filtered_scores = cls._aggregate_scores(filtered_scores, aggregate_fn, dim=-1, **fn_kwargs)
            new_sequence_scores[scores_id] = filtered_scores
        return new_sequence_scores
    @staticmethod
    def aggregate_attr_pos_start(attr: "FeatureAttributionSequenceOutput", **kwargs):
        return attr.attr_pos_start
    @staticmethod
    def aggregate_attr_pos_end(attr: "FeatureAttributionSequenceOutput", **kwargs):
        return attr.attr_pos_end
    @staticmethod
    def is_compatible(attr: "FeatureAttributionSequenceOutput"):
        from .attribution import FeatureAttributionSequenceOutput
        assert isinstance(attr, FeatureAttributionSequenceOutput)
        if attr.source_attributions is not None:
            assert attr.source_attributions.shape[0] == len(attr.source)
            assert attr.source_attributions.shape[1] == attr.attr_pos_end - attr.attr_pos_start
        if attr.target_attributions is not None:
            assert attr.target_attributions.shape[0] == min(len(attr.target), attr.attr_pos_end)
            assert attr.target_attributions.shape[1] == attr.attr_pos_end - attr.attr_pos_start
        if attr.step_scores is not None:
            for step_score in attr.step_scores.values():
                assert len(step_score) == attr.attr_pos_end - attr.attr_pos_start
    @staticmethod
    def _filter_scores(
        scores: torch.Tensor,
        dim: int = -1,
        indices: OneOrMoreIndices | None = None,
    ) -> torch.Tensor:
        indexed = scores.index_select(dim, validate_indices(scores, dim, indices).to(scores.device))
        if isinstance(indices, int):
            return indexed.squeeze(dim)
        return indexed
    @staticmethod
    def _aggregate_scores(
        scores: torch.Tensor | tuple[torch.Tensor, ...],
        aggregate_fn: AggregationFunction,
        dim: int = -1,
        **kwargs,
    ) -> torch.Tensor | tuple[torch.Tensor, ...]:
        if isinstance(scores, tuple) and aggregate_fn.takes_single_tensor:
            return tuple(aggregate_fn(score, dim=dim, **kwargs) for score in scores)
        return aggregate_fn(scores, dim=dim, **kwargs)
class ContiguousSpanAggregator(SequenceAttributionAggregator):
    """Reduces sequence attributions across one or more contiguous spans.
    Args:
        attr (:class:`~inseq.data.FeatureAttributionSequenceOutput`): The attribution object to aggregate.
        aggregate_fn (:obj:`Callable`, optional): Function used to aggregate sequence attributions.
            Defaults to the highest absolute value score across the aggregated span, with original sign
            preserved (e.g. [0.3, -0.7, 0.1] -> -0.7).
        source_spans (tuple of [int, int] or sequence of tuples of [int, int], optional): Spans to aggregate
            over for the source sequence. Defaults to no aggregation performed.
        target_spans (tuple of [int, int] or sequence of tuples of [int, int], optional): Spans to aggregate
            over for the target sequence. Defaults to no aggregation performed.
    """
    aggregator_name = "spans"
    aggregator_family = "spans"
    default_fn = "absmax"
    @classmethod
    def start_aggregation_hook(
        cls,
        attr: "FeatureAttributionSequenceOutput",
        source_spans: IndexSpan | None = None,
        target_spans: IndexSpan | None = None,
        **kwargs,
    ):
        super().start_aggregation_hook(attr, **kwargs)
        cls.validate_spans(attr.source, source_spans)
        cls.validate_spans(attr.target, target_spans)
    @classmethod
    def end_aggregation_hook(cls, attr: "FeatureAttributionSequenceOutput", **kwargs):
        pass
    @classmethod
    def aggregate(
        cls,
        attr: "FeatureAttributionSequenceOutput",
        source_spans: IndexSpan | None = None,
        target_spans: IndexSpan | None = None,
        **kwargs,
    ):
        """Spans can be:
        1. A list of the form [pos_start, pos_end] including the contiguous positions of tokens that
            are to be aggregated, if all values are integers and len(span) < len(original_seq)
        2. A list of the form [(pos_start_0, pos_end_0), (pos_start_1, pos_end_1)], same as above but
            for multiple contiguous spans.
        """
        source_spans = cls.format_spans(source_spans)
        target_spans = cls.format_spans(target_spans)
        return super().aggregate(attr, source_spans=source_spans, target_spans=target_spans, **kwargs)
    @staticmethod
    def format_spans(spans) -> list[tuple[int, int]]:
        if not spans:
            return spans
        return [spans] if isinstance(spans[0], int) else spans
    @classmethod
    def validate_spans(cls, span_sequence: list[TokenWithId], spans: IndexSpan | None = None):
        if not spans:
            return
        allmatch = lambda l, type: all(isinstance(x, type) for x in l)
        assert allmatch(spans, int) or allmatch(
            spans, tuple
        ), f"All items must be either indices (int) or spans (tuple), got {spans}"
        spans = cls.format_spans(spans)
        prev_span_max = -1
        for span in spans:
            assert len(span) == 2, f"Spans must contain two indexes, got {spans}"
            assert span[1] >= span[0] + 1, f"Spans must be non-empty, got {spans}"
            assert (
                span[0] >= prev_span_max
            ), f"Spans must be postive-valued, non-overlapping and in ascending order, got {spans}"
            assert span[1] <= len(span_sequence), f"Span values must be indexes of the original span, got {spans}"
            prev_span_max = span[1]
    @staticmethod
    def _aggregate_sequential_scores(scores, x_spans, y_spans, aggregate_fn):
        # First aggregate alongside the y-axis
        scores_aggregated_y = aggregate_contiguous(scores, y_spans, aggregate_fn, aggregate_dim=1)
        # Then aggregate alonside the x-axis
        scores_aggregated_x = aggregate_contiguous(scores_aggregated_y, x_spans, aggregate_fn, aggregate_dim=0)
        return scores_aggregated_x
    @staticmethod
    def _relativize_target_spans(spans: list[tuple[int, int]], start: int):
        if start != 0 and spans:
            # Remove target spans referring to the unattributed prefix, rescale remaining spans to relative idxs
            # of the generated sequences and set 0 if the span starts before the generation begins.
            spans = [(s[0] - start if s[0] > start else 0, s[1] - start) for s in spans if s[1] > start]
        return spans
    @staticmethod
    def aggregate_source(attr, source_spans, **kwargs):
        return aggregate_token_sequence(attr.source, source_spans)
    @staticmethod
    def aggregate_target(attr, target_spans, **kwargs):
        return aggregate_token_sequence(attr.target, target_spans)
    @staticmethod
    def aggregate_source_attributions(attr, source_spans, target_spans, aggregate_fn, **kwargs):
        if attr.source_attributions is None:
            return attr.source_attributions
        # Handle the case in which generation starts from a prefix
        target_spans = ContiguousSpanAggregator._relativize_target_spans(target_spans, attr.attr_pos_start)
        # First aggregate along generated target sequence, then along attributed source
        return ContiguousSpanAggregator._aggregate_sequential_scores(
            attr.source_attributions, source_spans, target_spans, aggregate_fn
        )
    @staticmethod
    def aggregate_target_attributions(attr, target_spans, aggregate_fn, **kwargs):
        if attr.target_attributions is None:
            return attr.target_attributions
        # Handle the case in which generation starts from a prefix
        gen_spans = ContiguousSpanAggregator._relativize_target_spans(target_spans, attr.attr_pos_start)
        # First aggregate along generated target sequence, then along attributed prefix
        return ContiguousSpanAggregator._aggregate_sequential_scores(
            attr.target_attributions, target_spans, gen_spans, aggregate_fn
        )
    @staticmethod
    def aggregate_step_scores(attr, target_spans, aggregate_fn, **kwargs):
        if not attr.step_scores:
            return attr.step_scores
        out_dict = {}
        # Handle the case in which generation starts from a prefix
        target_spans = ContiguousSpanAggregator._relativize_target_spans(target_spans, attr.attr_pos_start)
        for name, step_scores in attr.step_scores.items():
            agg_fn = aggregate_fn[name] if isinstance(aggregate_fn, dict) else aggregate_fn
            out_dict[name] = aggregate_contiguous(step_scores, target_spans, agg_fn, aggregate_dim=0)
        return out_dict
    @staticmethod
    def aggregate_sequence_scores(attr, source_spans, target_spans, aggregate_fn, **kwargs):
        # Assume sequence scores are shaped like source attributions
        if not attr.sequence_scores:
            return attr.sequence_scores
        out_dict = {}
        # Handle the case in which generation starts from a prefix
        target_spans = ContiguousSpanAggregator._relativize_target_spans(target_spans, attr.attr_pos_start)
        for name, step_scores in attr.sequence_scores.items():
            aggregate_fn = aggregate_fn[name] if isinstance(aggregate_fn, dict) else aggregate_fn
            if name.startswith("decoder"):
                out_dict[name] = ContiguousSpanAggregator._aggregate_sequential_scores(
                    step_scores, target_spans, target_spans, aggregate_fn
                )
            elif name.startswith("encoder"):
                out_dict[name] = ContiguousSpanAggregator._aggregate_sequential_scores(
                    step_scores, source_spans, source_spans, aggregate_fn
                )
            else:
                out_dict[name] = ContiguousSpanAggregator._aggregate_sequential_scores(
                    step_scores, source_spans, target_spans, aggregate_fn
                )
        return out_dict
    @staticmethod
    def aggregate_attr_pos_start(attr, target_spans, **kwargs):
        if not target_spans:
            return attr.attr_pos_start
        tot_merged_prefix = sum([s[1] - s[0] - 1 for s in target_spans if s[1] <= attr.attr_pos_start])
        new_pos_start = attr.attr_pos_start - tot_merged_prefix
        # Handle the case in which tokens before and after the starting position are merged
        # The resulting merged span will include the full merged token, but merged scores will reflect only the portion
        # that was actually attributed. E.g. if "Hello world" if the prefix, ", how are you?" is the generation and the
        # token "world," is formed during merging, the "world," token will be included in the attributed targets, but
        # only scores of "," will be used for aggregation (i.e. no aggregation since it's a single token).
        overlapping = [s for s in target_spans if s[0] < attr.attr_pos_start < s[1]]
        if overlapping and len(overlapping) == 1:
            new_pos_start -= attr.attr_pos_start - overlapping[0][0]
        elif len(overlapping) > 1:
            raise RuntimeError(f"Multiple overlapping spans detected for the starting position {attr.attr_pos_start}.")
        return new_pos_start
    @staticmethod
    def aggregate_attr_pos_end(attr, target_spans, **kwargs):
        if not target_spans:
            return attr.attr_pos_end
        new_start = ContiguousSpanAggregator.aggregate_attr_pos_start(attr, target_spans, **kwargs)
        target_spans = ContiguousSpanAggregator._relativize_target_spans(target_spans, attr.attr_pos_start)
        tot_merged_sequence = sum([s[1] - s[0] - 1 for s in target_spans])
        return new_start + ((attr.attr_pos_end - attr.attr_pos_start) - tot_merged_sequence)
class SubwordAggregator(ContiguousSpanAggregator):
    """Aggregates over subwords by automatic detecting contiguous subword spans.
    Args:
        attr (:class:`~inseq.data.FeatureAttributionSequenceOutput`): The attribution object to aggregate.
        aggregate_fn (:obj:`Callable`, optional): Function to aggregate over the subwords.
            Defaults to the highest absolute value score across the aggregated span, with original sign
            preserved (e.g. [0.3, -0.7, 0.1] -> -0.7).
        aggregate_source (bool, optional): Whether to aggregate over the source sequence. Defaults to True.
        aggregate_target (bool, optional): Whether to aggregate over the target sequence. Defaults to True.
        special_chars (str or tuple of str, optional): One or more characters used to identify subword boundaries.
            Defaults to 'â–', used by SentencePiece. If is_suffix_symbol=True, then this symbol is used to identify
            parts to be aggregated (e.g. # in WordPiece, ['phen', '##omen', '##al']). Otherwise, it identifies the
            roots that should be preserved (e.g. â– in SentencePiece, ['â–phen', 'omen', 'al']).
        is_suffix_symbol (bool, optional): Whether the special symbol is used to identify suffixes or prefixes.
            Defaults to False.
    """
    aggregator_name = "subwords"
    @classmethod
    def aggregate(
        cls,
        attr: "FeatureAttributionSequenceOutput",
        aggregate_source: bool = True,
        aggregate_target: bool = True,
        special_chars: str | tuple[str, ...] = "â–",
        is_suffix_symbol: bool = False,
        **kwargs,
    ):
        source_spans = []
        target_spans = []
        if aggregate_source:
            source_spans = cls.get_spans(attr.source, special_chars, is_suffix_symbol)
        if aggregate_target:
            target_spans = cls.get_spans(attr.target, special_chars, is_suffix_symbol)
        return super().aggregate(attr, source_spans=source_spans, target_spans=target_spans, **kwargs)
    @staticmethod
    def get_spans(tokens: list[TokenWithId], special_chars: str | tuple[str, ...], is_suffix_symbol: bool):
        spans = []
        last_prefix_idx = 0
        has_special_chars = any(sym in token.token for token in tokens for sym in special_chars)
        if not has_special_chars:
            logger.warning(
                f"The {special_chars} character is currently used for subword aggregation, but no instances "
                "have been detected in the sequence. Change the special symbols using e.g. special_chars=('Ä ', 'ÄŠ')"
                ", and set is_suffix_symbol=True if they are used as suffix word separators (e.g. Hello</w> world</w>)"
            )
            return spans
        for curr_idx, token in enumerate(tokens):
            # Suffix if token start with special suffix symbol, or if it doesn't have the special prefix symbol.
            is_suffix = token.token.startswith(special_chars) == is_suffix_symbol
            if is_suffix:
                if curr_idx == len(tokens) - 1 and curr_idx - last_prefix_idx > 1:
                    spans.append((last_prefix_idx, curr_idx))
                continue
            if curr_idx - last_prefix_idx > 1:
                spans.append((last_prefix_idx, curr_idx))
            last_prefix_idx = curr_idx
        return spans
class StringSplitAggregator(ContiguousSpanAggregator):
    """Aggregates contiguous tokens using specified strings as separators.
    Args:
        attr (:class:`~inseq.data.FeatureAttributionSequenceOutput`): The attribution object to aggregate.
        aggregate_fn (:obj:`Callable`, optional): Function to aggregate over the subwords.
            Defaults to the highest absolute value score across the aggregated span, with original sign
            preserved (e.g. [0.3, -0.7, 0.1] -> -0.7).
        aggregate_source (bool, optional): Whether to aggregate over the source sequence. Defaults to True.
        aggregate_target (bool, optional): Whether to aggregate over the target sequence. Defaults to True.
        split_pattern (str): Regular expression pattern used to split the sequences.
        split_mode (str, optional): Treatment for split tokens. If "single", these are kept separate from previous and
            following tokens. If "start", they are concatenated with following tokens. If "end", they are concatenated
            to previous tokens. Defaults to "single".
    """
    aggregator_name = "split"
    class SplitStrategy(Enum):
        SINGLE = "single"
        START = "start"
        END = "end"
    @classmethod
    def aggregate(
        cls,
        attr: "FeatureAttributionSequenceOutput",
        aggregate_source: bool = True,
        aggregate_target: bool = True,
        split_pattern: str = None,
        split_mode: Literal["single", "start", "end"] = SplitStrategy.SINGLE.value,
        **kwargs,
    ):
        source_spans = []
        target_spans = []
        if split_pattern is None:
            raise ValueError("split_pattern is None. Provide a valid regular expression pattern to split the string.")
        if aggregate_source:
            source_spans = cls.get_spans(attr.source, split_pattern, split_mode)
        if aggregate_target:
            target_spans = cls.get_spans(attr.target, split_pattern, split_mode)
        return super().aggregate(attr, source_spans=source_spans, target_spans=target_spans, **kwargs)
    @classmethod
    def get_spans(
        cls,
        tokens: list[TokenWithId],
        split_pattern: str,
        split_mode: Literal["single", "start", "end"] = SplitStrategy.SINGLE.value,
    ) -> list[tuple[int, int]]:
        full_text = "".join(t.token for t in tokens)
        curr_idx = 0
        token_spans = []
        # Generate token spans
        for tok in tokens:
            token_spans.append((curr_idx, curr_idx + len(tok.token)))
            curr_idx += len(tok.token)
        # Find all matches for the given pattern
        matches = list(re.finditer(split_pattern, full_text))
        if not matches:
            return []
        matches_spans = [(m.start(), m.end()) for m in matches]
        # Create matches_tokens list
        matches_tokens = []
        for start, end in matches_spans:
            token_start = next((i for i, (ts, te) in enumerate(token_spans) if ts <= start < te), None)
            token_end = next((i for i, (ts, te) in enumerate(token_spans) if ts < end <= te), None) + 1
            if token_start is not None and token_end is not None:
                matches_tokens.append((token_start, token_end))
        # Remove duplicate spans
        seen_tokens = set()
        matches_tokens = [m for m in matches_tokens if not (m in seen_tokens or seen_tokens.add(m))]
        # If overlapping token spans are found, split them
        non_overlapping_matches = []
        for curr_idx, (start, end) in enumerate(matches_tokens):
            curr_start, curr_end = start, end
            if len(matches_tokens) > curr_idx + 1 and end > matches_tokens[curr_idx + 1][0]:
                curr_end = matches_tokens[curr_idx + 1][0]
            if curr_idx > 0 and start < non_overlapping_matches[-1][1]:
                curr_start = non_overlapping_matches[-1][1]
            non_overlapping_matches.append((curr_start, curr_end))
            if curr_end != end and end < matches_tokens[curr_idx + 1][1]:
                non_overlapping_matches.append((curr_end, end))
        matches_tokens = non_overlapping_matches
        # Fill missing spans
        aggregate_spans = []
        matched_span = []
        if matches_tokens[0][0] != 0:
            aggregate_spans.append((0, matches_tokens[0][0]))
            matched_span.append(False)
        for i in range(len(matches_tokens) - 1):
            aggregate_spans.append(matches_tokens[i])
            matched_span.append(True)
            if matches_tokens[i][1] != matches_tokens[i + 1][0]:
                aggregate_spans.append((matches_tokens[i][1], matches_tokens[i + 1][0]))
                matched_span.append(False)
        aggregate_spans.append(matches_tokens[-1])
        matched_span.append(True)
        if matches_tokens[-1][1] != len(tokens):
            aggregate_spans.append((matches_tokens[-1][1], len(tokens)))
            matched_span.append(False)
        # Create aggregate spans based on the split strategy
        if split_mode == cls.SplitStrategy.SINGLE.value:
            return aggregate_spans
        elif split_mode in (cls.SplitStrategy.START.value, cls.SplitStrategy.END.value):
            merge_aggregate_spans = []
            curr_span_start = 0
            # If the strategy is "start", all match spans are concatenated to their following non-match spans
            # If the strategy is "end", all match spans are concatenated to their preceding non-match spans
            # Example:
            # aggregate_spans = [(0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 9)]
            # matched_span = [False, True, True, False, True, False]
            # Start strategy: [(0, 1), (1, 2), (2, 4), (4, 9)]
            # End strategy: [(0, 2), (2, 3), (3, 5), (5, 9)]
            for (start, end), is_match in zip(aggregate_spans, matched_span, strict=False):
                if is_match:
                    if split_mode == cls.SplitStrategy.START.value and start != curr_span_start:
                        merge_aggregate_spans.append((curr_span_start, start))
                        curr_span_start = start
                    elif split_mode == cls.SplitStrategy.END.value:
                        merge_aggregate_spans.append((curr_span_start, end))
                        curr_span_start = end
            if curr_span_start != aggregate_spans[-1][1]:
                merge_aggregate_spans.append((curr_span_start, aggregate_spans[-1][1]))
            return merge_aggregate_spans
        else:
            raise ValueError("Invalid split strategy: must be one of 'single', 'start', 'end'")
class PairAggregator(SequenceAttributionAggregator):
    """Aggregates two FeatureAttributionSequenceOutput object into a single one containing the diff.
    Args:
        attr (:class:`~inseq.data.FeatureAttributionSequenceOutput`): The starting attribution object.
        paired_attr (:class:`~inseq.data.FeatureAttributionSequenceOutput`): The attribution object with whom
            the diff is computed, representing a change from `attr_start` (e.g. minimal pair edit).
        aggregate_fn (:obj:`Callable`, optional): Function to aggregate elementwise values of the pair.
            Defaults to the difference between the two elements.
    """
    aggregator_name = "pair"
    aggregator_family = "pair"
    default_fn = lambda x, y: y - x
    @classmethod
    def pre_aggregate_hook(
        cls, attr: "FeatureAttributionSequenceOutput", paired_attr: "FeatureAttributionSequenceOutput", **kwargs
    ):
        super().pre_aggregate_hook(attr, **kwargs)
        cls.validate_pair(attr, paired_attr)
    @classmethod
    def validate_pair(cls, attr, paired_attr):
        assert len(attr.source) == len(paired_attr.source), "Source sequences must be the same length."
        assert len(attr.target) == len(paired_attr.target), "Target sequences must be the same length."
        if attr.source_attributions is not None:
            assert (
                attr.source_attributions.shape == paired_attr.source_attributions.shape
            ), "Source attributions must be the same shape."
        if attr.target_attributions is not None:
            assert (
                attr.target_attributions.shape == paired_attr.target_attributions.shape
            ), "Target attributions must be the same shape."
        if attr.step_scores is not None:
            assert paired_attr.step_scores is not None, "Paired attribution must have step scores."
            for key, value in attr.step_scores.items():
                assert key in paired_attr.step_scores, f"Step score {key} must be in paired attribution."
                assert value.shape == paired_attr.step_scores[key].shape, f"Step score {key} must be the same shape."
        if attr.sequence_scores is not None:
            assert paired_attr.sequence_scores is not None, "Paired attribution must have sequence scores."
            for key, value in attr.sequence_scores.items():
                assert key in paired_attr.sequence_scores, f"Sequence score {key} must be in paired attribution."
                assert (
                    value.shape == paired_attr.sequence_scores[key].shape
                ), f"Sequence score {key} must be the same shape."
    @staticmethod
    def aggregate_source(attr, paired_attr, **kwargs):
        return aggregate_token_pair(attr.source, paired_attr.source)
    @staticmethod
    def aggregate_target(attr, paired_attr, **kwargs):
        return aggregate_token_pair(attr.target, paired_attr.target)
    @staticmethod
    def aggregate_source_attributions(attr, paired_attr, aggregate_fn, **kwargs):
        if attr.source_attributions is None:
            return attr.source_attributions
        return aggregate_fn(attr.source_attributions, paired_attr.source_attributions)
    @staticmethod
    def aggregate_target_attributions(attr, paired_attr, aggregate_fn, **kwargs):
        if attr.target_attributions is None:
            return attr.target_attributions
        return aggregate_fn(attr.target_attributions, paired_attr.target_attributions)
    @staticmethod
    def aggregate_step_scores(attr, paired_attr, aggregate_fn, **kwargs):
        if not attr.step_scores:
            return attr.step_scores
        out_dict = {}
        for name, step_scores in attr.step_scores.items():
            agg_fn = aggregate_fn[name] if isinstance(aggregate_fn, dict) else aggregate_fn
            out_dict[name] = agg_fn(step_scores, paired_attr.step_scores[name])
        return out_dict
    @staticmethod
    def aggregate_sequence_scores(attr, paired_attr, aggregate_fn, **kwargs):
        if not attr.sequence_scores:
            return attr.sequence_scores
        out_dict = {}
        for name, sequence_scores in attr.sequence_scores.items():
            agg_fn = aggregate_fn[name] if isinstance(aggregate_fn, dict) else aggregate_fn
            out_dict[name] = agg_fn(sequence_scores, paired_attr.sequence_scores[name])
        return out_dict
class SliceAggregator(ContiguousSpanAggregator):
    """Slices the FeatureAttributionSequenceOutput object into a smaller one containing a subset of its elements.
    Args:
        attr (:class:`~inseq.data.FeatureAttributionSequenceOutput`): The starting attribution object.
        source_spans (tuple of [int, int] or sequence of tuples of [int, int], optional): Spans to slice for the
            source sequence. Defaults to no slicing performed.
        target_spans (tuple of [int, int] or sequence of tuples of [int, int], optional): Spans to slice for the
            target sequence. Defaults to no slicing performed.
    """
    aggregator_name = "slices"
    default_fn = None
    @classmethod
    def aggregate(
        cls,
        attr: "FeatureAttributionSequenceOutput",
        source_spans: IndexSpan | None = None,
        target_spans: IndexSpan | None = None,
        **kwargs,
    ):
        """Spans can be:
        1. A list of the form [pos_start, pos_end] including the contiguous positions of tokens that
            are to be aggregated, if all values are integers and len(span) < len(original_seq)
        2. A list of the form [(pos_start_0, pos_end_0), (pos_start_1, pos_end_1)], same as above but
            for multiple contiguous spans.
        """
        source_spans = cls.format_spans(source_spans)
        target_spans = cls.format_spans(target_spans)
        if attr.source_attributions is None:
            if source_spans is not None:
                logger.warn(
                    "Source spans are specified but no source scores are given for decoder-only models. "
                    "Ignoring source spans and using target spans instead."
                )
            source_spans = [(s[0], min(s[1], attr.attr_pos_start)) for s in target_spans]
        # Generated tokens are always included in the slices to preserve the output scores
        is_gen_added = False
        new_target_spans = []
        if target_spans is not None:
            for span in target_spans:
                if span[1] > attr.attr_pos_start and is_gen_added:
                    continue
                elif span[1] > attr.attr_pos_start and not is_gen_added:
                    new_target_spans.append((span[0], attr.attr_pos_end))
                    is_gen_added = True
                else:
                    new_target_spans.append(span)
        if not is_gen_added:
            new_target_spans.append((attr.attr_pos_start, attr.attr_pos_end))
        return super().aggregate(attr, source_spans=source_spans, target_spans=new_target_spans, **kwargs)
    @staticmethod
    def aggregate_source(attr: "FeatureAttributionSequenceOutput", source_spans: list[tuple[int, int]], **kwargs):
        sliced_source = []
        for span in source_spans:
            sliced_source.extend(attr.source[span[0] : span[1]])
        return sliced_source
    @staticmethod
    def aggregate_target(attr: "FeatureAttributionSequenceOutput", target_spans: list[tuple[int, int]], **kwargs):
        sliced_target = []
        for span in target_spans:
            sliced_target.extend(attr.target[span[0] : span[1]])
        return sliced_target
    @staticmethod
    def aggregate_source_attributions(attr: "FeatureAttributionSequenceOutput", source_spans, **kwargs):
        if attr.source_attributions is None:
            return attr.source_attributions
        return torch.cat(
            tuple(attr.source_attributions[span[0] : span[1], ...] for span in source_spans),
            dim=0,
        )
    @staticmethod
    def aggregate_target_attributions(attr: "FeatureAttributionSequenceOutput", target_spans, **kwargs):
        if attr.target_attributions is None:
            return attr.target_attributions
        return torch.cat(
            tuple(attr.target_attributions[span[0] : span[1], ...] for span in target_spans),
            dim=0,
        )
    @staticmethod
    def aggregate_step_scores(attr: "FeatureAttributionSequenceOutput", **kwargs):
        return attr.step_scores
    @classmethod
    def aggregate_sequence_scores(
        cls,
        attr: "FeatureAttributionSequenceOutput",
        source_spans,
        target_spans,
        **kwargs,
    ):
        if not attr.sequence_scores:
            return attr.sequence_scores
        out_dict = {}
        for name, step_scores in attr.sequence_scores.items():
            if name.startswith("decoder"):
                out_dict[name] = torch.cat(
                    tuple(step_scores[span[0] : span[1], ...] for span in target_spans),
                    dim=0,
                )
            elif name.startswith("encoder"):
                out_dict[name] = torch.cat(
                    tuple(step_scores[span[0] : span[1], span[0] : span[1], ...] for span in source_spans),
                    dim=0,
                )
            else:
                out_dict[name] = torch.cat(
                    tuple(step_scores[span[0] : span[1], ...] for span in source_spans),
                    dim=0,
                )
        return out_dict
    @staticmethod
    def aggregate_attr_pos_start(attr: "FeatureAttributionSequenceOutput", target_spans, **kwargs):
        if not target_spans:
            return attr.attr_pos_start
        tot_sliced_len = sum(min(s[1], attr.attr_pos_start) - s[0] for s in target_spans)
        return tot_sliced_len
    @staticmethod
    def aggregate_attr_pos_end(attr: "FeatureAttributionSequenceOutput", **kwargs):
        return attr.attr_pos_end

================
File: inseq/data/attribution.py
================
import base64
import logging
from collections.abc import Callable
from copy import deepcopy
from dataclasses import dataclass, field
from os import PathLike
from pathlib import Path
from typing import TYPE_CHECKING, Any
import torch
import treescope as ts
from ..utils import (
    convert_from_safetensor,
    convert_to_safetensor,
    drop_padding,
    get_sequences_from_batched_steps,
    json_advanced_dump,
    json_advanced_load,
    pad_with_nan,
    pretty_dict,
    remap_from_filtered,
)
from ..utils.typing import (
    MultipleScoresPerSequenceTensor,
    MultipleScoresPerStepTensor,
    OneOrMoreTokenWithIdSequences,
    ScorePrecision,
    SequenceAttributionTensor,
    SingleScorePerStepTensor,
    SingleScoresPerSequenceTensor,
    StepAttributionTensor,
    TargetIdsTensor,
    TextInput,
    TokenWithId,
)
from .aggregation_functions import DEFAULT_ATTRIBUTION_AGGREGATE_DICT
from .aggregator import AggregableMixin, Aggregator, AggregatorPipeline
from .batch import Batch, BatchEmbedding, BatchEncoding, DecoderOnlyBatch, EncoderDecoderBatch
from .data_utils import TensorWrapper
from .viz import get_saliency_heatmap_treescope, get_tokens_heatmap_treescope
if TYPE_CHECKING:
    from ..models import AttributionModel
FeatureAttributionInput = TextInput | BatchEncoding | Batch
logger = logging.getLogger(__name__)
DEFAULT_ATTRIBUTION_DIM_NAMES = {
    "source_attributions": {0: "Input Tokens", 1: "Generated Tokens"},
    "target_attributions": {0: "Input Tokens", 1: "Generated Tokens"},
}
def get_batch_from_inputs(
    attribution_model: "AttributionModel",
    inputs: FeatureAttributionInput,
    include_eos_baseline: bool = False,
    as_targets: bool = False,
    skip_special_tokens: bool = False,
) -> Batch:
    if isinstance(inputs, Batch):
        batch = inputs
    else:
        if isinstance(inputs, str | list):
            encodings: BatchEncoding = attribution_model.encode(
                inputs,
                as_targets=as_targets,
                return_baseline=True,
                include_eos_baseline=include_eos_baseline,
                add_special_tokens=not skip_special_tokens,
            )
        elif isinstance(inputs, BatchEncoding):
            encodings = inputs
        else:
            raise ValueError(
                f"Error: Found inputs of type {type(inputs)}. "
                "Inputs must be either a string, a list of strings, a BatchEncoding or a Batch."
            )
        embeddings = BatchEmbedding(
            input_embeds=attribution_model.embed(
                encodings.input_ids, as_targets=as_targets, add_special_tokens=not skip_special_tokens
            ),
            baseline_embeds=attribution_model.embed(
                encodings.baseline_ids, as_targets=as_targets, add_special_tokens=not skip_special_tokens
            ),
        )
        batch = Batch(encodings, embeddings)
    return batch
def merge_attributions(attributions: list["FeatureAttributionOutput"]) -> "FeatureAttributionOutput":
    """Merges multiple :class:`~inseq.data.FeatureAttributionOutput` objects into a single one.
    Merging is allowed only if the two outputs match on the fields specified in ``_merge_match_info_fields``.
    Args:
        attributions (:obj:`list` of :class:`~inseq.data.FeatureAttributionOutput`): The FeatureAttributionOutput
            objects to be merged.
    Returns:
        :class:`~inseq.data.FeatureAttributionOutput`: Merged object.
    """
    assert all(
        isinstance(x, FeatureAttributionOutput) for x in attributions
    ), "Only FeatureAttributionOutput objects can be merged."
    first = attributions[0]
    for match_field in FeatureAttributionOutput._merge_match_info_fields:
        assert all(
            (
                attr.info[match_field] == first.info[match_field]
                if match_field in first.info
                else match_field not in attr.info
            )
            for attr in attributions
        ), f"Cannot merge: incompatible values for field {match_field}"
    out_info = first.info.copy()
    if "attr_pos_end" in first.info:
        out_info.update({"attr_pos_end": max(attr.info["attr_pos_end"] for attr in attributions)})
    if "generated_texts" in first.info:
        out_info.update({"generated_texts": [text for attr in attributions for text in attr.info["generated_texts"]]})
    if "input_texts" in first.info:
        out_info.update({"input_texts": [text for attr in attributions for text in attr.info["input_texts"]]})
    return FeatureAttributionOutput(
        sequence_attributions=[seqattr for attr in attributions for seqattr in attr.sequence_attributions],
        step_attributions=(
            [stepattr for attr in attributions for stepattr in attr.step_attributions]
            if first.step_attributions is not None
            else None
        ),
        info=out_info,
    )
@dataclass(eq=False, repr=False)
class FeatureAttributionSequenceOutput(TensorWrapper, AggregableMixin):
    """Output produced by a standard attribution method.
    Attributes:
        source (list of :class:`~inseq.utils.typing.TokenWithId`): Tokenized source sequence.
        target (list of :class:`~inseq.utils.typing.TokenWithId`): Tokenized target sequence.
        source_attributions (:obj:`SequenceAttributionTensor`): Tensor of shape (`source_len`,
            `target_len`) plus an optional third dimension if the attribution is granular (e.g.
            gradient attribution) containing the attribution scores produced at each generation step of
            the target for every source token.
        target_attributions (:obj:`SequenceAttributionTensor`, optional): Tensor of shape
            (`target_len`, `target_len`), plus an optional third dimension if
            the attribution is granular containing the attribution scores produced at each generation
            step of the target for every token in the target prefix.
        step_scores (:obj:`dict[str, SingleScorePerStepTensor]`, optional): Dictionary of step scores
            produced alongside attributions (one per generation step).
        sequence_scores (:obj:`dict[str, MultipleScoresPerStepTensor]`, optional): Dictionary of sequence
            scores produced alongside attributions (n per generation step, as for attributions).
    """
    source: list[TokenWithId]
    target: list[TokenWithId]
    source_attributions: SequenceAttributionTensor | None = None
    target_attributions: SequenceAttributionTensor | None = None
    step_scores: dict[str, SingleScoresPerSequenceTensor] | None = None
    sequence_scores: dict[str, MultipleScoresPerSequenceTensor] | None = None
    attr_pos_start: int = 0
    attr_pos_end: int | None = None
    _aggregator: str | list[str] | None = None
    _dict_aggregate_fn: dict[str, str] | None = None
    _attribution_dim_names: dict[str, dict[int, str]] | None = None
    _num_dimensions: int | None = None
    def __post_init__(self):
        if self._dict_aggregate_fn is None:
            self._dict_aggregate_fn = {}
        default_aggregate_fn = DEFAULT_ATTRIBUTION_AGGREGATE_DICT
        default_aggregate_fn.update(self._dict_aggregate_fn)
        self._dict_aggregate_fn = default_aggregate_fn
        if self._attribution_dim_names is None:
            self._attribution_dim_names = {}
        default_dim_names = DEFAULT_ATTRIBUTION_DIM_NAMES
        default_dim_names.update(self._attribution_dim_names)
        self._attribution_dim_names = default_dim_names
        if self._aggregator is None:
            self._aggregator = "scores"
        if self._num_dimensions is None:
            self._num_dimensions = 0
        if self.attr_pos_end is None or self.attr_pos_end > len(self.target):
            self.attr_pos_end = len(self.target)
    def __getitem__(self, s: slice | int) -> "FeatureAttributionSequenceOutput":
        source_spans = None if self.source_attributions is None else (s.start, s.stop)
        target_spans = None if self.source_attributions is not None else (s.start, s.stop)
        return self.aggregate("slices", source_spans=source_spans, target_spans=target_spans)
    def __sub__(self, other: "FeatureAttributionSequenceOutput") -> "FeatureAttributionSequenceOutput":
        if not isinstance(other, self.__class__):
            raise ValueError(f"Cannot compare {type(other)} with {type(self)}")
        return self.aggregate("pair", paired_attr=other, do_post_aggregation_checks=False)
    def __treescope_repr__(
        self,
        path: str,
        subtree_renderer: Callable[[Any, str | None], ts.rendering_parts.Rendering],
    ) -> ts.rendering_parts.Rendering:
        def granular_attribution_visualizer(
            value: Any,
            path: tuple[Any, ...] | None,
        ):
            if isinstance(value, torch.Tensor):
                tname = path.split(".")[-1]
                column_labels = [t.token for t in self.target[self.attr_pos_start : self.attr_pos_end]]
                if tname == "source_attributions":
                    row_labels = [t.token for t in self.source]
                elif tname == "target_attributions":
                    row_labels = [t.token for t in self.target]
                elif tname.startswith("sequence_scores"):
                    tname = tname[17:].split("_")[0]
                    if tname.startswith("encoder"):
                        row_labels = [t.token for t in self.source]
                        column_labels = [t.token for t in self.source]
                    elif tname.startswith("decoder"):
                        row_labels = [t.token for t in self.target]
                        column_labels = [t.token for t in self.target]
                adapter = ts.type_registries.lookup_ndarray_adapter(value)
                if value.ndim >= 2:
                    return ts.IPythonVisualization(
                        ts.figures.inline(
                            adapter.get_array_summary(value, fast=False),
                            get_saliency_heatmap_treescope(
                                scores=value.numpy(),
                                column_labels=column_labels,
                                row_labels=row_labels,
                                dim_names=self._attribution_dim_names.get(tname, None),
                            ),
                        ),
                        replace=True,
                    )
                else:
                    return ts.IPythonVisualization(
                        ts.figures.inline(
                            adapter.get_array_summary(value, fast=False) + "\n\n",
                            ts.figures.figure_from_treescope_rendering_part(
                                ts.rendering_parts.indented_children(
                                    [
                                        get_tokens_heatmap_treescope(
                                            tokens=column_labels,
                                            scores=value.numpy(),
                                            max_val=value.max().item(),
                                        )
                                    ]
                                )
                            ),
                        ),
                        replace=True,
                    )
        with ts.active_autovisualizer.set_scoped(granular_attribution_visualizer):
            return ts.repr_lib.render_object_constructor(
                object_type=type(self),
                attributes=self.__dict__,
                path=path,
                subtree_renderer=subtree_renderer,
                roundtrippable=True,
            )
    def _convert_to_safetensors(self, scores_precision: ScorePrecision = "float32"):
        """
        Converts tensor attributes within the class to the specified precision.
        The conversion is based on the specified `scores_precision`.
        If the input tensor is already of the desired precision, no conversion occurs.
        For float8, the function performs scaling and converts to uint8, which can be later converted back to float16 upon reloading.
        Args:
            scores_precision (str, optional): Desired output data type precision. Defaults to "float32".
        Returns:
            self: The function modifies the class attributes in-place.
        """
        if self.source_attributions is not None:
            self.source_attributions = convert_to_safetensor(
                self.source_attributions.contiguous(), scores_precision=scores_precision
            )
        if self.target_attributions is not None:
            self.target_attributions = convert_to_safetensor(
                self.target_attributions.contiguous(), scores_precision=scores_precision
            )
        if self.step_scores is not None:
            self.step_scores = {
                k: convert_to_safetensor(v.contiguous(), scores_precision=scores_precision)
                for k, v in self.step_scores.items()
            }
        if self.sequence_scores is not None:
            self.sequence_scores = {
                k: convert_to_safetensor(v.contiguous(), scores_precision=scores_precision)
                for k, v in self.sequence_scores.items()
            }
        return self
    def _recover_from_safetensors(self):
        """
        Converts tensor attributes within the class from b64-encoded safetensors to torch tensors.`.
        """
        if self.source_attributions is not None:
            self.source_attributions = convert_from_safetensor(base64.b64decode(self.source_attributions))
        if self.target_attributions is not None:
            self.target_attributions = convert_from_safetensor(base64.b64decode(self.target_attributions))
        if self.step_scores is not None:
            self.step_scores = {k: convert_from_safetensor(base64.b64decode(v)) for k, v in self.step_scores.items()}
        if self.sequence_scores is not None:
            self.sequence_scores = {
                k: convert_from_safetensor(base64.b64decode(v)) for k, v in self.sequence_scores.items()
            }
        return self
    @property
    def config(self) -> dict[str, Any]:
        return {k: v for k, v in self.__dict__.items() if k.startswith("_")}
    @staticmethod
    def get_remove_pad_fn(attr: "FeatureAttributionStepOutput", name: str) -> Callable:
        if attr.source_attributions is None or name.startswith("decoder"):
            remove_pad_fn = lambda scores, _, targets, seq_id: scores[seq_id][
                : len(targets[seq_id]), : len(targets[seq_id]), ...
            ]
        elif name.startswith("encoder"):
            remove_pad_fn = lambda scores, sources, _, seq_id: scores[seq_id][
                : len(sources[seq_id]), : len(sources[seq_id]), ...
            ]
        else:  # default case: cross-attention
            remove_pad_fn = lambda scores, sources, targets, seq_id: scores[seq_id][
                : len(sources[seq_id]), : len(targets[seq_id]), ...
            ]
        return remove_pad_fn
    @classmethod
    def from_step_attributions(
        cls,
        attributions: list["FeatureAttributionStepOutput"],
        tokenized_target_sentences: list[list[TokenWithId]],
        pad_token: Any | None = None,
        attr_pos_end: int | None = None,
    ) -> list["FeatureAttributionSequenceOutput"]:
        """Converts a list of :class:`~inseq.data.attribution.FeatureAttributionStepOutput` objects containing multiple
        examples outputs per step into a list of :class:`~inseq.data.attribution.FeatureAttributionSequenceOutput` with
        every object containing all step outputs for an individual example.
        Raises:
            `ValueError`: If the number of sequences in the attributions is not the same for all input sequences.
        Returns:
            `List[FeatureAttributionSequenceOutput]`: List of
            :class:`~inseq.data.attribution.FeatureAttributionSequenceOutput` objects.
        """
        attr = attributions[0]
        num_sequences = len(attr.prefix)
        if not all(len(attr.prefix) == num_sequences for attr in attributions):
            raise ValueError("All the attributions must include the same number of sequences.")
        sources = []
        targets = []
        pos_start = []
        for seq_idx in range(num_sequences):
            if attr.source_attributions is not None:
                sources.append(drop_padding(attr.source[seq_idx], pad_token))
            curr_target = [a.target[seq_idx][0] for a in attributions]
            targets.append(drop_padding(curr_target, pad_token))
            if all(attr.prefix[seq_idx][0] == pad_token for seq_idx in range(num_sequences)):
                tokenized_target_sentences[seq_idx] = tokenized_target_sentences[seq_idx][:1] + drop_padding(
                    tokenized_target_sentences[seq_idx][1:], pad_token
                )
            else:
                tokenized_target_sentences[seq_idx] = drop_padding(tokenized_target_sentences[seq_idx], pad_token)
        if attr_pos_end is None:
            attr_pos_end = max(len(t) for t in tokenized_target_sentences)
        for seq_idx in range(num_sequences):
            # If the model is decoder-only, the source is the input prefix
            curr_pos_start = min(len(tokenized_target_sentences[seq_idx]), attr_pos_end) - len(targets[seq_idx])
            pos_start.append(curr_pos_start)
        if attr.source_attributions is not None:
            source_attributions = get_sequences_from_batched_steps([att.source_attributions for att in attributions])
            for seq_id in range(num_sequences):
                # Remove padding from tensor
                source_attributions[seq_id] = source_attributions[seq_id][
                    : len(sources[seq_id]), : len(targets[seq_id]), ...
                ]
        if attr.target_attributions is not None:
            target_attributions = get_sequences_from_batched_steps(
                [att.target_attributions for att in attributions], padding_dims=[1]
            )
            for seq_id in range(num_sequences):
                start_idx = max(pos_start) - pos_start[seq_id]
                end_idx = start_idx + len(tokenized_target_sentences[seq_id])
                target_attributions[seq_id] = target_attributions[seq_id][
                    start_idx:end_idx, : len(targets[seq_id]), ...  # noqa: E203
                ]
                if target_attributions[seq_id].shape[0] != len(tokenized_target_sentences[seq_id]):
                    target_attributions[seq_id] = pad_with_nan(target_attributions[seq_id], dim=0, pad_size=1)
        if attr.step_scores is not None:
            step_scores = [{} for _ in range(num_sequences)]
            for step_score_name in attr.step_scores.keys():
                out_step_scores = get_sequences_from_batched_steps(
                    [att.step_scores[step_score_name] for att in attributions], stack_dim=1
                )
                for seq_id in range(num_sequences):
                    step_scores[seq_id][step_score_name] = out_step_scores[seq_id][: len(targets[seq_id])]
        if attr.sequence_scores is not None:
            seq_scores = [{} for _ in range(num_sequences)]
            for seq_score_name in attr.sequence_scores.keys():
                # Since we need to know in advance the length of the sequence to remove padding from
                # batching sequences with different lengths, we rely on code names for sequence scores
                # that are not source-to-target (default for encoder-decoder) or target-to-target
                # (default for decoder only).
                remove_pad_fn = cls.get_remove_pad_fn(attr, seq_score_name)
                if seq_score_name.startswith("encoder") or seq_score_name.startswith("decoder"):
                    out_seq_scores = [attr.sequence_scores[seq_score_name][i, ...] for i in range(num_sequences)]
                else:
                    out_seq_scores = get_sequences_from_batched_steps(
                        [att.sequence_scores[seq_score_name] for att in attributions], padding_dims=[1]
                    )
                for seq_id in range(num_sequences):
                    seq_scores[seq_id][seq_score_name] = remove_pad_fn(out_seq_scores, sources, targets, seq_id)
        seq_attributions: list[FeatureAttributionSequenceOutput] = []
        for seq_idx in range(num_sequences):
            curr_seq_attribution: FeatureAttributionSequenceOutput = attr.get_sequence_cls(
                source=deepcopy(
                    tokenized_target_sentences[seq_idx][: pos_start[seq_idx]] if not sources else sources[seq_idx]
                ),
                target=deepcopy(tokenized_target_sentences[seq_idx]),
                source_attributions=source_attributions[seq_idx] if attr.source_attributions is not None else None,
                target_attributions=target_attributions[seq_idx] if attr.target_attributions is not None else None,
                step_scores=step_scores[seq_idx] if attr.step_scores is not None else None,
                sequence_scores=seq_scores[seq_idx] if attr.sequence_scores is not None else None,
                attr_pos_start=pos_start[seq_idx],
                attr_pos_end=attr_pos_end,
            )
            seq_attributions.append(curr_seq_attribution)
        return seq_attributions
    def show(
        self,
        min_val: int | None = None,
        max_val: int | None = None,
        max_show_size: int | None = None,
        show_dim: int | str | None = None,
        slice_dims: dict[int | str, tuple[int, int]] | None = None,
        display: bool = True,
        return_html: bool | None = False,
        return_figure: bool = False,
        aggregator: AggregatorPipeline | type[Aggregator] = None,
        do_aggregation: bool = True,
        **kwargs,
    ) -> str | None:
        """Visualize the attributions.
        Args:
            min_val (:obj:`int`, *optional*, defaults to None):
                Minimum value in the color range of the visualization. If None, the minimum value of the attributions
                across all visualized examples is used.
            max_val (:obj:`int`, *optional*, defaults to None):
                Maximum value in the color range of the visualization. If None, the maximum value of the attributions
                across all visualized examples is used.
            max_show_size (:obj:`int`, *optional*, defaults to None):
                For granular visualization, this parameter specifies the maximum dimension size for additional dimensions
                to be visualized. Default: 20.
            show_dim (:obj:`int` or :obj:`str`, *optional*, defaults to None):
                For granular visualization, this parameter specifies the dimension that should be visualized along with
                the source and target tokens. Can be either the dimension index or the dimension name. Works only if
                the dimension size is less than or equal to `max_show_size`.
            slice_dims (:obj:`dict[int or str, tuple[int, int]]`, *optional*, defaults to None):
                For granular visualization, this parameter specifies the dimensions that should be sliced and visualized
                along with the source and target tokens. The dictionary should contain the dimension index or name as the
                key and the slice range as the value.
            display (:obj:`bool`, *optional*, defaults to True):
                Whether to display the visualization. Can be set to False if the visualization is produced and stored
                for later use.
            return_html (:obj:`bool`, *optional*, defaults to False):
                Whether to return the HTML code of the visualization.
            return_figure (:obj:`bool`, *optional*, defaults to False):
                For granular visualization, whether to return the Treescope figure object for further manipulation.
            aggregator (:obj:`AggregatorPipeline`, *optional*, defaults to None):
                Aggregates attributions before visualizing them. If not specified, the default aggregator for the class
                is used.
            do_aggregation (:obj:`bool`, *optional*, defaults to True):
                Whether to aggregate the attributions before visualizing them. Allows to skip aggregation if the
                attributions are already aggregated.
        Returns:
            :obj:`str`: The HTML code of the visualization if :obj:`return_html` is set to True, otherwise None.
        """
        from inseq import show_attributions, show_granular_attributions
        # If no aggregator is specified, the default aggregator for the class is used
        aggregated = self.aggregate(aggregator, **kwargs) if do_aggregation else self
        if (aggregated.source_attributions is not None and aggregated.source_attributions.shape[1] == 0) or (
            aggregated.target_attributions is not None and aggregated.target_attributions.shape[1] == 0
        ):
            tokens = "".join(tid.token for tid in self.target)
            logger.warning(f"Found empty attributions, skipping attribution matching generation: {tokens}")
        if (
            (aggregated.source_attributions is not None and aggregated.source_attributions.ndim == 2)
            or (aggregated.target_attributions is not None and aggregated.target_attributions.ndim == 2)
            or (aggregated.source_attributions is None and aggregated.target_attributions is None)
        ):
            return show_attributions(
                attributions=aggregated, min_val=min_val, max_val=max_val, display=display, return_html=return_html
            )
        else:
            return show_granular_attributions(
                attributions=aggregated,
                max_show_size=max_show_size,
                min_val=min_val,
                max_val=max_val,
                show_dim=show_dim,
                display=display,
                return_html=return_html,
                return_figure=return_figure,
                slice_dims=slice_dims,
            )
    def show_granular(
        self,
        min_val: int | None = None,
        max_val: int | None = None,
        max_show_size: int | None = None,
        show_dim: int | str | None = None,
        slice_dims: dict[int | str, tuple[int, int]] | None = None,
        display: bool = True,
        return_html: bool | None = False,
        return_figure: bool = False,
    ) -> str | None:
        """Visualizes granular attribution heatmaps in HTML format.
        Args:
            min_val (:obj:`int`, *optional*, defaults to None):
                Lower attribution score threshold for color map.
            max_val (:obj:`int`, *optional*, defaults to None):
                Upper attribution score threshold for color map.
            max_show_size (:obj:`int`, *optional*, defaults to None):
                Maximum dimension size for additional dimensions to be visualized. Default: 20.
            show_dim (:obj:`int` or :obj:`str`, *optional*, defaults to None):
                Dimension to be visualized along with the source and target tokens. Can be either the dimension index or
                the dimension name. Works only if the dimension size is less than or equal to `max_show_size`.
            slice_dims (:obj:`dict[int or str, tuple[int, int]]`, *optional*, defaults to None):
                Dimensions to be sliced and visualized along with the source and target tokens. The dictionary should
                contain the dimension index or name as the key and the slice range as the value.
            display (:obj:`bool`, *optional*, defaults to True):
                Whether to show the output of the visualization function.
            return_html (:obj:`bool`, *optional*, defaults to False):
                If true, returns the HTML corresponding to the notebook visualization of the attributions in
                string format, for saving purposes.
            return_figure (:obj:`bool`, *optional*, defaults to False):
                If true, returns the Treescope figure object for further manipulation.
        Returns:
            `str`: Returns the HTML output if `return_html=True`
        """
        from inseq import show_granular_attributions
        return show_granular_attributions(
            attributions=self,
            max_show_size=max_show_size,
            min_val=min_val,
            max_val=max_val,
            show_dim=show_dim,
            slice_dims=slice_dims,
            display=display,
            return_html=return_html,
            return_figure=return_figure,
        )
    def show_tokens(
        self,
        min_val: int | None = None,
        max_val: int | None = None,
        display: bool = True,
        return_html: bool | None = False,
        return_figure: bool = False,
        replace_char: dict[str, str] | None = None,
        wrap_after: int | str | list[str] | tuple[str] | None = None,
        step_score_highlight: str | None = None,
        aggregator: AggregatorPipeline | type[Aggregator] = None,
        do_aggregation: bool = True,
        **kwargs,
    ) -> str | None:
        """Visualizes token-level attributions in HTML format.
        Args:
            attributions (:class:`~inseq.data.attribution.FeatureAttributionSequenceOutput`):
                Sequence attributions to be visualized.
            min_val (:obj:`int`, *optional*, defaults to None):
                Lower attribution score threshold for color map.
            max_val (:obj:`int`, *optional*, defaults to None):
                Upper attribution score threshold for color map.
            display (:obj:`bool`, *optional*, defaults to True):
                Whether to show the output of the visualization function.
            return_html (:obj:`bool`, *optional*, defaults to False):
                If true, returns the HTML corresponding to the notebook visualization of the attributions in string format,
                for saving purposes.
            return_figure (:obj:`bool`, *optional*, defaults to False):
                If true, returns the Treescope figure object for further manipulation.
            replace_char (:obj:`dict[str, str]`, *optional*, defaults to None):
                Dictionary mapping strings to be replaced to replacement options, used for cleaning special characters.
                Default: {}.
            wrap_after (:obj:`int` or :obj:`str` or :obj:`list[str]` :obj:`tuple[str]]`, *optional*, defaults to None):
                Token indices or tokens after which to wrap lines. E.g. 10 = wrap after every 10 tokens, "hi" = wrap after
                word hi occurs, ["." "!", "?"] or ".!?" = wrap after every sentence-ending punctuation.
            step_score_highlight (`str`, *optional*, defaults to None):
                Name of the step score to use to highlight generated tokens in the visualization. If None, no highlights are
                shown. Default: None.
        """
        from inseq import show_token_attributions
        aggregated = self.aggregate(aggregator, **kwargs) if do_aggregation else self
        return show_token_attributions(
            attributions=aggregated,
            min_val=min_val,
            max_val=max_val,
            display=display,
            return_html=return_html,
            return_figure=return_figure,
            replace_char=replace_char,
            wrap_after=wrap_after,
            step_score_highlight=step_score_highlight,
        )
    @property
    def minimum(self) -> float:
        minimum = 0
        if self.source_attributions is not None:
            minimum = min(minimum, float(torch.nan_to_num(self.source_attributions).min()))
        if self.target_attributions is not None:
            minimum = min(minimum, float(torch.nan_to_num(self.target_attributions).min()))
        return minimum
    @property
    def maximum(self) -> float:
        maximum = 0
        if self.source_attributions is not None:
            maximum = max(maximum, float(torch.nan_to_num(self.source_attributions).max()))
        if self.target_attributions is not None:
            maximum = max(maximum, float(torch.nan_to_num(self.target_attributions).max()))
        return maximum
    def weight_attributions(self, step_fn_id: str):
        """Weights attribution scores in place by the value of the selected step function for every generation step.
        Args:
            step_fn_id (`str`):
                The id of the step function to use for weighting the attributions (e.g. ``probability``)
        """
        aggregated_attr = self.aggregate()
        step_scores = self.step_scores[step_fn_id].T.unsqueeze(1)
        if self.source_attributions is not None:
            source_attr = aggregated_attr.source_attributions.float().T
            self.source_attributions = (step_scores * source_attr).T
        if self.target_attributions is not None:
            target_attr = aggregated_attr.target_attributions.float().T
            self.target_attributions = (step_scores * target_attr).T
        # Empty aggregator pipeline -> no aggregation
        self._aggregator = []
        return self
    def get_scores_dicts(
        self,
        aggregator: AggregatorPipeline | type[Aggregator] = None,
        do_aggregation: bool = True,
        **kwargs,
    ) -> dict[str, dict[str, dict[str, float]]]:
        # If no aggregator is specified, the default aggregator for the class is used
        aggr = self.aggregate(aggregator, **kwargs) if do_aggregation else self
        return_dict = {"source_attributions": {}, "target_attributions": {}, "step_scores": {}}
        for tgt_idx in range(aggr.attr_pos_start, aggr.attr_pos_end):
            tgt_tok = aggr.target[tgt_idx]
            if aggr.source_attributions is not None:
                return_dict["source_attributions"][(tgt_idx, tgt_tok.token)] = {}
                for src_idx, src_tok in enumerate(aggr.source):
                    return_dict["source_attributions"][(tgt_idx, tgt_tok.token)][
                        (src_idx, src_tok.token)
                    ] = aggr.source_attributions[src_idx, tgt_idx - aggr.attr_pos_start].item()
            if aggr.target_attributions is not None:
                return_dict["target_attributions"][(tgt_idx, tgt_tok.token)] = {}
                for tgt_idx_attr in range(aggr.attr_pos_end):
                    tgt_tok_attr = aggr.target[tgt_idx_attr]
                    return_dict["target_attributions"][(tgt_idx, tgt_tok.token)][
                        (tgt_idx_attr, tgt_tok_attr.token)
                    ] = aggr.target_attributions[tgt_idx_attr, tgt_idx - aggr.attr_pos_start].item()
            if aggr.step_scores is not None:
                return_dict["step_scores"][(tgt_idx, tgt_tok.token)] = {}
                for step_score_id, step_score in aggr.step_scores.items():
                    return_dict["step_scores"][(tgt_idx, tgt_tok.token)][step_score_id] = step_score[
                        tgt_idx - aggr.attr_pos_start
                    ].item()
        return return_dict
@dataclass(eq=False, repr=False)
class FeatureAttributionStepOutput(TensorWrapper):
    """Output of a single step of feature attribution, plus extra information related to what was attributed."""
    source_attributions: StepAttributionTensor | None = None
    step_scores: dict[str, SingleScorePerStepTensor] | None = None
    target_attributions: StepAttributionTensor | None = None
    sequence_scores: dict[str, MultipleScoresPerStepTensor] | None = None
    source: OneOrMoreTokenWithIdSequences | None = None
    prefix: OneOrMoreTokenWithIdSequences | None = None
    target: OneOrMoreTokenWithIdSequences | None = None
    _num_dimensions: int | None = None
    _sequence_cls: type["FeatureAttributionSequenceOutput"] = FeatureAttributionSequenceOutput
    def __post_init__(self):
        self.to(torch.float32)
        if self._num_dimensions is None:
            self._num_dimensions = 0
        if self.step_scores is None:
            self.step_scores = {}
        if self.sequence_scores is None:
            self.sequence_scores = {}
    def get_sequence_cls(self, **kwargs):
        return self._sequence_cls(**kwargs)
    def remap_from_filtered(
        self,
        target_attention_mask: TargetIdsTensor,
        batch: DecoderOnlyBatch | EncoderDecoderBatch,
        is_final_step_method: bool = False,
    ) -> None:
        """Remaps the attributions to the original shape of the input sequence."""
        batch_size = (
            len(batch.sources.input_tokens) if self.source_attributions is not None else len(batch.target_tokens)
        )
        source_len = len(batch.sources.input_tokens[0])
        target_len = len(batch.target_tokens[0])
        # Normal per-step attribution outputs have shape (batch_size, seq_len, ...)
        other_dims_start_idx = 2
        # Final step attribution outputs have shape (batch_size, seq_len, seq_len, ...)
        if is_final_step_method:
            other_dims_start_idx += 1
        other_dims = (
            self.source_attributions.shape[other_dims_start_idx:]
            if self.source_attributions is not None
            else self.target_attributions.shape[other_dims_start_idx:]
        )
        if self.source_attributions is not None:
            self.source_attributions = remap_from_filtered(
                original_shape=(batch_size, *self.source_attributions.shape[1:]),
                mask=target_attention_mask,
                filtered=self.source_attributions,
            )
        if self.target_attributions is not None:
            self.target_attributions = remap_from_filtered(
                original_shape=(batch_size, *self.target_attributions.shape[1:]),
                mask=target_attention_mask,
                filtered=self.target_attributions,
            )
        if self.step_scores is not None:
            for score_name, score_tensor in self.step_scores.items():
                self.step_scores[score_name] = remap_from_filtered(
                    original_shape=(batch_size, 1),
                    mask=target_attention_mask,
                    filtered=score_tensor.unsqueeze(-1),
                ).squeeze(-1)
        if self.sequence_scores is not None:
            for score_name, score_tensor in self.sequence_scores.items():
                if score_name.startswith("decoder"):
                    original_shape = (batch_size, target_len, target_len, *other_dims)
                elif score_name.startswith("encoder"):
                    original_shape = (batch_size, source_len, source_len, *other_dims)
                else:  # default case: cross-attention
                    original_shape = (batch_size, source_len, target_len, *other_dims)
                self.sequence_scores[score_name] = remap_from_filtered(
                    original_shape=original_shape,
                    mask=target_attention_mask,
                    filtered=score_tensor,
                )
@dataclass
class FeatureAttributionOutput:
    """Output produced by the `AttributionModel.attribute` method.
    Attributes:
        sequence_attributions (list of :class:`~inseq.data.FeatureAttributionSequenceOutput`): List
                containing all attributions performed on input sentences (one per input sentence, including
                source and optionally target-side attribution).
        step_attributions (list of :class:`~inseq.data.FeatureAttributionStepOutput`, optional): List
                containing all step attributions (one per generation step performed on the batch), returned if
                `output_step_attributions=True`.
        info (dict with str keys and any values): Dictionary including all available parameters used to
                perform the attribution.
    """
    # These fields of the info dictionary should be matching to allow merging
    _merge_match_info_fields = [
        "attribute_target",
        "attribution_method",
        "constrained_decoding",
        "include_eos_baseline",
        "model_class",
        "model_name",
        "step_scores",
        "tokenizer_class",
        "tokenizer_name",
    ]
    sequence_attributions: list[FeatureAttributionSequenceOutput]
    step_attributions: list[FeatureAttributionStepOutput] | None = None
    info: dict[str, Any] = field(default_factory=dict)
    def __str__(self):
        return f"{self.__class__.__name__}({pretty_dict(self.__dict__)})"
    def __repr__(self):
        return self.__str__()
    def __eq__(self, other):
        for self_seq, other_seq in zip(self.sequence_attributions, other.sequence_attributions, strict=False):
            if self_seq != other_seq:
                return False
        if self.step_attributions is not None and other.step_attributions is not None:
            for self_step, other_step in zip(self.step_attributions, other.step_attributions, strict=False):
                if self_step != other_step:
                    return False
        if self.info != other.info:
            return False
        return True
    def __getitem__(self, item) -> FeatureAttributionSequenceOutput:
        return self.sequence_attributions[item]
    def __len__(self) -> int:
        return len(self.sequence_attributions)
    def __iter__(self):
        return iter(self.sequence_attributions)
    def __add__(self, other) -> "FeatureAttributionOutput":
        return merge_attributions([self, other])
    def __radd__(self, other) -> "FeatureAttributionOutput":
        return self.__add__(other)
    def save(
        self,
        path: PathLike,
        overwrite: bool = False,
        compress: bool = False,
        ndarray_compact: bool = True,
        use_primitives: bool = False,
        split_sequences: bool = False,
        scores_precision: ScorePrecision = "float32",
    ) -> None:
        """Save class contents to a JSON file.
        Args:
            path (:obj:`os.PathLike`): Path to the folder where the attribution output will be stored
                (e.g. ``./out.json``).
            overwrite (:obj:`bool`, *optional*, defaults to False):
                If True, overwrite the file if it exists, raise error otherwise.
            compress (:obj:`bool`, *optional*, defaults to False):
                If True, the output file is compressed using gzip. Especially useful for large sequences and granular
                attributions with umerged hidden dimensions.
            ndarray_compact (:obj:`bool`, *optional*, defaults to True):
                If True, the arrays for scores and attributions are stored in a compact b64 format. Otherwise, they are
                stored as plain lists of floats.
            use_primitives (:obj:`bool`, *optional*, defaults to False):
                If True, the output is stored as a list of dictionaries with primitive types (e.g. int, float, str).
                Note that an attribution saved with this option cannot be loaded with the `load` method.
            split_sequences (:obj:`bool`, *optional*, defaults to False):
                If True, the output is split into multiple files, one per sequence. The file names are generated by
                appending the sequence index to the given path (e.g. ``./out.json`` with two sequences ->
                ``./out_0.json``, ``./out_1.json``)
            scores_precision (:obj:`str`, *optional*, defaults to "float32"):
                Rounding precision for saved scores. Can be used to reduce space on disk but introduces rounding
                errors. Can be combined with compress=True for further space reduction.
                Accepted values: "float32", "float16", or "float8". Default: "float32" (no rounding).
        """
        if not overwrite and Path(path).exists():
            raise ValueError(f"{path} already exists. Override with overwrite=True.")
        save_outs = []
        paths = []
        if split_sequences:
            for seq_id in range(len(self.sequence_attributions)):
                attr_out = deepcopy(self)
                attr_out.sequence_attributions = [
                    attr_out.sequence_attributions[seq_id]._convert_to_safetensors(scores_precision=scores_precision)
                ]  # this overwrites the original
                attr_out.step_attributions = None
                attr_out.info["input_texts"] = [attr_out.info["input_texts"][seq_id]]
                attr_out.info["generated_texts"] = [attr_out.info["generated_texts"][seq_id]]
                save_outs.append(attr_out)
                paths.append(f"{str(path).split('.json')[0]}_{seq_id}.json{'.gz' if compress else ''}")
        else:
            self_out = deepcopy(self)
            self_out.sequence_attributions = [
                seq._convert_to_safetensors(scores_precision=scores_precision)
                for seq in self_out.sequence_attributions
            ]
            save_outs.append(self_out)
            paths.append(path)
        for attr_out, path_out in zip(save_outs, paths, strict=False):
            with open(path_out, f"w{'b' if compress else ''}") as f:
                json_advanced_dump(
                    attr_out,
                    f,
                    allow_nan=True,
                    indent=4,
                    sort_keys=True,
                    ndarray_compact=ndarray_compact,
                    compression=compress,
                    use_primitives=use_primitives,
                )
    @staticmethod
    def load(
        path: PathLike,
        decompress: bool = False,
    ) -> "FeatureAttributionOutput":
        """Load saved attribution output into a new :class:`~inseq.data.FeatureAttributionOutput` object.
        Args:
            path (:obj:`str`): Path to the JSON file containing the saved attribution output.
                Note that the file must have been saved with the :meth:`~inseq.data.FeatureAttributionOutput.save`
                method with ``use_primitives=False`` in order to be loaded correctly.
            decompress (:obj:`bool`, *optional*, defaults to False):
                If True, the input file is decompressed using gzip.
        Returns:
            :class:`~inseq.data.FeatureAttributionOutput`: Loaded attribution output
        """
        out = json_advanced_load(path, decompression=decompress)
        out.sequence_attributions = [seq._recover_from_safetensors() for seq in out.sequence_attributions]
        if out.step_attributions is not None:
            out.step_attributions = [step._recover_from_safetensors() for step in out.step_attributions]
        return out
    def aggregate(
        self,
        aggregator: AggregatorPipeline | type[Aggregator] = None,
        **kwargs,
    ) -> "FeatureAttributionOutput":
        """Aggregate the sequence attributions using one or more aggregators.
        Args:
            aggregator (:obj:`AggregatorPipeline` or :obj:`Type[Aggregator]`, optional): Aggregator
                or pipeline to use. If not provided, the default aggregator for every sequence attribution
                is used.
        Returns:
            :class:`~inseq.data.FeatureAttributionOutput`: Aggregated attribution output
        """
        aggregated = deepcopy(self)
        for idx, seq in enumerate(aggregated.sequence_attributions):
            aggregated.sequence_attributions[idx] = seq.aggregate(aggregator, **kwargs)
        return aggregated
    def show(
        self,
        min_val: int | None = None,
        max_val: int | None = None,
        max_show_size: int | None = None,
        show_dim: int | str | None = None,
        slice_dims: dict[int | str, tuple[int, int]] | None = None,
        display: bool = True,
        return_html: bool | None = False,
        return_figure: bool = False,
        aggregator: AggregatorPipeline | type[Aggregator] = None,
        do_aggregation: bool = True,
        **kwargs,
    ) -> str | list | None:
        """Visualize the sequence attributions.
        Args:
            min_val (int, optional): Minimum value for color scale.
            max_val (int, optional): Maximum value for color scale.
            max_show_size (int, optional): Maximum size of the dimension to show.
            show_dim (int or str, optional): Dimension to show.
            slice_dims (dict[int or str, tuple[int, int]], optional): Dimensions to slice.
            display (bool, optional): If True, display the attribution visualization.
            return_html (bool, optional): If True, return the attribution visualization as HTML.
            return_figure (bool, optional): If True, return the Treescope figure object for further manipulation.
            aggregator (:obj:`AggregatorPipeline` or :obj:`Type[Aggregator]`, optional): Aggregator
                or pipeline to use. If not provided, the default aggregator for every sequence attribution
                is used.
            do_aggregation (:obj:`bool`, *optional*, defaults to True):
                Whether to aggregate the attributions before visualizing them. Allows to skip aggregation if the
                attributions are already aggregated.
        Returns:
            str: Attribution visualization as HTML if `return_html=True`
            list: List of Treescope figure objects if `return_figure=True`
            None if `return_html=False` and `return_figure=False`
        """
        out_str = ""
        out_figs = []
        for attr in self.sequence_attributions:
            curr_out = attr.show(
                min_val=min_val,
                max_val=max_val,
                max_show_size=max_show_size,
                show_dim=show_dim,
                slice_dims=slice_dims,
                display=display,
                return_html=return_html,
                return_figure=return_figure,
                aggregator=aggregator,
                do_aggregation=do_aggregation,
                **kwargs,
            )
            if return_html:
                out_str += curr_out
            if return_figure:
                out_figs.append(curr_out)
        if return_html:
            return out_str
        if return_figure:
            return out_figs
    def show_granular(
        self,
        min_val: int | None = None,
        max_val: int | None = None,
        max_show_size: int | None = None,
        show_dim: int | str | None = None,
        slice_dims: dict[int | str, tuple[int, int]] | None = None,
        display: bool = True,
        return_html: bool = False,
        return_figure: bool = False,
    ) -> str | None:
        """Visualizes granular attribution heatmaps in HTML format.
        Args:
            min_val (:obj:`int`, *optional*, defaults to None):
                Lower attribution score threshold for color map.
            max_val (:obj:`int`, *optional*, defaults to None):
                Upper attribution score threshold for color map.
            max_show_size (:obj:`int`, *optional*, defaults to None):
                Maximum dimension size for additional dimensions to be visualized. Default: 20.
            show_dim (:obj:`int` or :obj:`str`, *optional*, defaults to None):
                Dimension to be visualized along with the source and target tokens. Can be either the dimension index or
                the dimension name. Works only if the dimension size is less than or equal to `max_show_size`.
            slice_dims (:obj:`dict[int or str, tuple[int, int]]`, *optional*, defaults to None):
                Dimensions to be sliced and visualized along with the source and target tokens. The dictionary should
                contain the dimension index or name as the key and the slice range as the value.
            display (:obj:`bool`, *optional*, defaults to True):
                Whether to show the output of the visualization function.
            return_html (:obj:`bool`, *optional*, defaults to False):
                If true, returns the HTML corresponding to the notebook visualization of the attributions in
                string format, for saving purposes.
            return_figure (:obj:`bool`, *optional*, defaults to False):
                If true, returns the Treescope figure object for further manipulation.
        Returns:
            `str`: Returns the HTML output if `return_html=True`
        """
        out_str = ""
        out_figs = []
        for attr in self.sequence_attributions:
            curr_out = attr.show_granular(
                min_val=min_val,
                max_val=max_val,
                max_show_size=max_show_size,
                show_dim=show_dim,
                slice_dims=slice_dims,
                display=display,
                return_html=return_html,
            )
            if return_html:
                out_str += curr_out
            if return_figure:
                out_figs.append(curr_out)
        if return_html:
            return out_str
        if return_figure:
            return out_figs
    def show_tokens(
        self,
        min_val: int | None = None,
        max_val: int | None = None,
        display: bool = True,
        return_html: bool = False,
        return_figure: bool = False,
        replace_char: dict[str, str] | None = None,
        wrap_after: int | str | list[str] | tuple[str] | None = None,
        step_score_highlight: str | None = None,
        aggregator: AggregatorPipeline | type[Aggregator] = None,
        do_aggregation: bool = True,
        **kwargs,
    ) -> str | None:
        """Visualizes token-level attributions in HTML format.
        Args:
            min_val (:obj:`int`, *optional*, defaults to None):
                Lower attribution score threshold for color map.
            max_val (:obj:`int`, *optional*, defaults to None):
                Upper attribution score threshold for color map.
            display (:obj:`bool`, *optional*, defaults to True):
                Whether to show the output of the visualization function.
            return_html (:obj:`bool`, *optional*, defaults to False):
                If true, returns the HTML corresponding to the notebook visualization of the attributions in string format,
                for saving purposes.
            return_figure (:obj:`bool`, *optional*, defaults to False):
                If true, returns the Treescope figure object for further manipulation.
            replace_char (:obj:`dict[str, str]`, *optional*, defaults to None):
                Dictionary mapping strings to be replaced to replacement options, used for cleaning special characters.
                Default: {}.
            wrap_after (:obj:`int` or :obj:`str` or :obj:`list[str]` :obj:`tuple[str]]`, *optional*, defaults to None):
                Token indices or tokens after which to wrap lines. E.g. 10 = wrap after every 10 tokens, "hi" = wrap after
                word hi occurs, ["." "!", "?"] or ".!?" = wrap after every sentence-ending punctuation.
            step_score_highlight (`str`, *optional*, defaults to None):
                Name of the step score to use to highlight generated tokens in the visualization. If None, no highlights are
                shown. Default: None.
        """
        out_str = ""
        out_figs = []
        for attr in self.sequence_attributions:
            curr_out = attr.show_tokens(
                min_val=min_val,
                max_val=max_val,
                display=display,
                return_html=return_html,
                return_figure=return_figure,
                replace_char=replace_char,
                wrap_after=wrap_after,
                step_score_highlight=step_score_highlight,
                aggregator=aggregator,
                do_aggregation=do_aggregation,
                **kwargs,
            )
            if return_html:
                out_str += curr_out
            if return_figure:
                out_figs.append(curr_out)
        if return_html:
            return out_str
        if return_figure:
            return out_figs
    def weight_attributions(self, step_score_id: str):
        for i, attr in enumerate(self.sequence_attributions):
            self.sequence_attributions[i] = attr.weight_attributions(step_score_id)
    def get_scores_dicts(
        self, aggregator: AggregatorPipeline | type[Aggregator] = None, do_aggregation: bool = True, **kwargs
    ) -> list[dict[str, dict[str, dict[str, float]]]]:
        """Get all computed scores (attributions and step scores) for all sequences as a list of dictionaries.
        Returns:
            :obj:`list(dict)`: List containing one dictionary per sequence. Every dictionary contains the keys
            "source_attributions", "target_attributions" and "step_scores". For each of these keys, the value is a
            dictionary with generated tokens as keys, and for values a final dictionary. For  "step_scores", the keys
            of the final dictionary are the step score ids, and the values are the scores.
            For "source_attributions" and "target_attributions", the keys of the final dictionary are respectively
            source and target tokens, and the values are the attribution scores.
        This output is intended to be easily converted to a pandas DataFrame. The following example produces a list of
        DataFrames, one for each sequence, matching the source attributions that would be visualized by out.show().
        ```python
        dfs = [pd.DataFrame(x["source_attributions"]) for x in out.get_scores_dicts()]
        ```
        """
        return [attr.get_scores_dicts(aggregator, do_aggregation, **kwargs) for attr in self.sequence_attributions]
@dataclass(eq=False, repr=False)
class GranularFeatureAttributionSequenceOutput(FeatureAttributionSequenceOutput):
    """Raw output of a single sequence of granular feature attribution.
    An example of granular feature attribution methods are gradient-based attribution methods such as Integrated
    Gradients, returning one score per hidden dimension of the model for every generated token.
    Adds the convergence delta and default L2 + normalization merging of attributions to the base class.
    """
    def __post_init__(self):
        super().__post_init__()
        self._aggregator = "vnorm"
        self._dict_aggregate_fn["source_attributions"]["scores"] = "vnorm"
        self._dict_aggregate_fn["target_attributions"]["scores"] = "vnorm"
        if "deltas" not in self._dict_aggregate_fn["step_scores"]["spans"]:
            self._dict_aggregate_fn["step_scores"]["spans"]["deltas"] = "absmax"
        self._attribution_dim_names = {
            "source_attributions": {0: "Input Tokens", 1: "Generated Tokens", 2: "Embedding Dimension"},
            "target_attributions": {0: "Input Tokens", 1: "Generated Tokens", 2: "Embedding Dimension"},
        }
@dataclass(eq=False, repr=False)
class GranularFeatureAttributionStepOutput(FeatureAttributionStepOutput):
    """Raw output of a single step of gradient feature attribution."""
    _sequence_cls: type["FeatureAttributionSequenceOutput"] = GranularFeatureAttributionSequenceOutput
@dataclass(eq=False, repr=False)
class CoarseFeatureAttributionSequenceOutput(FeatureAttributionSequenceOutput):
    """Raw output of a single sequence of coarse-grained feature attribution.
    An example of coarse-grained feature attribution methods are occlusion methods in which a whole token is masked at
    once, producing a single output score per token.
    """
    def __post_init__(self):
        super().__post_init__()
        self._aggregator = []
@dataclass(eq=False, repr=False)
class CoarseFeatureAttributionStepOutput(FeatureAttributionStepOutput):
    """Raw output of a single step of coarse-grained feature attribution."""
    _sequence_cls: type["FeatureAttributionSequenceOutput"] = CoarseFeatureAttributionSequenceOutput
@dataclass(eq=False, repr=False)
class MultiDimensionalFeatureAttributionSequenceOutput(FeatureAttributionSequenceOutput):
    """Raw output of a single sequence of multi-dimensional feature attribution.
    Multi-dimensional feature attribution methods are a generalization of granular feature attribution methods
    allowing for an arbitrary number of extra dimensions. For example, the attention method returns one score per
    attention head and per layer for every source-target token pair in the source attributions (i.e. 2 dimensions).
    """
    def __post_init__(self):
        super().__post_init__()
        self._aggregator = ["mean"] * self._num_dimensions
        self._attribution_dim_names = {
            "source_attributions": {0: "Input Tokens", 1: "Generated Tokens", 2: "Model Layer"},
            "target_attributions": {0: "Input Tokens", 1: "Generated Tokens", 2: "Model Layer"},
            "encoder": {0: "Input Tokens", 1: "Input Tokens", 2: "Model Layer"},
            "decoder": {0: "Generated Tokens", 1: "Generated Tokens", 2: "Model Layer"},
        }
        if self._num_dimensions == 2:
            for key in self._attribution_dim_names.keys():
                self._attribution_dim_names[key][3] = "Attention Head"
@dataclass(eq=False, repr=False)
class MultiDimensionalFeatureAttributionStepOutput(FeatureAttributionStepOutput):
    """Raw output of a single step of multi-dimensional feature attribution."""
    _sequence_cls: type["FeatureAttributionSequenceOutput"] = MultiDimensionalFeatureAttributionSequenceOutput
    def get_sequence_cls(self, **kwargs):
        return MultiDimensionalFeatureAttributionSequenceOutput(_num_dimensions=self._num_dimensions, **kwargs)

================
File: inseq/data/batch.py
================
from dataclasses import dataclass
from ..utils import get_aligned_idx
from ..utils.typing import EmbeddingsTensor, ExpandedTargetIdsTensor, IdsTensor, OneOrMoreTokenSequences
from .data_utils import TensorWrapper
@dataclass(eq=False, repr=False)
class BatchEncoding(TensorWrapper):
    """Output produced by the tokenization process using :meth:`~inseq.models.AttributionModel.encode`.
    Attributes:
        input_ids (:obj:`torch.Tensor`): Batch of token ids with shape ``[batch_size, longest_seq_length]``.
            Extra tokens for each sentence are padded, and truncation to ``max_seq_length`` is performed.
        input_tokens (:obj:`list(list(str))`): List of lists containing tokens for each sentence in the batch.
        attention_mask (:obj:`torch.Tensor`): Batch of attention masks with shape ``[batch_size, longest_seq_length]``.
            1 for positions that are valid, 0 for padded positions.
        baseline_ids (torch.Tensor, optional): Batch of reference token ids with shape
            ``[batch_size, longest_seq_length]``. Used for attribution methods requiring a baseline input (e.g. IG).
    """
    input_ids: IdsTensor
    attention_mask: IdsTensor
    input_tokens: OneOrMoreTokenSequences | None = None
    baseline_ids: IdsTensor | None = None
    def __len__(self) -> int:
        return len(self.input_tokens)
    @property
    def num_sequences(self) -> int:
        return self.input_ids.shape[0]
@dataclass(eq=False, repr=False)
class BatchEmbedding(TensorWrapper):
    """Embeddings produced by the embedding process using :meth:`~inseq.models.AttributionModel.embed`.
    Attributes:
        input_embeds (:obj:`torch.Tensor`): Batch of token embeddings with shape
            ``[batch_size, longest_seq_length, embedding_size]`` for each sentence in the batch.
        baseline_embeds (:obj:`torch.Tensor`, optional): Batch of reference token embeddings with shape
            ``[batch_size, longest_seq_length, embedding_size]`` for each sentence in the batch.
    """
    input_embeds: EmbeddingsTensor | None = None
    baseline_embeds: EmbeddingsTensor | None = None
    def __len__(self) -> int | None:
        if self.input_embeds is not None:
            return self.input_embeds.shape[0]
        return None
@dataclass(eq=False, repr=False)
class Batch(TensorWrapper):
    """Batch of input data for the attribution model.
    Attributes:
        encoding (:class:`~inseq.data.BatchEncoding`): Output produced by the tokenization process using
            :meth:`~inseq.models.AttributionModel.encode`.
        embedding (:class:`~inseq.data.BatchEmbedding`): Embeddings produced by the embedding process using
            :meth:`~inseq.models.AttributionModel.embed`.
    All attribute fields are accessible as properties (e.g. ``batch.input_ids`` corresponds to
        ``batch.encoding.input_ids``)
    """
    encoding: BatchEncoding
    embedding: BatchEmbedding
    @property
    def input_ids(self) -> IdsTensor:
        return self.encoding.input_ids
    @property
    def input_tokens(self) -> OneOrMoreTokenSequences:
        return self.encoding.input_tokens
    @property
    def attention_mask(self) -> IdsTensor:
        return self.encoding.attention_mask
    @property
    def baseline_ids(self) -> IdsTensor | None:
        return self.encoding.baseline_ids
    @property
    def input_embeds(self) -> EmbeddingsTensor | None:
        return self.embedding.input_embeds
    @property
    def baseline_embeds(self) -> EmbeddingsTensor | None:
        return self.embedding.baseline_embeds
    @input_ids.setter
    def input_ids(self, value: IdsTensor):
        self.encoding.input_ids = value
    @input_tokens.setter
    def input_tokens(self, value: list[list[str]]):
        self.encoding.input_tokens = value
    @attention_mask.setter
    def attention_mask(self, value: IdsTensor):
        self.encoding.attention_mask = value
    @baseline_ids.setter
    def baseline_ids(self, value: IdsTensor | None):
        self.encoding.baseline_ids = value
    @input_embeds.setter
    def input_embeds(self, value: EmbeddingsTensor | None):
        self.embedding.input_embeds = value
    @baseline_embeds.setter
    def baseline_embeds(self, value: EmbeddingsTensor | None):
        self.embedding.baseline_embeds = value
@dataclass(eq=False, repr=False)
class EncoderDecoderBatch(TensorWrapper):
    """Batch of input data for the encoder-decoder attribution model, including information for the source text and the
    target prefix.
    Attributes:
        sources (:class:`~inseq.data.Batch`): Batch of input data for the source text.
        targets (:class:`~inseq.data.Batch`): Batch of input data for the target prefix.
    """
    sources: Batch
    targets: Batch
    def __getitem__(self, subscript: slice | int) -> "EncoderDecoderBatch":
        return EncoderDecoderBatch(sources=self.sources, targets=self.targets[subscript])
    @property
    def max_generation_length(self) -> int:
        return self.targets.input_ids.shape[1]
    @property
    def source_tokens(self) -> OneOrMoreTokenSequences:
        return self.sources.input_tokens
    @property
    def target_tokens(self) -> OneOrMoreTokenSequences:
        return self.targets.input_tokens
    @property
    def source_ids(self) -> IdsTensor:
        return self.sources.input_ids
    @property
    def target_ids(self) -> IdsTensor:
        return self.targets.input_ids
    @property
    def source_embeds(self) -> EmbeddingsTensor:
        return self.sources.input_embeds
    @property
    def target_embeds(self) -> EmbeddingsTensor:
        return self.targets.input_embeds
    @property
    def source_mask(self) -> IdsTensor:
        return self.sources.attention_mask
    @property
    def target_mask(self) -> IdsTensor:
        return self.targets.attention_mask
    def get_step_target(
        self, step: int, with_attention: bool = False
    ) -> ExpandedTargetIdsTensor | tuple[ExpandedTargetIdsTensor, ExpandedTargetIdsTensor]:
        tgt = self.targets.input_ids[:, step]
        if with_attention:
            return tgt, self.targets.attention_mask[:, step]
        return tgt
@dataclass(eq=False, repr=False)
class DecoderOnlyBatch(Batch):
    """Input batch adapted for decoder-only attribution models, including information for the target prefix."""
    @property
    def max_generation_length(self) -> int:
        return self.input_ids.shape[1]
    @property
    def source_tokens(self) -> OneOrMoreTokenSequences:
        return None
    @property
    def target_tokens(self) -> OneOrMoreTokenSequences:
        return self.input_tokens
    @property
    def source_ids(self) -> IdsTensor:
        return None
    @property
    def target_ids(self) -> IdsTensor:
        return self.input_ids
    @property
    def source_embeds(self) -> EmbeddingsTensor:
        return None
    @property
    def target_embeds(self) -> EmbeddingsTensor:
        return self.input_embeds
    @property
    def source_mask(self) -> IdsTensor:
        return None
    @property
    def target_mask(self) -> IdsTensor:
        return self.attention_mask
    def get_step_target(
        self, step: int, with_attention: bool = False
    ) -> ExpandedTargetIdsTensor | tuple[ExpandedTargetIdsTensor, ExpandedTargetIdsTensor]:
        tgt = self.input_ids[:, step]
        if with_attention:
            return tgt, self.attention_mask[:, step]
        return tgt
    @classmethod
    def from_batch(self, batch: Batch) -> "DecoderOnlyBatch":
        return DecoderOnlyBatch(
            encoding=batch.encoding,
            embedding=batch.embedding,
        )
def slice_batch_from_position(
    batch: DecoderOnlyBatch, curr_idx: int, alignments: list[tuple[int, int]] | None = None
) -> tuple[DecoderOnlyBatch, IdsTensor]:
    if len(alignments) > 0 and isinstance(alignments[0], list):
        alignments = alignments[0]
    truncate_idx = get_aligned_idx(curr_idx, alignments)
    tgt_ids = batch.target_ids[:, truncate_idx]
    return batch[:truncate_idx], tgt_ids

================
File: inseq/data/data_utils.py
================
from copy import deepcopy
from dataclasses import dataclass, fields
from typing import Any, TypeVar
import numpy as np
import torch
from jaxtyping import Int
from ..utils import pretty_dict
TensorClass = TypeVar("TensorClass", bound="TensorWrapper")
@dataclass
class TensorWrapper:
    """Wrapper for tensors and lists of tensors to allow for easy access to their attributes."""
    @staticmethod
    def _getitem(attr, subscript):
        if isinstance(attr, torch.Tensor):
            if attr.ndim == 1:
                return attr[subscript]
            if attr.ndim >= 2:
                return attr[:, subscript, ...]
        elif isinstance(attr, TensorWrapper):
            return attr[subscript]
        elif isinstance(attr, list) and isinstance(attr[0], list):
            return [seq[subscript] for seq in attr]
        elif isinstance(attr, dict):
            return {key: TensorWrapper._getitem(val, subscript) for key, val in attr.items()}
        else:
            return attr
    @staticmethod
    def _slice_batch(attr, subscript):
        if isinstance(attr, torch.Tensor):
            if attr.ndim == 1:
                return attr[subscript]
            if attr.ndim >= 2:
                return attr[subscript, ...]
        elif isinstance(attr, TensorWrapper | list):
            return attr[subscript]
        elif isinstance(attr, dict):
            return {key: TensorWrapper._slice_batch(val, subscript) for key, val in attr.items()}
        else:
            return attr
    @staticmethod
    def _select_active(attr, mask):
        if isinstance(attr, torch.Tensor):
            if attr.ndim <= 1:
                return attr
            else:
                curr_mask = mask.clone()
                if curr_mask.dtype != torch.bool:
                    curr_mask = curr_mask.bool()
                while curr_mask.ndim < attr.ndim:
                    curr_mask = curr_mask.unsqueeze(-1)
                orig_shape = attr.shape[1:]
                return attr.masked_select(curr_mask).reshape(-1, *orig_shape)
        elif isinstance(attr, TensorWrapper):
            return attr.select_active(mask)
        elif isinstance(attr, list):
            return [val for i, val in enumerate(attr) if mask.tolist()[i]]
        elif isinstance(attr, dict):
            return {key: TensorWrapper._select_active(val, mask) for key, val in attr.items()}
        else:
            return attr
    @staticmethod
    def _to(attr, device: str):
        if isinstance(attr, torch.Tensor | TensorWrapper):
            return attr.to(device)
        elif isinstance(attr, dict):
            return {key: TensorWrapper._to(val, device) for key, val in attr.items()}
        else:
            return attr
    @staticmethod
    def _detach(attr):
        if isinstance(attr, torch.Tensor | TensorWrapper):
            return attr.detach()
        elif isinstance(attr, dict):
            return {key: TensorWrapper._detach(val) for key, val in attr.items()}
        else:
            return attr
    @staticmethod
    def _numpy(attr):
        if isinstance(attr, torch.Tensor | TensorWrapper):
            np_array = attr.numpy()
            if isinstance(np_array, np.ndarray):
                return np.ascontiguousarray(np_array, dtype=np_array.dtype)
            return np_array
        elif isinstance(attr, dict):
            return {key: TensorWrapper._numpy(val) for key, val in attr.items()}
        else:
            return attr
    @staticmethod
    def _torch(attr):
        if isinstance(attr, np.ndarray):
            return torch.tensor(attr)
        elif isinstance(attr, TensorWrapper):
            return attr.torch()
        elif isinstance(attr, dict):
            return {key: TensorWrapper._torch(val) for key, val in attr.items()}
        else:
            return attr
    @staticmethod
    def _eq(self_attr: TensorClass, other_attr: TensorClass) -> bool:
        try:
            if isinstance(self_attr, torch.Tensor):
                return torch.allclose(self_attr, other_attr, equal_nan=True, atol=1e-5)
            elif isinstance(self_attr, dict):
                return all(TensorWrapper._eq(self_attr[k], other_attr[k]) for k in self_attr.keys())
            else:
                return self_attr == other_attr
        except:  # noqa: E722
            return False
    def __getitem__(self: TensorClass, subscript) -> TensorClass:
        """By default, idiomatic slicing is used for the sequence dimension across batches.
        For batching use `slice_batch` instead.
        """
        return self.__class__(
            **{field.name: self._getitem(getattr(self, field.name), subscript) for field in fields(self.__class__)}
        )
    def slice_batch(self: TensorClass, subscript) -> TensorClass:
        return self.__class__(
            **{field.name: self._slice_batch(getattr(self, field.name), subscript) for field in fields(self.__class__)}
        )
    def select_active(self: TensorClass, mask: Int[torch.Tensor, "batch_size 1"]) -> TensorClass:
        return self.__class__(
            **{field.name: self._select_active(getattr(self, field.name), mask) for field in fields(self.__class__)}
        )
    def to(self: TensorClass, device: str) -> TensorClass:
        for field in fields(self.__class__):
            attr = getattr(self, field.name)
            setattr(self, field.name, self._to(attr, device))
        if device == "cpu" and torch.cuda.is_available():
            torch.cuda.empty_cache()
        return self
    def detach(self: TensorClass) -> TensorClass:
        for field in fields(self.__class__):
            attr = getattr(self, field.name)
            setattr(self, field.name, self._detach(attr))
        return self
    def numpy(self: TensorClass) -> TensorClass:
        for field in fields(self.__class__):
            attr = getattr(self, field.name)
            setattr(self, field.name, self._numpy(attr))
        return self
    def torch(self: TensorClass) -> TensorClass:
        for field, val in self.to_dict().items():
            setattr(self, field, self._torch(val))
        return self
    def clone(self: TensorClass) -> TensorClass:
        out_params = {}
        for field in fields(self.__class__):
            attr = getattr(self, field.name)
            if isinstance(attr, torch.Tensor | TensorWrapper):
                out_params[field.name] = attr.clone()
            elif attr is not None:
                out_params[field.name] = deepcopy(attr)
            else:
                out_params[field.name] = None
        return self.__class__(**out_params)
    def clone_empty(self: TensorClass) -> TensorClass:
        out_params = {k: v for k, v in self.__dict__.items() if k.startswith("_") and v is not None}
        return self.__class__(**out_params)
    def to_dict(self: TensorClass) -> dict[str, Any]:
        return {k: v for k, v in self.__dict__.items() if not k.startswith("_")}
    def __str__(self):
        return f"{self.__class__.__name__}({pretty_dict(self.to_dict())})"
    def __repr__(self):
        return f"{self.__class__.__name__}({pretty_dict(self.__dict__)})"
    def __eq__(self, other):
        equals = {field: self._eq(val, getattr(other, field)) for field, val in self.__dict__.items()}
        return all(x for x in equals.values())
    def __json_encode__(self):
        return self.clone().detach().to("cpu").numpy().to_dict()
    def __json_decode__(self, **attrs):
        # Does not contemplate the usage of __slots__
        self.__dict__ = attrs
        self.__post_init__()
    def __post_init__(self):
        pass

================
File: inseq/data/viz.py
================
# Adapted from https://github.com/slundberg/shap/blob/v0.39.0/shap/plots/_text.py, licensed MIT:
# Copyright Â© 2021 Scott Lundberg. All rights reserved.
# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and
# associated documentation files (the â€œSoftwareâ€), to deal in the Software without restriction,
# including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense,
#  and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so,
# subject to the following conditions:
# The above copyright notice and this permission notice shall be included in all copies
# or substantial portions of the Software.
# THE SOFTWARE IS PROVIDED â€œAS ISâ€, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT
# LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
# IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,
# WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE
# OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
import random
import string
from typing import TYPE_CHECKING, Literal
import numpy as np
import treescope as ts
import treescope.figures as fg
import treescope.rendering_parts as rp
from matplotlib.colors import Colormap
from rich import box
from rich.color import Color
from rich.console import Console
from rich.live import Live
from rich.markup import escape
from rich.padding import Padding
from rich.panel import Panel
from rich.progress import BarColumn, Progress, TextColumn, TimeRemainingColumn
from rich.style import Style
from rich.table import Column, Table
from rich.text import Text
from tqdm.std import tqdm
from ..utils import isnotebook
from ..utils.misc import clean_tokens
from ..utils.typing import TextSequences
from ..utils.viz_utils import (
    final_plot_html,
    get_colors,
    get_instance_html,
    maybe_add_linebreak,
    red_transparent_blue_colormap,
    saliency_heatmap_html,
    saliency_heatmap_table_header,
    sanitize_html,
    test_dim,
    treescope_cmap,
)
if TYPE_CHECKING:
    from .attribution import FeatureAttributionSequenceOutput
if isnotebook():
    cmap = treescope_cmap()
    ts.basic_interactive_setup(autovisualize_arrays=True)
    ts.default_diverging_colormap.set_globally(cmap)
    ts.default_sequential_colormap.set_globally(cmap)
def show_attributions(
    attributions: "FeatureAttributionSequenceOutput",
    min_val: int | None = None,
    max_val: int | None = None,
    display: bool = True,
    return_html: bool | None = False,
) -> str | None:
    """Core function allowing for visualization of feature attribution maps in console/HTML format.
    Args:
        attributions (:class:`~inseq.data.attribution.FeatureAttributionSequenceOutput`):
            Sequence attributions to be visualized.
        min_val (:obj:`Optional[int]`, *optional*, defaults to None):
            Lower attribution score threshold for color map.
        max_val (`Optional[int]`, *optional*, defaults to None):
            Upper attribution score threshold for color map.
        display (`bool`, *optional*, defaults to True):
            Whether to show the output of the visualization function.
        return_html (`Optional[bool]`, *optional*, defaults to False):
            If true, returns the HTML corresponding to the notebook visualization of the attributions in string format,
            for saving purposes.
    Returns:
        `Optional[str]`: Returns the HTML output if `return_html=True`
    """
    from inseq.data.attribution import FeatureAttributionSequenceOutput
    if isinstance(attributions, FeatureAttributionSequenceOutput):
        attributions = [attributions]
    html_out = ""
    html_colors = get_attribution_colors(attributions, min_val, max_val, cmap=red_transparent_blue_colormap())
    if not isnotebook():
        colors = get_attribution_colors(attributions, min_val, max_val, return_alpha=False, return_strings=False)
    idx = 0
    for ex_id, attribution in enumerate(attributions):
        instance_html = get_instance_html(ex_id)
        curr_html = ""
        curr_html_color = None
        if attribution.source_attributions is not None:
            curr_html_color = html_colors[idx]
            curr_html += instance_html
            curr_html += get_heatmap_type(attribution, curr_html_color, "Source", use_html=True)
            if attribution.target_attributions is not None:
                curr_html_color = html_colors[idx + 1]
        display_scores = attribution.source_attributions is None and attribution.step_scores
        if attribution.target_attributions is not None or display_scores:
            if curr_html_color is None and html_colors:
                curr_html_color = html_colors[idx]
            curr_html += instance_html
            curr_html += get_heatmap_type(attribution, curr_html_color, "Target", use_html=True)
        if display and isnotebook():
            from IPython.core.display import HTML, display
            display(HTML(curr_html))
        html_out += curr_html
        if not isnotebook():
            console = Console()
            curr_color = None
            if attribution.source_attributions is not None:
                curr_color = colors[idx]
                if display:
                    print("\n\n")
                    console.print(
                        get_heatmap_type(attribution, curr_color, "Source", use_html=False), overflow="ignore"
                    )
                if attribution.target_attributions is not None:
                    curr_color = colors[idx + 1]
            display_scores = attribution.source_attributions is None and attribution.step_scores
            if (attribution.target_attributions is not None or display_scores) and display:
                if curr_color is None and colors:
                    curr_color = colors[idx]
                print("\n\n")
                console.print(get_heatmap_type(attribution, curr_color, "Target", use_html=False), overflow="ignore")
        if any(x is None for x in [attribution.source_attributions, attribution.target_attributions]):
            idx += 1
        else:
            idx += 2
    if return_html:
        return html_out
def show_granular_attributions(
    attributions: "FeatureAttributionSequenceOutput",
    max_show_size: int = 20,
    min_val: int | None = None,
    max_val: int | None = None,
    show_dim: int | str | None = None,
    slice_dims: dict[int | str, tuple[int, int]] | None = None,
    display: bool = True,
    return_html: bool | None = False,
    return_figure: bool = False,
) -> str | None:
    """Visualizes granular attribution heatmaps in HTML format.
    Args:
        attributions (:class:`~inseq.data.attribution.FeatureAttributionSequenceOutput`):
            Sequence attributions to be visualized. Does not require pre-aggregation.
        min_val (:obj:`int`, *optional*, defaults to None):
            Lower attribution score threshold for color map.
        max_val (:obj:`int`, *optional*, defaults to None):
            Upper attribution score threshold for color map.
        max_show_size (:obj:`int`, *optional*, defaults to None):
            Maximum dimension size for additional dimensions to be visualized. Default: 20.
        show_dim (:obj:`int` or :obj:`str`, *optional*, defaults to None):
            Dimension to be visualized along with the source and target tokens. Can be either the dimension index or
            the dimension name. Works only if the dimension size is less than or equal to `max_show_size`.
        slice_dims (:obj:`dict[int or str, tuple[int, int]]`, *optional*, defaults to None):
            Dimensions to be sliced and visualized along with the source and target tokens. The dictionary should
            contain the dimension index or name as the key and the slice range as the value.
        display (:obj:`bool`, *optional*, defaults to True):
            Whether to show the output of the visualization function.
        return_html (:obj:`bool`, *optional*, defaults to False):
            If true, returns the HTML corresponding to the notebook visualization of the attributions in
            string format, for saving purposes.
        return_figure (:obj:`bool`, *optional*, defaults to False):
            If true, returns the Treescope figure object for further manipulation.
    Returns:
        `str`: Returns the HTML output if `return_html=True`
    """
    from inseq.data.attribution import FeatureAttributionSequenceOutput
    if isinstance(attributions, FeatureAttributionSequenceOutput):
        attributions: list["FeatureAttributionSequenceOutput"] = [attributions]
    if not isnotebook() and display:
        raise ValueError(
            "Granular attribution heatmaps visualization is  only supported in Jupyter notebooks. "
            "Please set `display=False` and `return_html=True` to avoid this error."
        )
    if return_html and return_figure:
        raise ValueError("Only one of `return_html` and `return_figure` can be set to True.")
    items_to_render = []
    for attribution in attributions:
        if attribution.source_attributions is not None:
            items_to_render += [
                fg.bolded("Source Saliency Heatmap"),
                get_saliency_heatmap_treescope(
                    attribution.source_attributions.numpy(),
                    [t.token for t in attribution.target[attribution.attr_pos_start : attribution.attr_pos_end]],
                    [t.token for t in attribution.source],
                    attribution._attribution_dim_names["source_attributions"],
                    max_show_size=max_show_size,
                    max_val=max_val,
                    min_val=min_val,
                    show_dim=show_dim,
                    slice_dims=slice_dims,
                ),
            ]
        if attribution.target_attributions is not None:
            items_to_render += [
                fg.bolded("Target Saliency Heatmap"),
                get_saliency_heatmap_treescope(
                    attribution.target_attributions.numpy(),
                    [t.token for t in attribution.target[attribution.attr_pos_start : attribution.attr_pos_end]],
                    [t.token for t in attribution.target],
                    attribution._attribution_dim_names["target_attributions"],
                    max_show_size=max_show_size,
                    max_val=max_val,
                    min_val=min_val,
                    show_dim=show_dim,
                    slice_dims=slice_dims,
                ),
            ]
        items_to_render.append("")
    fig = fg.inline(*items_to_render)
    if return_figure:
        return fig
    if display:
        ts.show(fig)
    if return_html:
        return ts.lowering.render_to_html_as_root(fg.treescope_part_from_display_object(fig))
def show_token_attributions(
    attributions: "FeatureAttributionSequenceOutput",
    min_val: int | None = None,
    max_val: int | None = None,
    display: bool = True,
    return_html: bool | None = False,
    return_figure: bool = False,
    replace_char: dict[str, str] | None = None,
    wrap_after: int | str | list[str] | tuple[str] | None = None,
    step_score_highlight: str | None = None,
):
    """Visualizes token-level attributions in HTML format.
    Args:
        attributions (:class:`~inseq.data.attribution.FeatureAttributionSequenceOutput`):
            Sequence attributions to be visualized.
        min_val (:obj:`Optional[int]`, *optional*, defaults to None):
            Lower attribution score threshold for color map.
        max_val (`Optional[int]`, *optional*, defaults to None):
            Upper attribution score threshold for color map.
        display (`bool`, *optional*, defaults to True):
            Whether to show the output of the visualization function.
        return_html (`Optional[bool]`, *optional*, defaults to False):
            If true, returns the HTML corresponding to the notebook visualization of the attributions in string format,
            for saving purposes.
        return_figure (`Optional[bool]`, *optional*, defaults to False):
            If true, returns the Treescope figure object for further manipulation.
        replace_char (`Optional[dict[str, str]]`, *optional*, defaults to None):
            Dictionary mapping strings to be replaced to replacement options, used for cleaning special characters.
            Default: {}.
        wrap_after (`Optional[int | str | list[str] | tuple[str]]`, *optional*, defaults to None):
            Token indices or tokens after which to wrap lines. E.g. 10 = wrap after every 10 tokens, "hi" = wrap after
            word hi occurs, ["." "!", "?"] or ".!?" = wrap after every sentence-ending punctuation.
        step_score_highlight (`Optional[str]`, *optional*, defaults to None):
            Name of the step score to use to highlight generated tokens in the visualization. If None, no highlights are
            shown. Default: None.
    """
    from inseq.data.attribution import FeatureAttributionSequenceOutput
    if isinstance(attributions, FeatureAttributionSequenceOutput):
        attributions: list["FeatureAttributionSequenceOutput"] = [attributions]
    if not isnotebook() and display:
        raise ValueError(
            "Token attribution visualization is only supported in Jupyter notebooks. "
            "Please set `display=False` and `return_html=True` to avoid this error."
        )
    if return_html and return_figure:
        raise ValueError("Only one of `return_html` and `return_figure` can be set to True.")
    if replace_char is None:
        replace_char = {}
    if max_val is None:
        max_val = max(attribution.maximum for attribution in attributions)
    if step_score_highlight is not None and (
        attributions[0].step_scores is None or step_score_highlight not in attributions[0].step_scores
    ):
        raise ValueError(
            f'The requested step score "{step_score_highlight}" is not available for highlights in the provided '
            "attribution object. Please set `step_score_highlight=None` or recompute `model.attribute` by passing "
            f'`step_scores=["{step_score_highlight}"].'
        )
    generated_token_parts = []
    for attr in attributions:
        cleaned_generated_tokens = clean_tokens(
            [t.token for t in attr.target[attr.attr_pos_start : attr.attr_pos_end]], replace_chars=replace_char
        )
        cleaned_input_tokens = clean_tokens([t.token for t in attr.source], replace_chars=replace_char)
        cleaned_target_tokens = clean_tokens([t.token for t in attr.target], replace_chars=replace_char)
        step_scores = None
        title = "Generated text:\n\n"
        if step_score_highlight is not None:
            step_scores = attr.step_scores[step_score_highlight]
            scores_vmax = step_scores.max().item()
            # Use different cmap to differentiate from attribution scores
            scores_cmap = (
                treescope_cmap("greens") if all(x >= 0 for x in step_scores) else treescope_cmap("brown_to_green")
            )
            title = f"Generated text with {step_score_highlight} highlights:\n\n"
        generated_token_parts.append(rp.custom_style(rp.text(title), css_style="font-weight: bold;"))
        for gen_idx, curr_gen_tok in enumerate(cleaned_generated_tokens):
            attributed_token_parts = [rp.text("\n")]
            if attr.source_attributions is not None:
                attributed_token_parts.append(
                    get_tokens_heatmap_treescope(
                        tokens=cleaned_input_tokens,
                        scores=attr.source_attributions[:, gen_idx].numpy(),
                        title=f'Source attributions for "{curr_gen_tok}"',
                        title_style="font-style: italic; color: #888888;",
                        min_val=min_val,
                        max_val=max_val,
                        wrap_after=wrap_after,
                    )
                )
                attributed_token_parts.append(rp.text("\n\n"))
            if attr.target_attributions is not None:
                attributed_token_parts.append(
                    get_tokens_heatmap_treescope(
                        tokens=cleaned_target_tokens[: attr.attr_pos_start + gen_idx],
                        scores=attr.target_attributions[:, gen_idx].numpy(),
                        title=f'Target attributions for "{curr_gen_tok}"',
                        title_style="font-style: italic; color: #888888;",
                        min_val=min_val,
                        max_val=max_val,
                        wrap_after=wrap_after,
                    )
                )
                attributed_token_parts.append(rp.text("\n\n"))
            if step_scores is not None:
                gen_tok_label = get_single_token_heatmap_treescope(
                    curr_gen_tok,
                    step_scores[gen_idx].item(),
                    max_val=scores_vmax,
                    colormap=scores_cmap,
                    show_empty_tokens=True,
                )[0]
            else:
                gen_tok_label = rp.text(curr_gen_tok)
            generated_token_parts.append(
                rp.build_full_line_with_annotations(
                    rp.build_custom_foldable_tree_node(
                        label=gen_tok_label,
                        contents=rp.fold_condition(
                            collapsed=rp.text(" "),
                            expanded=rp.indented_children([rp.siblings(*attributed_token_parts)]),
                        ),
                    )
                )
            )
    fig = fg.figure_from_treescope_rendering_part(
        rp.custom_style(rp.siblings(*generated_token_parts), css_style="white-space: pre-wrap")
    )
    if return_figure:
        return fig
    if display:
        ts.show(fig)
    if return_html:
        return ts.lowering.render_to_html_as_root(fg.treescope_part_from_display_object(fig))
def get_attribution_colors(
    attributions: list["FeatureAttributionSequenceOutput"],
    min_val: int | None = None,
    max_val: int | None = None,
    cmap: str | Colormap | None = None,
    return_alpha: bool = True,
    return_strings: bool = True,
) -> list[list[list[str | tuple[float, float, float]]]]:
    """A list (one element = one sentence) of lists (one element = attributions for one token)
    of lists (one element = one attribution) of colors. Colors are either strings or RGB(A) tuples.
    """
    if max_val is None:
        max_val = max(attribution.maximum for attribution in attributions)
    if min_val is None:
        min_val = -max_val
    colors = []
    for attribution in attributions:
        if attribution.source_attributions is not None:
            colors.append(
                get_colors(
                    attribution.source_attributions.numpy(), min_val, max_val, cmap, return_alpha, return_strings
                )
            )
        if attribution.target_attributions is not None:
            colors.append(
                get_colors(
                    attribution.target_attributions.numpy(), min_val, max_val, cmap, return_alpha, return_strings
                )
            )
    return colors
def get_heatmap_type(
    attribution: "FeatureAttributionSequenceOutput",
    colors,
    heatmap_type: Literal["Source", "Target"] = "Source",
    use_html: bool = False,
) -> str:
    heatmap_func = get_saliency_heatmap_html if use_html else get_saliency_heatmap_rich
    step_scores = None
    if attribution.step_scores is not None:
        step_scores = {k: v.numpy() for k, v in attribution.step_scores.items()}
    if heatmap_type == "Source":
        return heatmap_func(
            attribution.source_attributions.numpy(),
            [t.token for t in attribution.target[attribution.attr_pos_start : attribution.attr_pos_end]],  # noqa
            [t.token for t in attribution.source],
            colors,
            step_scores,
            label="Source",
        )
    elif heatmap_type == "Target":
        if attribution.target_attributions is not None:
            target_attributions = attribution.target_attributions.numpy()
        else:
            target_attributions = None
        return heatmap_func(
            target_attributions,
            [t.token for t in attribution.target[attribution.attr_pos_start : attribution.attr_pos_end]],  # noqa
            [t.token for t in attribution.target],
            colors,
            step_scores,
            label="Target",
        )
    else:
        raise ValueError(f"Type {heatmap_type} is not supported.")
def get_saliency_heatmap_html(
    scores: np.ndarray | None,
    column_labels: list[str],
    row_labels: list[str],
    input_colors: list[list[str]],
    step_scores: dict[str, np.ndarray] | None = None,
    label: str = "",
    step_scores_threshold: float | dict[str, float] = 0.5,
):
    # unique ID added to HTML elements and function to avoid collision of differnent instances
    uuid = "".join(random.choices(string.ascii_lowercase, k=20))
    out = saliency_heatmap_table_header
    # add top row containing target tokens
    out += "<tr><th></th><th></th>"
    for column_idx in range(len(column_labels)):
        out += f"<th>{column_idx}</th>"
    out += "</tr><tr><th></th><th></th>"
    for column_label in column_labels:
        out += f"<th>{sanitize_html(column_label)}</th>"
    out += "</tr>"
    if scores is not None:
        for row_index in range(scores.shape[0]):
            out += f"<tr><th>{row_index}</th><th>{sanitize_html(row_labels[row_index])}</th>"
            for col_index in range(scores.shape[1]):
                score = ""
                if not np.isnan(scores[row_index, col_index]):
                    score = round(float(scores[row_index][col_index]), 3)
                out += f'<th style="background:{input_colors[row_index][col_index]}">{score}</th>'
            out += "</tr>"
    if step_scores is not None:
        for step_score_name, step_score_values in step_scores.items():
            out += f'<tr style="outline: thin solid"><th></th><th><b>{step_score_name}</b></th>'
            if isinstance(step_scores_threshold, float):
                threshold = step_scores_threshold
            else:
                threshold = step_scores_threshold.get(step_score_name, 0.5)
            style = lambda val, limit: abs(val) >= limit and isinstance(val, float)
            for col_index in range(len(column_labels)):
                if isinstance(step_score_values[col_index].item(), float):
                    score = round(step_score_values[col_index].item(), 3)
                else:
                    score = step_score_values[col_index].item()
                is_bold = style(score, threshold)
                out += f'<th>{"<b>" if is_bold else ""}{score}{"</b>" if is_bold else ""}</th>'
    out += "</table>"
    saliency_heatmap_markup = saliency_heatmap_html.format(uuid=uuid, content=out, label=label)
    plot_uuid = "".join(random.choices(string.ascii_lowercase, k=20))
    return final_plot_html.format(
        uuid=plot_uuid,
        saliency_plot_markup=saliency_heatmap_markup,
    )
def get_saliency_heatmap_rich(
    scores: np.ndarray | None,
    column_labels: list[str],
    row_labels: list[str],
    input_colors: list[list[str]],
    step_scores: dict[str, np.ndarray] | None = None,
    label: str = "",
    step_scores_threshold: float | dict[str, float] = 0.5,
):
    columns = [
        Column(header="", justify="right", overflow="fold"),
        Column(header="", justify="right", overflow="fold"),
    ]
    for idx, column_label in enumerate(column_labels):
        columns.append(Column(header=f"{idx}\n{escape(column_label)}", justify="center", overflow="fold"))
    table = Table(
        *columns,
        title=f"{label + ' ' if label else ''}Saliency Heatmap",
        caption="â†’ : Generated tokens, â†“ : Attributed tokens",
        padding=(0, 1, 0, 1),
        show_lines=False,
        box=box.HEAVY_HEAD,
    )
    if scores is not None:
        for row_index in range(scores.shape[0]):
            row = [Text(f"{row_index}", style="bold"), Text(escape(row_labels[row_index]), style="bold")]
            for col_index in range(scores.shape[1]):
                color = Color.from_rgb(*input_colors[row_index][col_index])
                score = ""
                if not np.isnan(scores[row_index][col_index]):
                    score = round(float(scores[row_index][col_index]), 2)
                row.append(Text(f"{score}", justify="center", style=Style(color=color)))
            table.add_row(*row, end_section=row_index == scores.shape[0] - 1)
    if step_scores is not None:
        for step_score_name, step_score_values in step_scores.items():
            if isinstance(step_scores_threshold, float):
                threshold = step_scores_threshold
            else:
                threshold = step_scores_threshold.get(step_score_name, 0.5)
            style = lambda val, limit: "bold" if abs(val) >= limit and isinstance(val, float) else ""
            score_row = [Text(""), Text(escape(step_score_name), style="bold")]
            for score in step_score_values:
                curr_score = round(score.item(), 2) if isinstance(score, float) else score.item()
                score_row.append(Text(f"{score:.2f}", justify="center", style=style(curr_score, threshold)))
            table.add_row(*score_row, end_section=True)
    return table
def get_saliency_heatmap_treescope(
    scores: np.ndarray | None,
    column_labels: list[str],
    row_labels: list[str],
    dim_names: dict[int, str] | None = None,
    max_show_size: int | None = None,
    max_val: float | None = None,
    min_val: float | None = None,
    show_dim: int | str | None = None,
    slice_dims: dict[int | str, tuple[int, int]] | None = None,
):
    if max_show_size is None:
        max_show_size = 20
    if dim_names is None:
        dim_names = {}
    item_labels_dict = {0: row_labels, 1: column_labels}
    rev_dim_names = {v: k for k, v in dim_names.items()}
    col_dims = [1]
    slider_dims = []
    if slice_dims is not None:
        slices = [slice(None)] * scores.ndim
        for dim_name, slice_idxs in slice_dims.items():
            dim_idx = test_dim(dim_name, dim_names, rev_dim_names, scores)
            slices[dim_idx] = slice(slice_idxs[0], slice_idxs[1])
        scores = scores[tuple(slices)]
    if show_dim is not None:
        show_dim_idx = test_dim(show_dim, dim_names, rev_dim_names, scores)
        if scores.shape[show_dim_idx] > max_show_size:
            raise ValueError(
                f"Dimension {show_dim_idx} has size {scores.shape[show_dim_idx]} which is greater than the maximum "
                f"show size {max_show_size}. Please choose a different dimension or slice the tensor before "
                "visualizing it using SliceAggregator."
            )
        col_dims.append(show_dim_idx)
    for dim_idx, dim_name in dim_names.items():
        if dim_idx > 1:
            if scores.shape[dim_idx] <= max_show_size and len(col_dims) < 2:
                col_dims.append(dim_idx)
            else:
                slider_dims.append(dim_idx)
            item_labels_dict[dim_idx] = [f"{dim_name} #{i}" for i in range(scores.shape[dim_idx])]
    return ts.render_array(
        scores,
        rows=[0],
        columns=col_dims,
        sliders=slider_dims,
        axis_labels={k: f"{v}: {scores.shape[k]}" for k, v in dim_names.items()},
        axis_item_labels=item_labels_dict,
        vmax=max_val,
        vmin=min_val,
    )
def get_single_token_heatmap_treescope(
    token: str,
    score: float,
    min_val: float | None = None,
    max_val: float = 1,
    rounding: int = 4,
    colormap: list[tuple[int, int, int]] | None = None,
    strip_chars: dict[str, str] = {},
    show_empty_tokens: bool = False,
    return_highlighted_idx: bool = False,
) -> list[rp.RenderableTreePart] | tuple[list[rp.RenderableTreePart], int]:
    parts = [None]
    idx_highlight = 0
    curr_tok = token
    for char, repl in strip_chars.items():
        if curr_tok.startswith(char):
            curr_tok = curr_tok.lstrip(char)
            parts = [rp.text(repl)] + parts
            idx_highlight += 1
        if curr_tok.endswith(char):
            curr_tok = curr_tok.rstrip(char)
            parts.append(rp.text(repl))
    if (show_empty_tokens and token != "") or curr_tok != "":
        show_token = token if show_empty_tokens and curr_tok == "" else curr_tok
    else:
        show_token = " "
    highlighted_text = fg.treescope_part_from_display_object(
        fg.text_on_color(show_token, value=round(score, rounding), vmin=min_val, vmax=max_val, colormap=colormap)
    )
    parts[idx_highlight] = highlighted_text
    if return_highlighted_idx:
        return parts, idx_highlight, show_token
    return parts
def get_tokens_heatmap_treescope(
    tokens: list[str],
    scores: np.ndarray,
    title: str | None = None,
    title_style: str | None = None,
    min_val: float | None = None,
    max_val: float = 1,
    rounding: int = 4,
    wrap_after: int | str | list[str] | tuple[str] | None = None,
    colormap: str | list[tuple[int, int, int]] = "blue_to_red",
    strip_chars: dict[str, str] = {},
    show_empty_tokens: bool = True,
):
    parts = []
    if title is not None:
        parts.append(rp.custom_style(rp.text(title + ":\n"), css_style=title_style))
    if isinstance(colormap, str):
        colormap = treescope_cmap(colormap)
    if not isinstance(colormap, list):
        raise ValueError("If specified, colormap must be a string or a list of RGB tuples.")
    for tok_idx, tok in enumerate(tokens):
        parts += get_single_token_heatmap_treescope(
            tok,
            scores[tok_idx],
            min_val=min_val,
            max_val=max_val,
            rounding=rounding,
            colormap=colormap,
            strip_chars=strip_chars,
            show_empty_tokens=show_empty_tokens,
        )
        parts += maybe_add_linebreak(tok, tok_idx, wrap_after)
    return rp.siblings(*parts)
# Progress bar utilities
def get_progress_bar(
    sequences: TextSequences,
    target_lengths: list[int],
    method_name: str,
    show: bool,
    pretty: bool,
    attr_pos_start: int,
    attr_pos_end: int,
) -> tqdm | tuple[Progress, Live] | None:
    if not show:
        return None
    elif show and not pretty:
        return tqdm(
            total=attr_pos_end,
            desc=f"Attributing with {method_name}...",
            initial=attr_pos_start,
        )
    elif show and pretty:
        job_progress = Progress(
            TextColumn("{task.description}", table_column=Column(ratio=3, no_wrap=False)),
            BarColumn(table_column=Column(ratio=1)),
            TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
            TimeRemainingColumn(),
        )
        for idx, (tgt, tgt_len) in enumerate(zip(sequences.targets, target_lengths, strict=False)):
            clean_tgt = escape(tgt.replace("\n", "\\n"))
            job_progress.add_task(f"{idx}. {clean_tgt}", total=tgt_len)
        progress_table = Table.grid()
        row_contents = [
            Panel.fit(
                job_progress,
                title=f"[b]Attributing with {escape(method_name)}",
                border_style="green",
                padding=(1, 2),
            )
        ]
        if sequences.sources is not None:
            sources = []
            for idx, src in enumerate(sequences.sources):
                clean_src = escape(src.replace("\n", "\\n"))
                sources.append(f"{idx}. {clean_src}")
            row_contents = [
                Panel.fit(
                    "\n".join(sources),
                    title="Source sentences",
                    border_style="red",
                    padding=(1, 2),
                )
            ] + row_contents
        progress_table.add_row(*row_contents)
        live = Live(Padding(progress_table, (1, 0, 1, 0)), refresh_per_second=10)
        live.start(refresh=live._renderable is not None)
        return job_progress, live
def update_progress_bar(
    pbar: tqdm | tuple[Progress, Live] | None,
    skipped_prefixes: list[str] | None = None,
    attributed_sentences: list[str] | None = None,
    unattributed_suffixes: list[str] | None = None,
    skipped_suffixes: list[str] | None = None,
    whitespace_indexes: list[list[int]] = None,
    show: bool = False,
    pretty: bool = False,
) -> None:
    if not show:
        return
    elif show and not pretty:
        pbar.update(1)
    else:
        split_targets = (skipped_prefixes, attributed_sentences, unattributed_suffixes, skipped_suffixes)
        for job in pbar[0].tasks:
            if not job.finished:
                pbar[0].advance(job.id)
                formatted_desc = f"{job.id}. "
                past_length = 0
                for split, color in zip(split_targets, ["grey58", "green", "orange1", "grey58"], strict=False):
                    if split[job.id]:
                        formatted_desc += f"[{color}]" + escape(split[job.id].replace("\n", "\\n")) + "[/]"
                        past_length += len(split[job.id])
                        if past_length in whitespace_indexes[job.id]:
                            formatted_desc += " "
                            past_length += 1
                pbar[0].update(job.id, description=formatted_desc, refresh=True)
def close_progress_bar(pbar: tqdm | tuple[Progress, Live] | None, show: bool, pretty: bool) -> None:
    if not show:
        return
    elif show and not pretty:
        pbar.close()
    else:
        _, live = pbar
        live.stop()

================
File: inseq/models/__init__.py
================
import logging
from typing import Optional, Union
from rich.status import Status
from ..utils import isnotebook, optional
from ..utils.typing import ModelClass, ModelIdentifier
from .attribution_model import AttributionModel, InputFormatter
from .decoder_only import DecoderOnlyAttributionModel
from .encoder_decoder import EncoderDecoderAttributionModel
from .huggingface_model import HuggingfaceDecoderOnlyModel, HuggingfaceEncoderDecoderModel, HuggingfaceModel
from .model_config import ModelConfig, register_model_config
logger = logging.getLogger(__name__)
FRAMEWORKS_MAP = {
    "hf_transformers": HuggingfaceModel,
}
def load_model(
    model: ModelIdentifier | ModelClass,
    attribution_method: str | None = None,
    framework: str = "hf_transformers",
    **kwargs,
) -> AttributionModel:
    """Factory function to load a model with or without attribution methods.
    Args:
        model (`Union[ModelIdentifier, ModelClass]`):
            Either a model identifier (e.g. `gpt2` in HF transformers) or an instance of a model class supported by the
            selected modeling framework.
        attribution_method (`Optional[str]`, *optional*, defaults to None):
            Identifier for the attribution method to use. If `None`, the model will be loaded without any attribution
            methods, which can be added during attribution.
        framework (`str`, *optional*, defaults to "hf_transformers"):
            The framework to use for loading the model. Currently, only HF transformers is supported.
    Returns:
        `AttributionModel`: An instance of one of `AttributionModel` children classes matching the selected framework
        and model architecture.
    """
    model_name = model if isinstance(model, str) else "model"
    method_desc = f"with {attribution_method} method..." if attribution_method else " without attribution methods..."
    load_msg = f"Loading {model_name} {method_desc}"
    with optional(not isnotebook(), Status(load_msg), logger.info, msg=load_msg):
        return FRAMEWORKS_MAP[framework].load(model, attribution_method, **kwargs)
def list_supported_frameworks() -> list[str]:
    """Lists identifiers for all available frameworks. These can be used to load models with the `framework` argument
    in the :meth:`~inseq.load_model` function.
    """
    return list(FRAMEWORKS_MAP.keys())
__all__ = [
    "AttributionModel",
    "InputFormatter",
    "HuggingfaceModel",
    "HuggingfaceEncoderDecoderModel",
    "HuggingfaceDecoderOnlyModel",
    "DecoderOnlyAttributionModel",
    "EncoderDecoderAttributionModel",
    "load_model",
    "list_supported_frameworks",
    "ModelConfig",
    "register_model_config",
]

================
File: inseq/models/attribution_model.py
================
import logging
from abc import ABC, abstractmethod
from collections.abc import Callable
from functools import wraps
from typing import Any, Protocol, TypeVar
import torch
from ..attr import STEP_SCORES_MAP, StepFunctionArgs
from ..attr.feat import FeatureAttribution, extract_args, join_token_ids
from ..data import (
    BatchEncoding,
    DecoderOnlyBatch,
    EncoderDecoderBatch,
    FeatureAttributionInput,
    FeatureAttributionOutput,
    FeatureAttributionStepOutput,
    merge_attributions,
)
from ..utils import (
    MissingAttributionMethodError,
    check_device,
    format_input_texts,
    get_adjusted_alignments,
    get_default_device,
    isnotebook,
    pretty_tensor,
)
from ..utils.typing import (
    EmbeddingsTensor,
    ExpandedTargetIdsTensor,
    IdsTensor,
    LogitsTensor,
    OneOrMoreIdSequences,
    OneOrMoreTokenSequences,
    SingleScorePerStepTensor,
    TargetIdsTensor,
    TextInput,
    TextSequences,
    TokenWithId,
    VocabularyEmbeddingsTensor,
)
from .model_config import ModelConfig
from .model_decorators import unhooked
ModelOutput = TypeVar("ModelOutput")
CustomForwardOutput = TypeVar("CustomForwardOutput")
logger = logging.getLogger(__name__)
class ForwardMethod(Protocol):
    def __call__(
        self,
        batch: DecoderOnlyBatch | EncoderDecoderBatch,
        target_ids: ExpandedTargetIdsTensor,
        attributed_fn: Callable[..., SingleScorePerStepTensor],
        use_embeddings: bool,
        attributed_fn_argnames: list[str] | None,
        *args,
    ) -> CustomForwardOutput:
        ...
class InputFormatter:
    @staticmethod
    @abstractmethod
    def prepare_inputs_for_attribution(
        attribution_model: "AttributionModel",
        inputs: FeatureAttributionInput,
        include_eos_baseline: bool = False,
        skip_special_tokens: bool = False,
    ) -> DecoderOnlyBatch | EncoderDecoderBatch:
        raise NotImplementedError()
    @staticmethod
    @abstractmethod
    def format_attribution_args(
        batch: DecoderOnlyBatch | EncoderDecoderBatch,
        target_ids: TargetIdsTensor,
        attributed_fn: Callable[..., SingleScorePerStepTensor],
        attribute_target: bool = False,
        attributed_fn_args: dict[str, Any] = {},
        attribute_batch_ids: bool = False,
        forward_batch_embeds: bool = True,
        use_baselines: bool = False,
    ) -> tuple[dict[str, Any], tuple[IdsTensor | EmbeddingsTensor | None, ...]]:
        raise NotImplementedError()
    @staticmethod
    @abstractmethod
    def enrich_step_output(
        attribution_model: "AttributionModel",
        step_output: FeatureAttributionStepOutput,
        batch: DecoderOnlyBatch | EncoderDecoderBatch,
        target_tokens: OneOrMoreTokenSequences,
        target_ids: TargetIdsTensor,
        contrast_batch: DecoderOnlyBatch | None = None,
        contrast_targets_alignments: list[list[tuple[int, int]]] | None = None,
    ) -> FeatureAttributionStepOutput:
        r"""Enriches the attribution output with token information, producing the finished
        :class:`~inseq.data.FeatureAttributionStepOutput` object.
        Args:
            step_output (:class:`~inseq.data.FeatureAttributionStepOutput`): The output produced
                by the attribution step, with missing batch information.
            batch (:class:`~inseq.data.DecoderOnlyBatch` or :class:`~inseq.data.EncoderDecoderOnlyBatch`): The batch on
                which attribution was performed.
            target_ids (:obj:`torch.Tensor`): Target token ids of size `(batch_size, 1)` corresponding to tokens
                for which the attribution step was performed.
        Returns:
            :class:`~inseq.data.FeatureAttributionStepOutput`: The enriched attribution output.
        """
        raise NotImplementedError()
    @staticmethod
    @abstractmethod
    def convert_args_to_batch(args: StepFunctionArgs = None, **kwargs) -> DecoderOnlyBatch | EncoderDecoderBatch:
        raise NotImplementedError()
    @staticmethod
    def format_forward_args(forward: ForwardMethod) -> Callable[..., CustomForwardOutput]:
        @wraps(forward)
        def formatted_forward_input_wrapper(self, *args, **kwargs) -> CustomForwardOutput:
            raise NotImplementedError()
        return formatted_forward_input_wrapper
    @staticmethod
    @abstractmethod
    def format_step_function_args(
        attribution_model: "AttributionModel",
        forward_output: ModelOutput,
        target_ids: ExpandedTargetIdsTensor,
        batch: DecoderOnlyBatch,
        is_attributed_fn: bool = False,
    ) -> StepFunctionArgs:
        raise NotImplementedError()
    @staticmethod
    @abstractmethod
    def get_text_sequences(
        attribution_model: "AttributionModel", batch: DecoderOnlyBatch | EncoderDecoderBatch
    ) -> TextSequences:
        raise NotImplementedError()
    @staticmethod
    @abstractmethod
    def get_step_function_reserved_args() -> list[str]:
        raise NotImplementedError()
    @staticmethod
    def format_contrast_targets_alignments(
        contrast_targets_alignments: list[tuple[int, int]] | list[list[tuple[int, int]]] | str,
        target_sequences: list[str],
        target_tokens: list[list[str]],
        contrast_sequences: list[str],
        contrast_tokens: list[list[str]],
        special_tokens: list[str] = [],
        start_pos: int = 0,
        end_pos: int | None = None,
    ) -> tuple[DecoderOnlyBatch, list[list[tuple[int, int]]] | None]:
        # Ensure that the contrast_targets_alignments are in the correct format (list of lists of idxs pairs)
        if contrast_targets_alignments:
            if isinstance(contrast_targets_alignments, list) and len(contrast_targets_alignments) > 0:
                if isinstance(contrast_targets_alignments[0], tuple):
                    contrast_targets_alignments = [contrast_targets_alignments]
                if not isinstance(contrast_targets_alignments[0], list):
                    raise ValueError("Invalid contrast_targets_alignments were provided.")
            elif not isinstance(contrast_targets_alignments, str):
                raise ValueError("Invalid contrast_targets_alignments were provided.")
        adjusted_alignments = []
        aligns = contrast_targets_alignments
        for seq_idx, (tgt_seq, tgt_tok, c_seq, c_tok) in enumerate(
            zip(target_sequences, target_tokens, contrast_sequences, contrast_tokens, strict=False)
        ):
            if isinstance(contrast_targets_alignments, list):
                aligns = contrast_targets_alignments[seq_idx]
            adjusted_alignments.append(
                get_adjusted_alignments(
                    aligns,
                    target_sequence=tgt_seq,
                    target_tokens=tgt_tok,
                    contrast_sequence=c_seq,
                    contrast_tokens=c_tok,
                    fill_missing=True,
                    special_tokens=special_tokens,
                    start_pos=start_pos,
                    end_pos=end_pos,
                )
            )
        return adjusted_alignments
class AttributionModel(ABC, torch.nn.Module):
    """Base class for all attribution models.
    Attributes:
        model: The wrapped model to be attributed.
        model_name (:obj:`str`): The name of the model.
        is_encoder_decoder (:obj:`bool`): Whether the model is an encoder-decoder model.
        pad_token (:obj:`str`): The pad token used by the model.
        embed_scale (:obj:`float`): Value used to scale the embeddings.
        device (:obj:`str`): The device on which the model is located.
        attribution_method (:class:`~inseq.attr.FeatureAttribution`): The attribution method used alongside the model.
        is_hooked (:obj:`bool`): Whether the model is currently hooked by the attribution method.
        default_attributed_fn_id (:obj:`str`): The id for the default step function used as attribution target.
    """
    formatter = InputFormatter
    def __init__(self, **kwargs) -> None:
        super().__init__()
        if not hasattr(self, "model"):
            self.model = None
            self.model_name: str = None
            self.is_encoder_decoder: bool = True
        self.pad_token: str | None = None
        self.embed_scale: float | None = None
        self._device: str | None = None
        self.device_map: dict[str, str | int | torch.device] | None = None
        self.attribution_method: FeatureAttribution | None = None
        self.is_hooked: bool = False
        self._default_attributed_fn_id: str = "probability"
        self.config: ModelConfig | None = None
        self.is_distributed: bool | None = None
    @property
    def device(self) -> str | None:
        return self._device
    @device.setter
    def device(self, new_device: str) -> None:
        check_device(new_device)
        self._device = new_device
        if self.model:
            self.model.to(self._device)
    def setup(self, device: str | None = None, attribution_method: str | None = None, **kwargs) -> None:
        """Move the model to device and in eval mode."""
        self.device = device if device is not None else get_default_device()
        if self.model:
            self.model.eval()
            self.model.zero_grad()
            self.attribution_method = self.get_attribution_method(attribution_method, **kwargs)
            self.is_distributed = self.model.__class__.__name__.startswith("Distributed")
    @property
    def default_attributed_fn_id(self) -> str:
        return self._default_attributed_fn_id
    @default_attributed_fn_id.setter
    def set_attributed_fn(self, fn: str):
        if fn not in STEP_SCORES_MAP:
            raise ValueError(f"Unknown function: {fn}. Register custom functions with inseq.register_step_function")
        self._default_attributed_fn_id = fn
    @property
    def info(self) -> dict[str | None, str | None]:
        return {
            "model_name": self.model_name,
            "model_class": self.model.__class__.__name__ if self.model is not None else None,
        }
    def get_attribution_method(
        self,
        method: str | None = None,
        override_default_attribution: bool | None = False,
        **kwargs,
    ) -> FeatureAttribution:
        # No method present -> missing method error
        if not method:
            if not self.attribution_method:
                raise MissingAttributionMethodError()
        else:
            if self.attribution_method:
                self.attribution_method.unhook()
            # If either the default method is missing or the override is set,
            # set the default method to the given method
            if override_default_attribution or not self.attribution_method:
                self.attribution_method = FeatureAttribution.load(method, attribution_model=self, **kwargs)
            # Temporarily use the current method without overriding the default
            else:
                return FeatureAttribution.load(method, attribution_model=self, **kwargs)
        return self.attribution_method
    def get_attributed_fn(
        self, attributed_fn: str | Callable[..., SingleScorePerStepTensor] | None = None
    ) -> Callable[..., SingleScorePerStepTensor]:
        if attributed_fn is None:
            attributed_fn = self.default_attributed_fn_id
        if isinstance(attributed_fn, str):
            if attributed_fn not in STEP_SCORES_MAP:
                raise ValueError(
                    f"Unknown function: {attributed_fn}. Register custom functions with inseq.register_step_function"
                )
            attributed_fn = STEP_SCORES_MAP[attributed_fn]
        return attributed_fn
    def validate_attribute_args(
        self,
        input_texts: TextInput,
        generated_texts: TextInput,
        has_generated_texts: bool,
        attribution_method: FeatureAttribution,
        batch_size: int,
        attr_pos_start: int | None,
        attr_pos_end: int | None,
    ) -> int:
        logger.debug(f"reference_texts={generated_texts}")
        if not self.is_encoder_decoder:
            error_input_gen_mismatch = "Forced generations of decoder-only models must start with the input texts:\n\n"
            mismatch_seqs = []
            for idx in range(len(input_texts)):
                if not generated_texts[idx].startswith(input_texts[idx]):
                    mismatch_seqs.append(f"{repr(input_texts[idx])}\n!=\n{repr(generated_texts[idx])}")
            assert len(mismatch_seqs) == 0, error_input_gen_mismatch + "\n\n".join(mismatch_seqs)
            if has_generated_texts and len(input_texts) > 1:
                logger.warning(
                    "Batched constrained decoding is currently not supported for decoder-only models."
                    " Using batch size of 1."
                )
                batch_size = 1
            if len(input_texts) > 1 and (attr_pos_start is not None or attr_pos_end is not None):
                logger.warning(
                    "Custom attribution positions are currently not supported when batching generations for"
                    " decoder-only models. Using batch size of 1."
                )
                batch_size = 1
        elif attribution_method.is_final_step_method and len(input_texts) > 1:
            logger.warning(
                "Batched attribution with encoder-decoder models currently not supported for final-step methods."
                " Using batch size of 1."
            )
            batch_size = 1
        if attribution_method.method_name == "lime":
            logger.warning("Batched attribution currently not supported for LIME. Using batch size of 1.")
            batch_size = 1
        return batch_size
    def attribute(
        self,
        input_texts: TextInput,
        generated_texts: TextInput | None = None,
        method: str | None = None,
        override_default_attribution: bool | None = False,
        attr_pos_start: int | None = None,
        attr_pos_end: int | None = None,
        show_progress: bool = True,
        pretty_progress: bool = True,
        output_step_attributions: bool = False,
        attribute_target: bool = False,
        step_scores: list[str] = [],
        include_eos_baseline: bool = False,
        attributed_fn: str | Callable[..., SingleScorePerStepTensor] | None = None,
        device: str | None = None,
        batch_size: int | None = None,
        generate_from_target_prefix: bool = False,
        skip_special_tokens: bool = False,
        clean_special_chars: bool = False,
        generation_args: dict[str, Any] = {},
        **kwargs,
    ) -> FeatureAttributionOutput:
        """Perform sequential attribution of input texts for every token in generated texts using the specified method.
        Args:
            input_texts (:obj:`str` or :obj:`list(str)`): One or more input texts to be attributed.
            generated_texts (:obj:`str` or :obj:`list(str)`, `optional`): One or more generated texts to be used as
                targets for the attribution. Must match the number of input texts. If not provided, the model will be
                used to generate the texts from the input texts (default behavior). Specifying this argument enables
                attribution for constrained decoding, which should be interpreted carefully in presence of
                distributional shifts compared to natural generations (`Vamvas and Sennrich, 2021
                <https://doi.org/10.18653/v1/2021.blackboxnlp-1.5>`__).
            method (:obj:`str`, `optional`): The identifier associated to the attribution method to use.
                If not provided, the default attribution method specified when initializing the model will be used.
            override_default_attribution (:obj:`bool`, `optional`): Whether to override the default attribution method
                specified when initializing the model permanently, or to use the method above for a single attribution.
            attr_pos_start (:obj:`int`, `optional`): The starting position of the attribution. If not provided, the
                whole input text will be attributed. Allows for span-targeted attribution of generated texts.
            attr_pos_end (:obj:`int`, `optional`): The ending position of the attribution. If not provided, the
                whole input text will be attributed. Allows for span-targeted attribution of generated texts.
            show_progress (:obj:`bool`): Whether to show a progress bar for the attribution, default True.
            pretty_progress (:obj:`bool`, `optional`): Whether to show a pretty progress bar for the attribution.
                Automatically set to False for IPython environments due to visualization issues. If False, a simple
                tqdm progress bar will be used. default: True.
            output_step_attributions (:obj:`bool`, `optional`): Whether to fill the ``step_attributions`` field in
                :class:`~inseq.FeatureAttributionOutput` with step-wise attributions for each generated token. default:
                False.
            attribute_target (:obj:`bool`, `optional`): Specific to encoder-decoder models. Whether to attribute the
                target prefix alongside the input text. default: False. Note that an encoder-decoder attribution not
                accounting for the target prefix does not correctly reflect the overall input importance, since part of
                the input is not included in the attribution.
            step_scores (:obj:`list(str)`): A list of step function identifiers specifying the step scores to be
                computed alongside the attribution process. Available step functions are listed in
                :func:`~inseq.list_step_functions`.
            include_eos_baseline (:obj:`bool`, `optional`): Whether to include the EOS token in attributed tokens when
                using an attribution method requiring a baseline. default: False.
            attributed_fn (:obj:`str` or :obj:`Callable`, `optional`): The identifier associated to the step function
                to be used as attribution target. If not provided, the one specified in ``default_attributed_fn_id`` (
                model default) will be used. If the provided string is not a registered step function, an error will be
                raised. If a callable is provided, it must be a function matching the requirements for a step function.
            device (:obj:`str`, `optional`): The device to use for the attribution. If not provided, the default model
                device will be used.
            batch_size (:obj:`int`, `optional`): The batch size to use to dilute the attribution computation over the
                set of inputs. If no batch size is provided, the full set of input texts will be attributed at once.
            generate_from_target_prefix (:obj:`bool`, `optional`): Whether the ``generated_texts`` should be used as
                target prefixes for the generation process. If False, the ``generated_texts`` will be used as full
                targets. This option is only available for encoder-decoder models, since the same behavior can be
                achieved by modifying the input texts for decoder-only models. Default: False.
            skip_special_tokens (:obj:`bool`, `optional`): Whether to skip special tokens when attributing the input
                texts. Default: False.
            clean_special_chars (:obj:`bool`, `optional`): Whether to clean special characters from the input and
                generated texts. Default: False.
            **kwargs: Additional keyword arguments. These can include keyword arguments for the attribution method, for
                the generation process or for the attributed function. Generation arguments can be provided explicitly
                as a dictionary named ``generation_args``.
        Returns:
            :class:`~inseq.FeatureAttributionOutput`: The attribution output object containing the attribution scores,
            step-scores, optionally step-wise attributions and general information concerning attributed texts and the
            attribution process.
        """
        if self.is_encoder_decoder and not input_texts:
            raise ValueError("At least one text must be provided to perform attribution.")
        if attribute_target and not self.is_encoder_decoder:
            logger.warning("attribute_target parameter is set to True, but will be ignored (not an encoder-decoder).")
            attribute_target = False
        if generate_from_target_prefix and not self.is_encoder_decoder:
            logger.warning(
                "generate_from_target_prefix parameter is set to True, but will be ignored (not an encoder-decoder)."
            )
            generate_from_target_prefix = False
        original_device = self.device
        if device is not None:
            self.device = device
        attribution_method = self.get_attribution_method(method, override_default_attribution)
        attributed_fn = self.get_attributed_fn(attributed_fn)
        if skip_special_tokens:
            kwargs["skip_special_tokens"] = True
        attribution_args, attributed_fn_args, step_scores_args = extract_args(
            attribution_method,
            attributed_fn,
            step_scores,
            default_args=self.formatter.get_step_function_reserved_args(),
            **kwargs,
        )
        if isnotebook():
            logger.debug("Pretty progress currently not supported in notebooks, falling back to tqdm.")
            pretty_progress = False
        if attribution_method.is_final_step_method:
            if step_scores:
                raise ValueError(
                    "Step scores are not supported for final step methods since they do not iterate over the full"
                    " sequence. Please remove the step scores and compute them separatly passing method='dummy'."
                )
        input_texts, generated_texts = format_input_texts(
            input_texts, generated_texts, skip_special_tokens, self.special_tokens
        )
        has_generated_texts = generated_texts is not None
        if not self.is_encoder_decoder:
            for i in range(len(input_texts)):
                if not input_texts[i]:
                    input_texts[i] = self.bos_token
                    if has_generated_texts and not generated_texts[i].startswith(self.bos_token):
                        generated_texts[i] = " ".join([self.bos_token, generated_texts[i]])
        if batch_size is not None:
            n_batches = len(input_texts) // batch_size + ((len(input_texts) % batch_size) > 0)
            logger.info(f"Splitting input texts into {n_batches} batches of size {batch_size}.")
        # If constrained decoding is not enabled, output texts are generated from input texts.
        if not has_generated_texts or generate_from_target_prefix:
            encoded_input = self.encode(
                input_texts,
                return_baseline=True,
                include_eos_baseline=include_eos_baseline,
                add_special_tokens=not skip_special_tokens,
            )
            if generate_from_target_prefix:
                decoder_input = self.encode(
                    generated_texts, as_targets=True, add_special_tokens=not skip_special_tokens
                )
                generation_args["decoder_input_ids"] = decoder_input.input_ids
            generated_texts = self.generate(
                encoded_input, return_generation_output=False, batch_size=batch_size, **generation_args
            )
        elif generation_args:
            logger.warning(
                f"Generation arguments {generation_args} are provided, but will be ignored (constrained decoding)."
            )
        batch_size = self.validate_attribute_args(
            input_texts=input_texts,
            generated_texts=generated_texts,
            has_generated_texts=has_generated_texts,
            attribution_method=attribution_method,
            batch_size=batch_size,
            attr_pos_start=attr_pos_start,
            attr_pos_end=attr_pos_end,
        )
        attribution_outputs = attribution_method.prepare_and_attribute(
            input_texts,
            generated_texts,
            batch_size=batch_size,
            attr_pos_start=attr_pos_start,
            attr_pos_end=attr_pos_end,
            show_progress=show_progress,
            pretty_progress=pretty_progress,
            output_step_attributions=output_step_attributions,
            attribute_target=attribute_target,
            step_scores=step_scores,
            include_eos_baseline=include_eos_baseline,
            skip_special_tokens=skip_special_tokens,
            clean_special_chars=clean_special_chars,
            attributed_fn=attributed_fn,
            attribution_args=attribution_args,
            attributed_fn_args=attributed_fn_args,
            step_scores_args=step_scores_args,
        )
        attribution_output = merge_attributions(attribution_outputs)
        attribution_output.info["input_texts"] = input_texts
        attribution_output.info["generated_texts"] = (
            [generated_texts] if isinstance(generated_texts, str) else generated_texts
        )
        attribution_output.info["generation_args"] = generation_args
        attribution_output.info["constrained_decoding"] = has_generated_texts
        attribution_output.info["generate_from_target_prefix"] = generate_from_target_prefix
        if device and original_device:
            self.device = original_device
        return attribution_output
    def embed(self, inputs: TextInput | IdsTensor, as_targets: bool = False, add_special_tokens: bool = True):
        if isinstance(inputs, str) or (
            isinstance(inputs, list) and len(inputs) > 0 and all(isinstance(x, str) for x in inputs)
        ):
            batch = self.encode(inputs, as_targets, add_special_tokens=add_special_tokens)
            inputs = batch.input_ids
        return self.embed_ids(inputs, as_targets=as_targets)
    def get_token_with_ids(
        self,
        batch: EncoderDecoderBatch | DecoderOnlyBatch,
        contrast_target_tokens: OneOrMoreTokenSequences | None = None,
        contrast_targets_alignments: list[list[tuple[int, int]]] | None = None,
    ) -> list[list[TokenWithId]]:
        if contrast_target_tokens is not None:
            return join_token_ids(
                batch.target_tokens,
                batch.target_ids.tolist(),
                contrast_target_tokens,
                contrast_targets_alignments,
            )
        return join_token_ids(batch.target_tokens, batch.target_ids.tolist())
    # Framework-specific methods
    @unhooked
    @abstractmethod
    def generate(
        self,
        encodings: TextInput | BatchEncoding,
        return_generation_output: bool | None = False,
        **kwargs,
    ) -> list[str] | tuple[list[str], Any]:
        pass
    @staticmethod
    @abstractmethod
    def output2logits(forward_output) -> LogitsTensor:
        pass
    @abstractmethod
    def encode(
        self,
        texts: TextInput,
        as_targets: bool = False,
        return_baseline: bool = False,
        include_eos_baseline: bool = False,
        add_special_tokens: bool = True,
    ) -> BatchEncoding:
        pass
    @abstractmethod
    def decode(self, ids: IdsTensor, skip_special_tokens: bool = True) -> list[str]:
        pass
    @abstractmethod
    def embed_ids(self, ids: IdsTensor, as_targets: bool = False) -> EmbeddingsTensor:
        pass
    @abstractmethod
    def convert_ids_to_tokens(
        self, ids: torch.Tensor, skip_special_tokens: bool | None = True
    ) -> OneOrMoreTokenSequences:
        pass
    @abstractmethod
    def convert_tokens_to_ids(
        self,
        tokens: list[str] | list[list[str]],
    ) -> OneOrMoreIdSequences:
        pass
    @abstractmethod
    def convert_tokens_to_string(
        self,
        tokens: OneOrMoreTokenSequences,
        skip_special_tokens: bool | None = True,
        as_targets: bool = False,
    ) -> TextInput:
        pass
    @abstractmethod
    def convert_string_to_tokens(
        self,
        text: TextInput,
        skip_special_tokens: bool = True,
        as_targets: bool = False,
    ) -> OneOrMoreTokenSequences:
        pass
    @abstractmethod
    def clean_tokens(
        self,
        tokens: OneOrMoreTokenSequences,
        skip_special_tokens: bool = False,
        as_targets: bool = False,
    ):
        pass
    @property
    @abstractmethod
    def special_tokens(self) -> list[str]:
        pass
    @property
    @abstractmethod
    def special_tokens_ids(self) -> list[int]:
        pass
    @property
    @abstractmethod
    def vocabulary_embeddings(self) -> VocabularyEmbeddingsTensor:
        pass
    @abstractmethod
    def get_embedding_layer(self) -> torch.nn.Module:
        pass
    def configure_interpretable_embeddings(self, **kwargs) -> None:
        """Configure the model with interpretable embeddings for gradient attribution.
        This method needs to be defined for models that cannot receive embeddings directly from their
        forward method parameters, to allow the usage of interpretable embeddings as surrogate for
        feature attribution methods. Model that support precomputed embedding inputs by design can
        skip this method.
        """
        pass
    def remove_interpretable_embeddings(self, **kwargs) -> None:
        """Removes interpretable embeddings used for gradient attribution.
        If the configure_interpretable_embeddings method is defined, this method needs to be defined
        to allow restoring original embeddings for the model. This is required for methods using the
        decorator @unhooked since they require the original model capabilities.
        """
        pass
    # Architecture-specific methods
    @abstractmethod
    def get_forward_output(
        self,
        **kwargs,
    ) -> ModelOutput:
        pass
    @abstractmethod
    def get_encoder(self) -> torch.nn.Module:
        pass
    @abstractmethod
    def get_decoder(self) -> torch.nn.Module:
        pass
    @staticmethod
    @abstractmethod
    def get_attentions_dict(output: ModelOutput) -> dict[str, torch.Tensor]:
        pass
    @staticmethod
    @abstractmethod
    def get_hidden_states_dict(output: ModelOutput) -> dict[str, torch.Tensor]:
        pass
    # Model forward
    def _forward(
        self,
        batch: DecoderOnlyBatch | EncoderDecoderBatch,
        target_ids: ExpandedTargetIdsTensor,
        attributed_fn: Callable[..., SingleScorePerStepTensor],
        use_embeddings: bool = True,
        attributed_fn_argnames: list[str] | None = None,
        *args,
        **kwargs,
    ) -> LogitsTensor:
        assert len(args) == len(attributed_fn_argnames), "Number of arguments and number of argnames must match"
        target_ids = target_ids.squeeze(-1)
        output = self.get_forward_output(batch, use_embeddings=use_embeddings, **kwargs)
        logger.debug(f"logits: {pretty_tensor(output.logits)}")
        step_fn_args = self.formatter.format_step_function_args(
            attribution_model=self, forward_output=output, target_ids=target_ids, is_attributed_fn=True, batch=batch
        )
        step_fn_extra_args = {k: v for k, v in zip(attributed_fn_argnames, args, strict=False) if v is not None}
        return attributed_fn(step_fn_args, **step_fn_extra_args)
    def _forward_with_output(
        self,
        batch: DecoderOnlyBatch | EncoderDecoderBatch,
        use_embeddings: bool = True,
        *args,
        **kwargs,
    ) -> ModelOutput:
        return self.get_forward_output(batch, use_embeddings=use_embeddings, **kwargs)
    @formatter.format_forward_args
    def forward(self, *args, **kwargs) -> LogitsTensor:
        return self._forward(*args, **kwargs)
    @formatter.format_forward_args
    def forward_with_output(self, *args, **kwargs) -> ModelOutput:
        return self._forward_with_output(*args, **kwargs)

================
File: inseq/models/decoder_only.py
================
import logging
from collections.abc import Callable
from functools import wraps
from typing import Any, TypeVar
import torch
from ..attr.feat import join_token_ids
from ..attr.step_functions import StepFunctionDecoderOnlyArgs
from ..data import (
    BatchEmbedding,
    BatchEncoding,
    DecoderOnlyBatch,
    FeatureAttributionInput,
    FeatureAttributionStepOutput,
    get_batch_from_inputs,
)
from ..utils import get_aligned_idx
from ..utils.typing import (
    AttributionForwardInputs,
    EmbeddingsTensor,
    ExpandedTargetIdsTensor,
    IdsTensor,
    LogitsTensor,
    OneOrMoreTokenSequences,
    SingleScorePerStepTensor,
    TargetIdsTensor,
    TextSequences,
)
from .attribution_model import AttributionModel, ForwardMethod, InputFormatter, ModelOutput
CustomForwardOutput = TypeVar("CustomForwardOutput")
logger = logging.getLogger(__name__)
class DecoderOnlyInputFormatter(InputFormatter):
    @staticmethod
    def prepare_inputs_for_attribution(
        attribution_model: "DecoderOnlyAttributionModel",
        inputs: FeatureAttributionInput,
        include_eos_baseline: bool = False,
        skip_special_tokens: bool = False,
    ) -> DecoderOnlyBatch:
        batch = get_batch_from_inputs(
            attribution_model,
            inputs=inputs,
            include_eos_baseline=include_eos_baseline,
            as_targets=False,
            skip_special_tokens=skip_special_tokens,
        )
        return DecoderOnlyBatch.from_batch(batch)
    @staticmethod
    def format_attribution_args(
        batch: DecoderOnlyBatch,
        target_ids: TargetIdsTensor,
        attributed_fn: Callable[..., SingleScorePerStepTensor],
        attribute_target: bool = False,  # Needed for compatibility with EncoderDecoderAttributionModel
        attributed_fn_args: dict[str, Any] = {},
        attribute_batch_ids: bool = False,
        forward_batch_embeds: bool = True,
        use_baselines: bool = False,
    ) -> tuple[dict[str, Any], tuple[IdsTensor | EmbeddingsTensor | None, ...]]:
        if attribute_batch_ids:
            inputs = (batch.input_ids,)
            baselines = (batch.baseline_ids,)
        else:
            inputs = (batch.input_embeds,)
            baselines = (batch.baseline_embeds,)
        attribute_fn_args = {
            "inputs": inputs,
            "additional_forward_args": (
                # Ids are always explicitly passed as extra arguments to enable
                # usage in custom attribution functions.
                batch.input_ids,
                # Making targets 2D enables _expand_additional_forward_args
                # in Captum to preserve the expected batch dimension for methods
                # such as intergrated gradients.
                target_ids.unsqueeze(-1),
                attributed_fn,
                batch.attention_mask,
                # Defines how to treat source and target tensors
                # Maps on the use_embeddings argument of forward
                forward_batch_embeds,
                list(attributed_fn_args.keys()),
            )
            + tuple(attributed_fn_args.values()),
        }
        if use_baselines:
            attribute_fn_args["baselines"] = baselines
        return attribute_fn_args
    @staticmethod
    def enrich_step_output(
        attribution_model: "DecoderOnlyAttributionModel",
        step_output: FeatureAttributionStepOutput,
        batch: DecoderOnlyBatch,
        target_tokens: OneOrMoreTokenSequences,
        target_ids: TargetIdsTensor,
        contrast_batch: DecoderOnlyBatch | None = None,
        contrast_targets_alignments: list[list[tuple[int, int]]] | None = None,
    ) -> FeatureAttributionStepOutput:
        r"""Enriches the attribution output with token information, producing the finished
        :class:`~inseq.data.FeatureAttributionStepOutput` object.
        Args:
            step_output (:class:`~inseq.data.FeatureAttributionStepOutput`): The output produced
                by the attribution step, with missing batch information.
            batch (:class:`~inseq.data.DecoderOnlyBatch`): The batch on which attribution was performed.
            target_ids (:obj:`torch.Tensor`): Target token ids of size `(batch_size, 1)` corresponding to tokens
                for which the attribution step was performed.
        Returns:
            :class:`~inseq.data.FeatureAttributionStepOutput`: The enriched attribution output.
        """
        if target_ids.ndim == 0:
            target_ids = target_ids.unsqueeze(0)
        step_output.source = None
        if contrast_batch is not None:
            contrast_aligned_idx = get_aligned_idx(len(batch.target_tokens[0]), contrast_targets_alignments[0])
            contrast_target_ids = contrast_batch.target_ids[:, contrast_aligned_idx]
            step_output.target = join_token_ids(
                tokens=target_tokens,
                ids=attribution_model.convert_ids_to_tokens(contrast_target_ids, skip_special_tokens=False),
                contrast_tokens=attribution_model.convert_ids_to_tokens(
                    contrast_target_ids[None, ...], skip_special_tokens=False
                ),
            )
            step_output.prefix = join_token_ids(tokens=batch.target_tokens, ids=batch.target_ids.tolist())
        else:
            step_output.target = join_token_ids(target_tokens, [[idx] for idx in target_ids.tolist()])
            step_output.prefix = join_token_ids(batch.target_tokens, batch.target_ids.tolist())
        return step_output
    @staticmethod
    def format_step_function_args(
        attribution_model: "DecoderOnlyAttributionModel",
        forward_output: ModelOutput,
        target_ids: ExpandedTargetIdsTensor,
        batch: DecoderOnlyBatch,
        is_attributed_fn: bool = False,
    ) -> StepFunctionDecoderOnlyArgs:
        return StepFunctionDecoderOnlyArgs(
            attribution_model=attribution_model,
            forward_output=forward_output,
            target_ids=target_ids,
            is_attributed_fn=is_attributed_fn,
            decoder_input_ids=batch.target_ids,
            decoder_attention_mask=batch.target_mask,
            decoder_input_embeds=batch.target_embeds,
        )
    @staticmethod
    def convert_args_to_batch(
        args: StepFunctionDecoderOnlyArgs = None,
        decoder_input_ids: IdsTensor | None = None,
        decoder_attention_mask: IdsTensor | None = None,
        decoder_input_embeds: EmbeddingsTensor | None = None,
        **kwargs,
    ) -> DecoderOnlyBatch:
        if args is not None:
            decoder_input_ids = args.decoder_input_ids
            decoder_attention_mask = args.decoder_attention_mask
            decoder_input_embeds = args.decoder_input_embeds
        encoding = BatchEncoding(decoder_input_ids, decoder_attention_mask)
        embedding = BatchEmbedding(decoder_input_embeds)
        return DecoderOnlyBatch(encoding, embedding)
    @staticmethod
    def format_forward_args(forward_fn: ForwardMethod) -> Callable[..., CustomForwardOutput]:
        @wraps(forward_fn)
        def formatted_forward_input_wrapper(
            self: "DecoderOnlyAttributionModel",
            forward_tensor: AttributionForwardInputs,
            input_ids: IdsTensor,
            target_ids: ExpandedTargetIdsTensor,
            attributed_fn: Callable[..., SingleScorePerStepTensor],
            attention_mask: IdsTensor | None = None,
            use_embeddings: bool = True,
            attributed_fn_argnames: list[str] | None = None,
            *args,
            **kwargs,
        ) -> CustomForwardOutput:
            batch = self.formatter.convert_args_to_batch(
                decoder_input_ids=input_ids,
                decoder_attention_mask=attention_mask,
                decoder_input_embeds=forward_tensor if use_embeddings else None,
            )
            return forward_fn(
                self, batch, target_ids, attributed_fn, use_embeddings, attributed_fn_argnames, *args, **kwargs
            )
        return formatted_forward_input_wrapper
    @staticmethod
    def get_text_sequences(attribution_model: "DecoderOnlyAttributionModel", batch: DecoderOnlyBatch) -> TextSequences:
        return TextSequences(
            sources=None,
            targets=attribution_model.decode(batch.target_ids),
        )
    @staticmethod
    def get_step_function_reserved_args() -> list[str]:
        return [f.name for f in StepFunctionDecoderOnlyArgs.__dataclass_fields__.values()]
class DecoderOnlyAttributionModel(AttributionModel):
    """AttributionModel class for attributing encoder-decoder models."""
    formatter = DecoderOnlyInputFormatter
    def get_forward_output(
        self,
        batch: DecoderOnlyBatch,
        use_embeddings: bool = True,
        **kwargs,
    ) -> ModelOutput:
        return self.model(
            input_ids=batch.input_ids if not use_embeddings else None,
            inputs_embeds=batch.input_embeds if use_embeddings else None,
            # Hacky fix for petals' distributed models while awaiting attention_mask support:
            # https://github.com/bigscience-workshop/petals/pull/206
            attention_mask=batch.attention_mask if not self.is_distributed else None,
            **kwargs,
        )
    @formatter.format_forward_args
    def forward(self, *args, **kwargs) -> LogitsTensor:
        return self._forward(*args, **kwargs)
    @formatter.format_forward_args
    def forward_with_output(self, *args, **kwargs) -> ModelOutput:
        return self._forward_with_output(*args, **kwargs)
    def get_encoder(self) -> torch.nn.Module:
        raise NotImplementedError("Decoder-only models do not have an encoder.")
    def get_decoder(self) -> torch.nn.Module:
        return self.model

================
File: inseq/models/encoder_decoder.py
================
import logging
from collections.abc import Callable
from functools import wraps
from typing import Any, TypeVar
from ..attr.feat import join_token_ids
from ..attr.step_functions import StepFunctionEncoderDecoderArgs
from ..data import (
    Batch,
    BatchEmbedding,
    BatchEncoding,
    DecoderOnlyBatch,
    EncoderDecoderBatch,
    FeatureAttributionInput,
    FeatureAttributionStepOutput,
    get_batch_from_inputs,
)
from ..utils import get_aligned_idx
from ..utils.typing import (
    AttributionForwardInputs,
    EmbeddingsTensor,
    ExpandedTargetIdsTensor,
    IdsTensor,
    LogitsTensor,
    OneOrMoreTokenSequences,
    SingleScorePerStepTensor,
    TargetIdsTensor,
    TextSequences,
)
from .attribution_model import AttributionModel, ForwardMethod, InputFormatter, ModelOutput
CustomForwardOutput = TypeVar("CustomForwardOutput")
logger = logging.getLogger(__name__)
class EncoderDecoderInputFormatter(InputFormatter):
    def prepare_inputs_for_attribution(
        attribution_model: "EncoderDecoderAttributionModel",
        inputs: tuple[FeatureAttributionInput, FeatureAttributionInput],
        include_eos_baseline: bool = False,
        skip_special_tokens: bool = False,
    ) -> EncoderDecoderBatch:
        r"""Prepares sources and target to produce an :class:`~inseq.data.EncoderDecoderBatch`.
        There are two stages of preparation:
            1. Raw text sources and target texts are encoded by the model.
            2. The encoded sources and targets are converted to tensors for the forward pass.
        This method is agnostic of the preparation stage of sources and targets. If they are both
        raw text, they will undergo both steps. If they are already encoded, they will only be embedded.
        If the feature attribution method works on layers, the embedding step is skipped and embeddings are
        set to None.
        The final result will be consistent in both cases.
        Args:
            inputs (:obj:`tuple` of `FeatureAttributionInput`): A tuple containing sources and targets provided to the
                :meth:`~inseq.attr.feat.FeatureAttribution.prepare` method.
            include_eos_baseline (:obj:`bool`, `optional`): Whether to include the EOS token in the baseline for
                attribution. By default the EOS token is not used for attribution. Defaults to False.
        Returns:
            :obj:`EncoderDecoderBatch`: An :class:`~inseq.data.EncoderDecoderBatch` object containing sources
                and targets in encoded and embedded formats for all inputs.
        """
        sources, targets = inputs
        source_batch = get_batch_from_inputs(
            attribution_model,
            inputs=sources,
            include_eos_baseline=include_eos_baseline,
            as_targets=False,
            skip_special_tokens=skip_special_tokens,
        )
        target_batch = get_batch_from_inputs(
            attribution_model,
            inputs=targets,
            include_eos_baseline=include_eos_baseline,
            as_targets=True,
            skip_special_tokens=skip_special_tokens,
        )
        return EncoderDecoderBatch(source_batch, target_batch)
    @staticmethod
    def format_attribution_args(
        batch: EncoderDecoderBatch,
        target_ids: TargetIdsTensor,
        attributed_fn: Callable[..., SingleScorePerStepTensor],
        attribute_target: bool = False,
        attributed_fn_args: dict[str, Any] = {},
        attribute_batch_ids: bool = False,
        forward_batch_embeds: bool = True,
        use_baselines: bool = False,
    ) -> tuple[dict[str, Any], tuple[IdsTensor | EmbeddingsTensor | None, ...]]:
        if attribute_batch_ids:
            inputs = (batch.sources.input_ids,)
            baselines = (batch.sources.baseline_ids,)
        else:
            inputs = (batch.sources.input_embeds,)
            baselines = (batch.sources.baseline_embeds,)
        if attribute_target:
            if attribute_batch_ids:
                inputs += (batch.targets.input_ids,)
                baselines += (batch.targets.baseline_ids,)
            else:
                inputs += (batch.targets.input_embeds,)
                baselines += (batch.targets.baseline_embeds,)
        attribute_fn_args = {
            "inputs": inputs,
            "additional_forward_args": (
                # Ids are always explicitly passed as extra arguments to enable
                # usage in custom attribution functions.
                batch.sources.input_ids,
                batch.targets.input_ids,
                # Making targets 2D enables _expand_additional_forward_args
                # in Captum to preserve the expected batch dimension for methods
                # such as intergrated gradients.
                target_ids.unsqueeze(-1),
                attributed_fn,
                batch.sources.attention_mask,
                batch.targets.attention_mask,
                # Defines how to treat source and target tensors
                # Maps on the use_embeddings argument of forward
                forward_batch_embeds,
                list(attributed_fn_args.keys()),
            )
            + tuple(attributed_fn_args.values()),
        }
        if not attribute_target:
            attribute_fn_args["additional_forward_args"] = (batch.targets.input_embeds,) + attribute_fn_args[
                "additional_forward_args"
            ]
        if use_baselines:
            attribute_fn_args["baselines"] = baselines
        return attribute_fn_args
    @staticmethod
    def enrich_step_output(
        attribution_model: "EncoderDecoderAttributionModel",
        step_output: FeatureAttributionStepOutput,
        batch: EncoderDecoderBatch,
        target_tokens: OneOrMoreTokenSequences,
        target_ids: TargetIdsTensor,
        contrast_batch: DecoderOnlyBatch | None = None,
        contrast_targets_alignments: list[list[tuple[int, int]]] | None = None,
    ) -> FeatureAttributionStepOutput:
        r"""Enriches the attribution output with token information, producing the finished
        :class:`~inseq.data.FeatureAttributionStepOutput` object.
        Args:
            step_output (:class:`~inseq.data.FeatureAttributionStepOutput`): The output produced
                by the attribution step, with missing batch information.
            batch (:class:`~inseq.data.EncoderDecoderBatch`): The batch on which attribution was performed.
            target_ids (:obj:`torch.Tensor`): Target token ids of size `(batch_size, 1)` corresponding to tokens
                for which the attribution step was performed.
        Returns:
            :class:`~inseq.data.FeatureAttributionStepOutput`: The enriched attribution output.
        """
        if target_ids.ndim == 0:
            target_ids = target_ids.unsqueeze(0)
        step_output.source = join_token_ids(batch.sources.input_tokens, batch.sources.input_ids.tolist())
        if contrast_batch is not None:
            contrast_aligned_idx = get_aligned_idx(len(batch.target_tokens[0]), contrast_targets_alignments[0])
            contrast_target_ids = contrast_batch.target_ids[:, contrast_aligned_idx]
            step_output.target = join_token_ids(
                tokens=target_tokens,
                ids=[[idx] for idx in target_ids.tolist()],
                contrast_tokens=attribution_model.convert_ids_to_tokens(
                    contrast_target_ids[None, ...], skip_special_tokens=False
                ),
            )
            step_output.prefix = join_token_ids(tokens=batch.target_tokens, ids=batch.target_ids.tolist())
        else:
            step_output.target = join_token_ids(target_tokens, [[idx] for idx in target_ids.tolist()])
            step_output.prefix = join_token_ids(batch.targets.input_tokens, batch.targets.input_ids.tolist())
        return step_output
    @staticmethod
    def format_step_function_args(
        attribution_model: "EncoderDecoderAttributionModel",
        forward_output: ModelOutput,
        target_ids: ExpandedTargetIdsTensor,
        batch: EncoderDecoderBatch,
        is_attributed_fn: bool = False,
    ) -> StepFunctionEncoderDecoderArgs:
        return StepFunctionEncoderDecoderArgs(
            attribution_model=attribution_model,
            forward_output=forward_output,
            target_ids=target_ids,
            is_attributed_fn=is_attributed_fn,
            encoder_input_ids=batch.source_ids,
            decoder_input_ids=batch.target_ids,
            encoder_input_embeds=batch.source_embeds,
            decoder_input_embeds=batch.target_embeds,
            encoder_attention_mask=batch.source_mask,
            decoder_attention_mask=batch.target_mask,
        )
    @staticmethod
    def convert_args_to_batch(
        args: StepFunctionEncoderDecoderArgs = None,
        encoder_input_ids: IdsTensor | None = None,
        decoder_input_ids: IdsTensor | None = None,
        encoder_attention_mask: IdsTensor | None = None,
        decoder_attention_mask: IdsTensor | None = None,
        encoder_input_embeds: EmbeddingsTensor | None = None,
        decoder_input_embeds: EmbeddingsTensor | None = None,
        **kwargs,
    ) -> EncoderDecoderBatch:
        if args is not None:
            encoder_input_ids = args.encoder_input_ids
            decoder_input_ids = args.decoder_input_ids
            encoder_attention_mask = args.encoder_attention_mask
            decoder_attention_mask = args.decoder_attention_mask
            encoder_input_embeds = args.encoder_input_embeds
            decoder_input_embeds = args.decoder_input_embeds
        source_encoding = BatchEncoding(encoder_input_ids, encoder_attention_mask)
        source_embedding = BatchEmbedding(encoder_input_embeds)
        source_batch = Batch(source_encoding, source_embedding)
        target_encoding = BatchEncoding(decoder_input_ids, decoder_attention_mask)
        target_embedding = BatchEmbedding(decoder_input_embeds)
        target_batch = Batch(target_encoding, target_embedding)
        return EncoderDecoderBatch(source_batch, target_batch)
    @staticmethod
    def format_forward_args(forward_fn: ForwardMethod) -> Callable[..., CustomForwardOutput]:
        @wraps(forward_fn)
        def formatted_forward_input_wrapper(
            self: "EncoderDecoderAttributionModel",
            encoder_tensors: AttributionForwardInputs,
            decoder_input_embeds: AttributionForwardInputs,
            encoder_input_ids: IdsTensor,
            decoder_input_ids: IdsTensor,
            target_ids: ExpandedTargetIdsTensor,
            attributed_fn: Callable[..., SingleScorePerStepTensor],
            encoder_attention_mask: IdsTensor | None = None,
            decoder_attention_mask: IdsTensor | None = None,
            use_embeddings: bool = True,
            attributed_fn_argnames: list[str] | None = None,
            *args,
            **kwargs,
        ) -> CustomForwardOutput:
            batch = self.formatter.convert_args_to_batch(
                encoder_input_ids=encoder_input_ids,
                decoder_input_ids=decoder_input_ids,
                encoder_attention_mask=encoder_attention_mask,
                decoder_attention_mask=decoder_attention_mask,
                encoder_input_embeds=encoder_tensors if use_embeddings else None,
                decoder_input_embeds=decoder_input_embeds,
            )
            return forward_fn(
                self, batch, target_ids, attributed_fn, use_embeddings, attributed_fn_argnames, *args, **kwargs
            )
        return formatted_forward_input_wrapper
    @staticmethod
    def get_text_sequences(
        attribution_model: "EncoderDecoderAttributionModel", batch: EncoderDecoderBatch
    ) -> TextSequences:
        return TextSequences(
            sources=attribution_model.convert_tokens_to_string(batch.sources.input_tokens),
            targets=attribution_model.decode(batch.targets.input_ids),
        )
    @staticmethod
    def get_step_function_reserved_args() -> list[str]:
        return [f.name for f in StepFunctionEncoderDecoderArgs.__dataclass_fields__.values()]
class EncoderDecoderAttributionModel(AttributionModel):
    """AttributionModel class for attributing encoder-decoder models."""
    formatter = EncoderDecoderInputFormatter
    def get_forward_output(
        self,
        batch: EncoderDecoderBatch,
        use_embeddings: bool = True,
        **kwargs,
    ) -> ModelOutput:
        return self.model(
            input_ids=None if use_embeddings else batch.source_ids,
            inputs_embeds=batch.source_embeds if use_embeddings else None,
            attention_mask=batch.source_mask,
            decoder_inputs_embeds=batch.target_embeds,
            decoder_attention_mask=batch.target_mask,
            **kwargs,
        )
    @formatter.format_forward_args
    def forward(self, *args, **kwargs) -> LogitsTensor:
        return self._forward(*args, **kwargs)
    @formatter.format_forward_args
    def forward_with_output(self, *args, **kwargs) -> ModelOutput:
        return self._forward_with_output(*args, **kwargs)

================
File: inseq/models/huggingface_model.py
================
"""HuggingFace Seq2seq model."""
import logging
from abc import abstractmethod
from typing import Any, NoReturn
import torch
from torch import long
from transformers import (
    AutoConfig,
    AutoModelForCausalLM,
    AutoModelForSeq2SeqLM,
    AutoTokenizer,
    PreTrainedModel,
    PreTrainedTokenizerBase,
)
from transformers.modeling_outputs import CausalLMOutput, ModelOutput, Seq2SeqLMOutput
from ..attr.attribution_decorators import batched
from ..data import BatchEncoding
from ..utils import check_device
from ..utils.typing import (
    EmbeddingsTensor,
    IdsTensor,
    LogitsTensor,
    MultiLayerEmbeddingsTensor,
    MultiLayerMultiUnitScoreTensor,
    OneOrMoreIdSequences,
    OneOrMoreTokenSequences,
    OneOrMoreTokenWithIdSequences,
    TextInput,
    TokenWithId,
    VocabularyEmbeddingsTensor,
)
from .attribution_model import AttributionModel
from .decoder_only import DecoderOnlyAttributionModel
from .encoder_decoder import EncoderDecoderAttributionModel
from .model_decorators import unhooked
logger = logging.getLogger(__name__)
logging.getLogger("urllib3").setLevel(logging.WARNING)
# Update if other model types are added
SUPPORTED_AUTOCLASSES = [AutoModelForSeq2SeqLM, AutoModelForCausalLM]
class HuggingfaceModel(AttributionModel):
    """Model wrapper for any ForCausalLM and ForConditionalGeneration model on the HuggingFace Hub used to enable
    feature attribution. Corresponds to AutoModelForCausalLM and AutoModelForSeq2SeqLM auto classes.
    Attributes:
        _autoclass (:obj:`Type[transformers.AutoModel`]): The HuggingFace model class to use for initialization.
            Must be defined in subclasses.
        model (:obj:`transformers.AutoModelForSeq2SeqLM` or :obj:`transformers.AutoModelForSeq2SeqLM`):
            the model on which attribution is performed.
        tokenizer (:obj:`transformers.AutoTokenizer`): the tokenizer associated to the model.
        device (:obj:`str`): the device on which the model is run.
        encoder_int_embeds (:obj:`captum.InterpretableEmbeddingBase`): the interpretable embedding layer for the
            encoder, used for layer attribution methods in Captum.
        decoder_int_embeds (:obj:`captum.InterpretableEmbeddingBase`): the interpretable embedding layer for the
            decoder, used for layer attribution methods in Captum.
        embed_scale (:obj:`float`, *optional*): scale factor for embeddings.
        tokenizer_name (:obj:`str`, *optional*): The name of the tokenizer in the Huggingface Hub.
            Default: use model name.
    """
    _autoclass = None
    def __init__(
        self,
        model: str | PreTrainedModel,
        attribution_method: str | None = None,
        tokenizer: str | PreTrainedTokenizerBase | None = None,
        device: str | None = None,
        model_kwargs: dict[str, Any] | None = {},
        tokenizer_kwargs: dict[str, Any] | None = {},
        **kwargs,
    ) -> None:
        """AttributionModel subclass for Huggingface-compatible models.
        Args:
            model (:obj:`str` or :obj:`transformers.PreTrainedModel`): the name of the model in the
                Huggingface Hub or path to folder containing local model files.
            attribution_method (str, optional): The attribution method to use.
                Passing it here reduces overhead on attribute call, since it is already
                initialized.
            tokenizer (:obj:`str` or :obj:`transformers.PreTrainedTokenizerBase`, optional): the name of the tokenizer
                in the Huggingface Hub or path to folder containing local tokenizer files.
                Default: use model name.
            device (str, optional): the Torch device on which the model is run.
            **kwargs: additional arguments for the model and the tokenizer.
        """
        super().__init__(**kwargs)
        if self._autoclass is None or self._autoclass not in SUPPORTED_AUTOCLASSES:
            raise ValueError(
                f"Invalid autoclass {self._autoclass}. Must be one of {[x.__name__ for x in SUPPORTED_AUTOCLASSES]}."
            )
        if isinstance(model, PreTrainedModel):
            self.model = model
        else:
            self.model = self._autoclass.from_pretrained(model, **model_kwargs)
        self.model_name = self.model.config.name_or_path
        self.tokenizer_name = tokenizer if isinstance(tokenizer, str) else None
        if tokenizer is None:
            tokenizer = model if isinstance(model, str) else self.model_name
            if not tokenizer:
                raise ValueError(
                    "Unspecified tokenizer for model loaded from scratch. Use explicit identifier as tokenizer=<ID>"
                    "during model loading."
                )
        if isinstance(tokenizer, PreTrainedTokenizerBase):
            self.tokenizer = tokenizer
        else:
            self.tokenizer = AutoTokenizer.from_pretrained(tokenizer, **tokenizer_kwargs)
        self.eos_token_id = getattr(self.model.config, "eos_token_id", None)
        if isinstance(self.eos_token_id, list):
            self.eos_token_id = self.eos_token_id[0]
        pad_token_id = self.model.config.pad_token_id
        if pad_token_id is None:
            if self.tokenizer.pad_token_id is None:
                logger.info(f"Setting `pad_token_id` to `eos_token_id`:{self.eos_token_id} for open-end generation.")
                pad_token_id = self.eos_token_id
            else:
                pad_token_id = self.tokenizer.pad_token_id
        self.pad_token = self._convert_ids_to_tokens(pad_token_id, skip_special_tokens=False)
        if isinstance(self.pad_token, list):
            self.pad_token = self.pad_token[0]
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.pad_token
        if self.model.config.pad_token_id is None:
            self.model.config.pad_token_id = pad_token_id
        self.bos_token_id = getattr(self.model.config, "decoder_start_token_id", None)
        if self.bos_token_id is None:
            self.bos_token_id = self.model.config.bos_token_id
        self.bos_token = self._convert_ids_to_tokens(self.bos_token_id, skip_special_tokens=False)
        if self.eos_token_id is None:
            self.eos_token_id = self.tokenizer.pad_token_id
        if self.tokenizer.unk_token_id is None:
            self.tokenizer.unk_token_id = self.tokenizer.pad_token_id
        self.embed_scale = 1.0
        self.encoder_int_embeds = None
        self.decoder_int_embeds = None
        self.device_map = None
        if hasattr(self.model, "hf_device_map") and self.model.hf_device_map is not None:
            self.device_map = self.model.hf_device_map
        self.is_encoder_decoder = self.model.config.is_encoder_decoder
        self.configure_embeddings_scale()
        self.setup(device, attribution_method, **kwargs)
    @staticmethod
    def load(
        model: str | PreTrainedModel,
        attribution_method: str | None = None,
        tokenizer: str | PreTrainedTokenizerBase | None = None,
        device: str = None,
        model_kwargs: dict[str, Any] | None = {},
        tokenizer_kwargs: dict[str, Any] | None = {},
        **kwargs,
    ) -> "HuggingfaceModel":
        """Loads a HuggingFace model and tokenizer and wraps them in the appropriate AttributionModel."""
        if isinstance(model, str):
            is_encoder_decoder = AutoConfig.from_pretrained(model, **model_kwargs).is_encoder_decoder
        else:
            is_encoder_decoder = model.config.is_encoder_decoder
        if is_encoder_decoder:
            return HuggingfaceEncoderDecoderModel(
                model, attribution_method, tokenizer, device, model_kwargs, tokenizer_kwargs, **kwargs
            )
        else:
            return HuggingfaceDecoderOnlyModel(
                model, attribution_method, tokenizer, device, model_kwargs, tokenizer_kwargs, **kwargs
            )
    @AttributionModel.device.setter
    def device(self, new_device: str) -> None:
        check_device(new_device)
        self._device = new_device
        is_loaded_in_8bit = getattr(self.model, "is_loaded_in_8bit", False)
        is_loaded_in_4bit = getattr(self.model, "is_loaded_in_4bit", False)
        is_quantized = is_loaded_in_8bit or is_loaded_in_4bit
        has_device_map = self.device_map is not None
        # Enable compatibility with 8bit models
        if self.model:
            if is_quantized:
                mode = "8bit" if is_loaded_in_8bit else "4bit"
                logger.warning(
                    f"The model is loaded in {mode} mode. The device cannot be changed after loading the model."
                )
            elif has_device_map:
                logger.warning("The model is loaded with a device map. The device cannot be changed after loading.")
            else:
                self.model.to(self._device)
    @abstractmethod
    def configure_embeddings_scale(self) -> None:
        """Configure the scale factor for embeddings."""
        pass
    @property
    def info(self) -> dict[str, str]:
        dic_info: dict[str, str] = super().info
        extra_info = {
            "tokenizer_name": self.tokenizer_name,
            "tokenizer_class": self.tokenizer.__class__.__name__,
        }
        dic_info.update(extra_info)
        return dic_info
    @unhooked
    @batched
    def generate(
        self,
        inputs: TextInput | BatchEncoding,
        return_generation_output: bool = False,
        skip_special_tokens: bool | None = None,
        output_generated_only: bool = False,
        **kwargs,
    ) -> list[str] | tuple[list[str], ModelOutput]:
        """Wrapper of model.generate to handle tokenization and decoding.
        Args:
            inputs (`Union[TextInput, BatchEncoding]`):
                Inputs to be provided to the model for generation.
            return_generation_output (`bool`, *optional*, defaults to False):
                If true, generation outputs are returned alongside the generated text.
            output_generated_only (`bool`, *optional*, defaults to False):
                If true, only the generated text is returned. Relevant for decoder-only models that would otherwise return
                the full input + output.
        Returns:
            `Union[List[str], Tuple[List[str], ModelOutput]]`: Generated text or a tuple of generated text and
            generation outputs.
        """
        if isinstance(inputs, str) or (
            isinstance(inputs, list) and len(inputs) > 0 and all(isinstance(x, str) for x in inputs)
        ):
            inputs = self.encode(inputs, add_special_tokens=not skip_special_tokens)
        inputs: BatchEncoding = inputs.to(self.device)
        generation_out = self.model.generate(
            inputs=inputs.input_ids,
            return_dict_in_generate=True,
            **kwargs,
        )
        sequences = generation_out.sequences
        if output_generated_only and not self.is_encoder_decoder:
            sequences = sequences[:, inputs.input_ids.shape[1] :]
        # Left-padding in multi-sentence sequences is skipped by default.
        if skip_special_tokens is None:
            skip_special_tokens = inputs.num_sequences != 1 or self.is_encoder_decoder
        texts = self.decode(ids=sequences, skip_special_tokens=skip_special_tokens)
        if return_generation_output:
            return texts, generation_out
        return texts
    @staticmethod
    def output2logits(forward_output: Seq2SeqLMOutput | CausalLMOutput) -> LogitsTensor:
        # Full logits for last position of every sentence:
        # (batch_size, tgt_seq_len, vocab_size) => (batch_size, vocab_size)
        return forward_output.logits[:, -1, :].squeeze(1)
    def encode(
        self,
        texts: TextInput,
        as_targets: bool = False,
        return_baseline: bool = False,
        include_eos_baseline: bool = False,
        add_bos_token: bool = True,
        add_special_tokens: bool = True,
    ) -> BatchEncoding:
        """Encode one or multiple texts, producing a BatchEncoding.
        Args:
            texts (str or list of str): the texts to tokenize.
            return_baseline (bool, optional): if True, baseline token ids are returned.
        Returns:
            BatchEncoding: contains ids and attention masks.
        """
        if as_targets and not self.is_encoder_decoder:
            raise ValueError("Decoder-only models should use tokenization as source only.")
        batch = self.tokenizer(
            text=texts if not as_targets else None,
            text_target=texts if as_targets else None,
            add_special_tokens=add_special_tokens,
            padding=True,
            truncation=True,
            return_tensors="pt",
        ).to(self.device)
        baseline_ids = None
        # Fix: If two BOS tokens are present (e.g. when using chat templates), the second one is removed.
        if (
            batch["input_ids"].shape[0] == 1
            and len(batch["input_ids"][0]) >= 2
            and batch["input_ids"][0][0] == batch["input_ids"][0][1] == self.bos_token_id
        ):
            batch["input_ids"] = batch["input_ids"][:, 1:]
            batch["attention_mask"] = batch["attention_mask"][:, 1:]
        if return_baseline:
            if include_eos_baseline:
                baseline_ids = torch.ones_like(batch["input_ids"]).long() * self.tokenizer.unk_token_id
            else:
                baseline_ids_non_eos = batch["input_ids"].ne(self.eos_token_id).long() * self.tokenizer.unk_token_id
                baseline_ids_eos = batch["input_ids"].eq(self.eos_token_id).long() * self.eos_token_id
                baseline_ids = baseline_ids_non_eos + baseline_ids_eos
        # We prepend a BOS token only when tokenizing target texts.
        if as_targets and self.is_encoder_decoder and add_bos_token:
            ones_mask = torch.ones((batch["input_ids"].shape[0], 1), device=self.device, dtype=long)
            batch["attention_mask"] = torch.cat((ones_mask, batch["attention_mask"]), dim=1)
            bos_ids = ones_mask * self.bos_token_id
            batch["input_ids"] = torch.cat((bos_ids, batch["input_ids"]), dim=1)
            if return_baseline:
                baseline_ids = torch.cat((bos_ids, baseline_ids), dim=1)
        return BatchEncoding(
            input_ids=batch["input_ids"],
            input_tokens=[self._convert_ids_to_tokens(x, skip_special_tokens=False) for x in batch["input_ids"]],
            attention_mask=batch["attention_mask"],
            baseline_ids=baseline_ids,
        )
    def decode(
        self,
        ids: list[int] | list[list[int]] | IdsTensor,
        skip_special_tokens: bool = True,
    ) -> list[str]:
        return self.tokenizer.batch_decode(
            ids,
            skip_special_tokens=skip_special_tokens,
            clean_up_tokenization_spaces=False,
        )
    def embed_ids(self, ids: IdsTensor, as_targets: bool = False) -> EmbeddingsTensor:
        if as_targets and not self.is_encoder_decoder:
            raise ValueError("Decoder-only models should use tokenization as source only.")
        if self.encoder_int_embeds is not None and not as_targets:
            embeddings = self.encoder_int_embeds.indices_to_embeddings(ids)
        elif self.decoder_int_embeds is not None and as_targets:
            embeddings = self.decoder_int_embeds.indices_to_embeddings(ids)
        else:
            embeddings = self.get_embedding_layer()(ids)
        return embeddings * self.embed_scale
    def _convert_ids_to_tokens(self, ids: IdsTensor, skip_special_tokens: bool = True) -> OneOrMoreTokenSequences:
        tokens = self.tokenizer.convert_ids_to_tokens(ids, skip_special_tokens=skip_special_tokens)
        if isinstance(tokens, bytes) and not isinstance(tokens, str):
            return tokens.decode("utf-8")
        elif isinstance(tokens, list):
            return [t.decode("utf-8") if isinstance(t, bytes) else t for t in tokens]
        return tokens
    def convert_ids_to_tokens(
        self, ids: IdsTensor, skip_special_tokens: bool | None = True
    ) -> OneOrMoreTokenSequences:
        if ids.ndim < 2:
            return self._convert_ids_to_tokens(ids, skip_special_tokens)
        return [self._convert_ids_to_tokens(id_slice, skip_special_tokens) for id_slice in ids]
    def convert_tokens_to_ids(self, tokens: TextInput) -> OneOrMoreIdSequences:
        if isinstance(tokens[0], str):
            return self.tokenizer.convert_tokens_to_ids(tokens)
        return [self.tokenizer.convert_tokens_to_ids(token_slice) for token_slice in tokens]
    def convert_tokens_to_string(
        self,
        tokens: OneOrMoreTokenSequences,
        skip_special_tokens: bool = True,
        as_targets: bool = False,
    ) -> TextInput:
        if isinstance(tokens, list) and len(tokens) == 0:
            return ""
        elif isinstance(tokens[0], bytes | str):
            tmp_decode_state = self.tokenizer._decode_use_source_tokenizer
            self.tokenizer._decode_use_source_tokenizer = not as_targets
            out_strings = self.tokenizer.convert_tokens_to_string(
                tokens if not skip_special_tokens else [t for t in tokens if t not in self.special_tokens]
            )
            self.tokenizer._decode_use_source_tokenizer = tmp_decode_state
            return out_strings
        return [self.convert_tokens_to_string(token_slice, skip_special_tokens, as_targets) for token_slice in tokens]
    def convert_string_to_tokens(
        self,
        text: TextInput,
        skip_special_tokens: bool = True,
        as_targets: bool = False,
    ) -> OneOrMoreTokenSequences:
        if isinstance(text, str):
            ids = self.tokenizer(
                text=text if not as_targets else None,
                text_target=text if as_targets else None,
                add_special_tokens=not skip_special_tokens,
            )["input_ids"]
            return self._convert_ids_to_tokens(ids, skip_special_tokens)
        return [self.convert_string_to_tokens(t, skip_special_tokens, as_targets) for t in text]
    def clean_tokens(
        self,
        tokens: OneOrMoreTokenSequences | OneOrMoreTokenWithIdSequences,
        skip_special_tokens: bool = False,
        as_targets: bool = False,
    ) -> OneOrMoreTokenSequences:
        """Cleans special characters from tokens.
        Args:
            tokens (`OneOrMoreTokenSequences`):
                A list containing one or more lists of tokens.
            skip_special_tokens (`bool`, *optional*, defaults to True):
                If true, special tokens are skipped.
            as_targets (`bool`, *optional*, defaults to False):
                If true, a target tokenizer is used to clean the tokens.
        Returns:
            `OneOrMoreTokenSequences`: A list containing one or more lists of cleaned tokens.
        """
        if isinstance(tokens, list) and len(tokens) == 0:
            return []
        elif isinstance(tokens[0], bytes | str | TokenWithId):
            clean_tokens = []
            for tok in tokens:
                str_tok = tok.token if isinstance(tok, TokenWithId) else tok
                clean_str_tok = self.convert_tokens_to_string(
                    [str_tok], skip_special_tokens=skip_special_tokens, as_targets=as_targets
                )
                if not clean_str_tok and tok:
                    clean_str_tok = tok
                clean_tok = TokenWithId(clean_str_tok, tok.id) if isinstance(tok, TokenWithId) else clean_str_tok
                clean_tokens.append(clean_tok)
            return clean_tokens
        return [self.clean_tokens(token_seq, skip_special_tokens, as_targets) for token_seq in tokens]
    @property
    def special_tokens(self) -> list[str]:
        return self.tokenizer.all_special_tokens
    @property
    def special_tokens_ids(self) -> list[int]:
        return self.tokenizer.all_special_ids
    @property
    def vocabulary_embeddings(self) -> VocabularyEmbeddingsTensor:
        return self.get_embedding_layer().weight
    def get_embedding_layer(self) -> torch.nn.Module:
        return self.model.get_input_embeddings()
class HuggingfaceEncoderDecoderModel(HuggingfaceModel, EncoderDecoderAttributionModel):
    """Model wrapper for any ForConditionalGeneration model on the HuggingFace Hub used to enable
    feature attribution. Corresponds to AutoModelForSeq2SeqLM auto classes in HF transformers.
    Attributes:
        model (::obj:`transformers.AutoModelForSeq2SeqLM`):
            the model on which attribution is performed.
    """
    _autoclass = AutoModelForSeq2SeqLM
    def configure_embeddings_scale(self):
        encoder = self.model.get_encoder()
        decoder = self.model.get_decoder()
        if hasattr(encoder, "embed_scale"):
            self.embed_scale = encoder.embed_scale
        if hasattr(decoder, "embed_scale") and decoder.embed_scale != self.embed_scale:
            raise ValueError("Different encoder and decoder embed scales are not supported")
    def get_encoder(self) -> torch.nn.Module:
        return self.model.get_encoder()
    def get_decoder(self) -> torch.nn.Module:
        return self.model.get_decoder()
    @staticmethod
    def get_attentions_dict(
        output: Seq2SeqLMOutput,
    ) -> dict[str, MultiLayerMultiUnitScoreTensor]:
        if output.encoder_attentions is None or output.decoder_attentions is None:
            raise ValueError("Model does not support attribution relying on attention outputs.")
        if output.encoder_attentions is not None:
            output.encoder_attentions = tuple(att.to("cpu") for att in output.encoder_attentions)
        if output.decoder_attentions is not None:
            output.decoder_attentions = tuple(att.to("cpu") for att in output.decoder_attentions)
        if output.cross_attentions is not None:
            output.cross_attentions = tuple(att.to("cpu") for att in output.cross_attentions)
        return {
            "encoder_self_attentions": torch.stack(output.encoder_attentions, dim=1),
            "decoder_self_attentions": torch.stack(output.decoder_attentions, dim=1),
            "cross_attentions": torch.stack(output.cross_attentions, dim=1),
        }
    @staticmethod
    def get_hidden_states_dict(output: Seq2SeqLMOutput) -> dict[str, MultiLayerEmbeddingsTensor]:
        return {
            "encoder_hidden_states": torch.stack(output.encoder_hidden_states, dim=1),
            "decoder_hidden_states": torch.stack(output.decoder_hidden_states, dim=1),
        }
class HuggingfaceDecoderOnlyModel(HuggingfaceModel, DecoderOnlyAttributionModel):
    """Model wrapper for any ForCausalLM or LMHead model on the HuggingFace Hub used to enable
    feature attribution. Corresponds to AutoModelForCausalLM auto classes in HF transformers.
    Attributes:
        model (::obj:`transformers.AutoModelForCausalLM`):
            the model on which attribution is performed.
    """
    _autoclass = AutoModelForCausalLM
    def __init__(
        self,
        model: str | PreTrainedModel,
        attribution_method: str | None = None,
        tokenizer: str | PreTrainedTokenizerBase | None = None,
        device: str = None,
        model_kwargs: dict[str, Any] | None = {},
        tokenizer_kwargs: dict[str, Any] | None = {},
        **kwargs,
    ) -> NoReturn:
        super().__init__(model, attribution_method, tokenizer, device, model_kwargs, tokenizer_kwargs, **kwargs)
        self.tokenizer.padding_side = "left"
        self.tokenizer.truncation_side = "left"
        if self.pad_token is None:
            self.pad_token = self.tokenizer.bos_token
            self.tokenizer.pad_token = self.tokenizer.bos_token
    def configure_embeddings_scale(self):
        if hasattr(self.model, "embed_scale"):
            self.embed_scale = self.model.embed_scale
    @staticmethod
    def get_attentions_dict(output: CausalLMOutput) -> dict[str, MultiLayerMultiUnitScoreTensor]:
        if output.attentions is None:
            raise ValueError("Model does not support attribution relying on attention outputs.")
        else:
            output.attentions = tuple(att.to("cpu") for att in output.attentions)
        return {
            "decoder_self_attentions": torch.stack(output.attentions, dim=1),
        }
    @staticmethod
    def get_hidden_states_dict(output: CausalLMOutput) -> dict[str, MultiLayerEmbeddingsTensor]:
        return {
            "decoder_hidden_states": torch.stack(output.hidden_states, dim=1),
        }

================
File: inseq/models/model_config.py
================
import logging
from dataclasses import dataclass
from pathlib import Path
import yaml
logger = logging.getLogger(__name__)
@dataclass
class ModelConfig:
    """Configuration used by the methods for which the attribute ``use_model_config=True``.
    Args:
        self_attention_module (:obj:`str`):
            The name of the module performing the self-attention computation (e.g.``attn`` for the GPT-2 model in
            transformers). Can be identified by looking at the name of the self-attention module attribute
            in the model's transformer block class (e.g. :obj:`transformers.models.gpt2.GPT2Block` for GPT-2).
        cross_attention_module (:obj:`str`):
            The name of the module performing the cross-attention computation (e.g.``encoder_attn`` for MarianMT models
            in transformers). Can be identified by looking at the name of the cross-attention module attribute
            in the model's transformer block class (e.g. :obj:`transformers.models.marian.MarianDecoderLayer`).
        value_vector (:obj:`str`):
            The name of the variable in the forward pass of the attention module containing the value vector
            (e.g. ``value`` for the GPT-2 model in transformers). Can be identified by looking at the forward pass of
            the attention module (e.g. :obj:`transformers.models.gpt2.modeling_gpt2.GPT2Attention.forward` for GPT-2).
    """
    self_attention_module: str
    value_vector: str
    cross_attention_module: str | None = None
MODEL_CONFIGS = {
    model_type: ModelConfig(**cfg)
    for model_type, cfg in yaml.safe_load(open(Path(__file__).parent / "model_config.yaml", encoding="utf8")).items()
}
def get_model_config(model_type: str) -> ModelConfig:
    if model_type not in MODEL_CONFIGS:
        raise ValueError(
            f"A configuration for the {model_type} model is not defined. "
            "You can register a configuration with :meth:`~inseq.models.register_model_config`, "
            "or request it to be added to the library by opening an issue on GitHub: "
            "https://github.com/inseq-team/inseq/issues"
        )
    return MODEL_CONFIGS[model_type]
def register_model_config(
    model_type: str,
    config: dict,
    overwrite: bool = False,
    allow_partial: bool = False,
) -> None:
    """Allows to register a model configuration for a given model type. The configuration is a dictionary containing
    information required the methods for which the attribute ``use_model_config=True``.
    Args:
        model_type (`str`):
            The class of the model for which the configuration is registered, used as key in the stored configuration.
            E.g. GPT2LMHeadModel for the GPT-2 model in HuggingFace Transformers.
        config (`dict`):
            A dictionary containing the configuration for the model. The fields should match those of the
            :class:`~inseq.models.ModelConfig` class.
        overwrite (`bool`, *optional*, defaults to False):
            If `True`, the configuration will be overwritten if it already exists.
        allow_partial (`bool`, *optional*, defaults to False):
            If `True`, the configuration can be partial, i.e. it can contain only a subset of the fields of the
            :class:`~inseq.models.ModelConfig` class. The missing fields will be set to `None`.
    Raises:
        `ValueError`: If the model type is already registered and `overwrite=False`, or if the configuration is partial
            and `allow_partial=False`.
    """
    if model_type in MODEL_CONFIGS:
        if not overwrite:
            raise ValueError(
                f"{model_type} is already registered in model configurations.Override with overwrite=True."
            )
        logger.warning(f"Overwriting {model_type} config.")
    all_fields = set(ModelConfig.__dataclass_fields__.keys())
    config_fields = set(config.keys())
    diff = all_fields - config_fields
    if diff and not allow_partial:
        raise ValueError(
            f"Missing fields {','.join(diff)} in model configuration for {model_type}."
            "Set allow_partial=True to allow partial configuration."
        )
    if allow_partial:
        config = {**{field: None for field in diff}, **config}
    MODEL_CONFIGS[model_type] = ModelConfig(**config)

================
File: inseq/models/model_decorators.py
================
from collections.abc import Callable
from functools import wraps
from typing import Any
def unhooked(f: Callable[..., Any]) -> Callable[..., Any]:
    @wraps(f)
    def attribution_free_wrapper(self, *args, **kwargs):
        was_hooked = False
        if self.is_hooked:
            was_hooked = True
            self.attribution_method.unhook()
        out = f(self, *args, **kwargs)
        if was_hooked:
            self.attribution_method.hook()
        return out
    return attribution_free_wrapper

================
File: inseq/utils/__init__.py
================
from .alignment_utils import get_adjusted_alignments, get_aligned_idx
from .argparse import InseqArgumentParser, cli_arg
from .cache import INSEQ_ARTIFACTS_CACHE, INSEQ_HOME_CACHE, cache_results
from .errors import (
    InseqDeprecationWarning,
    LengthMismatchError,
    MissingAlignmentsError,
    MissingAttributionMethodError,
    UnknownAttributionMethodError,
)
from .hooks import get_post_variable_assignment_hook
from .import_utils import (
    is_accelerate_available,
    is_captum_available,
    is_datasets_available,
    is_ipywidgets_available,
    is_joblib_available,
    is_nltk_available,
    is_scikitlearn_available,
    is_sentencepiece_available,
    is_transformers_available,
)
from .misc import (
    aggregate_token_pair,
    aggregate_token_sequence,
    bin_str_to_ndarray,
    drop_padding,
    extract_signature_args,
    find_char_indexes,
    format_input_texts,
    get_cls_from_instance_type,
    get_module_name_from_object,
    gzip_compress,
    gzip_decompress,
    hashodict,
    isnotebook,
    lists_of_numbers_to_ndarray,
    ndarray_to_bin_str,
    optional,
    pad,
    pretty_dict,
    pretty_list,
    pretty_tensor,
    rgetattr,
    save_to_file,
    scalar_to_numpy,
)
from .registry import Registry, available_classes
from .serialization import json_advanced_dump, json_advanced_dumps, json_advanced_load, json_advanced_loads
from .torch_utils import (
    aggregate_contiguous,
    check_device,
    convert_from_safetensor,
    convert_to_safetensor,
    euclidean_distance,
    filter_logits,
    find_block_stack,
    get_default_device,
    get_front_padding,
    get_sequences_from_batched_steps,
    normalize,
    pad_with_nan,
    recursive_get_submodule,
    remap_from_filtered,
    rescale,
    top_p_logits_mask,
    validate_indices,
)
__all__ = [
    "LengthMismatchError",
    "MissingAttributionMethodError",
    "UnknownAttributionMethodError",
    "MissingAlignmentsError",
    "cache_results",
    "convert_to_safetensor",
    "convert_from_safetensor",
    "optional",
    "pad",
    "pretty_list",
    "pretty_tensor",
    "pretty_dict",
    "aggregate_token_pair",
    "aggregate_token_sequence",
    "format_input_texts",
    "rgetattr",
    "available_classes",
    "isnotebook",
    "find_char_indexes",
    "extract_signature_args",
    "remap_from_filtered",
    "drop_padding",
    "normalize",
    "rescale",
    "aggregate_contiguous",
    "get_front_padding",
    "get_sequences_from_batched_steps",
    "euclidean_distance",
    "Registry",
    "INSEQ_HOME_CACHE",
    "INSEQ_ARTIFACTS_CACHE",
    "InseqArgumentParser",
    "is_ipywidgets_available",
    "is_scikitlearn_available",
    "is_transformers_available",
    "is_sentencepiece_available",
    "is_datasets_available",
    "is_captum_available",
    "is_joblib_available",
    "is_nltk_available",
    "check_device",
    "get_default_device",
    "ndarray_to_bin_str",
    "hashodict",
    "InseqDeprecationWarning",
    "get_module_name_from_object",
    "gzip_compress",
    "gzip_decompress",
    "save_to_file",
    "json_advanced_dump",
    "json_advanced_dumps",
    "bin_str_to_ndarray",
    "lists_of_numbers_to_ndarray",
    "scalar_to_numpy",
    "get_cls_from_instance_type",
    "json_advanced_loads",
    "json_advanced_load",
    "get_nn_submodule",
    "find_block_stack",
    "get_adjusted_alignments",
    "get_aligned_idx",
    "top_p_logits_mask",
    "filter_logits",
    "cli_arg",
    "get_post_variable_assignment_hook",
    "validate_indices",
    "pad_with_nan",
    "recursive_get_submodule",
    "is_accelerate_available",
]

================
File: inseq/utils/alignment_utils.py
================
import logging
import re
from dataclasses import dataclass
from enum import Enum
from functools import lru_cache
from itertools import chain
import torch
from transformers import AutoModel, AutoTokenizer, PreTrainedModel, PreTrainedTokenizerBase
from .misc import clean_tokens
logger = logging.getLogger(__name__)
ALIGN_MODEL_ID = "sentence-transformers/LaBSE"
@dataclass
class AlignedSequences:
    source_tokens: list[str]
    target_tokens: list[str]
    alignments: list[tuple[int, int]]
    @property
    def aligned_tokens(self) -> list[tuple[str, str]]:
        return [(self.source_tokens[a_idx], self.target_tokens[b_idx]) for a_idx, b_idx in self.alignments]
    def reverse(self) -> "AlignedSequences":
        return AlignedSequences(
            source_tokens=self.target_tokens,
            target_tokens=self.source_tokens,
            alignments=[(b_idx, a_idx) for a_idx, b_idx in self.alignments],
        )
    def __str__(self) -> str:
        return f"{', '.join([f'{a}â†’{b} ({self.source_tokens[a]}â†’{self.target_tokens[b]})'for a,b in self.alignments])}"
class AlignmentMethod(Enum):
    AUTO = "auto"
@lru_cache
def get_aligner_model() -> PreTrainedModel:
    return AutoModel.from_pretrained(ALIGN_MODEL_ID)
@lru_cache
def get_aligner_tokenizer() -> PreTrainedTokenizerBase:
    return AutoTokenizer.from_pretrained(ALIGN_MODEL_ID)
def _preprocess_sequence_for_alignment(tokenized_seq: list[str]) -> tuple[torch.Tensor, list[list[int]]]:
    aligner_tokenizer = get_aligner_tokenizer()
    idxs = [aligner_tokenizer.convert_tokens_to_ids(x) for x in tokenized_seq]
    idxs = aligner_tokenizer.prepare_for_model(
        list(chain(*idxs)),
        return_tensors="pt",
        truncation=True,
        model_max_length=aligner_tokenizer.model_max_length,
    )["input_ids"]
    sub2word_map = []
    for i, word_list in enumerate(tokenized_seq):
        sub2word_map += [i for x in word_list]
    return idxs, sub2word_map
def _get_aligner_subword_aligns(
    src: list[str],
    tgt: list[str],
    align_layer: int,
    score_threshold: float,
) -> torch.Tensor:
    aligner = get_aligner_model()
    tokenizer = get_aligner_tokenizer()
    tokenized_src = [tokenizer.tokenize(word) for word in src]
    tokenized_tgt = [tokenizer.tokenize(word) for word in tgt]
    ids_src, sub2word_map_src = _preprocess_sequence_for_alignment(tokenized_src)
    ids_tgt, sub2word_map_tgt = _preprocess_sequence_for_alignment(tokenized_tgt)
    aligner.eval()
    with torch.no_grad():
        out_src = aligner(ids_src.unsqueeze(0), output_hidden_states=True)[2][align_layer][0, 1:-1]
        out_tgt = aligner(ids_tgt.unsqueeze(0), output_hidden_states=True)[2][align_layer][0, 1:-1]
        dot_prod = torch.matmul(out_src, out_tgt.transpose(-1, -2))
        softmax_srctgt = torch.nn.Softmax(dim=-1)(dot_prod)
        softmax_tgtsrc = torch.nn.Softmax(dim=-2)(dot_prod)
        softmax_inter = (softmax_srctgt > score_threshold) * (softmax_tgtsrc > score_threshold)
    align_subwords = torch.nonzero(softmax_inter, as_tuple=False)
    return align_subwords, sub2word_map_src, sub2word_map_tgt
def compute_word_aligns(
    src: str | list[str],
    tgt: str | list[str],
    split_pattern: str = r"\s+|\b",
    align_layer: int = 8,
    score_threshold: float = 1e-3,
) -> AlignedSequences:
    if isinstance(src, str):
        src = [word for word in re.split(split_pattern, src) if word]
    if isinstance(tgt, str):
        tgt = [word for word in re.split(split_pattern, tgt) if word]
    align_subwords, sub2word_map_src, sub2word_map_tgt = _get_aligner_subword_aligns(
        src, tgt, align_layer, score_threshold
    )
    align_words = set()
    for i, j in align_subwords:
        align_words.add((sub2word_map_src[i], sub2word_map_tgt[j]))
    word_alignments = sorted(align_words, key=lambda x: (x[0], x[1]))
    return AlignedSequences(
        source_tokens=src.copy(),
        target_tokens=tgt.copy(),
        alignments=word_alignments.copy(),
    )
def align_tokenizations(
    tok_a: list[str],
    tok_b: list[str],
) -> AlignedSequences:
    """Align tokens from a sentence tokenized by different tokenizers.
    Args:
        tok_a (:obj:`str` or :obj:`list` of :obj:`str`):
            Sequence of tokens produced by the first tokenizer.
        tok_b (:obj:`str` or :obj:`list` of :obj:`str`):
            Sequence of tokens produced by the second tokenizer.
    Raises:
        `ValueError`: Raised if the provided sequences do not have the same contents when concatenated.
    """
    if "".join(tok_a) != "".join(tok_b):
        raise ValueError(
            "The provided sequences must have the same contents when concatenated.\n"
            f"Sequence A: {tok_a}\nSequence B: {tok_b}\n"
        )
    aligns = []
    orig_tok_a = tok_a.copy()
    orig_tok_b = tok_b.copy()
    a_idx, b_idx = 0, 0
    while a_idx < len(tok_a):
        curr_tok_a = tok_a[a_idx]
        curr_tok_b = tok_b[b_idx]
        if curr_tok_a == curr_tok_b:
            aligns.append((a_idx, b_idx))
            a_idx += 1
            b_idx += 1
        elif curr_tok_a in curr_tok_b:
            aligns.append((a_idx, b_idx))
            tok_b[b_idx] = tok_b[b_idx].replace(curr_tok_a, "", 1)
            a_idx += 1
        elif curr_tok_b in curr_tok_a:
            aligns.append((a_idx, b_idx))
            tok_a[a_idx] = tok_a[a_idx].replace(curr_tok_b, "", 1)
            b_idx += 1
        else:
            raise ValueError(
                f"Found mismatching tokens '{curr_tok_a}' and '{curr_tok_b}' when aligning tokens. "
                "Please provide tokenizations that can be aligned."
            )
    return AlignedSequences(
        source_tokens=orig_tok_a,
        target_tokens=orig_tok_b,
        alignments=aligns.copy(),
    )
def propagate_alignments(aligns_a_b: AlignedSequences, aligns_b_c: AlignedSequences) -> AlignedSequences:
    """Given two set of alignments corresponding to the aligned tokens of strings A and B
    and those of strings B and C respectively, returns the alignment of tokens between
    string A and C.
    Args:
        aligns_a_b (:obj:`list` of :obj:`tuple` of :obj:`int`): List of alignment index pairs
            between sequences A and B.
        aligns_b_c (:obj:`list` of :obj:`tuple` of :obj:`int`): List of alignment index pairs
            between sequences B and C.
    Returns:
        :class:`AlignedSequences`: Alignment pairs between sequences A and C.
    """
    aligns_a_c = []
    for idx_a, idx_b_in_ab in aligns_a_b.alignments:
        for idx_b_in_bc, idx_c in aligns_b_c.alignments:
            if idx_b_in_ab == idx_b_in_bc:
                aligns_a_c.append((idx_a, idx_c))
    return AlignedSequences(
        source_tokens=aligns_a_b.source_tokens.copy(),
        target_tokens=aligns_b_c.target_tokens.copy(),
        alignments=aligns_a_c.copy(),
    )
def add_alignment_extra_positions(
    alignments: list[tuple[int, int]], extra_positions: list[tuple[int, int]]
) -> list[tuple[int, int]]:
    for x_idx_a, x_idx_b in extra_positions:
        for pos, (idx_a, idx_b) in enumerate(alignments):
            a_val, b_val = idx_a, idx_b
            if idx_a >= x_idx_a:
                a_val += 1
            if idx_b >= x_idx_b:
                b_val += 1
            alignments[pos] = (a_val, b_val)
    return alignments + extra_positions
def auto_align_sequences(
    a_sequence: str | None = None,
    a_tokens: list[str] | None = None,
    b_sequence: str | None = None,
    b_tokens: list[str] | None = None,
    filter_special_tokens: list[str] = [],
    split_pattern: str = r"\s+|\b",
) -> AlignedSequences:
    if not a_sequence or not b_sequence or not a_tokens or not b_tokens:
        raise ValueError(
            "Missing required arguments to compute alignments. Please provide target and contrast sequence and tokens."
        )
    try:
        for token in filter_special_tokens:
            b_sequence = b_sequence.replace(token, "")
        # 1. Use aligner to get alignments at word level
        # Alignments are target to contrast word-level alignment pairs
        a_words = [word for word in re.split(split_pattern, a_sequence) if word]
        b_words = [word for word in re.split(split_pattern, b_sequence) if word]
        a_to_b_word_align = compute_word_aligns(a_words, b_words)
        # 2. Align word-level alignments to token-level alignments from the generative model tokenizer.
        # Requires cleaning up the model tokens from special tokens (special characters already removed)
        clean_a_tokens, removed_a_token_idxs = clean_tokens(a_tokens, filter_special_tokens, return_removed_idxs=True)
        clean_b_tokens, removed_b_token_idxs = clean_tokens(b_tokens, filter_special_tokens, return_removed_idxs=True)
        if len(removed_a_token_idxs) != len(removed_b_token_idxs):
            logger.debug(
                "The number of special tokens in the target and contrast sequences do not match. "
                "Trying to match special tokens based on their identity."
            )
            removed_a_tokens = [a_tokens[idx] for idx in removed_a_token_idxs]
            removed_b_tokens = [b_tokens[idx] for idx in removed_b_token_idxs]
            aligned_special_tokens = []
            for curr_idx, rm_a in enumerate(removed_a_tokens):
                rm_a_idx = removed_a_token_idxs[curr_idx]
                if rm_a not in removed_b_tokens:
                    aligned_special_tokens.append((rm_a_idx, rm_a_idx))
                else:
                    rm_b_idx = removed_b_token_idxs[removed_b_tokens.index(rm_a)]
                    aligned_special_tokens.append((rm_a_idx, rm_b_idx))
        else:
            aligned_special_tokens = list(zip(removed_a_token_idxs, removed_b_token_idxs, strict=False))
        a_word_to_token_align = align_tokenizations(a_words, clean_a_tokens)
        b_word_to_token_align = align_tokenizations(b_words, clean_b_tokens)
        # 3. Propagate word-level alignments to token-level alignments.
        # target token-level -> target word-level -> contrast word-level -> contrast token-level
        # First step: get target token-level -> contrast word-level
        a_token_to_word_align = a_word_to_token_align.reverse()
        a_token_to_b_word_align = propagate_alignments(a_token_to_word_align, a_to_b_word_align)
        # Second step: get target token-level -> contrast token-level using previous step outputs
        a_to_b_token_align = propagate_alignments(a_token_to_b_word_align, b_word_to_token_align)
        # 4. Add special tokens alignments
        a_to_b_aligns_with_special_tokens = add_alignment_extra_positions(
            a_to_b_token_align.alignments.copy(), aligned_special_tokens
        )
        return AlignedSequences(
            source_tokens=a_tokens,
            target_tokens=b_tokens,
            alignments=a_to_b_aligns_with_special_tokens,
        )
    except Exception as e:
        logger.error(
            "Failed to compute alignments using the aligner. "
            f"Please check the following error and provide custom alignments if needed.\n{e}"
        )
        raise e
def get_adjusted_alignments(
    alignments: list[tuple[int, int]] | str,
    target_sequence: str | None = None,
    target_tokens: list[str] | None = None,
    contrast_sequence: str | None = None,
    contrast_tokens: list[str] | None = None,
    fill_missing: bool = False,
    special_tokens: list[str] = [],
    start_pos: int = 0,
    end_pos: int | None = None,
) -> list[tuple[int, int]]:
    is_auto_aligned = False
    if fill_missing and not target_tokens:
        raise ValueError("Missing target tokens. Please provide target tokens to fill missing alignments.")
    if alignments is None:
        alignments = []
    if end_pos is None:
        end_pos = len(target_tokens)
    elif isinstance(alignments, str):
        if alignments == AlignmentMethod.AUTO.value:
            alignments = auto_align_sequences(
                a_sequence=target_sequence,
                a_tokens=target_tokens,
                b_sequence=contrast_sequence,
                b_tokens=contrast_tokens,
                filter_special_tokens=special_tokens,
            ).alignments
            alignments = [(a_idx, b_idx) for a_idx, b_idx in alignments if start_pos <= a_idx < end_pos]
            is_auto_aligned = True
            logger.debug(
                f"Using {ALIGN_MODEL_ID} for automatic alignments. Provide custom alignments for non-linguistic "
                f"sequences, or for languages not covered by the aligner."
            )
        else:
            raise ValueError(
                f"Unknown alignment method: {alignments}. "
                f"Available methods: {','.join([m.value for m in AlignmentMethod])}"
            )
    # Sort alignments
    alignments = sorted(set(alignments), key=lambda x: (x[0], x[1]))
    # Filter alignments (restrict to one per token)
    filter_aligns = []
    if len(alignments) > 0:
        for pair_idx in range(start_pos, end_pos):
            match_pairs = [(p0, p1) for p0, p1 in alignments if p0 == pair_idx and 0 <= p1 < len(contrast_tokens)]
            if match_pairs:
                # If found, use the first match that containing an unaligned target token, first match otherwise
                match_pairs_unaligned = [p for p in match_pairs if p[1] not in [f[1] for f in filter_aligns]]
                valid_match = match_pairs_unaligned[0] if match_pairs_unaligned else match_pairs[0]
                filter_aligns.append(valid_match)
    # Filling alignments with missing tokens
    if fill_missing:
        filled_alignments = filter_aligns.copy()
        for step_idx, pair_idx in enumerate(reversed(range(start_pos, end_pos)), start=1):
            match_pairs = [(p0, p1) for p0, p1 in filter_aligns if p0 == pair_idx and 0 <= p1 < len(contrast_tokens)]
            # Default behavior: fill missing alignments with 1:1 position alignments starting from the bottom of the
            # two sequences
            if not match_pairs:
                if (len(contrast_tokens) - step_idx) > 0:
                    filled_alignments.append((pair_idx, len(contrast_tokens) - step_idx))
                else:
                    filled_alignments.append((pair_idx, len(contrast_tokens) - 1))
        if filter_aligns != filled_alignments:
            existing_aligns_message = (
                f"Provided target alignments do not cover all {end_pos - start_pos} tokens from the original sequence."
            )
            no_aligns_message = (
                "No target alignments were provided for the contrastive target. "
                "Use e.g. 'contrast_targets_alignments=[(0,1), ...] to provide them in model.attribute"
            )
            logger.debug(
                f"{existing_aligns_message if filter_aligns else no_aligns_message}\n"
                "Filling missing position with right-aligned 1:1 position alignments."
            )
            filter_aligns = sorted(set(filled_alignments), key=lambda x: (x[0], x[1]))
    if is_auto_aligned or (fill_missing and filter_aligns != filled_alignments):
        logger.debug(f"Generated alignments: {filter_aligns}")
    return filter_aligns
def get_aligned_idx(a_idx: int, alignments: list[tuple[int, int]]) -> int:
    if alignments:
        # Find all alignment pairs for the current original target
        aligned_idxs = [t_idx for s_idx, t_idx in alignments if s_idx == a_idx]
        if not aligned_idxs:
            # To be handled separately
            return -1
        # Select the minimum index to identify the next target token
        return min(aligned_idxs)
    return a_idx

================
File: inseq/utils/argparse.py
================
# Copyright 2020 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import dataclasses
import json
import sys
import types
from argparse import ArgumentDefaultsHelpFormatter, ArgumentParser, ArgumentTypeError
from collections.abc import Callable, Iterable
from copy import copy
from enum import Enum
from inspect import isclass
from pathlib import Path
from typing import Any, Literal, NewType, Union, get_type_hints
import yaml
DataClass = NewType("DataClass", Any)
DataClassType = NewType("DataClassType", Any)
# From https://stackoverflow.com/questions/15008758/parsing-boolean-values-with-argparse
def string_to_bool(v):
    if isinstance(v, bool):
        return v
    if v.lower() in ("yes", "true", "t", "y", "1"):
        return True
    elif v.lower() in ("no", "false", "f", "n", "0"):
        return False
    else:
        raise ArgumentTypeError(
            f"Truthy value expected: got {v} but expected one of yes/no, true/false, t/f, y/n, 1/0 (case insensitive)."
        )
def make_choice_type_function(choices: list) -> Callable[[str], Any]:
    """
    Creates a mapping function from each choices string representation to the actual value. Used to support multiple
    value types for a single argument.
    Args:
        choices (list): List of choices.
    Returns:
        Callable[[str], Any]: Mapping function from string representation to actual value for each choice.
    """
    str_to_choice = {str(choice): choice for choice in choices}
    return lambda arg: str_to_choice.get(arg, arg)
def cli_arg(
    *,
    aliases: str | list[str] = None,
    help: str = None,
    default: Any = dataclasses.MISSING,
    default_factory: Callable[[], Any] = dataclasses.MISSING,
    choices: list[Any] = None,
    metadata: dict = None,
    **kwargs,
) -> dataclasses.Field:
    """Argument helper enabling a concise syntax to create dataclass fields for parsing with `InseqArgumentParser`.
    Example comparing the use of `cli_arg` and `dataclasses.field`:
    ```
    @dataclass
    class Args:
        regular_arg: str = dataclasses.field(default="Test", metadata={"aliases": ["--example", "-e"], "help": "Long"})
        cli_arg: str = cli_arg(default="Test", aliases=["--example", "-e"], help="What a nice syntax!")
    ```
    Args:
        aliases (Union[str, List[str]], optional):
            Single string or list of strings of aliases to pass on to argparse, e.g. `aliases=["--example", "-e"]`.
            Defaults to None.
        help (str, optional): Help string to pass on to argparse that can be displayed with --help. Defaults to None.
        default (Any, optional):
            Default value for the argument. If not default or default_factory is specified, the argument is required.
            Defaults to dataclasses.MISSING.
        default_factory (Callable[[], Any], optional):
            The default_factory is a 0-argument function called to initialize a field's value. It is useful to provide
            default values for mutable types, e.g. lists: `default_factory=list`. Mutually exclusive with `default=`.
            Defaults to dataclasses.MISSING.
        choices (List[Any], optional):
            List of choices. If specified, the argument will be restricted to these choices. Defaults to None.
        metadata (dict, optional): Further metadata to pass on to `dataclasses.field`. Defaults to None.
    Returns:
        Field: A `dataclasses.Field` with the desired properties.
    """
    if metadata is None:
        # Important, don't use as default param in function signature: dict is mutable and shared across function calls
        metadata = {}
    if aliases is not None:
        metadata["aliases"] = aliases
    if help is not None:
        metadata["help"] = help
    if choices is not None:
        metadata["choices"] = choices
    return dataclasses.field(metadata=metadata, default=default, default_factory=default_factory, **kwargs)
class InseqArgumentParser(ArgumentParser):
    """Adapted from https://github.com/huggingface/transformers/blob/main/src/transformers/hf_argparser.py.
    This subclass of `argparse.ArgumentParser` uses type hints on dataclasses to generate arguments.
    The class is designed to play well with the native argparse. In particular, you can add more (non-dataclass backed)
    arguments to the parser after initialization and you'll get the output back after parsing as an additional
    namespace. Optional: To create sub argument groups use the `_argument_group_name` attribute in the dataclass.
    """
    dataclass_types: Iterable[DataClassType]
    def __init__(self, dataclass_types: DataClassType | Iterable[DataClassType] | None = None, **kwargs):
        """
        Args:
            dataclass_types (`Union[DataClassType, Iterable[DataClassType]]`, *optional*):
                Dataclass type, or list of dataclass types for which we will "fill" instances with the parsed args.
            kwargs (`Dict[str, Any]`, *optional*):
                Passed to `argparse.ArgumentParser()` in the regular way.
        """
        # To make the default appear when using --help
        if "formatter_class" not in kwargs:
            kwargs["formatter_class"] = ArgumentDefaultsHelpFormatter
        super().__init__(**kwargs)
        if dataclass_types is not None:
            if dataclasses.is_dataclass(dataclass_types):
                dataclass_types = [dataclass_types]
            self.dataclass_types = list(dataclass_types)
            for dtype in self.dataclass_types:
                self._add_dataclass_arguments(dtype)
    @staticmethod
    def _parse_dataclass_field(parser: ArgumentParser, field: dataclasses.Field):
        field_name = f"--{field.name}"
        kwargs = field.metadata.copy()
        # field.metadata is not used at all by Data Classes,
        # it is provided as a third-party extension mechanism.
        if isinstance(field.type, str):
            raise RuntimeError(
                "Unresolved type detected, which should have been done with the help of "
                "`typing.get_type_hints` method by default"
            )
        aliases = kwargs.pop("aliases", [])
        if isinstance(aliases, str):
            aliases = [aliases]
        origin_type = getattr(field.type, "__origin__", field.type)
        if origin_type is Union or (hasattr(types, "UnionType") and isinstance(origin_type, types.UnionType)):
            if str not in field.type.__args__ and (
                len(field.type.__args__) != 2 or type(None) not in field.type.__args__
            ):
                raise ValueError(
                    "Only `Union[X, NoneType]` (i.e., `Optional[X]`) is allowed for `Union` because"
                    " the argument parser only supports one type per argument."
                    f" Problem encountered in field '{field.name}'."
                )
            if type(None) not in field.type.__args__:
                # filter `str` in Union
                field.type = field.type.__args__[0] if field.type.__args__[1] == str else field.type.__args__[1]
                origin_type = getattr(field.type, "__origin__", field.type)
            elif bool not in field.type.__args__:
                # filter `NoneType` in Union (except for `Union[bool, NoneType]`)
                field.type = (
                    field.type.__args__[0] if isinstance(None, field.type.__args__[1]) else field.type.__args__[1]
                )
                origin_type = getattr(field.type, "__origin__", field.type)
        # A variable to store kwargs for a boolean field, if needed
        # so that we can init a `no_*` complement argument (see below)
        bool_kwargs = {}
        if origin_type is Literal or (isinstance(field.type, type) and issubclass(field.type, Enum)):
            if origin_type is Literal:
                kwargs["choices"] = field.type.__args__
            else:
                kwargs["choices"] = [x.value for x in field.type]
            kwargs["type"] = make_choice_type_function(kwargs["choices"])
            if field.default is not dataclasses.MISSING:
                kwargs["default"] = field.default
            else:
                kwargs["required"] = True
        elif field.type is bool or field.type == bool | None:
            # Copy the currect kwargs to use to instantiate a `no_*` complement argument below.
            # We do not initialize it here because the `no_*` alternative must be instantiated after the real argument
            bool_kwargs = copy(kwargs)
            # Hack because type=bool in argparse does not behave as we want.
            kwargs["type"] = string_to_bool
            if field.type is bool or (field.default is not None and field.default is not dataclasses.MISSING):
                # Default value is False if we have no default when of type bool.
                default = False if field.default is dataclasses.MISSING else field.default
                # This is the value that will get picked if we don't include --field_name in any way
                kwargs["default"] = default
                # This tells argparse we accept 0 or 1 value after --field_name
                kwargs["nargs"] = "?"
                # This is the value that will get picked if we do --field_name (without value)
                kwargs["const"] = True
        elif field.type is dict:
            kwargs["type"] = json.loads
            kwargs["default"] = {}
        elif isclass(origin_type) and issubclass(origin_type, list):
            kwargs["type"] = field.type.__args__[0]
            kwargs["nargs"] = "+"
            if field.default_factory is not dataclasses.MISSING:
                kwargs["default"] = field.default_factory()
            elif field.default is dataclasses.MISSING:
                kwargs["required"] = True
        else:
            kwargs["type"] = field.type
            if field.default is not dataclasses.MISSING:
                kwargs["default"] = field.default
            elif field.default_factory is not dataclasses.MISSING:
                kwargs["default"] = field.default_factory()
            else:
                kwargs["required"] = True
        parser.add_argument(field_name, *aliases, **kwargs)
        # Add a complement `no_*` argument for a boolean field AFTER the initial field has already been added.
        # Order is important for arguments with the same destination!
        # We use a copy of earlier kwargs because the original kwargs have changed a lot before reaching down
        # here and we do not need those changes/additional keys.
        if field.default is True and (field.type is bool or field.type == bool | None):
            bool_kwargs["default"] = False
            parser.add_argument(f"--no_{field.name}", action="store_false", dest=field.name, **bool_kwargs)
    def _add_dataclass_arguments(self, dtype: DataClassType):
        if hasattr(dtype, "_argument_group_name"):
            parser = self.add_argument_group(dtype._argument_group_name)
        else:
            parser = self
        try:
            type_hints: dict[str, type] = get_type_hints(dtype)
        except NameError as ex:
            raise RuntimeError(
                f"Type resolution failed for {dtype}. Try declaring the class in global scope or "
                "removing line of `from __future__ import annotations` which opts in Postponed "
                "Evaluation of Annotations (PEP 563)"
            ) from ex
        for field in dataclasses.fields(dtype):
            if not field.init:
                continue
            field.type = type_hints[field.name]
            self._parse_dataclass_field(parser, field)
    def parse_args_into_dataclasses(
        self,
        args=None,
        return_remaining_strings=False,
        look_for_args_file=True,
        args_filename=None,
        args_file_flag=None,
    ) -> tuple[DataClass, ...]:
        """
        Parse command-line args into instances of the specified dataclass types.
        This relies on argparse's `ArgumentParser.parse_known_args`. See the doc at:
        docs.python.org/3.7/library/argparse.html#argparse.ArgumentParser.parse_args
        Args:
            args:
                List of strings to parse. The default is taken from sys.argv. (same as argparse.ArgumentParser)
            return_remaining_strings:
                If true, also return a list of remaining argument strings.
            look_for_args_file:
                If true, will look for a ".args" file with the same base name as the entry point script for this
                process, and will append its potential content to the command line args.
            args_filename:
                If not None, will uses this file instead of the ".args" file specified in the previous argument.
            args_file_flag:
                If not None, will look for a file in the command-line args specified with this flag. The flag can be
                specified multiple times and precedence is determined by the order (last one wins).
        Returns:
            Tuple consisting of:
                - the dataclass instances in the same order as they were passed to the initializer.abspath
                - if applicable, an additional namespace for more (non-dataclass backed) arguments added to the parser
                  after initialization.
                - The potential list of remaining argument strings. (same as argparse.ArgumentParser.parse_known_args)
        """
        if args_file_flag or args_filename or (look_for_args_file and len(sys.argv)):
            args_files = []
            if args_filename:
                args_files.append(Path(args_filename))
            elif look_for_args_file and len(sys.argv):
                args_files.append(Path(sys.argv[0]).with_suffix(".args"))
            # args files specified via command line flag should overwrite default args files so we add them last
            if args_file_flag:
                # Create special parser just to extract the args_file_flag values
                args_file_parser = ArgumentParser()
                args_file_parser.add_argument(args_file_flag, type=str, action="append")
                # Use only remaining args for further parsing (remove the args_file_flag)
                cfg, args = args_file_parser.parse_known_args(args=args)
                cmd_args_file_paths = vars(cfg).get(args_file_flag.lstrip("-"), None)
                if cmd_args_file_paths:
                    args_files.extend([Path(p) for p in cmd_args_file_paths])
            file_args = []
            for args_file in args_files:
                if args_file.exists():
                    file_args += args_file.read_text().split()
            # in case of duplicate arguments the last one has precedence
            # args specified via the command line should overwrite args from files, so we add them last
            args = file_args + args if args is not None else file_args + sys.argv[1:]
        namespace, remaining_args = self.parse_known_args(args=args)
        outputs = []
        for dtype in self.dataclass_types:
            keys = {f.name for f in dataclasses.fields(dtype) if f.init}
            inputs = {k: v for k, v in vars(namespace).items() if k in keys}
            for k in keys:
                delattr(namespace, k)
            obj = dtype(**inputs)
            outputs.append(obj)
        if len(namespace.__dict__) > 0:
            # additional namespace.
            outputs.append(namespace)
        if return_remaining_strings:
            return (*outputs, remaining_args)
        else:
            if remaining_args:
                raise ValueError(f"Some specified arguments are not used by the HfArgumentParser: {remaining_args}")
            return (*outputs,)
    def parse_dict(self, args: dict[str, Any], allow_extra_keys: bool = False) -> tuple[DataClass, ...]:
        """
        Alternative helper method that does not use `argparse` at all, instead uses a dict and populating the dataclass
        types.
        Args:
            args (`dict`):
                dict containing config values
            allow_extra_keys (`bool`, *optional*, defaults to `False`):
                Defaults to False. If False, will raise an exception if the dict contains keys that are not parsed.
        Returns:
            Tuple consisting of:
                - the dataclass instances in the same order as they were passed to the initializer.
        """
        unused_keys = set(args.keys())
        outputs = []
        for dtype in self.dataclass_types:
            keys = {f.name for f in dataclasses.fields(dtype) if f.init}
            inputs = {k: v for k, v in args.items() if k in keys}
            unused_keys.difference_update(inputs.keys())
            obj = dtype(**inputs)
            outputs.append(obj)
        if not allow_extra_keys and unused_keys:
            raise ValueError(f"Some keys are not used by the HfArgumentParser: {sorted(unused_keys)}")
        return tuple(outputs)
    def parse_json_file(self, json_file: str, allow_extra_keys: bool = False) -> tuple[DataClass, ...]:
        """
        Alternative helper method that does not use `argparse` at all, instead loading a json file and populating the
        dataclass types.
        Args:
            json_file (`str` or `os.PathLike`):
                File name of the json file to parse
            allow_extra_keys (`bool`, *optional*, defaults to `False`):
                Defaults to False. If False, will raise an exception if the json file contains keys that are not
                parsed.
        Returns:
            Tuple consisting of:
                - the dataclass instances in the same order as they were passed to the initializer.
        """
        with open(Path(json_file), encoding="utf-8") as open_json_file:
            data = json.loads(open_json_file.read())
        outputs = self.parse_dict(data, allow_extra_keys=allow_extra_keys)
        return tuple(outputs)
    def parse_yaml_file(self, yaml_file: str, allow_extra_keys: bool = False) -> tuple[DataClass, ...]:
        """
        Alternative helper method that does not use `argparse` at all, instead loading a yaml file and populating the
        dataclass types.
        Args:
            yaml_file (`str` or `os.PathLike`):
                File name of the yaml file to parse
            allow_extra_keys (`bool`, *optional*, defaults to `False`):
                Defaults to False. If False, will raise an exception if the json file contains keys that are not
                parsed.
        Returns:
            Tuple consisting of:
                - the dataclass instances in the same order as they were passed to the initializer.
        """
        outputs = self.parse_dict(yaml.safe_load(Path(yaml_file).read_text()), allow_extra_keys=allow_extra_keys)
        return tuple(outputs)

================
File: inseq/utils/cache.py
================
import logging
import os
import pickle
from collections.abc import Callable
from functools import wraps
from pathlib import Path
from typing import Any
logger = logging.getLogger(__name__)
# Cache location
DEFAULT_XDG_CACHE_HOME = "~/.cache"
XDG_CACHE_HOME = os.getenv("XDG_CACHE_HOME", DEFAULT_XDG_CACHE_HOME)
DEFAULT_INSEQ_HOME_CACHE = os.path.join(XDG_CACHE_HOME, "inseq")
INSEQ_HOME_CACHE = os.path.expanduser(os.getenv("INSEQ_HOME", DEFAULT_INSEQ_HOME_CACHE))
DEFAULT_INSEQ_ARTIFACTS_CACHE = os.path.join(INSEQ_HOME_CACHE, "artifacts")
INSEQ_ARTIFACTS_CACHE = Path(os.getenv("INSEQ_ARTIFACTS_CACHE", DEFAULT_INSEQ_ARTIFACTS_CACHE))
def cache_results(func: Callable[[Any], Any]) -> Callable[[Any], Any]:
    @wraps(func)
    def cache_results_wrapper(
        cache_dir: str,
        cache_filename: str,
        save_cache: bool = True,
        overwrite_cache: bool = False,
        *args,
        **kwargs,
    ):
        cache_dir = os.path.expanduser(cache_dir)
        if not os.path.exists(cache_dir):
            os.makedirs(cache_dir)
        if os.path.exists(cache_filename) and not overwrite_cache:
            logger.info(f"Loading cached objects from {cache_filename}")
            with open(cache_filename, "rb") as f:
                cached = pickle.load(f)  # nosec
        else:
            logger.info(f"Cached objects not found in {cache_filename}. Computing...")
            cached = func(*args, **kwargs)
            if save_cache:
                with open(cache_filename, "wb") as f:
                    pickle.dump(cached, f)
        return cached
    return cache_results_wrapper

================
File: inseq/utils/contrast_utils.py
================
import logging
import warnings
from collections.abc import Callable
from dataclasses import dataclass
from typing import TYPE_CHECKING
import torch
from ..data import (
    DecoderOnlyBatch,
    EncoderDecoderBatch,
    FeatureAttributionInput,
    get_batch_from_inputs,
    slice_batch_from_position,
)
from ..utils.typing import TargetIdsTensor
if TYPE_CHECKING:
    from ..attr.step_functions import StepFunction, StepFunctionArgs
logger = logging.getLogger(__name__)
CONTRAST_FN_ARGS_DOCSTRING = """Args:
        contrast_sources (:obj:`str` or :obj:`list(str)`): Source text(s) used as contrastive inputs to compute
            the contrastive step function for encoder-decoder models. If not specified, the source text is assumed to
            match the original source text. Defaults to :obj:`None`.
        contrast_targets (:obj:`str` or :obj:`list(str)`): Contrastive target text(s) to be compared to the original
            target text. If not specified, the original target text is used as contrastive target (will result in same
            output unless ``contrast_sources`` are specified). Defaults to :obj:`None`.
        contrast_targets_alignments (:obj:`list(tuple(int, int))`, `optional`): A list of tuples of indices, where the
            first element is the index of the original target token and the second element is the index of the
            contrastive target token, used only if :obj:`contrast_targets` is specified. If an explicit alignment is
            not specified, the alignment of the original and contrastive target texts is assumed to be 1:1 for all
            available tokens. Defaults to :obj:`None`.
        contrast_force_inputs (:obj:`bool`, `optional`, defaults to :obj:`False`): Whether to force the contrastive
            inputs to be used for attribution. Defaults to :obj:`False`.
"""
def contrast_fn_docstring() -> Callable[..., "StepFunction"]:
    def docstring_decorator(fn: "StepFunction") -> "StepFunction":
        """Returns the docstring for the contrastive step functions."""
        if fn.__doc__ is not None:
            if "Args:\n" in fn.__doc__:
                fn.__doc__ = fn.__doc__.replace("Args:\n", CONTRAST_FN_ARGS_DOCSTRING)
            else:
                fn.__doc__ = fn.__doc__ + "\n    " + CONTRAST_FN_ARGS_DOCSTRING
        else:
            fn.__doc__ = CONTRAST_FN_ARGS_DOCSTRING
        return fn
    return docstring_decorator
@dataclass
class ContrastInputs:
    batch: EncoderDecoderBatch | DecoderOnlyBatch | None = None
    target_ids: TargetIdsTensor | None = None
def _get_contrast_inputs(
    args: "StepFunctionArgs",
    contrast_sources: FeatureAttributionInput | None = None,
    contrast_targets: FeatureAttributionInput | None = None,
    contrast_targets_alignments: list[list[tuple[int, int]]] | None = None,
    return_contrastive_target_ids: bool = False,
    return_contrastive_batch: bool = False,
    skip_special_tokens: bool = False,
    **forward_kwargs,
) -> ContrastInputs:
    """Utility function to return the output of the model for given contrastive inputs.
    Args:
        return_contrastive_target_ids (:obj:`bool`, `optional`, defaults to :obj:`False`): Whether to return the
            contrastive target ids as well as the model output. Defaults to :obj:`False`.
        **forward_kwargs: Additional keyword arguments to be passed to the model's forward pass.
    """
    c_tgt_ids = None
    is_enc_dec = args.attribution_model.is_encoder_decoder
    if contrast_targets:
        c_batch = DecoderOnlyBatch.from_batch(
            get_batch_from_inputs(
                attribution_model=args.attribution_model,
                inputs=contrast_targets,
                as_targets=is_enc_dec,
                skip_special_tokens=skip_special_tokens,
            )
        ).to(args.decoder_input_ids.device)
        curr_prefix_len = args.decoder_input_ids.size(1)
        c_batch, c_tgt_ids = slice_batch_from_position(c_batch, curr_prefix_len, contrast_targets_alignments)
        if args.decoder_input_ids.size(0) != c_batch.target_ids.size(0):
            raise ValueError(
                f"Contrastive batch size ({c_batch.target_ids.size(0)}) must match candidate batch size"
                f" ({args.decoder_input_ids.size(0)}). Multi-sentence attribution and methods expanding inputs to"
                " multiple steps (e.g. Integrated Gradients) are not yet supported for contrastive attribution."
            )
        if (
            args.decoder_input_ids.shape != c_batch.target_ids.shape
            or torch.ne(args.decoder_input_ids, c_batch.target_ids).any()
        ):
            args.decoder_input_ids = c_batch.target_ids
            args.decoder_input_embeds = c_batch.target_embeds
            args.decoder_attention_mask = c_batch.target_mask
    if contrast_sources:
        from ..attr.step_functions import StepFunctionEncoderDecoderArgs
        if not (is_enc_dec and isinstance(args, StepFunctionEncoderDecoderArgs)):
            raise ValueError(
                "Contrastive source inputs can only be used with encoder-decoder models. "
                "Use `contrast_targets` to set a contrastive target containing a prefix for decoder-only models."
            )
        c_enc_in = args.attribution_model.encode(contrast_sources, add_special_tokens=not skip_special_tokens).to(
            args.encoder_input_ids.device
        )
        if (
            args.encoder_input_ids.shape != c_enc_in.input_ids.shape
            or torch.ne(args.encoder_input_ids, c_enc_in.input_ids).any()
        ):
            args.encoder_input_ids = c_enc_in.input_ids
            args.encoder_input_embeds = args.attribution_model.embed(args.encoder_input_ids, as_targets=False)
            args.encoder_attention_mask = c_enc_in.attention_mask
    c_batch = args.attribution_model.formatter.convert_args_to_batch(args)
    return ContrastInputs(
        batch=c_batch if return_contrastive_batch else None,
        target_ids=c_tgt_ids if return_contrastive_target_ids else None,
    )
def _setup_contrast_args(
    args: "StepFunctionArgs",
    contrast_sources: FeatureAttributionInput | None = None,
    contrast_targets: FeatureAttributionInput | None = None,
    contrast_targets_alignments: list[list[tuple[int, int]]] | None = None,
    contrast_force_inputs: bool = False,
    skip_special_tokens: bool = False,
):
    c_inputs = _get_contrast_inputs(
        args,
        contrast_sources=contrast_sources,
        contrast_targets=contrast_targets,
        contrast_targets_alignments=contrast_targets_alignments,
        return_contrastive_target_ids=True,
        return_contrastive_batch=True,
        skip_special_tokens=skip_special_tokens,
    )
    if args.is_attributed_fn:
        if contrast_force_inputs:
            warnings.warn(
                "Forcing contrastive inputs to be used for attribution. This may result in unexpected behavior for "
                "gradient-based attribution methods.",
                stacklevel=1,
            )
        else:
            warnings.warn(
                "Contrastive inputs do not match original inputs when using a contrastive attributed function.\n"
                "By default we force the original inputs to be used (i.e. only the contrastive predicted target is "
                "different).\nThis is a requirement for gradient-based attribution method, as contrastive inputs don't"
                " participate in gradient computation.\nFor attribution methods with less stringent requirements, "
                "set --contrast_force_inputs to True to use the contrastive inputs for attribution instead.",
                stacklevel=1,
            )
    use_original_output = args.is_attributed_fn and not contrast_force_inputs
    if use_original_output:
        forward_output = args.forward_output
    else:
        forward_output = args.attribution_model.get_forward_output(
            c_inputs.batch, use_embeddings=args.attribution_model.is_encoder_decoder
        )
    c_args = args.attribution_model.formatter.format_step_function_args(
        args.attribution_model,
        forward_output=forward_output,
        target_ids=c_inputs.target_ids if c_inputs.target_ids is not None else args.target_ids,
        batch=c_inputs.batch,
        is_attributed_fn=args.is_attributed_fn,
    )
    return c_args

================
File: inseq/utils/errors.py
================
from typing import Any
from .registry import available_classes
class InseqDeprecationWarning(UserWarning):
    """Special deprecation warning because the built-in one is ignored by default."""
    def __init__(self, msg):
        super().__init__(msg)
class UnknownAttributionMethodError(Exception):
    """Raised when an attribution method is not valid."""
    UNKNOWN_ATTRIBUTION_METHOD_MSG = (
        "Unknown attribution method: {attribution_method}.\nAvailable methods: {available_methods}"
    )
    def __init__(
        self,
        method_name: str,
        msg: str = UNKNOWN_ATTRIBUTION_METHOD_MSG,
        *args: tuple[Any],
    ) -> None:
        from inseq.attr import FeatureAttribution
        msg = msg.format(
            attribution_method=method_name,
            available_methods=", ".join(available_classes(FeatureAttribution)),
        )
        super().__init__(msg, *args)
class MissingAttributionMethodError(Exception):
    """Raised when an attribution method is not found."""
    MISSING_ATTRIBUTION_METHOD_MSG = (
        "Attribution methods is not set. "
        "You can either define it permanently when instancing the AttributionModel, "
        "or pass it to the attribute method.\nAvailable methods: {available_methods}"
    )
    def __init__(self, msg: str = MISSING_ATTRIBUTION_METHOD_MSG, *args: tuple[Any]) -> None:
        from inseq.attr import FeatureAttribution
        msg = msg.format(available_methods=", ".join(available_classes(FeatureAttribution)))
        super().__init__(msg, *args)
class LengthMismatchError(Exception):
    """Raised when lengths do not match."""
    pass
class MissingAlignmentsError(Exception):
    """Raised when lengths do not match."""
    pass

================
File: inseq/utils/hooks.py
================
import re
from collections.abc import Callable
from inspect import getsourcelines
from sys import gettrace, settrace
from types import FrameType
from torch import nn
from .misc import get_left_padding
def get_last_variable_assignment_position(
    module: nn.Module,
    varname: str,
    fname: str = "forward",
) -> int | None:
    """Extract the code line number of the last variable assignment for a variable of interest in the specified method
    of a `nn.Module` object.
    Args:
        module (`nn.Module`):
            A PyTorch module containing a method with a variable assignment after which the hook should be executed.
        varname (`str`):
            The name of the variable to use as anchor for the hook.
        fname (`str`, *optional*, defaults to "forward"):
            The name of the method in which the variable assignment should be searched.
    Returns:
        `Optional[int]`: Returns the line number in the file (not relative to the method) of the last variable
        assignment. Returns None if no assignment to the variable was found.
    """
    # Matches any assignment of variable varname
    pattern = rf"^\s*(?:\w+\s*,\s*)*\b{varname}\b\s*(?:,.+\s*)*=\s*[^\W=]+.*$"
    code, startline = getsourcelines(getattr(module, fname))
    line_numbers = []
    i = 0
    while i < len(code):
        line = code[i]
        # Handles multi-line assignments
        if re.match(pattern, line):
            parentheses_count = line.count("(") - line.count(")")
            ends_with_newline = lambda l: l.strip().endswith("\\")
            follow_indent = lambda l, i: len(code) > i + 1 and get_left_padding(code[i + 1]) > get_left_padding(l)
            while (ends_with_newline(line) or follow_indent(line, i) or parentheses_count > 0) and len(code) > i + 1:
                i += 1
                line = code[i]
                parentheses_count += line.count("(") - line.count(")")
            line_numbers.append(i)
        i += 1
    if len(line_numbers) == 0:
        return None
    return line_numbers[-1] + startline + 1
def get_post_variable_assignment_hook(
    module: nn.Module,
    varname: str,
    fname: str = "forward",
    hook_fn: Callable[[FrameType], None] = lambda **kwargs: None,
    **kwargs,
) -> Callable[[], None]:
    """Creates a hook that is called after the last variable assignment in the specified method of a `nn.Module`.
    This is a hacky method using the ``sys.settrace()`` function to circumvent the limited hook points of Pytorch hooks
    and set a custom hook point dynamically. This approach is preferred to ensure a broader compatibility with Hugging
    Face transformers models that do not provide hook points in their architectures for the moment.
    Args:
        module (`nn.Module`):
            A PyTorch module containing a method with a variable assignment after which the hook should be executed.
        varname (`str`):
            The name of the variable to use as anchor for the hook.
        fname (`str`, *optional*, defaults to "forward"):
            The name of the method in which the variable assignment should be searched.
        hook_fn (`Callable[[FrameType], None]`, *optional*, defaults to lambdaframe):
            A custom hook function that is called after the last variable assignment in the specified method. The first
            parameter is the current frame in the execution at the hook point, and any additional arguments can be
            passed when creating the hook. ``frame.f_locals`` is a dictionary containing all local variables.
    Returns:
        The hook function that can be registered with the module. If hooking the module's ``forward()`` method, the
        hook can be registered with Pytorch native hook methods.
    """
    hook_line_num = get_last_variable_assignment_position(module, varname, fname)
    curr_trace_fn = gettrace()
    if hook_line_num is None:
        raise ValueError(f"Could not find assignment to {varname} in {module}'s {fname}() method")
    def var_tracer(frame, event, arg=None):
        curr_line_num = frame.f_lineno
        curr_func_name = frame.f_code.co_name
        # Matches the first executable line after hook_line_num in the same function of the same module
        if (
            event == "line"
            and curr_line_num >= hook_line_num
            and curr_func_name == fname
            and isinstance(frame.f_locals.get("self"), nn.Module)
            and frame.f_locals.get("self")._get_name() == module._get_name()
        ):
            # Call the custom hook providing the current frame and any additional arguments as context
            hook_fn(frame, **kwargs)
            settrace(curr_trace_fn)
        return var_tracer
    def hook(*args, **kwargs):
        settrace(var_tracer)
    return hook

================
File: inseq/utils/id_utils.py
================
from ..attr import STEP_SCORES_MAP, FeatureAttribution
from ..data import AggregationFunction, Aggregator
INSEQ_ID_MAP = {
    **FeatureAttribution.available_classes(),
    **Aggregator.available_classes(),
    **AggregationFunction.available_classes(),
    **STEP_SCORES_MAP,
}
def explain(id: str) -> None:
    """Given an identifier, prints a short explanation of what it represents in the Inseq library. Identifiers are
    used for attribution methods, aggregators, aggregation functions, and step functions.
    Example: `explain("attention")` prints the documentation for the attention attribution method.
    """
    if id not in INSEQ_ID_MAP:
        raise ValueError(f"Unknown identifier: {id}")
    doc = INSEQ_ID_MAP[id].__doc__
    if doc is None:
        print(
            "No documentation is available for this identifier. Please refer to the Inseq documentation for more "
            "information, or raise an issue on https://github.com/inseq-team/inseq/issues to request documentation "
            "for this identifier."
        )
    print(doc)

================
File: inseq/utils/import_utils.py
================
from importlib.util import find_spec
_ipywidgets_available = find_spec("ipywidgets") is not None
_scikitlearn_available = find_spec("sklearn") is not None
_transformers_available = find_spec("transformers") is not None
_sentencepiece_available = find_spec("sentencepiece") is not None and find_spec("protobuf") is not None
_datasets_available = find_spec("datasets") is not None
_captum_available = find_spec("captum") is not None
_joblib_available = find_spec("joblib") is not None
_nltk_available = find_spec("nltk") is not None
_accelerate_available = find_spec("accelerate") is not None
def is_ipywidgets_available():
    return _ipywidgets_available
def is_scikitlearn_available():
    return _scikitlearn_available
def is_transformers_available():
    return _transformers_available
def is_sentencepiece_available():
    return _sentencepiece_available
def is_datasets_available():
    return _datasets_available
def is_captum_available():
    return _captum_available
def is_joblib_available():
    return _joblib_available
def is_nltk_available():
    return _nltk_available
def is_accelerate_available():
    return _accelerate_available

================
File: inseq/utils/misc.py
================
import functools
import gzip
import io
import logging
import math
from base64 import standard_b64decode, standard_b64encode
from collections import OrderedDict
from collections.abc import Callable, Sequence
from contextlib import contextmanager
from functools import wraps
from importlib import import_module
from inspect import signature
from numbers import Number
from os import PathLike, fsync
from typing import Any
from numpy import asarray, frombuffer
from torch import Tensor
from .errors import LengthMismatchError
from .typing import TextInput, TokenWithId
logger = logging.getLogger(__name__)
@contextmanager
def optional(condition, context_manager, alternative_fn=None, **alternative_fn_kwargs):
    if condition:
        with context_manager:
            yield
    else:
        if alternative_fn is not None:
            alternative_fn(**alternative_fn_kwargs)
        yield
def _pretty_list_contents(l: Sequence[Any]) -> str:
    quote = "'" if l and type(l[0]) in [str, TokenWithId] else ""
    get_space = lambda s: "" if not isinstance(s, Number) or s < 0 else "  " if math.isnan(s) else " "
    return (
        quote
        + f"{quote}, {quote}".join([get_space(v) + (f"{v:.2f}" if isinstance(v, float) else f"{v}") for v in l])
        + quote
    )
def _pretty_list(l: Sequence[Any] | None, lpad: int = 8) -> str:
    if all(isinstance(x, list) for x in l):
        line_sep = f" ],\n{' ' * lpad}[ "
        contents = " " * lpad + "[ " + line_sep.join([_pretty_list_contents(subl) for subl in l]) + " ]"
    elif all(hasattr(x, "to_dict") for x in l):
        contents = ",\n".join(
            [f"{' ' * lpad + x.__class__.__name__}({pretty_dict(x.to_dict(), lpad + 4)})" for x in l]
        )
    else:
        contents = " " * lpad + _pretty_list_contents(l)
    return "[\n" + contents + f"\n{' ' * (lpad - 4)}]"
def pretty_list(l: Sequence[Any] | None, lpad: int = 8) -> str:
    if l is None:
        return "None"
    if len(l) == 0:
        return "list with 0 elements"
    out_txt = f"list with {len(l)} elements of type {l[0].__class__.__name__}"
    if all(isinstance(x, list) for x in l):
        out_txt = f"list with {len(l)} sub-lists"
        if any(len(sl) > 20 for sl in l) or len(l) > 15:
            return out_txt
        if all(isinstance(ssl, list) for sl in l for ssl in sl):
            return out_txt
    if len(l) > 20:
        return out_txt
    return f"{out_txt}: {_pretty_list(l, lpad)}"
def pretty_tensor(t: Tensor | None = None, lpad: int = 8) -> str:
    if t is None:
        return "None"
    if t.ndim > 2 or any(x > 20 for x in t.shape):
        return f"{t.dtype} tensor of shape {list(t.shape)} on {t.device}"
    else:
        out_list = t.tolist()
        out_list = _pretty_list(out_list, lpad) if isinstance(out_list, list) else out_list
        return f"{t.dtype} tensor of shape {list(t.shape)} on {t.device}: {out_list}"
def pretty_dict(d: dict[str, Any], lpad: int = 4) -> str:
    if not d:
        return "{}"
    out_txt = "{\n"
    for k, v in d.items():
        out_txt += f"{' ' * lpad}{k}: "
        if isinstance(v, list | tuple):
            out_txt += pretty_list(v, lpad + 4)
        elif isinstance(v, Tensor):
            out_txt += pretty_tensor(v, lpad + 4)
        elif isinstance(v, dict):
            out_txt += pretty_dict(v, lpad + 4)
        elif hasattr(v, "to_dict") and not isinstance(v, type):
            out_txt += f"{v.__class__.__name__}({pretty_dict(v.to_dict(), lpad + 4)})"
        elif v is None:
            out_txt += "None"
        elif isinstance(v, str):
            out_txt += f'"{v}"'
        else:
            out_txt += str(v)
        out_txt += ",\n"
    return out_txt + f"{' ' * (lpad - 4)}}}"
def extract_signature_args(
    full_args: dict[str, Any],
    func: Callable[[Any], Any],
    exclude_args: Sequence[str] | None = None,
    return_remaining: bool = False,
) -> dict[str, Any] | tuple[dict[str, Any], dict[str, Any]]:
    extracted_args = {
        k: v
        for k, v in full_args.items()
        if k in signature(func).parameters and (exclude_args is None or k not in exclude_args)
    }
    if return_remaining:
        return extracted_args, {k: v for k, v in full_args.items() if k not in extracted_args}
    return extracted_args
def ordinal_str(n: int):
    """Converts a number to an ordinal string."""
    return str(n) + {1: "st", 2: "nd", 3: "rd"}.get(4 if 10 <= n % 100 < 20 else n % 10, "th")
def rgetattr(obj, attr, *args):
    """Recursively access attributes from nested classes.
    E.g. rgetattr(attr_model, 'model.model.decoder.layers[4].self_attn')
    >> MarianAttention(
        (k_proj): Linear(in_features=512, out_features=512, bias=True)
        (v_proj): Linear(in_features=512, out_features=512, bias=True)
        (q_proj): Linear(in_features=512, out_features=512, bias=True)
        (out_proj): Linear(in_features=512, out_features=512, bias=True)
    )
    """
    def _getattr(obj, attr):
        return getattr(obj, attr, *args)
    attr = attr.replace("[", ".").replace("]", "")
    return functools.reduce(_getattr, [obj] + attr.split("."))
def find_char_indexes(strings: Sequence[str], char: str = " "):
    """Finds the indexes of a character in a list of strings."""
    whitespace_indexes = []
    for sent in strings:
        idx = 0
        curr_idxs = []
        for token in sent.split(char):
            idx += len(token)
            curr_idxs.append(idx)
            idx += 1
        whitespace_indexes.append(curr_idxs)
    return whitespace_indexes
def pad(seq: Sequence[Sequence[Any]], pad_id: Any):
    """Pads a list of sequences to the same length."""
    max_len = max(len(x) for x in seq)
    seq = [x + [pad_id] * (max_len - len(x)) for x in seq]
    return seq
def drop_padding(seq: Sequence[TokenWithId], pad_id: str):
    if pad_id is None:
        return seq
    return [x for x in seq if x.token != pad_id]
def isnotebook():
    """Returns true if code is being executed in a notebook, false otherwise.
    Currently supported: Jupyter Notebooks, Google Colab
    To validate: Kaggle Notebooks, JupyterLab
    """
    try:
        from IPython import get_ipython
        shell = get_ipython().__class__.__name__
        module = get_ipython().__class__.__module__
        if shell == "ZMQInteractiveShell" or module == "google.colab._shell":
            return True  # Jupyter notebook, Google Colab or qtconsole
        elif shell == "TerminalInteractiveShell":
            return False  # Terminal running IPython
        else:
            return False  # Other type (?)
    except NameError:
        return False  # Probably standard Python interpreter
    except ModuleNotFoundError:
        return False  # IPython not installed
def format_input_texts(
    texts: TextInput,
    ref_texts: TextInput | None = None,
    skip_special_tokens: bool = False,
    special_tokens: list[str] = [],
) -> tuple[list[str], list[str]]:
    texts = [texts] if isinstance(texts, str) else texts
    reference_texts = [ref_texts] if isinstance(ref_texts, str) else ref_texts
    if reference_texts and texts and len(texts) != len(reference_texts):
        raise LengthMismatchError(
            "Length mismatch for texts and reference_texts.Input length: {}, reference length: {} ".format(
                len(texts), len(reference_texts)
            )
        )
    if skip_special_tokens:
        for special_token in special_tokens:
            texts = [text.replace(special_token, "") for text in texts]
            if reference_texts is not None:
                reference_texts = [text.replace(special_token, "") for text in reference_texts]
    return texts, reference_texts
def aggregate_token_sequence(token_sequence, spans):
    if not spans:
        return token_sequence
    out_sequence = []
    span_start_idxs = [span[0] for span in spans]
    curr_idx = 0
    for tok_idx, token in enumerate(token_sequence):
        if tok_idx < curr_idx:
            continue
        if curr_idx in span_start_idxs:
            end_idx = spans[span_start_idxs.index(curr_idx)][1]
            # We use -1 as token index to indicate the token is product of an aggregation
            # (i.e. not contained in the original vocabulary)
            out_sequence.append(TokenWithId("".join([t.token for t in token_sequence[curr_idx:end_idx]]), -1))
            curr_idx = end_idx
        else:
            out_sequence.append(token)
            curr_idx += 1
    return out_sequence
def aggregate_token_pair(tokens: list[TokenWithId], other_tokens: list[TokenWithId]):
    if not other_tokens:
        return tokens
    out_tokens = []
    for tok, other in zip(tokens, other_tokens, strict=False):
        if tok.token == other.token:
            out_tokens.append(TokenWithId(tok.token, tok.id))
        else:
            out_tokens.append(TokenWithId(tok.token + " â†’ " + other.token, -1))
    return out_tokens
def gzip_compress(data, compresslevel):
    """Do gzip compression, without the timestamp."""
    buf = io.BytesIO()
    with gzip.GzipFile(fileobj=buf, mode="wb", compresslevel=compresslevel, mtime=0) as fh:
        fh.write(data)
    return buf.getvalue()
def gzip_decompress(data):
    """Do gzip decompression, without the timestamp."""
    with gzip.GzipFile(fileobj=io.BytesIO(data)) as f:
        return f.read()
def ndarray_to_bin_str(array, do_compress):
    """From ndarray to base64 encoded, gzipped binary data."""
    assert array.flags["C_CONTIGUOUS"], "only C memory order is (currently) supported for compact ndarray format"
    original_size = array.size * array.itemsize
    header = "b64:"
    data = array.data
    if do_compress:
        small = gzip_compress(data, compresslevel=9)
        if len(small) < 0.9 * original_size and len(small) < original_size - 8:
            header = "b64.gz:"
            data = small
    data = standard_b64encode(data)
    return header + data.decode("ascii")
class hashodict(OrderedDict):
    """This dictionary is hashable. It should NOT be mutated, or all kinds of weird
    bugs may appear. This is not enforced though, it's only used for encoding.
    """
    def __hash__(self):
        return hash(frozenset(self.items()))
def get_module_name_from_object(obj):
    mod = obj.__class__.__module__
    if mod == "__main__":
        mod = None
        logger.warning(
            f"class {obj.__class__} seems to have been defined in the main file; unfortunately this means"
            " that it's module/import path is unknown, so you might have to provide cls_lookup_map when decoding"
        )
    return mod
def save_to_file(f: Callable[[Any], Any]) -> Callable[[Any], Any]:
    """Serializes the function output to a file, performing the required checks."""
    @wraps(f)
    def save_to_file_wrapper(
        obj: Any,
        fp: str | bytes | PathLike = None,
        *args,
        compression: int | bool = None,
        force_flush: bool = False,
        return_output: bool = True,
        **kwargs,
    ) -> Any | None:
        if "compression" in signature(f).parameters:
            kwargs["compression"] = compression
        txt = f(obj, *args, **kwargs)
        if compression:
            if compression is True:
                compression = 5
            txt = txt.encode("UTF-8")
            txt = gzip_compress(txt, compresslevel=compression)
        if isinstance(fp, str):
            if compression:
                fh = open(fp, "wb+")
            else:
                fh = open(fp, "w+")
        else:
            fh = fp
        try:
            if compression and "b" not in getattr(fh, "mode", "b?") and not isinstance(txt, str):
                raise OSError("If compression is enabled, the file must be opened in binary mode.")
            try:
                fh.write(txt)
            except TypeError as err:
                err.args = (
                    err.args[0] + ". A possible reason is that the file is not opened in binary mode; "
                    "be sure to set file mode to something like 'wb'.",
                )
                raise
        finally:
            if force_flush:
                fh.flush()
                try:
                    if fh.fileno() is not None:
                        fsync(fh.fileno())
                except ValueError:
                    pass
            if isinstance(fp, str):
                fh.close()
        if return_output:
            return txt
    return save_to_file_wrapper
def bin_str_to_ndarray(data, order, shape, dtype):
    """From base64 encoded, gzipped binary data to ndarray."""
    assert order in [
        None,
        "C",
    ], f"specifying different memory order is not (yet) supported for binary numpy format (got order = {order})"
    if data.startswith("b64.gz:"):
        data = standard_b64decode(data[7:])
        data = gzip_decompress(data)
    elif data.startswith("b64:"):
        data = standard_b64decode(data[4:])
    else:
        raise ValueError("found numpy array buffer, but did not understand header; supported: b64 or b64.gz")
    data = frombuffer(data, dtype=dtype)
    return data.reshape(shape)
def lists_of_numbers_to_ndarray(data, order, shape, dtype):
    """From nested list of numbers to ndarray."""
    arr = asarray(data, dtype=dtype, order=order)
    if 0 in shape:
        return arr.reshape(shape)
    if shape != arr.shape:
        logger.warning(f"size mismatch decoding numpy array: expected {shape}, got {arr.shape}")
    return arr
def scalar_to_numpy(data, dtype):
    """From scalar value to numpy type."""
    import numpy as nptypes
    dtype = getattr(nptypes, dtype)
    return dtype(data)
def get_cls_from_instance_type(mod, name, cls_lookup_map):
    curr_class = ValueError()
    if mod is None:
        try:
            curr_class = getattr((__import__("__main__")), name)
        except (ImportError, AttributeError) as err:
            if name not in cls_lookup_map:
                raise ImportError(
                    f"class {name} seems to have been exported from the main file, which means "
                    "it has no module/import path set; you need to provide loads argument"
                    f"`cls_lookup_map={{'{name}': Class}}` to locate the class"
                ) from err
            curr_class = cls_lookup_map[name]
    else:
        imp_err = None
        try:
            module = import_module(f"{mod}")
        except ImportError as err:
            imp_err = (
                f"encountered import error '{err}' while importing '{mod}' to decode a json file; perhaps "
                f"it was encoded in a different environment where {mod}.{name} was available"
            )
        else:
            if hasattr(module, name):
                curr_class = getattr(module, name)
            else:
                imp_err = (
                    f"imported '{module}' but could find '{name}' inside while decoding a json file "
                    f"(found {', '.join(attr for attr in dir(module) if not attr.startswith('_'))})"
                )
        if imp_err:
            curr_class = cls_lookup_map.get(name, None)
            if curr_class is None:
                raise ImportError(f"{imp_err}; add the class to `cls_lookup_map={{'{name}': Class}}` argument")
    return curr_class
def clean_tokens(
    tokens: list[str],
    remove_tokens: list[str] = [],
    return_removed_idxs: bool = False,
    replace_chars: dict[str, str] | None = None,
) -> list[str] | tuple[list[str], list[int]]:
    """Removes tokens from a list of tokens and returns the cleaned list and the removed token indexes."""
    clean_tokens = []
    removed_token_idxs = []
    for idx, tok in enumerate(tokens):
        new_tok = tok
        if new_tok in remove_tokens:
            removed_token_idxs += [idx]
        else:
            if replace_chars is not None:
                for k, v in replace_chars.items():
                    new_tok = new_tok.replace(k, v)
            clean_tokens += [new_tok.strip()]
    if return_removed_idxs:
        return clean_tokens, removed_token_idxs
    return clean_tokens
def get_left_padding(text: str):
    """Returns the number of spaces at the beginning of a string."""
    return len(text) - len(text.lstrip())

================
File: inseq/utils/registry.py
================
from __future__ import annotations
from abc import ABC
from typing import TypeVar
R = TypeVar("R", bound="Registry")
class Registry(ABC):
    registry_attr = "None"  # Override in child classes
    def __init__(self) -> None:
        if self.__class__ is Registry or self.__class__ in Registry.__subclasses__():
            raise OSError(
                f"{self.__class__.__name__} is designed to be instantiated "
                f"using the `{self.__class__.__name__}.load(name, **kwargs)` method."
            )
    @classmethod
    def subclasses(cls: type[R]) -> set[type[R]]:
        registry = set()
        for subclass in cls.__subclasses__():
            if subclass not in registry and subclass not in Registry.__subclasses__():
                registry.add(subclass)
            registry |= subclass.subclasses()
        return registry
    @classmethod
    def available_classes(cls: type[R]) -> dict[str, type[R]]:
        methods = {getattr(c, cls.registry_attr): c for c in cls.subclasses()}
        if cls is not Registry and cls not in Registry.__subclasses__():
            methods[getattr(cls, cls.registry_attr)] = cls
        return methods
def available_classes(cls: type[Registry]) -> list[str]:
    return list(cls.available_classes().keys())

================
File: inseq/utils/serialization.py
================
# Code adapted from json-tricks codebase (https://github.com/mverleg/pyjson_tricks)
#
# LICENSE: BSD-3-Clause
#
# Copyright (c) 2022 Mark V. All rights reserved.
#
# Redistribution and use in source and binary forms, with or without modification,
# are permitted provided that the following conditions are met:
#
# 1. Redistributions of source code must retain the above copyright notice,
# this list of conditions and the following disclaimer.
#
# 2. Redistributions in binary form must reproduce the above copyright notice,
# this list of conditions and the following disclaimer in the documentation
# and/or other materials provided with the distribution.
#
# 3. Neither the name of the copyright holder nor the names of its contributors
# may be used to endorse or promote products derived from this software without
# specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE
# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE
# USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
import base64
import json
from collections import OrderedDict
from collections.abc import Callable
from json import JSONEncoder
from os import PathLike
from typing import Any, TypeVar
from numpy import generic, ndarray
from ..utils import (
    bin_str_to_ndarray,
    get_cls_from_instance_type,
    get_module_name_from_object,
    gzip_decompress,
    hashodict,
    lists_of_numbers_to_ndarray,
    ndarray_to_bin_str,
    save_to_file,
    scalar_to_numpy,
)
EncodableObject = TypeVar("EncodableObject")
DecodableObject = TypeVar("DecodableObject")
def class_instance_encode(obj: EncodableObject, use_primitives: bool = True, **kwargs):
    """Encodes a class instance to json. Note that it can only be recovered if the environment allows the class to be
    imported in the same way.
    """
    if isinstance(obj, list | dict):
        return obj
    if isinstance(obj, bytes):
        return base64.b64encode(obj).decode("UTF8")
    if hasattr(obj, "__class__") and hasattr(obj, "__dict__"):
        if not hasattr(obj, "__new__"):
            raise TypeError(f"class '{obj.__class__}' does not have a __new__ method; ")
        if isinstance(obj, type(lambda: 0)):
            raise TypeError(f"instance '{obj}' of class '{obj.__class__}' cannot be encoded, it is a function.")
        try:
            obj.__new__(obj.__class__)
        except TypeError as err:
            raise TypeError(
                f"instance '{obj}' of class '{obj.__class__}' cannot be encoded, perhaps because its"
                " __new__ method cannot be called because it requires extra parameters"
            ) from err
        mod = get_module_name_from_object(obj)
        name = obj.__class__.__name__
        if hasattr(obj, "__json_encode__"):
            attrs = obj.__json_encode__()
            if use_primitives or not isinstance(attrs, dict):
                return attrs
            else:
                return hashodict((("__instance_type__", (mod, name)), ("attributes", attrs)))
        dct = hashodict([("__instance_type__", (mod, name))])
        if hasattr(obj, "__dict__"):
            dct["attributes"] = hashodict(obj.__dict__)
        if use_primitives:
            attrs = dct.get("attributes", {})
        return attrs if use_primitives else dct
    return obj
def ndarray_encode(
    obj: EncodableObject,
    use_primitives: bool = True,
    ndarray_compact: bool | None = None,
    compression: bool = False,
    **kwargs,
) -> dict[str, Any]:
    """Encodes numpy ``ndarray`` as lists with meta data.
    Encodes numpy scalar types as Python equivalents. Special encoding is not possible,
    because float64 is a subclass of primitives, which never reach the encoder.
    """
    if isinstance(obj, ndarray):
        if use_primitives:
            return obj.tolist()
        else:
            # Property 'ndarray_compact' may also be an integer, in which case it's the number of
            # elements from which compact storage is used.
            if isinstance(ndarray_compact, int) and not isinstance(ndarray_compact, bool):
                ndarray_compact = obj.size >= ndarray_compact
            if ndarray_compact:
                # If the overall json file is compressed, then don't compress the array.
                data_json = ndarray_to_bin_str(obj, do_compress=not compression)
            else:
                data_json = obj.tolist()
            dct = hashodict(
                (
                    ("__ndarray__", data_json),
                    ("dtype", str(obj.dtype)),
                    ("shape", obj.shape),
                )
            )
            if obj.ndim > 1:
                dct["Corder"] = obj.flags["C_CONTIGUOUS"]
            return dct
    elif isinstance(obj, generic):
        return obj.item()
    return obj
ENCODE_HOOKS = [class_instance_encode, ndarray_encode]
class AttributionSerializer(JSONEncoder):
    def __init__(
        self,
        encoders: list[Callable] | None = None,
        use_primitives: bool = True,
        ndarray_compact: bool | None = None,
        compression: bool = False,
        **json_kwargs,
    ):
        self.encoders = []
        if encoders:
            self.encoders = list(encoders)
        self.use_primitives = use_primitives
        self.ndarray_compact = ndarray_compact
        self.compression = compression
        super().__init__(**json_kwargs)
    def default(self, obj: EncodableObject, *args, **kwargs):
        """This is the method of JSONEncoders that is called for each object; it calls all the encoders with previous
        one's output used as input. Works for Encoder instances, but they are expected not to throw ``TypeError`` for
        unrecognized types (the super method does that by default). It never calls the ``super`` method so if there are
        non-primitive types left at the end, you'll get an encoding error.
        """
        prev_id = id(obj)
        for encoder in self.encoders:
            obj = encoder(
                obj,
                use_primitives=self.use_primitives,
                ndarray_compact=self.ndarray_compact,
                compression=self.compression,
            )
        if id(obj) == prev_id:
            raise TypeError(
                f"Object of type {type(obj)} could not be encoded by {self.__class__.__name__} using encoders"
                f" [{', '.join(str(encoder) for encoder in self.encoders)}]."
            )
        return obj
def json_advanced_dumps(
    obj: EncodableObject,
    sort_keys: bool = True,
    encoders: list[Callable] = ENCODE_HOOKS,
    use_primitives: bool = True,
    allow_nan: bool = True,
    ndarray_compact: bool | None = None,
    compression: bool = False,
    **jsonkwargs,
) -> str:
    """Dumps a complex object containing classes and arrays object to a string.
    Args:
        obj (:obj:`Any`):
            Object to be dumped to file.
        sort_keys (:obj:`bool`, *optional*, defaults to ``True``):
            Whether to object fields should be sorted in the serialized output.
        encoders (:obj:`list` of callables, *optional*):
            A list of encoders to run on the object fields for encoding it to JSON-compatible format. By default,
            encoders that serialize classes and numpy arrays are used.
        use_primitives (:obj:`bool`, *optional*, defaults to ``False``):
            If specified, encoders will use primitive types instead of special formats for classes and numpy arrays.
            Note that this will not allow for decoding the object back to its original form.
        allow_nan (:obj:`bool`, *optional*, defaults to ``True``):
            Whether to allow NaN values in the serialized output.
        ndarray_compact (:obj:`bool`, *optional*, defaults to ``None``):
            Whether to use compact storage for numpy arrays. If ``None``, arrays are serialized as lists.
        compression (:obj:`bool`, *optional*, defaults to ``False``):
            Whether to compress the serialized output using GZip.
        **jsonkwargs: Additional keyword arguments passed to :func:`json.dumps`.
    Returns:
        The dumped object in string format.
    """
    combined_encoder = AttributionSerializer(
        encoders=encoders,
        use_primitives=use_primitives,
        sort_keys=sort_keys,
        allow_nan=allow_nan,
        ndarray_compact=ndarray_compact,
        compression=compression,
        **jsonkwargs,
    )
    return combined_encoder.encode(obj)
@save_to_file
def json_advanced_dump(
    obj: EncodableObject,
    sort_keys: bool = True,
    encoders: list[Callable] = ENCODE_HOOKS,
    use_primitives: bool = False,
    allow_nan: bool = True,
    ndarray_compact: bool | None = None,
    compression: bool = False,
    **jsonkwargs,
) -> str:
    """Dumps a complex object containing classes and arrays object to a file.
    Args:
        obj (:obj:`Any`):
            Object to be dumped to file.
        fp (:obj:`str` or :obj:`os.PathLike`):
            File path to which the object should be dumped.
        sort_keys (:obj:`bool`, *optional*, defaults to ``True``):
            Whether to object fields should be sorted in the serialized output.
        encoders (:obj:`list` of callables, *optional*):
            A list of encoders to run on the object fields for encoding it to JSON format. By default, encoders that
            serialize classes and numpy arrays are used.
        use_primitives (:obj:`bool`, *optional*, defaults to ``False``):
            If specified, encoders will use primitive types instead of special formats for classes and numpy arrays.
            Note that this will not allow for decoding the object back to its original form.
        allow_nan (:obj:`bool`, *optional*, defaults to ``True``):
            Whether to allow NaN values in the serialized output.
        ndarray_compact (:obj:`bool`, *optional*, defaults to ``None``):
            Whether to use compact storage for numpy arrays. If ``None``, arrays are serialized as lists.
        compression (:obj:`bool`, *optional*, defaults to ``False``):
            Whether to compress the serialized output using GZip.
        force_flush (:obj:`bool`, *optional*, defaults to ``False``):
            Whether to force flushing the file after writing.
        return_output (:obj:`bool`, *optional*, defaults to ``True``):
            Whether to return the serialized output as a string.
        **jsonkwargs: Additional keyword arguments passed to :func:`json.dumps`.
    Returns:
        The dumped object in string format.
    """
    if isinstance(obj, str) or hasattr(obj, "write"):
        raise ValueError("dump arguments are in the wrong order: provide the data to be serialized before file handle")
    txt = json_advanced_dumps(
        obj,
        sort_keys=sort_keys,
        encoders=encoders,
        use_primitives=use_primitives,
        allow_nan=allow_nan,
        ndarray_compact=ndarray_compact,
        compression=compression,
        **jsonkwargs,
    )
    return txt
def ndarray_hook(dct: Any, **kwargs) -> DecodableObject:
    """Replace any numpy arrays previously encoded by NumpyEncoder to their proper shape, data type and data.
    Args:
        dct: The object to be decoded. Will be processed by the hook if it is a dictionary containing the attribute
            ``__ndarray__`` provided by ``ndarray_encode``.
    """
    if not isinstance(dct, dict):
        return dct
    if "__ndarray__" not in dct:
        return dct
    order = None
    if "Corder" in dct:
        order = "C" if dct["Corder"] else "F"
    data_json = dct["__ndarray__"]
    shape = tuple(dct["shape"])
    nptype = dct["dtype"]
    if shape:
        if nptype == "object":
            raise ValueError("Cannot decode object arrays. Object arrays not supported.")
        if isinstance(data_json, str):
            return bin_str_to_ndarray(data_json, order, shape, nptype)
        else:
            return lists_of_numbers_to_ndarray(data_json, order, shape, nptype)
    else:
        return scalar_to_numpy(data_json, nptype)
def class_instance_hook(dct: Any, cls_lookup_map: dict[str, type] | None = None, **kwargs) -> DecodableObject:
    """This hook tries to convert json encoded by class_instance_encoder back to it's original instance.
    It only works if the environment is the same, e.g. the class is similarly importable and hasn't changed.
    Args:
        dct:
            The object to be decoded. Will be processed by the hook if it is a dictionary containing the attribute
            ``__instance_type__`` provided by ``class_instance_encode``.
        cls_lookup_map (:obj:`dict`, *optional*):
            A dictionary mapping class names to classes. This is used to look up classes when decoding class instances.
            This is useful when the class is not directly importable in the current environment.
    """
    if not isinstance(dct, dict):
        return dct
    if "__instance_type__" not in dct:
        return dct
    mod, name = dct["__instance_type__"]
    curr_class = get_cls_from_instance_type(mod, name, cls_lookup_map=cls_lookup_map)
    try:
        obj = curr_class.__new__(curr_class)
    except TypeError as err:
        raise TypeError(
            f"problem while decoding instance of '{name}'; this instance has a special __new__ method"
        ) from err
    if hasattr(obj, "__json_decode__"):
        properties = {}
        if "attributes" in dct:
            properties.update(dct["attributes"])
        obj.__json_decode__(**properties)
    elif "attributes" in dct:
        obj.__dict__ = dict(dct["attributes"])
    return obj
class AttributionDeserializer:
    """Hook that converts json maps to the appropriate python type (dict or OrderedDict)
    and then runs any number of hooks on the individual maps.
    """
    def __init__(
        self,
        ordered: bool = False,
        hooks: list[Callable] | None = None,
        cls_lookup_map: dict[str, type] | None = None,
    ):
        self.map_type = OrderedDict if ordered else dict
        self.hooks = hooks if hooks else []
        self.cls_lookup_map = cls_lookup_map
    def __call__(self, pairs):
        """Applies all hooks to the map."""
        map = self.map_type(pairs)
        for hook in self.hooks:
            map = hook(map, cls_lookup_map=self.cls_lookup_map)
        return map
DECODE_HOOKS = [class_instance_hook, ndarray_hook]
def json_advanced_loads(
    string: str,
    ordered: bool = False,
    decompression: bool = False,
    hooks: list[Callable] = DECODE_HOOKS,
    cls_lookup_map: dict[str, type] | None = None,
    **jsonkwargs,
) -> Any:
    """Load a complex object containing classes and arrays object from a string.
    Args:
        string (:obj:`str`):
            String to be loaded into an object.
        ordered (:obj:`bool`, *optional*, defaults to ``True``):
            Whether to use an :obj:`OrderedDict` to store the loaded data in the original order.
        decompression (:obj:`bool`, *optional*, defaults to ``False``):
            Whether to decompress the file with GZip before loading it.
        hooks (:obj:`list` of callables, *optional*):
            A list of hooks to run on the loaded data for decoding. By default hooks to deserialize classes and numpy
            arrays are used.
        cls_lookup_map (:obj:`dict`, *optional*):
            A dictionary mapping class names to classes. This is used to look up classes when decoding class instances.
            This is useful when the class is not directly importable in the current environment.
        **jsonkwargs: Additional keyword arguments passed to :func:`json.loads`.
    Returns:
        The loaded object.
    """
    if decompression:
        string = gzip_decompress(string).decode("UTF-8")
    if not isinstance(string, str):
        raise TypeError(f"The input was of non-string type '{type(string)}' in `load(s)`. ")
    hook = AttributionDeserializer(
        ordered=ordered,
        cls_lookup_map=cls_lookup_map,
        hooks=hooks,
    )
    return json.loads(string, object_pairs_hook=hook, **jsonkwargs)
def json_advanced_load(
    fp: str | bytes | PathLike,
    ordered: bool = True,
    decompression: bool = False,
    hooks: list[Callable] = DECODE_HOOKS,
    cls_lookup_map: dict[str, type] | None = None,
    **jsonkwargs,
) -> Any:
    """Load a complex object containing classes and arrays from a JSON file.
    Args:
        fp (:obj:`str`, :obj:`bytes`, or :obj:`os.PathLike`):
            Path to the file to load.
        ordered (:obj:`bool`, *optional*, defaults to ``True``):
            Whether to use an :obj:`OrderedDict` to store the loaded data in the original order.
        decompression (:obj:`bool`, *optional*, defaults to ``False``):
            Whether to decompress the file with GZip before loading it.
        hooks (:obj:`list` of callables, *optional*):
            A list of hooks to run on the loaded data for decoding. By default hooks to deserialize classes and numpy
            arrays are used.
        cls_lookup_map (:obj:`dict`, *optional*):
            A dictionary mapping class names to classes. This is used to look up classes when decoding class instances.
            This is useful when the class is not directly importable in the current environment.
        **jsonkwargs: Additional keyword arguments passed to :func:`json.loads`.
    Returns:
        The loaded object.
    """
    try:
        if isinstance(fp, PathLike | bytes | str):
            with open(fp, "rb" if decompression else "r") as fh:
                string = fh.read()
        else:
            string = fp.read()
    except UnicodeDecodeError as err:
        raise Exception(
            "There was a problem decoding the file content. A possible reason is that the file is not "
            "opened  in binary mode; be sure to set file mode to something like 'rb'."
        ) from err
    return json_advanced_loads(
        string=string,
        ordered=ordered,
        decompression=decompression,
        hooks=hooks,
        cls_lookup_map=cls_lookup_map,
        **jsonkwargs,
    )

================
File: inseq/utils/torch_utils.py
================
import logging
from collections.abc import Callable, Sequence
from functools import wraps
from inspect import signature
from typing import TYPE_CHECKING, Literal
import safetensors
import torch
import torch.nn.functional as F
from jaxtyping import Int, Num
from torch import nn
from torch.backends.cuda import is_built as is_cuda_built
from torch.backends.mps import is_available as is_mps_available
from torch.backends.mps import is_built as is_mps_built
from torch.cuda import is_available as is_cuda_available
from .typing import OneOrMoreIndices
if TYPE_CHECKING:
    pass
logger = logging.getLogger(__name__)
TORCH_BACKEND_DEVICE_MAP = {
    "cuda": (is_cuda_built, is_cuda_available),
    "mps": (is_mps_built, is_mps_available),
}
@torch.no_grad()
def remap_from_filtered(
    original_shape: tuple[int, ...],
    mask: Int[torch.Tensor, "batch_size 1"],
    filtered: Num[torch.Tensor, "filtered_batch_size"],
) -> Num[torch.Tensor, "batch_size"]:
    index = mask.squeeze(-1).nonzero().reshape(-1, 1)
    while index.ndim < filtered.ndim:
        index = index.unsqueeze(-1)
    index = index.expand_as(filtered)
    new_source = torch.ones(original_shape, dtype=filtered.dtype, device=filtered.device) * float("nan")
    return new_source.scatter(0, index, filtered)
def convert_to_safetensor(tensor: torch.Tensor, scores_precision="float32") -> bytes:
    """
    Converts a torch tensor to a safetensor.
    Args:
        tensor (torch.Tensor): some torch tensor
        scores_precision (str): format to convert weights to: [float32, float16, float8]
    Returns:
        bytes: A safetensor in bytes format
    Raises:
        ValueError if `scores_precision` doesn't match the possible options
    """
    if scores_precision == "float32":
        return safetensors.torch.save({"attribution": tensor})
    elif scores_precision == "float16":
        return safetensors.torch.save({"attribution": tensor.to(torch.float16)})
    elif scores_precision == "float8":
        logger.warning("Float8 precision is experimental and may result in loss of precision.")
        return safetensors.torch.save({"attribution": tensor.to(torch.float8_e4m3fn)})
    else:
        raise ValueError("`scores_precision` has to be one of [float32, float16, float8]")
def convert_from_safetensor(safetensor: bytes) -> torch.Tensor:
    """
    Convert a safetensor to a torch tensor and convert weights to float32.
    Adapted from https://huggingface.co/docs/safetensors/metadata_parsing
    """
    return safetensors.torch.load(safetensor)["attribution"].to(torch.float32)
def postprocess_attribution_scores(func: Callable) -> Callable:
    @wraps(func)
    def postprocess_scores_wrapper(
        attributions: torch.Tensor | tuple[torch.Tensor, ...], dim: int = 0, *args, **kwargs
    ) -> torch.Tensor | tuple[torch.Tensor, ...]:
        multi_input = False
        if isinstance(attributions, tuple):
            orig_sizes = [a.shape[dim] for a in attributions]
            attributions = torch.cat(attributions, dim=dim)
            multi_input = True
        nan_mask = attributions.isnan()
        attributions[nan_mask] = 0.0
        if "dim" in signature(func).parameters:
            kwargs["dim"] = dim
        attributions = func(attributions, *args, **kwargs)
        attributions[nan_mask] = float("nan")
        if multi_input:
            return tuple(attributions.split(orig_sizes, dim=dim))
        return attributions
    return postprocess_scores_wrapper
@postprocess_attribution_scores
def normalize(
    attributions: torch.Tensor | tuple[torch.Tensor, ...],
    dim: int = 0,
    norm_ord: int = 1,
) -> torch.Tensor | tuple[torch.Tensor, ...]:
    return F.normalize(attributions, p=norm_ord, dim=dim)
@postprocess_attribution_scores
def rescale(
    attributions: torch.Tensor | tuple[torch.Tensor, ...],
) -> torch.Tensor | tuple[torch.Tensor, ...]:
    return attributions / attributions.abs().max()
def top_p_logits_mask(logits: torch.Tensor, top_p: float, min_tokens_to_keep: int) -> torch.Tensor:
    """Adapted from https://github.com/huggingface/transformers/blob/main/src/transformers/generation/logits_process.py"""
    # Compute cumulative probabilities of sorted tokens
    if top_p < 0 or top_p > 1.0:
        raise ValueError(f"`top_p` has to be a float > 0 and < 1, but is {top_p}")
    if not isinstance(min_tokens_to_keep, int) or (min_tokens_to_keep < 1):
        raise ValueError(f"`min_tokens_to_keep` has to be a positive integer, but is {min_tokens_to_keep}")
    sorted_logits, sorted_indices = torch.sort(logits, descending=False)
    cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)
    # Remove tokens with cumulative top_p above the threshold (token with 0 are kept)
    sorted_indices_to_remove = cumulative_probs <= (1 - top_p)
    # Keep at least min_tokens_to_keep
    sorted_indices_to_remove[..., -min_tokens_to_keep:] = 0
    # scatter sorted tensors to original indexing
    indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)
    return indices_to_remove
def top_k_logits_mask(logits: torch.Tensor, top_k: int, min_tokens_to_keep: int) -> torch.Tensor:
    """Adapted from https://github.com/huggingface/transformers/blob/main/src/transformers/generation/logits_process.py"""
    top_k = max(top_k, min_tokens_to_keep)
    return logits < logits.topk(top_k).values[..., -1, None]
def get_logits_from_filter_strategy(
    filter_strategy: Literal["original"] | Literal["contrast"] | Literal["merged"],
    original_logits: torch.Tensor,
    contrast_logits: torch.Tensor | None = None,
) -> torch.Tensor:
    if filter_strategy == "original":
        return original_logits
    elif filter_strategy == "contrast":
        return contrast_logits
    elif filter_strategy == "merged":
        return original_logits + contrast_logits
def filter_logits(
    original_logits: torch.Tensor,
    contrast_logits: torch.Tensor | None = None,
    top_p: float = 1.0,
    top_k: int = 0,
    min_tokens_to_keep: int = 1,
    filter_strategy: Literal["original"] | Literal["contrast"] | Literal["merged"] | None = None,
) -> torch.Tensor | tuple[torch.Tensor, torch.Tensor]:
    """Applies top-k and top-p filtering to logits, and optionally to an additional set of contrastive logits."""
    if top_k > original_logits.size(-1) or top_k < 0:
        raise ValueError(f"`top_k` has to be a positive integer < {original_logits.size(-1)}, but is {top_k}")
    if filter_strategy and filter_strategy != "original" and contrast_logits is None:
        raise ValueError(f"`filter_strategy` {filter_strategy} can only be used if `contrast_logits` is provided")
    if not filter_strategy:
        if contrast_logits is None:
            filter_strategy = "original"
        else:
            filter_strategy = "merged"
    if top_p < 1.0:
        indices_to_remove = top_p_logits_mask(
            get_logits_from_filter_strategy(filter_strategy, original_logits, contrast_logits),
            top_p,
            min_tokens_to_keep,
        )
        original_logits = original_logits.masked_fill(indices_to_remove, float("-inf"))
        if contrast_logits is not None:
            contrast_logits = contrast_logits.masked_fill(indices_to_remove, float("-inf"))
    if top_k > 0:
        indices_to_remove = top_k_logits_mask(
            get_logits_from_filter_strategy(filter_strategy, original_logits, contrast_logits),
            top_k,
            min_tokens_to_keep,
        )
        original_logits = original_logits.masked_fill(indices_to_remove, float("-inf"))
        if contrast_logits is not None:
            contrast_logits = contrast_logits.masked_fill(indices_to_remove, float("-inf"))
    if contrast_logits is not None:
        return original_logits, contrast_logits
    return original_logits
def euclidean_distance(vec_a: torch.Tensor, vec_b: torch.Tensor) -> torch.Tensor:
    """Compute the Euclidean distance between two points."""
    return (vec_a - vec_b).pow(2).sum(-1).sqrt()
def aggregate_contiguous(
    t: torch.Tensor,
    spans: Sequence[tuple[int, int]],
    aggregate_fn: Callable | None = None,
    aggregate_dim: int = 0,
):
    """Given a tensor, aggregate contiguous spans of the tensor along a given dimension using the provided
    aggregation function. If no aggregation function is provided, the mean is used.
    Args:
        t: Tensor to aggregate
        spans: Sequence of (start, end) tuples indicating contiguous spans to aggregate
        aggregate_fn: Aggregation function to use. If None, torch.mean is used.
        aggregate_dim: Dimension to aggregate along. Default is 0.
    """
    if not spans:
        return t
    if aggregate_fn is None:
        aggregate_fn = torch.mean
    if aggregate_dim > t.ndim:
        raise ValueError(f"aggregate_dim {aggregate_dim} is greater than tensor dimension {t.ndim}")
    if aggregate_dim != 0:
        t = t.transpose(aggregate_dim, 0)
    slices = []
    base_val = 0
    for start, end in spans:
        if start > base_val:
            slices.append(t[base_val:start, ...])
        slices.append(aggregate_fn(t[start:end, ...], dim=0).unsqueeze(0))
        base_val = end
    if base_val < t.shape[0]:
        slices.append(t[base_val:, ...])
    out_cat = torch.cat(slices, dim=0)
    if aggregate_dim != 0:
        out_cat = out_cat.transpose(aggregate_dim, 0)
    return out_cat
def get_front_padding(t: torch.Tensor, pad: int = 0, dim: int = 1) -> list[int]:
    """Given a tensor of shape (batch, seq_len) of ids, return a list of length batch containing
    the number of padding tokens at the beginning of each sequence.
    """
    return (t != pad).int().argmax(dim).tolist()
def get_sequences_from_batched_steps(
    bsteps: list[torch.Tensor], padding_dims: Sequence[int] = [], stack_dim: int = 2
) -> list[torch.Tensor]:
    """Given a sequence of batched step tensors of shape (batch_size, seq_len, ...) builds a sequence
    of tensors of shape (seq_len, ...) where each resulting tensor is the aggregation
    across batch steps for every batch element.
    Source attribution shape: (batch_size, source_seq_len, ...)
    Target attribution shape: (batch_size, target_seq_len, ...)
    Step scores shape: (batch_size)
    Sequence scores shape: (batch_size, source/target_seq_len, ...)
    Input tensors will be padded with nans up to max length in non-uniform dimensions to allow for stacking.
    """
    bsteps_num_dims = bsteps[0].ndim
    if stack_dim > bsteps_num_dims:
        raise ValueError(f"Stack dimension {stack_dim} is greater than tensor dimension {bsteps_num_dims}")
    if not padding_dims:
        sequences = torch.stack(bsteps, dim=stack_dim).split(1, dim=0)
        return [seq.squeeze(0) for seq in sequences]
    for dim in padding_dims:
        if dim >= bsteps_num_dims:
            raise ValueError(f"Padding dimension {dim} is greater than tensor dimension {bsteps_num_dims}")
    padding_dims = set(padding_dims)
    max_dims = tuple(max([bstep.shape[dim] for bstep in bsteps]) for dim in padding_dims)
    for bstep_idx, bstep in enumerate(bsteps):
        for curr_dim, max_dim in zip(padding_dims, max_dims, strict=False):
            bstep_dim = bstep.shape[curr_dim]
            if bstep_dim < max_dim:
                # Pad the end of curr_dim with nans
                pad_shape = (0,) * ((bsteps_num_dims - curr_dim) * 2 - 1) + (max_dim - bstep_dim,)
                padded_bstep = F.pad(bstep, pad=pad_shape, mode="constant", value=float("nan"))
                bsteps[bstep_idx] = padded_bstep
    sequences = torch.stack(bsteps, dim=stack_dim).split(1, dim=0)
    return [seq.squeeze(0) for seq in sequences]
def check_device(device_name: str) -> bool:
    if device_name == "cpu":
        return True
    if device_name not in TORCH_BACKEND_DEVICE_MAP:
        raise ValueError(f"Unknown device {device_name}")
    available_fn, built_fn = TORCH_BACKEND_DEVICE_MAP[device_name]
    if not available_fn():
        raise ValueError(f"Cannot use {device_name} device, {device_name} is not available.")
    if not built_fn():
        raise ValueError(f"Current Pytorch distribution does not support {device_name} execution")
    return True
def get_default_device() -> str:
    if is_cuda_available() and is_cuda_built():
        return "cuda"
    elif is_mps_available() and is_mps_built():
        # temporarily fix mps-enabled devices on cpu until mps is able to support all operations this package needs
        # change this value on your own risk as it might break things depending on the attribution functions used
        return "cpu"
    else:
        return "cpu"
def find_block_stack(module):
    """Recursively searches for the first instance of a `nn.ModuleList` submodule within a given `torch.nn.Module`.
    Args:
        module (:obj:`torch.nn.Module`): A Pytorch :obj:`nn.Module` object.
    Returns:
        :obj:`torch.nn.ModuleList`: The first instance of a :obj:`nn.Module` submodule found within the given object.
        None: If no `nn.ModuleList` submodule is found within the given `nn.Module` object.
    """
    # Check if the current module is an instance of nn.ModuleList
    if isinstance(module, nn.ModuleList):
        return module
    # Recursively search for nn.ModuleList in the submodules of the current module
    for submodule in module.children():
        module_list = find_block_stack(submodule)
        if module_list is not None:
            return module_list
    # If nn.ModuleList is not found in any submodules, return None
    return None
def validate_indices(
    scores: torch.Tensor,
    dim: int = -1,
    indices: OneOrMoreIndices | None = None,
) -> OneOrMoreIndices:
    """Validates a set of indices for a given dimension of a tensor of scores. Supports single indices, spans and lists
    of indices, including negative indices to specify positions relative to the end of the tensor.
    Args:
        scores (torch.Tensor): The tensor of scores.
        dim (int, optional): The dimension of the tensor that will be indexed. Defaults to -1.
        indices (Union[int, tuple[int, int], list[int], None], optional):
            - If an integer, it is interpreted as a single index for the dimension.
            - If a tuple of two integers, it is interpreted as a span of indices for the dimension.
            - If a list of integers, it is interpreted as a list of individual indices for the dimension.
    Returns:
        ``Union[int, tuple[int, int], list[int]]``: The validated list of positive indices for indexing the dimension.
    """
    if dim >= scores.ndim:
        raise IndexError(f"Dimension {dim} is greater than tensor dimension {scores.ndim}")
    n_units = scores.shape[dim]
    if not isinstance(indices, int | tuple | list) and indices is not None:
        raise TypeError(
            "Indices must be an integer, a (start, end) tuple of indices representing a span, a list of individual"
            " indices or a single index."
        )
    if hasattr(indices, "__iter__"):
        if len(indices) == 0:
            raise RuntimeError("An empty sequence of indices is not allowed.")
        if len(indices) == 1:
            indices = indices[0]
    if isinstance(indices, int):
        if indices not in range(-n_units, n_units):
            raise IndexError(f"Index out of range. Scores only have {n_units} units.")
        indices = indices if indices >= 0 else n_units + indices
        return torch.tensor(indices)
    else:
        if indices is None:
            indices = (0, n_units)
            logger.info("No indices specified. Using all indices by default.")
        # Convert negative indices to positive indices
        if hasattr(indices, "__iter__"):
            indices = type(indices)([h_idx if h_idx >= 0 else n_units + h_idx for h_idx in indices])
        if not hasattr(indices, "__iter__") or (
            len(indices) == 2 and isinstance(indices, tuple) and indices[0] >= indices[1]
        ):
            raise RuntimeError(
                "A (start, end) tuple of indices representing a span, a list of individual indices"
                " or a single index must be specified."
            )
        max_idx_val = n_units if isinstance(indices, list) else n_units + 1
        if not all(h in range(-n_units, max_idx_val) for h in indices):
            raise IndexError(f"One or more index out of range. Scores only have {n_units} units.")
        if len(set(indices)) != len(indices):
            raise IndexError("Duplicate indices are not allowed.")
        if isinstance(indices, tuple):
            return torch.arange(indices[0], indices[1])
        else:
            return torch.tensor(indices)
def pad_with_nan(t: torch.Tensor, dim: int, pad_size: int, front: bool = False) -> torch.Tensor:
    """Utility to pad a tensor with nan values along a given dimension."""
    nan_tensor = torch.ones(
        *t.shape[:dim],
        pad_size,
        *t.shape[dim + 1 :],
        device=t.device,
    ) * float("nan")
    if front:
        return torch.cat([nan_tensor, t], dim=dim)
    return torch.cat([t, nan_tensor], dim=dim)
def recursive_get_submodule(parent: nn.Module, target: str) -> nn.Module | None:
    if target == "":
        return parent
    mod = None
    if hasattr(parent, target):
        mod = getattr(parent, target)
    else:
        for submodule in parent.children():
            mod = recursive_get_submodule(submodule, target)
            if mod is not None:
                break
    return mod

================
File: inseq/utils/typing.py
================
from collections.abc import Callable, Sequence
from dataclasses import dataclass
from typing import TYPE_CHECKING, Literal
import torch
from captum.attr._utils.attribution import Attribution
from jaxtyping import Float, Float32, Int64
from transformers import PreTrainedModel
TextInput = str | Sequence[str]
if TYPE_CHECKING:
    from inseq.models import AttributionModel
@dataclass
class TokenWithId:
    token: str
    id: int
    def __str__(self):
        return self.token
    def __eq__(self, other: "str | int | TokenWithId"):
        if isinstance(other, str):
            return self.token == other
        elif isinstance(other, int):
            return self.id == other
        elif isinstance(other, TokenWithId):
            return self.token == other.token and self.id == other.id
        else:
            return False
class InseqAttribution(Attribution):
    """A wrapper class for the Captum library's Attribution class to type hint the ``forward_func`` attribute
    as an :class:`~inseq.models.AttributionModel`.
    """
    def __init__(self, forward_func: "AttributionModel") -> None:
        r"""
        Args:
            forward_func (:class:`~inseq.models.AttributionModel`): The model hooker to the attribution method.
        """
        self.forward_func = forward_func
    attribute: Callable
    @property
    def multiplies_by_inputs(self):
        return False
    def has_convergence_delta(self) -> bool:
        return False
    compute_convergence_delta: Callable
    @classmethod
    def get_name(cls: type["InseqAttribution"]) -> str:
        return "".join([char if char.islower() or idx == 0 else " " + char for idx, char in enumerate(cls.__name__)])
@dataclass
class TextSequences:
    targets: TextInput
    sources: TextInput | None = None
OneOrMoreIdSequences = Sequence[Sequence[int]]
OneOrMoreTokenSequences = Sequence[Sequence[str]]
OneOrMoreTokenWithIdSequences = Sequence[Sequence[TokenWithId]]
OneOrMoreAttributionSequences = Sequence[Sequence[float]]
ScorePrecision = Literal["float32", "float16", "float8"]
IndexSpan = tuple[int, int] | Sequence[tuple[int, int]]
OneOrMoreIndices = int | list[int] | tuple[int, int]
OneOrMoreIndicesDict = dict[int, OneOrMoreIndices]
IdsTensor = Int64[torch.Tensor, "batch_size seq_len"]
TargetIdsTensor = Int64[torch.Tensor, "batch_size"]
ExpandedTargetIdsTensor = Int64[torch.Tensor, "batch_size 1"]
EmbeddingsTensor = Float[torch.Tensor, "batch_size seq_len embed_size"]
MultiStepEmbeddingsTensor = Float[Float, "batch_size_x_n_steps seq_len embed_size"]
VocabularyEmbeddingsTensor = Float[torch.Tensor, "vocab_size embed_size"]
LogitsTensor = Float[torch.Tensor, "batch_size vocab_size"]
ScoreTensor = Float[torch.Tensor, "batch_size other_dims"]
MultiUnitScoreTensor = Float[torch.Tensor, "batch_size n_units other_dims"]
MultiLayerScoreTensor = Float[torch.Tensor, "batch_size n_layers other_dims"]
MultiLayerMultiUnitScoreTensor = Float[torch.Tensor, "batch_size n_layers n_units seq_len seq_len"]
MultiLayerEmbeddingsTensor = Float[torch.Tensor, "batch_size n_layers seq_len embed_size"]
# Step and sequence objects used for stepwise scores (e.g. convergence deltas, probabilities)
SingleScorePerStepTensor = Float32[torch.Tensor, "batch_size"]
SingleScoresPerSequenceTensor = Float32[torch.Tensor, "generated_seq_len"]
# Step and sequence objects used for sequence scores (e.g. attributions over tokens)
MultipleScoresPerStepTensor = Float32[torch.Tensor, "batch_size attributed_seq_len"]
MultipleScoresPerSequenceTensor = Float32[torch.Tensor, "attributed_seq_len generated_seq_len"]
# One attribution score per embedding value for every attributed token
# in a single attribution step. Produced by gradient attribution methods.
GranularStepAttributionTensor = EmbeddingsTensor
# One attribution score per token for a single attribution step
# Either product of aggregation of GranularStepAttributionTensor over last dimension,
# or produced by methods that work at token-level (e.g. attention)
TokenStepAttributionTensor = MultipleScoresPerStepTensor
StepAttributionTensor = GranularStepAttributionTensor | TokenStepAttributionTensor
# One attribution score per embedding value for every attributed token in attributed_seq
# for all generated tokens in generated_seq. Produced by aggregating GranularStepAttributionTensor
# across multiple steps and separating batches.
GranularSequenceAttributionTensor = Float32[torch.Tensor, "attributed_seq_len generated_seq_len embed_size"]
# One attribution score for every token in attributed_seq for every generated token
# in generated_seq. Produced by aggregating GranularSequenceAttributionTensor over the last dimension,
# or by aggregating TokenStepAttributionTensor across multiple steps and separating batches.
TokenSequenceAttributionTensor = MultipleScoresPerSequenceTensor
SequenceAttributionTensor = GranularSequenceAttributionTensor | TokenSequenceAttributionTensor
# For Huggingface it's a string identifier e.g. "t5-base", "Helsinki-NLP/opus-mt-en-it"
# For Fairseq it's a tuple of strings containing repo and model name
# e.g. ("pytorch/fairseq", "transformer.wmt14.en-fr")
ModelIdentifier = str  # Union[str, Tuple[str, str]]
ModelClass = PreTrainedModel
AttributionForwardInputs = IdsTensor | EmbeddingsTensor
AttributionForwardInputsPair = tuple[IdsTensor, IdsTensor] | tuple[EmbeddingsTensor, EmbeddingsTensor]
OneOrTwoAttributionForwardInputs = AttributionForwardInputs | AttributionForwardInputsPair

================
File: inseq/utils/viz_utils.py
================
# Adapted from https://github.com/slundberg/shap/blob/v0.39.0/shap/plots/_text.py, licensed MIT:
# Copyright Â© 2021 Scott Lundberg. All rights reserved.
# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and
# associated documentation files (the â€œSoftwareâ€), to deal in the Software without restriction,
# including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense,
#  and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so,
# subject to the following conditions:
# The above copyright notice and this permission notice shall be included in all copies
# or substantial portions of the Software.
# THE SOFTWARE IS PROVIDED â€œAS ISâ€, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT
# LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
# IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,
# WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE
# OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
from collections.abc import Callable
from functools import wraps
from typing import Any, Literal
import matplotlib.pyplot as plt
import numpy as np
import treescope as ts
from matplotlib.colors import Colormap, LinearSegmentedColormap
from numpy.typing import NDArray
from .misc import isnotebook, ordinal_str
from .typing import TokenWithId
red = (178, 24, 43)
beige = (247, 252, 253)
blue = (33, 102, 172)
green = (0, 109, 44)
brown = (140, 81, 10)
def get_instance_html(i: int):
    return "<br/><b>" + ordinal_str(i) + " instance:</b><br/>"
def interpolate_color(color1, color2, t):
    return tuple(int(c1 + (c2 - c1) * t) for c1, c2 in zip(color1, color2, strict=False))
def generate_colormap(start_color, end_color, num_colors):
    return [interpolate_color(start_color, end_color, t) for t in np.linspace(0, 1, num_colors)]
def red_transparent_blue_colormap():
    colors = []
    for l in np.linspace(1, 0, 100):
        colors.append((*(float(c) / 255 for c in blue), l))
    for l in np.linspace(0, 1, 100):
        colors.append((*(float(c) / 255 for c in red), l))
    return LinearSegmentedColormap.from_list("red_transparent_blue", colors)
def treescope_cmap(colors: Literal["blue_to_red", "brown_to_green", "greens", "blues"] = "blue_to_red", n: int = 200):
    match colors:
        case "blue_to_red":
            first_half = generate_colormap(blue, beige, n // 2)
            second_half = generate_colormap(beige, red, n - len(first_half))
            cmap = first_half + second_half
        case "brown_to_green":
            first_half = generate_colormap(brown, beige, n // 2)
            second_half = generate_colormap(beige, green, n - len(first_half))
            cmap = first_half + second_half
        case "greens":
            cmap = generate_colormap(beige, green, n)
        case "blues":
            cmap = generate_colormap(beige, blue, n)
        case _:
            raise ValueError(f"Invalid color scheme {colors}: valid options are 'blue_to_red', 'greens', 'blues'")
    return cmap
def get_color(
    score: float,
    min_value: float | int,
    max_value: float | int,
    cmap: Colormap,
    return_alpha: bool = True,
    return_string: bool = True,
):
    # Normalize between 0-1 for the color scale
    scaled_value = (score - min_value) / (max_value - min_value)
    color = cmap(scaled_value)
    if return_alpha:
        color = (color[0] * 255, color[1] * 255, color[2] * 255, color[3])
        if return_string:
            color = "rgba" + str(color)
    else:
        color = (color[0] * 255, color[1] * 255, color[2] * 255)
        if return_string:
            color = "rgba" + str(color)
    return color
def sanitize_html(txt: str | TokenWithId) -> str:
    if isinstance(txt, TokenWithId):
        txt = txt.token
    return txt.replace("<", "&lt;").replace(">", "&gt;")
def get_colors(
    scores: NDArray,
    min_value: float | int,
    max_value: float | int,
    cmap: str | Colormap | None = None,
    return_alpha: bool = True,
    return_strings: bool = True,
):
    if isinstance(cmap, Colormap):
        out_cmap = cmap
    else:
        out_cmap: Colormap = plt.get_cmap(cmap if isinstance(cmap, str) else "coolwarm", 200)
    input_colors = []
    for row_idx in range(scores.shape[0]):
        input_colors_row = []
        for col_idx in range(scores.shape[1]):
            color = get_color(scores[row_idx, col_idx], min_value, max_value, out_cmap, return_alpha, return_strings)
            input_colors_row.append(color)
        input_colors.append(input_colors_row)
    return input_colors
def test_dim(dim: int | str, dim_names: dict[int, str], rev_dim_names: dict[str, int], scores: np.ndarray) -> int:
    if isinstance(dim, str):
        if dim not in rev_dim_names:
            raise ValueError(f"Invalid dimension name {dim}: valid names are {list(rev_dim_names.keys())}")
        dim_idx = rev_dim_names[dim]
    else:
        dim_idx = dim
    if dim_idx <= 1 or dim_idx > scores.ndim or dim_idx not in dim_names:
        raise ValueError(f"Invalid dimension {dim_idx}: valid indices are {list(range(2, scores.ndim))}")
    return dim_idx
def maybe_add_linebreak(tok: str, i: int, wrap_after: int | str | list[str] | tuple[str]) -> list[str]:
    if isinstance(wrap_after, str) and tok == wrap_after:
        return [ts.rendering_parts.text("\n")]
    elif isinstance(wrap_after, list | tuple) and tok in wrap_after:
        return [ts.rendering_parts.text("\n")]
    elif isinstance(wrap_after, int) and i % wrap_after == 0:
        return [ts.rendering_parts.text("\n")]
    else:
        return []
def treescope_ignore(f: Callable[..., Any]) -> Callable[..., Any]:
    @wraps(f)
    def treescope_unhooked_wrapper(self, *args, **kwargs):
        if isnotebook():
            # Unhook the treescope visualization to allow `rich.jupyter.JupyterRenderable` to render correctly
            import IPython
            del IPython.get_ipython().display_formatter.formatters["text/html"].type_printers[object]
        out = f(self, *args, **kwargs)
        if isnotebook():
            # Re-hook the treescope visualization
            ts.register_as_default()
        return out
    return treescope_unhooked_wrapper
# Full plot
final_plot_html = """
<html>
<div id="{uuid}_viz_container">
    <div id="{uuid}_content" style="padding:15px;border-style:solid;margin:5px;">
        <div id = "{uuid}_saliency_plot_container" class="{uuid}_viz_container" style="display:block">
            {saliency_plot_markup}
        </div>
    </div>
</div>
</html>
"""
# Saliency plot
saliency_heatmap_table_header = """
<table border="1" cellpadding="5" cellspacing="5"
    style="overflow-x:scroll;display:block;">
"""
saliency_heatmap_html = """
<div id="{uuid}_saliency_plot" class="{uuid}_viz_content">
    <div style="margin:5px;font-family:sans-serif;font-weight:bold;">
        <span style="font-size: 20px;">{label} Saliency Heatmap</span>
        <br>
        â†’ : Generated tokens, â†“ : Attributed tokens
    </div>
    {content}
</div>
"""

================
File: README.md
================
<div align="center">
  <img src="https://raw.githubusercontent.com/inseq-team/inseq/main/docs/source/images/inseq_logo.png" width="300"/>
  <h4>Intepretability for Sequence Generation Models ðŸ”</h4>
</div>
<br/>
<div align="center">


[![Build status](https://img.shields.io/github/actions/workflow/status/inseq-team/inseq/build.yml?branch=main)](https://github.com/inseq-team/inseq/actions?query=workflow%3Abuild)
[![Docs status](https://img.shields.io/readthedocs/inseq)](https://inseq.readthedocs.io)
[![Version](https://img.shields.io/pypi/v/inseq?color=blue)](https://pypi.org/project/inseq/)
[![Python Version](https://img.shields.io/pypi/pyversions/inseq.svg?color=blue)](https://pypi.org/project/inseq/)
[![Downloads](https://static.pepy.tech/badge/inseq)](https://pepy.tech/project/inseq)
[![License](https://img.shields.io/github/license/inseq-team/inseq)](https://github.com/inseq-team/inseq/blob/main/LICENSE)
[![Demo Paper](https://img.shields.io/badge/ACL%20Anthology%20-%20?logo=data%3Aimage%2Fx-icon%3Bbase64%2CAAABAAEAIBIAAAEAIABwCQAAFgAAACgAAAAgAAAAJAAAAAEAIAAAAAAAAAkAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACEa7k0mH%2B%2F5JBzt%2FyQc7f8kHO3%2FJBzt%2FyQc7f8kHO3%2FJBzt%2FyQc7f8kHO3%2FJB3t%2FyQd7f8kHe3%2FJBzt%2FyQc7f8kHO3%2FJBzt%2FyQd7f8kHO3%2FJB3t%2FyQd7f8kHO3%2FJB3t%2FyQd7f8kHe3%2FJB3t%2FyMc79EkGP8VAAAAAAAAAAAAAAAAIRruTSYf7%2FkkHO3%2FJBzt%2FyQd7f8kHe3%2FJB3t%2FyQd7f8kHO3%2FJB3t%2FyQc7f8kHe3%2FJBzt%2FyQc7f8kHO3%2FJB3t%2FyQc7f8kHO3%2FJBzt%2FyQc7f8kHO3%2FJB3t%2FyQc7f8kHO3%2FJB3t%2FyQd7f8kHe3%2FIxzv0SQY%2FxUAAAAAAAAAAAAAAAAhIe5NJh%2Fv%2BSQd7f8kHe3%2FJB3t%2FyQd7f8kHO3%2FJBzt%2FyQc7f8kHe3%2FJBzt%2FyQd7f8kHe3%2FJBzt%2FyQc7f8kHe3%2FJB3t%2FyQd7f8kHe3%2FJB3t%2FyQc7f8kHe3%2FJB3t%2FyQd7f8kHe3%2FJBzt%2FyQd7f8jHO%2FRJBj%2FFQAAAAAAAAAAAAAAACEa7k0mH%2B%2F5JB3t%2FyQc7f8kHO3%2FJBzt%2FyQc7f8kHe3%2FJBzt%2FyQd7f8kHe3%2FJBzt%2FyQc7f8kHO3%2FJBzt%2FyQc7f8kHe3%2FJBzt%2FyQd7f8kHe3%2FJB3t%2FyQc7f8kHe3%2FJB3t%2FyQc7f8kHO3%2FJBzt%2FyMc79EkGP8VAAAAAAAAAAAAAAAAIRruTSYf7%2FkkHO3%2FJBzt%2FyQc7f8kHe3%2FJB3t%2FyQd7f8kHO3%2FJB3t%2FyQd7f8kHO3%2FJB3t%2FyQc7f8kHe3%2FJBzt%2FyQc7f8kHe3%2FJB3t%2FyQd7f8kHe3%2FJBzt%2FyQd7f8kHO3%2FJB3t%2FyQc7f8kHO3%2FIxzv0SQY%2FxUAAAAAAAAAAAAAAAAhGu5NJh%2Fv%2BSQd7f8kHO3%2FJBzt%2FyQd7f8jIOzYIxvtgiQc8X8kHPF%2FJBzxfyQc8X8kHPF%2FJBzxfyQc8X8kHPF%2FIx%2FuiiMf7OgkHe3%2FJBzt%2FyQc7f8kHO3%2FJhzs9CUg7JYkHPF%2FJBzxfyQc8X8iHfBoMzP%2FCgAAAAAAAAAAAAAAACEa7k0mHu%2F5JBzt%2FyQc7f8kHO3%2FJBzt%2FyQb7LEAAP8FAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAkGP8VIxzv0SQc7f8kHe3%2FJB3t%2FyQc7f8jHOzpIhzuLQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIRruTSYf7%2FkkHe3%2FJB3t%2FyQd7f8kHO3%2FJBvssQAA%2FwUAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACQY%2FxUjHO%2FRJB3t%2FyQc7f8kHO3%2FJBzt%2FyMb7OkiHO4tAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAhGu5NJh%2Fv%2BSQd7f8kHe3%2FJBzt%2FyQc7f8kHuyxAAD%2FBQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAJBj%2FFSMc79EkHe3%2FJBzt%2FyQc7f8kHO3%2FIxvs6SIc7i0AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACEa7k0mH%2B%2F5JBzt%2FyQc7f8kHO3%2FJBzt%2FyQb7LEAAP8FAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAkGP8VIx3v0SQd7f8kHe3%2FJBzt%2FyQc7f8jHOzpIhzuLQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIRruTSYf7%2FkkHO3%2FJBzt%2FyQc7f8kHe3%2FJBzuxSgi81MhGu5NIRruTSEa7k0hGu5NISHuTSEh7k0hGu5NIRruTSIa72EjHe3aJBzt%2FyQd7f8kHO3%2FJBzt%2FyMc7OkiHO4tAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAhGu5NJh7v%2BSQc7f8kHe3%2FJBzt%2FyQc7f8kHO3%2FJh%2Fv%2BSYf7%2FkmHu%2F5Jh%2Fv%2BSYf7%2FkmH%2B%2F5Jh%2Fv%2BSYf7%2FkmH%2B%2F5Jh7v%2BSQc7f8kHO3%2FJBzt%2FyQc7f8kHO3%2FIxzs6SIc7i0AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACEa7k0mHu%2F5JBzt%2FyQd7f8kHO3%2FJBzt%2FyQc7f8kHe3%2FJB3t%2FyQd7f8kHO3%2FJBzt%2FyQc7f8kHe3%2FJB3t%2FyQc7f8kHO3%2FJBzt%2FyQc7f8kHe3%2FJBzt%2FyQd7f8jHOzpIhzuLQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAIRruTSYe7%2FkkHO3%2FJB3t%2FyQc7f8kHO3%2FJBzt%2FyQd7f8kHe3%2FJB3t%2FyQc7f8kHe3%2FJB3t%2FyQd7f8kHO3%2FJBzt%2FyQc7f8kHO3%2FJB3t%2FyQc7f8kHO3%2FJBzt%2FyMc7OkiHO4tAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAhGu5NJh%2Fv%2BSQc7f8kHO3%2FJB3t%2FyQc7f8kHO3%2FJBzt%2FyQc7f8kHO3%2FJBzt%2FyQc7f8kHO3%2FJBzt%2FyQc7f8kHO3%2FJBzt%2FyQc7f8kHO3%2FJBzt%2FyQc7f8kHO3%2FIxzs6SIc7i0AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACEa7k0mH%2B%2F5JB3t%2FyQc7f8kHe3%2FJBzt%2FyQc7f8kHO3%2FJB3t%2FyQc7f8kHO3%2FJBzt%2FyQd7f8kHO3%2FJBzt%2FyQc7f8kHO3%2FJB3t%2FyQc7f8kHO3%2FJBzt%2FyQc7f8jHOzpIhzuLQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAKB%2FtOSUc7askHuyxJBvssSQe7LEkHuyxJB7ssSQe7LEkHuyxJBvssSQb7LEkHuyxJB7ssSQe7LEkHuyxJB7ssSUc7LMjHe31JB3t%2FyQd7f8kHe3%2FJBzt%2FyMc7OkiHO4tAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD%2FBQAA%2FwUAAP8FAAD%2FBQAA%2FwUAAP8FAAD%2FBQAA%2FwUAAP8FAAD%2FBQAA%2FwUAAP8FAAD%2FBQAA%2FwUAAP8FHBzsGyMd7qYjHO%2FRIxzv0SMd79EjHO%2FRIx7tux4Y%2BSoAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADAAAABwAAAAcAAAAHAAAABwAAAAcAAAAHAP8A%2FwD%2FAP8A%2FwD%2FAP8A%2FwAAAP8AAAD%2FAAAA%2FwAAAP8AAAD%2FAAAA%2FwAAAP%2BAAAD8%3D&labelColor=white&color=red&link=https%3A%2F%2Faclanthology.org%2F2023.acl-demo.40%2F
)](https://aclanthology.org/2023.acl-demo.40)

</div>
<div align="center">

  [![Follow Inseq on Twitter](https://img.shields.io/badge/Twitter-1DA1F2?style=for-the-badge&logo=twitter&logoColor=white)](https://twitter.com/InseqLib)
  [![Join the Inseq Discord server](https://img.shields.io/badge/Discord-7289DA?style=for-the-badge&logo=discord&logoColor=white)](https://discord.gg/V5VgwwFPbu)
  [![Read the Docs](https://img.shields.io/badge/-Docs-blue?style=for-the-badge&logo=Read-the-Docs&logoColor=white&link=https://inseq.org)](https://inseq.org)
  [![Tutorial](https://img.shields.io/badge/-Tutorial-orange?style=for-the-badge&logo=Jupyter&logoColor=white&link=https://github.com/inseq-team/inseq/blob/main/examples/inseq_tutorial.ipynb)](https://github.com/inseq-team/inseq/blob/main/examples/inseq_tutorial.ipynb)


</div>

Inseq is a Pytorch-based hackable toolkit to democratize access to common post-hoc **in**terpretability analyses of **seq**uence generation models.

## Installation

Inseq is available on PyPI and can be installed with `pip` for Python >= 3.10, <= 3.12:

```bash
# Install latest stable version
pip install inseq

# Alternatively, install latest development version
pip install git+https://github.com/inseq-team/inseq.git
```

Install extras for visualization in Jupyter Notebooks and ðŸ¤— datasets attribution as `pip install inseq[notebook,datasets]`.

<details>
  <summary>Dev Installation</summary>
To install the package, clone the repository and run the following commands:

```bash
cd inseq
make uv-download # Download and install the UV package manager
make install # Installs the package and all dependencies
```

For library developers, you can use the `make install-dev` command to install all development dependencies (quality, docs, extras).

After installation, you should be able to run `make fast-test` and `make lint` without errors.
</details>

<details>
  <summary>FAQ Installation</summary>

- Installing the `tokenizers` package requires a Rust compiler installation. You can install Rust from [https://rustup.rs](https://rustup.rs) and add `$HOME/.cargo/env` to your PATH.

- Installing `sentencepiece` requires various packages, install with `sudo apt-get install cmake build-essential pkg-config` or `brew install cmake gperftools pkg-config`.

</details>

## Example usage in Python

This example uses the Integrated Gradients attribution method to attribute the English-French translation of a sentence taken from the WinoMT corpus:

```python
import inseq

model = inseq.load_model("Helsinki-NLP/opus-mt-en-fr", "integrated_gradients")
out = model.attribute(
  "The developer argued with the designer because her idea cannot be implemented.",
  n_steps=100
)
out.show()
```

This produces a visualization of the attribution scores for each token in the input sentence (token-level aggregation is handled automatically). Here is what the visualization looks like inside a Jupyter Notebook:

![WinoMT Attribution Map](https://raw.githubusercontent.com/inseq-team/inseq/main/docs/source/images/heatmap_winomt.png)

Inseq also supports decoder-only models such as [GPT-2](https://huggingface.co/transformers/model_doc/gpt2.html), enabling usage of a variety of attribution methods and customizable settings directly from the console:

```python
import inseq

model = inseq.load_model("gpt2", "integrated_gradients")
model.attribute(
    "Hello ladies and",
    generation_args={"max_new_tokens": 9},
    n_steps=500,
    internal_batch_size=50
).show()
```

![GPT-2 Attribution in the console](https://raw.githubusercontent.com/inseq-team/inseq/main/docs/source/images/inseq_python_console.gif)

## Features

- ðŸš€ Feature attribution of sequence generation for most `ForConditionalGeneration` (encoder-decoder) and `ForCausalLM` (decoder-only) models from ðŸ¤— Transformers

- ðŸš€ Support for multiple feature attribution methods, extending the ones supported by [Captum](https://captum.ai/docs/introduction)

- ðŸš€ Post-processing, filtering and merging of attribution maps via `Aggregator` classes.

- ðŸš€ Attribution visualization in notebooks, browser and command line.

- ðŸš€ Efficient attribution of single examples or entire ðŸ¤— datasets with the Inseq CLI.

- ðŸš€ Custom attribution of target functions, supporting advanced methods such as [contrastive feature attributions](https://aclanthology.org/2022.emnlp-main.14/) and [context reliance detection](https://arxiv.org/abs/2310.01188).

- ðŸš€ Extraction and visualization of custom scores (e.g. probability, entropy) at every generation step alongsides attribution maps.

### Supported methods

Use the `inseq.list_feature_attribution_methods` function to list all available method identifiers and `inseq.list_step_functions` to list all available step functions. The following methods are currently supported:

#### Gradient-based attribution

- `saliency`: [Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps](https://arxiv.org/abs/1312.6034) (Simonyan et al., 2013)

- `input_x_gradient`: [Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps](https://arxiv.org/abs/1312.6034) (Simonyan et al., 2013)

- `integrated_gradients`: [Axiomatic Attribution for Deep Networks](https://arxiv.org/abs/1703.01365) (Sundararajan et al., 2017)

- `deeplift`: [Learning Important Features Through Propagating Activation Differences](https://arxiv.org/abs/1704.02685) (Shrikumar et al., 2017)

- `gradient_shap`: [A unified approach to interpreting model predictions](https://dl.acm.org/doi/10.5555/3295222.3295230) (Lundberg and Lee, 2017)

- `discretized_integrated_gradients`: [Discretized Integrated Gradients for Explaining Language Models](https://aclanthology.org/2021.emnlp-main.805/) (Sanyal and Ren, 2021)

- `sequential_integrated_gradients`: [Sequential Integrated Gradients: a simple but effective method for explaining language models](https://aclanthology.org/2023.findings-acl.477/) (Enguehard, 2023)

#### Internals-based attribution

- `attention`: Attention Weight Attribution, from [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473) (Bahdanau et al., 2014)

#### Perturbation-based attribution

- `occlusion`: [Visualizing and Understanding Convolutional Networks](https://link.springer.com/chapter/10.1007/978-3-319-10590-1_53) (Zeiler and Fergus, 2014)

- `lime`: ["Why Should I Trust You?": Explaining the Predictions of Any Classifier](https://arxiv.org/abs/1602.04938) (Ribeiro et al., 2016)

- `value_zeroing`: [Quantifying Context Mixing in Transformers](https://aclanthology.org/2023.eacl-main.245/) (Mohebbi et al. 2023)

- `reagent`: [ReAGent: A Model-agnostic Feature Attribution Method for Generative Language Models](https://arxiv.org/abs/2402.00794) (Zhao et al., 2024)

#### Step functions

Step functions are used to extract custom scores from the model at each step of the attribution process with the `step_scores` argument in `model.attribute`. They can also be used as targets for attribution methods relying on model outputs (e.g. gradient-based methods) by passing them as the `attributed_fn` argument. The following step functions are currently supported:

- `logits`: Logits of the target token.
- `probability`: Probability of the target token. Can also be used for log-probability by passing `logprob=True`.
- `entropy`: Entropy of the predictive distribution.
- `crossentropy`: Cross-entropy loss between target token and predicted distribution.
- `perplexity`: Perplexity of the target token.
- `contrast_logits`/`contrast_prob`: Logits/probabilities of the target token when different contrastive inputs are provided to the model. Equivalent to `logits`/`probability` when no contrastive inputs are provided.
- `contrast_logits_diff`/`contrast_prob_diff`: Difference in logits/probability between original and foil target tokens pair, can be used for contrastive evaluation as in [contrastive attribution](https://aclanthology.org/2022.emnlp-main.14/) (Yin and Neubig, 2022).
- `pcxmi`: Point-wise Contextual Cross-Mutual Information (P-CXMI) for the target token given original and contrastive contexts [(Yin et al. 2021)](https://arxiv.org/abs/2109.07446).
- `kl_divergence`: KL divergence of the predictive distribution given original and contrastive contexts. Can be restricted to most likely target token options using the `top_k` and `top_p` parameters.
- `in_context_pvi`: In-context Pointwise V-usable Information (PVI) to measure the amount of contextual information used in model predictions [(Lu et al. 2023)](https://arxiv.org/abs/2310.12300).
- `mc_dropout_prob_avg`: Average probability of the target token across multiple samples using [MC Dropout](https://arxiv.org/abs/1506.02142) (Gal and Ghahramani, 2016).
- `top_p_size`: The number of tokens with cumulative probability greater than `top_p` in the predictive distribution of the model.

The following example computes contrastive attributions using the `contrast_prob_diff` step function:

```python
import inseq

attribution_model = inseq.load_model("gpt2", "input_x_gradient")

# Perform the contrastive attribution:
# Regular (forced) target -> "The manager went home because he was sick"
# Contrastive target      -> "The manager went home because she was sick"
out = attribution_model.attribute(
    "The manager went home because",
    "The manager went home because he was sick",
    attributed_fn="contrast_prob_diff",
    contrast_targets="The manager went home because she was sick",
    # We also visualize the corresponding step score
    step_scores=["contrast_prob_diff"]
)
out.show()
```

Refer to the [documentation](https://inseq.readthedocs.io/examples/custom_attribute_target.html) for an example including custom function registration.

## Using the Inseq CLI

The Inseq library also provides useful client commands to enable repeated attribution of individual examples and even entire ðŸ¤— datasets directly from the console. See the available options by typing `inseq -h` in the terminal after installing the package.

Three commands are supported:

- `inseq attribute`: Wrapper for enabling `model.attribute` usage in console.

- `inseq attribute-dataset`: Extends `attribute` to full dataset using Hugging Face `datasets.load_dataset` API.

- `inseq attribute-context`: Detects and attribute context dependence for generation tasks using the approach of [Sarti et al. (2023)](https://arxiv.org/abs/2310.01188).

All commands support the full range of parameters available for `attribute`, attribution visualization in the console and saving outputs to disk.

<details>
  <summary><code>inseq attribute</code> example</summary>

  The following example performs a simple feature attribution of an English sentence translated into Italian using a MarianNMT translation model from <code>transformers</code>. The final result is printed to the console.
  ```bash
  inseq attribute \
  --model_name_or_path Helsinki-NLP/opus-mt-en-it \
  --attribution_method saliency \
  --input_texts "Hello world this is Inseq\! Inseq is a very nice library to perform attribution analysis"
  ```

</details>

<details>
  <summary><code>inseq attribute-dataset</code> example</summary>

  The following code can be used to perform attribution (both source and target-side) of Italian translations for a dummy sample of 20 English sentences taken from the FLORES-101 parallel corpus, using a MarianNMT translation model from Hugging Face <code>transformers</code>. We save the visualizations in HTML format in the file <code>attributions.html</code>. See the <code>--help</code> flag for more options.

  ```bash
  inseq attribute-dataset \
    --model_name_or_path Helsinki-NLP/opus-mt-en-it \
    --attribution_method saliency \
    --do_prefix_attribution \
    --dataset_name inseq/dummy_enit \
    --input_text_field en \
    --dataset_split "train[:20]" \
    --viz_path attributions.html \
    --batch_size 8 \
    --hide
  ```
</details>

<details>
  <summary><code>inseq attribute-context</code> example</summary>

  The following example uses a small LM to generate a continuation of <code>input_current_text</code>, and uses the additional context provided by <code>input_context_text</code> to estimate its influence on the the generation. In this case, the output <code>"to the hospital. He said he was fine"</code> is produced, and the generation of token <code>hospital</code> is found to be dependent on context token <code>sick</code> according to the <code>contrast_prob_diff</code> step function.

  ```bash
  inseq attribute-context \
    --model_name_or_path HuggingFaceTB/SmolLM-135M \
    --input_context_text "George was sick yesterday." \
    --input_current_text "His colleagues asked him to come" \
    --attributed_fn "contrast_prob_diff"
  ```

  **Result:**

  <img src="https://raw.githubusercontent.com/inseq-team/inseq/main/docs/source/images/attribute_context_hospital_output.png" style="width:500px">
</details>

## Planned Development

- âš™ï¸ Support more attention-based and occlusion-based feature attribution methods (documented in [#107](https://github.com/inseq-team/inseq/issues/107) and [#108](https://github.com/inseq-team/inseq/issues/108)).

- âš™ï¸ Interoperability with [ferret](https://ferret.readthedocs.io/en/latest/) for attribution plausibility and faithfulness evaluation.

- âš™ï¸ Rich and interactive visualizations in a tabbed interface using [Gradio Blocks](https://gradio.app/docs/#blocks).

## Contributing

Our vision for Inseq is to create a centralized, comprehensive and robust set of tools to enable fair and reproducible comparisons in the study of sequence generation models. To achieve this goal, contributions from researchers and developers interested in these topics are more than welcome. Please see our [contributing guidelines](CONTRIBUTING.md) and our [code of conduct](CODE_OF_CONDUCT.md) for more information.

## Citing Inseq

If you use Inseq in your research we suggest including a mention of the specific release (e.g. v0.6.0) and we kindly ask you to cite our reference paper as:

```bibtex
@inproceedings{sarti-etal-2023-inseq,
    title = "Inseq: An Interpretability Toolkit for Sequence Generation Models",
    author = "Sarti, Gabriele  and
      Feldhus, Nils  and
      Sickert, Ludwig  and
      van der Wal, Oskar and
      Nissim, Malvina and
      Bisazza, Arianna",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-demo.40",
    doi = "10.18653/v1/2023.acl-demo.40",
    pages = "421--435",
}

```

## Research using Inseq

Inseq has been used in various research projects. A list of known publications that use Inseq to conduct interpretability analyses of generative models is shown below.

> [!TIP]
> Last update: August 2024. Please open a pull request to add your publication to the list.

<details>
  <summary><b>2023</b></summary>
  <ol>
    <li> <a href="https://aclanthology.org/2023.acl-demo.40/">Inseq: An Interpretability Toolkit for Sequence Generation Models</a> (Sarti et al., 2023) </li>
    <li> <a href="https://doi.org/10.1162/tacl_a_00651">Are Character-level Translations Worth the Wait? Comparing ByT5 and mT5 for Machine Translation</a> (Edman et al., 2023) </li>
    <li> <a href="https://aclanthology.org/2023.nlp4convai-1.1/">Response Generation in Longitudinal Dialogues: Which Knowledge Representation Helps?</a> (Mousavi et al., 2023)  </li>
    <li> <a href="https://openreview.net/forum?id=XTHfNGI3zT">Quantifying the Plausibility of Context Reliance in Neural Machine Translation</a> (Sarti et al., 2023)</li>
    <li> <a href="https://aclanthology.org/2023.emnlp-main.243/">A Tale of Pronouns: Interpretability Informs Gender Bias Mitigation for Fairer Instruction-Tuned Machine Translation</a> (Attanasio et al., 2023)</li>
    <li> <a href="https://aclanthology.org/2023.conll-1.18/">Attribution and Alignment: Effects of Local Context Repetition on Utterance Production and Comprehension in Dialogue</a> (Molnar et al., 2023)</li>
  </ol>

</details>

<details>
  <summary><b>2024</b></summary>
  <ol>
    <li> <a href="https://aclanthology.org/2024.naacl-long.46/">Assessing the Reliability of Large Language Model Knowledge</a> (Wang et al., 2024)</li>
    <li><a href="https://aclanthology.org/2024.hcinlp-1.9">LLMCheckup: Conversational Examination of Large Language Models via Interpretability Tools</a> (Wang et al., 2024)</li>
    <li><a href="https://arxiv.org/abs/2402.00794">ReAGent: A Model-agnostic Feature Attribution Method for Generative Language Models</a> (Zhao et al., 2024)</li>
    <li><a href="https://aclanthology.org/2024.naacl-long.284">Revisiting subword tokenization: A case study on affixal negation in large language models</a> (Truong et al., 2024)</li>
    <li><a href="https://hal.science/hal-04581586">Exploring NMT Explainability for Translators Using NMT Visualising Tools</a> (Gonzalez-Saez et al., 2024)</li>
    <li><a href="https://openreview.net/forum?id=uILj5HPrag">DETAIL: Task DEmonsTration Attribution for Interpretable In-context Learning</a> (Zhou et al., 2024)</li>
    <li><a href="https://arxiv.org/abs/2406.06399">Should We Fine-Tune or RAG? Evaluating Different Techniques to Adapt LLMs for Dialogue</a> (Alghisi et al., 2024)</li>
    <li><a href="https://arxiv.org/abs/2406.13663">Model Internals-based Answer Attribution for Trustworthy Retrieval-Augmented Generation</a> (Qi, Sarti et al., 2024)</li>
    <li><a href="https://link.springer.com/chapter/10.1007/978-3-031-63787-2_14">NoNE Found: Explaining the Output of Sequence-to-Sequence Models When No Named Entity Is Recognized</a> (dela Cruz et al., 2024)</li>
  </ol>

</details>

================
File: requirements-dev.txt
================
# This file was autogenerated by uv via the following command:
#    uv pip compile --all-extras pyproject.toml -o requirements-dev.txt
aiohttp==3.9.5
    # via
    #   datasets
    #   fsspec
aiosignal==1.3.1
    # via aiohttp
alabaster==0.7.16
    # via sphinx
appnope==0.1.4
    # via ipykernel
asttokens==2.4.1
    # via stack-data
attrs==23.2.0
    # via aiohttp
authlib==1.3.1
    # via safety
babel==2.14.0
    # via sphinx
bandit==1.7.7
    # via inseq (pyproject.toml)
beautifulsoup4==4.12.3
    # via furo
captum==0.7.0
    # via inseq (pyproject.toml)
certifi==2024.7.4
    # via requests
cffi==1.16.0
    # via cryptography
cfgv==3.4.0
    # via pre-commit
charset-normalizer==3.3.2
    # via requests
click==8.1.7
    # via
    #   nltk
    #   pydoclint
    #   safety
    #   typer
comm==0.2.1
    # via
    #   ipykernel
    #   ipywidgets
contourpy==1.2.0
    # via matplotlib
coverage==7.4.1
    # via pytest-cov
cryptography==43.0.0
    # via authlib
cycler==0.12.1
    # via matplotlib
datasets==2.17.0
    # via inseq (pyproject.toml)
debugpy==1.8.1
    # via ipykernel
decorator==5.1.1
    # via ipython
dill==0.3.8
    # via
    #   datasets
    #   multiprocess
distlib==0.3.8
    # via virtualenv
docstring-parser-fork==0.0.5
    # via pydoclint
docutils==0.20.1
    # via sphinx
dparse==0.6.4b0
    # via
    #   safety
    #   safety-schemas
execnet==2.0.2
    # via pytest-xdist
executing==2.0.1
    # via stack-data
filelock==3.13.1
    # via
    #   datasets
    #   huggingface-hub
    #   torch
    #   transformers
    #   virtualenv
fonttools==4.48.1
    # via matplotlib
frozenlist==1.4.1
    # via
    #   aiohttp
    #   aiosignal
fsspec==2023.10.0
    # via
    #   datasets
    #   huggingface-hub
    #   torch
furo==2024.1.29
    # via inseq (pyproject.toml)
gitdb==4.0.11
    # via gitpython
gitpython==3.1.41
    # via sphinx-gitstamp
huggingface-hub==0.20.3
    # via
    #   datasets
    #   tokenizers
    #   transformers
identify==2.5.34
    # via pre-commit
idna==3.7
    # via
    #   requests
    #   yarl
imagesize==1.4.1
    # via sphinx
iniconfig==2.0.0
    # via pytest
ipykernel==6.29.2
    # via inseq (pyproject.toml)
ipython==8.18.1
    # via
    #   ipykernel
    #   ipywidgets
ipywidgets==8.1.2
    # via inseq (pyproject.toml)
jaxtyping==0.2.25
    # via inseq (pyproject.toml)
jedi==0.19.1
    # via ipython
jinja2==3.1.4
    # via
    #   safety
    #   sphinx
    #   torch
joblib==1.3.2
    # via
    #   inseq (pyproject.toml)
    #   nltk
    #   scikit-learn
jupyter-client==8.6.0
    # via ipykernel
jupyter-core==5.7.1
    # via
    #   ipykernel
    #   jupyter-client
jupyterlab-widgets==3.0.10
    # via ipywidgets
kiwisolver==1.4.5
    # via matplotlib
markdown-it-py==3.0.0
    # via rich
markupsafe==2.1.5
    # via jinja2
marshmallow==3.20.2
    # via safety
matplotlib==3.8.2
    # via
    #   inseq (pyproject.toml)
    #   captum
matplotlib-inline==0.1.6
    # via
    #   ipykernel
    #   ipython
mdurl==0.1.2
    # via markdown-it-py
mpmath==1.3.0
    # via sympy
multidict==6.0.5
    # via
    #   aiohttp
    #   yarl
multiprocess==0.70.16
    # via datasets
nest-asyncio==1.6.0
    # via ipykernel
networkx==3.2.1
    # via torch
nltk==3.8.1
    # via inseq (pyproject.toml)
nodeenv==1.8.0
    # via pre-commit
numpy==1.26.4
    # via
    #   inseq (pyproject.toml)
    #   captum
    #   contourpy
    #   datasets
    #   jaxtyping
    #   matplotlib
    #   pandas
    #   pyarrow
    #   scikit-learn
    #   scipy
    #   transformers
    #   treescope
packaging==23.2
    # via
    #   datasets
    #   dparse
    #   huggingface-hub
    #   ipykernel
    #   marshmallow
    #   matplotlib
    #   pytest
    #   safety
    #   safety-schemas
    #   sphinx
    #   transformers
pandas==2.2.0
    # via datasets
parso==0.8.3
    # via jedi
pbr==6.0.0
    # via stevedore
pexpect==4.9.0
    # via ipython
pillow==10.4.0
    # via matplotlib
platformdirs==4.2.0
    # via
    #   jupyter-core
    #   virtualenv
pluggy==1.4.0
    # via pytest
pre-commit==3.6.1
    # via inseq (pyproject.toml)
prompt-toolkit==3.0.43
    # via ipython
protobuf==4.25.2
    # via
    #   inseq (pyproject.toml)
    #   transformers
psutil==5.9.8
    # via ipykernel
ptyprocess==0.7.0
    # via pexpect
pure-eval==0.2.2
    # via stack-data
pyarrow==15.0.0
    # via datasets
pyarrow-hotfix==0.6
    # via datasets
pycparser==2.21
    # via cffi
pydantic==1.10.14
    # via
    #   safety
    #   safety-schemas
pydoclint==0.4.0
    # via inseq (pyproject.toml)
pygments==2.17.2
    # via
    #   furo
    #   ipython
    #   rich
    #   sphinx
pyparsing==3.1.1
    # via matplotlib
pytest==8.0.0
    # via
    #   inseq (pyproject.toml)
    #   pytest-cov
    #   pytest-xdist
pytest-cov==4.1.0
    # via inseq (pyproject.toml)
pytest-xdist==3.5.0
    # via inseq (pyproject.toml)
python-dateutil==2.8.2
    # via
    #   jupyter-client
    #   matplotlib
    #   pandas
pytz==2024.1
    # via pandas
pyyaml==6.0.1
    # via
    #   bandit
    #   datasets
    #   huggingface-hub
    #   pre-commit
    #   transformers
pyzmq==25.1.2
    # via
    #   ipykernel
    #   jupyter-client
regex==2023.12.25
    # via
    #   nltk
    #   transformers
requests==2.32.3
    # via
    #   datasets
    #   fsspec
    #   huggingface-hub
    #   safety
    #   sphinx
    #   transformers
rich==13.7.0
    # via
    #   inseq (pyproject.toml)
    #   bandit
    #   safety
ruamel-yaml==0.18.6
    # via
    #   safety
    #   safety-schemas
ruamel-yaml-clib==0.2.8
    # via ruamel-yaml
ruff==0.2.1
    # via inseq (pyproject.toml)
safetensors==0.4.2
    # via transformers
safety==3.1.0
    # via inseq (pyproject.toml)
safety-schemas==0.0.2
    # via safety
scikit-learn==1.5.1
    # via inseq (pyproject.toml)
scipy==1.12.0
    # via scikit-learn
sentencepiece==0.2.0
    # via transformers
setuptools==72.2.0
    # via
    #   nodeenv
    #   safety
six==1.16.0
    # via
    #   asttokens
    #   python-dateutil
smmap==5.0.1
    # via gitdb
snowballstemmer==2.2.0
    # via sphinx
soupsieve==2.5
    # via beautifulsoup4
sphinx==7.2.6
    # via
    #   inseq (pyproject.toml)
    #   furo
    #   sphinx-basic-ng
    #   sphinx-copybutton
    #   sphinx-design
    #   sphinx-gitstamp
    #   sphinxemoji
    #   sphinxext-opengraph
sphinx-basic-ng==1.0.0b2
    # via furo
sphinx-copybutton==0.5.2
    # via inseq (pyproject.toml)
sphinx-design==0.5.0
    # via inseq (pyproject.toml)
sphinx-gitstamp==0.4.0
    # via inseq (pyproject.toml)
sphinxcontrib-applehelp==1.0.8
    # via sphinx
sphinxcontrib-devhelp==1.0.6
    # via sphinx
sphinxcontrib-htmlhelp==2.0.5
    # via sphinx
sphinxcontrib-jsmath==1.0.1
    # via sphinx
sphinxcontrib-qthelp==1.0.7
    # via sphinx
sphinxcontrib-serializinghtml==1.1.10
    # via sphinx
sphinxemoji==0.3.1
    # via inseq (pyproject.toml)
sphinxext-opengraph==0.9.1
    # via inseq (pyproject.toml)
stack-data==0.6.3
    # via ipython
stevedore==5.1.0
    # via bandit
sympy==1.12
    # via torch
threadpoolctl==3.2.0
    # via scikit-learn
tokenizers==0.15.2
    # via transformers
torch==2.3.1
    # via
    #   inseq (pyproject.toml)
    #   captum
tornado==6.4.1
    # via
    #   ipykernel
    #   jupyter-client
tqdm==4.66.4
    # via
    #   inseq (pyproject.toml)
    #   captum
    #   datasets
    #   huggingface-hub
    #   nltk
    #   transformers
traitlets==5.14.1
    # via
    #   comm
    #   ipykernel
    #   ipython
    #   ipywidgets
    #   jupyter-client
    #   jupyter-core
    #   matplotlib-inline
transformers==4.38.1
    # via inseq (pyproject.toml)
treescope==0.1.0
    # via inseq (pyproject.toml)
typeguard==2.13.3
    # via
    #   inseq (pyproject.toml)
    #   jaxtyping
typer==0.9.0
    # via safety
typing-extensions==4.9.0
    # via
    #   huggingface-hub
    #   jaxtyping
    #   pydantic
    #   safety
    #   safety-schemas
    #   torch
    #   typer
tzdata==2024.1
    # via pandas
urllib3==2.2.2
    # via
    #   requests
    #   safety
virtualenv==20.27.1
    # via
    #   inseq (pyproject.toml)
    #   pre-commit
wcwidth==0.2.13
    # via prompt-toolkit
widgetsnbextension==4.0.10
    # via ipywidgets
xxhash==3.4.1
    # via datasets
yarl==1.9.4
    # via aiohttp

================
File: requirements.txt
================
# This file was autogenerated by uv via the following command:
#    uv pip compile pyproject.toml -o requirements.txt
captum==0.7.0
    # via inseq (pyproject.toml)
certifi==2024.7.4
    # via requests
charset-normalizer==3.3.2
    # via requests
contourpy==1.2.0
    # via matplotlib
cycler==0.12.1
    # via matplotlib
filelock==3.13.1
    # via
    #   huggingface-hub
    #   torch
    #   transformers
fonttools==4.48.1
    # via matplotlib
fsspec==2023.10.0
    # via
    #   huggingface-hub
    #   torch
huggingface-hub==0.20.3
    # via
    #   tokenizers
    #   transformers
idna==3.7
    # via requests
jaxtyping==0.2.25
    # via inseq (pyproject.toml)
jinja2==3.1.4
    # via torch
kiwisolver==1.4.5
    # via matplotlib
markdown-it-py==3.0.0
    # via rich
markupsafe==2.1.5
    # via jinja2
matplotlib==3.8.2
    # via
    #   inseq (pyproject.toml)
    #   captum
mdurl==0.1.2
    # via markdown-it-py
mpmath==1.3.0
    # via sympy
networkx==3.2.1
    # via torch
numpy==1.26.4
    # via
    #   inseq (pyproject.toml)
    #   captum
    #   contourpy
    #   jaxtyping
    #   matplotlib
    #   transformers
    #   treescope
packaging==23.2
    # via
    #   huggingface-hub
    #   matplotlib
    #   transformers
pillow==10.4.0
    # via matplotlib
protobuf==4.25.2
    # via
    #   inseq (pyproject.toml)
    #   transformers
pygments==2.17.2
    # via rich
pyparsing==3.1.1
    # via matplotlib
python-dateutil==2.8.2
    # via matplotlib
pyyaml==6.0.1
    # via
    #   huggingface-hub
    #   transformers
regex==2023.12.25
    # via transformers
requests==2.32.3
    # via
    #   huggingface-hub
    #   transformers
rich==13.7.0
    # via inseq (pyproject.toml)
safetensors==0.4.2
    # via transformers
sentencepiece==0.2.0
    # via transformers
six==1.16.0
    # via python-dateutil
sympy==1.12
    # via torch
tokenizers==0.15.2
    # via transformers
torch==2.3.1
    # via
    #   inseq (pyproject.toml)
    #   captum
tqdm==4.66.4
    # via
    #   inseq (pyproject.toml)
    #   captum
    #   huggingface-hub
    #   transformers
transformers==4.38.1
    # via inseq (pyproject.toml)
treescope==0.1.0
    # via inseq (pyproject.toml)
typeguard==2.13.3
    # via
    #   inseq (pyproject.toml)
    #   jaxtyping
typing-extensions==4.9.0
    # via
    #   huggingface-hub
    #   jaxtyping
    #   torch
urllib3==2.2.2
    # via requests

================
File: SECURITY.md
================
# Security

## ðŸ” Reporting Security Issues

> Do not open issues that might have security implications!
> It is critical that security related issues are reported privately so we have time to address them before they become public knowledge.

Vulnerabilities can be reported by emailing core members:

- [Gabriele Sarti](https://github.com/gsarti) <[gabriele.sarti996@gmail.com](mailto:gabriele.sarti996@gmail.com)>

Please include the requested information listed below (as much as you can provide) to help us better understand the nature and scope of the possible issue:

- Type of issue (e.g. buffer overflow, SQL injection, cross-site scripting, etc.)
- Full paths of source file(s) related to the manifestation of the issue
- The location of the affected source code (tag/branch/commit or direct URL)
- Any special configuration required to reproduce the issue
- Environment (e.g. Linux / Windows / macOS)
- Step-by-step instructions to reproduce the issue
- Proof-of-concept or exploit code (if possible)
- Impact of the issue, including how an attacker might exploit the issue

This information will help us triage your report more quickly.

## Preferred Languages

We prefer all communications to be in English.

================
File: tests/__init__.py
================
import os
TEST_DIR = os.path.dirname(os.path.abspath(__file__))

================
File: tests/attr/feat/ops/test_monotonic_path_builder.py
================
from itertools import islice
from inseq.utils import is_joblib_available
if is_joblib_available():
    from joblib import Parallel, delayed
import pytest
import torch
import inseq
from inseq.attr.feat.ops import MonotonicPathBuilder
from inseq.utils import euclidean_distance
@pytest.fixture(scope="session")
def dig_model():
    return inseq.load_model("Helsinki-NLP/opus-mt-de-en", "discretized_integrated_gradients", device="cpu")
def original_monotonic(vec1, vec2, vec3):
    """Taken verbatim from https://github.com/INK-USC/DIG/blob/main/monotonic_paths.py."""
    increasing_dims = vec1 > vec2  # dims where baseline > input
    decreasing_dims = vec1 < vec2  # dims where baseline < input
    equal_dims = vec1 == vec2  # dims where baseline == input
    vec3_greater_vec1 = vec3 >= vec1
    vec3_greater_vec2 = vec3 >= vec2
    vec3_lesser_vec1 = vec3 <= vec1
    vec3_lesser_vec2 = vec3 <= vec2
    vec3_equal_vec1 = vec3 == vec1
    vec3_equal_vec2 = vec3 == vec2
    valid = (
        increasing_dims * vec3_lesser_vec1 * vec3_greater_vec2
        + decreasing_dims * vec3_greater_vec1 * vec3_lesser_vec2
        + equal_dims * vec3_equal_vec1 * vec3_equal_vec2
    )
    return valid  # vec condition
def original_dummy_find_word_path(wrd_idx: int, n_steps: int):
    word_path = [wrd_idx]
    last_idx = wrd_idx
    for _ in range(n_steps):
        # The original code finds the next word
        # We only want to test the walrus operator variant, so any method is ok. We use hash.
        next_idx = hash(last_idx + 0.01 + len(word_path) / 1000)
        word_path.append(next_idx)
        last_idx = next_idx
    return word_path
def walrus_operator_find_word_path(wrd_idx: int, n_steps: int):
    word_path = [wrd_idx]
    for _ in range(n_steps):
        word_path.append(wrd_idx := hash(wrd_idx + 0.01 + len(word_path) / 1000))
    return word_path
@pytest.mark.parametrize(
    "input_dims",
    [
        ((512,)),
        ((512, 12)),
        ((512, 12, 3)),
    ],
)
def test_equivalent_monotonic_method(input_dims: tuple[int, ...]) -> None:
    torch.manual_seed(42)
    baseline_embeds = torch.randn(input_dims)
    input_embeds = torch.randn(input_dims)
    interpolated_embeds = torch.randn(input_dims)
    out = MonotonicPathBuilder.get_monotonic_dims(interpolated_embeds, baseline_embeds, input_embeds)
    orig_out = original_monotonic(baseline_embeds, input_embeds, interpolated_embeds)
    assert torch.equal(out.int(), orig_out.int())
def test_valid_distance_multidim_tensors() -> None:
    torch.manual_seed(42)
    vec_a = torch.randn((512,))
    vec_b = torch.randn((512,))
    vec_a_multi = torch.stack([vec_a, vec_a, vec_a], dim=0)
    vec_b_multi = torch.stack([vec_b, vec_b, vec_b], dim=0)
    assert list(vec_a.shape) == list(vec_b.shape) == [512]
    assert list(vec_a_multi.shape) == list(vec_b_multi.shape) == [3, 512]
    dist = euclidean_distance(vec_a, vec_b)
    dist_multi = euclidean_distance(vec_a_multi, vec_b_multi)
    assert not list(dist.shape)  # scalar
    assert list(dist_multi.shape)[0] == vec_a_multi.shape[0] == vec_b_multi.shape[0]
    assert torch.equal(dist_multi[0], dist) and torch.equal(dist_multi[1], dist) and torch.equal(dist_multi[2], dist)
@pytest.mark.parametrize(
    ("wrd_idx", "n_steps"),
    [
        (0, 10),
        (10, 100),
        (100, 1000),
    ],
)
def test_walrus_find_word_path(wrd_idx: int, n_steps: int) -> None:
    assert original_dummy_find_word_path(wrd_idx, n_steps) == walrus_operator_find_word_path(wrd_idx, n_steps)
@pytest.mark.slow
@pytest.mark.parametrize(
    "word_idx",
    [(0), (1), (735), (111), (10296)],
)
def test_scaled_monotonic_path_embeddings(word_idx: int, dig_model) -> None:
    assert torch.allclose(
        dig_model.embed(torch.tensor([word_idx])),
        dig_model.attribution_method.method.path_builder.vocabulary_embeddings[word_idx],
    )
@pytest.mark.slow
@pytest.mark.skipif(
    not is_joblib_available(),
    reason="joblib is not available",
)
@pytest.mark.parametrize(
    "ids",
    [
        (
            [
                [226, 1127, 499, 3, 1046, 24, 9, 387, 513, 49, 0],
                [975, 444, 53, 360, 471, 4, 308, 19, 0, 58100, 58100],
            ]
        )
    ],
)
def test_parallel_find_word(ids: list[list[int]], dig_model) -> None:
    pathsa = []
    for seq in ids:
        tok_paths = []
        for tok in seq:
            tok_paths.append(dig_model.attribution_method.method.path_builder.find_path(tok, 58100))
        pathsa.append(tok_paths)
    tmp_all = Parallel(n_jobs=3, prefer="threads")(
        delayed(dig_model.attribution_method.method.path_builder.find_path)(tok, 58100) for seq in ids for tok in seq
    )
    elems = iter(tmp_all)
    pathsb = [list(islice(elems, len(seq))) for seq in ids]
    assert pathsa == pathsb

================
File: tests/attr/feat/test_attribution_utils.py
================
import pytest
import torch
import inseq
from inseq.attr.step_functions import get_step_scores
from ...inference_commons import get_example_batches
@pytest.fixture(scope="session")
def encoder_decoder_batches():
    return get_example_batches()
@pytest.fixture(scope="session")
def m2m100_model():
    return inseq.load_model(
        "facebook/m2m100_418M",
        "integrated_gradients",
        tokenizer_kwargs={"src_lang": "en", "tgt_lang": "en", "use_fast": False},
    )
def test_get_step_prediction_probabilities(m2m100_model, encoder_decoder_batches):
    # fmt: off
    probas = [
        0.622, 0.008, 0.006, 0.841, 0.002, 0.127, 0.003, 0.087, 0.0,
        0.843, 0.744, 0.865, 0.012,  0.27, 0.085, 0.739, 0.749, 0.9,
    ]
    # fmt: on
    for i, (batch, next_batch) in enumerate(
        zip(encoder_decoder_batches["batches"][1:], encoder_decoder_batches["batches"][2:], strict=False)
    ):
        output = m2m100_model.get_forward_output(
            batch.to(m2m100_model.device), use_embeddings=m2m100_model.attribution_method.forward_batch_embeds
        )
        target_ids = next_batch.targets.encoding.input_ids[0, -1].to(m2m100_model.device)
        step_fn_args = m2m100_model.formatter.format_step_function_args(
            attribution_model=m2m100_model, forward_output=output, target_ids=target_ids, batch=batch
        )
        pred_proba = get_step_scores("probability", step_fn_args)
        assert float(pred_proba) == pytest.approx(probas[i], abs=1e-3)
def test_crossentropy_nlll_equivalence(m2m100_model, encoder_decoder_batches):
    for batch, next_batch in zip(
        encoder_decoder_batches["batches"][1:], encoder_decoder_batches["batches"][2:], strict=False
    ):
        batch.to(m2m100_model.device)
        next_batch.to(m2m100_model.device)
        output = m2m100_model.model(
            input_ids=None,
            inputs_embeds=batch.sources.input_embeds,
            attention_mask=batch.sources.attention_mask,
            decoder_inputs_embeds=batch.targets.input_embeds,
            decoder_attention_mask=batch.targets.attention_mask,
        )
        # Full logits for last position of every sentence:
        # (batch_size, tgt_seq_len, vocab_size) => (batch_size, vocab_size)
        logits = output.logits[:, -1, :].squeeze(1).detach().to("cpu")
        batch.detach().to("cpu")
        next_batch.detach().to("cpu")
        cross_entropy = torch.nn.functional.cross_entropy(logits, next_batch.targets.encoding.input_ids[:, -1])
        nlll = torch.nn.functional.nll_loss(
            torch.log(torch.softmax(logits, dim=-1)), next_batch.targets.encoding.input_ids[:, -1]
        )
        assert cross_entropy == pytest.approx(nlll, abs=1e-3)

================
File: tests/attr/feat/test_feature_attribution.py
================
from typing import Any
import torch
from captum._utils.typing import TensorOrTupleOfTensorsGeneric
from pytest import fixture
import inseq
from inseq.attr.feat.internals_attribution import InternalsAttributionRegistry
from inseq.data import MultiDimensionalFeatureAttributionStepOutput
from inseq.models import HuggingfaceDecoderOnlyModel, HuggingfaceEncoderDecoderModel
from inseq.utils.typing import InseqAttribution, MultiLayerMultiUnitScoreTensor
@fixture(scope="session")
def saliency_mt_model_larger() -> HuggingfaceEncoderDecoderModel:
    return inseq.load_model("Helsinki-NLP/opus-mt-en-it", "saliency")
@fixture(scope="session")
def saliency_gpt_model_larger() -> HuggingfaceDecoderOnlyModel:
    return inseq.load_model("gpt2", "saliency")
@fixture(scope="session")
def saliency_mt_model() -> HuggingfaceEncoderDecoderModel:
    return inseq.load_model("hf-internal-testing/tiny-random-MarianMTModel", "saliency")
@fixture(scope="session")
def saliency_gpt_model() -> HuggingfaceDecoderOnlyModel:
    return inseq.load_model("hf-internal-testing/tiny-random-GPT2LMHeadModel", "saliency")
def test_contrastive_attribution_seq2seq(saliency_mt_model_larger: HuggingfaceEncoderDecoderModel):
    """Runs a contrastive feature attribution using the method relying on logits difference
    introduced by [Yin and Neubig '22](https://arxiv.org/pdf/2202.10419.pdf), taking advantage of
    the custom feature attribution target function module.
    """
    # Perform the contrastive attribution:
    # Regular (forced) target -> "Non posso crederci."
    # Contrastive target      -> "Non posso crederlo."
    # contrast_ids & contrast_attention_mask are kwargs defined in the function definition
    out = saliency_mt_model_larger.attribute(
        "I can't believe it",
        "Non posso crederci.",
        attributed_fn="contrast_prob_diff",
        contrast_targets="Non posso crederlo.",
        show_progress=False,
    )
    attribution_scores = out.sequence_attributions[0].source_attributions
    # Since the two target strings are identical for the first three tokens (Non posso creder)
    # the resulting contrastive source attributions should be all 0
    assert attribution_scores[:, :3].sum().eq(0)
    # Starting at the following token in which they differ, scores should diverge
    assert not attribution_scores[:, :4].sum().eq(0)
def test_contrastive_attribution_gpt(saliency_gpt_model: HuggingfaceDecoderOnlyModel):
    out = saliency_gpt_model.attribute(
        "The female student didn't participate because",
        "The female student didn't participate because she was sick.",
        attributed_fn="contrast_prob_diff",
        contrast_targets="The female student didn't participate because he was sick.",
        show_progress=False,
    )
    attribution_scores = out.sequence_attributions[0].target_attributions
    assert attribution_scores.shape == torch.Size([23, 5, 32])
def test_contrastive_attribution_seq2seq_alignments(saliency_mt_model_larger: HuggingfaceEncoderDecoderModel):
    aligned = {
        "src": "UN peacekeepers",
        "orig_tgt": "I soldati della pace ONU",
        "contrast_tgt": "Le forze militari di pace delle Nazioni Unite",
        "alignments": [[(0, 0), (1, 1), (2, 2), (3, 4), (4, 5), (5, 7), (6, 9)]],
        "aligned_tgts": ["<pad>", "â–Le â†’ â–I", "â–forze â†’ â–soldati", "â–di â†’ â–della", "â–pace", "â–Nazioni â†’ â–ONU", "</s>"],
    }
    out = saliency_mt_model_larger.attribute(
        aligned["src"],
        aligned["orig_tgt"],
        attributed_fn="contrast_prob_diff",
        step_scores=["contrast_prob_diff"],
        contrast_targets=aligned["contrast_tgt"],
        contrast_targets_alignments=aligned["alignments"],
        show_progress=False,
    )
    # Check tokens are aligned as expected
    assert [t.token for t in out[0].target] == aligned["aligned_tgts"]
    # Check that a single list of alignments is correctly processed
    out_single_list = saliency_mt_model_larger.attribute(
        aligned["src"],
        aligned["orig_tgt"],
        attributed_fn="contrast_prob_diff",
        step_scores=["contrast_prob_diff"],
        contrast_targets=aligned["contrast_tgt"],
        contrast_targets_alignments=aligned["alignments"][0],
        attribute_target=True,
        show_progress=False,
    )
    assert out[0].target == out_single_list[0].target
    assert torch.allclose(
        out[0].source_attributions,
        out_single_list[0].source_attributions,
        atol=8e-2,
    )
def test_mcd_weighted_attribution_seq2seq(saliency_mt_model):
    """Runs a MCD-weighted feature attribution taking advantage of
    the custom feature attribution target function module.
    """
    out = saliency_mt_model.attribute(
        "Hello ladies and badgers!",
        attributed_fn="mc_dropout_prob_avg",
        n_mcd_steps=3,
        show_progress=False,
    )
    attribution_scores = out.sequence_attributions[0].source_attributions
    assert isinstance(attribution_scores, torch.Tensor)
def test_mcd_weighted_attribution_gpt(saliency_gpt_model):
    """Runs a MCD-weighted feature attribution taking advantage of
    the custom feature attribution target function module.
    """
    out = saliency_gpt_model.attribute(
        "Hello ladies and badgers!",
        attributed_fn="mc_dropout_prob_avg",
        n_mcd_steps=3,
        generation_args={"max_new_tokens": 3},
        show_progress=False,
    )
    attribution_scores = out.sequence_attributions[0].target_attributions
    assert isinstance(attribution_scores, torch.Tensor)
class MultiStepAttentionWeights(InseqAttribution):
    """Variant of the AttentionWeights class with is_final_step_method = False.
    As a result, the attention matrix is computed and sliced at every generation step.
    We define it here to test consistency with the final step method.
    """
    def attribute(
        self,
        inputs: TensorOrTupleOfTensorsGeneric,
        additional_forward_args: TensorOrTupleOfTensorsGeneric,
        encoder_self_attentions: MultiLayerMultiUnitScoreTensor | None = None,
        decoder_self_attentions: MultiLayerMultiUnitScoreTensor | None = None,
        cross_attentions: MultiLayerMultiUnitScoreTensor | None = None,
    ) -> MultiDimensionalFeatureAttributionStepOutput:
        # We adopt the format [batch_size, sequence_length, num_layers, num_heads]
        # for consistency with other multi-unit methods (e.g. gradient attribution)
        decoder_self_attentions = decoder_self_attentions[..., -1, :].to("cpu").clone().permute(0, 3, 1, 2)
        if self.forward_func.is_encoder_decoder:
            sequence_scores = {}
            if len(inputs) > 1:
                target_attributions = decoder_self_attentions
            else:
                target_attributions = None
                sequence_scores["decoder_self_attentions"] = decoder_self_attentions
            sequence_scores["encoder_self_attentions"] = (
                encoder_self_attentions.to("cpu").clone().permute(0, 4, 3, 1, 2)
            )
            return MultiDimensionalFeatureAttributionStepOutput(
                source_attributions=cross_attentions[..., -1, :].to("cpu").clone().permute(0, 3, 1, 2),
                target_attributions=target_attributions,
                sequence_scores=sequence_scores,
                _num_dimensions=2,  # num_layers, num_heads
            )
        else:
            return MultiDimensionalFeatureAttributionStepOutput(
                source_attributions=None,
                target_attributions=decoder_self_attentions,
                _num_dimensions=2,  # num_layers, num_heads
            )
class MultiStepAttentionWeightsAttribution(InternalsAttributionRegistry):
    """Variant of the basic attention attribution method computing attention weights at every generation step."""
    method_name = "per_step_attention"
    def __init__(self, attribution_model, **kwargs):
        super().__init__(attribution_model)
        # Attention weights will be passed to the attribute_step method
        self.use_attention_weights = True
        # Does not rely on predicted output (i.e. decoding strategy agnostic)
        self.use_predicted_target = False
        self.method = MultiStepAttentionWeights(attribution_model)
    def attribute_step(
        self,
        attribute_fn_main_args: dict[str, Any],
        attribution_args: dict[str, Any],
    ) -> MultiDimensionalFeatureAttributionStepOutput:
        return self.method.attribute(**attribute_fn_main_args, **attribution_args)
def test_seq2seq_final_step_per_step_conformity(saliency_mt_model_larger: HuggingfaceEncoderDecoderModel):
    out_per_step = saliency_mt_model_larger.attribute(
        "Hello ladies and badgers!",
        method="per_step_attention",
        attribute_target=True,
        show_progress=False,
        output_step_attributions=True,
    )
    out_final_step = saliency_mt_model_larger.attribute(
        "Hello ladies and badgers!",
        method="attention",
        attribute_target=True,
        show_progress=False,
        output_step_attributions=True,
    )
    assert out_per_step[0] == out_final_step[0]
def test_gpt_final_step_per_step_conformity(saliency_gpt_model_larger: HuggingfaceDecoderOnlyModel):
    out_per_step = saliency_gpt_model_larger.attribute(
        "Hello ladies and badgers!",
        method="per_step_attention",
        show_progress=False,
        output_step_attributions=True,
    )
    out_final_step = saliency_gpt_model_larger.attribute(
        "Hello ladies and badgers!",
        method="attention",
        show_progress=False,
        output_step_attributions=True,
    )
    assert out_per_step[0] == out_final_step[0]
# Batching for Seq2Seq models is not supported when using is_final_step methods
# Passing several sentences will attributed them one by one under the hood
# def test_seq2seq_multi_step_attention_weights_batched_full_match(saliency_mt_model: HuggingfaceEncoderDecoderModel):
def test_gpt_multi_step_attention_weights_batched_full_match(saliency_gpt_model_larger: HuggingfaceDecoderOnlyModel):
    out_per_step = saliency_gpt_model_larger.attribute(
        ["Hello world!", "Colorless green ideas sleep furiously."],
        method="per_step_attention",
        show_progress=False,
    )
    out_final_step = saliency_gpt_model_larger.attribute(
        ["Hello world!", "Colorless green ideas sleep furiously."],
        method="attention",
        show_progress=False,
    )
    for i in range(2):
        assert out_per_step[i].target_attributions.shape == out_final_step[i].target_attributions.shape
        assert torch.allclose(
            out_per_step[i].target_attributions, out_final_step[i].target_attributions, equal_nan=True, atol=1e-5
        )

================
File: tests/attr/feat/test_step_functions.py
================
import torch
from pytest import fixture
import inseq
from inseq.attr.step_functions import StepFunctionArgs, _get_contrast_inputs, probability_fn
from inseq.models import DecoderOnlyAttributionModel, EncoderDecoderAttributionModel
@fixture(scope="session")
def saliency_gpt2():
    return inseq.load_model("distilgpt2", "saliency")
@fixture(scope="session")
def saliency_mt_model():
    return inseq.load_model("Helsinki-NLP/opus-mt-en-it", "saliency")
def test_contrast_prob_consistency_decoder(saliency_gpt2: DecoderOnlyAttributionModel):
    out_contrast = saliency_gpt2.attribute(
        " the manager opened",
        " the manager opened her own restaurant.",
        attribute_target=True,
        step_scores=["contrast_prob"],
        contrast_targets="After returning to her hometown, the manager opened her own restaurant.",
    )
    contrast_prob = out_contrast.sequence_attributions[0].step_scores["contrast_prob"]
    out_regular = saliency_gpt2.attribute(
        "After returning to her hometown, the manager opened",
        "After returning to her hometown, the manager opened her own restaurant.",
        attribute_target=True,
        step_scores=["probability"],
    )
    regular_prob = out_regular.sequence_attributions[0].step_scores["probability"]
    assert all(c == r for c, r in zip(contrast_prob, regular_prob, strict=False))
def test_contrast_prob_consistency_enc_dec(saliency_mt_model: EncoderDecoderAttributionModel):
    out_contrast = saliency_mt_model.attribute(
        "she started working as a cook in London.",
        "ha iniziato a lavorare come cuoca a Londra.",
        attribute_target=True,
        step_scores=["contrast_prob"],
        contrast_sources="After finishing her studies, she started working as a cook in London.",
        contrast_targets="Dopo aver terminato gli studi, ha iniziato a lavorare come cuoca a Londra.",
    )
    contrast_prob = out_contrast.sequence_attributions[0].step_scores["contrast_prob"]
    out_regular = saliency_mt_model.attribute(
        "After finishing her studies, she started working as a cook in London.",
        "Dopo aver terminato gli studi, ha iniziato a lavorare come cuoca a Londra.",
        attribute_target=True,
        step_scores=["probability"],
    )
    regular_prob = out_regular.sequence_attributions[0].step_scores["probability"]
    assert all(c == r for c, r in zip(contrast_prob, regular_prob[-len(contrast_prob) :], strict=False))
def attr_prob_diff_fn(
    args: StepFunctionArgs,
    contrast_targets,
    contrast_targets_alignments=None,
    logprob: bool = False,
):
    model_probs = probability_fn(args, logprob=logprob)
    c_out = _get_contrast_inputs(
        args,
        contrast_targets=contrast_targets,
        contrast_targets_alignments=contrast_targets_alignments,
        return_contrastive_target_ids=True,
    )
    args.target_ids = c_out.target_ids
    contrast_probs = probability_fn(args, logprob=logprob)
    return model_probs - contrast_probs
def test_contrast_attribute_target_only_enc_dec(saliency_mt_model: EncoderDecoderAttributionModel):
    inseq.register_step_function(fn=attr_prob_diff_fn, identifier="attr_prob_diff", overwrite=True)
    src = "The nurse was tired and went home."
    tgt = "L'infermiere era stanco e andÃ² a casa."
    contrast_tgt = "L'infermiera era stanca e andÃ² a casa."
    out_explicit_logit_prob_diff = saliency_mt_model.attribute(
        src,
        tgt,
        contrast_targets=contrast_tgt,
        attributed_fn="attr_prob_diff",
        step_scores=["attr_prob_diff", "contrast_prob_diff"],
        attribute_target=True,
    )
    out_default_prob_diff = saliency_mt_model.attribute(
        src,
        tgt,
        contrast_targets=contrast_tgt,
        attributed_fn="contrast_prob_diff",
        step_scores=["contrast_prob_diff"],
        attribute_target=True,
    )
    assert torch.allclose(
        out_explicit_logit_prob_diff[0].step_scores["contrast_prob_diff"],
        out_default_prob_diff[0].step_scores["contrast_prob_diff"],
    )
    assert torch.allclose(
        out_explicit_logit_prob_diff[0].source_attributions,
        out_default_prob_diff[0].source_attributions,
    )
    assert torch.allclose(
        out_explicit_logit_prob_diff[0].target_attributions,
        out_default_prob_diff[0].target_attributions,
        equal_nan=True,
    )
    out_contrast_force_inputs_prob_diff = saliency_mt_model.attribute(
        src,
        tgt,
        contrast_targets=contrast_tgt,
        attributed_fn="contrast_prob_diff",
        step_scores=["contrast_prob_diff"],
        attribute_target=True,
        contrast_force_inputs=True,
    )
    assert not torch.allclose(
        out_explicit_logit_prob_diff[0].source_attributions,
        out_contrast_force_inputs_prob_diff[0].source_attributions,
    )
    assert not torch.allclose(
        out_explicit_logit_prob_diff[0].target_attributions,
        out_contrast_force_inputs_prob_diff[0].target_attributions,
        equal_nan=True,
    )
    assert torch.allclose(
        out_explicit_logit_prob_diff[0].step_scores["contrast_prob_diff"],
        out_default_prob_diff[0].step_scores["contrast_prob_diff"],
    )

================
File: tests/commands/test_attribute_context.py
================
import json
import pytest
from pytest import fixture
from transformers import AutoModelForCausalLM, AutoModelForSeq2SeqLM, GPT2LMHeadModel, MarianMTModel
from inseq.commands.attribute_context import AttributeContextArgs, AttributeContextOutput, CCIOutput
from inseq.commands.attribute_context.attribute_context import attribute_context
@fixture(scope="session")
def encdec_model() -> MarianMTModel:
    return AutoModelForSeq2SeqLM.from_pretrained("Helsinki-NLP/opus-mt-en-fr")
@fixture(scope="session")
def deconly_model() -> GPT2LMHeadModel:
    return AutoModelForCausalLM.from_pretrained("gpt2")
def round_scores(cli_out: AttributeContextOutput) -> AttributeContextOutput:
    cli_out.cti_scores = [round(score, 2) for score in cli_out.cti_scores]
    for idx in range(len(cli_out.cci_scores)):
        cci = cli_out.cci_scores[idx]
        cli_out.cci_scores[idx].cti_score = round(cli_out.cci_scores[idx].cti_score, 2)
        if cci.input_context_scores is not None:
            cli_out.cci_scores[idx].input_context_scores = [round(s, 2) for s in cci.input_context_scores]
        if cci.output_context_scores is not None:
            cli_out.cci_scores[idx].output_context_scores = [round(s, 2) for s in cci.output_context_scores]
    return cli_out
def test_in_out_ctx_encdec_whitespace_sep(encdec_model: MarianMTModel):
    # Base case for context-aware encoder-decoder: no language tag, no special token in separator
    # source context (input context) is translated into target context (output context).
    in_out_ctx_encdec_whitespace_sep = AttributeContextArgs(
        model_name_or_path=encdec_model,
        input_context_text="The girls were away.",
        input_current_text="Where are they?",
        output_template="{context} {current}",
        input_template="{context} {current}",
        attributed_fn="contrast_prob_diff",
        show_viz=False,
        show_intermediate_outputs=True,
        # Pre-defining natural model outputs to avoid user input in unit tests
        output_context_text="",
        output_current_text="OÃ¹ sont-elles?",
        add_output_info=False,
    )
    expected_output = AttributeContextOutput(
        input_context="The girls were away.",
        input_context_tokens=["â–The", "â–girls", "â–were", "â–away", "."],
        output_context="",
        output_context_tokens=[],
        output_current="OÃ¹ sont-elles?",
        output_current_tokens=["â–OÃ¹", "â–sont", "-", "elles", "?"],
        cti_scores=[1.36, 0.08, 0.34, 1.23, 0.27],
        cci_scores=[
            CCIOutput(
                cti_idx=0,
                cti_token="â–OÃ¹",
                contrast_token="â–OÃ¹",
                cti_score=1.36,
                contextual_output="OÃ¹",
                contextless_output="OÃ¹",
                input_context_scores=[0.01, 0.01, 0.01, 0.01, 0.01],
                output_context_scores=[],
            )
        ],
        info=None,
    )
    cli_out = attribute_context(in_out_ctx_encdec_whitespace_sep)
    assert round_scores(cli_out) == expected_output
def test_in_ctx_deconly(deconly_model: GPT2LMHeadModel):
    # Base case for context-aware decoder-only model with input-only context.
    in_ctx_deconly = AttributeContextArgs(
        model_name_or_path=deconly_model,
        input_context_text="George was sick yesterday.",
        input_current_text="His colleagues asked him to come",
        output_current_text="to the hospital. He said he was fine",
        attributed_fn="contrast_prob_diff",
        show_viz=False,
        add_output_info=False,
    )
    expected_output = AttributeContextOutput(
        input_context="George was sick yesterday.",
        input_context_tokens=["George", "Ä was", "Ä sick", "Ä yesterday", "."],
        output_context=None,
        output_context_tokens=None,
        output_current="to the hospital. He said he was fine",
        output_current_tokens=["Ä to", "Ä the", "Ä hospital", ".", "Ä He", "Ä said", "Ä he", "Ä was", "Ä fine"],
        cti_scores=[0.31, 0.25, 0.55, 0.16, 0.43, 0.19, 0.13, 0.07, 0.37],
        cci_scores=[
            CCIOutput(
                cti_idx=2,
                cti_token="Ä hospital",
                contrast_token="Ä office",
                cti_score=0.55,
                contextual_output="George was sick yesterday. His colleagues asked him to come to the hospital",
                contextless_output="His colleagues asked him to come to the office",
                input_context_scores=[0.39, 0.29, 0.52, 0.26, 0.16],
                output_context_scores=None,
            )
        ],
        info=None,
    )
    cli_out = attribute_context(in_ctx_deconly)
    assert round_scores(cli_out) == expected_output
def test_out_ctx_deconly(deconly_model: GPT2LMHeadModel):
    # Base case for context-aware decoder-only model with forced output context mocking a reasoning chain.
    out_ctx_deconly = AttributeContextArgs(
        model_name_or_path=deconly_model,
        output_template="Let's think step by step:\n{context}\n\nAnswer:\n{current}",
        input_template="{current}",
        input_current_text="Question: How many pairs of legs do 10 horses have?",
        output_context_text="1. A horse has 4 legs.\n2. 10 horses have 40 legs.\n3. 40 legs make 20 pairs of legs.",
        output_current_text="20 pairs of legs.",
        attributed_fn="contrast_prob_diff",
        decoder_input_output_separator="\n",
        contextless_output_current_text="Answer:\n{current}",
        show_viz=False,
        add_output_info=False,
    )
    expected_output = AttributeContextOutput(
        input_context=None,
        input_context_tokens=None,
        output_context="1. A horse has 4 legs.\n2. 10 horses have 40 legs.\n3. 40 legs make 20 pairs of legs.",
        output_context_tokens=[
            "1",
            ".",
            "Ä A",
            "Ä horse",
            "Ä has",
            "Ä 4",
            "Ä legs",
            ".",
            "ÄŠ",
            "2",
            ".",
            "Ä 10",
            "Ä horses",
            "Ä have",
            "Ä 40",
            "Ä legs",
            ".",
            "ÄŠ",
            "3",
            ".",
            "Ä 40",
            "Ä legs",
            "Ä make",
            "Ä 20",
            "Ä pairs",
            "Ä of",
            "Ä legs",
            ".",
        ],
        output_current="20 pairs of legs.",
        output_current_tokens=["20", "Ä pairs", "Ä of", "Ä legs", "."],
        cti_scores=[0.77, 1.39, 0.54, 0.48, 0.91],
        cci_scores=[
            CCIOutput(
                cti_idx=1,
                cti_token="Ä pairs",
                contrast_token="Ä horses",
                cti_score=1.39,
                contextual_output="Question: How many pairs of legs do 10 horses have?\nLet's think step by step:\n1. A horse has 4 legs.\n2. 10 horses have 40 legs.\n3. 40 legs make 20 pairs of legs.\n\nAnswer:\n20 pairs",
                contextless_output="Question: How many pairs of legs do 10 horses have?\nAnswer:\n20 horses",
                input_context_scores=None,
                output_context_scores=[
                    0.1,
                    0.1,
                    0.06,
                    0.19,
                    0.05,
                    0.07,
                    0.17,
                    0.1,
                    0.08,
                    0.07,
                    0.11,
                    0.22,
                    0.44,
                    0.07,
                    0.15,
                    0.17,
                    0.12,
                    0.13,
                    0.06,
                    0.14,
                    0.11,
                    0.19,
                    0.16,
                    0.34,
                    1.33,
                    0.04,
                    0.13,
                    0.07,
                ],
            ),
        ],
        info=None,
    )
    cli_out = attribute_context(out_ctx_deconly)
    assert round_scores(cli_out).cci_scores[0] == expected_output.cci_scores[0]
def test_in_out_ctx_deconly(deconly_model: GPT2LMHeadModel):
    # Base case for context-aware decoder-only model with input and forced output context.
    in_out_ctx_deconly = AttributeContextArgs(
        model_name_or_path=deconly_model,
        input_context_text="George was sick yesterday.",
        input_current_text="His colleagues asked him if",
        output_context_text="something was wrong. He said",
        output_current_text="he was fine.",
        attributed_fn="contrast_prob_diff",
        show_viz=False,
        add_output_info=False,
    )
    expected_output = AttributeContextOutput(
        input_context="George was sick yesterday.",
        input_context_tokens=["George", "Ä was", "Ä sick", "Ä yesterday", "."],
        output_context="something was wrong. He said",
        output_context_tokens=["something", "Ä was", "Ä wrong", ".", "Ä He", "Ä said"],
        output_current="he was fine.",
        output_current_tokens=["Ä he", "Ä was", "Ä fine", "."],
        cti_scores=[1.2, 0.72, 1.5, 0.49],
        cci_scores=[
            CCIOutput(
                cti_idx=2,
                cti_token="Ä fine",
                contrast_token="Ä a",
                cti_score=1.5,
                contextual_output="George was sick yesterday. His colleagues asked him if something was wrong. He said he was fine",
                contextless_output="His colleagues asked him if he was a",
                input_context_scores=[0.19, 0.15, 0.33, 0.13, 0.15],
                output_context_scores=[0.08, 0.07, 0.14, 0.12, 0.09, 0.14],
            )
        ],
        info=None,
    )
    cli_out = attribute_context(in_out_ctx_deconly)
    assert round_scores(cli_out) == expected_output
def test_in_ctx_encdec_special_sep():
    # Encoder-decoder model with special separator tags in input only, context is given only in the source
    # (input context) but not produced in the target (output context).
    in_ctx_encdec_special_sep = AttributeContextArgs(
        model_name_or_path="context-mt/scat-marian-small-ctx4-cwd1-en-fr",
        input_context_text="The girls were away.",
        input_current_text="Where are they?",
        output_template="{current}",
        input_template="{context} <brk> {current}",
        special_tokens_to_keep=["<brk>"],
        attributed_fn="contrast_prob_diff",
        show_viz=False,
        add_output_info=False,
    )
    expected_output = AttributeContextOutput(
        input_context="The girls were away.",
        input_context_tokens=["â–The", "â–girls", "â–were", "â–away", "."],
        output_context=None,
        output_context_tokens=None,
        output_current="OÃ¹ sont-elles ?",
        output_current_tokens=["â–OÃ¹", "â–sont", "-", "elles", "â–?"],
        cti_scores=[0.08, 0.04, 0.01, 0.32, 0.06],
        cci_scores=[
            CCIOutput(
                cti_idx=3,
                cti_token="elles",
                contrast_token="elles",
                cti_score=0.32,
                contextual_output="OÃ¹ sont-elles",
                contextless_output="OÃ¹ sont-elles",
                input_context_scores=[0.0, 0.0, 0.0, 0.0, 0.0],
                output_context_scores=None,
            )
        ],
        info=None,
    )
    cli_out = attribute_context(in_ctx_encdec_special_sep)
    assert round_scores(cli_out) == expected_output
def test_in_out_ctx_encdec_special_sep():
    # Encoder-decoder model with special separator tags in input and output, context is given in the source (input context)
    # and produced in the target (output context) before the special token separator.
    in_out_ctx_encdec_special_sep = AttributeContextArgs(
        model_name_or_path="context-mt/scat-marian-small-target-ctx4-cwd0-en-fr",
        input_context_text="The girls were away.",
        input_current_text="Where are they?",
        output_template="{context}<brk> {current}",
        input_template="{context} <brk> {current}",
        special_tokens_to_keep=["<brk>"],
        attributed_fn="contrast_prob_diff",
        show_viz=False,
        add_output_info=False,
        # Pre-defining natural model outputs to avoid user input in unit tests
        output_context_text="Les filles Ã©taient parties.",
    )
    expected_output = AttributeContextOutput(
        input_context="The girls were away.",
        input_context_tokens=["â–The", "â–girls", "â–were", "â–away", "."],
        output_context="Les filles Ã©taient parties.",
        output_context_tokens=["â–Les", "â–filles", "â–Ã©taient", "â–parties", "."],
        output_current="OÃ¹ sont-elles ?",
        output_current_tokens=["â–OÃ¹", "â–sont", "-", "elles", "â–?"],
        cti_scores=[0.17, 0.03, 0.02, 3.99, 0.0],
        cci_scores=[
            CCIOutput(
                cti_idx=3,
                cti_token="elles",
                contrast_token="ils",
                cti_score=3.99,
                contextual_output="Les filles Ã©taient parties.<brk>  OÃ¹ sont-elles",
                contextless_output="OÃ¹ sont-ils",
                input_context_scores=[0.0, 0.0, 0.0, 0.0, 0.0],
                output_context_scores=[0.0] * 5,
            )
        ],
    )
    cli_out = attribute_context(in_out_ctx_encdec_special_sep)
    assert round_scores(cli_out) == expected_output
@pytest.mark.slow
def test_in_out_ctx_encdec_langtag_whitespace_sep():
    # Base case for context-aware encoder-decoder model with language tag in input and output.
    # Context is given in the source (input context) and translated into target context (output context)
    in_out_ctx_encdec_langtag_whitespace_sep = AttributeContextArgs(
        model_name_or_path="facebook/mbart-large-50-one-to-many-mmt",
        input_context_text="The girls were away.",
        input_current_text="Where are they?",
        output_template="{context} {current}",
        input_template="{context} {current}",
        tokenizer_kwargs={"src_lang": "en_XX", "tgt_lang": "fr_XX"},
        attributed_fn="contrast_prob_diff",
        show_viz=False,
        add_output_info=False,
        show_intermediate_outputs=False,
        # Pre-defining natural model outputs to avoid user input in unit tests
        output_context_text="Les filles Ã©taient loin.",
    )
    expected_output = AttributeContextOutput(
        input_context="The girls were away.",
        input_context_tokens=["â–The", "â–girls", "â–were", "â–away", "."],
        output_context="Les filles Ã©taient loin.",
        output_context_tokens=["â–Les", "â–filles", "â–Ã©taient", "â–loin", "."],
        output_current="OÃ¹ sont-elles?",
        output_current_tokens=["â–O", "Ã¹", "â–sont", "-", "elles", "?"],
        cti_scores=[0.33, 0.03, 0.21, 0.52, 4.49, 0.01],
        cci_scores=[
            CCIOutput(
                cti_idx=4,
                cti_token="elles",
                contrast_token="ils",
                cti_score=4.49,
                contextual_output="Les filles Ã©taient loin. OÃ¹ sont-elles",
                contextless_output="OÃ¹ sont-ils",
                input_context_scores=[0.0, 0.0, 0.0, 0.0, 0.0],
                output_context_scores=[0.0, 0.01, 0.0, 0.0, 0.0],
            )
        ],
    )
    cli_out = attribute_context(in_out_ctx_encdec_langtag_whitespace_sep)
    assert round_scores(cli_out) == expected_output
def test_save_reload_attribute_context_outputs(tmp_path):
    args = AttributeContextArgs(
        model_name_or_path="gpt2",
        input_context_text="George was sick yesterday.",
        input_current_text="His colleagues asked him to come",
        attributed_fn="contrast_prob_diff",
        show_viz=False,
        save_path=str(tmp_path) + "/test.json",
    )
    out_pre_save = attribute_context(args)
    with open(tmp_path / "test.json") as f:
        out_post_save = AttributeContextOutput.from_dict(json.load(f))
    assert out_pre_save == out_post_save

================
File: tests/data/test_aggregator.py
================
import json
import os
import torch
from pytest import fixture
import inseq
from inseq.data import FeatureAttributionSequenceOutput
from inseq.data.aggregator import (
    AggregatorPipeline,
    ContiguousSpanAggregator,
    PairAggregator,
    SequenceAttributionAggregator,
    SubwordAggregator,
)
from inseq.models import HuggingfaceDecoderOnlyModel, HuggingfaceEncoderDecoderModel
EXAMPLES_FILE = os.path.join(os.path.dirname(os.path.abspath(__file__)), "../fixtures/aggregator.json")
EXAMPLES = json.load(open(EXAMPLES_FILE))
@fixture(scope="session")
def saliency_mt_model() -> HuggingfaceEncoderDecoderModel:
    return inseq.load_model("Helsinki-NLP/opus-mt-en-it", "saliency", device="cpu")
@fixture(scope="session")
def saliency_gpt_model() -> HuggingfaceDecoderOnlyModel:
    return inseq.load_model("gpt2", "saliency", device="cpu")
def test_sequence_attribution_aggregator(saliency_mt_model: HuggingfaceEncoderDecoderModel):
    out = saliency_mt_model.attribute(
        "This is a test.",
        step_scores=["probability"],
        attribute_target=True,
        output_step_attributions=True,
        device="cpu",
        show_progress=False,
    )
    seqattr = out.sequence_attributions[0]
    assert seqattr.source_attributions.shape == (6, 7, 512)
    assert seqattr.target_attributions.shape == (8, 7, 512)
    assert seqattr.step_scores["probability"].shape == (7,)
    for i, step in enumerate(out.step_attributions):
        assert step.source_attributions.shape == (1, 6, 512)
        assert step.target_attributions.shape == (1, i + 1, 512)
    out_agg = seqattr.aggregate()
    assert out_agg.source_attributions.shape == (6, 7)
    assert out_agg.target_attributions.shape == (8, 7)
    assert out_agg.step_scores["probability"].shape == (7,)
def test_continuous_span_aggregator(saliency_mt_model: HuggingfaceEncoderDecoderModel):
    out = saliency_mt_model.attribute(
        "This is a test.", attribute_target=True, step_scores=["probability"], device="cpu", show_progress=False
    )
    seqattr = out.sequence_attributions[0]
    out_agg = seqattr.aggregate(ContiguousSpanAggregator, source_spans=(3, 5), target_spans=[(0, 3), (4, 6)])
    assert out_agg.source_attributions.shape == (5, 5, 512)
    assert out_agg.target_attributions.shape == (5, 5, 512)
    assert out_agg.step_scores["probability"].shape == (5,)
def test_span_aggregator_with_prefix(saliency_gpt_model: HuggingfaceDecoderOnlyModel):
    out = saliency_gpt_model.attribute("Hello, world! I am,:.", "Hello, world! I am,:.!,. Last")
    aggregated = out.aggregate("subwords", special_chars=("Ä ", "ÄŠ")).aggregate()
    assert aggregated[0].target_attributions.shape == (5, 2)
    assert aggregated[0].attr_pos_start == 3
    assert aggregated[0].attr_pos_end == 5
def test_aggregator_pipeline(saliency_mt_model: HuggingfaceEncoderDecoderModel):
    out = saliency_mt_model.attribute(
        "This is a test.", attribute_target=True, step_scores=["probability"], device="cpu", show_progress=False
    )
    seqattr = out.sequence_attributions[0]
    squeezesum = AggregatorPipeline([ContiguousSpanAggregator, SequenceAttributionAggregator])
    out_agg_squeezesum = seqattr.aggregate(squeezesum, source_spans=(3, 5), target_spans=[(0, 3), (4, 6)])
    assert out_agg_squeezesum.source_attributions.shape == (5, 5)
    assert out_agg_squeezesum.target_attributions.shape == (5, 5)
    assert out_agg_squeezesum.step_scores["probability"].shape == (5,)
    sumsqueeze = AggregatorPipeline([SequenceAttributionAggregator, ContiguousSpanAggregator])
    out_agg_sumsqueeze = seqattr.aggregate(sumsqueeze, source_spans=(3, 5), target_spans=[(0, 3), (4, 6)])
    assert out_agg_sumsqueeze.source_attributions.shape == (5, 5)
    assert out_agg_sumsqueeze.target_attributions.shape == (5, 5)
    assert out_agg_sumsqueeze.step_scores["probability"].shape == (5,)
    assert not torch.allclose(out_agg_squeezesum.source_attributions, out_agg_sumsqueeze.source_attributions)
    assert not torch.allclose(out_agg_squeezesum.target_attributions, out_agg_sumsqueeze.target_attributions)
    # Named indexing version
    named_squeezesum = ["spans", "scores"]
    named_sumsqueeze = ["scores", "spans"]
    out_agg_squeezesum_named = seqattr.aggregate(named_squeezesum, source_spans=(3, 5), target_spans=[(0, 3), (4, 6)])
    out_agg_sumsqueeze_named = seqattr.aggregate(named_sumsqueeze, source_spans=(3, 5), target_spans=[(0, 3), (4, 6)])
    assert out_agg_squeezesum_named.source_attributions.shape == (5, 5)
    assert out_agg_squeezesum_named.target_attributions.shape == (5, 5)
    assert out_agg_squeezesum_named.step_scores["probability"].shape == (5,)
    assert out_agg_sumsqueeze_named.source_attributions.shape == (5, 5)
    assert out_agg_sumsqueeze_named.target_attributions.shape == (5, 5)
    assert out_agg_sumsqueeze_named.step_scores["probability"].shape == (5,)
    assert not torch.allclose(
        out_agg_squeezesum_named.source_attributions, out_agg_sumsqueeze_named.source_attributions
    )
    assert not torch.allclose(
        out_agg_squeezesum_named.target_attributions, out_agg_sumsqueeze_named.target_attributions
    )
def test_subword_aggregator(saliency_mt_model: HuggingfaceEncoderDecoderModel):
    out = saliency_mt_model.attribute(EXAMPLES["source"], show_progress=False)
    seqattr = out.sequence_attributions[0]
    for idx, token in enumerate(seqattr.source):
        assert token.token == EXAMPLES["source_subwords"][idx]
    for idx, token in enumerate(seqattr.target):
        assert token.token == EXAMPLES["target_subwords"][idx]
    # Full aggregation
    out_agg = seqattr.aggregate(SubwordAggregator)
    for idx, token in enumerate(out_agg.source):
        assert token.token == EXAMPLES["source_merged"][idx]
    for idx, token in enumerate(out_agg.target):
        assert token.token == EXAMPLES["target_merged"][idx]
    # Source-only aggregation
    out_agg = seqattr.aggregate(SubwordAggregator, aggregate_target=False)
    for idx, token in enumerate(out_agg.source):
        assert token.token == EXAMPLES["source_merged"][idx]
    for idx, token in enumerate(out_agg.target):
        assert token.token == EXAMPLES["target_subwords"][idx]
    # Target-only aggregation
    out_agg = seqattr.aggregate(SubwordAggregator, aggregate_source=False)
    for idx, token in enumerate(out_agg.source):
        assert token.token == EXAMPLES["source_subwords"][idx]
    for idx, token in enumerate(out_agg.target):
        assert token.token == EXAMPLES["target_merged"][idx]
def test_pair_aggregator(saliency_mt_model: HuggingfaceEncoderDecoderModel):
    out = saliency_mt_model.attribute([EXAMPLES["source"], EXAMPLES["alternative_source"]], show_progress=False)
    orig_seqattr = out.sequence_attributions[0].aggregate(["vnorm"])
    alt_seqattr = out.sequence_attributions[1].aggregate(["vnorm"])
    diff_seqattr = orig_seqattr.aggregate(PairAggregator, paired_attr=alt_seqattr)
    for idx, token in enumerate(diff_seqattr.source):
        assert token.token == EXAMPLES["diff_subwords"][idx]
    assert torch.allclose(
        alt_seqattr.source_attributions - orig_seqattr.source_attributions, diff_seqattr.source_attributions
    )
    # Default aggregation with SequenceAttributionAggregator
    orig_seqattr_other = out.sequence_attributions[0].aggregate()
    alt_seqattr_other = out.sequence_attributions[1].aggregate()
    # Aggregate with aggregator name
    diff_seqattr_other = orig_seqattr_other.aggregate("pair", paired_attr=alt_seqattr_other)
    assert torch.allclose(diff_seqattr_other.source_attributions, diff_seqattr.source_attributions)
    # Aggregate with __sub__
    diff_seqattr_sub = orig_seqattr - alt_seqattr
    assert diff_seqattr_other == diff_seqattr_sub
def test_named_aggregate_fn_aggregation(saliency_mt_model: HuggingfaceEncoderDecoderModel):
    out = saliency_mt_model.attribute(
        [EXAMPLES["source"], EXAMPLES["alternative_source"]],
        show_progress=False,
        attribute_target=True,
        method="attention",
    )
    out_headmean = out.aggregate(aggregator=["mean", "mean"])
    assert out_headmean.sequence_attributions[0].source_attributions.ndim == 2
    assert out_headmean.sequence_attributions[0].target_attributions.ndim == 2
    assert out_headmean.sequence_attributions[1].source_attributions.ndim == 2
    assert out_headmean.sequence_attributions[1].target_attributions.ndim == 2
    out_allmean_subwords = out.aggregate(aggregator=["mean", "mean", "subwords"])
    # Check whether scores aggregation worked correctly
    assert out_allmean_subwords.sequence_attributions[0].source_attributions.ndim == 2
    assert out_allmean_subwords.sequence_attributions[0].target_attributions.ndim == 2
    assert out_allmean_subwords.sequence_attributions[1].source_attributions.ndim == 2
    assert out_allmean_subwords.sequence_attributions[1].target_attributions.ndim == 2
    # Check whether subword aggregation worked correctly
    assert (
        out_allmean_subwords.sequence_attributions[0].source_attributions.shape[0]
        < out.sequence_attributions[0].source_attributions.shape[0]
    )
    assert (
        out_allmean_subwords.sequence_attributions[0].target_attributions.shape[0]
        < out.sequence_attributions[0].target_attributions.shape[0]
    )
    assert (
        out_allmean_subwords.sequence_attributions[1].source_attributions.shape[0]
        < out.sequence_attributions[1].source_attributions.shape[0]
    )
    assert (
        out_allmean_subwords.sequence_attributions[1].target_attributions.shape[0]
        < out.sequence_attributions[1].target_attributions.shape[0]
    )
    out_allmean_subwords_expanded = out.aggregate(
        aggregator=["scores", "scores", "subwords"], aggregate_fn=["mean", "mean", None]
    )
    assert out_allmean_subwords == out_allmean_subwords_expanded
def test_slice_aggregator_decoder_only(saliency_gpt_model: HuggingfaceDecoderOnlyModel):
    out = saliency_gpt_model.attribute(
        EXAMPLES["source_summary"], EXAMPLES["source_summary"] + EXAMPLES["gen_summary"], show_progress=False
    )[0]
    out_sliced: FeatureAttributionSequenceOutput = out.aggregate("slices", target_spans=(14, 75))
    assert [t.token for t in out_sliced.source] == EXAMPLES["source_summary_tokens"]
    assert [t.token for t in out_sliced.target] == EXAMPLES["source_summary_tokens"] + EXAMPLES["gen_summary_tokens"]
    # Slice with __getitem__
    out_sliced_getitem = out[14:75]
    assert out_sliced == out_sliced_getitem
def test_slice_aggregator_encoder_decoder(saliency_mt_model: HuggingfaceEncoderDecoderModel):
    out = saliency_mt_model.attribute(
        EXAMPLES["source"], EXAMPLES["target"], show_progress=False, attribute_target=True
    )[0]
    out_sliced: FeatureAttributionSequenceOutput = out.aggregate("slices", source_spans=(1, 4))
    assert [t.token for t in out_sliced.source] == EXAMPLES["source_subwords"][1:4]
    assert [t.token for t in out_sliced.target] == EXAMPLES["target_subwords"][1:]
    # Slice with __getitem__
    out_sliced_getitem = out[1:4]
    assert out_sliced == out_sliced_getitem

================
File: tests/data/test_attribution.py
================
import torch
from pytest import fixture
from inseq import FeatureAttributionOutput, load_model
@fixture(scope="session")
def saliency_mt_model():
    return load_model("Helsinki-NLP/opus-mt-en-it", "saliency")
@fixture(scope="session")
def saliency_gpt2_model_tiny():
    return load_model("hf-internal-testing/tiny-random-GPT2LMHeadModel", "saliency")
def test_save_load_attribution(tmp_path, saliency_mt_model):
    out_path = tmp_path / "tmp_attr.json"
    out = saliency_mt_model.attribute("This is a test.", device="cpu", show_progress=False)
    out.save(out_path)
    loaded_out = FeatureAttributionOutput.load(out_path)
    assert out == loaded_out
def test_save_load_attribution_split(tmp_path, saliency_mt_model):
    out_path = tmp_path / "tmp_attr.json"
    out = saliency_mt_model.attribute(["This is a test.", "sequence number two"], device="cpu", show_progress=False)
    out.save(out_path, split_sequences=True)
    out_path_1 = tmp_path / "tmp_attr_0.json"
    loaded_out = FeatureAttributionOutput.load(out_path_1)
    assert torch.allclose(
        out.sequence_attributions[0].source_attributions, loaded_out.sequence_attributions[0].source_attributions
    )
def test_save_load_attribution_compressed(tmp_path, saliency_mt_model):
    out_path = tmp_path / "tmp_attr_compress.json.gz"
    out = saliency_mt_model.attribute("This is a test.", device="cpu", show_progress=False)
    out.save(out_path, compress=True)
    loaded_out = FeatureAttributionOutput.load(out_path, decompress=True)
    assert out == loaded_out
def test_save_load_attribution_float16(tmp_path, saliency_mt_model):
    out_path = tmp_path / "tmp_attr_compress.json.gz"
    out = saliency_mt_model.attribute("This is a test.", device="cpu", show_progress=False)
    out.save(out_path, compress=True, scores_precision="float16")
    loaded_out = FeatureAttributionOutput.load(out_path, decompress=True)
    assert torch.allclose(
        out.sequence_attributions[0].source_attributions,
        loaded_out.sequence_attributions[0].source_attributions,
        atol=1e-05,
    )
def test_save_load_attribution_float8(tmp_path, saliency_mt_model):
    out_path = tmp_path / "tmp_attr_compress.json.gz"
    out = saliency_mt_model.attribute("This is a test.", device="cpu", show_progress=False)
    out.save(out_path, compress=True, scores_precision="float8")
    loaded_out = FeatureAttributionOutput.load(out_path, decompress=True)
    assert torch.allclose(
        out.sequence_attributions[0].source_attributions,
        loaded_out.sequence_attributions[0].source_attributions,
        atol=1e-02,
    )
def test_get_scores_dicts_encoder_decoder(saliency_mt_model):
    out = saliency_mt_model.attribute(["This is a test.", "Hello world!"], device="cpu", show_progress=False)
    dicts = out.get_scores_dicts()
    assert len(dicts) == 2
    assert isinstance(dicts[0], dict) and isinstance(dicts[1], dict)
    assert "source_attributions" in dicts[0] and "target_attributions" in dicts[0] and "step_scores" in dicts[0]
def test_get_scores_dicts_decoder_only(saliency_gpt2_model_tiny):
    out = saliency_gpt2_model_tiny.attribute(
        ["This is a test", "Hello world!"],
        ["This is a test generation", "Hello world! Today is a beautiful day."],
        show_progress=False,
        device="cpu",
    )
    dicts = out.get_scores_dicts()
    assert len(dicts) == 2
    assert isinstance(dicts[0], dict) and isinstance(dicts[1], dict)
    assert "source_attributions" in dicts[0] and "target_attributions" in dicts[0] and "step_scores" in dicts[0]

================
File: tests/inference_commons.py
================
import json
import os
from inseq.data import EncoderDecoderBatch
from inseq.utils import json_advanced_load
from . import TEST_DIR
def get_example_batches():
    dict_batches = json_advanced_load(f"{TEST_DIR}/fixtures/m2m_418M_batches.json")
    dict_batches["batches"] = [batch.torch() for batch in dict_batches["batches"]]
    assert all(isinstance(batch, EncoderDecoderBatch) for batch in dict_batches["batches"])
    return dict_batches
def load_examples() -> dict:
    file = os.path.join(os.path.dirname(os.path.abspath(__file__)), "fixtures/huggingface_model.json")
    return json.load(open(file))

================
File: tests/models/test_huggingface_model.py
================
"""TODO: Skipping layer attribution when attributing target and the DIG method
since it is bugged is not very elegant, this will need to be refactored.
"""
import pytest
import torch
from pytest import fixture, mark
from transformers import AutoTokenizer
import inseq
from inseq import list_feature_attribution_methods
from inseq.data import FeatureAttributionOutput, FeatureAttributionSequenceOutput
from inseq.utils import get_default_device
from ..inference_commons import load_examples
EXAMPLES = load_examples()
USE_REFERENCE_TEXT = [True, False]
ATTRIBUTE_TARGET = [True, False]
RETURN_CONVERGENCE_DELTA = [True, False]
STEP_SCORES = [[], ["probability"]]
ATTRIBUTION_METHODS = list_feature_attribution_methods()
ATTENTION_IDX = [-2, [0, 5, 1], (1, -2), None]
ATTENTION_AGGREGATE_FN = ["average", None]
@fixture(scope="session")
def saliency_mt_model():
    return inseq.load_model("Helsinki-NLP/opus-mt-en-it", "saliency")
@fixture(scope="session")
def saliency_gpt2_model():
    return inseq.load_model("distilgpt2", "saliency")
@fixture(scope="session")
def saliency_gpt2_model_tiny():
    return inseq.load_model("hf-internal-testing/tiny-random-GPT2LMHeadModel", "saliency")
def test_tokenizer_consistency():
    texts = EXAMPLES["texts"][2][0]
    inf_model = inseq.load_model("distilgpt2", "saliency")
    tok_model = inseq.load_model(
        "distilgpt2", "saliency", tokenizer=AutoTokenizer.from_pretrained("distilgpt2", use_fast=False)
    )
    fast_tok_model = inseq.load_model("distilgpt2", "saliency", tokenizer=AutoTokenizer.from_pretrained("distilgpt2"))
    out_inferenced = inf_model.attribute(
        texts,
        show_progress=False,
        device=get_default_device(),
    )
    out_tok = tok_model.attribute(
        texts,
        show_progress=False,
        device=get_default_device(),
    )
    out_fast_tok = fast_tok_model.attribute(
        texts,
        show_progress=False,
        device=get_default_device(),
    )
    assert torch.allclose(
        out_inferenced.sequence_attributions[0].target_attributions,
        out_tok.sequence_attributions[0].target_attributions,
        atol=8e-2,
        equal_nan=True,
    )
    assert torch.allclose(
        out_inferenced.sequence_attributions[0].target_attributions,
        out_fast_tok.sequence_attributions[0].target_attributions,
        atol=8e-2,
        equal_nan=True,
    )
    assert torch.allclose(
        out_tok.sequence_attributions[0].target_attributions,
        out_fast_tok.sequence_attributions[0].target_attributions,
        atol=8e-2,
        equal_nan=True,
    )
@mark.slow
@mark.require_cuda_gpu
@mark.parametrize(("texts", "reference_texts"), EXAMPLES["short_texts"])
@mark.parametrize("attribute_target", ATTRIBUTE_TARGET)
def test_cuda_attribution_consistency_seq2seq(texts, reference_texts, attribute_target, saliency_mt_model):
    out = {}
    for device in ["cpu", "cuda"]:
        out[device] = saliency_mt_model.attribute(
            texts, reference_texts, show_progress=False, attribute_target=attribute_target, device=device
        )
        assert isinstance(out[device], FeatureAttributionOutput)
        assert isinstance(out[device].sequence_attributions[0], FeatureAttributionSequenceOutput)
    for out_cpu, out_gpu in zip(out["cpu"].sequence_attributions, out["cuda"].sequence_attributions, strict=False):
        assert all(tok_cpu == tok_gpu for tok_cpu, tok_gpu in zip(out_cpu.target, out_gpu.target, strict=False))
        attr_score_matches = [
            torch.allclose(cpu_attr, gpu_attr, atol=1e-3)
            for cpu_attr, gpu_attr in zip(out_cpu.source_attributions, out_gpu.source_attributions, strict=False)
        ]
        assert all(attr_score_matches)
@mark.slow
@mark.parametrize("attribution_method", ATTRIBUTION_METHODS)
@mark.parametrize("use_reference", USE_REFERENCE_TEXT)
@mark.parametrize("attribute_target", ATTRIBUTE_TARGET)
def test_batched_attribution_consistency_seq2seq(
    attribution_method, use_reference, attribute_target, saliency_mt_model
):
    if attribution_method == "discretized_integrated_gradients":
        pytest.skip("discretized_integrated_gradients currently unsupported")
    if attribution_method.startswith("layer_") and attribute_target:
        pytest.skip("Layer attribution methods do not support attribute_target=True")
    texts_single, reference_single = EXAMPLES["texts"][0]
    texts_batch, reference_batch = EXAMPLES["texts"][1]
    if not use_reference:
        reference_single, reference_batch = None, None
    out_single = saliency_mt_model.attribute(
        texts_single,
        reference_single,
        show_progress=False,
        attribute_target=attribute_target,
        device=get_default_device(),
        method=attribution_method,
    )
    out_batch = saliency_mt_model.attribute(
        texts_batch,
        reference_batch,
        show_progress=False,
        attribute_target=attribute_target,
        device=get_default_device(),
        method=attribution_method,
    )
    assert torch.allclose(
        out_single.sequence_attributions[0].source_attributions,
        out_batch.sequence_attributions[0].source_attributions,
        atol=8e-2,
    )
    if attribute_target:
        assert torch.allclose(
            out_single.sequence_attributions[0].target_attributions,
            out_batch.sequence_attributions[0].target_attributions,
            atol=8e-2,
            equal_nan=True,
        )
def test_batched_attribution_consistency_decoder_only(saliency_gpt2_model):
    texts_single, reference_single = EXAMPLES["short_texts_decoder"][0]
    texts_batch, reference_batch = EXAMPLES["short_texts_decoder"][1]
    out_single = saliency_gpt2_model.attribute(
        texts_single,
        reference_single,
        show_progress=False,
        device=get_default_device(),
    )
    out_batch = saliency_gpt2_model.attribute(
        texts_batch,
        reference_batch,
        show_progress=False,
        device=get_default_device(),
    )
    assert torch.allclose(
        out_single.sequence_attributions[0].target_attributions,
        out_batch.sequence_attributions[0].target_attributions,
        atol=8e-2,
        equal_nan=True,
    )
@mark.slow
@mark.parametrize(("texts", "reference_texts"), EXAMPLES["texts"])
@mark.parametrize("attribution_method", ATTRIBUTION_METHODS)
@mark.parametrize("use_reference", USE_REFERENCE_TEXT)
@mark.parametrize("attribute_target", ATTRIBUTE_TARGET)
@mark.parametrize("return_convergence_delta", RETURN_CONVERGENCE_DELTA)
@mark.parametrize("step_scores", STEP_SCORES)
def test_attribute_seq2seq(
    texts,
    reference_texts,
    attribution_method,
    use_reference,
    attribute_target,
    return_convergence_delta,
    step_scores,
    saliency_mt_model,
):
    if attribution_method == "discretized_integrated_gradients":
        pytest.skip("discretized_integrated_gradients currently unsupported")
    if attribution_method.startswith("layer_") and attribute_target:
        pytest.skip("Layer attribution methods do not support attribute_target=True")
    if not use_reference:
        reference_texts = None
    out = saliency_mt_model.attribute(
        texts,
        reference_texts,
        method=attribution_method,
        show_progress=False,
        attribute_target=attribute_target,
        return_convergence_delta=return_convergence_delta,
        step_scores=step_scores,
        internal_batch_size=50,
        n_steps=100,
        device=get_default_device(),
    )
    assert isinstance(out, FeatureAttributionOutput)
    assert isinstance(out.sequence_attributions[0], FeatureAttributionSequenceOutput)
    assert out.info["model_name"] == "Helsinki-NLP/opus-mt-en-it"
    assert out.info["constrained_decoding"] == use_reference
    assert out.info["attribution_method"] == attribution_method
    assert out.info["attribute_target"] == attribute_target
    assert out.info["step_scores"] == step_scores
    if "return_convergence_delta" in out.info:
        assert out.info["return_convergence_delta"] == return_convergence_delta
    if "internal_batch_size" in out.info:
        assert out.info["internal_batch_size"] == 50
    if "n_steps" in out.info:
        assert out.info["n_steps"] == 100
@mark.slow
@mark.parametrize(("texts", "reference_texts"), EXAMPLES["long_text"])
@mark.parametrize("attribution_method", ATTRIBUTION_METHODS)
@mark.parametrize("use_reference", USE_REFERENCE_TEXT)
def test_attribute_long_text_seq2seq(texts, reference_texts, attribution_method, use_reference, saliency_mt_model):
    if attribution_method == "discretized_integrated_gradients":
        pytest.skip("discretized_integrated_gradients currently unsupported")
    if not use_reference:
        reference_texts = None
    out = saliency_mt_model.attribute(
        texts,
        reference_texts,
        method=attribution_method,
        show_progress=False,
        internal_batch_size=10,
        n_steps=100,
        device=get_default_device(),
    )
    assert isinstance(out, FeatureAttributionOutput)
    assert isinstance(out.sequence_attributions[0], FeatureAttributionSequenceOutput)
def test_attribute_slice_seq2seq(saliency_mt_model):
    texts, reference_texts = EXAMPLES["texts"][2][0], EXAMPLES["texts"][2][1]
    out = saliency_mt_model.attribute(
        texts,
        reference_texts,
        show_progress=False,
        device=get_default_device(),
        attr_pos_start=13,
        attr_pos_end=17,
        attribute_target=True,
    )
    assert isinstance(out, FeatureAttributionOutput)
    assert len(out.sequence_attributions) == 3
    assert isinstance(out.sequence_attributions[0], FeatureAttributionSequenceOutput)
    ex1, ex2, ex3 = out.sequence_attributions[0], out.sequence_attributions[1], out.sequence_attributions[2]
    assert ex1.attr_pos_start == 12
    assert ex1.attr_pos_end == 17
    assert ex1.source_attributions.shape[1] == ex1.attr_pos_end - ex1.attr_pos_start
    assert ex1.target_attributions.shape[1] == ex1.attr_pos_end - ex1.attr_pos_start
    assert ex1.target_attributions.shape[0] == ex1.attr_pos_end
    # Empty attributions outputs have start and end set to seq length
    assert ex2.attr_pos_start == len(ex2.target)
    assert ex2.attr_pos_end == len(ex2.target)
    assert ex2.source_attributions.shape[1] == 0 and ex2.target_attributions.shape[1] == 0
    assert ex3.attr_pos_start == 13
    assert ex3.attr_pos_end == 16
    assert ex1.source_attributions.shape[1] == ex1.attr_pos_end - ex1.attr_pos_start
    assert ex1.target_attributions.shape[1] == ex1.attr_pos_end - ex1.attr_pos_start
    assert ex1.target_attributions.shape[0] == ex1.attr_pos_end
    assert out.info["attr_pos_start"] == 13
    assert out.info["attr_pos_end"] == 17
    aggregated = [ex1.aggregate(), ex2.aggregate(), ex3.aggregate()]
    assert all(isinstance(aggr_attr, FeatureAttributionSequenceOutput) for aggr_attr in aggregated)
def test_attribute_decoder(saliency_gpt2_model):
    texts = EXAMPLES["texts"][2][0]
    out = saliency_gpt2_model.attribute(
        texts,
        show_progress=False,
        device=get_default_device(),
        generation_args={"max_new_tokens": 10},
    )
    assert isinstance(out, FeatureAttributionOutput)
    assert len(out.sequence_attributions) == 3
    assert isinstance(out.sequence_attributions[0], FeatureAttributionSequenceOutput)
    ex1, ex2, ex3 = out.sequence_attributions[0], out.sequence_attributions[1], out.sequence_attributions[2]
    assert ex1.attr_pos_start == 17
    assert ex1.attr_pos_end == 22
    assert ex1.target_attributions.shape[1] == ex1.attr_pos_end - ex1.attr_pos_start
    assert ex1.target_attributions.shape[0] == ex1.attr_pos_end
    # Empty attributions outputs have start and end set to seq length
    assert ex2.attr_pos_start == 9
    assert ex2.attr_pos_end == 14
    assert ex2.target_attributions.shape[1] == ex2.attr_pos_end - ex2.attr_pos_start
    assert ex2.target_attributions.shape[0] == ex2.attr_pos_end
    assert ex3.attr_pos_start == 12
    assert ex3.attr_pos_end == 17
    assert ex3.target_attributions.shape[1] == ex3.attr_pos_end - ex3.attr_pos_start
    assert ex3.target_attributions.shape[0] == ex3.attr_pos_end
    assert out.info["attr_pos_start"] == 17
    assert out.info["attr_pos_end"] == 22
    aggregated = [attr.aggregate(attr._aggregator) for attr in out.sequence_attributions]
    assert all(isinstance(aggr_attr, FeatureAttributionSequenceOutput) for aggr_attr in aggregated)
def test_attribute_decoder_forced(saliency_gpt2_model_tiny):
    texts = [
        "Colorless green ideas sleep",
        "The scientist told the director that",
    ]
    forced_generations = [
        "Colorless green ideas sleep furiously.",
        "The scientist told the director that the experiment was a success.",
    ]
    out = saliency_gpt2_model_tiny.attribute(
        texts,
        forced_generations,
        show_progress=False,
        device=get_default_device(),
    )
    assert isinstance(out, FeatureAttributionOutput)
    assert len(out.sequence_attributions) == 2
    assert isinstance(out.sequence_attributions[0], FeatureAttributionSequenceOutput)
    ex1, ex2 = out.sequence_attributions[0], out.sequence_attributions[1]
    assert ex1.attr_pos_start == 14
    assert ex1.attr_pos_end == 19
    assert ex1.target_attributions.shape[1] == ex1.attr_pos_end - ex1.attr_pos_start
    assert ex1.target_attributions.shape[0] == ex1.attr_pos_end
    # Empty attributions outputs have start and end set to seq length
    assert ex2.attr_pos_start == 13
    assert ex2.attr_pos_end == 24
    assert ex2.target_attributions.shape[1] == ex2.attr_pos_end - ex2.attr_pos_start
    assert ex2.target_attributions.shape[0] == ex2.attr_pos_end
    assert out.info["attr_pos_start"] == 14
    assert out.info["attr_pos_end"] == 24
    aggregated = [attr.aggregate(attr._aggregator) for attr in out.sequence_attributions]
    assert all(isinstance(aggr_attr, FeatureAttributionSequenceOutput) for aggr_attr in aggregated)
def test_attribute_decoder_forced_sliced(saliency_gpt2_model_tiny):
    texts = [
        "Colorless green ideas sleep",
        "The scientist told the director that",
    ]
    forced_generations = [
        "Colorless green ideas sleep furiously.",
        "The scientist told the director that the experiment was a success.",
    ]
    out = saliency_gpt2_model_tiny.attribute(
        texts,
        forced_generations,
        show_progress=False,
        device=get_default_device(),
        attr_pos_start=16,
        attr_pos_end=20,
    )
    assert isinstance(out, FeatureAttributionOutput)
    assert len(out.sequence_attributions) == 2
    assert isinstance(out.sequence_attributions[0], FeatureAttributionSequenceOutput)
    ex1, ex2 = out.sequence_attributions[0], out.sequence_attributions[1]
    assert ex1.attr_pos_start == 16
    assert ex1.attr_pos_end == 19
    assert ex1.target_attributions.shape[1] == ex1.attr_pos_end - ex1.attr_pos_start
    assert ex1.target_attributions.shape[0] == ex1.attr_pos_end
    assert ex2.attr_pos_start == 16
    assert ex2.attr_pos_end == 20
    assert ex2.target_attributions.shape[1] == ex2.attr_pos_end - ex2.attr_pos_start
    assert ex2.target_attributions.shape[0] == ex2.attr_pos_end
    assert out.info["attr_pos_start"] == 16
    assert out.info["attr_pos_end"] == 20
    aggregated = [attr.aggregate(attr._aggregator) for attr in out.sequence_attributions]
    assert all(isinstance(aggr_attr, FeatureAttributionSequenceOutput) for aggr_attr in aggregated)
@mark.slow
@mark.parametrize(("texts", "reference_texts"), EXAMPLES["texts"])
@mark.parametrize("layers", ATTENTION_IDX)
@mark.parametrize("heads", ATTENTION_IDX)
@mark.parametrize("aggregate_heads_fn", ATTENTION_AGGREGATE_FN)
@mark.parametrize("aggregate_layers_fn", ATTENTION_AGGREGATE_FN)
def test_attention_attribution_seq2seq(
    texts,
    reference_texts,
    layers,
    heads,
    aggregate_heads_fn,
    aggregate_layers_fn,
    saliency_mt_model,
):
    if isinstance(layers, int):
        aggregate_layers_fn = "single"
    if isinstance(heads, int):
        aggregate_heads_fn = "single"
    out = saliency_mt_model.attribute(
        texts,
        method="attention",
        show_progress=False,
        attribute_target=True,
        device=get_default_device(),
        layers=layers,
        heads=heads,
        aggregate_heads_fn=aggregate_heads_fn,
        aggregate_layers_fn=aggregate_layers_fn,
    )
    assert isinstance(out, FeatureAttributionOutput)
    assert isinstance(out.sequence_attributions[0], FeatureAttributionSequenceOutput)
    assert out.info["model_name"] == "Helsinki-NLP/opus-mt-en-it"
    assert out.info["constrained_decoding"] is False
    assert out.info["attribution_method"] == "attention"
    assert out.info["attribute_target"] is True
    assert out.sequence_attributions[0].source_attributions.ndim == 2
@mark.slow
@mark.parametrize(("texts", "reference_texts"), EXAMPLES["texts"])
@mark.parametrize("layers", ATTENTION_IDX)
@mark.parametrize("heads", ATTENTION_IDX)
@mark.parametrize("aggregate_heads_fn", ATTENTION_AGGREGATE_FN)
@mark.parametrize("aggregate_layers_fn", ATTENTION_AGGREGATE_FN)
def test_attention_attribution_decoder(
    texts,
    reference_texts,
    layers,
    heads,
    aggregate_heads_fn,
    aggregate_layers_fn,
    saliency_gpt2_model,
):
    if isinstance(layers, int):
        aggregate_layers_fn = "single"
    if isinstance(heads, int):
        aggregate_heads_fn = "single"
    out = saliency_gpt2_model.attribute(
        texts,
        method="attention",
        show_progress=False,
        device=get_default_device(),
        layers=layers,
        heads=heads,
        aggregate_heads_fn=aggregate_heads_fn,
        aggregate_layers_fn=aggregate_layers_fn,
    )
    assert isinstance(out, FeatureAttributionOutput)
    assert isinstance(out.sequence_attributions[0], FeatureAttributionSequenceOutput)
    assert out.info["model_name"] == "gpt2"
    assert out.info["constrained_decoding"] is False
    assert out.info["attribution_method"] == "attention"
    assert out.sequence_attributions[0].target_attributions.ndim == 2

================
File: tests/models/test_model_config.py
================
import pytest
import inseq
from inseq.attr.feat.feature_attribution import DummyAttribution
from inseq.models import HuggingfaceDecoderOnlyModel
from inseq.models.model_config import MODEL_CONFIGS, ModelConfig
class MockModelConfig(ModelConfig):
    def __init__(self):
        super().__init__(**{field: "test" for field in ModelConfig.__dataclass_fields__.keys()})
class MockRequireConfigAttribution(DummyAttribution):
    """Mock attribution requiring model config."""
    method_name = "mock_require_config"
    def __init__(self, attribution_model, **kwargs):
        super().__init__(attribution_model, hook_to_model=False)
        self.use_model_config = True
        self.hook(**kwargs)
def test_missing_model_config_error():
    del MODEL_CONFIGS["GPT2LMHeadModel"]
    with pytest.raises(ValueError):
        inseq.load_model("hf-internal-testing/tiny-random-GPT2LMHeadModel", "mock_require_config")
    MODEL_CONFIGS["GPT2LMHeadModel"] = MockModelConfig()
    model = inseq.load_model("hf-internal-testing/tiny-random-GPT2LMHeadModel", "mock_require_config")
    assert isinstance(model, HuggingfaceDecoderOnlyModel)
    assert isinstance(model.attribution_method, MockRequireConfigAttribution)

================
File: tests/utils/test_torch_utils.py
================
import pytest
import torch
from inseq.utils.misc import pretty_tensor
from inseq.utils.torch_utils import filter_logits
@pytest.mark.parametrize(
    ("tensor", "output"),
    [
        (None, "None"),
        (torch.tensor([1, 1], dtype=torch.long), "torch.int64 tensor of shape [2]"),
        (
            torch.tensor([[1, 1, 1], [1, 1, 1]], dtype=torch.long),
            "torch.int64 tensor of shape [2, 3]",
        ),
        (torch.randn(4, 1, 1, 10), "torch.float32 tensor of shape [4, 1, 1, 10]"),
        (torch.randn(2, 21, 1), "torch.float32 tensor of shape [2, 21, 1]"),
    ],
)
def test_pretty_tensor(tensor: torch.Tensor, output: str) -> None:
    assert pretty_tensor(tensor).startswith(output)
def test_probits2prob():
    # Test with batch of size > 1
    probits = torch.stack(
        [
            torch.arange(0, 30000, 1.0),
            torch.arange(30000, 60000, 1.0),
            torch.arange(60000, 90000, 1.0),
            torch.arange(90000, 120000, 1.0),
        ]
    )
    target_ids = torch.tensor([10, 77, 999, 1765]).unsqueeze(-1)
    probs = torch.gather(probits, -1, target_ids.T)
    assert probs.shape == (1, 4)
    assert torch.eq(probs, torch.tensor([10.0, 77.0, 999.0, 1765.0])).all()
    # Test with batch of size 1
    probits = torch.stack(
        [
            torch.arange(0, 30000, 1.0),
        ]
    )
    target_ids = torch.tensor([23456]).unsqueeze(-1)
    probs = torch.gather(probits, -1, target_ids.T)
    assert probs.shape == (1, 1)
    assert torch.eq(probs, torch.tensor([23456.0])).all()
def test_filter_logits():
    original_logits = torch.tensor(
        [
            [1.0, 2.0, 3.0, 4.0, 5.0],
            [2.0, 3.0, 4.0, 5.0, 6.0],
            [3.0, 4.0, 5.0, 6.0, 7.0],
            [4.0, 5.0, 6.0, 7.0, 8.0],
        ]
    )
    filtered_logits = filter_logits(original_logits, top_k=2)
    topk2 = original_logits.clone().index_fill(1, torch.tensor([0, 1, 2]), float("-inf"))
    assert torch.eq(filtered_logits, topk2).all()
    filtered_logits = filter_logits(original_logits, top_p=0.9)
    topp90 = original_logits.clone().index_fill(1, torch.tensor([0, 1]), float("-inf"))
    assert torch.eq(filtered_logits, topp90).all()
    contrast_logits = torch.tensor(
        [
            [15.0, 13.0, 11.0, 9.0, 7.0],
            [13.0, 11.0, 9.0, 7.0, 5.0],
            [11.0, 9.0, 7.0, 5.0, 3.0],
            [9.0, 7.0, 5.0, 3.0, 1.0],
        ]
    )
    filtered_logits, contrast_logits = filter_logits(original_logits, contrast_logits=contrast_logits, top_k=2)
    top2merged = original_logits.clone().index_fill(1, torch.tensor([2, 3, 4]), float("-inf"))
    assert torch.eq(filtered_logits, top2merged).all()



================================================================
End of Codebase
================================================================
