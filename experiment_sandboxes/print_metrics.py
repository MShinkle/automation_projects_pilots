"""
Script to load and print evaluation metrics for trained SAE variants.

This script reads JSON results files generated by `benchmark_sae.py`
from the `./benchmark_results/` directory. It extracts key metrics
from different evaluation types (core, absorption, etc.) and prints
a formatted summary to the console for one or all SAE variants found
in `./custom_saes/`.
"""

import os
import json
import argparse
from typing import Dict, List

def get_sae_types() -> List[str]:
    """
    Scans the 'custom_saes' directory and returns a list of SAE type names.
    
    Assumes each .py file in 'custom_saes' corresponds to an SAE variant.
    Excludes '__init__.py' and any non-.py files.

    Returns:
        List[str]: A list of SAE variant names (without the .py extension).
    """
    return [f.replace(".py", "") for f in os.listdir("custom_saes") 
            if f.endswith(".py") and f != "__init__.py"]

def get_eval_types() -> List[str]:
    """
    Scans the 'benchmark_results' directory for subdirectories representing evaluations.

    Returns:
        List[str]: A list of directory names, assumed to be evaluation types (e.g., 'core').
    """
    results_dir = "benchmark_results"
    if not os.path.exists(results_dir):
        return []
    return [d for d in os.listdir(results_dir) 
            if os.path.isdir(os.path.join(results_dir, d))]

def load_metrics(sae_type: str) -> Dict:
    """
    Loads and aggregates metrics for a specific SAE type from its result files.

    Iterates through known evaluation types (subdirectories in 'benchmark_results'),
    finds the corresponding JSON result file for the given `sae_type`,
    parses the JSON, and extracts relevant metrics into a dictionary.

    Args:
        sae_type (str): The name of the SAE variant (e.g., 'base', 'top_k').

    Returns:
        Dict: A dictionary containing aggregated metrics for the SAE type.
              Keys are metric names (e.g., 'mse', 'l0_sparsity'), values are the
              corresponding metric values. Returns an empty dict if no results found.
    """
    metrics = {}
    base_path = "benchmark_results"
    for eval_type in get_eval_types():
        result_path = f"{base_path}/{eval_type}/pythia-70m-deduped_layer_3_{sae_type}_custom_sae_eval_results.json"
        if not os.path.exists(result_path):
            continue
            
        with open(result_path, 'r') as f:
            data = json.load(f)
            print(f"Loaded data from {result_path}")
            
        if eval_type == "absorption":
            metrics.update({
                "mean_absorption_fraction": data["eval_result_metrics"]["mean"]["mean_absorption_fraction_score"],
                "mean_full_absorption": data["eval_result_metrics"]["mean"]["mean_full_absorption_score"],
                "mean_split_features": data["eval_result_metrics"]["mean"]["mean_num_split_features"]
            })
            
        elif eval_type == "core":
            metrics.update({
                "ce_loss_score": data["eval_result_metrics"]["model_performance_preservation"]["ce_loss_score"],
                "explained_variance": data["eval_result_metrics"]["reconstruction_quality"]["explained_variance"],
                "mse": data["eval_result_metrics"]["reconstruction_quality"]["mse"],
                "l0_sparsity": data["eval_result_metrics"]["sparsity"]["l0"],
                "l1_sparsity": data["eval_result_metrics"]["sparsity"]["l1"],
                "frac_alive": data["eval_result_metrics"]["misc_metrics"]["frac_alive"]
            })
            
        elif eval_type == "scr":
            metrics.update({
                f"scr_metric_threshold_{n}": data["eval_result_metrics"]["scr_metrics"][f"scr_metric_threshold_{n}"]
                for n in [2, 5, 10, 20, 50, 100, 500]
            })
            
        elif eval_type == "sparse_probing":
            metrics.update({
                "sae_test_accuracy": data["eval_result_metrics"]["sae"]["sae_test_accuracy"],
                "sae_top_1_accuracy": data["eval_result_metrics"]["sae"]["sae_top_1_test_accuracy"],
                "sae_top_2_accuracy": data["eval_result_metrics"]["sae"]["sae_top_2_test_accuracy"],
                "sae_top_5_accuracy": data["eval_result_metrics"]["sae"]["sae_top_5_test_accuracy"],
                "llm_test_accuracy": data["eval_result_metrics"]["llm"]["llm_test_accuracy"],
                "llm_top_1_accuracy": data["eval_result_metrics"]["llm"]["llm_top_1_test_accuracy"],
                "llm_top_2_accuracy": data["eval_result_metrics"]["llm"]["llm_top_2_test_accuracy"],
                "llm_top_5_accuracy": data["eval_result_metrics"]["llm"]["llm_top_5_test_accuracy"]
            })
            
        elif eval_type == "tpp":
            metrics.update({
                f"tpp_total_metric_threshold_{n}": data["eval_result_metrics"]["tpp_metrics"][f"tpp_threshold_{n}_total_metric"]
                for n in [2, 5, 10, 20, 50, 100, 500]
            })
            metrics.update({
                "tpp_intended_diff_500": data["eval_result_metrics"]["tpp_metrics"]["tpp_threshold_500_intended_diff_only"],
                "tpp_unintended_diff_500": data["eval_result_metrics"]["tpp_metrics"]["tpp_threshold_500_unintended_diff_only"]
            })
    
    return metrics

def format_value(value, format_str='.4f'):
    """
    Formats a numerical value into a string, handling 'N/A' cases.

    Args:
        value (Any): The value to format. Expected to be numeric or 'N/A'.
        format_str (str, optional): The f-string format specifier for numeric values.
                                   Defaults to '.4f'.

    Returns:
        str: The formatted string representation of the value.
    """
    if value == 'N/A':
        return 'N/A'
    return f"{value:{format_str}}"

def print_metrics_for_sae(sae_type: str):
    """
    Loads and prints formatted metrics for a specific SAE type.

    Calls `load_metrics` to get the data and then prints it to the console
    in a structured format, grouped by evaluation type (implicitly).

    Args:
        sae_type (str): The name of the SAE variant to print metrics for.
    """
    print("print_metrics_for_sae called")
    print(f"\n{'='*80}")
    print(f"Metrics for {sae_type}")
    print(f"{'='*80}")
    
    metrics = load_metrics(sae_type)
    
    # Core metrics
    print("\nCore Metrics:")
    print(f"  MSE: {format_value(metrics.get('mse', 'N/A'))}")
    print(f"  Explained Variance: {format_value(metrics.get('explained_variance', 'N/A'))}")
    print(f"  CE Loss Score: {format_value(metrics.get('ce_loss_score', 'N/A'))}")
    print(f"  L0 Sparsity: {format_value(metrics.get('l0_sparsity', 'N/A'), '.2f')}")
    print(f"  L1 Sparsity: {format_value(metrics.get('l1_sparsity', 'N/A'), '.2f')}")
    print(f"  Fraction Alive: {format_value(metrics.get('frac_alive', 'N/A'))}")
    
    # # Sparse Probing metrics
    # print("\nSparse Probing Metrics:")
    # print("  SAE:")
    # print(f"    Test Accuracy: {format_value(metrics.get('sae_test_accuracy', 'N/A'))}")
    # print(f"    Top-1 Accuracy: {format_value(metrics.get('sae_top_1_accuracy', 'N/A'))}")
    # print(f"    Top-2 Accuracy: {format_value(metrics.get('sae_top_2_accuracy', 'N/A'))}")
    # print(f"    Top-5 Accuracy: {format_value(metrics.get('sae_top_5_accuracy', 'N/A'))}")

def print_metrics():
    """
    Prints formatted metrics for all available SAE types.

    Retrieves all SAE types using `get_sae_types` and then calls
    `print_metrics_for_sae` for each one.
    """
    sae_types = get_sae_types()
    
    for sae_type in sae_types:
        print_metrics_for_sae(sae_type)

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Print metrics for SAE(s)')
    parser.add_argument('sae_type', nargs='?', help='Optional: Specific SAE type to print metrics for')
    args = parser.parse_args()
    
    if args.sae_type:
        # Check if the specified SAE type exists
        sae_types = get_sae_types()
        if args.sae_type not in sae_types:
            print(f"Error: SAE type '{args.sae_type}' not found. Available types: {', '.join(sae_types)}")
            exit(1)
        print_metrics_for_sae(args.sae_type)
    else:
        print_metrics()
